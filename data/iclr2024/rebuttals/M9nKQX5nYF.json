[
    {
        "title": "On the Effect of Defection in Federated Learning and How to Prevent It"
    },
    {
        "review": {
            "id": "zVgCykxqe6",
            "forum": "M9nKQX5nYF",
            "replyto": "M9nKQX5nYF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8625/Reviewer_bhy3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8625/Reviewer_bhy3"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies federated learning problems where the agents can strategically choose the early quit the FL process. The authors present examples when such early quitting can happen and how these cases can affect the losses of all agents. The authors later present an algorithm: Adaptive Defection-aware Aggregation for Gradient Descent (ADA-GD) and claim that this algorithm can disincentivize early quitting under some assumptions on the loss functions, parameter space, and the agents' heterogeneity."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The authors are able to make several observations on why the defective early quitting can happen and identified cases when such quitting may or may not negatively influence the agents that have not quitted. Overall, the problem of preventing early quitting is an interesting branch in strategic FL problems."
                },
                "weaknesses": {
                    "value": "1. Unrealistic problem setting on the agents' incentives. The authors assume that the agents quit the FL process as long as they are \\epsilon close to optimal solution. In ADA-GD, the agents are simply ignored when they are sufficiently close to the \\epsilon-optimal set and are kept waiting in the system. This is highly unrealistic, if the faster agents know the server will implement ADA-GD, then the very first time they are ignored, why don't they early quit and run another round of local gradient update? The assumption that the agents are only first-order strategic is not a good enough assumption, there should be more careful design on the backward induction steps, and I think we need a much more sophisticated algorithm than ADA-GD to make sure strategic agents are happy (incentive compatible and individually rational) to participate in the algorithm given they have realistic outside options (like training the final gradient step themselves). \n2. Unrealistic assumptions on the server's information availability. In ADA-GD, the server can observe the agents' losses instead of just their gradients is not realistic.\n3. In sufficient discussion on the field of FL, strategic learning and insufficient comparisons with related works, I suggest the authors provide a table that list out the (1) type of strategic manipulations of related works in FL, as well as the (2) assumptions those paper make, and (3) convergence as well as robustness guarantees. Moreover, I'm very suspicious about the claim on page 4 \"if there is no shared minima for all agents, applying FL is not reasonable\". First of all, this is not known prior to participating in the FL process for all agents. Secondly, I suggest the authors discuss related works in personalized FL and further explain this claim here.\n4. Very restrictive settings like \"strong convexity, smoothness, realizability, and minimal heterogeneity\" makes the algorithm unable to run on deep learning tasks like vision and NLP.\n5. Unclear why the server wants to find a solution in W^*, as long as the agents are all \\epsilon happy, all participants should be fine with the outcome."
                },
                "questions": {
                    "value": "Please refer to the weaknesses part and explain why the unrealistic claims are in fact realistic. \n\nIn addition,\n1. What is the model used for 2 class Cifar-10 classification? Why does this satisfy all the assumptions in the paper?\n2. Which of the examples satisfy Assumption 4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8625/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698552564900,
            "cdate": 1698552564900,
            "tmdate": 1699637079009,
            "mdate": 1699637079009,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ertq5IBZSm",
                "forum": "M9nKQX5nYF",
                "replyto": "zVgCykxqe6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8625/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8625/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1/2"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and address their concerns below. We hope the reviewer will re-consider their score. \n\n> Unrealistic problem setting on the agents' incentives. The authors assume that the agents quit the FL process as long as they are \\epsilon close to optimal solution. In ADA-GD, the agents are simply ignored when they are sufficiently close to the \\epsilon-optimal set and are kept waiting in the system. This is highly unrealistic, if the faster agents know the server will implement ADA-GD, then the very first time they are ignored, why don't they early quit and run another round of local gradient update? The assumption that the agents are only first-order strategic is not a good enough assumption, there should be more careful design on the backward induction steps, and I think we need a much more sophisticated algorithm than ADA-GD to make sure strategic agents are happy (incentive compatible and individually rational) to participate in the algorithm given they have realistic outside options (like training the final gradient step themselves).\n\n\n\n\u201cthe very first time they are ignored, why don't they early quit and run another round of local gradient update\u201d is an interesting question. In this work, we restrict the action space of agents to \u201cdeciding if they want to quit\u201d. The reviewer suggests broadening this action space to include \u201cdeciding both on quitting and running additional rounds\u201d. This will make avoiding defections much more challenging.\n\nWhile we acknowledge this as an intriguing avenue for future exploration, our work is an initial step in comprehending agent defections in Federated Learning. One primary contribution of our study demonstrates that even within the simple action space of 'deciding whether to quit,' the widely used FedAvg method fails. Conversely, our algorithm proves effective. The next interesting question, as hinted by the reviewer, is: \"How can we prevent defections when the workers' action space becomes more complex?\" \n\nHaving said that, we encourage the reviewer to read our response to reviewer sato. It is possible to incorporate strategizing during local update steps, i.e., agents can leave the collaboration within the communication round. If the algorithm wants to be conservative against further strategizing, the rule in line 4 of the algorithm can be made more stringent. The natural trade-off here is that the more stringent the rule to detect the defecting agents, the worse the guarantee of the final model will be.\n\n\n> Unrealistic assumptions on the server's information availability. In ADA-GD, the server can observe the agents' losses instead of just their gradients is not realistic.\n\nWe would like first to address that this is the standard definition of a first-order oracle\u2014zeroth and first-order information. In most deep learning implementations, obtaining the loss of the model every few rounds is standard, and most optimization procedures have their stopping criterion based on the current loss. This is in no way contrary to the goal of federated learning, which only prohibits sharing raw data. If we were to consider the setting where the algorithms have gradient information only, there would be no way for the server to track how close a client is to meeting its goal: a client with a flat loss function might be very far from its optima, while a client with a very sharp function might be very close to it. Can the reviewer highlight why they think access to loss is unrealistic? \n  \nWe want to highlight the negative result of this work: FedAvg's inability to prevent harmful defections and the necessity of assumption 4 (minimal heterogeneity) for averting harmful defections in small step-size ICFO algorithms (as illustrated in Figure 6). While our algorithm relies on loss information, it can be a building block for developing algorithms in a broader context.\n\n> In sufficient discussion on the field of FL, strategic learning and insufficient comparisons with related works, I suggest the authors provide a table that list out the (1) type of strategic manipulations of related works in FL, as well as the (2) assumptions those paper make, and (3) convergence as well as robustness guarantees. \n\nThanks for the suggestion! We will add such a table. Please also see the response to reviewer y9Cn regarding related work and our three-page literature review in the appendix."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8625/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700430572536,
                "cdate": 1700430572536,
                "tmdate": 1700430572536,
                "mdate": 1700430572536,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SbJ3uzDvqr",
                "forum": "M9nKQX5nYF",
                "replyto": "zVgCykxqe6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8625/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8625/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 2/2"
                    },
                    "comment": {
                        "value": "> Moreover, I'm very suspicious about the claim on page 4 \"if there is no shared minima for all agents, applying FL is not reasonable\". First of all, this is not known prior to participating in the FL process for all agents. Secondly, I suggest the authors discuss related works in personalized FL and further explain this claim here.\n\nWe did not intend to say that applying FL is not reasonable if there is no shared optima for all agents. Indeed, personalized approaches can still benefit from collaboration. However, the vanilla FL objective we discuss and define in the paper (c.f., objective (2)) uses the same consensus model for all the agents. Without a shared optimum, collaboration (instead of training a local model) is not guaranteed to benefit the agents due to data heterogeneity. While this information is not known a priori, this paper aims to start with a simple, clean setting where joining collaboration can guarantee a good consensus model. We could have considered relaxations to this assumption, such as bounded data heterogeneity assumptions, but we strongly believe that it would only obfuscate the paper's main message. We will clarify this discussion in our final version.  \n\n> Very restrictive settings like \"strong convexity, smoothness, realizability, and minimal heterogeneity\" makes the algorithm unable to run on deep learning tasks like vision and NLP.\n\nThese assumptions are common in theoretical analysis, and for deep learning, we run experiments to justify the effectiveness of our algorithm. Specifically, our empirical validation of MNIST and CIFAR-10 with fully connected neural networks substantiates our theoretical analysis. Note that we do not assume strong convexity. We would like to highlight again that we do not know of any other theoretical work that explicitly and provably prevents defection. We start with tractable assumptions, and as we discuss in the paper, the problem is hard for existing algorithms, thus motivating additional assumptions like minimal heterogeneity. We will explore more relaxed settings in future work.\n\n> Unclear why the server wants to find a solution in W^*, as long as the agents are all \\epsilon happy, all participants should be fine with the outcome. \n\nTo clarify (see discussion below assumption 3 on page 5), our goal is to make all the agents $\\epsilon$ happy instead of finding a solution in $W^*$. We introduce $W^*$ because a model that is $\\epsilon$ good for every agent is close to $W^*$. The server wants to find a good model for all agents because our original learning goal (explained through the hospital example) is (1). A model that works well for all agents participating in the optimization will likely have a low population loss.\n\n> What is the model used for 2 class Cifar-10 classification? Why does this satisfy all the assumptions in the paper?   \n   \nWe run empirical validation of MNIST and CIFAR-10 with a (two-layer) fully connected neural network, they are mainly for empirically validating that our algorithm can indeed run on deep learning tasks like vision and NLP, as questioned by the reviewer as well.\n\n> Which of the examples satisfy Assumption 4?    \n    \nThe examples we gave in the papers serve as counterexamples, illustrating the outcomes when our specified assumptions are not met. For an example that satisfies assumption 4, please refer to [this anonymized figure](https://anonymous.4open.science/r/IncentivesFL-E225/intersection.png)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8625/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700430673085,
                "cdate": 1700430673085,
                "tmdate": 1700450506160,
                "mdate": 1700450506160,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WUElaV334V",
            "forum": "M9nKQX5nYF",
            "replyto": "M9nKQX5nYF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8625/Reviewer_VX8w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8625/Reviewer_VX8w"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied the problem of defections in federated learning, where agents may choose to defect permanently, i.e. withdrawing from the collaboration, if they are content with their instantaneous model in a round. The paper first analyzed the potential negative impact of such defection on the final model's robustness and ability to generalize. It distinguished between benign and harmful defections and explore the influence of (i) initial conditions (ii) learning rates (iii) aggregation methods on the occurrence of harmful defections. The paper then proposed a new algorithm which prevents defection and analyzed its properties theoretically and empirically (in comparison to FedAvg)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Overall this paper is written with a good clarity, and the illustrative examples are helpful for understanding the mechanisms / possible impact of defection\n- The paper studies an important and practical game theoretical consideration in federated learning, where the agents are not always incentivized to stay. Through detailed examples with two agents, the paper provided nice examples where the defections can be benign or harmful depending on the different initializations, learning rates or aggregation algorithms. \n- The proposed algorithm offers an intuitive and natural solution, where the server simply tunes the update direction to avoid the defection of any agent which is near the defection threshold. Theoretically under assumptions 1-4, this algorithm is guaranteed to prevent defection.\n- Numerical simulations in comparison to FedAvg are provided to support the theoretical result."
                },
                "weaknesses": {
                    "value": "- Under the current algorithm design, it seems that Assumption 4 (Minimal Heterogeneity) is rather crucial. However, such an assumption which essentially assumes no \"similar\" agents seem to be hard to achieve in practice, in particular agents tend to join federated learning if they share similar goals and have similar loss functions to minimize. It would be good to see more discussion how deviation from the perfect heterogeneity can impact the proposed algorithm's outcome.\n- The simulations presented in the paper were relatively week and were conducted in simple settings with only two agents. The empirical evaluations need to be strengthened with larger number of agents / more realistic datasets."
                },
                "questions": {
                    "value": "- Will algorithm 1 incentivize the agents that are on the verge of defecting to report non-truthful losses to the server? \n- How does the violation of minimal heterogeneity affect the guarantee of algorithm 1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8625/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698629731586,
            "cdate": 1698629731586,
            "tmdate": 1699637078903,
            "mdate": 1699637078903,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KM0sl5DwWH",
                "forum": "M9nKQX5nYF",
                "replyto": "WUElaV334V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8625/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8625/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comments and address their concerns about assumption 4 and experiments below. \n\n> Under the current algorithm design, it seems that Assumption 4 (Minimal Heterogeneity) is rather crucial. However, such an assumption which essentially assumes no \"similar\" agents seem to be hard to achieve in practice, in particular agents tend to join federated learning if they share similar goals and have similar loss functions to minimize. It would be good to see more discussion how deviation from the perfect heterogeneity can impact the proposed algorithm's outcome\n\n> How does the violation of minimal heterogeneity affect the guarantee of algorithm 1?\n\nIn Figure 6 on page 7, we presented an example that doesn't meet assumption 4 and concluded that no algorithm employing small step sizes can prevent harmful defections. Essentially, it is crucial to assume the presence of a continuous descent path from the initialization to the target region that doesn't cross the $\\epsilon$-sublevel set of any agent. Without this assumption, no gradient descent-based algorithm can prevent defections. Our assumption ensures the existence of such a path. We appreciate the reviewer for highlighting this aspect and plan to expand on this in our paper.\n\n> The simulations presented in the paper were relatively week and were conducted in simple settings with only two agents. The empirical evaluations need to be strengthened with larger number of agents / more realistic datasets.\n\nOur proposed method works for more than two agents and $K>1$.  We have conducted one additional experiment, please refer to [this anonymized figure](https://anonymous.4open.science/r/iclr_anony-3E5D/additioanl_results.png) with $10$ agents and $K=5$. We are running more extensive experiments and will include them in the camera-ready version of our paper. \n\n\n> Will algorithm 1 incentivize the agents that are on the verge of defecting to report non-truthful losses to the server?\n\nThis is an excellent question. This work assumes that all agents will report truthful first-order information. If the agents are allowed to report non-truthful losses, then every agent can tell the server their loss is maximal, and our algorithm 1 will degenerate to FedAvg; hence, we can\u2019t avoid defections. The problem will become much more complicated if the agents can tell non-truthful losses. In future work, we would like to explore this direction to ensure that truthful reporting is indeed incentive-compatible. \n\nWe hope the reviewer will reconsider their score."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8625/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700325039359,
                "cdate": 1700325039359,
                "tmdate": 1700325039359,
                "mdate": 1700325039359,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vRaSGMEp9p",
            "forum": "M9nKQX5nYF",
            "replyto": "M9nKQX5nYF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8625/Reviewer_sato"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8625/Reviewer_sato"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses when agents in a federated system will defect, and proposes a novel aggregation strategy to prevent defection.\nDefection occurs when any particular agent achieves an $\\epsilon$-optimal answer and quits from the federation system.\nThis paper demonstrates how the occurrence of defection is related to choices of initialization and learning rates.\nMoreover, the authors propose an example to show that defection is inevitable for the strategy of uniform aggregation.\nInstead of uniform aggregation, the paper proposes a novel strategy that predicts whether an agent will defect in the next round and projects the aggregated direction to the orthogonal space of its gradient.\nIt is claimed that no agent will defect under the novel aggregation strategy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper considers the quit of agents in federated systems, which is missing in previous works of federated learning. Agents stop contributing their data when their local models achieve the $\\epsilon$-optimal set of their local problems. The quitting mechanism naturally matches most real-life situations.\n2. The paper discusses the effects of defection in detail with an easy but clear example. Moreover, a counterexample is proposed to show failures of the uniform aggregation in avoiding defection."
                },
                "weaknesses": {
                    "value": "1. The quitting mechanism introduced in 'Rational Agents' is weird. Specifically, an agent only considers quitting after communication rounds while it does not consider quitting during local updates.\n2. The empirical experiment is not enough to justify the proposed method. Firstly, $K=1$ is not enough for the application of federated learning. Most applications of federated learning are carried out with a larger number of local updates, $K>1$.\n3. The performance of ADA-GD is not comparable with local SGD using uniform aggregation. Figure 7(a) reveals that local SGD achieves higher accuracy and lower error before the defection happens. In this way, I believe that uniform aggregation with a controller detecting the defection for an early stop will beat ADA-GD."
                },
                "questions": {
                    "value": "See Weaknesses and there are some typos as follows:\n1. Why is the updating direction of $w_2$ in Figure 5(a) not parallel with those of $w_1,w_3,w_4$? \n2. What do you mean with $(2\\epsilon,\\epsilon)$ in explaining Observation 3?\n3. The definition of $\\nabla F(w_{t-1})$ is missing in Algorithm 1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8625/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8625/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8625/Reviewer_sato"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8625/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698659004160,
            "cdate": 1698659004160,
            "tmdate": 1699637078764,
            "mdate": 1699637078764,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oQ9y52jFUq",
                "forum": "M9nKQX5nYF",
                "replyto": "vRaSGMEp9p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8625/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8625/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comments. We address the reviewer\u2019s main concerns about $K>1$ local steps in our theory and experiments below. We hope the reviewer will reconsider their score.     \n\n> The quitting mechanism introduced in 'Rational Agents' is weird. Specifically, an agent only considers quitting after communication rounds while it does not consider quitting during local updates.\n\nFirst, note that we demonstrate that FedAvg fails to prevent harmful defections, **even in the simple scenario where agents only quit after communication and not during local update steps**. Thus, considering the more complicated quitting mechanism proposed by the reviewer will only make preventing defections more challenging for existing algorithms. On the other hand, our method can easily be extended to the setting where the agents can potentially defect during the local steps. \n\nTo do so, we must change the rule predicting the defecting agents in line 4. We require a more conservative lower bound on the function value on the agent after $K>1$ local updates. We can get such a lower bound by noting that the Hessian is positive semi-definite for convex functions. This implies that the per-step improvement in function value decreases as we approach the optima (on a descent path). Now, as long as this conservative lower bound of the function value after $K$ steps is larger than the defection threshold for that agent, this agent will provably not defect during local updates. In our paper, we chose to discuss only $K=1$ as the main idea of our algorithm can succinctly be described in that simpler setting. We will add a remark discussing the quitting mechanism proposed by the reviewer. \n\n> The empirical experiment is not enough to justify the proposed method. Firstly, $K=1$ is not enough for the application of federated learning. Most applications of federated learning are carried out with a larger number of local updates, $K>1$.\n\nOur proposed method works for more than two agents and $K>1$.  We have conducted one additional experiment, please refer to [this anonymized figure](https://anonymous.4open.science/r/iclr_anony-3E5D/additioanl_results.png).\n\n> The performance of ADA-GD is not comparable with local SGD using uniform aggregation. Figure 7(a) reveals that local SGD achieves higher accuracy and lower error before the defection happens. In this way, I believe that uniform aggregation with a controller detecting the defection for an early stop will beat ADA-GD.\n\nWe respectfully disagree. As we have already mentioned in the caption of Figure 7, \u201cit is unfair to compare the highest accuracy of Local SGD with our method as Local SGD is not defection aware and could not simply stop at the highest point.\u201d Specifically, it will keep running without knowing any agent could defect in the next round, thus leading to poor accuracy. To illustrate this, consider a scenario with frequent defections. In such cases, determining the optimal stopping point becomes challenging. Regardless, in our new experiment (mentioned above), even the highest point of local SGD is worse than ADA-GD. \n\n> See Weaknesses and there are some typos as follows\u2026\n\nThanks, we will address these typos in our revision."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8625/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700286089557,
                "cdate": 1700286089557,
                "tmdate": 1700323644305,
                "mdate": 1700323644305,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "D0xX3njBYy",
            "forum": "M9nKQX5nYF",
            "replyto": "M9nKQX5nYF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8625/Reviewer_y9Cn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8625/Reviewer_y9Cn"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies possible defection in federated learning where clients can choose to opt out, which can negatively affect the final model performance. This paper proposes a new optimization algorithm that aggregates the clients differently to avoid defections and achieve convergence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The investigated topic is interesting and relevant for current FL algorithms.\n- Theoretical analysis is provided for smooth and convex problems.\n- Provide simple experiments to show that the proposed algorithms outperforms the previous algorithms."
                },
                "weaknesses": {
                    "value": "- A concern I have is that ADA-GD assumes that clients are required to be rational in the way that ADA-GD defines them to be. In other words, the fixed precision parameter $\\epsilon$ is the universal learning goal which may not be practical in practice where clients can have heterogeneous goals. The proposed definition of the rational agents is rather ambiguous, and also how to set the precision parameters is tricky. What does it really mean for a potentially defecting worker to be content in practice? \n\n- Another concern I have is the way the ADA-GD excludes updates from the defecting workers. Wouldn't this lead to a biased model towards non-defecting clients? \n\n- Also, I wonder that if the workers have the freedom opt in or opt out, wouldn't the server not be able to force the clients to participate? \n\n- Lastly, is there a reason that the work has not compared with other incentivized FL work such as [1-3] below?\n  [1] Yae Jee Cho, Divyansh Jhunjhunwala, Tian Li, Virginia Smith, and Gauri Joshi. To federate or not to federate: Incentivizing client participation in federated learning. arXiv preprint arXiv:2205.14840, 2022.\n  [2] Avrim Blum, Nika Haghtalab, Richard Lanas Phillips, and Han Shao. One for one, or all for all: Equilibria and optimality of collaboration in federated learning. In International Conference on Machine Learning, pp. 1005\u20131014. PMLR, 2021.\n  [3] Rachael Hwee Ling Sim, Yehong Zhang, Mun Choon Chan, and Bryan Kian Hsiang Low. Collaborative machine learning with incentive aware model rewards. In International Conference on Machine Learning, pp. 8927\u20138936. PMLR, 2020.\n\nI noticed that the authors have referenced the literature but I do not agree with the authors' note that these work are a bit orthogonal to your work. Why is this the case?"
                },
                "questions": {
                    "value": "See Weaknesses Above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8625/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8625/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8625/Reviewer_y9Cn"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8625/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698870375323,
            "cdate": 1698870375323,
            "tmdate": 1699637078611,
            "mdate": 1699637078611,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zTJ2kPKmLN",
                "forum": "M9nKQX5nYF",
                "replyto": "D0xX3njBYy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8625/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8625/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1/2"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their review. We address the reviewer's concerns below and hope they will reconsider their score.  \n\n> A concern I have is that ADA-GD assumes that clients are required to be rational in the way that ADA-GD defines them to be. In other words, the fixed precision parameter $\\epsilon$ is the universal learning goal which may not be practical in practice where clients can have heterogeneous goals. The proposed definition of the rational agents is rather ambiguous, and also how to set the precision parameters is tricky. What does it really mean for a potentially defecting worker to be content in practice?\n\nPlease note that it is not mandatory for all clients to have the same precision parameter, $\\epsilon$, nor is it necessary for the algorithm to set specific precision parameters. Each client determines their objectives, such as achieving a 90% model accuracy, **a decision that rests with the client rather than the server**. For the sake of clarity in our presentation, we use a uniform $\\epsilon$ precision parameter.\n\nIf clients have varying goals\u2014for example, some targeting 90% accuracy while others aim for 95%\u2014our algorithm can accommodate these differences. It does so by adjusting $\\epsilon$ to a personalized precision parameter $\\epsilon_m$ in line 4 of Algorithm 1. This modification allows the algorithm to predict a client's potential defection based on their unique learning objective. Consequently, the final output of our algorithm aims to satisfy each client\u2019s specific goal. Given that clients choose their own learning goals, they tend to be satisfied with the model provided to them. We will include this simple extension as an additional remark under the theorem.\n\n\n> Another concern I have is the way the ADA-GD excludes updates from the defecting workers. Wouldn't this lead to a biased model towards non-defecting clients?\n\nOur ADA-GD algorithm ensures that the resulting model is not biased toward clients who do not defect. It only produces a model (as described in case 3) when it predicts every client is likely to defect. This means that each client already possesses a nearly optimal model for them. We will make this point clear in the theorem's statement. This approach is feasible because ADA-GD dynamically assesses which clients are on the verge of defection at each time step. As a result, if ADA-GD predicts that a client is close to defection in a given round, it will not include that client's update. However, if the same client later shows a significant loss and is deemed less likely to defect, their updates will be included again. This method creates a back-and-forth movement near the edge of the client's $\\epsilon$-sublevel set, ensuring that the model remains effective for that client. For a visual representation, please refer to [this anonymized figure](https://anonymous.4open.science/r/IncentivesFL-E225/intersection.png).  \n\n\n\n> Also, I wonder that if the workers have the freedom opt in or opt out, wouldn't the server not be able to force the clients to participate?\n\nCompelling one party to act a certain way is difficult in real-world collaborations due to emerging data protection and privacy laws. For instance, in our running example with hospitals, several laws such as [HIPPA](https://www.hhs.gov/hipaa/index.html) and [CMIA](https://consumercal.org/about-cfc/cfc-education-foundation/cfceducation-foundationyour-medical-privacy-rights/confidentiality-of-medical-information-act/) prohibit a hospital from using a patient\u2019s data without prior permission, and the patient can choose to revoke this permission. This would make it hard for any agency to force hospitals to participate. It is true that in some scenarios, the server can enforce contractual obligations. However, the spirit of federated learning is to foster organic collaborations that continually benefit all the participants as opposed to such contractual collaborations."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8625/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700270025303,
                "cdate": 1700270025303,
                "tmdate": 1700270025303,
                "mdate": 1700270025303,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]