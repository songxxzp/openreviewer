[
    {
        "title": "One For All: Towards Training One Graph Model For All Classification Tasks"
    },
    {
        "review": {
            "id": "cTkh8jfrbF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2877/Reviewer_WF3H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2877/Reviewer_WF3H"
            ],
            "forum": "4IT2pgc9v6",
            "replyto": "4IT2pgc9v6",
            "content": {
                "summary": {
                    "value": "The paper proposes One for All (OFA), a novel framework that addresses how to unify various graph tasks with various graph data.  OFA uses text-attributed graphs, allowing nodes and edges to be described in natural language, and introduces a new graph prompting method for diverse tasks without fine-tuning. When trained across multiple graph data domains, OFA demonstrates strong performance, making it a pioneering multi-purpose graph classification model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.LLM and graph prompting are both highly effective methods in their respective domains. Combining the two to address the cross-domain TAG problem is both convincing and well-motivated. I highly appreciate the idea of this paper, which is quite enlightening.\n2. This paper is well-articulated."
                },
                "weaknesses": {
                    "value": "1. A work on graph prompt learning should be cited and discussed.\n* Tan, Zhen, et al. \"Virtual Node Tuning for Few-shot Node Classification.\" arXiv preprint arXiv:2306.06063 (2023).\n2. I highly recommend the authors to make the code public for readers to follow the work.\n3. I suggest exploring non-meta-learning scenario such as in GraphPrompt.\n* Liu, Zemin, et al. \"Graphprompt: Unifying pre-training and downstream tasks for graph neural networks.\" Proceedings of the ACM Web Conference 2023. 2023."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2877/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697461897797,
            "cdate": 1697461897797,
            "tmdate": 1699636231257,
            "mdate": 1699636231257,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "95bSjwJXww",
                "forum": "4IT2pgc9v6",
                "replyto": "cTkh8jfrbF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WF3H"
                    },
                    "comment": {
                        "value": "We sincerely thank reviewer WF3H for acknowledging our work and providing positive feedback. We address the reviewer\u2019s concerns as follows.\n\n> W1: A work on graph prompt learning [1] should be cited and discussed.\n> \n\nWe appreciate the recommendation to discuss this related work. We briefly discuss it here and will add it to our revision. VNT [1] offers a novel method for few-shot node classification (FSNC) by integrating trainable virtual node embeddings into the original graph, which serves as a form of graph prompting. For each new FSNC task, the pre-trained graph transformer model can be frozen and only the embedding of virtual nodes needs to be trained. However, VNT only focuses on node classification and the proposed virtual node needs to be retrained for each task. In contrast, OFA can perform node/link/graph few-shot classification without any retraining. \n\n> W2: I highly recommend the authors to make the code public for readers to follow the work.\n> \n\nThanks for the suggestion! We will publicize our code in the future. Meanwhile, we have already provided an anonymized link to our code repository in the paper (in Appendix D). This repository contains the complete implementation details along with necessary documentation.\n\n> W3: I suggest exploring non-meta-learning scenario such as in GraphPrompt.\n> \n\nWe thank the reviewer for raising this novel perspective and interesting work! During training for few-shot tasks, instead of constructing N-way K-shot tasks like meta learning, GraphPrompt [2] directly optimizes the task to similarize the embedding of nodes that are connected (each represented by node-centered subgraph) and dissimilarize those that are not connected. During the test, GraphPrompt determines the class of a target node by comparing its embedding with the prototype of each class. \n\nThe key advantage of GraphPrompt lies in its unsupervised training approach, and we recognize the potential for our OFA framework to adopt a similar strategy with minimal adjustments. For instance, we could design pre-text tasks within OFA to predict the connection relationship between two Node of Interest (NOI) graphs. A straightforward implementation could be connecting two NOI graphs to the NOI prompt node. The raw text for the class nodes in this scenario could be sentences like 'these two nodes are connected' or 'these two nodes are not connected.\u2019 Other contrastive methods can also be explored. We will add this discussion and related works in the revision and regard this as an important direction to explore in the future!\n\nReferences:\n\n[1] Tan et al., Virtual Node Tuning for Few-shot Node Classification, arXiv, 2023.\n\n[2] Liu et al., Graphprompt: Unifying pre-training and downstream tasks for graph neural networks, WWW, 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699993456551,
                "cdate": 1699993456551,
                "tmdate": 1699993456551,
                "mdate": 1699993456551,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Gvj9nNEofa",
                "forum": "4IT2pgc9v6",
                "replyto": "cTkh8jfrbF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer WF3H:\n\nAs the discussion period comes to a close, we would like to thank you again for your positive assessment. Your support and inspirational comments have been invaluable to us. We remain open and eager to incorporate any further feedback or insights you might offer."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364492850,
                "cdate": 1700364492850,
                "tmdate": 1700364492850,
                "mdate": 1700364492850,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HBmCCx0L56",
            "forum": "4IT2pgc9v6",
            "replyto": "4IT2pgc9v6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2877/Reviewer_A8Qx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2877/Reviewer_A8Qx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes One-for-All (OFA), a framework for building and training a single graph model that can perform various graph-related tasks across different domains. The experimental results on real-world datasets demonstrate the effectiveness and efficiency of the proposed framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tA unified graph model is designed for various tasks and different domains of graph data.\n\n2.\tThe introduced model can encode features of different graphs into the same embedding space.\n\n3.\tExtensive experiments are conducted on various settings and datasets from different domains."
                },
                "weaknesses": {
                    "value": "1.\tThe design of prompt nodes and utilizing NOI-subgraph to unify different tasks seem to be similar to several previous works. For example,  [1] introduced a learnable prompt vector to unify tasks, and [2-3] proposed to utilize super nodes to perform pooling readout. The proposed NOI prompt node appears to be a combination of these two lines of works. Could you include more in-depth discussions and comparisons with these related works.\n\n2.\tComparison with methodologies that integrate LLMs and graph models, such as [4-7], is necessary. \n\n3.\tThe Few-shot and Zero-shot results, especially for TLP-SURGL and TLP-BGRL, do not outperform baseline models. Could you include further discussions and analysis of these underwhelming performance results? \n\n4. The ability of the LLMs to unify the feature space across datasets from diverse domains is very interesting. It would be insightful to see the unified feature representations of datasets within the same domain (e.g., two citation networks) and the datasets spanning different domains (e.g., a citation network and a molecular network). For example, some visual analysis will offer insights into the effectiveness of the LLM in harmonizing feature spaces across heterogeneous datasets.\n\n5. Additional ablation studies can be conducted to see whether the constructed graph structure is helpful. For example, directly using the unified node features (produced by LLMs) as the input feature of some backbone GNN models like GIN, GCN, GAT.\n\n6. The article introduces a method to employ LLMs to generate inputs for GNNs. Another line of works aims to utilize GNNs to generate inputs for LLMs, and it seems that this approach can better utilized LLMs' significant capabilities in addressing various problems on vast data. I'm curious about the difference between these two pipelines. Can you provide some explanations and compare the performance of these two pipelines.\n\n\n\n[1] Zemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. In WWW, 2023.\n\n[2] F. Hu, Y. Zhu, S. Wu, and T. Tan. Hierarchical graph convolutional networks for semi-supervised node classification. In IJCAI, 2019.\n\n[3] Matthias Fey, Jan-Gin Yuen and Frank Weichert. Hierarchical Inter-Message Passing for Learning on Molecular Graphs. In ICML, 2020.\n\n[4] Xiaoxin He, Xavier Bresson, Thomas Laurent, and Bryan Hooi. Explanations as features: Llm-based features for text-attributed graphs. arXiv preprint arXiv:2305.19523, 2023.\n\n[5] Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and Jian Tang. Learning on large-scale text-attributed graphs via variational inference. arXiv preprint arXiv:2210.14709, 2022.\n\n[6] Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. Natural language is all a graph needs. arXiv preprint arXiv:2308.07134, 2023.\n\n[7] Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, and Jian Tang. Graphtext: Graph reasoning in text space. arXiv preprint arXiv:2310.01089, 2023."
                },
                "questions": {
                    "value": "see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2877/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2877/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2877/Reviewer_A8Qx"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2877/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698673622135,
            "cdate": 1698673622135,
            "tmdate": 1700729502981,
            "mdate": 1700729502981,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hOXBYGE7zL",
                "forum": "4IT2pgc9v6",
                "replyto": "HBmCCx0L56",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer A8Qx Part 1/3"
                    },
                    "comment": {
                        "value": "We greatly appreciate reviewer A8Qx for the insightful review to help us improve the paper. We address the reviewer\u2019s concerns as follows:\n\n> W1: The design of prompt nodes and utilizing NOI-subgraph to unify different tasks seem to be similar to several previous works. For example, [1] introduced a learnable prompt vector to unify tasks, and [2-3] proposed to utilize super nodes to perform pooling readout. The proposed NOI prompt node appears to be a combination of these two lines of works. Could you include more in-depth discussions and comparisons with these related works.\n> \n\nWe thank the reviewer for mentioning these works and some of the works are indeed inspirations to our paper. We briefly discuss them here and will include all discussions in the revision. Concretely, GraphPrompt [1] introduces a trainable vector to prompts on subgraphs for downstream tasks. This means that for a new task, it needs to train a new prompting vector, and hence it cannot perform zero-shot tasks. Whereas in our work, the prompting is through an NOI subgraph that is connected to the original graph. Particularly, task information is described in natural language and encoded by LLM as node features in the NOI subgraph. Thus, for a new class/task, we can describe the task by human-interpretable text and inject the information into the input graph to make predictions without any fine-tuning, like in the case of language models.\n\nWe would like to emphasize the distinct roles of supernodes in H-GCN [2] and hierarchical inter-message passing [3] and the NOI prompt node in our work. Specifically, [2] and [3] both use a predefined non-neural algorithm to automatically organize nodes into groups pooled into supernodes and then perform message-passing on the supernodes. The algorithm generates fixed results based on the graph structure. The goal of these approaches is to use a pre-defined algorithm to better reason about particular structures in the graph. However, in our work, the connections between the NOI prompt node and nodes of interest are independent of the graph structure but user-defined and task-dependent. It is used to instruct the GNN to attend to the nodes that are the targets of the task.\n\n> W2: Comparison with methodologies that integrate LLMs and graph models, such as [4-7], is necessary.\n> \n\nWe thank reviewer A8Qx for the suggestion to include more related works and briefly discuss them here. As this is a quickly emerging field, we were not able to include some of the mentioned concurrent work in the original submission [6, 7], but we will include all discussions in the revision. First, All of these methods [4-7] focus on node classification, while our method OFA trains one model for all the node-/link-/graph-level tasks across multiple datasets. Both ExpAsFeat [4] and GLEM [5] apply GNN to the graph with encoded text embedding generated by a Language Model (LM). ExpAsFeat uses LM to generate several different texts to better describe a node. GLEM aims to improve the text embedding quality by a variational EM algorithm that alternately updates GNN and LM. However, these methods require distinct training for different datasets. Both InstructGLM [6] and GraphText [7] transfer graph information into texts as the input to the LLM, with InstructGLM designing a prompt paradigm to describe node neighbors and GraphText proposing a graph-syntax tree to provide sorted sequences of nodes. These methods only describe graph connections by texts and do not use GNNs, thus they might lose essential graph structure information. \n\n> W3: The Few-shot and Zero-shot results, especially for TLP-SURGL and TLP-BGRL, do not outperform baseline models. Could you include further discussions and analysis of these underwhelming performance results?\n> \n\nWe would like to first clarify that while TLP-SUGRL outperforms OFA in few-shot tasks, OFA outperforms TLP-BGRL on all tasks except the Cora 5-shot task. Note that OFA is tested in a transfer setting on the Cora dataset: the Cora dataset is not included in the training set, while TLP-BGRL and TLP-SUGRL are both trained and tested on the Cora dataset. Also, neither TLP-SUGRL nor TLP-BGRL can perform zero-shot tasks, while OFA can.\n\nThe outstanding performance of TLP-SUGRL, as pointed out by recent research [8], can be attributed to its unsupervised contrastive training paradigm which also includes test data during training (no label information is leaked, but test node information is present during contrastive learning). Such a paradigm is valid for transductive learning settings, where training and test nodes are on the same graph, but will fail in inductive settings, where training and test nodes are not on the same graph. Conversely, OFA still works on inductive settings."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699993082972,
                "cdate": 1699993082972,
                "tmdate": 1699993082972,
                "mdate": 1699993082972,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0rjTbORCXa",
                "forum": "4IT2pgc9v6",
                "replyto": "HBmCCx0L56",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer A8Qx Part 2/3"
                    },
                    "comment": {
                        "value": "> W4: The ability of the LLMs to unify the feature space across datasets from diverse domains is very interesting. It would be insightful to see the unified feature representations of datasets within the same domain (e.g., two citation networks) and the datasets spanning different domains (e.g., a citation network and a molecular network). For example, some visual analysis will offer insights into the effectiveness of the LLM in harmonizing feature spaces across heterogeneous datasets.\n> \n\nThanks for your insightful suggestion! We provide visualization of generated node embedding for different datasets in (https://anonymous.4open.science/r/OFA-6745/visualization_dataset.pdf) and will include it in the revision. We can see that LLM indeed embeds nodes from different domain to different space, which further explain the reason why a single GNN can work effectively for data from different domain using the OFA framework. \n\n> W5: Additional ablation studies can be conducted to see whether the constructed graph structure is helpful. For example, directly using the unified node features (produced by LLMs) as the input feature of some backbone GNN models like GIN, GCN, GAT.\n> \n\nWe would like to first highlight the ablation study in Table 12. We conducted experiments on ArXiv and HIV datasets. The experiment includes three cases: 1, Full, which uses the proposed NOI prompt graph. 2, -NOI prompt node, which connects class nodes directly to the NOI. 3, *(The case that the reviewer kindly suggested)* -Class node or no prompting, which has no prompt node and feeds the original graph to a GNN. In this case, to make a prediction, we apply mean pooling to the NOI, concatenate the pooled vector with the class description embedding vector, and apply MLP to the final vector for binary classification.\n\nAll three cases can either **jointly** or **separately** learn both ArXiv and HIV datasets. We compare their performance under these two cases. We observe that the proposed graph structure does not help with learning when datasets are trained separately because the downstream MLP only needs to learn one prediction target, and the proposed structure is essentially pooling on the NOI. However, we also observe that when the two datasets are trained together, the proposed structure **considerably increases the performance** over other cases. In joint training, as the downstream MLP needs to learn multiple tasks, it will be very difficult for the MLP to switch to corresponding tasks based only on a pooled vector. On the contrary, the proposed graph structure helps to distinguish different tasks for the MLP, because text-based task description is provided as the prompt node\u2019s feature.\n\nWe additionally conducted an experiment comparing *cases of Full and -Class node* using the same setup but with more datasets, and show the results below. Note we use GCN as the backbone model.\n\n|  | Joint/Full | Joint/-Class node | Separate/Full | Separate/-Class node |\n| --- | --- | --- | --- | --- |\n| ArXiv | 75.23\u00b10.03 | 75.19\u00b10.10 | 75.06\u00b10.08 | 75.39\u00b10.09 |\n| HIV | 75.92\u00b10.45 | 71.60\u00b10.54 | 75.81\u00b10.36 | 75.43\u00b10.50 |\n| Cora-node | 75.48\u00b10.29 | 71.39\u00b10.07 | 75.72\u00b10.49 | 76.12\u00b10.87 |\n| Cora-link | 92.27\u00b10.84 | 89.51\u00b10.46 | 93.16\u00b10.95 | 92.80\u00b10.62 |\n\nThe models show similar patterns when there are more datasets. We notice that the joint training performance on three out of the four datasets significantly drops when we switch from the proposed prompting to no prompting. Besides the performance consideration, the design of graph prompting is to unify different tasks for in-context learning. For example, we need to significantly alter the model to perform few-/zero-shot learning using the no prompting approach, while the current approach does not need to modify any component of the trained model."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699993208548,
                "cdate": 1699993208548,
                "tmdate": 1699993208548,
                "mdate": 1699993208548,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jiTWk7E61Y",
                "forum": "4IT2pgc9v6",
                "replyto": "HBmCCx0L56",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer A8Qx Part 3/3"
                    },
                    "comment": {
                        "value": "> W6: The article introduces a method to employ LLMs to generate inputs for GNNs. Another line of works aims to utilize GNNs to generate inputs for LLMs, and it seems that this approach can better utilized LLMs' significant capabilities in addressing various problems on vast data. I'm curious about the difference between these two pipelines. Can you provide some explanations and compare the performance of these two pipelines.\n> \n\nWe thank the reviewer again for this thoughtful comment. We did some research and didn\u2019t find an exact match to the reviewer\u2019s suggestion \u201cutilize GNNs to generate inputs for LLMs\u201d. But we did find related research that uses graph descriptive language to transform a graph into a text sequence, and directly apply an LLM to the sequence to make downstream predictions. (If this is not what the reviewer meant, please let us know in the comment.) For a detailed comparison between using LLM as a graph feature enhancer versus as a predictor, we kindly refer the reviewer to our general response.\n\nThese works are certainly influential. Their most prominent advantage is that they are naturally integrated into LLM. Consequently, it is easy to adapt it to have conversational/explanatory abilities. However, these approaches only minimally incorporate graph topological information which is the core of using graph data, because the connections are described by plain texts and such description is difficult for the LLM to learn as pointed out by preliminary works like [9]. Note that most of these works also extract subgraphs around prediction targets first and use the graph descriptive language to describe the subgraph. The LLM might only serve as a summarizer of the neighborhood information rather than learning the structure. On the contrary, in OFA, the graph structures are explicitly preserved. Recent development on GNN shows that GNN is a reliable architecture for learning graph structure which guarantees that important topological information is not lost.\n\nReferences:\n\n[1] Liu et al., Graphprompt: Unifying pre-training and downstream tasks for graph neural networks, WWW, 2023.\n\n[2] Hu et al., Hierarchical graph convolutional networks for semi-supervised node classification, IJCAI, 2019.\n\n[3] Fey et al., Hierarchical Inter-Message Passing for Learning on Molecular Graphs, ICML, 2020.\n\n[4] He et al., Explanations as features: Llm-based features for text-attributed graphs, arXiv, 2023.\n\n[5] Zhao et al., Learning on large-scale text-attributed graphs via variational inference, arXiv 2022.\n\n[6] Ye et al., Natural language is all a graph needs, arXiv, 2023.\n\n[7] Zhao et al., Graphtext: Graph reasoning in text space, arXiv, 2023.\n\n[8] Liu et al., Graph Contrastive Learning Meets Graph Meta Learning: A Unified Method for Few-shot Node Tasks, arXiv, 2023. \n\n[9] Guo et al. GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking, arXiv, 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699993248081,
                "cdate": 1699993248081,
                "tmdate": 1699993248081,
                "mdate": 1699993248081,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k6wYhcn0bN",
                "forum": "4IT2pgc9v6",
                "replyto": "HBmCCx0L56",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer A8Qx:\n\nWe sincerely appreciate your detailed and constructive review of our paper. As the discussion period is near its end, we would like to ensure our response aligns with your expectations and addresses your concerns. In our response, we have discussed the uniqueness of our method OFA compared to related works and clarified our few-shot and zero-shot results. We also provided additional visualization and experiments to better demonstrate the model design. We appreciate your feedback and look forward to any further comments."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364430281,
                "cdate": 1700364430281,
                "tmdate": 1700364430281,
                "mdate": 1700364430281,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x2yPyMI4Nd",
                "forum": "4IT2pgc9v6",
                "replyto": "HBmCCx0L56",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer A8Qx:\n\nWe just want to reach out to you again and see if our response addresses your concern. Your comments really inspire us, and we are eager to continue discussing our work with you."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691579117,
                "cdate": 1700691579117,
                "tmdate": 1700691579117,
                "mdate": 1700691579117,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "McMhsa6U5R",
                "forum": "4IT2pgc9v6",
                "replyto": "x2yPyMI4Nd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2877/Reviewer_A8Qx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2877/Reviewer_A8Qx"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thank you for providing comprehensive responses regarding additional results and discussions. The majority of my previous concerns have been addressed. Consequently, I will raise my score from 5 to 6."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729477328,
                "cdate": 1700729477328,
                "tmdate": 1700729477328,
                "mdate": 1700729477328,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vWri1qqDk1",
            "forum": "4IT2pgc9v6",
            "replyto": "4IT2pgc9v6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2877/Reviewer_RxxK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2877/Reviewer_RxxK"
            ],
            "content": {
                "summary": {
                    "value": "In the field of artificial intelligence, creating a single model to handle diverse tasks has been a longstanding goal. Large language models have excelled in language-related tasks, but applying this versatility to graph-based tasks is challenging. Graph data from different domains have unique attributes and distributions, making uniform representation difficult. Additionally, graph tasks encompass nodes, links, and graphs, necessitating distinct strategies. Furthermore, context-aware learning for graphs lacks a clear method.\n\nTo address these challenges, this paper introduced \"One for All\" (OFA), a framework that uses a single graph model to conquer these obstacles. OFA uses text-attributed graphs, unifying diverse graph data and standardizing task representation with \"nodes-of-interest.\" It pioneers a novel graph prompting approach for versatile task handling without fine-tuning. They train OFA using data from various domains and evaluate its performance in diverse learning scenarios. The results establish OFA as the first general-purpose graph classification model, revolutionizing the field of graph-based artificial intelligence."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The motivation of this paper is well-delivered and interesting\n\nThe proposed method is novel and seems to be good according to their experimental report"
                },
                "weaknesses": {
                    "value": "W1. translating graph data with text feature sequences might lose some key information from graphs. \n\nW2. According to Figure 1, LLM enhances text for text-attributed graphs. it still needs a GNN as a predictor. (1) I wonder whether the GNN model is also pre-trained and frozen. and (2) what are the results compared with using LLM as a predictor instead of an enhancer?\n\nW3. It is unclear how to use NOI prompt nodes and class nodes with GNN to predict the downstream tasks. according to Figure 2, can I say that the downstream tasks (node, edge, graph classifications) are treated as predicting links between NOI prompt and class nodes? It seems that NOI prompt nodes can be treated as a special case of the prompt graph mentioned in the paper \"All in One\" (Sun et al., 2023), what's the differences between them? and why the authors use only one NOI prompt node instead of multiple NOI prompt nodes.\n\nW4. I wonder why the authors use sentence transformer instead of ChatGPT API, or LLAMA, etc since they claim that \"any kind of LLM can be used as the encoder, and a stronger LLM potentially yields better overall performance.\" Is it possible that a very large language model contains too much unrelated knowledge/intelligence that may reduce task performance?\n\nW5. I wonder what would happen if you Re-order the item in your prompt for ChatGPT.  It seems the reason why you use sentence transformer is that it is not sensitive to the order of your prompt? if you change to ChatGPT, different orders may generate entirely different sentences, making the downstream performance unpredictable"
                },
                "questions": {
                    "value": "see W1-4\n\nI would like to see the rebuttal to the questions mentioned in the above section \u201cPaper Weakness\u201d. I\u2019m afraid that I might have not sufficient time to see a very long rebuttal. A concise and clear one would be good.\n\nThe potential weakness won't prevent me from raising my final score. I just want to make clear whether my understanding is correct."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2877/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816114838,
            "cdate": 1698816114838,
            "tmdate": 1699636231076,
            "mdate": 1699636231076,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ictx7LCozb",
                "forum": "4IT2pgc9v6",
                "replyto": "vWri1qqDk1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RxxK"
                    },
                    "comment": {
                        "value": "We thank reviewer RxxK for detailed and knowledgeable feedback, and we address the reviewer\u2019s concerns as follows. Per the reviewer\u2019s request, we tried to keep the response succinct. We are happy to explain further if the reviewer\u2019s concerns remain after the response. \n\n> W1: translating graph data with text feature sequences might lose some key information from graphs.\n> \n\nOur method uses LLM to only encode text that exists in graphs (node and edge attributes) from different domains to the same vector embedding space. The embedding vectors then serve as original node/edge features in graphs for GNN to predict. The graph structure/adjacency information is intact. Therefore, our method can maintain both graph structure and feature information. \n\n> W2: According to Figure 1, LLM enhances text for text-attributed graphs. it still needs a GNN as a predictor. (1) I wonder whether the GNN model is also pre-trained and frozen. and (2) what are the results compared with using LLM as a predictor instead of an enhancer?\n> \n\n(1) For both supervised and few-shot scenarios, we first train our randomly initialized GNN model on the training data using the label information. Then, our GNN model is frozen for any new tasks. (2) We provide an additional comparison with baselines using LLM as a predictor in the general response. Key findings show that these models often excel in node classification but may overlook graph structure. In contrast, OFA maintains graph topology using GNNs and is capable of handling multiple task levels across various datasets simultaneously.\n\n> W3: It is unclear how to use NOI prompt nodes and class nodes with GNN to predict the downstream tasks. according to Figure 2, can I say that the downstream tasks (node, edge, graph classifications) are treated as predicting links between NOI prompt and class nodes? It seems that NOI prompt nodes can be treated as a special case of the prompt graph mentioned in the paper \"All in One\" (Sun et al., 2023), what's the differences between them? and why the authors use only one NOI prompt node instead of multiple NOI prompt nodes.\n> \n\n**Short response:** (1) We apply an MLP binary classifier to the embedding of the class nodes generated from a GNN to make classifications. (2) The embedding of NOI prompt nodes encodes task information with human-readable texts, which endows OFA with zero-shot ability. Whereas, the prompt graph in All-in-One [1] is obtained through task-specific training, which needs to be retrained once facing new tasks. (3) The NOI prompt node is used to integrate both task and graph information and one NOI prompt node with our designed connection pattern is both effective and simple. \n\n**Additional discussion:** Here we provide a detailed discussion on point (1). All nodes in the prompted graph, including the NOI prompt nodes and class nodes, have text descriptions. The texts of NOI nodes are task descriptions, such as \u201cnode classification on literature category\u201d and \u201cgraph classification on molecule property\u201d. The texts of class nodes are descriptions of specific classes for a task described in the NOI prompt node, such as \u201cComputer science literature category\u201d and \u201cHIV inhibition molecule property\u201d. We use LLMs to convert texts to node vector features, and the graphs with vector features are fed to GNN to generate node vector representations. The vector representations of the class nodes contain **task, class, and graph information** because of the GNN. We then apply MLP to the class nodes to make binary classification and select the class with the highest probability. \n\n> W4: I wonder why the authors use sentence transformer instead of ChatGPT API, or LLAMA, etc. Is it possible that a very large language model contains too much unrelated knowledge/intelligence that may reduce task performance?\n> \n\nWe chose a smaller version of LLM for two reasons: (1) It is easier to work with, which allows flexible deployment and development. (2) As the reviewer\u2019s perceptive question suggested, concurrent work [2] shows that larger LLMs such as ChatGPT and LLaMa are specifically tuned for conversational purposes, and might not perform as well in sentence encoding scenarios. \n\n> W5: I wonder what would happen if you Re-order the item in your prompt for ChatGPT. It seems the reason why you use sentence transformer is that it is not sensitive to the order of your prompt?\n> \n\nOur method uses LLM to only encode text that exists in graphs (node and edge attributes)  to the same vector embedding space. Since each node and edges are encoded independently, our encoding schema is invariant to the order of nodes and edges.\n\nReference:\n\n[1] Sun et al., All in One: Multi-task Prompting for Graph Neural Networks, KDD, 2023. \n\n[2] Chen et al., Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs, arXiv, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699991963995,
                "cdate": 1699991963995,
                "tmdate": 1699991963995,
                "mdate": 1699991963995,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TTEaCHmoSy",
                "forum": "4IT2pgc9v6",
                "replyto": "vWri1qqDk1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer RxxK:\n\nThank you again for acknowledging our contribution and raising insightful points to improve our paper. As the discussion period ends soon, we would like to check whether our response addresses your concerns. Our explanations have provided clarity on the NOI prompt structure, the training procedure, and the comparison between models using LLM as a predictor. Looking forward to your feedback!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364402752,
                "cdate": 1700364402752,
                "tmdate": 1700364402752,
                "mdate": 1700364402752,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lHmfE7v1Zt",
            "forum": "4IT2pgc9v6",
            "replyto": "4IT2pgc9v6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2877/Reviewer_LMu3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2877/Reviewer_LMu3"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a unified paradigm for learning different tasks over graphs and across different domains. Notably, the proposed algorithms achieve competitive performance in various tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The framework proposed by the author is very interesting. The idea of prompted nodes naturally unifies the three major tasks in one simple but powerful problem formulation. \n2. The presentation of the result is clear with both quantitative and qualitative results."
                },
                "weaknesses": {
                    "value": "1. The soundness of the experiments seems lacking. Based on my understanding, LLM is one if the key component to achieve cross-task and cross-domain generalization. However, the choice of LLM is rarely discussed. I didn't find detailed description of the LLM used in the main experiments. Neither a comparison between different LLMs is missing in main paper and appendix. \n2. The modeling of link-level task lack details. For example, do you have two or one class node?"
                },
                "questions": {
                    "value": "Please refer to the weakness for questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2877/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2877/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2877/Reviewer_LMu3"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2877/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699513922347,
            "cdate": 1699513922347,
            "tmdate": 1700641386427,
            "mdate": 1700641386427,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LQhbISgjBb",
                "forum": "4IT2pgc9v6",
                "replyto": "lHmfE7v1Zt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LMu3"
                    },
                    "comment": {
                        "value": "We greatly appreciate reviewer LMu3\u2019s insightful feedback and critical comments to help us refine our work. We address the reviewer\u2019s concerns as follows:\n\n> W1: The soundness of the experiments seems lacking. Based on my understanding, LLM is one if the key component to achieve cross-task and cross-domain generalization. However, the choice of LLM is rarely discussed. I didn't find detailed description of the LLM used in the main experiments. Neither a comparison between different LLMs is missing in main paper and appendix.\n> \n\nWe have mentioned the use of sentence transformer as our LLM at the end of section 3.1, and we will give a more detailed discussion regarding the choice of the LLM in our revision. As reviewer LMu3 acutely pointed out, LLM is indeed an indispensable part of our framework, but the framework is not limited to any particular LLM model. Rather, the key contribution of the paper is that we can use any LLM to unify cross-domain graph tasks, and we chose the LLM that is easy to work with to demonstrate the validity of such an approach.\n\nFurthermore, we agree with reviewer LMu3 that a study of the effect of LLM in our model is important to evaluate the framework. Hence, we provide additional experimental results. We select another LLM e5-large-v2 [1] (will include results for Llama2-7B and Llama2-13B [2] once we have it) and conduct experiments on the supervised learning scenario using all datasets (same experimental setting as OFA-joint in the paper). The results are shown in the following:\n\n| Dataset | Cora  | Cora  | PubMed  | PubMed  | ogbn-arxiv  | Wiki-CS | FB15K237 | WN18RR | HIV | PCBA |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Task Type | Link | Node | Link | Node | Node | Node | Link | Link | Graph | Graph |\n| Metric | AUC \u2191 | Acc \u2191 | AUC \u2191 | Acc \u2191 | Acc \u2191 | Acc \u2191 | Acc \u2191 | Acc \u2191 | AUC \u2191 | APR \u2191 |\n| OFA-joint-ST | 94.04\u00b10.49 | 75.90\u00b11.26 | 98.21\u00b10.02 | 75.54\u00b10.05 | 75.54\u00b10.11 | 78.34\u00b10.35 | 95.54\u00b10.06 | 96.91\u00b10.11 | 78.02\u00b10.17 | 24.83\u00b10.10 |\n| OFA-joint-e5 | 92.75\u00b10.42 | 70.73\u00b11.69 | 98.44\u00b10.06 | 78.09\u00b11.71 | 75.81\u00b10.11 | 72.51\u00b10.39 | 95.27\u00b10.35 | 97.76\u00b10.39 | 78.56\u00b11.69 | 25.30\u00b10.29 |\n\nFrom the result, we can see that our framework still works effectively with different LLMs. Moreover, we can see that different LLMs encode language differently which results in slightly different performance on individual datasets.\n\n> W2: The modeling of link-level task lack details. For example, do you have two or one class node?\n> \n\nThe number of class nodes depends on the number of target link classes. \n\n**For homogeneous graphs**, where the goal is to predict whether a link exists between two nodes, we have one class node. Since we perform binary classification using class node embedding, a prediction result of 1 indicates that a link exists between the two nodes, and a result of 0 indicates the opposite. For example, in a co-citation network where links represent co-citations, the goal is to predict if two papers are co-cited. Then for two target papers, we connect a NOI prompt node to the two papers\u2019 corresponding nodes and connect one class node to the NOI prompt node. The text of the class node is \u201cThe papers are co-cited\u201d. Then, if the binary classification result gives 1, the model predicts the link exits, and if the result gives 0, the model predicts the link does not exist. **For heterogeneous graphs/knowledge graphs**, we can have more class nodes because two nodes can be connected by different relations, like \u201clives in\u201d, \u201cparent of\u201d, and \u201cworks for\u201d. The construction of the prompt graph is similar. Both nodes in the link connect to the NOI prompt node, and the NOI prompt node connects to k class nodes, where k is the number of target relations. The binary classification results on class nodes represent the likelihood of relations corresponding to the class nodes. Note that link-level tasks are very similar to node-level and graph-level tasks and can be unified into the same graph-prompting framework introduced in the paper. They only differ by their nodes of interest (NOI), and hence their connections between the NOI and the NOI prompt nodes are different.\n\nReference:\n\n[1] Wang et al., Text Embeddings by Weakly-Supervised Contrastive Pre-training, arXiv 2022. \n\n[2] Touvron et al., Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699991238294,
                "cdate": 1699991238294,
                "tmdate": 1699991238294,
                "mdate": 1699991238294,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R8Dmqr8Nga",
                "forum": "4IT2pgc9v6",
                "replyto": "lHmfE7v1Zt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2877/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer LMu3:\n\nAs the discussion period ends soon, we would like to check whether our responses answer your questions. Following your comments, we conducted experiments to test different choices of LLM and provided a detailed explanation of the model to address your concerns. Thank you again for your comments and suggestions to improve our paper, and we look forward to your reply."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364363917,
                "cdate": 1700364363917,
                "tmdate": 1700364363917,
                "mdate": 1700364363917,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q3lV7iLO6C",
                "forum": "4IT2pgc9v6",
                "replyto": "LQhbISgjBb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2877/Reviewer_LMu3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2877/Reviewer_LMu3"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the additional results"
                    },
                    "comment": {
                        "value": "Thanks for author's clarification on link prediction and different backbone LMs. It would be nice if you can consider more recent LLMs in your results to demonstrates the applicability of the proposed framework. As a result, I would like to raise my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641353501,
                "cdate": 1700641353501,
                "tmdate": 1700641353501,
                "mdate": 1700641353501,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]