[
    {
        "title": "Efficient local linearity regularization to overcome catastrophic overfitting"
    },
    {
        "review": {
            "id": "gsFTjWJSF2",
            "forum": "SZzQz8ikwg",
            "replyto": "SZzQz8ikwg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2787/Reviewer_e1KG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2787/Reviewer_e1KG"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to mitigate the catastrophic overfitting (CO) phenomenon in adversarial training by enforcing local linearity in the underlying model. Their proposed solution (ELLE) is a computationally efficient approach to enforce local linearity without using double backpropagation, which is time-intensive. Instead, this approach enforces local linearity using multiple forward passes, which is memory-intensive, and hence trading off time for space. Experiments show that their proposed approach is faster than the alternatives, and is successful at mitigating CO."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- This paper proposes a simple but effective strategy to enforce local linearity that does not incur double backpropagation costs. Their theoretical results also show that their method implicitly reduces a quantity related to the Hessian, which is expected.\n\n- The paper has a nice study on the CO problem in the experiments section, especially where it defines \u201cperturbation-wise\u201d and \u201cduration-wise\u201d CO, and has nice figures demonstrating CO and its avoidance with the provided method."
                },
                "weaknesses": {
                    "value": "**Not conceptually novel, fails to discuss simpler solutions**\n\nThe main idea presented is not conceptually novel. Local linearity has been presented as a solution to catastrophic overfitting several times in the literature (which the paper also discusses). The only novelty is regularizing for linearity using an objective that does not involve gradients. I am unsure how novel or interesting this part itself is \u2013 for instance, one can just as well use any other local linearity regularizer (for example, LLR or GradAlign), and replace the gradient terms therein using finite differences. This would eliminate the need for the \u201cdouble backpropagation\u201d as well and meet their desiderata. The paper needs to discuss why these simple solutions are undesirable if at all they are. \n\n**Misses key references**\n\nThe paper misses a couple of key references (see [1,2]), which aim to achieve the same objective as this given paper. It would be great if the authors could comment on how their method compares with the approaches presented in these works.\n\n[1] Singla et al., Low Curvature Activations Reduce Overfitting in Adversarial Training, 2021  \n[2] Srinivas et al., Efficient Training of Low-Curvature Neural Networks, 2022\n\n**Experiments ignore a canonical adversarial defense: PGD**\n\nWhile the experimental section nicely demonstrates CO and the effect of its method, it **fails to present comparisons** with the canonical method of PGD [Madry et al., 2018]. While speed comparisons are made with PGD in Figure 1, there do not appear to be accuracy comparisons (AA, Clean accuracy) or comparisons with regard to the ability of PGD to locally linearize the model or usage of the proposed method (ELLE) with PGD. This notable omission of PGD significantly weakens the paper. Rather, the paper focuses primarily on improving FGSM-based defenses (although it also discusses improving GAT and N-FGSM in the experiments), which is a significantly weaker defense than PGD"
                },
                "questions": {
                    "value": "- The authors claim in Section 3.3 that the GAT and NuAT methods are not relevant since they attempt to make models locally constant as opposed to locally linear. But surely, local constancy is a special case of local linearity, and thus it is not inconceivable to compare the two methods on equal grounds, like you do for LLR and GradAlign?\n\n\n- The authors are encouraged to comment on the highlighted weaknesses of the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2787/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2787/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2787/Reviewer_e1KG"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2787/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697581549036,
            "cdate": 1697581549036,
            "tmdate": 1700586239928,
            "mdate": 1700586239928,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H9XHlT0cje",
                "forum": "SZzQz8ikwg",
                "replyto": "gsFTjWJSF2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2787/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2787/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer e1KG for their feedback. We have made the changes in the text visible in red. We address below their concerns. \n\n- **(Q1): Not conceptually novel**\n\n**(A1):** We respectfully disagree with this statement. Admittedly, the spirit of enforcing local linearity has been well applied as a solution to CO. However, there are several key contributions introduced in our paper:\n- Our method is the first to avoid the 3x time cost coming from Double Backpropagation while being based on the same local linearity principle. \n- We theoretically link our regularization term and LLR [1] as approximants of second-order directional derivatives (Prop. 1 and 3). Compared to LLR, ELLE poses the advantage of not requiring double backpropagation.\n- We develop the first method to adaptively regularize local linearity and showcase its benefits.\n- We showcase the appearance of Delayed CO for several other methods and demonstrate that adding our regularization can resolve this issue.\n\nWhile our approach maintains simplicity, the results in the paper demonstrate the robustness of the method to tackle CO and outperform SOTA for large $\\epsilon$. If the reviewer has noticed any of the previous contributions in related papers, we are happy to reevaluate this list. \n\n- **(Q2): Why not use Finite Differences to approximate LLR or GradAlign?**\n\n**(A2):** Let us explain why Finite Differences don\u2019t fit well in this setup. We theoretically link LLR and ELLE by showing both regularization terms approximate second-order directional derivatives (see Prop. 3 and 1). Therefore, using a Finite Differences approximation of the gradient in the LLR case is unnecessary.\n\nWe elaborate on the approximation of GradAlign with Finite Differences in Appendix B.14. Estimating the gradients via Finite Differences is computationally expensive with $d+1$ function evaluations needed for estimating every gradient ($d = 248,832$ for ImageNet). Additionally, when more efficient approximations are involved [2], the regularization term becomes non-smooth due to the appearance of the sign function and it is not directly differentiable, as we would desire for a regularization term. Moreover, it requires one more function evaluation than ELLE. If the reviewer has in mind an alternative way to compute the finite differences, we would be happy to discuss this further. \n\n- **(Q3): Missing References.**\n\n**(A3):** We are thankful for the references that link curvature and robust generalization. We accordingly cite [3] and [4] in the introduction. However, we emphasize that there exist distinct differences between these papers and our method.\n\n[3] analyze the role of the curvature of the activation functions in robust generalization when performing AT. Our method is not based on architectural design, but the optimization method used to train a given architecture. The combination of low curvature architecture design and regularization terms to enforce local linearity is an interesting future direction.\n\n[4] propose a method for controlling the global curvature of twice differentiable neural networks to improve robust generalization. Our method controls the local curvature of the network and is not restricted to twice-differentiable classifiers.\n\n- **(Q4): Experiments ignore a canonical adversarial defense: PGD**\n\n**(A4):** AT is an effective but expensive defense. This is precisely the reason that faster single-step alternatives flourished [6,7], trading off performance for a 10x speed up. The main target of our work is handling critical failures and weaknesses of single-step adversarial training methods. Ideally, we would like single-step methods as reliable as AT PGD-10 but without the computational burden of PGD. \n\nTo address the reviewer\u2019s concern, we have included the AT PGD-10 evaluation in [Fig 1](https://imgur.com/a/xjV7Bdc), [Fig 2](https://imgur.com/a/sMToIKd), [Fig 3](https://imgur.com/a/aFpRbkz) and [Fig 5](https://imgur.com/a/e7Z8mQ1). The ability of AT PGD-10 to generate locally linear models is well known and displayed in Fig. 2, where both the gradient misalignment and our local linear approximation error are controlled during training. N-FGSM+ELLE-A is the method that attains the closest performance to AT PGD-10.\n\n- **(Q5): Isn\u2019t enforcing local constancy (GAT and NuAT) a special case of local linearity?**\n\n**(A5):** We thank the reviewer for the attentive reading. We have revised the text in section 3.3.\n\nLet us clarify that in the original submission, we already had comparisons against GAT. As studied in [5], GAT and NuAT are not able to overcome CO for large values of $\\epsilon$. In [Fig 5](https://imgur.com/a/e7Z8mQ1), we show that GAT suffers from CO for all cases except PRN at $\\epsilon=8/255$. When combined with ELLE-A, GAT training is stabilized and CO does not appear.\n\nWe will be happy to answer any other comments or questions the reviewer might have."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700041201229,
                "cdate": 1700041201229,
                "tmdate": 1700041201229,
                "mdate": 1700041201229,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hEy7vNZVLl",
                "forum": "SZzQz8ikwg",
                "replyto": "qTW6iu5Y3n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2787/Reviewer_e1KG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2787/Reviewer_e1KG"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you for the rebuttal, and the experiments with PGD. \n\n(A2) \" a Finite Differences approximation of the gradient in the LLR case is unnecessary\"; \"Estimating the gradients via Finite Differences is computationally expensive with function evaluations\"\n\nPlease note; finite-difference style approaches are necessary primarily to enable efficient computation, in your case, to avoid the double backpropogation problem. These approaches have been used in the literature before, for example (Moosavi-Dezfooli et al., 2019), which the paper also cites in section 3.3. \n\n(A3) \"Our method controls the local curvature of the network and is not restricted to twice-differentiable classifiers\"\n\nPlease note; the curvature of models is only defined for twice differentiable functions. The theory presented in your paper (e.g.: definition 2) also assumes twice differentiability.\n\n----\n\nThe new experiments with PGD slightly strengthen the paper, so I will increase my score, but due to the reasons presented in my original review, I still cannot advocate for its acceptance."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586217929,
                "cdate": 1700586217929,
                "tmdate": 1700586217929,
                "mdate": 1700586217929,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HnjssdnSna",
            "forum": "SZzQz8ikwg",
            "replyto": "SZzQz8ikwg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2787/Reviewer_vQnz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2787/Reviewer_vQnz"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new regularization techinque which address the catasterophic overfitting of AT approaches.\nThe new requalrization term is combined with FGSM AT method to improve the robustness of the model during training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The method addresses a significant challenge in the field of adversarial training with a fairly simple yet effective approach. \nI appreciate the theoretical analysis provided in the paper and the level of details shared by the authors."
                },
                "weaknesses": {
                    "value": "I understand the paper mainly focuses on the theoretical aspect of the proposed algorithm. \nHowever, there are some aspects missing which make it difficult to understand how practical the proposed method is.\n \n1- There are several SOTA AT methods currently being used which have demonstrated significant improvement in this area. It would be benficial to have other SOTA adversarial attacks evaluation in the paper as well to show how this approach helps against other SOTA  attacks as well."
                },
                "questions": {
                    "value": "The main question is that how the proposed method is compared with SOTA adversarial attack algorithms and how this method can be combined with other AT algorithm than FGSM and whether it would be effective in the sence or not."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2787/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2787/Reviewer_vQnz",
                        "ICLR.cc/2024/Conference/Submission2787/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2787/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698457041399,
            "cdate": 1698457041399,
            "tmdate": 1700694208624,
            "mdate": 1700694208624,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XN1WHEBCLM",
                "forum": "SZzQz8ikwg",
                "replyto": "HnjssdnSna",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2787/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2787/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer vQnz for their feedback. We have made the changes in the text visible in red. We address below their concerns. \n\n- **(Q1): Can you include the evaluation with other SOTA adversarial attacks?**\n\n**(A1):** Our models are evaluated with AutoAttack [1] (See Tab. 1 and 2 and Fig. 5), which contains the SOTA attacks FAB [2] and Square [3]. Moreover, AutoAttack is the standard evaluation metric in the [RobustBench](https://robustbench.github.io/) benchmark. We are not aware of more effective adversarial attacks than the AutoAttack ensemble. We will be happy to include any other SOTA adversarial attacks in the evaluation if the reviewer has any concrete methods in mind.\n\n- **(Q2): Can you integrate your regularization term with other SOTA adversarial training methods other than FGSM?**\n\n**(A2):** Yes, please let us explain how we have already integrated ELLE to other methods. In this paper, we present a thorough evaluation against SOTA single-step adversarial training methods. ELLE(-A) can be integrated as a plug-in regularization term into any other adversarial training method. This is demonstrated in the paper by combining ELLE-A with FGSM, GAT [4], and N-FGSM [5] (See Fig. 3 to 5). As we demonstrate, our method is effective to overcome CO when combined with all these methods.\n\nAdditionally, during the rebuttal we have included an analysis of the integration of ELLE-A with single-step variants of AdvMixUp [6] and TRADES [7]. These two methods are only able to overcome CO when in combination with ELLE-A, see [Tab. 3](https://imgur.com/a/lhXoSZN) and [Fig. 20](https://imgur.com/a/SkItddC).\n\nWe will be happy to answer further comments the reviewer might have.\n\n**References:**\n\n[1] Croce and Hein, Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks, ICML 2020\n\n[2] Croce and Hein. Minimally distorted adversarial examples with a fast adaptive boundary attack, ICML 2020\n\n[3] Andriushchenko et al., Square Attack: a query-efficient black-box adversarial attack via random search, ECCV 2020\n\n[4] Sriramanan et al., Guided Adversarial Attack for Evaluating and Enhancing Adversarial Defenses, NeurIPS 2020\n\n[5] de Jorge et al., Make Some Noise: Reliable and Efficient Single-Step Adversarial Training, NeurIPS 2022"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700040865650,
                "cdate": 1700040865650,
                "tmdate": 1700040865650,
                "mdate": 1700040865650,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZHYLmIkGBb",
                "forum": "SZzQz8ikwg",
                "replyto": "HnjssdnSna",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2787/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2787/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Have the concerns of the reviewer been addressed?"
                    },
                    "comment": {
                        "value": "Dear reviewer vQnz,\n\nWe are thankful for the time invested in reviewing our work. The reviewer was concerned about a) the attacks used for evaluating our models b) the possibility of combining our regularization term with other SOTA robust training methods. Our rebuttal further motivates the use of AutoAttack, clarifies the usage of our method in combination with GAT and N-FGSM and extended to combine with AdvMixUp and TRADES. \n\nIf the concerns of the reviewer are addressed, we would appreciate it if the reviewer increases their score. If the reviewer still has any remaining concerns, we are more than happy to clarify further. \n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700322200069,
                "cdate": 1700322200069,
                "tmdate": 1700322280907,
                "mdate": 1700322280907,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0tQuoKa11M",
                "forum": "SZzQz8ikwg",
                "replyto": "XN1WHEBCLM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2787/Reviewer_vQnz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2787/Reviewer_vQnz"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the additional clarification and the newly added results."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694146837,
                "cdate": 1700694146837,
                "tmdate": 1700694146837,
                "mdate": 1700694146837,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oadEz8cU6A",
                "forum": "SZzQz8ikwg",
                "replyto": "HnjssdnSna",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2787/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2787/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thankful to the Reviewer vQnz"
                    },
                    "comment": {
                        "value": "Dear Reviewer vQnz,\n\nwe are grateful for your feedback and appreciation of our work. We are thankful for increasing the score. \n\nBest,\n\nAuthors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694970294,
                "cdate": 1700694970294,
                "tmdate": 1700695027999,
                "mdate": 1700695027999,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BqLOzCpG6E",
            "forum": "SZzQz8ikwg",
            "replyto": "SZzQz8ikwg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2787/Reviewer_4XDh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2787/Reviewer_4XDh"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors investigate the issue of catastrophic overfitting and propose an efficient and effective solution in the form of a local linear regularizer. The introduced algorithm not only mitigates catastrophic overfitting but also circumvents the double backpropagation problem. It performs well in challenging scenarios, such as dealing with large adversarial perturbations and long training schedules, especially when pursuing Fast-AT. Experimental results demonstrate that the proposed method achieves good results while effectively addressing catastrophic overfitting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed regularization scheme as well as the analysis are reasonable, and the introduced local linear approximation scheme is very simple.\n2. The experiments show that the proposed ELLE can work well under large adversarial perturbations and long training schedules."
                },
                "weaknesses": {
                    "value": "1. The definition of Equation 1 is excessively simplistic, lacking any explanation or validation. It's important to note that the majority of deep models and training losses do not conform to linear mappings, which leads me to question this particular configuration. Also, the authors claim that ''CO appears when the loss suddenly becomes non-linear, ...'' (on page 4) I find this description somewhat confusing. Since the loss function usually remains constant throughout the training process, how does this transition from linear to non-linear occur in the loss?\n\n1. The proposed local linear approximation error is similar to the mixup, which has been studied in adversrial training [a]. But, the comparison and analysis are missing.\n\n1. According to the results shown in Figure 4, I found that ELLE-A is not better than ELLE, but I cannot find any clear explanation.\n\n1. The proposed method does not work well under short training schedule and small adversarial perturbation. The authors have not provided a satisfactory explanation or analysis for this issue. Since the defensive performance obtained from long training schedule and short training schedule is comparable, why should we necessarily opt for the longer training schedule? Moreover, large perturbations imply that the noise becomes more pronounced, which contradicts real-world scenarios. \n\n1. In the review process, I attempted to use mathematical tools to show that the regularization introduced in this paper can be used for the approximate estimation of LLR. That is, \n\n   $L(f_\\theta(x+\\delta),y)-L(f_\\theta(x),y)-\\delta^T\\nabla_xL(f_\\theta(x),y)\\approx L(f_\\theta(x_c),y)-(1-\\alpha)L(f_\\theta(x_a),y)-\\alpha L(f_\\theta(x_b),y)$.\n\n   So, I think ELLE is an effective approximation for LLR. And it's better to compare LLR and ELLE in the all experiments. BTW, the authors only report some results in Figure 1, when considering methods enforcing local linearity. But, I cannot find any comparison in the experiments. I think this lack of comparison to be unfair. \n\n[a] Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization. CVPR 2020."
                },
                "questions": {
                    "value": "Please check the problems mentioned in the Weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2787/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2787/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2787/Reviewer_4XDh"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2787/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698662138806,
            "cdate": 1698662138806,
            "tmdate": 1700737721686,
            "mdate": 1700737721686,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cXENQ9j2DS",
                "forum": "SZzQz8ikwg",
                "replyto": "BqLOzCpG6E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2787/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2787/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer 4XDh for their feedback. We have made the changes in the text visible in red. We address below their concerns. \n\n- **(Q1): Equation (1) is overly excessively simplistic, additional explanations are needed. Deep models and training losses do not conform to linear mappings.**\n\n**(A1):** We thank the reviewer for highlighting this issue. We agree with the reviewer in that deep models and training losses are usually non-linear. However, even though they are globally non-linear, it has been observed that the loss of robust models behaves linearly in small regions around data points, see [1] and related citations in the third paragraph of the introduction. As an example, the output of Deep NNs with the ReLU activation function is highly non-linear, but piecewise linear, and thus they are locally linear in small regions.\n\nWe have clarified this in the preamble of equation (1) and accordingly updated equation (1) to better capture this intuition.\n\n- **(Q2): The loss function usually remains constant throughout the training process, how does this transition from linear to non-linear occur in the loss?**\n\n**(A2):**  \u201cThe loss function usually remains constant\u201d: We believe the reviewer refers to the minimization objective, e.g., the cross entropy loss. This is indeed kept the same throughout the training process in AT [2]. Nevertheless, the behavior of the loss landscape with respect to the input data evolves in time depending on the parameters of the model [1]. Let $h(\\theta_t, x, y) = L(f_{\\theta_t}(x), y)$, we are interested on the local linearity of $h$ w.r.t. $x$. The loss transitions from locally linear to non locally linear in the sense that for some $t = t_1$, we have that $h$ behaves linearly with respect to $x$, but it does not for some other $t = t_2 > t_1$. This behavior is clearly displayed in Fig. 2 (b-c), where the local linear approximation error suddenly increases at epoch 15 when training with FGSM.\n\n- **(Q3): The local, linear approximation error resembles the AdvMixUp formulation, why do you not compare against it?**\n\n**(A3):** We are thankful to the reviewer for the suggestion. We conduct the related experiment in Appendix B.13. The convex combination used in the design of the input points and labels given by AdvMixUp resembles the convex combination used for obtaining the point $x_c$ in our method. However, there are fundamental differences between AdvMixup and ELLE. Let $\\delta_{\\text{adv}}$ be the adversarial perturbation given by some adversarial attack:\n\n- AdvMixUp assigns the one-hot vector corresponding to the true class to  $x$ and uniform class probabilities to the point $x + 2\\delta_{\\text{adv}}$. Then, every point in the convex combination of $x$ and $x + 2\\delta_{\\text{adv}}$ is assigned a convex combination of the aforementioned labels.\n- ELLE enforces the training loss to be locally linear in the whole region $x + \\delta: ||\\delta|| \\leq \\epsilon$ via a loss involving a convex combination of terms.\n- In the single-step scenario, i.e. $\\delta_{\\text{adv}} = \\epsilon \\cdot \\text{sign}(\\nabla_{x}L(f_{\\theta}(x), y))$, AdvMixUp suffers from CO while our method does not.\n\nWe include an experimental evaluation of AdvMixUp and its combination with ELLE-A in Appendix B.13. Showing that combined with ELLE-A, CO is avoided, see [Tab. 3](https://imgur.com/a/lhXoSZN) and [Fig. 20](https://imgur.com/a/SkItddC).\n\n- **(Q4): ELLE-A is worse than ELLE in the long schedule but it is not present in the text.**\n\n**(A4):** We thank the reviewer for highlighting this missing explanation in the text. This is expected because the ELLE-A regularization is weaker. So overall, the model can still overfit in long schedules. ELLE is a stronger regularization and test performance decreases much less with time. We have included a related sentence in section 4.4.\n\n- **(Q5): The proposed regularization method does not perform well in small epsilon values and short schedules.**\n\n**(A5):** Guarding against attacks with a small perturbation budget doesn\u2019t guarantee the resistance against attacks with a larger budget. Additionally, large perturbation radiuses can still produce imperceptible perturbations as we show in [Fig. 6](https://imgur.com/a/QamLafl) for ImageNet. Small radiuses are also not so relevant when studying CO because it does not appear. In fact, even standard FGSM with $\\epsilon=2/255$ can be used for ImageNet training without the appearance of CO, see Tab. 1. \n\nThe biggest improvements with our method occur when $\\epsilon$ is large. However, when combining ELLE-A with N-FGSM, AA accuracy is even improved for $\\epsilon \\leq 8/255$ in SVHN and is comparable for CIFAR10/100, see Table 2 in the appendix."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700040629312,
                "cdate": 1700040629312,
                "tmdate": 1700040629312,
                "mdate": 1700040629312,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DD6ihkhSZY",
                "forum": "SZzQz8ikwg",
                "replyto": "BqLOzCpG6E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2787/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2787/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (continuation)"
                    },
                    "comment": {
                        "value": "- **(Q6): Why use long schedules and large $\\epsilon$?**\n\n**(A6):** Long schedules have been used for other existing robust training methods [4,5]. Other works argue the avoidance of CO in short schedules gives a false sense of robustness [6,7]. Additionally, the evaluation in long schedules is a standard practice [1,3]. The avoidance of CO in longer schedules is an additional benefit of our method. When training in less studied datasets where the appropriate length of the schedule is unknown, our method is safer against CO.\n\nRegarding larger $\\epsilon$, we should aim at being robust to the largest possible perturbations. Our attack visualization (See [Fig. 6](https://imgur.com/a/QamLafl)) does not reveal a change in the human prediction with the $\\epsilon$ values employed.\n\n- **(Q7): ELLE appears to be an approximation of LLR, LLR should be included in the main comparisons.**\n\n**(A7):** We thank the reviewer for the suggestion. Both LLR and ELLE approximate the second order derivatives up to constant factors. This is proven in Prop. 3 (End of the appendix) and Prop. 1 respectively. Considering this fact, the lower computational cost of ELLE favors its usage. The comparison against LLR is usually not included in the literature [1,3]. Moreover, our results in Fig. 1 show that LLR attains a significantly lower adversarial accuracy than ELLE.\n\nWe are currently running additional experiments on CIFAR100 and SVHN. We will post those results and revise the respective tables during the rebuttal period. We include an ablation on the $\\lambda$ parameter for LLR in Appendix B.12, [Fig. 19](https://imgur.com/a/SXLapTO), and the corresponding CIFAR10 results in [Fig. 3 (b)](https://imgur.com/a/Gz20Tfn). LLR does not match the performance of ELLE(-A), especially for large $\\epsilon$.\n\nWe will be happy to answer any other comments or questions the reviewer might have.\n\n**References:**\n[1] Andriushchenko and Flammarion, Understanding and improving fast adversarial training, NeurIPS 2020\n\n[2] Madry et al., Towards Deep Learning Models Resistant to Adversarial Attacks, ICLR 2018\n\n[3] de Jorge et al., Make Some Noise: Reliable and Efficient Single-Step Adversarial Training, NeurIPS 2022\n\n[4] Rice et al., Overfitting in adversarially robust deep learning, ICML 2020.\n\n[5] Xu et al., Exploring and exploiting decision boundary dynamics for adversarial robustness, ICLR 2023\n\n[6] Kim et al., Understanding catastrophic overfitting in single-step adversarial training, AAAI 2021\n\n[7] Li et al., Towards Understanding Fast Adversarial Training, arXiv 2020"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700040727241,
                "cdate": 1700040727241,
                "tmdate": 1700040749147,
                "mdate": 1700040749147,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EzPUAl7vkB",
                "forum": "SZzQz8ikwg",
                "replyto": "BqLOzCpG6E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2787/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2787/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Have the concerns of the reviewer been addressed?"
                    },
                    "comment": {
                        "value": "Dear reviewer 4XDh,\n\nWe are thankful for your effort to review our work. The main concerns expressed are the a) non-linear mappings learning, b) comparisons with AdvMixUp, and c) comparisons with LLR and d) the need for long schedules and large $\\epsilon$. Our rebuttal addresses those core concerns along with the rest of the questions of the reviewer.\n\nIn addition, we present have concluded the LLR experiments promised in our first response and include them in ([Fig. 19](https://imgur.com/a/Z1hWOTB)) and its evaluation ([Fig. 3](https://imgur.com/a/uFzb5d0)). We find LLR is more sensitive to hyperparameters and is not able to match the performance of ELLE(-A).\n\nIf the concerns of the reviewer are addressed, we would appreciate it if you re-evaluate our submission. If the reviewer still has any remaining concerns, we are more than happy to clarify further. \n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700322126487,
                "cdate": 1700322126487,
                "tmdate": 1700322299764,
                "mdate": 1700322299764,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TFWLmzmuCo",
                "forum": "SZzQz8ikwg",
                "replyto": "EzPUAl7vkB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2787/Reviewer_4XDh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2787/Reviewer_4XDh"
                ],
                "content": {
                    "title": {
                        "value": "Comments on the authors' response"
                    },
                    "comment": {
                        "value": "Thank the authors for addressing most of my concerns, and they have corrected them in the newly submitted version. Overall, I would like to increase my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737836823,
                "cdate": 1700737836823,
                "tmdate": 1700737836823,
                "mdate": 1700737836823,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]