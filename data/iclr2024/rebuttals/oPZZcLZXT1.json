[
    {
        "title": "Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning"
    },
    {
        "review": {
            "id": "wH9brKSQSH",
            "forum": "oPZZcLZXT1",
            "replyto": "oPZZcLZXT1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission997/Reviewer_6wvZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission997/Reviewer_6wvZ"
            ],
            "content": {
                "summary": {
                    "value": "The authors tackle the problem of single-demo imitation learning by introducing a reward function that rewards an agent for being at states within a single-step of an expert demonstration. They perform experiments on Mujoco navigation environments and 1 Adroit manipulation environment."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "******************Writing:****************** Overall, the writing was solid and generally clearly explained the method and the equations used to derive the reward function.\n\n****************************Motivation:**************************** The paper is addressing an important problem, that of limited-demonstration (or in their case, single) imitation learning.\n\n******************Novelty:****************** To the best of my knowledge, the method is novel in this problem setting.\n\n************************Experiments:************************ Results are moderately comprehensive and generally demonstrate that the authors\u2019 method, TDIL, outperforms other methods by a decent amount."
                },
                "weaknesses": {
                    "value": "**************Method:************** Some discussion is probably missing about how the method, combined with Q-learning, is likely to propagate values from the expert-proximal states to earlier states of the same trajectory.\n\n************************Experiments:************************\n\n- A missing ablation is one that ablates out $B^-_{\\text{reversed}}$ as this seemed like an arbitrary choice when presented in Eq.9\u2026 i\u2019d be curious to see what the performance without it looks like\n- Since $\\beta$ is set to 0 in the main experiments, there should be some ablation study on $\\beta$ since it is proposed in the overall rew function of the method.\n- The adroit experiments would also benefit from a comparison against at least another method other than BC for completeness\n\n****************Clarity:****************\n\n- Figure 1(a) is referenced in the intro yet doesn\u2019t appear until page 4. To make the intro easier to read, it should be put earlier in the paper (it\u2019s on page 4) or separated into multiple figures so that the relevant parts of Figure 1 appear when referenced in the intro.\n- There\u2019s some color-coding going on in Figure 1(b)-(d), (f)-(h) but they\u2019re not explained in the figure or caption (presumably gray intensity in (b)-(d) is for the reward but I have no idea what the colors for (f)-(h) mean)\n- Eq 6: In the 3rd line, I had no idea where $T$ came from (was it an MDP time horizon?) until I went back and saw in the preliminaries that the demonstration is assumed to be of length $T$. Would be helpful to specify this again here\n- Generally, the use of lowercase $p(a|s$ to describe a policy is confusing because of the use of $p_0(s_0)$ to describe the initial transition dist. and uppercase $P$ for transition probabilities. I think it would be better to use $\\pi(a|s)$ and $\\pi^*(a|s)$ to describe policies and optimal policies, respectively.\n- After Eq.6, it would be good to briefly clarify the use of actor-critic RL and TD-learning so that the expectation can safely be ignored.\n\n**************************Minor Issues:**************************\n\n- \u201cthat follows\u201d \u2192 \u201cthat follow\u201d between Eq3 and Eq4\n- \u201cTo ensure optimality, the agent is trained with the total reward\u2026\u201d \u2192 \u201censure\u201d is probably not the right term here as only using $R_{IRL}$ is the only way to ensure optimality\n- Fig2 should perhaps have an arrow pointing back from Step 4 to Step 1"
                },
                "questions": {
                    "value": "Why are the arrows in figure 1 curved? Are the direction arrows calculated based on the average direction represented by the logits for the discrete actions from the policy? \n\nDoes the assumption of optimality in the construction of $R_{TDIL}$ matter in practice? It makes the equations simpler, but in practice the reward function is more accurate if calculated under the expectation of the current policy.\n\nIs Eq10 used for actual reward calculation for the policy or ********************only model selection********************? Is it even used at all for true model selection in the main experiments or do the authors simply pick the latest checkpoint for all methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission997/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698184669249,
            "cdate": 1698184669249,
            "tmdate": 1699636025985,
            "mdate": 1699636025985,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LJInFuYZQV",
                "forum": "oPZZcLZXT1",
                "replyto": "wH9brKSQSH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response for Reviewer 6wvZ"
                    },
                    "comment": {
                        "value": "Dear reviewer 6wvZ, thank you for the positive feedback, detailed comments and constructive suggestions, we have addressed the concerns and suggestions as follows.\n\n\n---\n\n**Comments**\n---\n\n> **C1. :** Some discussion is probably missing about how the method, combined with Q-learning, is likely to propagate values from the expert-proximal states to earlier states of the same trajectory.\n\n**Response:** We appreciate the reviewer for bringing this to our attention. We have responded to this by incorporating relevant information at the end of Section 3.2 in our updated manuscript, in which we elaborated on how the proposed method, in conjunction with an agent (specifically the soft actor-critic (SAC) used in this paper), propagates values from states proximal to the expert to earlier states within the same trajectory.\n\n> **C2. :** A missing ablation study is one that ablates out $B^-_{Reversed}$ as this seemed like an arbitrary choice when presented in Eq.9\u2026 i\u2019d be curious to see what the performance without it looks like.\n\n**Response:** We would like to bring to the reviewer\u2019s kind attention that the ablation study presented in Table 2 of the original main manuscript, specifically in the column labeled \u201cw/o Hard Negative Samples\u201d. As discussed in Section 3.3, hard negative samples for the contrastive learning of the transition discriminator are denoted as $B^-{Reversed}$. These hard negative samples can consist of different combinations of data from reversed transitions, depending on prior knowledge about different environments. To maintain generality, we refer to these as hard negative samples throughout this paper, using the term to represent $B^-_{Reversed}$.\n\nThe experiments that ablate hard negative samples demonstrate that in simpler environments, the information contained in these samples may not be necessary. In contrast, their absence in the Humanoid-v3 environment impairs performance. This difference can be attributed to the vast state space of Humanoid-v3, which reduces the likelihood that easy negative samples encapsulate the essential information contained in hard negative samples. Furthermore, given the complexity of Humanoid-v3, the agent may be sensitive to incorrectly estimated rewards that stem from the absence of hard negative samples.\n\n> **C3. :** Since $\\beta$ is set to 0 in the main experiments, there should be some ablation study on it since it is proposed in the overall reward function of the method.\n\n**Response:** We appreciate the reviewer's suggestion and have augmented the ablation study on the hyperparameter $\\beta$ in Table A3 of the supplementary material. The study reveals that by setting $\\beta$ within the range of [0.1, 0.9] in $R_{agg} = \\beta R_{IRL} + (1-\\beta) R_{TDIL}$, the agent consistently achieves expert-level performance, **without the use of Behavioral Cloning (BC) loss**. This result validates efficacy of the proposed reward function $R_{agg}$.\n\n> **C4. :** The adroit experiments would also benefit from a comparison against at least another method other than BC for completeness.\n\n**Response:** We appreciate the reviewer's suggestion and have incorporated an experiment shown in Fig. A3 of the supplementary material. This includes comparisons with BC, CFIL, and f-IRL. The results demonstrate that TDIL achieves expert-level performance (i.e., a 100% success rate) within one million steps and surpasses the success rate of BC(10%), f-IRL(40%), and CFIL(10%).\n\n> **C5. :** Figure 1(a) is referenced in the intro yet doesn\u2019t appear until page 4. To make the intro easier to read, it should be put earlier in the paper (it\u2019s on page 4) or separated into multiple figures so that the relevant parts of Figure 1 appear when referenced in the intro.\n\n**Response:** We appreciate the kind suggestion from the reviewer. We have relocated Fig. 1 to page 2 to ensure it appears earlier in the paper for improving the alignment with references in the introduction.\n\n> **C6. :** There\u2019s some color-coding going on in Figure 1(b)-(d), (f)-(h) but they\u2019re not explained in the figure or caption (presumably gray intensity in (b)-(d) is for the reward but I have no idea what the colors for (f)-(h) mean)\n\n**Response:** We appreciate the reviewer's suggestion and have incorporated enhancements in the updated manuscript.\n\nThe color-coding in Figs. 1 (b)-(d) serves to visually represent the density of each reward signal. Meanwhile, in Figs. 1 (f)-(h), the color-coding is intended to visualize the actions taken by the agent. This approach is analogous to techniques employed in optical flow research, where color is used to provide an additional representation of direction beyond the traditional arrow visualization.\n\nWe also provide a reference in Section A.2.4 of the supplementary material to illustrate the relationship between color and direction. This is intended to offer a more comprehensive understanding of the color-coded information."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638630643,
                "cdate": 1700638630643,
                "tmdate": 1700733337271,
                "mdate": 1700733337271,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s7RldTEaYT",
            "forum": "oPZZcLZXT1",
            "replyto": "oPZZcLZXT1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission997/Reviewer_eL6k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission997/Reviewer_eL6k"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an expert proximity reward to improve the performance of existing Inverse RL and Adversarial IL algorithms. The intuition is based on a heuristic idea that the policy should go towards the expert transition in each step. The proposed algorithm is compared to various recent IRL/AIL algorithms on the Mujoco benchmark and can achieve better performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed idea is simple.\n2. The derivation of each step is in general clear."
                },
                "weaknesses": {
                    "value": "1. The novelty of the idea. In may existing works, people have discussed method that can try to encourage the agent to go back to the expert distribution such as FIST [1]. Among IRL methods, PWIL and OT[2] also incorporates a dense distance-based reward that encourage the agent to stay close to the expert per-transition. I am wondering why the proposed method is better than these prior approaches.\n\n2. Soundness of the method. If the agent has gone off the expert distribution, the proposed reward function can still degenerate. For example, if the minimal distance from a state $s_t$ to any expert state is 5 steps, the proposed reward $r(s_t, a_t)$ will output 0 no matter what action $a_t$ the policy chooses. Then how could the proposed method guide the policy?\n\n3. Unfair comparison. The proposed method integrates BC in its training. However, it looks like all the baselines the authors compare to does not involve BC in training. This makes the comparison unfair.\n\n4. Sensitivity analysis. The proposed algorithm involve a hyper parameter $\\alpha$, but the authors do not discuss the sensitivity of the performance with respect to different alphas.\n\n[1] Hakhamaneshi et al. HIERARCHICAL FEW-SHOT IMITATION WITH SKILL TRANSITION MODELS. NeurIPS 2021.\n[2] Haldar et al. Supercharging Imitation with Regularized Optimal Transport. CoRL 2022."
                },
                "questions": {
                    "value": "See weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission997/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698534528414,
            "cdate": 1698534528414,
            "tmdate": 1699636025907,
            "mdate": 1699636025907,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lQQkHaS5oQ",
                "forum": "oPZZcLZXT1",
                "replyto": "s7RldTEaYT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response for Reviewer eL6k"
                    },
                    "comment": {
                        "value": "Dear reviewer eL6K, thank you for the constructive comments and feedback, we have addressed your concerns as follows. \n\n\n---\n\n**Comments**\n---\n\n>**C1. :** In many existing works, people have discussed methods that can try to encourage the agent to go back to the expert distribution such as FIST [1]. Among IRL methods, PWIL and OT [2] also incorporate a dense distance-based reward that encourages the agent to stay close to the expert per-transition. I am wondering why the proposed method is better than these prior approaches.\n\n**Response:** We would like to thank the reviewer for raising these questions.\n\nAlthough prior approaches such as FIST [r1], PWIL [r2], and OT [r3] adopt strategies to motivate agents to return to the expert distribution, the distance measuring functions they employ, including Euclidean and Cosine distances, are not theoretically guaranteed to achieve this objective. Furthermore, if the reward function does not consider the environment dynamics, it may mistakenly direct the agent to states that are not genuinely \u201cclose\u201d, and may lead to the agent becoming stuck in those states. Therefore, the awareness of environmental dynamics is crucial for the reward function. Consider the toy example in Fig. 1 (a) (discussed in Section 3). When using the L2 distance (i.e., Euclidean distance) as the metric for measuring closeness, the reward system would inadvertently encourage the agent towards all adjacent states of the expert states, even if a barrier (indicated by a red line) separates the agent\u2019s current state from the expert state. As a result, as shown in Fig. 1 (f), even after training, the cells on the inner side of the L-shaped barrier still exhibit an incorrect policy (pointing to the bottom left).\n\nTo address these issues, a key insight of our paper is the use of a transition discriminator to obtain an environment-dynamic-aware reachability metric. This metric does not rely on the L2 distance. Instead, our transition discriminator considers transition reachability and thus is capable of providing more meaningful guidance for the agent to transition to the expert support and achieve expert-level performance in various environments.\n\nIn addition, our breakthrough lies not just in introducing a more theoretically sound distance measuring function but also in densifying the single demonstration from a one-dimensional manifold to an *n*-dimensional support. This enhancement facilitates the alignment of the demonstration with the agent support. The significance of this lies in the fact that a demonstration restricted to a 1-manifold does not overlap with the agent support in a low-dimensional manifold, as discussed in [r4]. We have elaborated on this in the updated version of Section 3.1 in our manuscript.\n\n[r1] Hakhamaneshi et al. Hierarchical Few-Shot Imitation with Skill Transition Models. NeurIPS 2021. \n\n[r2] Dadashi, Robert, et al. Primal Wasserstein imitation learning. arXiv:2006.04678 (2020).\n\n[r3] Haldar et al. Supercharging Imitation with Regularized Optimal Transport. CoRL 2022.\n\n[r4] Arjovsky, Martin, and L\u00e9on Bottou. Towards principled methods for training generative adversarial networks. arXiv:1701.04862 (2017).\n\n>**C2. :** If the agent has gone off the expert distribution, the proposed reward function can still degenerate. For example, if the minimal distance from a state to any expert state is 5 steps, the proposed reward will output 0 no matter what action the policy chooses. Then, how could the proposed method guide the policy?\n\n**Response:** We would like to thank the reviewer for raising this question.\n\nWe wish to draw the reviewer\u2019s attention to the fact that for states multiple steps away from expert proximity, RL methods like the soft actor-critic (SAC) used in this work can propagate the rewards to other states. This propagation occurs through value functions and the agent's interaction with the environment via exploration and exploitation.\n\nA key insight of our method is the densification of the single demonstration\u2019s 1-manifold into a theoretically sound *n*-dimensional support. This approach enables the RL agent to reach those states during exploration.\n\nOn the other hand, while methods like PWIL provide rewards for more states, they lack theoretical soundness. This is due to their use of Euclidean (L2) or Cosine distance measurements, which do not account for environmental dynamics, as discussed in Section 3.1. In addition, while our method has already achieved optimal performance in our experiments with a 1-step setup, it retains the flexibility to extend to $n$ steps if necessary. This extension can be achieved by augmenting $(s_t, s_{t+m}), 1 \\leq m \\leq n$, into the positive example set $B^+$. The theoretical details are provided in Section A5 of our supplementary material."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638480073,
                "cdate": 1700638480073,
                "tmdate": 1700728429431,
                "mdate": 1700728429431,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uXC3YVYgsX",
                "forum": "oPZZcLZXT1",
                "replyto": "s7RldTEaYT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission997/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**C3. :** The proposed method integrates BC in its training. However, it looks like all the baselines the authors compare do not involve BC in training. This makes the comparison unfair.\n\n**Response:** We would like to clarify the question as follows.\n\nFirstly, consider the scenario where BC loss is utilized. Table r1 presents the performance of CFIL, PWIL, f-IRL, and TDIL with BC loss. It is observed that although some baselines have improved their performance with BC loss (while others worsened), TDIL consistently outperforms all these baselines.\n\nSecond, we wish to highlight that the proposed reward function is actually $R_{agg} = \\beta R_{IRL} + (1 - \\beta)R_{TDIL}$ (refer to Eq. (5) in the manuscript). Using $R_{agg}$, our **proposed method can train an SAC agent without employing BC**. The results, as reported in Table r2 (which corresponds to Table A4 in the supplementary material), reveal that even without BC, our agent can easily reach expert performance for $\\beta \\in [0.1, 0.9]$.\n\nConsidering these two perspectives, it becomes evident that the proposed surrogate reward $R_{TDIL}$ can effectively assist the agent in consistently reaching expert performance. This is achievable as long as the agent is trained to perform the expert action on the demonstration since $R_{TDIL}$ only motivates the agent to move toward the expert\u2019s proximity support and does not dictate specific actions to be performed on the expert states.\n\n- **Table r1** Performance of baselines with BC loss\n\n|   | HalfCheetah-v3 | Walker2d-v3 | Humanoid-v3 | Ant-v3 |\n| - | - | - | - | - |\n|**Expert** | 15251 | 6123 | 5855 | 6561 |\n| **TDIL with $R_{agg}$ <br> (w/o BC)** | 15612 | 6349 | 6352 | 6632|\n| **TDIL with BC** | **15666** | **6312** | **5758** | **6434**|\n| **CFIL with BC** | 14853 | 6286 | 5343 | 4683 | \n| **PWIL with BC** | 4679 | 5489 | 5294 | 5925| \n| **f-IRL with BC** | 13638 | 4403 | -      | 5337 | \n\n\n\n- **Table r2** Performance of TDIL under different $\\beta$ value selection.\n\n|            \t| 0+BC  | 0\t| 0.1   | 0.2   | 0.5   | 0.8   | 0.9   | 0.95  | 0.99  | 1.0  | Expert |\n| -------------- | ----- | ---- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ---- | ------ |\n| HalfCheetah-v3 | **15666** | 12630 | 15100 | 15541 | 15479 | 15612 | 15289 | 15462 | 15529 | 9791 | 15251 |\n| Hopper-v3  \t| 4115  | 3890 | 4124  | 4126  | **4162** | 4128  | 4083  | 1887  | 3232  | 1950 | 4114   |\n| Ant-v3     \t| 6434  | 3995 | 6358  | 6513  | 6467  | 6611  | **6632** | 6560  | 6506  | 4216 | 6561   |\n| Humanoid-v3\t| 5758  | 5575 | 6288  | **6352** | 6312  | 6325  | 5680  | 5703  | 5235  | 1826 | 5855   |\n| Walker2d-v3\t| 6312  | 6281 | 6251  | 6204  | 6266  | 6346  | **6349** | 6296  | 6098  | 1769 | 6123   |\n\n\n>**C4. :** Sensitivity analysis. The proposed algorithm involves a hyperparameter but the authors do not discuss the sensitivity of the performance with respect to different alphas. \n\n**Response:** We would like to thank the reviewer for the suggestion concerning the sensitivity of performance to the hyperparameter $\\alpha$. To address this, we have included two additional results in the revised manuscript.\n\n- Table A4 in the supplementary material presents the accuracy of transition discriminators trained with different values of $\\alpha$. The results demonstrate that all setups of the transition discriminator can achieve an accuracy of over 0.988 for both positive and negative examples.\n- Furthermore, Fig. A6 in the supplementary material illustrates the training curve of the agent with different choices of $\\alpha$. These results indicate that the majority of the $\\alpha$ settings can attain expert performance.\n\nDrawing on the findings from these experiments, we can infer that the hyperparameter $\\alpha$ is relatively straightforward to calibrate, indicating its robustness and ease of adjustment in different scenarios."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638549376,
                "cdate": 1700638549376,
                "tmdate": 1700725734135,
                "mdate": 1700725734135,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "F0flUhIqY4",
            "forum": "oPZZcLZXT1",
            "replyto": "oPZZcLZXT1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission997/Reviewer_9w53"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission997/Reviewer_9w53"
            ],
            "content": {
                "summary": {
                    "value": "This work focuses on the single-demonstration imitation learning setting. This work highlighted a challenge of single-demonstration imitation, i.e., scarce expert data may result in sparse reward signals. To solve this issue, this work proposes to augment reward signals using a transition discriminator. The resulting algorithm is called transition discriminator-based imitation learning (TDIL)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThis work aims to use a transition discriminator to approximate surrogate rewards to enhance the convergence speed in the single-demonstration IL setting. The transition discriminator is introduced to capture the awareness of the environmental dynamics. The effectiveness of TDIL is evaluated in five MuJoCo tasks."
                },
                "weaknesses": {
                    "value": "1.\tThe presentation of the paper is not easy to follow, and sometimes confusing. For example, what is the difference between the single demonstration and sparse/scarce expert data, which one is the source of the sparse reward issue in single-demonstration imitation learning? \n\n2.\tThis work highlighted that the scarce expert data in single-demonstration imitation learning may result in sparse reward signals. However, the motivation example is not convincing. Section 3.1 claimed that the issue of sparse rewards is due to the difference in convergence rates originates from IRL\u2019s particular feature of allocating rewards solely to states that mirror those of the experts. There is no evidence to show the relationship between the convergence speed and sparseness of the rewards. Moreover, there are no statistical results to directly show the sparseness of the IRL\u2019s rewards. \n\n3.\tTDIL aims to use the expert reachability indicator to define a denser surrogate reward function. However, the indicator can only output binary values for state-action pairs, which will result in sparse rewards. It is confusing how such an indicator can be used to define a denser surrogate reward function. A potential explanation in section 3.2 is that the surrogate reward function encourages the agent to navigate to states that are in expert proximity. What is expert proximity?\n\n4.\tThe transition discriminator is trained in a way that is like contrastive learning. It can capture the transition dynamics because the input data is state transition pairs.  How the awareness of the environmental dynamics can help it output denser rewards is unclear."
                },
                "questions": {
                    "value": "1.\tSee weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission997/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698746328198,
            "cdate": 1698746328198,
            "tmdate": 1699636025806,
            "mdate": 1699636025806,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kOwHupF50s",
                "forum": "oPZZcLZXT1",
                "replyto": "F0flUhIqY4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response for Reviewer 9w53"
                    },
                    "comment": {
                        "value": "Dear 9w53, thank you for the constructive comments and feedback, we have addressed your concerns as follows.\n\n---\n\n**Comments**\n---\n\n>**C1. :** What is the difference between the single demonstration and sparse/scarce expert data, which one is the source of the sparse reward issue in single-demonstration imitation learning?\n\n**Response:** As discussed in Section 1 and mentioned in the other sections of the main manuscript, single demonstration imitation learning (abbreviated as single demo IL) differs from conventional IL. In conventional IL, an agent typically has access to a number of expert demonstrations, which could range from hundreds to thousands, as in the case of D4RL [r1]. In contrast, the most challenging aspect of single demo IL is the limitation to only **one** trajectory accessible to the learning agent as a demonstration. This challenge arises because obtaining expert demonstrations is often expensive and can incur prohibitive efforts. In such a single demo setting, the demonstration is sparse compared to conventional IL settings. Utilizing conventional inverse reinforcement learning (IRL) approaches in the single demo setting would result in a sparse reward scenario.\n\n>**C2. :** There is no evidence to show the relationship between the convergence speed and sparseness of the rewards. Moreover, there are no statistical results to directly show the sparseness of the IRL\u2019s rewards.\n\n**Response:** We would like to address the reviewer's question as follows. Consider a continuous environment with *n* dimensions. From a mathematical perspective, the reward sparseness of conventional IRL compared to TDIL can be described as follows:\n- For conventional IRL, one demonstration trajectory corresponds to only a 1-manifold (i.e., a 1-dimensional line). \n- In contrast, the TDIL method includes all states that are one-step reachable from the single demonstration trajectory. As a result, the reward function formulated by TDIL corresponds to an *n*-dimensional support and is infinitely denser than any 1-manifold [r2].\nA crucial aspect is to ensure that the demonstration is a support, as it is impossible to match an agent's support with a 1-manifold. We have enhanced Section 3.1 in the updated manuscript to improve accessibility and readability.\n\n>**C3. :** The indicator can only output binary values for state-action pairs, which will result in sparse rewards. It needs to be clarified how such an indicator can be used to define a denser surrogate reward function. A potential explanation in section 3.2 is that the surrogate reward function encourages the agent to navigate to states that are in expert proximity. What is expert proximity?\n\n**Response:** We would like to address the reviewer's question as follows. \n\nAs discussed in Section 1 and defined in Section 3, **expert proximity** is a set of states where each state in this set can reach an expert\u2019s demonstration state in one step. As discussed in Section 3, TDIL provides reward to the state-action pair $(s_t, a_t)$ if $s_{t+1}$ is in expert proximity. \n\nPlease note that outputting binary values for state-action pairs does not affect the reward density. The transition discriminator only tells the one-step reachability. The sparseness or denseness of the reward system only depends on the size of the manifold that can offer non-zero rewards.  We increase the density by densifying the non-zero reward from a one-dimensional manifold to an *n*-dimensional manifold. For example, in a high-dimensional continuous environment such as Mujoco, there will only be at most 1,000 state-action pairs provided by expert demonstrations. However, the states in expert proximity are infinite. As a result, the number of state-action pairs that can reach a state in expert proximity is also infinite. We have enhanced Section 3.2 in the updated manuscript for improved accessibility.\n\n[r1] Fu, Justin, et al. \"D4rl: Datasets for deep data-driven reinforcement learning.\" arXiv preprint arXiv:2004.07219 (2020).\n\n[r2] Arjovsky, Martin, and L\u00e9on Bottou. \"Towards principled methods for training generative adversarial networks.\" arXiv preprint arXiv:1701.04862 (2017)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638256436,
                "cdate": 1700638256436,
                "tmdate": 1700732836862,
                "mdate": 1700732836862,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mG6eq0czVv",
            "forum": "oPZZcLZXT1",
            "replyto": "oPZZcLZXT1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission997/Reviewer_Ka6u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission997/Reviewer_Ka6u"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method for single demonstration imitation learning which learns a discriminator that generates a pseudo reward by rewarding an agent for producing a state in the environment from which entering any state in an expert demonstration is possible. The pseudo reward is used to encourage the learned policy to be able to enter one of the states in the demonstration. The policy is also trained on a BC loss encouraging it to remain within the state distribution of the expert demonstration. The authors test their method on 5 mujoco environments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I had the pleasure of reviewing this paper as a submission to an earlier venue and I must congratulate the authors on this improved version. \n1. The method is principled and intuitive\n2. The paper is well-written and organized.\n3. The problem being addressed is crucial and the analysis includes model selection which is especially useful for deployment in real-world scenarios."
                },
                "weaknesses": {
                    "value": "I still think a couple of points need to be improved upon before this is ready to be published:\n1. **Additional experiments on a larger domain set**: Locomotion environments tend to be somewhat easier for RL agents (evidenced by a variety of self-supervised methods learning to walk). I would recommend increasing the scope of the experiments to a robotic manipulation domain which would allow readers to understand the limitations of the work.\n2. **Ablations utilizing high dimensional states**: How does the method perform when utilizing demonstrations from the visual domain? This is necessary for completeness as there are several recent works for instance FISH https://arxiv.org/abs/2303.01497, RoboCLIP https://arxiv.org/pdf/2310.07899.pdf which learn a reward from visual observations from a single demonstration."
                },
                "questions": {
                    "value": "I would be happy to increase my score if the authors can resolve the above questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission997/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698977166246,
            "cdate": 1698977166246,
            "tmdate": 1699636025728,
            "mdate": 1699636025728,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QyTgx3H3aK",
                "forum": "oPZZcLZXT1",
                "replyto": "mG6eq0czVv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response for Reviewer Ka6u"
                    },
                    "comment": {
                        "value": "Dear Reviewer Ka6u, thank you for your positive feedback and encouraging review. We have addressed your concerns as follows:\n\n\n---\n\n**Comments**\n---\n\n\n> **C1. Additional experiments on a larger domain set:** Locomotion environments tend to be somewhat easier for RL agents (evidenced by a variety of self-supervised methods learning to walk). I would recommend increasing the scope of the experiments to a robotic manipulation domain which would allow readers to understand the limitations of the work.\n\n**Response:** We would like to thank the reviewer for raising this question. We wish to draw the reviewer\u2019s attention to Section A.4.3 and Fig. A3 (or Section 5.4 and Fig. A4 in the old version), where we presented the experiment in the AdroitHandDoor environment [r1]. This environment is a task of the Adroit manipulation platform, which features a Shadow Dexterous Hand attached to a free arm with up to 30 actuated degrees of freedom. The results demonstrated that the proposed TDIL method attained expert-level performance (100% success rate) within one million steps and was able to surpass the performance of BC(10% success rate), CFIL(10% success rate), and f-IRL(40% success rate).\n\n>**C2. Ablations utilizing high dimensional states:** How does the method perform when utilizing demonstrations from the visual domain? This is necessary for completeness as there are several recent works, for instance FISH https://arxiv.org/abs/2303.01497, RoboCLIP https://arxiv.org/pdf/2310.07899.pdf which learn a reward from visual observations from a single demonstration.\n\n**Response:** \nThank you for mentioning these two highly related papers. We have cited them in our latest version.\nWe would like to address this question by elaborating on the differences in settings between these methods and ours.\n\nIn the case of RoboCLIP [r2], the reward signal was generated using a **pretrained** video-and-language model (VLM) that was trained with additional data. In contrast, our proposed TDIL method trains the agent from **scratch**.\n\nRegarding FISH [r3], it employed cost functions such as Euclidean (L2) and Cosine distance to derive rewards. However, such cost functions **are not theoretically guaranteed** since they do not consider the dynamics of the environment. For example, two states might be close in Eurclidean (L2) distance, but they can not reach to each other quickly since there are barriers between them. \nIn contrast, our reward function is specifically designed to capture these dynamics, which is a primary focus of our paper. By considering dynamics, TDIL achieves superior performance, as demonstrated in our motivational experiment (please kindly refer to Fig. 1) and in the experiments detailed in Section 5 (Note that PWIL uses L2).\n\nIn this paper, our main focus is to introduce a transition discriminator-based approach to enhance reward density and facilitate one-demonstration imitation learning scenarios. In the future, exploring the leverage of high-dimensional inputs (e.g., images) would be an interesting direction to investigate. \n\n[r1] Rajeswaran, Aravind, et al. Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations. arXiv:1709.10087 (2017).\\\n[r2] Sontakke, Sumedh A., et al. Roboclip: One Demonstration is Enough to Learn Robot Policies. arXiv:2310.07899 (2023).\\\n[r3] Haldar, Siddhant, et al. Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations. arXiv:2303.01497 (2023)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638171712,
                "cdate": 1700638171712,
                "tmdate": 1700733116580,
                "mdate": 1700733116580,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]