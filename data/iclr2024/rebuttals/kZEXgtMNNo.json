[
    {
        "title": "Large Language Models as Automated Aligners for  benchmarking  Vision-Language Models"
    },
    {
        "review": {
            "id": "K37rFMAxlJ",
            "forum": "kZEXgtMNNo",
            "replyto": "kZEXgtMNNo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8696/Reviewer_G18f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8696/Reviewer_G18f"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Auto-Bench, an automated benchmarking pipeline that utilizes Large Language Models (LLMs) to curate data and evaluate Vision-Language Models (VLMs). The pipeline includes LLMs as automatic curators to generate question-answer-reasoning triplets and LLMs as judges to assess VLMs' performance. The paper shows the effectiveness of Auto-Bench in data curation, model evaluation, and supervised fine-tuning of VLMs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper introduces an innovative and comprehensive benchmarking pipeline for VLMs.\n2. The use of LLMs as automatic curators and judges adds scalability to the evaluation process.\n3. The extensive dataset curated by Auto-Bench enables effective evaluation and fine-tuning of VLMs."
                },
                "weaknesses": {
                    "value": "1. From my perspective, the approach provided by Visual Instruction Tuning [1] for constructing multimodal instruction data also constitutes a scalable, user-friendly, and comprehensive automated pipeline. Therefore, the claim about data generation is relatively weak. I've noticed that you mention your method being more diverse and complex, but in the analysis of 3.3 BENCHMARK STATISTICS, there is no mention of Visual Instruction Tuning. Lastly, could you please elaborate on any other significant differences and advantages of your method against Visual Instruction Tuning?\n2. The use of GPT-4 or GPT-3.5 has already been mentioned in other articles, such as [2] and [3].\n3. Assessments of different visual capabilities have also been addressed in other evaluations, such as [2], [3], [4], and [5].\n[1] Visual Instruction Tuning\n[2] MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities\n[3] TouchStone: Evaluating Vision-Language Models by Language Models\n[4] MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models\n[5] MMBench: Is Your Multi-modal Model an All-around Player?"
                },
                "questions": {
                    "value": "Please check the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8696/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8696/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8696/Reviewer_G18f"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8696/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698485709408,
            "cdate": 1698485709408,
            "tmdate": 1699637089937,
            "mdate": 1699637089937,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uq9sgxUtNg",
                "forum": "kZEXgtMNNo",
                "replyto": "K37rFMAxlJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8696/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8696/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer G18f (Part I)"
                    },
                    "comment": {
                        "value": "Dear Review G18f,\n\nThank you for your valuable feedback and comments on our paper. We have carefully considered the raised concerns and address them accordingly in the following.\n\n> **Q1: Comparisons with Visual Instruction Tuning**\n\nWe appreciate the reviewer's observation on Visual Instruction Tuning. While we aggree that both our methods and Visual Instruction Tuning utilize LLMs to generate data automatically, our methods offer some significant differences as what follows:\n\n(1) **Diversity on the ability categories** \nThe data generated by Visual Instruction Tuning does not specify the abilities that need to be evaluated. The questions they generate only broadly cover five aspects: object types, counting the objects, object actions, object locations, and relative positions between objects. In contrast, our generation is ability-aware, where we have specific prompts for each ability to generate content that targets each skill individually. For the final generated data, **we have covered a total of 17 different abilities (which is more than 4 times of Visual Instruction Tuning)**. Therefore, our generated data exhibits higher diversity in terms of ability categories.\n\n(2) **Diversity on the visual symbolic information** \nVisual Instruction Tuning provides only limited caption and 2D bounding box information to prompt GPT in generating questions. This, to some extent, restricts the diversity of generated questions. For instance, its visual symbolic information lacks OCR information, resulting in the absence of OCR-related questions in its generated data. In contrast, our approach incorporates richer visual symbolic information, such as text, caption, instance relationships, and etc. Therefore, we can prompt GPT to generate data that covers a wider range of information.\n\n\n\n\n> **Q2: Utilization of GPT**\n\n\nThanks for highlighting the concurrent works [2] and [3]. While these referenced works also employ GPT for benchmark evaluation, notable distinctions exist between our approach and theirs, as outlined below.\n\n(1) **The way of leveraging GPT is different.** \nConcurrent works (e.g., [2] and [3]) solely employ GPT for open-ended evaluation, while our approach integrates GPT not only in automatic evaluation but also in the data curation process. Specifically, we utilized GPT to automatically generate ability-aware questions, spanning a total of 17 different abilities. In contrast, [2] and [3] rely on existing datasets for evaluation, which, to some extent, may lack comprehensiveness. For instance, [2] comprises only 187 images and 205 questions, posing challenges for a comprehensive evaluation. In our exploration, beyond conducting evaluations based on GPT, we also showcased the advantages of using GPT as data curators, highlighting the potential of GPT in this context.\n\n(2) **The purpose of leveraging GPT is different.** [2] and [3] utilized GPT-4 primarily for automated assessment. In contrast, we aim to present a system employing GPT for both data generation and evaluation, thereby offering a fully automated, scalable, user-friendly, and comprehensive pipeline"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8696/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555147175,
                "cdate": 1700555147175,
                "tmdate": 1700555235796,
                "mdate": 1700555235796,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KhiWHCRp1A",
                "forum": "kZEXgtMNNo",
                "replyto": "K37rFMAxlJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8696/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8696/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer G18f (Part II)"
                    },
                    "comment": {
                        "value": "> **Q3: Assessments of different visual capabilities**\n\nWe acknowledge that assessments of different visual capabilities have been mentioned in previous methods [2, 3, 4, 5].  Therefore, we conducted a quantitative comparison based on the number of assessed capabilities and the quantity of evaluation questions to highlight the diversity of our approach. The table below illustrates that our method significantly surpasses previous methods in the number of evaluation questions. For instance, we exceed the evaluation dataset of MMBench by tenfold (28,500 v.s. 2,974). Additionally, our method is comparable to previous methods in the number of assessed capabilities. We can also extend LLM to assess additional capabilities, thereby scaling our dataset further.\n\n| Methods        | MM-Vet[2] | TouchStone[3] | MME[4] | MMBench[5] | VisIT-Bench[6] |AutoBench(ours) |\n| -------------- | -- | -- | -- | -- | -- | -- |\n| Ability Amount | 6 | 27 | 14 | 20 | 70 | 17 |\n| QA Pairs Amount | 205 | 908 | 1477 | 2974 | 592 | 28500 |\n\n\nWe sincerely thank you for your time! Hope we have addressed your concerns. We look forward to your reply and further discussions, thanks!\n\nSincerely,\n\nAuthors\n\n\n[1] Haotian Liu, et al. Visual Instruction Tuning. arXiv preprint arXiv:2304.08485, 2023.\n\n[2] Weihao Yu, et al. MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. arXiv preprint arXiv:2308.02490, 2023.\n\n[3] Shuai Bai, et al. TouchStone: Evaluating Vision-Language Models by Language Models. arXiv preprint arXiv:2308.16890, 2023.\n\n[4] Chaoyou Fu, et al. MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models. arXiv preprint arXiv:2306.13394, 2023.\n\n[5] Yuan Liu, et al. MMBench: Is Your Multi-modal Model an All-around Player? arXiv preprint arXiv:2307.06281, 2023.\n\n[6] Yonatan Bitton, et al. VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. arXiv preprint arXiv:2308.06595. 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8696/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555177235,
                "cdate": 1700555177235,
                "tmdate": 1700555243211,
                "mdate": 1700555243211,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XCYwlpkTBP",
            "forum": "kZEXgtMNNo",
            "replyto": "kZEXgtMNNo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8696/Reviewer_69zX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8696/Reviewer_69zX"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes AutoBench, a method to generate a large amount of instruction-response pairs for evaluating large multimodal models. The main motivation of the work is centered around the fact that the current datasets are either not suitable for open-ended evaluation or rely on expensive and limited human evaluation. The approach involves using LLM as data creators + LLM as evaluators. I like the motivation of the work, however, the paper majorly lacks in the quality of the experiments and its setup.\n\nComments:\n\n- My major issue with the setup is using LLMs as data curators! Many existing LMMs are trained on GPT-4 generated data such as LLaVA, mPLUG-Owl etc. To what extent is the evaluation fair across all the models which are not specifically not trained on the GPT-4 generated data?\n- The experiments are performed by generating instructions and responses for the COCO images and their corresponding COCO captions. I find this setup highly concerning since InstructBLIP (best model in their eval) is trained with COCO captions. Similarly, other models like LLaVA are also trained with the COCO captions. Again, this makes the evaluation unfair to all the models. \n- Quoting from the paper: \u201c Besides, we carefully curated approximately 28.5K high-quality samples to comprise the validation dataset, which was subsequently utilized for performance evaluation.\u201d There is no information on how this curation was performed? \n- Benchmark scope: It remains unclear to me how an image in the dataset is assigned a particular subskill? When do you know that a particular image has physics related possible questions?\n- Quoting from the paper: \u201cDue to the nature of reasoning-based questions often lacking a unique answer, we format them as multiple-choice questions, thus reducing the difficulty of evaluation and ensuring its accuracy.\u201d It would be better to provide some examples to understand why reasoning-based questions need multiple choice questions. Why can\u2019t they be just open-ended even if they lack a unique answer.\n- Thanks for showing distributions of the question length and cosine similarity. Figure 3b looks more or less overlapping and hence does not shout semantic richness in comparison to other dataset. \n- Figure 3 should also have datasets like Visit-Bench or Touchstone where the questions are collected from humans. I feel that the human questions will be the most diverse.\n- How much does it cost to add a new model to your benchmark involving 28.5K instances?\n- To establish LLM as a valid judge, the paper does not have any correlations with human numbers. In addition, it would be important to show that simpler metrics that are based on lexical similarity are not good judges for this dataset, which it currently does not do.\n- I like the paper\u2019s diversification into understanding the usefulness of the models along different skills and sub-skills."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Mentioned in the summary"
                },
                "weaknesses": {
                    "value": "Mentioned in the summary"
                },
                "questions": {
                    "value": "Mentioned in the summary"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8696/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8696/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8696/Reviewer_69zX"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8696/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698732137659,
            "cdate": 1698732137659,
            "tmdate": 1700595618712,
            "mdate": 1700595618712,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "M6IuwN1SL6",
                "forum": "kZEXgtMNNo",
                "replyto": "XCYwlpkTBP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8696/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8696/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 69zX  (Part I)"
                    },
                    "comment": {
                        "value": "Dear Review 69zX,\n\nWe appreciate your insightful and constructive feedback. We address your comments below, and discuss the revisions we made accordingly. \n\n\n> **Q1: Fairness issue in evaluating VLMs that trained on other GPT-4 generated data.**\n\nWhile both the Auto-Bench and prior datasets (e.g., LLava-i-158K [0]) are curated using GPT-4, distinct variations exist as shown in the following, leading to a domain disparity between our data and that of others, thus ensuring fair evalution.\n\n- **Auto-Bench incoporates more visual symbolic information.** \nAuto-Bench integrates a more comprehensive set of visual symbolic cues, including OCR, captions, instance relationships, and others. This diversification marks a departure from existing GPT-4 systhesis data.\n\n- **Auto-Bench generates a more diverse set of ability types.** \nPrevious benchmark generates questions from limited aspects: object types, object counting, object actions, object locations, and relative positions between objects. In contrast, the dataset in Auto-Bench encompasses a total of 17 distinct abilities, surpassing LLava-i-158K by more than fourfold, for example.\n\nBesides, we observed that not only LLAVA but also most of the models (except BLIP2) compared in the main paper are trained with GPT4-generated data. However, Table 8 indicates there is no strong correlations bwteen the performance and the GPT4-generated data. For example, Blip2 (trained without GPT-4 data) perform better than MiniGPT-4 (trained with limited CC+ChatGPT instruction data) across various sub-skills. \n\nThese findings affirm the fairness and balance of our comparisons, addressing concerns about data homogeneity and model bias. \n\n> **Q2: Fairness issue in evaluation VLMs that trained on coco caption data**\n\nThanks for pointing out this concern. We believe the evaluation is fair due to the following two reasons:\n\n- Compared with COCO caption dataset, Auto-Bench offers **unique and previously unseen** QAs by prompting GPT4 from diverse visual symbolic information. Though sharing the same COCO images, these unsceen QAs and domain disparities ensure the fair evaluation. \n\n- We found that InstructBLIP is not the sole model trained with COCO captions; all models, with the exception of MiniGPT4, underwent training on COCO captions. Despite this shared training data, we did not observe an obvious positive correlation in performance when comparing models trained with and without COCO captions. This finding also underscores the distinctive and complex nature of the Auto-Bench dataset and indicates the fair evaluation.\n\nWe greatly appreciate your suggestions of Q1 and Q2 and have thoroughly discussed these potential issues in **Section A.1.5**.\n\n\n> **Q3: Human verfication in validation data**\n\n\nActually, we employed a crowdsourcing approach for manual validation, assessing each question against three criteria, including (1) the relevance to the sub-skill topic, (2) appropriateness of the answer, and (3) logical consistency in reasoning. The question is deemed valid only when it meets all criteria. **Table 2** in the original main paper presents review statistics for 800 questions across 16 skills. We appreciate your reminder and have updated these details in **Section 3.1** of the revised main paper.\n\n\n> **Q4: How are images in the dataset categorized by specific subskills?**\n\n\nThanks for the good questions. For general subskills, we designed questions for all images. For specific skills, we filtered images using COCO labels to ensure scenarios matching the skill. For instance, for physics, we only selected images with people and objects; for biology, we chose images with plants, vegetables, fruits, etc. This case-by-case approach also explains why there are fewer questions generated in the reasoning category. We have updated the details in **Section A.1.1** of the Appendix.\n\n\n> **Q5: Some illustrations of Open-ended questions**\n\nWe provide the followig case for a better illustration.\n- *Image context*: A downtown street with people hurrying by has a small store with colorful handicrafts in the window. A young man in sneakers and a sweatshirt stops and gazes at the window.\n- *Question related to reasoning*: Why the man gazes at the window of the store?\n- *Answer 1*: The young man is an art lover who is looking for a unique craft to decorate his home and highlight his taste in art.\n- *Answer 2*: The young man, who is actually a basketball player, saw some unique handcrafted basketballs in a store and he wanted to purchase one as a souvenir.\n\nAs we can see, for reasoning tasks, there might be numerous correct answers in an open-ended questions, which challenges the LLM's judgment capability, often leading to additional errors. To mitigate this, we structured the reasoning questions in a multiple-choice format. This approach limits the degrees of freedom, enhancing the stability and reliability of the assessment."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8696/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554985621,
                "cdate": 1700554985621,
                "tmdate": 1700554985621,
                "mdate": 1700554985621,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8W3qkXBmF7",
                "forum": "kZEXgtMNNo",
                "replyto": "XCYwlpkTBP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8696/Reviewer_69zX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8696/Reviewer_69zX"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Hi, thanks for answering my questions thoroughly. I am raising my score to 5 as their response alleviates some of my concerns, and will retain it since the idea of the work is not new to me -- GPT-4 has been used for generating instructions and judging responses in prior works separately if not in tandem. \n\nI am adding more comments to improve your current draft.  \n\n1. **Fairness**\n\n- I thank the authors for clarifications on the fairness of the evaluation. According to the authors, most (if not all) of the models have been trained with the GPT-4 generated instructions, COCO captions, and COCO images. I buy the argument that evaluation dataset is (un)fair to all the models equally. \n\n- I do think that this fairness issue needs to be discussed in the main text (even 2-3 lines is fine) instead of completely relegating it to a small Appendix A.1.5\n\n- GPT-4 generated data training: The authors utilize visual symbolic info, diverse set of ability types which they believe is absent in LLaVA-158K dataset. However, all such arguments make sense, I strong empirical evidence is still missing. I suggest the authors to perform similarity analysis between their dataset (and other LMM evaluation dataset) instructions vs LLaVA-158K instructions. It will give you a good sense of the diversity/difference with existing datasets. It will help you understand the nuances of the dataset better. \n\n- COCO Images: Same as above. You can compare the similarity between your dataset (and other LMM eval) dataset images and LLaVA-158K images to better argue the diversity in your dataset. \n\n2. **Human verfication in validation data**\n\n- If human verification is done on 800 instances, then why do the authors claim 27.5K highly curated samples?\n\n3. **How are images in the dataset categorized by specific subskills?**\n\n- The authors should provide the heuristics in greater detail in the revised paper.\n\n4. **Structure of the reasoning questions**\n\n- I do not fully buy the argument provided for keeping reasoning questions as multiple-choice questions. If the LLM is not good as judge on evaluating the responses for the reasoning questions, it is a drawback that should be discussed in the paper. The whole claim of the paper is that LLM can replace humans as judges. But it seems that it is not always the case.\n\nGood luck to the authors for their hard work!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8696/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595574720,
                "cdate": 1700595574720,
                "tmdate": 1700595588251,
                "mdate": 1700595588251,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vo30QuAT2I",
                "forum": "kZEXgtMNNo",
                "replyto": "XCYwlpkTBP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8696/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8696/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 69zX II (Part1)"
                    },
                    "comment": {
                        "value": "Thanks for the prompt feedback. We are glad to note that we've resolved some of your concerns, including: \n\n- (1) The evaluation on GPT-generated data is fair; \n- (2) The diverse visual symbolic information and ability types differs Auto-Bench from other datasets. \n\nSince there are still some concerns that have not been fully clarified, we aim to address your remaining comments in the following detailed responses.\n\n> Q1: Discussion of the fairness issue in the main paper\n\nThanks for the suggestions. We have incorporated the relevant discussions (as shown in the following italicized contents) into the Section 3.4 main of main paper to enhance reader comprehension of fairness issues. \n\n*''We aim to evaluate VLMs on the LLM-curated data using LLM's judgments. It is important to acknowledge a theoretical risk of unbalanced evaluation, given that some current VLMs are trained on LLM-generated datasets. However, considering that the Auto-Bench dataset offers substantially distinct datasets compared to previous ones, encompassing more abilities and diverse input visual symbolic information, and given that the selected VLMs are predominantly trained on LLM-generated data, the comparisons can be deemed fair.''*\n\n\n> Q2: The strong empirical evidence of the difference between datasets\n\nAs suggested, we have provided two empirical evidences to conduct the differences/similarities analysis between Auto-Bench and others, which are shown in the following:\n\n\n- (1) We have updated Figure 3 by incorporating the statistics of LLAVA-158K, generated by GPT4 as detailed in [1]. The results indicate that while Auto-Bench and LLAVA-158K share similar question lengths, Auto-Bench demonstrates greater diversity, as evidenced by the right part of Figure 3. We attribute this enhanced diversity in Auto-Bench to the incorporation of diverse visual symbolic information and ability types.\n- (2) In addition to intra-correlations, we have conducted inter-correlations as outlined in the following table. Specifically, we randomly selected 2,000 questions from two distinct datasets and computed cosine similarities on the corresponding features extracted by the Simcse text encoder. Results (last line in the following table) shows that though both are generated from GPT-4, LLAVA-158K is not the semantically closest (since the score are not the highest) in resemblance to our Auto-Bench. The following table and related analysis have been updated in Section A.1.5 of the appendix.\n\n\n\n|                | LLaVA[1]              | VQAv2[2] | VisIT-Bench[3]  | TouchStone[4] | GQA[5]    | OKVQA[6]  | Auto-Bench |\n|----------------|--------------------|---------|--------|------------|--------|--------|-----------|\n| **LLaVA[1]**           | -                  | 0.2270  | 0.2209 | 0.2677     | 0.2352 | 0.2351 | 0.2421    |\n| **VQAv2[2]**         | 0.2270             | -       | 0.1969 | 0.2479     | 0.2613 | 0.2486 | 0.2415    |\n| **VisIT-Bench[3]**           | 0.2209             | 0.1969  | -      | 0.2531     | 0.2108 | 0.2210 | 0.2209    |\n| **TouchStone[4]**     | 0.2677             | 0.2479  | 0.2531 | -          | 0.2549 | 0.2590 | 0.2507    |\n| **GQA[5]**            | 0.2352             | 0.2613  | 0.2108 | 0.2549     | -      | 0.2488 | 0.2540    |\n| **OKVQA[6]**          | 0.2351             | 0.2486  | 0.2210 | 0.2590     | 0.2488 | -      | 0.2378    |\n| **Auto-Bench**      | 0.2421             | 0.2415  | 0.2209 | 0.2507     | 0.2540 | 0.2378 | -         |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8696/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718462021,
                "cdate": 1700718462021,
                "tmdate": 1700718506380,
                "mdate": 1700718506380,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YFmZRnWrqm",
            "forum": "kZEXgtMNNo",
            "replyto": "kZEXgtMNNo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8696/Reviewer_m7V9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8696/Reviewer_m7V9"
            ],
            "content": {
                "summary": {
                    "value": "The authors use LLMs to generate and curate a new evaluation benchmark for vision-language models, dubbed Auto-Bench. By conditioned on image verbalizations (such as image captions, object locations, OCR etc) LLMs are used to generate question-answer-rationale triplets to evaluate a wide variety of vision-language capabilities, covering perception, reasoning, planning abilities and alignment with human values. To overcome evaluation bottlenecks such as surface form variation, he authors further propose using LLMs to judge model responses. Finally, the authors benchmark several state-of-the-art VL models using their proposed benchmark."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- This work addresses an important challenge -- existing vision-language benchmarks only cover a narrow range of capabilities, and are limited in size due to the difficulty of manual curation.\n\n- The primary contribution of this work is substantial -- a large benchmark containing more than 3 million examples, including a train split and a high-quality human-curated validation set, as well as an evaluation framework using LLMs.\n\n- The authors choose a large breadth of the different capabilities to evaluate VLMs on, including planning and human value alignment which have not been focused on much in prior work. I also like how different tasks were framed as open-ended and close-ended depending on the nature of the task. \n\n- I really like the comparison between AutoBench and existing human-curated VQA datasets in section 4.2.\n\n- The experiments are well done. I especially appreciate the Inclusion of qualitative model comparisons using an ELO system.\n\n- The analysis of alignment between LLM and humans as judges in Section 4.4.\n\n- The paper is well-written and easy to follow for the most part.\n\nWell done!"
                },
                "weaknesses": {
                    "value": "- The benchmark is built only on images sourced from MS-COCO. While I understand that this was done due to the richness of image annotations that could be fed to the LLM to generate questions, MS-COCO images also represent a narrow subset of all images which we would like to apply vision-language models to. Vision-language models have been oversaturated on COCO-based benchmarks, so performance on Auto-Bench may not be reflective of performance on other image domains (e.g. VizWiz, or medical images)"
                },
                "questions": {
                    "value": "In section 4.2, \"Users are guided to rank each sample based on its rationality and level of challenge.\" what does rationality mean?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8696/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698943804473,
            "cdate": 1698943804473,
            "tmdate": 1699637089723,
            "mdate": 1699637089723,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WZCyoKyvw6",
                "forum": "kZEXgtMNNo",
                "replyto": "YFmZRnWrqm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8696/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8696/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respones to Reviewer m7v9"
                    },
                    "comment": {
                        "value": "Dear Review m7V9,\n\nThanks for your comments and acknowledgment of our novelty. We provide responses to your concerns below.\n\n> **Q1: The used MS-COCO images may not cover the extensive image domains.**\n\nWe appreciate and acknowledge the reviewer's observation regarding the limitations of using the COCO dataset, as it may not cover all image domains, such as medical images. The choice of COCO for evaluation was made because of its rich annotations. However, we would like to emphasize that the contribution of this work extends beyond providing a COCO-based dataset for evaluating VLMs.\n\nThe primary contribution, in fact, lies in offering an automatic pipeline that is readily extendable to new data domains and scenarios, such as VizWiz. To achieve this, we can leverage off-the-shelf perception models to obtain visual symbolic representations, as mentioned in our paper. Subsequently, we can conduct data curation and model evaluation tailored to the specific domain. We have incorporated this discussion into the revision outlined in **Section A.1.1** of the Appendix.\n\n\n\n> **Q2: Meaning of \"Rationality\" in Section 4.2**\n\n*'Rationality'* in this context refers to the logical coherence of a question in relation to the specified curation topic and the context presented within the image. We have added the descriptions in **Section 4.2**, for better clarity. Please check the latest revision.\n\n\nWe sincerely thank you for your time. Hope we have addressed your concerns with the above responses. We look forward to your reply and further discussions.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8696/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554841325,
                "cdate": 1700554841325,
                "tmdate": 1700554985312,
                "mdate": 1700554985312,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]