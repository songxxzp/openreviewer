[
    {
        "title": "Hierarchical Gaussian Mixture Normalizing Flows Modeling for Multi-Class Anomaly Detection"
    },
    {
        "review": {
            "id": "WiTOEhn6tT",
            "forum": "hWF4KWeNgb",
            "replyto": "hWF4KWeNgb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4336/Reviewer_v6Sb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4336/Reviewer_v6Sb"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a normalizing flow-based unified anomaly detection method, i.e., Hierarchical Gaussian Anomaly Detection (HGAD). By designing a loss function, the proposed method attempts to handle the intra-class diversity and inter-class separation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The proposed method can model normalizing flows with hierarchical Gaussian mixture priors on the multi-class anomaly detection task.\n* The design of the proposed method can model the inter-class Gaussian mixture priors, learn class centers, and maximize mutual information."
                },
                "weaknesses": {
                    "value": "* The presentation and the layout of the manuscript are bad. For examples,\n  * Figure 2(b)/(d): The authors need to clarify the colors associated with the classes.\n  * What are the $\\lambda_1, \\lambda_2$ in Section 4.2? It is confusing that the authors list several separate loss functions in Section 3.3 without any articulation about how to deal with these equations to achieve the goal(s). I finally found the objective function of the target goal after checking with the appendix, but the authors didn't mention anything in the main paper.\n  * In Section 3.4: What did the authors mean by level-k? Additionally, since there is no access to the label in the test, which $y$ in $\\mu_i^y$ will be used for the test point?\n  * The limited explanation between problem formulation and the experiment setup: \n    * In Section 3.2, since the authors pointed out that Eq. (2) is used to maximize the log-likelihood of normal features, why do the normalizing flows present a large log-likelihood for both normal and abnormal features? In other words, are both normal and abnormal/anomaly observations used in this loss function?\n    * In Section 4, what is the partition for the data in experiments? What are the normal classes? What are the anomaly classes? If label information (including anomalies) is used in the training, why do we call this multi-class anomaly detection? What is the difference between this with the regular multi-class classification?\n\n* Since there are multiple goals contained in the objective function and different training strategies in the experiments, to clearly summarize the work, it would be better to use pseudocode to outline the algorithm.\n\n* The weak support of the necessity of the intra-class centers: From Figure 3(b), I cannot see there is a significant difference among different numbers of intra-centers."
                },
                "questions": {
                    "value": "* Section 3.1: Why $p_\\theta$ is a probability rather than a density? If it is a density function, why did the authors subtract that from 1 (any motivation)? \n\n* Figure 3: Why is the positional encoding added to the normalizing flow? Is this necessary? Did the authors conduct the ablation study of this design?\n\n* Bottom in Page 5: Why do not just use sample class priors to estimate $p(Y)$? Which part of the architecture in Figure 3 is used to estimate $p(Y)$? Could the authors explain in detail?\n\n* The notations in (8), and (9) are bad. What is $\\mu\\_{y^\\prime}$? Do you mean the center vector? Is loss (9) necessary? Why there is no penalty cost before this loss in the final object function? Did the authors conduct the ablation study for this loss function?\n\n* The discussion in Section 3.3:\n  * Could the authors further clarify this sentence: \"Because our method only explicitly uses class labels, while they implicitly use class labels (see App. A)\".\n  * I see one loss function is designed to maximize the log-likelihood of normal observations. Why did the author claim that using a label should not be a strict condition? Did the author conduct the experiment to support this conclusion?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4336/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698425236022,
            "cdate": 1698425236022,
            "tmdate": 1699636403622,
            "mdate": 1699636403622,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KxXE4iYLDK",
                "forum": "hWF4KWeNgb",
                "replyto": "WiTOEhn6tT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4336/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**[To W1].** The red means abnormal and the green means normal. In anomaly detection papers, authors usually use red to represent abnormal and green for normal, we are a bit careless in not clearly indicating the corresponding classes of the two colors in Figure 2. We will add a legend to the figure to indicate the meaning of two colors in the revision.\n\n**[To W2].** We are sorry about this confusion. Due to page limitation, we move the overall loss function to the Appendix. This results in the absence of definitions of hyperparameters $\\lambda_1$ and $\\lambda_2$. We really appreciate that you point out this problem. We will revise the layout of our paper to ensure that such a problem won't appear in our paper.\n\n**[To W3].** The Feature Extractor will extract multi-scale feature maps, level-k means the kth feature map level. Using multi-scale features is common in anomaly detection, and our method also builds a normalizing flow model at each feature level. We appreciate your suggestion and will add explanations for level-k in the revision. \n\nFirstly, we will explain the difference between multi-class anomaly detection (AD) and multi-class classification, you may misunderstand our multi-class AD task. Multi-class classification focuses on classifying input samples, while anomaly detection focuses on detecting abnormal regions in input samples. Previous AD methods follow the one-for-one paradigm (i.e., we train one model for each class). In multi-class AD, we only train one unified model for all classes. Thus, $y$ doesn't indicate whether the input is normal or abnormal but indicates what class the input belongs to. Unlike multi-class classification, we do not know whether the input is normal or abnormal, but we can know which class it belongs to. Please also see our responses to W5 and Q5.\n\n**[To W4].** In anomaly detection, we only use normal samples for training as our goal is to detect anomalies. In our paper, we have clearly explained that normalizing flow (NF) may have a \u201dhomogeneous mapping'' problem when used for multi-class anomaly detection, where NF may take a bias to map different input features to similar latent features (i.e., this means anomalies will have similar log-likelihoods to normal). Moreover, the goal of training is to maximize the log-likelihoods of normal features. Thus, normalizing flows will represent large likelihoods for both normal and abnormal features. Please see the sec. 3.2 in our paper, we have provided detailed explanations for this.\n\n**[To W5].** Firstly, we will indicate the difference between industrial anomaly detection and semantic anomaly detection. Our paper focuses on industrial anomaly detection rather than semantic anomaly detection. In industrial anomaly detection, all classes are normal, the anomalies are defective areas that exist in the image and don't have classes. Only in semantic anomaly detection (e.g., CIFAR10, CIFAR100, and ImageNet-30 are usually the datasets), we will select some classes as normal and the other classes are used as abnormal.\n\nThe partition follows the standard way of these industrial AD datasets, MVTecAD, BTAD, MVTec3D-RGB, and VisA. Generally speaking, most industrial AD papers will not specifically elaborate on the data partition, as the training and test sets are fixed. \n\nWe have clearly defined the multi-class AD in the second paragraph of Introduction: one unified model is trained with normal samples from multiple classes, and the objective is to detect anomalies in these classes. In training, we only use normal samples from multiple product classes without any anomaly, the label information is used to indicate one normal sample belongs to which class. For the last question, please see our response to W3.\n\n**[To W6].** Thanks for your suggestion, we will add the pseudocodes of our algorithm in the revision.\n\n**[To W7].** We validate the effectiveness of the intra-class centers in ablation studies (see Tab. 3(a)). The number of intra-class centers is the same for each class (we state this in our paper, please see sec. 4.2, Setup). Figure 3 is just a schematic diagram of our model. Moreover, we can see that the number of intra-class centers in the two classes is the same, but the intra-class distributions are different (this is our purpose of introducing intra-class centers)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699881151281,
                "cdate": 1699881151281,
                "tmdate": 1699881151281,
                "mdate": 1699881151281,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SGz5mttZmD",
                "forum": "hWF4KWeNgb",
                "replyto": "WiTOEhn6tT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4336/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**[To Q1].** $p_\\theta(x) = {\\rm e}^{{\\rm log}p_\\theta(x)}$ is coming from the log-likelihood ${\\rm log}p_\\theta(x)$ by exponential function. We call it probability as the value of $p_\\theta(x)$ is in $(0,1)$. Our statement is: $s(x) = 1 - p_\\theta(x)$, $p_\\theta(x)$ is a value for representing normality, $s(x)$ is thus a value for representing abnormality. \n\n**[To Q2].** It's necessary. With positional embeddings, we can achieve better results. We don't conduct an ablation study for this design, as the effectiveness of positional embeddings is already validated in the previous work CFLOW [1]. In our paper, we also state that the normalizing flow used in our model is the same as the one in CFLOW (please see sec. 4.2, Quantitative Results).\n\n[1] Denis Gudovskiy, Shun Ishizaka, and Kazuki Kozuka. Cflow-ad: Real-time unsupervised anomaly\ndetection with localization via conditional normalizing flows. In IEEE Winter Conference on\nApplication of Computer Vision, 2022.\n\n**[To Q3].** Because we think that parameterization $p(Y)$ can enable the network to adaptively learn the class weights, and parameterization $p(Y)$ only introduces a small number of parameters. The parameter to control $p(Y)$ is $\\psi$, and $\\psi$ is learned by optimizing the E.q(6). Then $p(y)$ is estimated by ${\\rm softmax}_y(\\psi)$. No module in Figure 3 is dedicated to estimating $p(Y)$, the parameter $\\psi$ belongs to the normalizing flow model.\n\n**[To Q4].** We respectfully disagree with this comment. $\\mu_{y^\\prime}$ means all other class centers except the class center $\\mu_y$ corresponding to $y$. Because our method is used for multi-class anomaly detection, there will be many class centers $\\{\\mu_y\\}, y \\in \\{1,\\dots,N\\}$. In Eq.(8) and (9), when we employ the softmax function to calculate the value for a special class $y$, all the other classes are naturally represented as $y^\\prime$. We think that such notations are commonly used in the softmax function. So, we don't specifically explain $y^\\prime$. We will add corresponding explanations to make E.q (8) and (9) easier to understand in the revision.\n\nLoss (9) is necessary. In Tab 3(c), we show that using Entropy is beneficial to achieve better results. And loss (9) is used to optimize entropy during training. The total loss is $\\mathcal{L} = \\lambda _1\\mathcal{L} _g + \\lambda _2\\mathcal{L} _{mi} + \\mathcal{L} _e + \\mathcal{L} _{in}$. Our design is to use $ \\mathcal{L} _e $ and $ \\mathcal{L} _{in} $ as auxiliary losses. As there are four optimization objectives, adding a weighting factor and conducting ablation studies for each loss item will result in a large number of combinational experiments, which will bring us too much burden. \n\n**[To Q5].** In Appendix A, we have clearly explained this statement. The existing AD datasets are collected for one-for-one anomaly detection (i.e., we need to train a model for each class). Thus, the existing AD datasets need to be separated according to classes, with each class as a subdataset. Therefore, one-for-one AD methods also need class labels, as they require normal samples from the same class to train, but they don't explicitly use the class label. Our method can achieve only training a unified model for all classes, this is a significant innovation compared to the one-for-one AD methods. Moreover, our method still follows the same data organization format as the one-for-one AD methods, but we need to explicitly assign a label for each class. This actually doesn't introduce any extra data collection cost. Please see Appendix A.1 for more discussions.\n\n**[To Q6].** As we mentioned above, the label is used to distinguish which class each sample belongs to. In a stricter condition, we may hope the multi-class AD models even don't need labels, which means that we have many normal samples from different classes but don't know which class they belong to. Because the AD datasets themselves separate samples according to classes, we can easily get the class label from these datasets. Therefore, we claim that using class labels should not be a strict condition (or constraint) when designing multi-class AD methods. We think that we explain this clearly, this conclusion seems not to require experimental validation."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699881211256,
                "cdate": 1699881211256,
                "tmdate": 1699881211256,
                "mdate": 1699881211256,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fjrOUjkoLg",
            "forum": "hWF4KWeNgb",
            "replyto": "hWF4KWeNgb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4336/Reviewer_tw2T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4336/Reviewer_tw2T"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a normalizing flow model with hierarchical Gaussian mixture prior for unified anomaly detection, HGAD. This method achieves the SOTA unified AD performance on four datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The analysis and discussion of the proposed model are detailed.\n\n2. The experiment results are superior to the comparison methods. \n\n3. The experiments in both the formal paper and appendix are relatively thorough."
                },
                "weaknesses": {
                    "value": "1. The abstraction is somewhat lengthy. Please polish the abstraction and make it concise.\n\n2. The size of coordinate/legend in Figure 2 is too small to recognize.\n\n3. The representation should be improved to be more professional. The explanations of some equations (eg. Eq6 and Eq9 ) are not easy-understood."
                },
                "questions": {
                    "value": "1. Is the homogeneous mapping issue intrinsically equal to the well-known identical shortcut problem?\n\n2. The citations might be wrong. Many citations should be placed in the brackets. Please pay attention to the difference between '\\citep' and '\\citet'.\n\n3. The full name of HGAD should be listed.\n\n4. Why the performance of multi-class case is lower than the unified case, as shown in Table 1.\n\n5. The best performance on MVTec in Table 1 are 98.4/97.9, but 97.7/97.6 in Table 3."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4336/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765440899,
            "cdate": 1698765440899,
            "tmdate": 1699636403536,
            "mdate": 1699636403536,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wto4A22txa",
                "forum": "hWF4KWeNgb",
                "replyto": "fjrOUjkoLg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4336/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**[To W1].** We are very grateful for this suggestion, we will polish the abstract to make it more concise in the reavision.\n\n**[To W2].** Thanks for your suggestion. We will attempt to use a larger font size to plot the figure for making the coordinate/legend large enough to directly read.\n\n**[To W3].** Thanks for your suggestion. E.q(6) is a derived loss (the detailed derivation is in Appendix E), we will include some key intermediate steps for assisting understanding. E.q(9) is actually the entropy formula, where we use $-||\\varphi_\\theta(x)-\\mu_y||^2_2/2$ as class logits. $\\mu_{y^\\prime}$ means all the other class centers except for $\\mu_y$. We don't specifically explain $\\mu_{y^\\prime}$ as we think this notation is commonly used in the softmax function. We will add more explanations to make E.q(9) easier to understand. For other equations, we will also carefully check and explain these equations more clearly in the revision.\n\n**[To Q1].** Thank you for the comment, but we cannot fully agree with this comment. The identical shortcut is essentially caused by the leakage of abnormal information. The process of reconstruction is to remove abnormal information in the input, resulting in the failure of reconstruction in abnormal regions. But if the reconstruction network is overfitted, the abnormal features may be leaked into the output, resulting in the reconstruction network directly returning a copy of the input as output. So, the reconstruction errors in abnormal regions will be small, leading to abnormal missing detection. This issue usually can be addressed by masking, such as the neighbor masking mechanism in UniAD. \n\nHomogeneous mapping is a specific issue in normalizing flow (NF) based AD methods. In previous NF-based AD methods, the latent feature space has a single center. When used for multi-class AD, we need to map different class features to the single latent center, this may cause the model more prone to take a bias to map different input features to similar latent features. Thus, with the bias, the log-likelihoods of abnormal features will become closer to the log-likelihoods of normal features, causing normal misdetection or abnormal missing detection. To address this issue, we propose hierarchical Gaussian Mixture normalizing flow modeling. Because there are significant differences in the causes and solutions of the two issues, we think that the two issues are not intrinsically equal. In Appendix A.2, we have provided a thorough discussion for this question.\n\n**[To Q2].** We are very grateful for this comment, it's really helpful for us to improve our paper. We will revise this issue in the revision.\n\n**[To Q3].** HGAD is taken from our paper title: **H**ierarchical **G**aussian Mixture Normalizing Flows Modeling for Multi-class **A**nomaly **D**etection. As this is too long, we didn't list the full name in the main text (only list \u201cHierarchical Gaussian mixture\u201d in Abstract). In the revision, we will explain the HGAD naming in the main text.\n\n**[To Q4].** In our paper, multi-class and unified actually have the same meaning. In the caption of Tab 1, we use \u201cunified/multi-class\u201d to express that multi-class is an alias for unified. The meaning of results is explained by the following sentence: \u201c$\\cdot$/$\\cdot$ means the image-level and pixel-level AUROCs''. We are sorry for this misunderstanding. We will revise this misleading caption in the revision.\n\n**[To Q5].** In Tab 3(a), the hyperparameters $\\lambda_1$ and $\\lambda_2$ are set as 1 and 10. However, we later find that setting $\\lambda_1$ and $\\lambda_2$ to 1 and 100 can get better results (see Tab 3(d)). In Tab 1, we update the better results, but most results in Tab 3 are obtained with the $\\lambda_1$ and $\\lambda_2$ set as 1 and 10."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699880274937,
                "cdate": 1699880274937,
                "tmdate": 1699880274937,
                "mdate": 1699880274937,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hK5Ciw2l7Z",
                "forum": "hWF4KWeNgb",
                "replyto": "wto4A22txa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4336/Reviewer_tw2T"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4336/Reviewer_tw2T"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. However, the explanation about Q1 is not convincing to me. \nThe overall representations need to be improved in the revised version. (Note that ICLR allows authors to upload the modified versions.)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641101892,
                "cdate": 1700641101892,
                "tmdate": 1700641101892,
                "mdate": 1700641101892,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iQHZ38XTnA",
            "forum": "hWF4KWeNgb",
            "replyto": "hWF4KWeNgb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4336/Reviewer_37tJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4336/Reviewer_37tJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of *supervised* *multiclass* anomaly detection, where the \"normal\" samples may belong to a pre-defined set of classes Y, and the goal is to detect anomalous samples that do not belong to any class in Y. The authors point to drawbacks with prior reconstruction-based and normalizing-flow (NF) based approaches to multiclass anomaly detection. They then propose a new approach for alleviating these drawbacks by building on existing NF-based methods replacing their unimodal Gaussian prior with a hierarchical Gaussian mixture prior. Experimental results and ablation studies demonstrate that the proposed approach is better on average compared to prior methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Extending existing NF-based approaches with a mixture of Gaussian prior looks like a natural approach to take for multiclass anomaly detection\n- Fairly extensive experimentation with ablation studies that attempt to show the role of individual loss components"
                },
                "weaknesses": {
                    "value": "- One of my main concerns is that most of the methods compared to (e.g. UniAD, FastFlow, etc) are *unsupervised* whereas the proposed method is a *supervised* approach explicitly requiring class labels to be provided (see e.g. discussion in Appendix A.1). On the face of it, this does not seem like a fair comparison to make. It is important that the authors explicitly summarize what supervision each method uses and justify why theirs is a better approach despite requiring explicit label information to be provided during training.\n\n- The writing and presentation is at places hard to follow. The authors are urged to present the high-level approach first before dwelling into the details of the individual loss components. Having an explicit pseudo-code stating what the supervision is for the algorithm, and how the overall optimization objective looks like would be very helpful.\n\n- The proposed approach appears to have a lot of moving parts: there are four loss components (one for a inter-class Gaussian mixture, one for an intra-class Gaussian mixture, a mutual information based and an entropy-based loss for class diversity), with two hyper-parameters for weighting them (Appendix C). Although the authors do conduct some analysis of different hyper-parameter combinations, one if left with a feeling that the approach is highly heuristic in nature, with the gains coming largely from heavy engineering effort. Improving the writing and presentation may help boost the reader's confidence in the proposed method."
                },
                "questions": {
                    "value": "Of the methods discussed, it appears that BGAD is supervised, but not compared to. Are there other methods you compare to in experiments which like your method are also supervised?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4336/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4336/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4336/Reviewer_37tJ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4336/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700706004397,
            "cdate": 1700706004397,
            "tmdate": 1700706004397,
            "mdate": 1700706004397,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]