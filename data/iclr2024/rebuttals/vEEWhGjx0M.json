[
    {
        "title": "Adversarial Attacks on Combinatorial Multi-Armed Bandits"
    },
    {
        "review": {
            "id": "BVIjAIYjdz",
            "forum": "vEEWhGjx0M",
            "replyto": "vEEWhGjx0M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6954/Reviewer_cTNV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6954/Reviewer_cTNV"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers reward poisoning attacks on Combinatorial Multi-armed Bandits (CMAB), and provides a sufficient and necessary condition for the attackability of CMAB. This condition depends on the intrinsic properties of the corresponding CMAB instance such as the reward distributions of super arms and outcome distributions of base arms. The paper further illustrates that the attackability of a specific CMAB instance also depends on whether the bandit instance is known or unknown to the adversary."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper considers the adversarial attacks on combinatorial multi-Armed bandit. The paper introduces a new notion of attackability, which has a stronger requirement than existing conditions. The paper also characterizes a necessary and sufficient condition for this attackability when the underlying CMAB instance in a known environment."
                },
                "weaknesses": {
                    "value": "1. The paper focuses on the new notion of attackability that requires the cost to scale polynomial in $m$. This is not well motivated. The paper only mentioned that \"in practice, the exponential cost in $m$ can exceed $T$, resulting in vacuous results.\" Note that $T$ is growing, and we care about how the regret and the attack costs grow in terms of $T$. On the other hand, $m$ is fixed. So why it is more important to focus on the scaling in terms of $m$ than the scaling in terms of $T$?\n\n2. The paper focuses mostly on the polynomial attackability of a CMAB instance in a known environment, i.e., all parameters of the instance such as the reward distributions of super arms and outcome distributions of base arms are given. This white box setting is of limited interest for practice."
                },
                "questions": {
                    "value": "1. Better justify why one should focus on the scaling of $m$ term.\n2. Can the authors provide the corresponding conditions for the black-box setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6954/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6954/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6954/Reviewer_cTNV"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6954/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698514092761,
            "cdate": 1698514092761,
            "tmdate": 1699636812490,
            "mdate": 1699636812490,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "riI4S0XXEz",
                "forum": "vEEWhGjx0M",
                "replyto": "BVIjAIYjdz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6954/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6954/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cTNV"
                    },
                    "comment": {
                        "value": "Thank you for the reviews and questions. We provide a detailed response to your comments as follows.\n\n**Q.** The paper focuses on the new notion of attackability that requires the cost to scale polynomial in $m$. This is not well motivated. The paper only mentioned that \"in practice, the exponential cost in $m$ can exceed $T$, resulting in vacuous results.\" Note that $T$ is growing, and we care about how the regret and the attack costs grow in terms of $T$. On the other hand, $m$ is fixed. So why it is more important to focus on the scaling in terms of $m$ than the scaling in terms of $T$?\n\n\n**A.** We thank the reviewer for raising this issue. We think the question has two parts: 1. why consider the dependency on $m$, and 2. why not try to improve the dependency on $T$.\n\n*As for the part 1*, we believe it is not only for the attack, but for the whole bandit theory. Note that the regret of the most basic multi-armed bandit also scales with the number of arms $K$, say $\\tilde O(\\sqrt{KT})$ for distribution-independent regret bound, and $\\tilde O(K\\log T)$ for distribution-dependent regret bound. However, people tried hard to improve the dependency on with respect to $K$, since in reality, people cannot have an infinite time horizon $T$. Thus, people considered many structures for the bandit instances, like linear (contextual) bandit [1] and combinatorial semi-bandit [2,3,4], which can have exponential number of arms. Without leveraging the internal structures, the regret bound will be $\\tilde O(\\exp(K) \\sqrt{T})$ for distribution-independent bound and $\\tilde O(\\exp(K)\\log T)$ for distribution-dependent bound, and when $T$ is not that large (i.e., exponential in $K$), the bounds are vacuous. These results highlight the importance of considering the dependency besides $T$.\n\nSimilar to the concern in the regret analysis in bandit, when considering the attack cost, people do not have infinite cost for the attack, and thus, constraining the attack cost to be polynomial dependent on different terms is quite important.\n\n*As for the part 2*, it is a really good question. Note that we aim to get a unified theory to attack any potential bandit algorithm with sublinear regret, and thus the exact attack cost with respect to $T$ will be dependent on the victim algorithm. As shown in Corollary 3.7, if the victim algorithm is CUCB, the attack cost is already $O(\\log T)$. Although it might be possible to even reduce the dependency, we believe that $O(\\log T)$ is already satisfactory and thus it is more important to reduce the dependency on $m$, i.e., reducing it from exponential dependency to polynomial dependency. \n\n\n[1] Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web (pp. 661-670).\n\n[2] Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed bandit: General framework and applications. ICML 2013.\n\n[3] Wei Chen, Yajun Wang, Yang Yuan, and Qinshi Wang. Combinatorial multi-armed bandit and its extension to probabilistically triggered arms. The Journal of Machine Learning Research, 17(1): 1746\u20131778, 2016\n\n[4] Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, and Brian Eriksson. Matroid bandits: Fast combinatorial optimization with learning. arXiv preprint arXiv:1403.5045, 2014.\n\n\n**Q.** The paper focuses mostly on the polynomial attackability of a CMAB instance in a known environment, i.e., all parameters of the instance such as the reward distributions of super arms and outcome distributions of base arms are given. This white box setting is of limited interest for practice.\nCan the authors provide the corresponding conditions for the black-box setting?\n\n**A.** We thank the reviewer for raising the concern. While it is true that  the black-box setting is more interesting, it is also harder to study. As the first study on the attackability of CMAB, we focus on analyzing the white-box setting and show that it is difficult to attack such an environment even when all parameters are known. We believe the characterization is highly non-trivial and significantly different from white-box setting considering the hardness example, where we showed an instance can be polynomially attackable when the the environment is known but becomes polynomially unattackable when the environment is unknown.\n\nOn the other hand, our attack algorithm is already applied to several applications under black-box setting, such as probabilistic max coverage, online minimum spanning tree and cascading bandits where the problems are always attackable. We can directly apply Algorithm 1 even under black-box setting for these applications. We emphasize that our experiments in Figure 2 (a-d), (g, h) are all attacked by Algorithm 1 under black-box setting."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6954/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561180037,
                "cdate": 1700561180037,
                "tmdate": 1700561180037,
                "mdate": 1700561180037,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Z8ynGCxD0l",
            "forum": "vEEWhGjx0M",
            "replyto": "vEEWhGjx0M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6954/Reviewer_grhj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6954/Reviewer_grhj"
            ],
            "content": {
                "summary": {
                    "value": "This work studies adversarial attacks on combinatorial bandits (CMAB)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Adversarial attack has been studied in stochastic bandits, linear bandits, adversarial bandits. The study of adversarial attack on CMAB is new. \nThis work proposes new notions of attackability based on the structure of CMAB."
                },
                "weaknesses": {
                    "value": "While the framework follows from previous work Wang & Chen, I feel the paper could benefit from a discussion on simpler CMAB models first (i.e. without the trigger function etc. )"
                },
                "questions": {
                    "value": "I did not quite follow the problem setup. The formulation follows from the previous work Wang & Chen, which used infinite action space with the trigger function. However, in this submission, in the definition of super arms, it is first mentioned the action space could be infinite. But it also mentions each super arm is a set of base arm, which implies the cardinality of super arms is 2^m. Also, it seems that if we define a super arm as a set of base arms, then there is no need to define the trigger function? \n\nGiven my unfamiliarity with this line of work, it is currently unclear to me exactly how the action set is defined in this current work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6954/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6954/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6954/Reviewer_grhj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6954/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698707248233,
            "cdate": 1698707248233,
            "tmdate": 1699636812209,
            "mdate": 1699636812209,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8qTrWT7NuP",
                "forum": "vEEWhGjx0M",
                "replyto": "Z8ynGCxD0l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6954/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6954/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer grhj"
                    },
                    "comment": {
                        "value": "Thank you for recognizing the novelty of our paper and we appreciate your reviews and questions. We provide a detailed response to your comments as follows.\n\n**Q.** I did not quite follow the problem setup. The formulation follows from the previous work Wang & Chen, which used infinite action space with the trigger function. However, in this submission, in the definition of super arms, it is first mentioned the action space could be infinite. But it also mentions each super arm is a set of base arm, which implies the cardinality of super arms is 2^m. Also, it seems that if we define a super arm as a set of base arms, then there is no need to define the trigger function?\n\n**A.** We thank the reviewer for the concerns raised. Firstly, we would like to apologize for and clarify the confusion here. \n\nWe define the action such that it will trigger a set of base arms (with certain triggering probability function). Because the triggering probability might be complicated, the total number of actions might exceed $2^m$. Here we take the shortest path and cascading bandit as an example for better understanding.\n\nIn shortest path, each action corresponds to a possible path in the graph. There is no triggering function in shortest path and the total number of paths is indeed bounded by $2^m$. As for the cascading bandit, each action corresponds to an ordered list of $K$ items (here $K$ is the max number of items to show). In this case, the total number of actions is computed as $A_m^K = m * (m-1) * \\cdots * (m-K+1)$. This number can be larger than $2^m$.\n\nBesides, the triggering probability is needed even if the number of actions is finite. Let's still use the cascading bandit as an example. Here, the triggering function would define whether the next item in the list is observed, as the user stops examining items after choosing his or her first attractive item. Thus even if the action is exactly the same, say we list item 1,2,3 in order and let the users to choose. Sometimes we only know that item 1 is chosen while not observing any reward for item 2 and 3 (thus only observe 1 item), while sometimes we know that 1 and 2 are not chosen by some user and the user chooses item 3 (thus observe 3 items).\n\n**Q.** Given my unfamiliarity with this line of work, it is currently unclear to me exactly how the action set is defined in this current work.\n\n**A.** Thanks for your question, please refer to our answer to your first question."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6954/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560913561,
                "cdate": 1700560913561,
                "tmdate": 1700560913561,
                "mdate": 1700560913561,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yTtkOfMkkq",
            "forum": "vEEWhGjx0M",
            "replyto": "vEEWhGjx0M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6954/Reviewer_UBzx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6954/Reviewer_UBzx"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores adversarial attacks on Combinatorial Multi-Armed Bandits (CMAB). It discusses a sufficient and necessary condition for the polynomial attackability of CMAB and presents an attack algorithm for attackable instances. The authors also investigate how the attackability of a CMAB instance is influenced by whether the bandit instance is known or unknown to the adversary, which indicates that adversarial attacks on CMAB are difficult in practice and a general attack strategy for any CMAB instance does not exist. The findings are validated through experiments on real-world CMAB applications."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality: The paper introduces a novel concept of polynomial attackability in the context of combinatorial multi-armed bandits (CMAB). This notion captures the vulnerability and robustness of CMAB systems, which is a unique contribution to the field.\n\nQuality: The paper provides a rigorous analysis of the attackability of CMAB systems and presents a sufficient and necessary condition for polynomial attackability. The paper also presents an attack algorithm for attackable instances. The paper validate the theoretical findings and demonstrate the effectiveness of the proposed attack via extensive experiments conducted on various CMAB applications.\n\nClarity: The paper is well-written and presents the concepts, definitions, and analysis in a clear and concise manner. The experimental setup and results are explained in detail, and the source code is provided, making it easy for readers to understand and replicate the experiments.\n\nSignificance: The paper addresses an important research question regarding the vulnerability of CMAB systems to adversarial attacks. By introducing the concept of polynomial attackability and providing a comprehensive analysis, the paper contributes to the understanding of the security and robustness of CMAB algorithms. The finding that the attackability of a specific CMAB instance also depends on whether the bandit instance is known or unknown to the adversary is impressive and may have practical implications for designing more secure CMAB systems in real-world applications."
                },
                "weaknesses": {
                    "value": "1. The limitations of the findings are less discussed.\n\nThe findings regard the polynomial attackability highly depends on the threat model, in which the outcome of the base arms can be modified by the adversary. However, recent researchers discussed different types of adversarial attacks on bandit and RL [1-5], including also environment poisoning attack and action poisoning attack. In the CMAB system, the environment-manipulation adversary could manipulate the reward function $r$ and the action-manipulation adversary could manipulate the super arm $S$.\n\nThe proposed sufficient and necessary condition of the polynomial attackability is limited to the specific reward-manipulation adversary. If one environment-manipulation adversary can manipulate the reward function, $\\Delta_M$ could be changed and the condition of the polynomial attackability does not work. Example 4.2 (Hard example) is also limited to the specific reward-manipulation adversary. Some statement in the abstract and introduction is inaccurate. The limitations of the findings are less discussed.\n\nA more thorough discussion about the scope and limitation of the findings would be helpful.\n\n2. Some problems in experimental results.\n\nI found that the numerical experiments do not reflect the effect of the proposed attack algorithm. For example, in (2(g), 2(h)), the number of the target arm pulls is at most 4e3 after 1e5 iterations. The target arm is pulled in only 4% rounds.\n\nIn addition, the experiment that reflects the polynomial unattackablity would be helpful. I recommend that the author can run some experiments on the hard example. The adversary can attack the hard example instance using heuristics with sublinear attack budget.\n\n[1] Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. Policy teaching via environment\npoisoning: Training-time adversarial attacks against reinforcement learning. In International Conference on\nMachine Learning, pages 7974\u20137984, 2020.\n \n[2] Guanlin Liu and Lifeng Lai, Action-Manipulation Attacks Against Stochastic Bandits: Attacks and Defense, in IEEE Transactions on Signal Processing, vol. 68, pp. 5152-5165, 2020\n\n[3] Yanchao Sun, Da Huo, and Furong Huang. Vulnerability-aware poisoning mechanism for online rl with unknown\ndynamics. In International Conference on Learning Representations, 2021.\n \n[4] Hang Xu, Rundong Wang, Lev Raizman, and Zinovi Rabinovich. Transferable environment poisoning: Training-time attack on reinforcement learning. In Proceedings of the 20th international conference on autonomous agents and multiagent systems, pages 1398\u20131406, 2021.\n \n[5] Anshuka Rangi, Haifeng Xu, Long Tran-Thanh, and Massimo Franceschetti. Understanding the limits of\npoisoning attacks in episodic reinforcement learning. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 3394\u20133400, 2022."
                },
                "questions": {
                    "value": "Overall, I like this paper but some statement in the abstract and introduction is inaccurate and the limitations of the findings are less discussed. Could the author provide more discussion about the scope and limitation of the findings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6954/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6954/Reviewer_UBzx",
                        "ICLR.cc/2024/Conference/Submission6954/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6954/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836908173,
            "cdate": 1698836908173,
            "tmdate": 1700583013337,
            "mdate": 1700583013337,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0v3Ni6AAeY",
                "forum": "vEEWhGjx0M",
                "replyto": "yTtkOfMkkq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6954/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6954/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UBzx"
                    },
                    "comment": {
                        "value": "Thank you for your detailed comments. Your recognition of the novelty and significance of our work is highly appreciated. We provide a detailed response to your comments as follows.\n\n**Q.** The limitations of the findings are less discussed.\nThe findings regard the polynomial attackability highly depends on the threat model, in which the outcome of the base arms can be modified by the adversary. However, recent researchers discussed different types of adversarial attacks on bandit and RL [1-5], including also environment poisoning attack and action poisoning attack. In the CMAB system, the environment-manipulation adversary could manipulate the reward function $r$ and the action-manipulation adversary could manipulate the super arm $\\mathcal{S}_i$.\n\nThe proposed sufficient and necessary condition of the polynomial attackability is limited to the specific reward-manipulation adversary. If one environment-manipulation adversary can manipulate the reward function, $\\Delta_\\mathcal{M}$ could be changed and the condition of the polynomial attackability does not work. Example 4.2 (Hard example) is also limited to the specific reward-manipulation adversary. Some statement in the abstract and introduction is inaccurate. The limitations of the findings are less discussed.\n\nA more thorough discussion about the scope and limitation of the findings would be helpful.\n\n**A.** We appreciate your suggestions on discussing other threat models such as environment poisoning attacks and action poisoning attacks. In the revised version, we have discussed other threat models in related works. We have clearly stated in abstract and introduction that the threat model in our paper is reward poisoning attacks. We would like to emphasize that reward poisoning attack is arguably one of the most popular threat models in adversarial attacks against bandits and RL. As the first paper studying attackability of combinatorial bandits, we focused on reward poisoning and left other threat models as future work. We also added discussion about the limitation of the work in Appendix D, where we offered insights on generalizing attackability condition to other threat models. Please see our revision for details.\n\n\n**Q.** Some problems in experimental results. I found that the numerical experiments do not reflect the effect of the proposed attack algorithm. For example, in (2(g), 2(h)), the number of the target arm pulls is at most 4e3 after 1e5 iterations. The target arm is pulled in only 4% rounds.\n\n**A.** Thank you for your detailed comments on experiments. We have performed additional experiments on cascading bandits and reported the results in Appendix A.2. We compared three settings with different number of items $K$ and items to rank $m$, including (1) $m = 25, K = 5000$, which is the original setting in Figure 2(g), 2(h), (2)$m = 35, K = 1000$, and (3) $m = 5, K = 1000$. We observe that with a smaller action space, the target arm is pulled around 50% rounds in 10K iterations for CascadeKLUCB with $m = 35, K = 1000$ in Figure 4(d), and the number increased to 90% rounds $m = 5, K = 1000$ in Figure 4(f). This trend suggested that with longer iterations the CMAB algorithm will pull target arm more often. The results also validated our theoretical analysis.\n\n**Q.** In addition, the experiment that reflects the polynomial unattackablity would be helpful. I recommend that the author can run some experiments on the hard example. The adversary can attack the hard example instance using heuristics with sublinear attack budget.\n\n**A.** We thank the reviewer for raising the concern, and we point the reader towards Figure 2(e) and 2(f). There we showed that in an unattackable case of online shortest path, while the adversary keeps attacking the CMAB algorithm and spends *linear* budget, the target arm is still not pulled."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6954/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560772775,
                "cdate": 1700560772775,
                "tmdate": 1700560772775,
                "mdate": 1700560772775,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YT0vYMV8eE",
                "forum": "vEEWhGjx0M",
                "replyto": "0v3Ni6AAeY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6954/Reviewer_UBzx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6954/Reviewer_UBzx"
                ],
                "content": {
                    "comment": {
                        "value": "My major concerns are addressed and thus I increased my score to reflect that. I recommend that the authors put the experimental results of Appendix A.2 into the main page. Some subfigures in Figure 2 cannot correctly illustrate the efficacy of the proposed algorithm."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6954/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582997505,
                "cdate": 1700582997505,
                "tmdate": 1700582997505,
                "mdate": 1700582997505,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2xs3jzmm2I",
            "forum": "vEEWhGjx0M",
            "replyto": "vEEWhGjx0M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6954/Reviewer_eFM9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6954/Reviewer_eFM9"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into adversarial attacks targeting combinatorial multi-armed bandits (CMAB). The authors introduce the concept of polynomial attackability in CMAB, wherein an attack is deemed successful if its cost remains sublinear with respect to the time horizon and polynomial, rather than exponential, in relation to the number of base arms. They provide both sufficient and necessary conditions for such polynomial attackability within CMAB and introduce an efficient attack algorithm. The discourse further extends to the challenges of polynomial attackability for CMAB instances in unknown environments, emphasizing the absence of a universal attack approach that guarantees success with polynomial costs under such unknown CMAB instances. Empirical results across diverse CMAB scenarios validate their theoretical findings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) This paper is the first to study adversarial attacks against CMAB algorithms, which is an interesting and timely topic.\n2) The novel characterization of the sufficient and necessary conditions for polynomial attackability in CMAB provides insight into the distinct challenges posed by CMAB instances with polynomial costs.\n3) The author presents a hard example highlighting that an instance can be polynomially attackable when the adversary is aware of the environment but becomes polynomially unattackable when the environment is unknown. This underscores the difficulty of launching general adversarial attacks on unknown CMAB instances."
                },
                "weaknesses": {
                    "value": "1) Algorithm 1 seems to be straightforward but may lead to large attack costs when $\\Delta$ is small. The attack cost's dependency on $\\Delta$ from previous works [Jun et al., 2018, Liu and Shroff, 2019] is usually linear in $\\sum_{i}  \\Delta_i$, while the dependency in this paper is $1 / \\Delta_{S^*}$, which is worse than $\\sum_{i}  \\Delta_i$ as $\\Delta_i \\le 1$. This is due to the lack of fine-grained attack value design.\n\n2) While Theorem 4.1 establishes the difficulty of successfully targeting general unknown CMAB instances, there remains a potential to execute attacks in particular CMAB settings, such as PMC with **unknown** base arms. This is important since learning from the attacker side is one of the main challenges of attack design in previous works: simple oracle attacks [Jun et al., 2018, Liu and Shroff, 2019] can easily attack known K-armed bandit instances. I would expect more algorithm design and analysis for the unknown environment."
                },
                "questions": {
                    "value": "1) In Corollary 4.3, there is no guarantee that the randomly picked super arm $\\mathcal{S}$ satisfied $\\Delta_{\\mathcal{S}} > 0$ (even for cascading bandits, online MST, and online PMC problem with greedy oracle, there exists $\\mathcal{S}$ such that $\\Delta_{\\mathcal{S}} = 0$)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6954/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838767913,
            "cdate": 1698838767913,
            "tmdate": 1699636811986,
            "mdate": 1699636811986,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dEvGd1NHs1",
                "forum": "vEEWhGjx0M",
                "replyto": "2xs3jzmm2I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6954/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6954/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eFM9 (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review and positive feedback. We appreciate your recognition of the importance of the study, the novelty of our characterization and the hardness example. We provide a detailed response to your comments as follows.\n\n**Q.** Algorithm 1 seems to be straightforward but may lead to large attack costs when $\\Delta$ is small. The attack cost's dependency on $\\Delta$ from previous works [Jun et al., 2018, Liu and Shroff, 2019] is usually linear in $\\sum_i \\Delta_i$, while the dependency in this paper is $1/\\Delta_s$, which is worse as $\\Delta_i \\leq 1$. This is due to the lack of fine-grained attack value design.\n\n**A.**  Thanks for this insightful question. We would like to emphasize that our attack cost does not fully come from the lack of fine-grained attack value design, but instead, it mainly comes from different settings for consideration and different definitions of the $\\Delta$.\n\n+ In [Jun et al,. 2018, Liu and Shroff, 2019], any real number reward is considered feasible, and the authors assume the reward distributions are $\\sigma$-subgaussian. In our setting, each base arms can only take value from $[0,1]$. This different assumption on the environment makes the attacks very different: In [Jun et al,. 2018, Liu and Shroff, 2019], it is possible to give any reward value back, while in our setting, the lowerest reward we can give is 0.\n+ The definition on $\\Delta$ is different in [Jun et al,. 2018, Liu and Shroff, 2019] and in our paper. Let's take the bandit environment with two arms as an example, and the expected reward of two arms are $\\mu_1, \\mu_2$ ($\\mu_1 > \\mu_2$). Here $\\mu_2$ is the target arm. In [Jun et al,. 2018, Liu and Shroff, 2019], $\\Delta_i$ is defined as the mean reward difference between arm $i$ and the target arm (which is the standard definition in bandit literature), and in the example, it will be $\\mu_1 - \\mu_2$; in our setting, $\\Delta$ is defined as the gap between the reward of the target arm and the best of other arms **after changing the reward to 0**. In the example, $\\mu_1$ is attacked to $0$, and the gap $\\Delta = \\mu_2$.\n\nWe will then explain why these two differences lead to different attack costs in [Jun et al,. 2018, Liu and Shroff, 2019] and our paper. First, we want to mention that the attack cost in [Jun et al,. 2018, Liu and Shroff, 2019] is not exactly $\\tilde O(\\sum\\Delta_i)$, instead, it is $\\tilde O(\\sum\\Delta_i + 1)$ (Corollary 1, 2 in [Jun et al,. 2018]). This $\\tilde O(1)$ is important for small $\\Delta$. Now let's consider the case where our algorithm leads to large attack cost in our setting: also consider two arm bandit instance, where there are two arms, $\\mu_1 = 2\\epsilon, \\mu_2 = \\epsilon$, $\\epsilon>0$ but is very small, and arm 2 is the target arm. In [Jun et al,. 2018], the attacker can assign reward $-1$ for the first arm, while in our setting, the worst possible reward we can assign to arm 1 will be $0$. In this case, you need to at least pay $\\tilde O(1/\\epsilon)$ attack cost to let the algorithm differentiate arm 1 and 2, since it is best to assign arm 1 with reward 0 and in this case the algorithm (UCB) will observe arm 1 (the non-target arm) for at least $1/\\epsilon^2$ times with each time paying $\\epsilon$ attack cost.\n\nAlso, to make a fair comparison with the setting in [Jun et al,. 2018], we should look at the case where $\\mu_1 = 2\\epsilon+1/2, \\mu_2 = \\epsilon+1/2$. For this instance, our attack can also assign a relatively small reward to the non-target arm, since in our setting, $\\Delta=1/2+\\epsilon$. Thus, our attack strategy will also have cost $\\tilde O(1)$.\n\nThus in conclusion, we believe that the different form of attack cost does not come from the lack of attack value design. Instead, it mainly comes from the different abilities of the attacker (if the attacker can assign any reward value) and different definitions of the notation. We believe that in [Jun et al,. 2018, Liu and Shroff, 2019], the fine-grind attck value design is crucial in their setting since the hard case is to have big gap, while in our setting, the fine-grind attack value design may not be that important, since we have bounded reward assumption in each base arm and thus the reward will not be that large, and the infeasibility result (the accurate characterization of polynomially attackable/unattackable) is a more valuable contribution compared with the exact cost."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6954/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560491744,
                "cdate": 1700560491744,
                "tmdate": 1700560491744,
                "mdate": 1700560491744,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]