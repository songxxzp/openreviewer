[
    {
        "title": "LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts"
    },
    {
        "review": {
            "id": "Lp4QnFFDqI",
            "forum": "mNYF0IHbRy",
            "replyto": "mNYF0IHbRy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4903/Reviewer_kt1C"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4903/Reviewer_kt1C"
            ],
            "content": {
                "summary": {
                    "value": "This proposed a framework for generating images from complex and detailed prompts, which took more work for previous models or frameworks. They utilize LLM's capability to generate augmented textual and visual prompts for better generations. And the framework comprises global scene generation and an iterative refinement scheme to align with conditional cues."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This work steps toward longer textual descriptions for image generations to ensure the fidelity of complex textual prompts. \n\n- Scene blueprints with the iterative refinement step ensure high prompt adherence recall, quantitatively validating its effectiveness."
                },
                "weaknesses": {
                    "value": "- Missing information on human study. Detailed information on human study could be provided to assess the reliability of the outcome. How do you recruit and select them based on what qualification? Isn't there any conflict of interest for the subjects and authors? How many subjects are recruited? What was the confidence of the votes? This issue is the major reason for leaning toward rejection. I am eager to see the author's feedback for reassessment.\n\n- This framework relies on recently proposed models like a strong LLM, CLIP, and an image composition model. The collection of previous works provides shallow techniques compared with them.\n\n- A missing related work. DenseDiffusion [1] may be worth being included in the layout-to-image generation subsection in Sec. 2 and the box-level multi-modal guidance in Sec. 3.4. DenseDiffusion tried to manipulate attentional weights to control the regions for layout guidance selectively. It would be appreciated if you could compare your method with it for readers in the upcoming revised manuscript. Note that, due to the narrow accessibility to this work at the time of submission, this is not considered in the score evaluation.\n\n[1] Kim, Y. et al. (2023). Dense Text-to-Image Generation with Attention Modulation. http://arxiv.org/abs/2308.12964"
                },
                "questions": {
                    "value": "- Minors:\n  - In Sec. 3.1, Models. -> Models (Please exclude the period in the section title.)\n  - In Fig. 2, it would be inappropriate to include the logo of a commercial product (ChatGPT from OpenAI) in an academic paper. And the company may not allow the usage of their logo to promote the work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I leave the comment that \"In Fig. 2, it would be inappropriate to include the logo of a commercial product (ChatGPT from OpenAI) in an academic paper. And the company may not allow the usage of their logo to promote the work.\" for the possible legal issue."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4903/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698389254900,
            "cdate": 1698389254900,
            "tmdate": 1699636475101,
            "mdate": 1699636475101,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f9Lg184BSq",
                "forum": "mNYF0IHbRy",
                "replyto": "Lp4QnFFDqI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4903/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4903/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kt1C"
                    },
                    "comment": {
                        "value": "We thank reviewer for providing insightful comments. Please find our responses addressing the specific queries below.\n\n**1. Missing information on human study.**\n\nOur approach is designed to overcome the limitations observed in diffusion models when confronted with lengthy and intricate textual prompts that involve multiple objects. Given the absence of an efficient metric tailored for this specific challenge, we conducted a thorough human study to offer a more comprehensive evaluation, as outlined in the main paper. The subjects for human study were recruited from a pool of academic students with backgrounds in computer science and engineering.\nTo ensure an unbiased evaluation and maintain objectivity, the subjects participating in the evaluation remained anonymous with no conflicts of interest with the authors.  Furthermore, we prioritized user confidentiality by refraining from collecting personal details such as names, addresses etc.\n\nThe entire survey process was anonymized, ensuring that there was no leakage of data to any user. Following a 2-AFC (two-alternative forced choice) design, each participant was tasked with selecting one image out of a pair of images, which best aligns with the given textual prompt and faithfully captures all the objects. Each pair of images was randomly sampled from a pool containing images from both baselines as well as our approach. This system mitigates biases which can originate from user-curated surveys. Further, the pool of images was shuffled before sampling each time. The system was further equipped with the feature of randomly flipping the position of the images, to remove the user-specific position bias in case any pair of images is repeated. Additionally, a 2-second delay between questions was introduced to facilitate more thoughtful decision-making.\n\nThe system yielded approximately 350 responses from 50 subjects, with each user answering an average of 7 questions. The results, as presented in Fig. 4 of the main paper, indicate that users predominantly selected images generated from our approach. This observation is further supported by qualitative analyses (Fig. 5 of the main paper and Figs. 9 and 10 of the appendix) and quantitative comparisons in Section 4 of the main paper and Table 1 of the revised appendix. We believe that this comprehensive approach and the subsequent results contribute significantly to the robustness and reliability of our outcomes. \n\n\n**2. This framework relies on recently proposed models like a strong LLM, CLIP, and an image composition model. The collection of previous works provides shallow techniques compared with them.**\n\nAs per our understanding of the question, our methodology leverages the complementary strengths of diverse existing approaches, enhancing and refining each component in the process. Furthermore, our approach incorporates specific modifications to optimize the performance of each individual element. To prevent the Large Language Model (LLM) from generating results that lack coherence, we implement an interpolation mechanism. Additionally, we address potential issues in the output of the first-stage model through an iterative refinement scheme. In the context of the composition model, we introduce multi-modal guidance. The the impact of these components is visually demonstrated in Figures 6 and 7 of the main paper. Thus, our proposed layout generation, iterative refinement and multi-modal guidance solves for the weaknesses of the existing components and enhances the overall generalization towards complex textual prompts. \n\n\n**3. Comparison with DenseDiffusion**\nPlease refer to Fig. 10 of revised appendix for qualitative comparisons with DenseDiffusion [1]. Additionally, refer to the Table 4 below  for quantitative comparisons. Our method compares favorably well against DenseDiffusion both qualitatively and quantitatively (in terms of PAR score).\n\nTable4. Quantitative comparison with DenseDiffusion in terms of proposed PAR score.\n\n| Method|PAR score |\n|-----|-----|\n| DenseDiffusion | 52\\% |\n| Ours  | 85\\%  | \n\n\n**4. Use of chatGPT logo.**\n\nWe understand the reviewer's concerns. Thank you for pointing it out. We have removed the ChaTGPT logo in the new revised version of the paper. We have further removed the period from Sec. 3.1 title.\n\n[1] Kim, Y. et al. (2023). Dense Text-to-Image Generation with Attention Modulation. http://arxiv.org/abs/2308.12964"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734834428,
                "cdate": 1700734834428,
                "tmdate": 1700740068218,
                "mdate": 1700740068218,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Fsv6fsKfEV",
            "forum": "mNYF0IHbRy",
            "replyto": "mNYF0IHbRy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4903/Reviewer_Br9r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4903/Reviewer_Br9r"
            ],
            "content": {
                "summary": {
                    "value": "This work aims to solve the problem that prior text-to-image models cannot accurately follow the object specifications in lengthy prompts. The work proposes a novel two-step pipeline that first uses a scene blueprint, which is an LLM-generated layout with object descriptions, to generate the overall image. A CLIP-based guidance is then applied to perform iterative refinement in order to make sure the content of each box is correct. The method enables accurate and diverse image generation with intricate and lengthy input text prompts. The user study and qualitative comparison indicate a non-trivial improvement over baseline methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. In addition to previous methods using the layouts for initial image generation, the method further proposes box-based refinement to improve the ability to generate all the objects mentioned in a lengthy and intricate prompt.\n2. The method points out the fact that there currently lacks a pipeline for benchmarking text-to-image methods with lengthy prompts and proposes a metric called prompt adherence recall (PAR) to evaluate their method and several baselines.\n3. Their method has better prompt adherence compared to baselines in prompt generation. The user study also confirms that the method can faithfully generate objects in the prompt."
                },
                "weaknesses": {
                    "value": "1. This work builds on LLM-grounded Diffusion, as mentioned in Sec 3.3. However, the difference between LLM-grounded Diffusion and the current work is not clearly explained. This work lists \"scene blueprints using LLMs\" as one of the contributions, but both LLM-grounded Diffusion and LayoutGPT (with both works cited in related work section) propose using LLM to generate the scene layouts with object descriptions. The authors show that the scene representation enhanced by the proposed method works better for long text prompts, but this claim of \"proposing a structured scene representation\" is not a contribution as it has been proposed and used in previous works.\n2. The interpolation of k generated layouts could potentially cause degenerate or undesired results. For example, for \"a cat and a dog, one on each side\", the LLM can generate 1) a cat on the left, a dog on the right, or alternatively 2) reverse the position for the cat and the dog. The interpolation of layout 1) and 2) results in two objects placed both at the center.\n3. Missing details: In Sec 3.3, the authors mention \"image-to-image translation\" with latent/stable diffusion that leads to generation with higher quality, but no details such as the prompts or the exact approach are given. Is this simply adding some noise and denoise?\n4. The evaluation of the work does not investigate whether the proposed method degrades the fidelity of the image. The user study only involves asking whether objects are present and layouts are accurately generated rather than comparing the overall image quality. The author needs to present evaluation results that show their method does not have significant degradations on quality. If only the presence of objects is considered, a simple \"copy-and-paste\" from baseline text-to-image model with individual object generation using the generated layout would also result in high object presence without bringing much utility to the research community and potential applications.\n\nA typo that does not affect the rating:\n1. api -> API (page 7)"
                },
                "questions": {
                    "value": "The authors are encouraged to respond to and address the weaknesses in the section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4903/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4903/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4903/Reviewer_Br9r"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4903/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698535407259,
            "cdate": 1698535407259,
            "tmdate": 1699636475011,
            "mdate": 1699636475011,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7wgAdWZPxt",
                "forum": "mNYF0IHbRy",
                "replyto": "Fsv6fsKfEV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4903/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4903/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Br9r (1/2)"
                    },
                    "comment": {
                        "value": "We thank reviewer for providing insightful comments. Please find our responses addressing the specific queries below.\n\n**1. Potential differences between our proposed scene blueprint with LLM Grounded Diffusion and LayoutGPT.**\n\nOur idea to generate a scene blueprint presents a mechanism to interact with the complex prompt. While the current method LLM grounded Diffusion provides a way to to interact with short prompts, our work, on the other hand extends the strength of LLM to interact and generate meaningful information from long complex prompt in the form a blueprint.  LayoutGPT, on the other hand, is a concurrent work and also works on the small prompts. We show the superiority of our approach compared to LayoutGPT visually in Fig. 5 of the main paper and Fig. 9 of appendix, and quantitatively in Sec. 4 of the main paper and Table 1 of revised appendix. In addition to this, the layouts generated by LLM grounded Diffusion and LayoutGPT only extract the specific objects with prominent features  (e.g. a white cat)  from the textual prompt. In contrast, our method extracts the objects along with their detailed description from the prompt (e.g. a white cat, gracefully stretching, showing off its fluffy, pristine fur) as shown in Fig. 1 of the main paper. The object descriptions play a key role in guiding our final object generation (please see Fig. 7 in the main paper). We provide the details of an additional novel prompt to extract object descriptions from complex textual prompts in Section A.4 of the appendix. Hence, our method has notable differences from these works, while having better generalization.\n\n\n**2. Interpolation results for the case \"a cat and a dog, one on each side\".**\n\nWe thank the reviewer for pointing out such a scenario which could pose a challenge in generating a coherent image as per the prompt. To further investigate this issue, we conducted an in-depth analysis using ChatGPT in Table 3 below. Specifically, we tasked ChatGPT with generating bounding boxes for both cats and dogs in 30 instances each, employing three distinct prompts: \"A living room with a cat and a dog sitting one on each side,\" \"A living room with a cat sitting on the right and a dog sitting on the left,\" and \"A living room with a dog sitting on the right and a cat sitting on the left.\"\n\nOur analysis reveals that on an average 60\\% of the times for the ambiguous prompt \"A living room with a cat sitting towards right and a dog sitting towards left\", ChatGPT generates bounding box on the right for the cat and on the left for the dog. While for the other two prompts where the locations of objects are well-defined, ChatGPT generates the correct location of bounding boxes for the cat and dog. This shows that ChatGPT works exceptionally well for unambiguous prompts with clearly defined spatial relationships, a finding further verified by [1]. For the ambiguous prompt, we notice an inherent bias inside ChatGPT. To account for the errors and to provide a meaningful fix to the inherent bias inside ChatGPT, we first cluster the boxes according to the spatial density and then interpolate boxes from the cluster containing the majority number of boxes. We additionally provide a hyperparameter $\\eta$ in the interpolation, which can be controlled to adjust for the position of the bounding box of each object (please see Fig. 3 in the main paper for a visual illustration of the effect of $\\eta$ parameter).\n\nTable 3. A study of the effectiveness of ChatGPT on inferring spatial relationships\n\n| Prompt|Object|right|left|arbitrary position\n|----------------------------------------|-----|---|---|---|\n| A living room with ***a cat and a dog sitting one on each side*** | dog  | 0.3 | 0.6| 0.1 |\n| | cat  | 0.7 | 0.2 | 0.1 |\n| A living room with ***a cat sitting towards right and a dog sitting towards left*** | dog | 0 | 1 |  0 |\n| | cat  | 1 | 0 | 0 |\n| A living room with ***a dog sitting towards right and a cat sitting towards left*** | dog  | 1 | 0 | 0 |\n| | cat  | 0 | 1 | 0\n\n\n**3. Details about Image-to-Image diffusion process.**\n\nTo further enhance the quality of the generated image, we introduce an image-to-image diffusion process after stage I generation. Specifically, we utilize the image-to-image translation method of stable diffusion which instead of starting from random noise, starts from an input image, adds noise to it  and then denoises it in the reverse process. The idea is to enhance the quality of the image while maintaining its semantics. We notice that this process removes the unwanted noise and artifacts present in the image (please refer to Fig 8 in the appendix). We have included this explanation in the Sec. A.2 of revised appendix. \n\n\n[1] Lian, L., Li, B., Yala, A., & Darrell, T. (2023). LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models. arXiv preprint arXiv:2305.13655."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734332679,
                "cdate": 1700734332679,
                "tmdate": 1700740053341,
                "mdate": 1700740053341,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CiOJLmPhWz",
            "forum": "mNYF0IHbRy",
            "replyto": "mNYF0IHbRy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4903/Reviewer_c4Qg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4903/Reviewer_c4Qg"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel approach for improving text-to-image generation in diffusion-based models when processing complex scenes with multiple objects and intricate text prompts. The authors leverage Large Language Models (LLMs) to extract the layout information, detailed text descriptions, and background information from text prompts. Then the proposed layout-to-image generation model is composed of two stages: Global Scene Generation and Iterative Refinement Scheme. The Global Scene Generation phase uses object layouts and background context to create an initial scene which roughly represents the target image layout but not very accurate. Then the Iterative Refinement Scheme iteratively evaluates and refines box-level content to align them with their textual descriptions and recompose objects to ensure consistency. Extensive experiments are conducted to validate the effectiveness of the proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed approach can handle the complex scenarios of text-to-image generation with long text prompts very well.\n2. The paper is well-written and easy to follow.\n3. The iterative refinement loop provides a possible solution to generating images of complex scenes."
                },
                "weaknesses": {
                    "value": "1. I think the major limitation of the proposed approach is efficiency. The complexity of the proposed approach increases as the number of objects increases for complex scenes. This might also be the major issue presenting this approach to be applied in real usage.\n2. It seems that although the generation of objects can be iteratively refined, the bounding box locations cannot be refined. If the LLM predicts unreasonable bounding box layouts at the first stage, it cannot be corrected. Have the authors think of introducing the refinement of bounding box locations into the pipeline?\n3. How many layouts are needed to interpolate? How does the number of layouts for interpolation affect the results?"
                },
                "questions": {
                    "value": "Please refer to the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4903/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699284577085,
            "cdate": 1699284577085,
            "tmdate": 1699636474897,
            "mdate": 1699636474897,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Jgzu6K2GHp",
                "forum": "mNYF0IHbRy",
                "replyto": "CiOJLmPhWz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4903/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4903/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer c4Qg"
                    },
                    "comment": {
                        "value": "We thank reviewer for providing insightful comments. Please find our responses addressing the specific queries below.\n\n**1. On efficiency of the proposed approach.**\n\nOur approach operates as a two-stage framework, relying significantly on the efficacy of the underlying models it employs. While some existing state-of-the-art methods exhibit faster performance, they encounter challenges in faithfully capturing every detail of lengthy and intricate textual prompts. Notably, these methods often fall short in generating all the objects specified in the prompt, as illustrated qualitatively in (Figs. 1 and 5) of main paper and (Figs. 9 and 10) of appendix. Our approach demonstrates a high abjectness score expressed as Prompt Adherence Recall (PAR) rate, indicating a notable alignment between the number of objects mentioned in the prompt and those actually present in the generated image (please see Sec. 4 in the main paper). Therefore, a discernible trade-off emerges between addressing complexity and ensuring the faithful reproduction of images. We provide a comparison of our method with existing approaches in terms of time efficiency and PAR score below.\n\nTable 2. Quantitative comparison of our approach with baselines in terms of PAR score and inference times. Kindly note that all inference times are measured on a single Nvidia A100 GPU.\n\n| Method|PAR score |Inference Time (min) |\n|-----|-----|---|\n| Stable diffusion | 49\\%  | 0.18 |\n| GLLIGEN |  57\\% | 0.5 |\n| LayoutGPT | 69\\%   | 0.83 |\n| DeepFloyd |  60\\% | 8.33 |\n| DenseDiffusion | 52\\%  | 2.5|\n| Ours  | 85\\%  | 3.16  |\n\nRecent works, such as those introduced by [1] and [2], have presented methods capable of scaling up diffusion inferences. Looking ahead, we anticipate incorporating these advancements in speed-boosting techniques into our ongoing research as a future work. \n\n\n**2. Unlike objects, bounding box locations cannot be refined. If the LLM predicts unreasonable bounding box layouts at the first stage, it cannot be corrected. Have the authors think of introducing the refinement of bounding box locations into the pipeline?**\n\nOur method is based on two stage process of layout generation and bounding box refinement. The spatial faithfulness of final generated image depends on the quality of layouts generated in the first stage.  We observe that ChatGPT is effective in generating faithful layouts  (please refer to Sec. A.7 of appendix) which exactly follow the details from the textual prompt. [3] further verifies this finding. However, in extreme cases, such as in Fig. 6 of the main paper and Fig. 12 of the revised appendix where object positions are not explicitly defined in the prompt, ChatGPT produces bounding boxes that do not coherently align with the prompts. We tackle such cases by generating multiple layouts, selecting the relevant ones via clustering and then interpolating them to a single optimal layout to lessen the influence of any outlier bounding boxes. We further provide a user-specific hyperparameter $\\eta$ (refer to Fig. 3 of main paper) which can be controlled to vary the location of bounding boxes. Refinement of bounding boxes along with object refinement is an interesting future research idea, but is likely to further scale up the compute cost which may not be ideal for user experience. \n\n**3. How many layouts are needed to interpolate? How does the number of layouts for interpolation affect the results?**\n\nWhile there is no restriction on the number of layouts needed for interpolation, our final results are generated with the interpolation of 3 layouts. Please refer to Figures 11 and 12 (Sec A.6) of the revised appendix for a visualization of generated images with different number of layouts. We conclude that while interpolation plays a significant role on the final generated image in extreme cases where spatial locations are not clear from the prompt, it does not have a drastic impact on the final generated image when object locations are well-defined.  \n\n\n[1] Sylvain Gugger et al. \"Accelerate: Training and inference at scale made simple, efficient and adaptable\". https://github.com/huggingface/accelerate\n\n[2] Zhang, Kexun, et al. \"ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval.\" ICML 2023.\n\n[3]  Lian, L., Li, B., Yala, A., & Darrell, T. (2023). LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models. arXiv preprint arXiv:2305.13655."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733448868,
                "cdate": 1700733448868,
                "tmdate": 1700740030870,
                "mdate": 1700740030870,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GSQu5jx5YE",
            "forum": "mNYF0IHbRy",
            "replyto": "mNYF0IHbRy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4903/Reviewer_XXwY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4903/Reviewer_XXwY"
            ],
            "content": {
                "summary": {
                    "value": "The paper extends recent works which leverages layouts to generate scenes corresponding to complex text-prompts. This work first shows that for complex prompts, existing layout to image generation methods have certain failure modes and proposes some practical modifications which are augmented with existing layout to scene generation methods.  First, the authors propose a scene blueprint to represent complex text-prompts; Secondly the authors design an iterative refinement process which improves the alignment of generated images with the complex text-prompts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The research question is a very practical problem \u2014 usually most of the text-to-image generation models are not good at coherent images corresponding to complex prompts, so providing a solution for it is important.\n- The method consists of various components (a lot of these components are existing though) and is conceptually intuitive!\n- The framework obtains strong results on human-study for fidelity of images generated for long prompts."
                },
                "weaknesses": {
                    "value": "Cons / Questions\n\n- While the writing is satisfactory, it can still be improved! The authors should provide more information in the paper on how Eq. (5) is used to guide the sampling process.\n- Can the authors provide more intuition on the interpolation step? \n- Given that there are stronger open-source diffusion models (e.g., DeepFloyd) \u2014 the authors should provide some context on how long prompts work in those cases, as they use a stronger text-encoder like T5. \n- While the authors comment that the size of the tokens (77 in CLIP) is one of the potential reasons on why SD cannot generate compositional prompts \u2014 I believe this is only partially true. Even for non-complex compositional prompts, SD is not able to generate coherent images. Can the authors comment in general on some potential reasons why these text-to-image models are not able to generate images corresponding to simple compositional or complex prompts? I think both are related somehow and it will be beneficial to provide some context regarding it."
                },
                "questions": {
                    "value": "See Cons/Questions;\nOverall, the paper is practical, but the various components though intuitive are not technically novel. While I do agree that not everything in a paper needs to be novel \u2014 the authors should provide solid justifications on the design of each component.  \n\nI am happy to revisit my scores after the rebuttal!"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4903/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699300622095,
            "cdate": 1699300622095,
            "tmdate": 1699636474793,
            "mdate": 1699636474793,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bNVXooHt6u",
                "forum": "mNYF0IHbRy",
                "replyto": "GSQu5jx5YE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4903/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4903/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XXwY (1/2)"
                    },
                    "comment": {
                        "value": "We thank reviewer for providing insightful comments. Please find our responses addressing the specific queries below.\n\n**1. Information on how Eq. (5) is used to guide the sampling process and clarity in writing.**\n\nWe kindly refer the Reviewer to section A.8 of revised appendix for a detailed mathematical explanation on sampling process using guidance. \n\nWe will refine the writing for better clarity and ease of understanding in our final manuscript.\n\n**2. More intuition on the interpolation step.**\n\nWe observed that proprietary LLMs such as ChatGPT (used in our approach) are good at following the details from the prompts and generating image blueprint in the form of box proposals as per the prompt. The same has been verified in [1]. However, we observed that in the multi-object settings where the spatial position of objects in unclear, ChatGPT has certain inherent bias for such cases. We conducted an experiment where we instructed ChatGPT to provide boxes for the prompt \"In a living room with a dog and a cat sitting one on each side\". Note that, in this prompt, the relative position of cat and dog is not clear. We observed that 60\\% of the times ChatGPT generated bounding boxes towards the left for the dog and only 30\\% times the dog was on the right. For the remaining 10\\% of the times, it had arbitrary position.\nTo avoid such errors and to account for this bias, we instead prompt ChatGPT to output $K$ bounding boxes per object proposal. To exploit the above bias, we perform density clustering and choose the cluster containing the majority of the boxes. After this, we introduce the interpolation step to merge these filtered bounding boxes into a single optimal box. This potentially removes the effect of the outlier boxes if present. We have further presented a visual demonstration of the effect of such irrelevant boxes on the final generated image in Fig. 6 of the main paper and Fig. 12 of the revised appendix. Kindly note that this is the extreme case where the position of the objects is not clear from the text. So to further facilitate the user-specific control, we provide a hyperparameter $\\eta$ which can be controlled to adjust the position of the boxes. Kindly refer to Sec. 3.3 of the main paper for the description of $\\eta$ and Fig. 3 (main paper) for qualitative visualization of the effect of $\\eta$.\n\n\n**3. Comparison with DeepFloyd.**\n\nAs recommended, we compare our method with the DeepFloyd and present visual comparisons in Fig. 10 (section A.5) of the revised appendix. Additionally, we also present quantitative comparisons in terms of Prompt Adherence Recall (PAR) below in Table 1. We observe that even with a strong T5 text encoder, DeepFloyd struggles to generate coherent images with complex compositional prompts. We conclude that our scene blueprint augmented with iterative refinement is effective for generating coherent images from complex compositional prompts.\n\nTable 1. Quantitative comparison of our approach with DeepFloyd.\n\n| Method| PAR score |Inference Time (min)|\n|-----|-----|---|\n| DeepFloyd |  60\\% | 8.3 |\n| Ours  | 85\\%  | 3.16 |\n\n**4. Potential reasons why these text-to-image models are not able to generate images corresponding to simple compositional or complex prompts?**\n\nWe agree with the reviewer's comments that current text-to-image diffusion models struggle to generate the image content even for non-complex compositional prompts. Experiments on DeepFloyd with a strong text encoder show that even stronger models struggle to generate coherent images. As per our intuition, there could be several factors that could contribute to inconsistencies in text-to-image diffusion models to generate images corresponding to simple compositional or complex prompts.\n\n- Lack of generalizability towards diverse textual prompts can lead to images with missing details.\n\n- The limitation could also stem from the lack of compositional prompts in the training dataset of text-to-image diffusion model inhibiting it from generalizing and generating diverse and accurate images for those prompts.\n\n- Lack of location-aware training of diffusion models on complex compositional prompts can limit their ability to capture intricate relationships between objects, scenes, or attributes, leading to incoherent images.\n\n\n\n[1] Lian, L., Li, B., Yala, A., \\& Darrell, T. (2023). LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models. arXiv preprint arXiv:2305.13655."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732681407,
                "cdate": 1700732681407,
                "tmdate": 1700740007663,
                "mdate": 1700740007663,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]