[
    {
        "title": "Encoding Speaker-Specific Latent Speech Feature for Speech Synthesis"
    },
    {
        "review": {
            "id": "pLSVTQblIC",
            "forum": "ngp5jzx5oK",
            "replyto": "ngp5jzx5oK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5050/Reviewer_reHV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5050/Reviewer_reHV"
            ],
            "content": {
                "summary": {
                    "value": "In the paper, the authors proposed a new method for modeling speaker timbre. Specifically, to encode the speaker characterize, a VAE-like model is trained to encode speech to \u03bc and \u03c3. And then the \u03bc is used to cluster to several cookbooks that represent the speaker information. Through the attention alignment between text representation and codebook, the text and speaker representation are fused into synthesis speech. The experiments shows the propose method performs superior to previous multi-speaker model and outperforms a zero-shot model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. A new speaker modeling method is proposed for zero-shot generation.\n2. The demo provided by the authors seems convincing.\n3. The experiment results show the performance of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The relations between previous works and this paper are not well presented in Section I.  \"The zero-shot method obtains a speaker vector from a short reference audio, and the timbre and prosody expressed in the given reference audio are aligned with its content. In other words, only a small portion of the speech characteristics that the speaker can express can be obtained from the reference audio.\" In recent studies, many methods try to capture more speaker timbre from reference speech. These methods share a similar process, which is to encode speech to serval vectors and then use attention to fuse the text and speaker timbre. For example, in TTS, https://www.isca-speech.org/archive/pdfs/interspeech_2022/zhou22d_interspeech.pdf, NANSY++, RetrieverTTS.\n2. The proposed method is not presented very clearly. For example:\n2.1 why \u03bc is chosen for clustering\n2.2 how to perform the clustering\n2.3 how many codebooks are used for representing a speech.  Does the number of codebooks have a relation to the number of cluster centers?\n2.4 how to obtain the codebook and what is the number of cookbooks in the codebook set?\n3. As mentioned in W.1,  many previous zero-shot TTS methods also represent reference speech to a set of vectors and fuse them with text via attention. Though I can understand the proposed system and comparison system all use VITS as the backbone for fair comparison, I think it better to compare your speaker modeling method with the others for a better understanding of the superiority, since the ELF is the main claimed contribution."
                },
                "questions": {
                    "value": "/"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5050/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5050/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5050/Reviewer_reHV"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5050/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698219451827,
            "cdate": 1698219451827,
            "tmdate": 1699636494819,
            "mdate": 1699636494819,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ngPYKoLUW9",
                "forum": "ngp5jzx5oK",
                "replyto": "pLSVTQblIC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5050/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments.\n\n**W1** \\\nWe agree that supplementing the manuscript with the mentioned related works is valuable. \\\nOur core goal is to naturally express a speaker's overall speech characteristics depending on input content, like a single-speaker model or multi-speaker model. Examples of this are available in the second section of our demo page (newly added in the middle of the review period). \\\nPrevious works [1,2,3] used multiple feature vectors but still focused on following short reference audio. Take the following two sentences as examples: \"I'm so happy!\" and \"I'm so sad.\". The speaker's psychological state uttering the two sentences will be greatly different, and it will be revealed as speech characteristics. If the sentence \"I'm so sad.\" is synthesized using the methods of the previous works and a recorded speech audio of \"I'm so happy!\" is given as the reference audio, the synthesized speech will be unnatural and show quite different speech characteristics from the actual speech when the target speaker utters \"I'm so sad.\". it will be synthesized with a bright timbre and prosody contrasting with typical human speech. \\\nSince previous methods do not condition a speaker's overall speech characteristics in training the TTS models, they do not allow the TTS model to learn how to select and fuse speakers' speech characteristics to express similarly according to current content. Therefore, even if fairly long reference audio or concatenated feature vectors are conditioned to the TTS models, it is not easy to expect to effectively express the speech characteristics of the target speaker depending on a given content. Additionally, this method can require a large amount of computation, and the length of features varies for each speaker, making it difficult to predict the required amount of computation and memory size. \\\nOur method is significantly different from the previous works in that it allows speakers' overall speech characteristics to be expressed while using feature vectors of a constant size regardless of the length of the given audio. We have summarized and supplemented this content in the manuscript.\n\n**W2** \\\nSince the posterior fit on Gaussian distribution, the mean value represents the highest probability point of the feature distribution. Therefore, using the mean value(\u03bc) rather than using other points is a good representative point for expressing the feature distribution.\nWe described our method for clustering and constructing the codebook in Section 3.2.\nIn our work, the codebook consists of centroids of 512 clusters, and k-means++ is used to cluster.\n\n**W3** \\\nAlthough it can be valuable to make comparisons in the manner mentioned by the reviewer, the lack of sufficient implementation details to reproduce the performance claimed by previous works makes such comparisons difficult. We believe that our comparison with state-of-the-art works using the same backbone has sufficiently shown the effectiveness of our method. Additionally, to the best of our knowledge, our work represents a significant advance in that it is the first to show superior performance over the multi-speaker model trained on the target speakers dataset in a non-training approach.\n\n\n[1] Zhou, Yixuan, et al. \"Content-dependent fine-grained speaker embedding for zero-shot speaker adaptation in text-to-speech synthesis.\" arXiv preprint arXiv:2204.00990 (2022).\n\n[2] Yin, Dacheng, et al. \"RetrieverTTS: Modeling decomposed factors for text-based speech insertion.\" arXiv preprint arXiv:2206.13865 (2022).\n\n[3] Choi, Hyeong-Seok, et al. \"NANSY++: Unified voice synthesis with neural analysis and synthesis.\" arXiv preprint arXiv:2211.09407 (2022)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700414810584,
                "cdate": 1700414810584,
                "tmdate": 1700414810584,
                "mdate": 1700414810584,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lC4j4kpSvn",
                "forum": "ngp5jzx5oK",
                "replyto": "pLSVTQblIC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5050/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate the reviewer's time and interest in reviewing our work. We hope that our response has addressed the reviewer's concerns and questions. Please let us know if the reviewer has concerns that have not been properly addressed. We will do our best to answer them."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626384102,
                "cdate": 1700626384102,
                "tmdate": 1700626384102,
                "mdate": 1700626384102,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iuoojpmas1",
                "forum": "ngp5jzx5oK",
                "replyto": "lC4j4kpSvn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5050/Reviewer_reHV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5050/Reviewer_reHV"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. But I still have some questions.\n\nW2. What is the impact of different codebook sizes (512 clusters in paper) on the model's performance?\n\nW3. I fully understand the difficulty of reproducing these systems. Since the proposed method actually has the ability to achieve speaker modeling from single and several utterances and obtain a variable-length speaker representation,  only selecting Yourtts (single speaker vector, different kind speaker modeling method) for zero-shot comparison seems insufficient for proving the SOTA performance of the proposed speaker modeling method."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635479787,
                "cdate": 1700635479787,
                "tmdate": 1700635479787,
                "mdate": 1700635479787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cBEU9vJKMj",
                "forum": "ngp5jzx5oK",
                "replyto": "pLSVTQblIC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5050/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank the reviewer for the valuable comments.\n\n**W2** \\\nIn the early stage of our work, we experimented with 128, 256, and 512 clusters and observed that performance improved slightly as the number of clusters increased. Considering that the performance difference between the 256 clusters setting and the 512 clusters setting is less than 3% in the evaluation of SECS and that the time required for clustering increases significantly as the number of clusters increases, we chose 512 as the number of clusters and conducted studies based on this.\n\n**W3** \\\nWe acknowledge that our previous response was insufficient. First, we do not claim in any part of our paper that our work is state-of-the-art. Recently, various new methods have been proposed, and we believe that it is difficult to accept a claim that any work is state-of-the-art unless it is compared fairly with other related works that are reproduced as the performance presented in its paper. \\\nIn that, we also do not claim that our work is state-of-the-art. What we explicitly claim is that our work outperforms the trained multi-speaker model, considered state-of-the-art in the community, in a non-training manner. \\\nTo the best of our knowledge, there is no work that has demonstrated these comparison results and performance.\n\n\nWe hope that our response adequately addresses the reviewer's concerns. If you have any additional comments, please let me know. We will do our best to address it."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652259127,
                "cdate": 1700652259127,
                "tmdate": 1700657869007,
                "mdate": 1700657869007,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qTtSNkGtLw",
            "forum": "ngp5jzx5oK",
            "replyto": "ngp5jzx5oK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5050/Reviewer_8YZU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5050/Reviewer_8YZU"
            ],
            "content": {
                "summary": {
                    "value": "The work introduces a method for speaker modeling targeting speech synthesis, capturing their characteristics without specific training on each speaker's dataset. This approach outperforms existing methods and can generate artificial speakers effectively. The encoded features are claimed as informative enough to fully reconstruct an original speaker's speech, making it versatile for various applications."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper addresses"
                },
                "weaknesses": {
                    "value": "1. The connection between the method itself and earlier works are not very clear. The author should have a short review of the related literature.\n2. The methods acquired themselves are sometimes a bit abrupt and lacks motivation.\n3. The prototype of the model does not show very notable improvements in metrics other than MOS scores for audio. Speaker blending should be applied to reach better performance.\n4. The study also lacks comparison with other fixed/earlier speech encoders or speaker encoders."
                },
                "questions": {
                    "value": "1. Possibly because of the first point in weaknesses, in Section 2.2, I do not see strong motivation of acquiring autoencoders, despite there are multiple speaker encoding methods available (e.g. speaker encoders; speech factorization methods). Could you please clarify the motivation?\n2. Why speaker blending is needed to reach better CER than the ground truth? And is there any alternative to compensate?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "There is no ethical concern for this study from the reviewer's perspective."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5050/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698474192903,
            "cdate": 1698474192903,
            "tmdate": 1699636494735,
            "mdate": 1699636494735,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "heLo4AV9Ca",
                "forum": "ngp5jzx5oK",
                "replyto": "qTtSNkGtLw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5050/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments.\n\n**W1, W2 and Q1** \\\nOur method aims to express the overall speech characteristics of a target speaker rather than only the speech characteristics presented in a short reference audio. Examples of it are available in the second section of our demo page (newly added in the middle of the review period). \\\nIn order to enable the TTS model to synthesize speech according to the overall speech characteristics of an unseen speaker and a given content, it is necessary to show the speaker's overall speech characteristics to the TTS model in training and to design the TTS model to combine the given speech characteristics according to the given content. It cannot be resolved with previous speaker encoders that take short reference audio as input and the TTS models [1,2,3] inputted with the output of the speaker encoders. \\\nTo address it, our method consists of two stages: 1) encoding speakers' speech characteristics and storing the encoded features with discretization; 2) conditioning them to synthesize a target speaker's speech with a given content. \\\nConsidering that the goal of our work is to synthesize a target speaker's speech similarly, the representation of speech characteristics for this should be sufficiently informative to reconstruct the target speaker's speech completely. An autoencoder serves the above purpose adequately by converting the input into a latent representation and reconstructing it back to the original input. We have summarized and supplemented this in the manuscript.\n\n**W3** \\\nAs shown in Table 1, our method is superior to the trained multi-speaker model VITS in the SMOS evaluation. It shows a more significant SMOS difference than the previous zero-shot approach model YourTTS. These differences are significantly larger than the differences in MOS evaluation results. To the best of our knowledge, our work is the first that shows superior results to the best-performing multi-speaker model trained on a target speaker's dataset with a non-training method. To enable intuitive confirmation in the table, we marked \"Seen\" for models trained with a target speaker's data and \"Unseen\" for models that were not trained on the dataset.\n\n**W4** \\\nWe compared our method with YourTTS, which uses a different speaker encoder. This work has been cited as a crucial comparative work in recent works [4,5,6] and was recently recognized as state-of-the-art. We increase the fairness of comparison by using the same backbone. Although comparisons with a wider range of other works would also be valuable, we believe that we have sufficiently demonstrated the effectiveness and contribution of our method by showing its superiority over the model VITS trained with a target speaker's dataset.\n\n**Q2** \\\nSince the ASR model is trained with data from numerous speakers, it learns pronunciation and speech characteristics commonly observed across many speakers at a higher frequency. If the speaker space we trained is fitted according to the numerous speakers' real distribution, blending multiple speakers is equivalent to approaching speech characteristics with a higher probability of occurrence. Therefore, theoretically, it is natural that blending more speakers results in better CER. It is also evidence that the speaker space we trained represents the real distribution of speakers well.\n\n[1] Zhou, Yixuan, et al. \"Content-dependent fine-grained speaker embedding for zero-shot speaker adaptation in text-to-speech synthesis.\" arXiv preprint arXiv:2204.00990 (2022).\n\n[2] Yin, Dacheng, et al. \"RetrieverTTS: Modeling decomposed factors for text-based speech insertion.\" arXiv preprint arXiv:2206.13865 (2022).\n\n[3] Choi, Hyeong-Seok, et al. \"NANSY++: Unified voice synthesis with neural analysis and synthesis.\" arXiv preprint arXiv:2211.09407 (2022).\n\n[4] Wang, Chengyi, et al. \"Neural codec language models are zero-shot text to speech synthesizers.\" arXiv preprint arXiv:2301.02111 (2023).\n\n[5] Shen, Kai, et al. \"Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers.\" arXiv preprint arXiv:2304.09116 (2023).\n\n[6] Le, Matthew, et al. \"Voicebox: Text-guided multilingual universal speech generation at scale.\" arXiv preprint arXiv:2306.15687 (2023)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700414582185,
                "cdate": 1700414582185,
                "tmdate": 1700414582185,
                "mdate": 1700414582185,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2wv2wwifb0",
                "forum": "ngp5jzx5oK",
                "replyto": "heLo4AV9Ca",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5050/Reviewer_8YZU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5050/Reviewer_8YZU"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for replying.\n\nW1: Thanks for such fruitful response. Then I wonder if you would like to have the response into the paper?\n\nQ1: Then could you please brief the potential benefit of using latent representations, instead of other speaker representations?"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558734584,
                "cdate": 1700558734584,
                "tmdate": 1700558734584,
                "mdate": 1700558734584,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mQQyLGdYQw",
            "forum": "ngp5jzx5oK",
            "replyto": "ngp5jzx5oK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5050/Reviewer_WfZD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5050/Reviewer_WfZD"
            ],
            "content": {
                "summary": {
                    "value": "In this work authors obtain a zero-shot speech cloning method via clustering. VAE type model is used to model the utterance latent space, which is then clustered and the key idea seems to be that each speaker falls into multiple clusters. Final speaker representation is the mix of these cluster centroids. When the query utterance is fed into the system it follows the same path as in training. So speakers not seen in training can be utilized cloned achieving zero-short method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Neat key idea, where speakers fall into multiple clusters. This achieves modeling diversity of speakers voice characteristics. \n- Good empirical results."
                },
                "weaknesses": {
                    "value": "- Final SFEN objective (Eqs. 2-3) comes out of thin air, it would be good to somehow try to explain theoretically how this can be derived, noting that VAE loss is derived from trying to model log p(x). \n- Continuing the above critizism, elements in the loss are by necessity weighted somehow. How you decide the proper weighting? \n- Plotting cosine scores as objective speajer recognition is ok, but proper speaker recognition results are needed. As in cosine score you only compare against targer speaker and totally miss the confusion with the non-target speaker. So presenting EER and minDCF values with the accomanying DET plots are necessary. \n- Paper neeeds more thorough language editing but this can be performed in rebuttal stage. Abstract was badly written, but some other parts of the paper are quite ok."
                },
                "questions": {
                    "value": "- I see that MT was used to obtain MOS and SMOS scores. How did you clean MT results as those can be sometimes extremely noisy. MT workers sometimes use scripting to speed up the work and so those those workers should be removed from the results. \n- How did you measure CI for MOS and SMOS?\n- Were all samples finally voceded using the same voceder before objective speaker recognition? If not, then you can easily add (inaudible) vocoder artifacts that your TDNN model can then pick up. Vocode all samples, even ground truth ones, using the same vocoder. \n- Why in Table 1, proposed CER is lower in #20 than in all audio?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5050/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5050/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5050/Reviewer_WfZD"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5050/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698583950368,
            "cdate": 1698583950368,
            "tmdate": 1700733329861,
            "mdate": 1700733329861,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Lb6114qpgB",
                "forum": "ngp5jzx5oK",
                "replyto": "mQQyLGdYQw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5050/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments.\n\n**W1** \\\nOur work presents a method combining a variational autoencoder(VAE) and adversarial learning. It significantly improves the reconstruction performance of SFEN. Therefore, the objective we present consists of terms for optimizing generative adversarial networks (GAN) and VAE. In the previous work [1, 2], the authors proved that these objectives are to model a real data distribution. Other widely cited works [3, 4] showed that modeling a real data distribution from an unknown input distribution is possible with the same theoretical background as the first work [1]. Thus, it is also possible to model the real data distribution by applying the adversarial learning method with the output distribution of the SFEN encoder. In conclusion, both the VAE and adversarial learning terms are aligned to model a real data distribution. Additionally, we follow LSGAN [5] as the objective of adversarial learning, which presents a solid theoretical background that allows GAN to model real data distribution better.\n\n**W2** \\\nWe followed the coefficient 45 for l1 reconstruction loss presented in the previous work [4] and did not use weight for other terms. We agree that details were omitted from the manuscript and have revised it.\n\n**W3** \\\nWe believe that the SMOS and SECS results have sufficiently demonstrated the effectiveness of our method. Also, subjective evaluation still carries more weight than objective evaluation in evaluating synthesized speech. The speaker vector plot we presented in the appendix is auxiliary data that demonstrates the superiority of our method. Referring to the related previous works [6,7,8,9,10], we can confirm that the metrics mentioned by the reviewer are unnecessary. In addition, as described in the manuscript, 21 test speakers are in our experimental settings. Measuring speaker verification performance with such few speakers does not provide valid evidence.\n\n**W4** \\\nWe have made minor corrections to the abstract. We will thoroughly review it again to enhance readability for readers.\n\n**Q1** \\\nWe agree that removing the mentioned noisy data is important when utilizing the crowd-sourced MOS method via Amazon Mturk. We removed noisy data by adding multiple evaluation samples that were not related to the comparisons, considering workers who gave clearly incorrect evaluations as random workers, and excluding all evaluation results from those workers.\n\n**Q2** \\\nThe mean and CI were calculated by aggregating all scores for each comparative method.\nWe followed the commonly used method of measuring 95% confidence intervals in statistics. Detailed explanations are in the materials below.\nhttp://www.stat.yale.edu/Courses/1997-98/101/confint.htm\n\n**Q3** \\\nOur method and the comparative methods do not use vocoders. Entire modules in the models are jointly trained; therefore, there are no separately operating vocoders.\n\n**Q4** \\\nThe difference between the two CER results is minimal. This degree of difference can be caused due to the randomness of model training. We see that they are almost identical results, and it is difficult to state that the robustness of the two methods is different.\n\n\n[1] Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in neural information processing systems 27 (2014).\n\n[2] Kingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\" arXiv preprint arXiv:1312.6114 (2013).\n\n[3] Isola, Phillip, et al. \"Image-to-image translation with conditional adversarial networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n\n[4] Kong, Jungil, Jaehyeon Kim, and Jaekyoung Bae. \"Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis.\" Advances in Neural Information Processing Systems 33 (2020): 17022-17033.\n\n[5] Mao, Xudong, et al. \"Least squares generative adversarial networks.\" Proceedings of the IEEE international conference on computer vision. 2017.\n\n[6] Casanova, Edresson, et al. \"Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone.\" International Conference on Machine Learning. PMLR, 2022.\n\n[7] Hsu, Wei-Ning, et al. \"Hierarchical Generative Modeling for Controllable Speech Synthesis.\" International Conference on Learning Representations. 2018.\n\n[8] Shen, Kai, et al. \"Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers.\" arXiv preprint arXiv:2304.09116 (2023).\n\n[9] Wang, Chengyi, et al. \"Neural codec language models are zero-shot text to speech synthesizers.\" arXiv preprint arXiv:2301.02111 (2023).\n\n[10] Le, Matthew, et al. \"Voicebox: Text-guided multilingual universal speech generation at scale.\" arXiv preprint arXiv:2306.15687 (2023)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413878851,
                "cdate": 1700413878851,
                "tmdate": 1700413878851,
                "mdate": 1700413878851,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ss91vKgmxp",
                "forum": "ngp5jzx5oK",
                "replyto": "mQQyLGdYQw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5050/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate the reviewer's time and interest in reviewing our work. We hope that our response has addressed the reviewer's concerns and questions. Please let us know if the reviewer has concerns that have not been properly addressed. We will do our best to answer them."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626366892,
                "cdate": 1700626366892,
                "tmdate": 1700626366892,
                "mdate": 1700626366892,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XwBIkwDZhN",
                "forum": "ngp5jzx5oK",
                "replyto": "Ss91vKgmxp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5050/Reviewer_WfZD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5050/Reviewer_WfZD"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer comment after rebuttal"
                    },
                    "comment": {
                        "value": "Partially I agree with authors in their rebuttal. One clear problem is in experimental evaluation, where authors, for some reason, did not want to use speaker veritication -based objective evaluation metrics. And consequently, for Q3, I find that authors did not probably understand my point. As if you compare clean audio (without vocoding) and some vocoded audio, such as the proposed method. Then by necessity there are some confounders present that only come from the vocoder artefacts. Authros should think how they can vocode also the clean speech and thus obtain comparable audio files. \n\nTheoretical basis of the loss is still missing even after rebuttal. In some sense it is ok to have purely empirical work. But some theoretical justification for the loss would definitely be beneficial. \n\nI will keep my reviewer score as is."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662155618,
                "cdate": 1700662155618,
                "tmdate": 1700662155618,
                "mdate": 1700662155618,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cEi5zFJvO5",
                "forum": "ngp5jzx5oK",
                "replyto": "mQQyLGdYQw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5050/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Partially I agree with authors in their rebuttal. One clear problem is in experimental evaluation, where authors, for some reason, did not want to use speaker veritication -based objective evaluation metrics. And consequently, for Q3, I find that authors did not probably understand my point. As if you compare clean audio (without vocoding) and some vocoded audio, such as the proposed method. Then by necessity there are some confounders present that only come from the vocoder artefacts. Authros should think how they can vocode also the clean speech and thus obtain comparable audio files.\n\nLet us explain again what we have presented in the paper.\n\n1. We have never presented the speaker recognition results that the reviewer mentioned in Q3. \\\nTherefore, there is no comparison that distinguishes between clean audio and synthesized audio (the reviewer mentioned \"vocoded\", but since none of the models in our paper use a vocoder, we'll say \"synthesized\"). \\\n2. The speaker vector plot we presented in Appendix A.3 are all for synthesized audio, so they are not related to the mentioned vocoder artifact. \\\n3. The WavLM-TDNN model was used to measure SECS, not speaker recognition. Since SECS were measured between ground truth audio and synthesized audio for all models, the evaluation conditions are fair. \\\n\nWe'd like to ask the reviewer where we should apply a vocoder. \\\nThe discussion will be more effective if the reviewer specifies it. \n\n\n> Theoretical basis of the loss is still missing even after rebuttal. In some sense it is ok to have purely empirical work. But some theoretical justification for the loss would definitely be beneficial.\n\nThe theoretical basis for modeling a real data distribution mentioned by the reviewer is explained in detail in Section 4 in the previous work [1] and Section 3 in the previous work [2]. If the reviewer specifically points out parts that require more theoretical basis for modeling a real data distribution, we will explain those parts.\n\n[1] Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in neural information processing systems 27 (2014).\n\n[2] Mao, Xudong, et al. \"Least squares generative adversarial networks.\" Proceedings of the IEEE international conference on computer vision. 2017.\n\nWe hope that effective discussions will proceed through specified comments, \\\nand we also hope that our work will be reviewed based on a sufficient understanding of our work and the related works."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666695545,
                "cdate": 1700666695545,
                "tmdate": 1700666716549,
                "mdate": 1700666716549,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YK30lLeTCP",
                "forum": "ngp5jzx5oK",
                "replyto": "cEi5zFJvO5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5050/Reviewer_WfZD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5050/Reviewer_WfZD"
                ],
                "content": {
                    "title": {
                        "value": "Answer to authors"
                    },
                    "comment": {
                        "value": "Whenever you synthesize audio, you are going to use some \"vocoder\" -type mechanism. Whether you directly sample samples, as in wavenet, you in effect use some vocoding mechanism. And as such this mechanism will cause artefacts. Then by comparing ground truth audio, which I guess is \"clean audio\", you are in effect measuring also these artefacts. Your SECS _IS_ speaker verification measure, but as I explained before you are measuring only target speaker scores and you _ARE_ missing non-target speaker scores. Measure that will take all of these into account is for example EER and minDCF. \n\nAbout theoretical basis, I am _NOT_ talking about theory of GANs. But I am saying that how you would derive your loss function. What assumptions you would need to be able to obtain the proposed loss. Sometimes you can obtain a partial explanation, like loss forms additive terms X, Y and Z. But then you experimentally add some tunable coefficients there."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680843114,
                "cdate": 1700680843114,
                "tmdate": 1700680843114,
                "mdate": 1700680843114,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NzYUhf6YM2",
                "forum": "ngp5jzx5oK",
                "replyto": "mQQyLGdYQw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5050/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We provide supplementary explanations and the EER and minDCF data requested by the reviewer.\n\n> Whenever you synthesize audio, you are going to use some \"vocoder\" -type mechanism...\n\n1. Obtaining high SECS values is advantageous for our work. As mentioned in the previous responses, our comparison is between ground truth audio and synthesized audio. As the reviewer mentioned, if the speaker verification model can easily distinguish between the ground truth audio and synthesized audio due to the influence of artifacts, SECS will be lower. Therefore, the evaluation settings we presented are more stringent than those that eliminate the influence of artifacts.\n\n2. Nevertheless, since there is a possibility that models with different structures have individual characteristics, we used the same backbone as the comparison models.\n\n3. In the Wavenet [1] era, TTS system was composed of a two-stage structure using mel-spectrogram as a medium. However, the models we used do not use intermediate features such as mel-spectrograms that can be easily obtained from raw waveforms. Please refer to the illustration below. \n\n---\nTacotron2 + Wavenet [1], FastSpeech [3], Glow-TTS [4] and HiFi-GAN [5] :\n\nText \u2014> [1st stage model]\u2014> mel-spectrogram \u2014> [2nd stage model (vocoder)] \u2014> waveform \n\n---\nOur work, VITS and YourTTS :\n\nText \u2014> [model] \u2014> waveform\n\n---\n\nTherefore, there is no way to compare ground truth audio by vocoding it. If we introduce a vocoder that is unrelated to our task, vocoding and comparing all the audio, the quality degradation makes it difficult to evaluate the actual performance of the model accurately. Also, considering that the output of our model is a complete waveform and is directly used in applications, it is not a reasonable evaluation setting.\n\n4. We measured EER and minDCF, as requested by the reviewer. We used the same speaker verification model mentioned in our paper. As mentioned in the previous response, since it consists of a much smaller number of test speakers than evaluations in typical speaker verification tasks, the result values are also very small. We hope that the reviewer's concerns will be addressed.\n\nAll measurements except ground truth were conducted between ground truth and synthesized audio.\n\n**Ground Truth** \\\nEER : 0.099 \\\nminDCF : 0.020\n\n**VITS - Seen Speakers** \\\nEER : 0.108 \\\nminDCF : 0.019\n\n**YouTTS - Unseen Speakers** \\\nEER : 0.201 \\\nminDCF : 0.045\n\n**ELF (# of audio = 1) - Unseen Speakers** \\\nEER : 0.169 \\\nminDCF : 0.039\n\n**ELF (# of audio = 20) - Unseen Speakers** \\\nEER : 0.158 \\\nminDCF : 0.042\n\n**ELF (all audio) - Unseen Speakers** \\\nEER : 0.112 \\\nminDCF : 0.040\n\n---\n\n> About theoretical basis, I am NOT talking about theory of GANs....\n\nWe will break down the loss equations and explain them.\nEq (2) in the manuscript is the same as the loss presented in the previous works [6, 5] for training the discriminator.\nEq (3) consists of a total of 4 terms. The first and second terms are for adversarial learning. These are the same as the terms presented in the previous work [5]. The second term is used in the previous [5] work under the name \"feature-matching loss\", which is described in detail in Section 2.3 of the previous work [7]. Next, the third term is reconstruction loss, assuming that the data distribution is a Laplace distribution. We applied the coefficient presented in the previous work [5] for this term. The fourth term is the KL divergence of VAE.\n\nThe reviewer's specified comments were very helpful. \\\nWe hope our efforts and responses adequately address the reviewer's concerns.\n\n[1] Oord, Aaron van den, et al. \"Wavenet: A generative model for raw audio.\" arXiv preprint arXiv:1609.03499 (2016).\n\n[2] Shen, Jonathan, et al. \"Natural tts synthesis by conditioning wavenet on mel spectrogram predictions.\" 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018.\n\n[3] Ren, Yi, et al. \"Fastspeech: Fast, robust and controllable text to speech.\" Advances in neural information processing systems 32 (2019).\n\n[4] Kim, Jaehyeon, et al. \"Glow-tts: A generative flow for text-to-speech via monotonic alignment search.\" Advances in Neural Information Processing Systems 33 (2020): 8067-8077.\n\n[5] Kong, Jungil, Jaehyeon Kim, and Jaekyoung Bae. \"Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis.\" Advances in Neural Information Processing Systems 33 (2020): 17022-17033.\n\n[6] Mao, Xudong, et al. \"Least squares generative adversarial networks.\" Proceedings of the IEEE international conference on computer vision. 2017.\n\n[7] Larsen, Anders Boesen Lindbo, et al. \"Autoencoding beyond pixels using a learned similarity metric.\" International conference on machine learning. PMLR, 2016."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728046931,
                "cdate": 1700728046931,
                "tmdate": 1700728203718,
                "mdate": 1700728203718,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ExXEZoOFQ0",
                "forum": "ngp5jzx5oK",
                "replyto": "NzYUhf6YM2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5050/Reviewer_WfZD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5050/Reviewer_WfZD"
                ],
                "content": {
                    "title": {
                        "value": "Reviewers response"
                    },
                    "comment": {
                        "value": "Ok thanks a lot for your detailed explanation. I do get the point why now in the modern TTS era vocoding block is impossible to separate and apply to ground truth audio. However, you probably still agree that any mechanism used to generate samples will in effect produce artefacts. I agree that taking those artefacts into account is beyond the present work. \n\nThanks a lot for proviuding EERs and minDCFs those are helpful for the reader. I hope those can be added to the appendix. \n\nI can raise my score by one point."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733298740,
                "cdate": 1700733298740,
                "tmdate": 1700733298740,
                "mdate": 1700733298740,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]