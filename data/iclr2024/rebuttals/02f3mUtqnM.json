[
    {
        "title": "Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing"
    },
    {
        "review": {
            "id": "RbpwVJ5FMO",
            "forum": "02f3mUtqnM",
            "replyto": "02f3mUtqnM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7318/Reviewer_ZnPu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7318/Reviewer_ZnPu"
            ],
            "content": {
                "summary": {
                    "value": "Deployment of large language models is costly whereas smaller models can be deployed on edge devices but tend to lag behind in response quality. This work proposes a hybrid inference approach to save cost and maintain response quality. A query router is employed for assigning queries to large or small language model depending upon the predicted query difficulty and the desired quality level. This desired quality level is dynamically tunable at test time for trading quality for cost. The proposed design achieves 40% fewer calls to large model with no drop in response quality."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper addresses and interesting problem considering that currently available smaller language models can fairly perform well. Depending upon the predicted difficulty level of the query, it is interesting to use a router to pass the relatively easier queries to smaller model. This approach can be cost effective.\n\nMultiple router score designs are proposed.\n\nThere is thorough empirical analysis with good discussion."
                },
                "weaknesses": {
                    "value": "The proposed design requires that for each LLM pair, a router is required to be trained which might be a costly undertaking in a production environment. \n\nThis paper discusses the cost/quality analysis in context of a language model pair. In real world scenarios, there might be multiple LLMs available and several competing factors to be optimized or traded-off. \n\nFigure 1 is not properly aligned and some inconsistent border is visible (Fig 1 (C))."
                },
                "questions": {
                    "value": "This Paper states that \u201cwe expect that using the router to route queries to the small model will not detract significantly from the realizable cost advantage.\u201d Is this an assumption or empirically verified conclusion?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7318/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698587887532,
            "cdate": 1698587887532,
            "tmdate": 1699636874627,
            "mdate": 1699636874627,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P3aCnMtbgz",
                "forum": "02f3mUtqnM",
                "replyto": "RbpwVJ5FMO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7318/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7318/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer ZnPu"
                    },
                    "comment": {
                        "value": "We thank you for your reviews and address your concerns as follows.\n\nQ1: The proposed design requires that for each LLM pair, a router is required to be trained which might be a costly undertaking in a production environment.\n\nA1: Thank you for this insightful comment. Though our routers are trained for each LLM pair, the learnt knowledge can be generalized to different LLM pairs of similar performance gaps. We will conduct experiments to examine if our trained routers can be applied to different LLM pairs from the one on which it was trained while still being effective. These experiment results will be added to the revised manuscript soon.\n\nQ2: This paper discusses the cost/quality analysis in context of a language model pair. In real world scenarios, there might be multiple LLMs available and several competing factors to be optimized or traded-off.\n\nA2: Thank you for this valuable comment. Routing between multiple LLMs is an important and natural extension to our formulation, which we have briefly discussed in Section 5. To the best of our knowledge, we are the first to explore cost-effective and quality-aware hybrid LLM inference. With our work in this paper on 2-model routing, we believe we have laid the ground for future research on N-model routing.\n\nQ3: Figure 1 is not properly aligned and some inconsistent border is visible (Fig 1 (C)).\n\nA3: Thank you for this comment. We will fix the presentation issues in our revision.\n\nQ4: This Paper states that \u201cwe expect that using the router to route queries to the small model will not detract significantly from the realizable cost advantage.\u201d Is this an assumption or empirically verified conclusion?\n\nA4: Thank you for the comment. This claim has been empirically verified. Since our router is an encoder model of a relatively small size (i.e., 300M) in comparison to LLMs we investigated in this work (e.g., Llama-2-13B), the computation overhead of using the router is negligible. We will further empirically validate this claim by comparing the inference latency of our router and different LLMs and add the results of those experiments to the revision.\n\nThank you for your time and consideration. We sincerely hope that you would consider increasing your rating if you find our responses helpful."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7318/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700180656825,
                "cdate": 1700180656825,
                "tmdate": 1700180656825,
                "mdate": 1700180656825,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MzQ8XSRmUG",
            "forum": "02f3mUtqnM",
            "replyto": "02f3mUtqnM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7318/Reviewer_VA7E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7318/Reviewer_VA7E"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel inference paradigm called hybrid inference, which utilizes two models of different sizes to handle queries. This approach aims to balance infference cost and response quality by routing easy queries to a smaller model while directing more complex queries to a larger model. The authors propose an orchestration framework that involves a router trained on a dataset of representative queries. The router dynamically routes queries to the appropriate model, thus reducing overall costs while maintaining response quality. They present three variations of the router: a deterministic router, a probabilistic router, and a probabilistic router with data transformation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper sets the problem in the context of LLM inference and focuses on the evaluation of response quality and cost advantage. It defines metrics for measuring the effectiveness of the routing strategy, considering the intrinsic uncertainties in natural language processing tasks. The evaluation is conducted on the MixInstruct dataset, which comprises a diverse range of tasks such as question answering, summarization, and information extraction. The experimntal results demonstrate the efficacy of the proposed routing strategies, especially in scenarios where the performance gap between the small and large models is minimal. The deterministic router achieves good cost advantages with negligible drops in response quality, while the probabilistic router further improves the cost advantage without compromising response quality. The probabilistic router with data transformation exhibits even more promising results, achieving significant cost advantages with no quality drop."
                },
                "weaknesses": {
                    "value": "The main limitation of the paper seems to be its reliance on the assumptions about the quality gaps and the routiing mechanisms. These assumptions could potentially affect the overall effectiveness and efficiency of the routing process. Additionally, the reliance on specific models and the need for manual intervention in setting the threshold for routing may limit the scalability and generalizability of the proposed framework."
                },
                "questions": {
                    "value": "The approach might encounter problems in accurately distinguishing between easy and hard queries, especially when dealing with a large performance gap between different models. How do you elaborate on this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7318/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7318/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7318/Reviewer_VA7E"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7318/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698616980620,
            "cdate": 1698616980620,
            "tmdate": 1700841210138,
            "mdate": 1700841210138,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rV4mqLxIyp",
                "forum": "02f3mUtqnM",
                "replyto": "MzQ8XSRmUG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7318/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7318/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer VA7E"
                    },
                    "comment": {
                        "value": "We thank you for your reviews and address your concerns as follows.\n\nQ1: The main limitation of the paper seems to be its reliance on the assumptions about the quality gaps and the routing mechanisms. These assumptions could potentially affect the overall effectiveness and efficiency of the routing process. \n\nA1: Thank you for this comment. First of all, we would like to ask for clarification on what assumptions you are referring to, given that we did not have any formal assumptions \u201cabout the quality gaps and the routing mechanisms\u201d in our paper. \n\nQuality gaps between responses from different LLMs is not an assumption but an important and real observation that has been widely reported and studied in previous work [1,2]. In general, LLMs of larger model sizes, trained with more data and computational resources have been found to give responses of higher quality [1,2]. The setting in which responses from LLMs are of negligible quality difference is an extreme case, and a degenerate solution which always routes queries to the small models will be optimal in this setting. In this work, we study the more general problem where response quality gaps can be significant. Our router is designed to identify easy queries, to which even small models can give high quality responses, and route them to small models to save costs while maintaining high performance.\n\nAs to the routing mechanism, we developed 3 routing strategies, r_det, r_prob, and r_trans. The deterministic router, r_det, is the only one relying on the assumption that \u201cneural models are deterministic functions\u201d. In our analysis, we show that this assumption may not hold in practice, which motivates our design of r_prob and r_trans. Both r_prob and r_trans are generic methods for LLM routing and we demonstrated their effectiveness with different LLMs and real-world queries.\n\nQ2: Additionally, the reliance on specific models and the need for manual intervention in setting the threshold for routing may limit the scalability and generalizability of the proposed framework.\n\nA2: Thank you for this comment. First of all, we would like to clarify that our routing framework is generic and can be applied on any given LLM pairs. In addition, we will conduct experiments to show that our trained routers can be applied to different LLM pairs while still being effective. The results of these experiments will be added into the revised manuscript soon.\n\nSecondly, the validity of our framework does not rely on the choices of thresholds. Instead, the threshold is a user-defined parameter used to control the efficiency-performance trade-off, to best serve the interests of different users. In practice, if needed, we could use a calibration set to recommend by-default thresholds for users to decide. We will illustrate this through extra experiments and report results in the revision.\n\nQ3: The approach might encounter problems in accurately distinguishing between easy and hard queries, especially when dealing with a large performance gap between different models. How do you elaborate on this?\n\nA3: Thank you for this comment. In Section 4.3, we demonstrate that our router can distinguish and route easy queries to small models while routing hard queries to large models, with LLMs of small, medium, and large performance gaps. In Figure 6, we plot the difference between the average quality gaps of queries routed to the small model and those routed to the large model for our router and the random baseline w.r.t. different values of cost advantages (i.e., the fraction of queries routed to the small model). The random baseline randomly assigns queries and therefore the average difference is nearly always zero. In contrast, the difference between the average quality gaps for our router always has significant positive values at all cost advantages for all LLM pairs, which indicates that our approach can successfully distinguish easy queries (i.e., queries of large quality gaps (i.e., q(S(x)) - q(L(x)))), and route more easy queries to the small model as expected.\n\nMoreover, though the routing performance may vary on different LLM pairs due to the intrinsic difference of routing settings, our approach is shown to consistently outperform competing methods with non-trivial performance gains, as discussed in Section 4.2.\n\nThank you for your time and consideration. We sincerely hope that you would consider increasing your rating if you find our responses helpful.\n\nReferences:  \n[1] Hoffmann, Jordan, et al. \"Training compute-optimal large language models.\" arXiv preprint arXiv:2203.15556 (2022).  \n[2] Kaplan, Jared, et al. \"Scaling laws for neural language models.\" arXiv preprint arXiv:2001.08361 (2020)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7318/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700180498326,
                "cdate": 1700180498326,
                "tmdate": 1700180498326,
                "mdate": 1700180498326,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U4s76jN4Rg",
            "forum": "02f3mUtqnM",
            "replyto": "02f3mUtqnM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7318/Reviewer_uyT6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7318/Reviewer_uyT6"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a hybrid inference strategy designed to minimize the computational expense by limiting the number of queries to the larger model and utilizing smaller models to function as decision-making routers.\nInitially, the approach assesses if the user's input query is easy or hard by evaluating the anticipated response quality from both the small and large models.\nTo evaluate the complexity of a query, the paper describes three distinct methodologies, each utilizing the same classification model but differing in their training and inference schemes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Paper presents a novel hybrid inference strategy designed to minimize the computational expense by limiting the number of queries to the larger model and utilizing smaller models to function as decision-making routers.\nMoreover, paper presents multiple different approaches to training the decision making routers and its effectiveness."
                },
                "weaknesses": {
                    "value": "I have following major concerns.\n\n1. **Reliability of BART scores for routing**\nI am uncertain about the efficiency of training the router model to decide whether the BART scores of the smaller model is similar to those of the larger one. BARTScore has demonstrated strong performance in extractive QA; however, its correlation may diminish in abstractive QA contexts [1], suggesting that the metric might not be suitable for assessing open-ended generation tasks. Establishing a correlation between evaluations of routing using BARTScore and human assessments would be beneficial to verify the reliability of BARTScore for routing evaluation purposes.\n\n2. **The Impact of Training Data Versus Model Size**\nI am of the opinion that the size of the model is not as critical as the differences in the training data used for each model in determining quality. For instance, consider evaluating the performance disparities between models like (Llama-2 7B and Llama-2 13B) versus those between (Llama-2-7B and the more recent Zephyr-7B [2]). Would the performance gap trend similar to the reported trend in Figure6?\n\n[1] G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment., Liu et al., 2023 \\\n[2] https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha"
                },
                "questions": {
                    "value": "Same as weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7318/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699478040270,
            "cdate": 1699478040270,
            "tmdate": 1699636874389,
            "mdate": 1699636874389,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j1WZIAFaBp",
                "forum": "02f3mUtqnM",
                "replyto": "U4s76jN4Rg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7318/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7318/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer uyT6 (Part I)"
                    },
                    "comment": {
                        "value": "We thank you for your reviews and address your concerns below.\n\nQ1: Reliability of BART scores for routing. I am uncertain about the efficiency of training the router model to decide whether the BART scores of the smaller model is similar to those of the larger one. BARTScore has demonstrated strong performance in extractive QA; however, its correlation may diminish in abstractive QA contexts [1], suggesting that the metric might not be suitable for assessing open-ended generation tasks. Establishing a correlation between evaluations of routing using BARTScore and human assessments would be beneficial to verify the reliability of BARTScore for routing evaluation purposes.\n\nA1: Thank you for this insightful comment. We address this concern from the following perspectives:\n1. Firstly, we would like to mention that the design of our routing strategies is agnostic to the choice of response quality metrics. In principle, our router can be trained with any text-generation metrics.\n2. Secondly, we demonstrate our approaches using BARTScore for two reasons. (1) BARTScore is inexpensive to compute in comparison to human and GPT-based evaluators [1,11] due to its small model size (e.g., BART-base is of 140M parameters), which makes it a more accessible choice especially with large training data. (2) BARTScore has been found to have good correlation with GPT-Rank [2], a strong GPT-based ranking metric, on MixInstruct dataset which includes open-ended question answering and creative writing tasks. Specifically, in [2], the authors found that \u201cBARTScore gets the highest correlation with GPT-Rank against other metrics, which suggests we use BARTScore to provide supervision for training\u201d. We follow this recent work by choosing BARTScore for router training and evaluation purposes.\n3. Thirdly, we would like to bring up that [1] and [2] are using BARTScore in two different ways which may explain their different observations on the effectiveness of BARTScore, especially on open-ended generation tasks. The general idea behind BARTScore is that \u201cmodels trained to convert the generated text to/from a reference output or the source text will achieve higher scores when the generated text is better\u201d [10]. With this in mind we make the following observations:\n    1. In [1], BARTScore measures the quality of the machine-generated summary in terms of the likelihood of obtaining the summary from the source text. The authors compare the quality as measured by the BARTScore and the quality as measured by GPT-4 (which is directly asked to rate the summary) and find that the latter is more correlated with the quality as measured by human evaluators.\n    2. In [2], BARTScore is used to compare the responses from different LLMs to the same query in terms of the likelihood of obtaining the given LLM response from a ground truth (human/GPT-4 generated response). As a baseline the authors also use ChatGPT to compare the two responses. In this case, the authors find that the BARTScore measurement is well correlated with the ChatGPT measurement i.e. the two typically provide the similar ranking of the LLM responses.\n\n    From the above observations we believe it can be concluded that the observations from [1] and [2] are not against but rather complementary to each other. A holistic view would be that, **the values of BARTScore may not correlate well with human assessments on open-ended generation tasks (as seen in [1]), but it is informative in ranking multiple responses to the same query (as seen in [2]) while being significantly cheaper to compute than human evaluation or GPT ranking**. Since the goal of this work is to route queries to the smaller LLM when we expect its response to be comparable to or better than that of the larger LLM, therefore we believe that our setting is closer to [2] and using a metric like BARTScore that is well correlated with the ranking of LLM responses is sufficient for our purpose. Overall, we do not claim that BARTScore is the best metric for all text-generation tasks, but argue that it is an empirically good fit for our routing task, due to the reasons discussed above.\n4. Last but not least, we will evaluate our approach with more effective metrics in addition to BARTScore. Since human assessment of semantic interpretation tasks by itself is a challenging problem, due to the subjectivity and inconsistency of human annotators among themselves [3,4,5,6], we are going to use GPT-based evaluators and report evaluation results in our revision."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7318/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700180177047,
                "cdate": 1700180177047,
                "tmdate": 1700180177047,
                "mdate": 1700180177047,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "01SJ6h5IHh",
            "forum": "02f3mUtqnM",
            "replyto": "02f3mUtqnM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7318/Reviewer_9pHQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7318/Reviewer_9pHQ"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a router that assigns queries to differently sized models. Their method results in 40% fewer calls to the large model, with no drop in response quality. They introduce two main techniques to improve performance:\n\n- using soft probabilities instead of hard probabilities\n- using a data transformation with a relaxation $t$ to provide stronger training signal."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Paper is well written, and authors do a good job of building upon concepts used in final technique.\n- Ablations and analysis are extensive and well-thought out, giving researchers ample inspiration to build upon this technique.\n- The analysis of performance on different model size pairs is interesting to me."
                },
                "weaknesses": {
                    "value": "Please cite these works:\n- https://arxiv.org/abs/2305.05176 - routing on a query level\n- https://arxiv.org/abs/2211.17192, https://arxiv.org/abs/2302.07863 - latency reduction using small and big models\n\nI believe writing a discussion of the tradeoffs of these approaches would improve the current draft."
                },
                "questions": {
                    "value": "- In this method, we are able to reduce cost and latency, but not as much latency reduction as methods such as speculative decoding (https://arxiv.org/abs/2211.17192). While there is added cost with speculative decoding, do you think there's any possibility of closing this gap?\n- Do you think this might be because of scoring query wise vs token-wise? Why not use this method token wise?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7318/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7318/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7318/Reviewer_9pHQ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7318/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699664186882,
            "cdate": 1699664186882,
            "tmdate": 1699664251880,
            "mdate": 1699664251880,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0RDByakEoL",
                "forum": "02f3mUtqnM",
                "replyto": "01SJ6h5IHh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7318/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7318/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer 9pHQ"
                    },
                    "comment": {
                        "value": "We thank you for your reviews and address your concerns below.\n\nQ1: Please cite these works: https://arxiv.org/abs/2305.05176 - routing on a query level; https://arxiv.org/abs/2211.17192, https://arxiv.org/abs/2302.07863 - latency reduction using small and big models. I believe writing a discussion of the tradeoffs of these approaches would improve the current draft.\n\nA1: Thank you for pointing out these important references. We will add a discussion of the tradeoffs of these approaches in our revision. In brief, \n1. FrugalGPT [1] studies how to reduce inference costs using LLM cascades and early-exit strategies. Specifically, for each user query, FrugalGPT will execute a sequence of LLMs and return answers whenever the confidence scores exceed the predefined thresholds. Though empirically effective, FrugalGPT is at the risk of executing multiple LLMs or even all LLMs in the cascade to answer a single query, which could be prohibitively expensive. Instead, our routing mechanism ensures that only one LLM will be used to generate answers for user queries and therefore is likely to be more efficient especially on challenging queries.\n2. Speculative decoding [2,3] speeds up decoding of expensive models by invoking small-and-efficient decoders on the \u201ceasy\u201d decoding steps. Instead, in our work we are interested in query routing which assigns \u201ceasy\u201d queries to small models to reduce overall inference costs while maintaining high performance. Clearly, speculative decoding [2,3] optimizes a different objective and is complementary to our approach. A straightforward framework combining the two will be as follows. Users can first use our router to decide if a query should be handled by the small or large model. If the query goes to the small model, we can save significant costs as demonstrated in our paper. If the query goes to the large model, we can still apply speculative decoding to achieve further efficiency improvements.\n\nQ2: In this method, we are able to reduce cost and latency, but not as much latency reduction as methods such as speculative decoding (https://arxiv.org/abs/2211.17192). While there is added cost with speculative decoding, do you think there's any possibility of closing this gap?\n\nA2: Thank you for this question. As discussed in A1, speculative decoding [2, 3] optimizes a different objective than ours, which may explain the difference in achieved cost reduction. Given that the two approaches are complementary, instead of asking how to \u201cclose this gap\u201d, it could be more interesting to ask how to achieve further efficiency improvements by combining our approach with speculative decoding. A simple strategy is discussed in A1 and we will investigate other possibilities in the future work.\n\nQ3: Do you think this might be because of scoring query wise vs token-wise? Why not use this method token wise?\n\nA3: Thank you for this question. As discussed above, the difference may largely come from the fact that we are optimizing different objectives than speculative decoding [2,3], with the distinction between query-wise and token-wise objective formulation being one of the contributing factors.\n\nIn this work, we did not consider applying our approach on token level mainly for two reasons: (1) We are interested in developing a generic approach that is agnostic to model implementation and generalizes to both open-sourced as well as black-box LLMs. Token-wise routing requires transferring KV values of previous tokens across models which can be infeasible if these intermediate results are unavailable (e.g., black-box LLM APIs) or the dimensions do not match across open-sourced LLMs. (2) A naive token-wise routing requires executing the router at each decoding step, which may lead to notable compute overheads and weaken the achieved cost reduction. \n\nThank you for your time and consideration. We sincerely hope that you would consider increasing your rating if you find our responses helpful. \n\nReferences:  \n[1] Chen, Lingjiao, Matei Zaharia, and James Zou. \"FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance.\" arXiv preprint arXiv:2305.05176 (2023).  \n[2] Leviathan, Yaniv, Matan Kalman, and Yossi Matias. \"Fast inference from transformers via speculative decoding.\" International Conference on Machine Learning. PMLR, 2023.  \n[3] Kim, Sehoon, et al. \"Speculative Decoding with Big Little Decoder.\" Thirty-seventh Conference on Neural Information Processing Systems. 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7318/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700179166019,
                "cdate": 1700179166019,
                "tmdate": 1700179166019,
                "mdate": 1700179166019,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bsJnrBdOWq",
                "forum": "02f3mUtqnM",
                "replyto": "01SJ6h5IHh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7318/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7318/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer 9pHQ - Revision is Ready"
                    },
                    "comment": {
                        "value": "We thank you again for your reviews and the positive feedback. We greatly appreciate your support of our ideas and have revised the paper as per your suggestions. Specifically, in Section 2.1 (Page 4), we have added a detailed discussion to compare our approach with respect to FrugalGPT [1] and speculative decoding [2,3].\n\nWe hope we have answered your questions to your satisfaction and hope you would consider increasing the score.\n\nReference:  \n[1] Chen, Lingjiao, Matei Zaharia, and James Zou. \"FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance.\" arXiv preprint arXiv:2305.05176 (2023).  \n[2] Leviathan, Yaniv, Matan Kalman, and Yossi Matias. \"Fast inference from transformers via speculative decoding.\" International Conference on Machine Learning. PMLR, 2023.  \n[3] Kim, Sehoon, et al. \"Speculative Decoding with Big Little Decoder.\" Thirty-seventh Conference on Neural Information Processing Systems. 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7318/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682527124,
                "cdate": 1700682527124,
                "tmdate": 1700683147191,
                "mdate": 1700683147191,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]