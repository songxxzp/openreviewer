[
    {
        "title": "CoSDA: Continual Source-Free Domain Adaptation"
    },
    {
        "review": {
            "id": "vXctnSuWZx",
            "forum": "Wure6HljpJ",
            "replyto": "Wure6HljpJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6802/Reviewer_CBo9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6802/Reviewer_CBo9"
            ],
            "content": {
                "summary": {
                    "value": "This study introduces a novel context in the realm of continual SFDA, a specific case within unsupervised domain adaptation. It focuses on the sequential adaptation of a robustly trained source model to multiple unlabeled target domains. The authors pinpoint the issue of catastrophic forgetting prevalent in current domain adaptation techniques and skillfully reconfigure existing baselines to function in this innovative setting. Subsequently, they present a teacher-student consistency learning approach designed to attenuate the effects of forgetting, thereby facilitating efficient adaptation across multiple targets sequentially. The empirical evidence provided substantially corroborates the assertion that this methodology not only enhances performance but also significantly curtails issues related to catastrophic forgetting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It is happy to see that this manuscript reimplements previous SFDA approaches within a unified framework and conducts a realistic evaluation of these methods under the continual SFDA settings.\n2. For the most part, the writing is clear and easy to understand."
                },
                "weaknesses": {
                    "value": "1. I'm primarily concerned about the relevance of the continual SFDA setting. The manuscript restricts its experiments to synthetic tests on established UDA benchmarks, raising questions about the practical value of continual SFDA in real-world applications. In actual practice, it seems feasible to merge all target domains into a single large one and adapt the source model accordingly. Additionally, identifying or defining the source and target domains in real-world applications is already a challenging task, complicating the applicability of this approach.\n\n2. Another concern pertains to the originality of CoSDA, as it appears to amalgamate various existing strategies, including the teacher-student model, mixup, and information maximization. While the use of the exponential moving average method is a common and sensible strategy to prevent overfitting, the rationale behind employing mixup and information maximization to tackle this issue isn't clear or intuitive.\n\n3. Lastly, the presentation of results in the tables is somewhat overwhelming and perplexing. The abundance of statistics, compounded by the use of multiple colors, makes it difficult to interpret the data and grasp the essential outcomes. Simplifying these tables for clarity and ease of understanding would be highly beneficial.\n\nSome typos in this manuscript: \n1. \"by by consolidating data\"\n2. \"In this section, We ...\""
                },
                "questions": {
                    "value": "1. What are the practical applications for continual SFDA in the real world?\n2. In real-world scenarios, how can we gather multiple domains that exhibit distribution shifts?\n3. Considering the era of large-scale models, what is the actual importance of continual SFDA? Specifically, if the source model is an extensive visual foundation model, is there a real need to sequentially adapt this model across various target domains?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6802/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697565929299,
            "cdate": 1697565929299,
            "tmdate": 1699636786547,
            "mdate": 1699636786547,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6eUEcaQtaZ",
                "forum": "Wure6HljpJ",
                "replyto": "vXctnSuWZx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6802/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6802/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response To Reviewer CBo9"
                    },
                    "comment": {
                        "value": "Thanks for your time and valuable reviews, here are our responses:\n\n**[W1]** There setting of continual SFDA is meaningful for the following reasons:\n1. Ease of massive data annotation. Note that in our setting, the label of data is only accessible in the first domain(i.e. source domain). The need for annotation on target domains is eliminated, which saves great effort.\n2. Prevention of potential performance degradation. Due to the domain shift, haphazardly combining different datasets may result in subpar performance.\n3. Spatio-temporal separation. During each adaptation process, we have only unlabeled data from current domain. Neither data nor weights from previous domains are stored. Therefore, each adaptation process can be performed at different time and location, which we refer to as spatio-temporal separation. With spatio-temporal separation, we can first deploy our model, and adapts it to unseen domains during pretraining upon encountering new data.\n\n**[W2]** The novelty of our method lies in dual-speed optimization strategy. Specifically, the student model is updated every batch, while the teacher model is updated every epoch. To the best of our knowledge, we are the first to propose such strategy. The rationale for choosing Mixup as augmentation and integrating information maximization is discussed in section 3.1 , appendix A.1 and A.2.\n\n**[W3]** Thanks for your advice. We will try to improve the presentation of results. Also thanks for pointing out typos.\n\n**[Q1]** One scenario is autonomous driving, where the system may continually encounter different conditions and situations, which can be viewed as different domains. In such scenario, performing well in all encountered domains is of great significance.\n\n**[Q2]** Multiple domains can come from diverse natural contiditions, such as different weather.\n\n**[Q3]** To the best of our knowledge, foundation vision models are not as prevalent as LLM in NLP. There is not a clear way of describing all CV tasks in a natural way as natural language does in NLP. Furthermore, as visual data is inherently far smaller than language data in amount, foundation models probably not perform well in all downsteam, highly specific tasks. The need of adaptation may long stand even if with the existence of foundation models, as the environment is constantly evolving and new domains are ceaselessly appearing."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6802/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485662634,
                "cdate": 1700485662634,
                "tmdate": 1700485662634,
                "mdate": 1700485662634,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l7ld6dfb2f",
                "forum": "Wure6HljpJ",
                "replyto": "6eUEcaQtaZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6802/Reviewer_CBo9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6802/Reviewer_CBo9"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your timely reply. As for the reply of W1, traditional unsupervised domain adaptation also eliminates the need for the labels of the target domain, and no evidence proves that haphazardly combining different datasets may result in subpar performance.\n\nIn my opinion, it is not a technical contribution that the student model is updated every batch while the teacher model is updated every epoch.\n\nBased on the response and the original comments, I hold my primary score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6802/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567638345,
                "cdate": 1700567638345,
                "tmdate": 1700567638345,
                "mdate": 1700567638345,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hYHYu7SRoA",
            "forum": "Wure6HljpJ",
            "replyto": "Wure6HljpJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6802/Reviewer_TdNX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6802/Reviewer_TdNX"
            ],
            "content": {
                "summary": {
                    "value": "The submission investigate continual source-free domain adaptation task, where a source pretrained model is continually adapted to a sequence of unlabeled target domains, without the access to all old domains (source and old target domains). The submission proposes several general methods to address this challenging task, which could be easily combined with the existing source-free domain adaptation methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The investigated continual source-free domain adaptation task has more practical value, as the adapted model is expected to keep good performance on all old domains after adapting to a new target domain. Also in the proposed method, domain ID is not needed which makes the method more readily deployed in the real world application.\n\n- The proposed method is relatively simple, which only contains a mixup consistency loss with teacher-student architecture, a mutual information maximization loss, along with a BN statistics updating trick. Thus, the method is quite general, and could be easily combined with the existing source-free domain adaptation method, which is proved in the experimental section.\n\n- The experimental sections are detailed, which cover several benchmarks, and reproduce lots of existing method for the continual source-free domain adaptation setting."
                },
                "weaknesses": {
                    "value": "Although the studied new setting is of high practical value, and the experiments are abundant, the major concern is that the proposed method(s) is not new/novel, the proposed modules are quite popular in the related areas.\n\n- Teacher-student architecture where teacher model is the EMA between old teacher model and the current student model, this technique is popular in almost every transfer learning topic.\n\n- Usage and discussion of mutual information maximization. As the paper mentioned, MI is proved to be very efficient in unsupervised clustering task [1], which is a similar topic to source-free domain adaptation. Also, AaD also discuss MI and relate it with several other different methods.\n\n- BN updating trick. Actually one paper in continual learning gives a thorough investigation about how BN influence the continual learning performance, with a short conclusion that the running statistics heavily biased towards the current task, which may influence the performance on the old task. In turn, BN statistics are also important for the current task, as mentioned in GSFDA that simply forwarding with the test data once before adaptation could improve the performance. The author could add some discussions with the above mentioned methods, as in the proposed way the teacher model has more information about the old domain while the student model focus on the statistics on the current domains.\n\n\n***reference***\n\n[1] Learning Discrete Representations via Information Maximizing Self-Augmented Training. ICML 2017\n\n[2] Continual Normalization: Rethinking Batch Normalization for Online Continual Learning. ICLR 2022"
                },
                "questions": {
                    "value": "Overall, the paper is sound and address a new but practical new task, as well as providing detailed experimental analysis. However, I think the proposed method is somehow incremental, as mentioned in the weakness part, and I do not really get some new and interesting insights from this submission."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6802/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697769011531,
            "cdate": 1697769011531,
            "tmdate": 1699636786423,
            "mdate": 1699636786423,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4R6PoPL0PA",
                "forum": "Wure6HljpJ",
                "replyto": "hYHYu7SRoA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6802/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6802/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response To Reviewer TdNX"
                    },
                    "comment": {
                        "value": "Thanks for your time and valuable reviews, here are our responses:\n\n**[W1]** The novelty of our method lies in dual-speed optimization strategy. Specifically, the student model is updated every batch, while the teacher model is updated every epoch. To the best of our knowledge, we are the first to propose such strategy.\n\n**[W2]** MI is surely effective. We also provide analysis both empirically(in ablation study) and theoretically(in appendix A.2)\n\n**[W3]** CoSDA aims at achieving continual SFDA. With multiple target domains, simply using BN statistics from the target domain may results in severe catastrophic forgetting. Furthermore, the moving average mean and variance are not the mean and variance of the whole dataset. They can be viewed as weighted mean, instead. As the adaptation process goes on, the mean and variance should be more and more precise, so later batches should have higher weight. \n\n**[Q]** Please refer to W1."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6802/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485617896,
                "cdate": 1700485617896,
                "tmdate": 1700485617896,
                "mdate": 1700485617896,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uiemXM8gSj",
                "forum": "Wure6HljpJ",
                "replyto": "4R6PoPL0PA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6802/Reviewer_TdNX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6802/Reviewer_TdNX"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply. My point is that all those three modules I listed in the weakness are already well investigated in the area or the related areas, the current submission seems more like a combination of them applying to a new but similar task. Thus I keep my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6802/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706536768,
                "cdate": 1700706536768,
                "tmdate": 1700706536768,
                "mdate": 1700706536768,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oyQOOfVigx",
            "forum": "Wure6HljpJ",
            "replyto": "Wure6HljpJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6802/Reviewer_GtVN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6802/Reviewer_GtVN"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the well-established problem of source-free unsupervised domain adaptation (SFDA) in a *continual learning* setting. While existing approaches on SFDA focus exclusively on target domain performance, the authors propose a new method, CoSDA, that not only enhances target domain adaptation but also preserves the source domain performance. The core idea revolves around using a student-teacher framework, where the teacher model is updated via EMA (exponential moving average) of the student model weights to prevent forgetting. The mixup augmentation is used to drive the learning process via consistency regularization between the pair of networks, and a mutual information maximization loss is also added to obtain better pseudo-labels. Experiments are shown on four classification benchmarks: DomainNet, OfficeHome, Office31, and VisDA. Beyond single target adaptation, results are also shown on multi-target adaptation (where the domains appear sequentially) to highlight the continual learning capabilities of the framework. When compared to existing works on SFDA and Test-Time Adaptation (TTA), CoSDA achieves both higher accuracy and good robustness against forgetting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The problem statement is quite relevant for practical applications of domain adaptation methods. Most of the literature on domain adaptation focuses on target domain performance without any concern for source-domain performance. On the contrary, this paper tries to remedy this overlooked aspect in domain adaptation. This is quite an important problem since data can come from any domain during inference."
                },
                "weaknesses": {
                    "value": "1. The proposed CoSDA approach lacks novelty and bears a *significant* similarity to CoTTA [1]. CoTTA tackles the closely related problem of test-time adaptation (TTA) and utilizes the exact same concept of a student-teacher framework trained using consistency regularization. CoSDA simply replaces the general augmentation set in CoTTA with mixup. The authors do compare with CoTTA and mention that \"... CoTTA (Wang et al., 2022) ensures knowledge preservation by stochastically preserving a subset of the source model\u2019s parameters during each update\" but fail to mention this other crucial aspect of this paper which is directly related to their method. The inclusion of the mutual information regularization loss for better pseudo-labels is also borrowed directly from a previous SFDA approach SHOT [2]. Finally, the claimed student-teacher EMA framework is very common is the SFDA literature, [3, 4], none of which are mentioned in the paper.\n\n2. A primary goal of this work is to prevent catastrophic forgetting of previously seen domains, however, there is no discussion of existing continual learning approaches and what distinguishes this work from these papers.\n\n3. The experiments on sequential target domains (Sec 4.3) is limited, despite being a prominent claim of this paper: (a) the max number of domains is only 4, while TTA methods experiment with up to 15, and (b) no experiments on effects of a domain being repeated.\n\n[1] Wang, Qin, et al. \"Continual test-time domain adaptation.\" CVPR 2022.\\\n[2] Liang, Jian, et al. \"Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation.\" ICML 2020.\\\n[3] Kim, Donghyun, et al. \"A unified framework for domain adaptive pose estimation.\" ECCV 2022.\\\n[4] Ohkawa, Takehiko, et al. \"Domain adaptive hand keypoint and pixel localization in the wild.\" ECCV 2022."
                },
                "questions": {
                    "value": "1. Why was mixup chosen as the preferred augmentation? Are there experiments with other augmentations? The authors mention that mixup can be applied to other domains (NLP, Audio), but no evidence is provided to prove the efficacy of CoSDA on other domains, let alone more challenging tasks on images, such as semantic segmentation. \n\n2. How does the performance vary with the number of unlabeled samples per domain?\n\n3. CoTTA utilizes a stochastic restore of the source weights since it tackles the more challenging TTA setup (performing adaptation with very few images) -  was this removed in the experiments? Furthermore, how well does CoSDA perform on the TTA benchmarks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6802/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6802/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6802/Reviewer_GtVN"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6802/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698547375045,
            "cdate": 1698547375045,
            "tmdate": 1699636786268,
            "mdate": 1699636786268,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5923NbtGMe",
                "forum": "Wure6HljpJ",
                "replyto": "oyQOOfVigx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6802/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6802/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response To Reviewer GtVN"
                    },
                    "comment": {
                        "value": "Thanks for your time and valuable reviews, here are our responses:\n\n**[W1]** CoSDA is clearly different from CoTTA in three aspects:\n1. The setting of continual test-time adaptation is not about catastrophic forgetting. It computes the average of results over multiple itrations. Whereas in our continual SFDA setting data from each domain is seen only once.\n2. The dual-speed optimization strategy is the module designed for continual SFDA. While in CoTTA, both teacher model and student model are updated after each epoch.\n3. CoTTA needs to inference $k$ times(typically $k=32$), while CoSDA only needs $k=1$.\nWe discuss knowledge distillation-based methods in related works section, which is more relative than EMA.\n\n**[W2]** We do discuss Continual DA methods in the last part of related works section.\n\n**[W3]** The max number of domain is not 4, but 6. DomainNet has 6 domains, and the results are shown in Figure 2. The domains in the studied datasets have distinct covariate shift, while in continual test-time adaptation setting, all domains are augmented with 15 types of corruptions from the same images, therefore sharing great similarity. Furthermore, we implement CoSDA in continual test-time adaptation setting, and find CoSDA is superior even in this setting. The table below shows the classification error on CIFAR-10-to-CIFAR-10C:\n| Method        | Mean |  \n| ------------- | ---- |  \n| Source        | 43.5 |  \n| BN Adapt      | 20.4 |  \n| Pseudo-label  | 19.8 |  \n| TENT          | 18.6 |  \n| CoTTA         | 16.2 |  \n| CoSDA         | 15.1 |  \n\n**[Q1]** The rationale for choosing Mixup as augmentation is discussed in Section 3.1 and Appendix A.1. We have tried CutMix and the result is not as well.\n\n**[Q2]** By default, all current SFDA works discussed in the paper train on the whole unlabeled dataset, so we follow the setting.\n\n**[Q3]** The stochastic restore part is not removed, which can varified from our source code(in supplementary materials). We implement CoSDA in continual test-time adaptation setting, and find CoSDA is superior even in this setting. The table below shows the classification error on CIFAR-10-to-CIFAR-10C:\n| Method        | Mean |  \n| ------------- | ---- |  \n| Source        | 43.5 |  \n| BN Adapt      | 20.4 |  \n| Pseudo-label  | 19.8 |  \n| TENT          | 18.6 |  \n| CoTTA         | 16.2 |  \n| CoSDA         | 15.1 |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6802/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485573437,
                "cdate": 1700485573437,
                "tmdate": 1700485573437,
                "mdate": 1700485573437,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6sfJ26lZDW",
                "forum": "Wure6HljpJ",
                "replyto": "5923NbtGMe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6802/Reviewer_GtVN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6802/Reviewer_GtVN"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. \n\n- **Relationship to CoTTA:** You mention that your setting is not about forgetting, however, the writing in the paper hints at the complete opposite. Here's a line from the caption in Figure 1: \"The pipeline of the proposed CoSDA method, utilizing a dual-speed optimized teacher-student model pair to adapt to new domains while avoiding forgetting.\" Furthermore, the reason CoTTA is updated in each epoch is because a single batch constitutes an epoch in TTA. I fail to find any distinct differences from the EMA approach and the so called \"dual-speed optimization\".\n- **Related works:** I clearly mention continual learning in general, not continual DA.\n- **Mixup:** I understand Mixup gives good results. As I mentioned, the \"why\" is not clearly defined. Section 3.1 lists \"why we chose Mixup\", not \"why Mixup works\". A specific data augmentation might not work for other datasets. \n\nBased on the response and the original comments, I will stick to my original score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6802/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686016662,
                "cdate": 1700686016662,
                "tmdate": 1700686016662,
                "mdate": 1700686016662,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RuFeGjpNVW",
            "forum": "Wure6HljpJ",
            "replyto": "Wure6HljpJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6802/Reviewer_qtge"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6802/Reviewer_qtge"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method for continual source-free domain adaptation (CoSDA), which transfers knowledge from a source-domain trained model to multiple target domains without accessing the source data. CoSDA uses a dual-speed optimized teacher-student model pair and consistency learning to mitigate forgetting and improve adaptation. CoSDA also incorporates mutual information loss to enhance robustness to hard domains. The paper evaluates CoSDA on four benchmarks and shows that it outperforms state-of-the-art methods in both single-target and multi-target sequential adaptation scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method is simple yet efficient. \n2. The experimental results are extensive, including a comparison of various baselines on different datasets. The results prove the effectiveness of the proposed method.\n3. Theoretical analysis and proofs are provided as necessary."
                },
                "weaknesses": {
                    "value": "1. The technique contribution is somewhat limited. The proposed method can be easily derived from existing methods. For example, the idea of the teacher-student framework[1] and the mix-up strategy[2] for continual source-free domain adaptation is nothing new. \n2. There is a need for deeper experiments. Could the authors test their proposed method on a variety of adaptation sequences, especially those that are more complex, instead of relying solely on one fixed combination sequence? In real-world scenarios, sequential target data might span a longer duration and encompass a wider range of distributions. For instance, the authors could establish longer sequences to evaluate the method for mitigating forgetting.\n3. In continual source-free domain adaptation. A primary challenge lies in managing the tradeoff between adapting to target domain and preventing the forgetting of previous domains. However, the proposed methods give limited attention to this challenge. The tradeoff is achieved through the EMA momentum m within a teacher-student learning framework in this paper. Thus, a hyper-parameter experiment of m should at least be conducted.\n\n[1] Wang, Qin, et al. \"Continual test-time domain adaptation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n[2] Taufique, Abu Md Niamul, Chowdhury Sadman Jahan, and Andreas Savakis. \"ConDA: Continual unsupervised domain adaptation.\" arXiv preprint arXiv:2103.11056 (2021)."
                },
                "questions": {
                    "value": "see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6802/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6802/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6802/Reviewer_qtge"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6802/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824304287,
            "cdate": 1698824304287,
            "tmdate": 1699636786155,
            "mdate": 1699636786155,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7S2Jr9dl37",
                "forum": "Wure6HljpJ",
                "replyto": "RuFeGjpNVW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6802/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6802/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response To Reviewer qtge"
                    },
                    "comment": {
                        "value": "Thanks for your time and valuable reviews, here are our responses:\n\n**[W1]** The technique contribution is somewhat limited. The proposed method can be easily derived from existing methods. For example, the idea of the teacher-student framework[1] and the mix-up strategy[2] for continual source-free domain adaptation is nothing new.\n\n**[W2]** We implement CoSDA in continual test-time adaptation setting, and find CoSDA is superior even in this setting. The table below shows the classification error on CIFAR-10-to-CIFAR-10C:\n| Method        | Mean |  \n| ------------- | ---- |  \n| Source        | 43.5 |  \n| BN Adapt      | 20.4 |  \n| Pseudo-label  | 19.8 |  \n| TENT          | 18.6 |  \n| CoTTA         | 16.2 |  \n| CoSDA         | 15.1 |  \n\n**[W3]** The dual-speed optimization strategy is the module designed for continual SFDA. The student model captures short-term features and update to adapt to target domains, while the teacher model filters out long-term domain-invariant features. The EMA momentum does not have much impact, so we fix the parameter over all experiments. Specifically, we follow the settings in MoCo and BYOL and increase the momemtum from 0.9 to 0.99 using a cosine schedule, as stated at the end of Section 3. We will carry out experiments to demonstrate that afterwards. Thanks for your advice!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6802/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485480551,
                "cdate": 1700485480551,
                "tmdate": 1700485480551,
                "mdate": 1700485480551,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YJGBsZxaVM",
            "forum": "Wure6HljpJ",
            "replyto": "Wure6HljpJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6802/Reviewer_XnHS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6802/Reviewer_XnHS"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on mitigating catastrophic forgetting in the context of source-free domain adaptation. The authors take several steps to address this challenge, including re-implementing existing methods and introducing a novel approach called CoSDA, which leverages a teacher-student model to achieve continuous adaptation. Specifically, the authors introduce a consistency loss based on KL-divergence to transfer knowledge from the teacher network to the student network. They also employ a KL-divergence-based regularization loss to stabilize training with Mixup augmentations. Additionally, the authors present two distinct optimization strategies for updating the teacher and student networks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation for using Mixup for the data augmentation is clear and well-described.\n\n2. Extensive experiments and ablation studies are performed."
                },
                "weaknesses": {
                    "value": "## Majors\n\n1. I have concerns regarding the formulation of consistency loss which is the main contribution of the work. Equation 1 is confusing. From my understanding, h_{\\psi}(\\tilde{x}) should refer to the output logits from the student network. If so, minimizing the divergence between two distributions from different mathematical spaces does not make any sense.  To be specific, \\tilde{p} is a softmax probability vector with each element in the range [0,1], while h_{\\psi}(\\tilde{x}) has elements ranging from negative infinity to positive infinity.\n\n2. Please add an ablation study to demonstrate the potential issue of the proposed consistency loss collapsing. My suspicion is that this collapse occurs due to the divergence between two distinct mathematical spaces.\n\n3. In batch normalization, the mean and variance are computed based on the activations within each mini-batch, which is a subset of the entire dataset. The statistics are calculated separately for each mini-batch as the model processes the data during training. This is what allows batch normalization to adapt to the statistics of the current batch and helps in stabilizing and accelerating training. However, the authors mentioned that their mean and variance are calculated based on the whole dataset for batch normalization, which does not make sense. As per my understanding, the moving average mean and variance are computed by accumulating the mean and variance from each batch using an exponential moving average formula (\\alpha*\\mu_{moving} + (1-\\alpha)*\\mu_{i}). The moving average mean and variance are not the mean and variance of the whole dataset. The lack of clarity on batch normalization in the paper raises concerns about its overall quality.\n\n4. I may have a misunderstanding regarding the evaluation settings; however, based on my interpretation of the paper, it appears that the proposed method does not demonstrate significant improvements over the baseline approaches in terms of target domain classification accuracies. I kindly request the authors to provide a more detailed explanation or consider revising the experiment section for improved clarity on this matter.\n\n## Minors\n\n1. The proposed work appears to be closely aligned with federated learning. I am curious about the authors' motivation for framing this work as a sequence of source-free domain adaptation. To me, the workflow seems to have stronger connections to federated learning rather than source-free domain adaptation. Thus, the literature on federated learning should be given.\n\n3. typos: 3.1 \u201dto consist with\u201d -> \u201cto be consistent with\u201d"
                },
                "questions": {
                    "value": "1. In the second paragraph of the introduction, the authors mentioned that \u201cSFDA also allows for spatio-temporal separation of the adaptation process since the model training on source domain is independent of the knowledge transfer on target domain\u201d. What does the spatio-temporal separation refer to? \n\n2. In Equation 1, is h_{\\psi}(\\tilde{x}) referring to the logits or the softmax probabilities? If it represents the logits, it raises a question regarding the use of KL-divergence between two distributions: \\tilde{p}, which is a softmax probability vector with each element in the range [0,1], and h_{\\psi}(\\tilde{x}), which has elements ranging from negative infinity to positive infinity.\n\n3. I do not quite understand the in-sequence evaluation settings. From the result tables, the authors listed, most baselines perform better without the proposed methods. Then, how could the readers evaluate the effectiveness of the proposed methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concerns were raised."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6802/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699140035527,
            "cdate": 1699140035527,
            "tmdate": 1699636786043,
            "mdate": 1699636786043,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xPUXiZhUum",
                "forum": "Wure6HljpJ",
                "replyto": "YJGBsZxaVM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6802/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6802/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response To Review XnHS"
                    },
                    "comment": {
                        "value": "Thanks for your time and valuable reviews, here are our responses:\n\n**[W1]** Both $\\tilde{p}$ and $h_{\\psi}(\\tilde{x})$ are in the range [0, 1]. The hard-label from teacher is computed as $p := softmax(h_{\\theta}(X)/\\tau)$, while $h_{\\psi}(\\tilde{x})$ denotes logits normalized by softmax function.\n\n**[W2]** As stated above, there is no distinct divergence in mathematical spaces. Also, ablation study is already provided as results (-) MI. You can find collapse in Table1 between columns \"CoSDA\" and \"CoSDA (-) MI\".\n\n**[W3]** Yes, the moving average mean and variance are not the mean and variance of the whole dataset. They can be viewed as weighted mean, instead. As the adaptation process goes on, the mean and variance should be more and more precise, so later batches should have higher weight.\n\n**[W4]** CoSDA outperforms all outher method either on adaptation accuracy(shown in green) or accuracy drop(shown in red), achieving best trade-off. For example, as reviewer nYkW metioned, comparing with EdgeMix in table1, in terms of the accuracy drop, our method is clearly at least 3x smaller than EdgeMix. Combined with other modules, our method lowers about 2x accuracy drop on source domain with neglectable decrease in adaptation accuracy.\n\n**[Minors]** The core idea of our paper is that CoSDA achieves \"continual SFDA\", while in federated setting, the predictions from multiple clients are gathered and averaged without the concept of \"continual\". Thanks for pointing out our typos.\n\n**[Q1]** During each adaptation process, we have only unlabeled data from current domain. Neither data nor weights from previous domains are stored. Therefore, each adaptation process can be performed at different time and location, which we refer to as spatio-temporal separation.\n\n**[Q2]** $h_{\\psi}(\\tilde{x})$ refers to softmax probabilitie.\n\n**[Q3]** CoSDA outperforms all outher method either on adaptation accuracy(shown in green) or accuracy drop(shown in red), achieving best trade-off. Ideally, adaptation accuracy should be as high as possible, while accuracy drop as small as possible."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6802/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485356084,
                "cdate": 1700485356084,
                "tmdate": 1700485356084,
                "mdate": 1700485356084,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UfOY1WsBPF",
                "forum": "Wure6HljpJ",
                "replyto": "YJGBsZxaVM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6802/Reviewer_XnHS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6802/Reviewer_XnHS"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Author's Response"
                    },
                    "comment": {
                        "value": "Thank you for the response.\n\nHowever, most of my concerns are not addressed properly. I will keep my score.\n\n[**W1** and. **W2**] then why you used $h$ for different mathmatical spaces? Clearly $h_{\\theta}$ and $h_{phi}$ point to different mathematical spaces based on the author's response, so why no clarification is made in the manuscript? It is a standard for academic writing to use consistent notations for the variables from the same mathematical space.\n\n[**W3**] The authors acknowledged my concern but did not make any revisions to their manuscript or give any plan to address my concern.\n\n[**W4**] My concern is the clarity of the table presentation. Readers find it hard to gather the key information (mentioned by the authors) from the current manuscript. Again, the authors did not propose any revision to address this.\n\n[**Minor**] Then, the literature review should be given to federated learning.\n\n[**Q1**] The authors addressed this question very well.\n\n[**Q2**] Again, the same notation $h$ should not be used to mention variables from different mathematical spaces.\n\n[**Q3**] Again, my concern is the clarity of the tables, not the performance lift. The lack of clarity in the experiment section makes readers feel hard to find the technical contributions of the proposed work."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6802/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598105601,
                "cdate": 1700598105601,
                "tmdate": 1700720751963,
                "mdate": 1700720751963,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JDqBDvi34e",
            "forum": "Wure6HljpJ",
            "replyto": "Wure6HljpJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6802/Reviewer_nYkW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6802/Reviewer_nYkW"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a simple continual SFDA framework, where a teacher-student framework is applied. The teacher model provides the hard labeling for the student model, and the student model will be penalized with the divergence from the teacher hard label, as well as a mutual information maximization regularization. There are extensive experiments compared to other SFDA methods and the baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well organized with sufficient background introduction.\n2. The paper provides illustrative figures and diagrams for readers to better understand their proposal.\n3. The paper conducts extensive experiments on DomainNet, OfficeHome and VisDA, which are the main DA datasets, and show fair performance on those SFDA tasks."
                },
                "weaknesses": {
                    "value": "1. The proposed method lacks novelty in terms of the framework design. For example, the teacher-student architecture has been proposed from the self-supervised learning framework, e.g., MoCo. The mix-up is another already invented technique to augment both the input space and the label space. The consistent loss has been utilized in those above mentioned methods already.\n\nMeanwhile, the mutual information maximization is a widely applied regularization technique during representation learning. Combining all, I think the novelty of the design is hard to justify.\n\n2. The paper claims continual SFDA, where from the method design, there is no specific module is designed to deal with the model catastrophic forgetting issue, except accepting the teacher model\u2019s hard label to measure the KL divergence from the student prediction on the mixed up sample. \n\nThe teacher model is leveraging exponential averaging, but the student model is not in any manner distilled from the teacher model. The model weights forgetting is not addressed in any technical design.\n\n3. Across the compared methods, the proposed CoSDA does not show advantageous results over the other methods. For example, in Table 1, CosDA is not compellingly better than EdgeMix, where actually many of the DA protocols EdgeMix shows better results.\n\nIn Table 2, CosDA even combined with some other modules, such as NRC or AaD, is not better than AaD. This suggests that the proposed way does not achieve as advantageous performance as the state-of-the-art methods."
                },
                "questions": {
                    "value": "Please refer to the weakness session for more detail."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6802/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6802/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6802/Reviewer_nYkW"
                    ]
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6802/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699211524379,
            "cdate": 1699211524379,
            "tmdate": 1699636785937,
            "mdate": 1699636785937,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1XOnRmTJya",
                "forum": "Wure6HljpJ",
                "replyto": "JDqBDvi34e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6802/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6802/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response To Reviewer nYkW"
                    },
                    "comment": {
                        "value": "Thanks for your time and valuable reviews, here are our responses:\n\n**[W1]** The novelty of our method lies in dual-speed optimization strategy. Specifically, the student model is updated every batch, while the teacher model is updated every epoch. To the best of our knowledge, we are the first to propose such strategy.\n\n**[W2]** The dual-speed optimization strategy is the module designed for continual SFDA. The student model captures short-term features and update to adapt to target domains, while the teacher model filters out long-term domain-invariant features. \n\n**[W3]** CoSDA outperforms all outher method either on adaptation accuracy(shown in green) or accuracy drop(shown in red), achieving best trade-off. In your example, in terms of the accuracy drop, our method is clearly at least 3x smaller than EdgeMix. Combined with other modules, our method lowers about 2x accuracy drop on source domain with neglectable decrease in adaptation accuracy."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6802/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485274603,
                "cdate": 1700485274603,
                "tmdate": 1700485274603,
                "mdate": 1700485274603,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]