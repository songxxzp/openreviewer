[
    {
        "title": "Rethinking Model Ensemble in Transfer-based Adversarial Attacks"
    },
    {
        "review": {
            "id": "AbpVUQZG9u",
            "forum": "AcJrSoArlh",
            "replyto": "AcJrSoArlh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission30/Reviewer_GTKq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission30/Reviewer_GTKq"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors rethink model ensemble in black-box adversarial attacks: exploring the relationship among the transferability of adversarial samples, the Hessian matrix\u2019s F-norm, and the distance between the local optimum of each model and the convergence\npoint. Based on the above theoretical analysis, the authors define common weaknesses and propose effective algorithms\nto find common weaknesses of the model ensemble."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The experiments are solid and compresive: the author consider image classification, object detection and large vision-language models, and the results demonstrate the effectivenss of the proposed method.\n- The authors introduce extensive mathmatical proofs, which makes the proposed method more convincing."
                },
                "weaknesses": {
                    "value": "- The description of the experimental part could be improved. Take \"Results on normal models\" as an example. In the experiment setup, the authors choose MI, VMI and SSA to validate the effectiveness of proposed CWA. However, in \"Results on normal models\", the authors only discuss MI, and ignore the VMI and SSA. I think the authors should pay more attention on the recent SOTA (e.g., SSA), since MI was proposed in 2018 (5 years ago)."
                },
                "questions": {
                    "value": "- How to explain the significant performance degradation of VMI-CWA compared to VMI in Tab 1when the target models are Swin-S and Max ViT-T?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission30/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission30/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission30/Reviewer_GTKq"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission30/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697867552117,
            "cdate": 1697867552117,
            "tmdate": 1699635926434,
            "mdate": 1699635926434,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1vC7XEEJLO",
                "forum": "AcJrSoArlh",
                "replyto": "AbpVUQZG9u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission30/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission30/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable review"
                    },
                    "comment": {
                        "value": "Thank you for appreciating our new contributions as well as providing the valuable feedback. We have uploaded a revision of our paper. Below we address the detailed comments, and hope that you may find our response satisfactory.\n\n***Question 1: The description of the experimental part could be improved.***\n\nThank you for the suggestion. We have improved the description of the experimental results and discussed the results of VMI and SSA in the revision. The detailed discussion is as follows.\n\n> By integrating our proposed CWA with recent state-of-the-art attacks VMI and SSA, the resultant attacks VMI-CWA and SSA-CWA achieve a significant level of attacking performance. Typically, SSA-CWA achieves more than 99\\% attack success rates for most normal models. VMI-CWA and SSA-CWA can also outperform their vanilla versions VMI and SSA by a large margin. The results not only demonstrate the effectiveness of our proposed method when integrating with other attacks, but also prove the vulnerability of existing image classifiers under strong transfer-based attacks.\n\n***Question 2: How to explain the significant performance degradation of VMI-CWA compared to VMI in Tab 1 when the target models are Swin-S and Max ViT-T?***\n\nWe are sorry that we showed the wrong results of VMI-CWA in Table 1. In our submitted code, we implemented two versions of VMI-CWA. The first one is based on Algorithm 4 in Appendix B.3, in which we adopt the gradients of VMI and perform iterative updates based on our algorithm. The second version is slightly different, which adopts the gradients of our method to perform updates based on VMI. In practice, we observed that the first version performs better, but we showed the results of the second version in Table 1. In the revision, we have corrected the results of VMI-CWA in Table 1. Now the performance of VMI-CWA is better than VMI for the transformers Swin-S and MaxViT-T."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission30/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700019818915,
                "cdate": 1700019818915,
                "tmdate": 1700019818915,
                "mdate": 1700019818915,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uGGeIKK9hf",
                "forum": "AcJrSoArlh",
                "replyto": "1vC7XEEJLO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission30/Reviewer_GTKq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission30/Reviewer_GTKq"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Could you present the experimental results of Q1 in a table?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission30/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700566585973,
                "cdate": 1700566585973,
                "tmdate": 1700566585973,
                "mdate": 1700566585973,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yDbleCm8ln",
                "forum": "AcJrSoArlh",
                "replyto": "45Gf2uZ26h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission30/Reviewer_GTKq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission30/Reviewer_GTKq"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your reply (2)"
                    },
                    "comment": {
                        "value": "I want to get this results: \"Typically, SSA-CWA achieves more than 99% attack success rates for most normal models. VMI-CWA and SSA-CWA can also outperform their vanilla versions VMI and SSA by a large margin\""
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission30/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706116214,
                "cdate": 1700706116214,
                "tmdate": 1700706116214,
                "mdate": 1700706116214,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fU4c43P7nZ",
                "forum": "AcJrSoArlh",
                "replyto": "AbpVUQZG9u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission30/Reviewer_GTKq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission30/Reviewer_GTKq"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your reply (3)"
                    },
                    "comment": {
                        "value": "Thanks for your reply. My concern has been addressed and I will keep the rating, i.e., tend to accept this paper."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission30/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740029497,
                "cdate": 1700740029497,
                "tmdate": 1700740051777,
                "mdate": 1700740051777,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ytRIyfQmu3",
            "forum": "AcJrSoArlh",
            "replyto": "AcJrSoArlh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission30/Reviewer_9Y8Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission30/Reviewer_9Y8Q"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the transfer-based attack towards the ensemble of models. The authors suggest that the ensemble model would have \"common weaknesses\" that are strongly correlated with adversarial examples' transferability and propose the common weaknesses attack (CWA). Theoretically, this paper provides clear intuition on why the models have common weaknesses. This paper also conducts extensive experiments to validate their theoretical findings."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper conducts extensive experiments on several datasets, demonstrating that CWA receives superior results than previous methods.\n2. The intuition of common weakness is strong and clear. This paper converts the task \"crafting adversarial examples on ensemble models\" to \"optimizing the second term in Equation (2)\". By Theorem 3.1, this term is further decomposed into a \"flatness term\" and a \"closeness term\". These two terms can be efficiently optimized by SAM and CSE."
                },
                "weaknesses": {
                    "value": "1. The intuition behind SAM and CSE is not clear enough. Equation (4) and (5) is confusing for those readers not familiar with this area.\n2. In Section 3.1, the authors mentioned that \"the goal of transfer-based attacks is to craft an adversarial example $x$ that is misclassified by all models in $\\mathcal{F}$\". However, fooling all target models seems to be an impossible job in practice. Besides, the experiments in this paper cannot support this claim.\n3. The perturbation in Figure 3 is not imperceptible to humans."
                },
                "questions": {
                    "value": "See the Weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I have no ethical concerns."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission30/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698744361782,
            "cdate": 1698744361782,
            "tmdate": 1699635926225,
            "mdate": 1699635926225,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b4aL72SmCv",
                "forum": "AcJrSoArlh",
                "replyto": "ytRIyfQmu3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission30/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission30/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable review"
                    },
                    "comment": {
                        "value": "Thank you for appreciating our new contributions as well as providing the valuable feedback. We have uploaded a revision of our paper. Below we address the detailed comments, and hope that you may find our response satisfactory.\n\n***Question 1: The intuition behind SAM and CSE is not clear enough.***\n\nThank you for pointing this out. SAM is an effective method to acquire a flatter landscape, which is formulated as a min-max optimization problem, in a similar way to adversarial training. The inner maximization aims to find a direction along which the loss changes more rapidly;\nwhile the outer problem minimizes the loss at this direction to improve the flatness of loss landscape. We have added the introduction of SAM in the beginning of Section 3.3. \n\nFor CSE, to maximize the cosine similarity between the gradients of different models, we are inspired by Nichol et al. [1] to develop a first-order algorithm. Nichol et al. propose a meta-learning algorithm that iteratively performs gradient updates for each task, and they theoretically show that this procedure can increase the inner product between gradients of different tasks. We extend this method to also increase the cosine similarity between the gradients of different models, with theoretical analyses in Appendix B.2.\n\n\n***Question 2: Overclaim on fooling all target models.***\n\nThank you for pointing this out. We have revised our paper to avoid this overclaim. Now the text becomes\n\n> Given a natural image $\\boldsymbol{x}_{nat}$ and the corresponding label $y$, transfer-based attacks aim to craft an adversarial example $\\boldsymbol{x}$ that could be misclassified by the models in $\\mathcal{F}$.\n\n***Question 3: The perturbation is Figure 3 is not imperceptible to humans.***\n\nWe are sorry that the first image in Figure 3 was placed wrongly -- we used the adversarial image with $\\epsilon=32/255$, such that this image is more perceptible. In the revision, it is changed to the correct image with $\\epsilon=16/255$. The perturbations are visually similar to those generated by other attack methods with the same perturbation budget.\n\n**Reference:**\n\n[1] Nichol et al., On first-order meta-learning algorithms, 2018."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission30/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700019690028,
                "cdate": 1700019690028,
                "tmdate": 1700019690028,
                "mdate": 1700019690028,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yQGOqDX52W",
                "forum": "AcJrSoArlh",
                "replyto": "b4aL72SmCv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission30/Reviewer_9Y8Q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission30/Reviewer_9Y8Q"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your reply"
                    },
                    "comment": {
                        "value": "The authors' reply addressed most of my concerns. The intuition behind SAM and CSE seems adequate to me. I hope that the authors can include them in future revisions so as to improve the overall interpretability of your method."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission30/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700030440139,
                "cdate": 1700030440139,
                "tmdate": 1700030440139,
                "mdate": 1700030440139,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GaV21kBKdY",
            "forum": "AcJrSoArlh",
            "replyto": "AcJrSoArlh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission30/Reviewer_JLTP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission30/Reviewer_JLTP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method for constructing adversarial examples for black-box models, using an ensemble of surrogate models. Using a second order approximation of the adversarial loss function, the authors construct an upper bound using the Hessian on the adversarial loss, and establish approximate methods for minimizing the upper bound, yielding their proposed attack: Common Weakness Attack (CWA). The CWA algorithm consists of two steps: first a gradient ascent step on the averaged logits, inspired by sharpness-aware minimization, and a second gradient descent step using their proposed cosine similarity encourager (CSE). The authors evaluate their attack method on a variety of model architectures using the NIPS2017 dataset, and compare with a variety of different attack algorithms from the literature."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper tackles an important subject, understanding how vulnerable can ML models be is crucial for safe-guarding them against possible adversaries\n\n- The proposed method, as far as I can tell, seems novel to me.\n\n- The strengths of this paper are primarily in the effectiveness of their proposed method, as it seems to combine very well with prior attacks (e.g, MI-CWA, SSA-CWA) and achieves superior results. The transfer-based black-box attacks against Bard is interesting.\n\n- The proposed algorithm seems reasonable to implement"
                },
                "weaknesses": {
                    "value": "- I felt that the paper was bit hard to follow. Section 3 mixes a lot of prior results with proposed ones. Thus it makes it a bit hard to distinguish original contributions vs reusing prior results. For instance, it would greatly improve the readability if SAM was properly explained prior to this section (or as a subsection). Figures 1 and 2 are not very informative (see Questions), they lack axis and/or labels. It would also improve the quality of the paper if the authors include a mathematical description of some of the prior ensemble attack methods (e.g. is it simply PGD on the averaged logits?), to better understand the contrast between them and the proposed method.\n\n- Despite the compactness of Algorithm 1, there is a lack of analysis on its complexity. How does it compare (in terms of wall-clock time) to prior attacks or vanilla PGD-style attacks?"
                },
                "questions": {
                    "value": "- In Fig 1, it is not clear what is being plotted, and the different colors are not well contrasted. \n- In Sec 3.1, below eq (2), the authors write \" ... we can see that a smaller value of ... means a smaller test error ...\". What is the test error in this case? is it the error on the black-box model? the term test error is rather confusing.\n- In Theorem 3.1, there is a typo, should be $||H_i||_F$? \n- In Sec 3.2: \" There are some researches ...\" typo\n- In Fig 2, what are $x_t$, $x_t^f$, .... The symbols are not defined. \n- In Alg1, does the order of the classifiers in the for loop matter? if so how is it currently chosen?\n- In Table 1, some of the highlighted numbers are not the best performing numbers, for instance the FastAT RN50 row. \n- Can the proposed method be also applied in the white-box setting? i.e. assume $\\mathcal{F} = [f]$ and simply attack the classifier.  Have the authors experimented with something like this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission30/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814225713,
            "cdate": 1698814225713,
            "tmdate": 1699635926146,
            "mdate": 1699635926146,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fgtiJyIxAK",
                "forum": "AcJrSoArlh",
                "replyto": "GaV21kBKdY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission30/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission30/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable review"
                    },
                    "comment": {
                        "value": "Thank you for the supportive review. We are encouraged by the appreciation of the significance, novelty, and effectiveness of our method. We have uploaded a revision of our paper. Below we address the detailed comments, and hope that you may find our response satisfactory.\n\n\n***Question 1: The paper was bit hard to follow.***\n\nThank you for pointing out the issues. We further improve the writing to address them in the revision. Below are the details.\n\n- **Improve the readability of SAM.** We have provided a brief introduction of SAM in the beginning of Section 3.3, to provide the background knowledge of SAM and explain why it can be used for our problem. \n\n- **Figures 1 and 2 are not very informative.** We have updated Figures 1 and 2 to be more informative, including adding the axes and labels, changing the colors, and specifying the symbols.\n\n- **Include a mathematical description of the prior ensemble attacks.** We have added a new subsection 3.1 in the revision to introduce the backgrounds of adversarial attacks.\n\nWe hope that the new revision can be easier to follow. We will further improve the paper in the final.\n\n***Question 2: Lack of analysis on complexity.***\n\nThank you for the suggestion. We discussed the computational efficiency in Section 4.4 and Appendix D.4. We further report the actual runtime of different attack methods in Table D.2.  Our method incurs only a slight\ncomputational overhead compared to the baseline attacks. This demonstrates that our method can\nserve as a plug-and-play algorithm to effectively and efficiently enhance transfer attack performance. As also discussed in Section 4.4, given a fixed computation overhead, we can use a smaller number of attack iterations in our method, which also leads to better results, as shown in Figure 4(c).\n\n***Question 3: Various typos and minor problems.***\n\nThank you for pointing them out. We address these issues and carefully proofread the paper. We have uploaded a new revision. Below are the details.\n\n- **What is being plotted in Figure 1.** Figure 1 shows an illustration of common weakness, where we plot the synthetic loss curves of different models w.r.t. the input $\\boldsymbol{x}$. We show that the generalization performance is correlated with the flatness of loss landscape and the distance to the local optimum of each model.\n\n- **Typos in Sec. 3.1, Theorem 3.1, Sec. 3.2.** Thank you for pointing them out. We have corrected them in the revision.\n\n- **The symbols in Figure 2 are not defined.** We have revised the caption to link the symbols to the equations in the main text.\n\n- **The order of classifiers in Alg. 1.** The order of classifiers is randomly set and fixed during the optimization process.\n\n- **Some highlighting numbers are not the best in Table 1.** Thank you for pointing this out. We carefully checked the results and marked the best results in bold in the revision.\n\n- **Can the method be applied in the white-box setting.** Yes, our method can be applied in the white-box setting. But when there is only one white-box model, our method would degenerate to PGD or MI since we focus on ensemble-based attacks. Therefore, we further show the attack performance of our method against the six white-box models in the ensemble, as detailed in Section 4.1, and compare with PGD-10 and MI. The results are shown below.\n\n|         | Epsilon | ResNet18 | ResNet34 | ResNet50 | ResNet101 | ResNet50 (Salman et al. (2020)) | XCiT-S12 (Debenedetti et al. (2023)) |\n|---------|:-------:|:--------:|:--------:|:--------:|:---------:|:-------------------------------:|:-----------------------------------:|\n| PGD     | 16/255  |   98.1   |   98.3   |   98.2   |   98.4    |              34.6               |                19.8                 |\n| MI | 16/255  |   98.5   |   98.6   |   98.4   |   98.5    |              41.9               |                27.0                 |\n| MI-CWA  | 16/255  |  100.0   |  100.0   |  100.0   |   100.0   |              91.1               |                92.2                 |\n| PGD     |  4/255  |   96.2   |   95.9   |   96.1   |   96.2    |              27.2               |                15.5                 |\n| MI |  4/255  |   97.0   |   96.8   |   97.1   |   97.4    |              28.0               |                16.2                 |\n| MI-CWA  |  4/255  |  100.0   |  100.0   |  100.0   |   100.0   |              47.0               |                39.1                 |\n\nIt can be seen that our method also leads to higher attack success rates over PGD and MI with a small number of attack iterations. It indicates that our method converges much faster than the baselines since it can align the gradients of different models."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission30/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700019531956,
                "cdate": 1700019531956,
                "tmdate": 1700019531956,
                "mdate": 1700019531956,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rj1s0Mf5Zr",
            "forum": "AcJrSoArlh",
            "replyto": "AcJrSoArlh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission30/Reviewer_s2Ct"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission30/Reviewer_s2Ct"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel adversarial attacks (Common Weakness Attack) against ensembles of model. It composes of two components, sharpness aware minimization (SAM) and cosine similarity encourager (CSE) that regularize the flatness of the loss landscape and the closeness to the optimum respectively. The author performs extensive experiments over extensive datasets and models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written in general and has very clear motivation and mathematical formulations. The derivation of attack based on the second order appropriation is intuitive. \n2. The author conducts very extensive empirical studies with many choices of architectures and datasets."
                },
                "weaknesses": {
                    "value": "1. In table 1, the author only provides the results of CWA in combintation with other methods. Is there results for CWA only, and how well it performs. \n2. As an ablation study, the author might want to try different norm decomposition, e.g. the operator norm of H.\n3. The author should clarify the novelty compared to the previous methods. Specially, MI, VMI, SSA are all existing proposed attacks. The sharpness aware minimization techniques have been previously used."
                },
                "questions": {
                    "value": "1. Can the author explain why the model is able to achieve significantly more effective attacks compared the existing methods for the adversarial trained model? I am mainly concerned about the fairness of the evaluation. Also, can the author explain how the AT model trained for ensembles in their experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission30/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819801434,
            "cdate": 1698819801434,
            "tmdate": 1699635926044,
            "mdate": 1699635926044,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4ReGdYkZdg",
                "forum": "AcJrSoArlh",
                "replyto": "rj1s0Mf5Zr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission30/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission30/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable review"
                    },
                    "comment": {
                        "value": "Thank you for the supportive review. We are encouraged by the appreciation of the novelty, clarity, and thorough experiments of this paper. We have uploaded a revision of our paper. Below we address the detailed comments, and hope that you may find our response satisfactory.\n\n***Question 1: The performance of CWA only.***\n\nThank you for the suggestion. We further conduct experiments of SAM, CSE, and CWA without integrating with other methods. The detailed results are shown in Table C.5 in Appendix C.7. The average performance of different methods over all black-box models are shown below.\n\n| Method | FGSM | BIM | MI | SAM | CSE | CWA | MI-SAM | MI-SCE | MI-CWA | \n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | \n| Attack success rate (\\%) | 50.89 | 63.71 | 71.65 | 73.38 | 85.85 | 82.55 | 77.92 | 90.68 | 90.76 |\n\nIt can be seen that SAM, CSE, and CWA outperform the baselines FGSM, BIM, and MI by a large margin. However, their performance is inferior to their counterparts with MI, indicating that our methods and MI are orthogonal and can be integrated together to further improve the performance.\n\n***Question 2: Different norm decomposition.***\n\nThank you for the suggestion. It is true that we can decompose the second-order term $(\\boldsymbol{x}-\\boldsymbol{p}_i)^\\top \\boldsymbol{H}_i (\\boldsymbol{x}-\\boldsymbol{p}_i)$ with different norms (see Appendix A.1 for a more general result). However, with a different norm of $\\boldsymbol{H}_i$, it is hard to interpret the meaning of minimizing this norm, and we can hardly associate it with generalization/transferability of adversarial examples. Besides, we need to calculate the third-order derivative of the loss function w.r.t. input $\\boldsymbol{x}$, which would be intractable for deep neural networks. Therefore, it is hard to deal with other norms of $\\boldsymbol{H}_i$, and we only focus on the Frobenius norm of  $\\boldsymbol{H}_i$ with a clear connection to loss flatness and an alternative method based on sharpness aware minimization for optimization.\n\n***Question 3: Clarify the novelty compared to the previous methods.***\n\nThe main novelty of this paper lies in a new adversarial attack method from the perspective of model ensembling. Specifically, we use a quadratic approximation of the loss function and discover that the second-order term can be upper bounded by the product of the Frobenius norm of the Hessian matrix and the distance to the local optimum of each model. Therefore, we propose a common weakness attack to minimize the upper bound with great effectiveness. The existing attacks (e.g., MI, VMI, SSA) mainly focus on developing optimization algorithms or data augmentation techniques to improve transferability, while do not consider model ensembling in adversarial attacks. The sharpness aware minimization (SAM) is commonly used to train deep neural networks, but we extend it to adversarial attacks to improve the transferability based on our analysis.\n\n***Question 4: Why is the method more effective for adversarial trained model?***\n\nThe main reason is that our method can better exploit the vulnerabilities of different surrogate models in the ensemble by finding their common weakness. As we included two adversarially trained models (which are publicly available by [1,2]) in the ensemble, the adversarial examples generated by our method can better transfer to attack other black-box adversarially trained models. But for baseline attacks that directly average different surrogate models, they tend to attack easier models (i.e., normally trained models) [3], such that the useful gradient information would be overwhelmed by these normal models, leading to inferior results for adversarially trained models.\n\n**Reference:**\n\n[1] Salman et al., Do adversarially robust imagenet models transfer better? NeurIPS 2020.\n\n[2] Debenedett et al., A light recipe to train robust vision\ntransformers. IEEE Conference on Secure and Trustworthy Machine Learning, 2023.\n\n[3] Dong et al., Discovering adversarial examples with momentum, https://arxiv.org/abs/1710.06081v1."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission30/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700019158459,
                "cdate": 1700019158459,
                "tmdate": 1700019158459,
                "mdate": 1700019158459,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IINtakThM4",
                "forum": "AcJrSoArlh",
                "replyto": "4ReGdYkZdg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission30/Reviewer_s2Ct"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission30/Reviewer_s2Ct"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I think it addresses all my concerns."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission30/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700254285426,
                "cdate": 1700254285426,
                "tmdate": 1700254285426,
                "mdate": 1700254285426,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]