[
    {
        "title": "Multilingual Visual Speech Recognition with a Single Model using Visual Speech Unit"
    },
    {
        "review": {
            "id": "NxvYGS6Go5",
            "forum": "M8J0b9gNfG",
            "replyto": "M8J0b9gNfG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3646/Reviewer_RNNa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3646/Reviewer_RNNa"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a visual speech recognition model that can perform recognition on multiple languages. The model consists of a AV-HuBERT module that extracts visual speech units, followed by transformer module that converts visual speech units into text. The proposed model is trained in three steps. The first step focus on training mAV-HuBERT module by regressing visual input to discrete visual speech unit with supervision coming from pre-trained multilingual HuBERT model. The second step conducts pre-training on transformer module by taking visual speech unit extracted by mAV-HuBERT model and predict text. The final step conducts an end-to-end fine-tuning of all modules by taking visual input and predict text. Experimental evaluation demonstrated that the proposed mAV-HuBERT model can effectively perform multilingual visual speech recognition compared to mono-lingual method which has to be trained on individual language. The proposed training strategy also improved over naive multilingual model trained on all languages while reducing training time by using discrete visual speech unit as input during pre-training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper extended AV-HuBERT to effectively handle multilingual scenario. Compared to mono-lingual AV-HuBERT that is trained on individual language, mAV-HuBERT achieved best WER on three out of five languages and second-best on two out of five languages. The improvement on under-resourced languages is especially encouraging as the proposed method demonstrate a possibility to improve VSR on a specific language by leveraging other languages. \n2. The proposed training strategy of mAV-HuBERT not only help improve the recognition accuracy of AV-HuBERT on different languages but also improves training efficiency by leveraging discrete visual speech unit during pre-training, which uses much less data storage and thus allows much higher batch size and reducing training time. The strategy of curating multilingual language dataset used during training is also a contribution\n3. Analysis on visual speech unit provides insight on the unit such as the unit captures well on the viseme information rather than other information such as speaker identity."
                },
                "weaknesses": {
                    "value": "1. Although the overall approach and the problem the paper tackles are novel, the core model is largely based on an existing model AV-HuBERT with minimal modification. The pre-training objective is also commonly used without much modification. Perhaps the authors could clarify a bit more on any contribution regarding extending AV-HuBERT in model architecture if applicable. \n2. The comparison in Section 4.3.1 may not be completely fair. For AV-HuBERT, the model is pre-trained with English only, so fine-tuning on other languages means the English pre-trained AV-HuBERT is trained with same loss function on a new language, which will result in 5 different fine-tuned model i.e. one for each language. For mAV-HuBERT, a same model is supposed to work for different languages. So the fine-tuning is not supposed to be done on each language, which would yield 5 different models. If this is how the experiment was done, then mAV-HuBERT has advantage by design as it was pre-trained with more language data. If my understanding was not correct, then the authors should clarify on the specific process of fine-tuning of mAV-HuBERT.\n3. The comparison with multilingual VSR approaches is weak. The only comparison done was with AV-HuBERT as the authors claim there is no prior work that can perform multilingual VSR with a single model. However, there are recent work and reference therein indicate exploration along this direction. For example,\n- Cheng et al., MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup for Visual Speech Translation and Recognition, ICCV 2023\n- Anwar et al., Muavic: A multilingual audio-visual corpus for robust speech recognition and robust speech-to-text\ntranslation, 2023."
                },
                "questions": {
                    "value": "1. I'm curious on how the number of visual token size used to determine visual speech unit used in the first step affect the final performance. Do the authors vary the number (1000 being used in the paper) and choose the one with better performance?\n2. Regarding the curriculum learning, I'm also curious on how the learning schedule affect the performance. And when p% reaches 100%, the embedding from audio speech unit is useless. Do we disregard the embedding completely (thus no concatenation needed) or we still retain the same process to generate concatenated embedding?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3646/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3646/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3646/Reviewer_RNNa"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698604449357,
            "cdate": 1698604449357,
            "tmdate": 1700724303966,
            "mdate": 1700724303966,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Bg48yGeZWZ",
                "forum": "M8J0b9gNfG",
                "replyto": "NxvYGS6Go5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer RNNa"
                    },
                    "comment": {
                        "value": "### Firstly, we would like to thank the reviewer for his/her valuable comment and time spent. We took all comments thoughtfully and will reflect them in our future modifications.\n\n> Below is the response to the comment. \n\n**(Q1) The contribution regarding AV-HuBERT**\n\n**(A1)** The mAV-HuBERT has the same model architecture as AV-HuBERT model. The main difference is the pre-training dataset. We trained the mAV-HuBERT on 5,512 hours of multilingual data (9 languages), while the original AV-HuBERT is trained on English-only data of 1,759 hours. Please note that by building mAV-HuBERT, we can greatly improve the VSR performances for multilingual speech while the original AV-HuBERT model shows insufficient ability for modeling languages other than English. We will release the pre-trained weight of mAV-HuBERT which we expect to boost the research stream in multilingual and multimodal speech processing.\n\nIn addition to mAV-HuBERT, we would like to highlight the novelty of this paper.\n\n- To the best of our knowledge, **this is the first work exploring multilingual Visual Speech Recognition (VSR) using a single model**. Previous VSR models that explored multiple languages are language-specific, so the same number of models with the number of languages should be prepared to perform VSR for multiple languages. However, with the proposed method, we can perform it with just a single trained model. In this paper, we verified the effectiveness of the proposed method in 5 languages.\n\n- To the best of our knowledge, **this is the first work exploring visual-only speech units (i.e., visual speech units)**. Through analysis, we showed that visual speech units also correctly contain linguistic information similar to audio speech units. By employing the useful characteristics of the visual speech units (1. efficiency, 2. pseudo text), we first propose a method of pre-training the multilingual VSR model. We show the proposed method is very efficient and powerful through extensive evaluations.\n\n---\n\n**(Q2) Clarification of Section 4.3.1**\n\n**(A2)** We would like to clarify how the AV-HuBERT was finetuned, in Section 4.3.1. In this section, our objective is to understand the multilingual speech modeling ability of AV-HuBERT and mAV-HuBERT. Therefore, we finetuned both AV-HuBERT and mAV-HuBERT with the same multilingual dataset with the same objective to perform multilingual VSR with a single model. Consequently, one AV-HuBERT model and one mAV-HuBERT model were utilized to measure the performances in Table 2 in the manuscript. Through the experiments, we showed that English-only trained AV-HuBERT is not appropriate to be directly utilized in multilingual modeling purposes and the proposed mAV-HuBERT is more appropriate for multilingual speech modeling.\n\n---\n\n**(Q3) Prior multilingual VSR works**\n\n**(A3)** We would like to point out that the suggested references by the reviewer did not handle multilingual VSR.\n\n1. The paper [1] first explored the visual-to-text translation task. Their model can be pre-trained with audio corpus by their proposed speech mixing. However, their lip reading setting is not multilingual. They trained each model on each language dataset (Table 3 of [1]). \n\n2. The paper [2] mainly proposed a new multilingual audio-visual dataset. They only investigated the performances of Audio-based Speech Recognition (ASR) and Audio-Visual Speech Recognition (AVSR) on the proposed databases. Therefore, there was no multilingual VSR explored. Especially, they utilized the AV-HuBERT model directly and trained it on their databases to obtain ASR and AVSR. We would like to point out that no new architecture was proposed, and a dataset was proposed in their paper.\n\nTherefore, we would like to highlight that to the best of our knowledge, this is the first work exploring multilingual VSR using a single model. We will add the suggested references and discuss them in the manuscript.\n\n---\n\n**(Q4) About the visual token size**\n\n**(A4)** We evaluated the purity of the clusters in advance to train our model, following [3]. The purity for different cluster sizes (200, 500, 1000, 2000) is shown in Table R5-1. Basically, because the clustering is the process of quantizing the continuous visual features into fixed cluster sizes, the more the cluster size yields better representation quality. Therefore, there is a trade-off between the performance and efficiency aspects. We used 1000 clusters which can be expressed with 10 bits, following the previous work [4].\n\n* **Table R5-1**. Clustering purity using different cluster sizes.\n| 200 | 500 | 1000 | 2000 |\n| :--------: | :--------: | :--------: | :--------: |\n| 20.71% | 31.06% | 35.38% | 43.51% |\n\n---"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700369410583,
                "cdate": 1700369410583,
                "tmdate": 1700369410583,
                "mdate": 1700369410583,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dVoCv5sqrO",
                "forum": "M8J0b9gNfG",
                "replyto": "NxvYGS6Go5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3646/Reviewer_RNNa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3646/Reviewer_RNNa"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks the authors for responding to my review. Most of my concerns are addressed and the authors promised to incorporate the response into the final version. Considering the overall novelty and promising experimental results. I'm willing to change my rating from marginally below the acceptance threshold to be marginally above the acceptance threshold."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724276198,
                "cdate": 1700724276198,
                "tmdate": 1700724336309,
                "mdate": 1700724336309,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "E1F4Gmr6CR",
            "forum": "M8J0b9gNfG",
            "replyto": "M8J0b9gNfG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3646/Reviewer_tK3Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3646/Reviewer_tK3Y"
            ],
            "content": {
                "summary": {
                    "value": "The paper extends a recent work on audio speech units to the audio-visual (AV) domain and tackle the associated challenge of the need for large amounts of labeled training and the computational cost. In doing to, they also extend the previous work on English language AV that leveraged visual phoneme units to a single-model solution for several popular languages."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. There is solid engineering of the final system, with sufficient details to support reproducibility.\n2. Builds on the recent state-of-the-art and leverages the nifty insights in training large scale ASR across languages and modalities.\n3. Good amount of ablation studies to demonstrate the efficacy of various system components.\n4. Source code, models, and samples (to be?) made publicly available."
                },
                "weaknesses": {
                    "value": "1. The reference list and the related work sections are relatively misleading for the title of the paper. Either the term, \"neural network/deep learning models,\" or a similar qualifier has to be added to the title and abstract or the related work should be expanded to include the historical work in the area, prior to the use of deep learning models.\n2. Section 3, particularly 3.2 is hard to follow with it not being self-contained. I recommend bringing the results that justify the variety of modeling and training choices being described here into this section itself.\n3.  This work leverages good tips and engineering practices in building the mAV-HuBERT solution but is obscures the focus from what is the big science/research delta between AV-HuBERT and mAV-HuBERT."
                },
                "questions": {
                    "value": "1. What is the justification of hyperparameter choices such as, \"We use the target size of 1,000 and train the model for 350k steps with one iteration?\"\n2. \"...reduce training speed by removing the visual front-end.\"  Unsure why this is a good thing, unless you are referring to reducing the step size.\n3. Why does the curse of multi-linguality only affecting the results for English? How does the relative sizes of training data in various languages affect this?\n4. \"After 70% of training, p is set to 100 so that only visual speech units are used.\" What is magical about 70?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No additional concerns with this work beyond what the ones that affect the publicly available data that this work leverages."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782597123,
            "cdate": 1698782597123,
            "tmdate": 1699636320744,
            "mdate": 1699636320744,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GJP6bA3ojd",
                "forum": "M8J0b9gNfG",
                "replyto": "E1F4Gmr6CR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer tK3Y"
                    },
                    "comment": {
                        "value": "### Firstly, we would like to thank the reviewer for his/her valuable comment and time spent. We took all comments thoughtfully and will reflect them in our future modifications.\n\n> Below is the response to the comment. \n\n**(Q1) Related work before deep learning**\n\n**(A1)** We would like to thank the reviewer for the valuable comment. We will add the related work to include historical work in VSR, prior to the use of deep learning models. Moreover, we will clarify that the proposed method is focused on using deep learning models.\n\n---\n\n**(Q2) Clarification of Section 3.2**\n\n**(A2)** As per the reviewer's comment, we will reorganize Section 3 so the manuscript can be more self-contained.\n\n---\n\n**(Q3) Differences between AV-HuBET and mAV-HuBERT**\n\n**(A3)** The proposed mAV-HuBERT has the same architecture and objective function as AV-HuBERT. The main difference between the two models is whether they are trained using multilingual data or English-only data. mAV-HuBERT is trained on 5,512 hours of multilingual data composed of 9 languages while AV-HuBERT is trained on 1,759 hours of English-only data. We show that the original AV-HuBERT has insufficient ability to model multilingual speech as shown in Table 2. By pre-training the mAV-HuBERT with large-scale multilingual audio-visual data, we can greatly improve its performance in terms of multilingual VSR. Please note that we will release the pre-trained weight of mAV-HuBERT which can be utilized in diverse multilingual multimodal speech processing.\n\n---\n\n**(Q4) The justification of unit size**\n\n**(A4)** We evaluated the purity of the clusters in advance to train our model, following [1]. The purity for different cluster sizes (200, 500, 1000, 2000) is shown in Table R4-1. Basically, because the clustering is the process of quantizing the continuous visual features into fixed cluster sizes, the more the cluster size yields better representation quality. Therefore, there is a trade-off between the performance and efficiency aspects. We used 1000 clusters which can be expressed with 10 bits, following the previous work [2].\n\n* **Table R4-1**. Clustering purity using different cluster sizes.\n| 200 | 500 | 1000 | 2000 |\n| :--------: | :--------: | :--------: | :--------: |\n| 20.71% | 31.06% | 35.38% | 43.51% |\n\n---\n\n**(Q5) Clarification for the words \"reduce training speed by removing the visual front-end\"**\n\n**(A5)** We would like to clarify what the words \"reduce training speed by removing the visual front-end\" mean. Since we don't need the visual front-end (ResNet-18) for pre-training the visual speech unit-to-text model, the computation time can be reduced. This is because we don't need the gradient calculation using backpropagate the visual front-end. Therefore, the iteration time taken for each weight update step can be reduced.\n\n---\n\n**(Q6) Why curse of multilinguality only affect English**\n\n**(A6)** The data proportion of English data for finetuning our model was about 70% (3,258hr/4,545hr), as shown in Table 1 and Table 9. As the English data is already large enough, the performance of the English-only trained model is very powerful. Therefore, it is not easy to outperform the English-only trained model with our multilingual-trained model which shares the model parameter for multi-task.\nIf we finetune the mAV-HuBERT to English-only data, LRS3, making all model parameters devoted to English modeling, we can greatly improve the performance on English data as shown in Table R4-2.\n\n* **Table R4-2**. English performances of mAV-HuBERT finetuning on English-only and multilingual data.\n| Method | WER (%) |\n| :--------: | :--------: |\n| Multilingual Finetuned | 33.7% |\n| English-only Finetuned | 27.3% |\n\n---\n\n**(Q7) About the number $p$**\n\n**(A7)** We evaluated the pre-training performance by differing the $p$ before finetuning, and we observed that it is not critical in final performances, shown in Table R4-3. We choose the value showing the best mean performance. Please note that the important part is not the value of $p$, but whether we apply the curriculum learning or not, as shown in Table 5 in the manuscript.\n\n* **Table R4-3**. Pre-training performances (WER %) using different $p$.\n| **$p$** | **En** | **It**  | **Fr** |\n| :--------: | :--------: | :--------: | :--------: |\n| **50%** | 37.4 | 71.3 | 73.8 |\n| **70%** | 37.5 | 70.3 | 74.0 |\n| **90%** | 37.9 | 71.5 | 73.7 |\n\n---\n**References**\n\n    [1] Shi, Bowen, et al. \"Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction.\" International Conference on Learning Representations. 2021.\n    [2] Lee, Ann, et al. \"Textless Speech-to-Speech Translation on Real Data.\" Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368185879,
                "cdate": 1700368185879,
                "tmdate": 1700368185879,
                "mdate": 1700368185879,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qz01hHPT7f",
                "forum": "M8J0b9gNfG",
                "replyto": "GJP6bA3ojd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3646/Reviewer_tK3Y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3646/Reviewer_tK3Y"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledging authors' responses"
                    },
                    "comment": {
                        "value": "Thank you for the responses. Some of the changes should improve the clarity of the work."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672524214,
                "cdate": 1700672524214,
                "tmdate": 1700672524214,
                "mdate": 1700672524214,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ENSKSoFtQu",
            "forum": "M8J0b9gNfG",
            "replyto": "M8J0b9gNfG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3646/Reviewer_SxNH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3646/Reviewer_SxNH"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a multilingual visual speech recognition, by using the visual speech units to improve the training efficiency. It also proposes a curriculum learning approach to also exploits audio speech units. The paper presented number of experiments to investigate the effectiveness and efficiency of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- a novel frame work\n- an efficient framework\n- good presentation and experiments"
                },
                "weaknesses": {
                    "value": "- reproducibility seems difficult"
                },
                "questions": {
                    "value": "1- Computation aside, how different performance would be if we use continues features compared to unit features?\n\n2- The paper mentions that one novelty is presenting the sentence level model. it would be good if some explanation is provided around advantage of sentence level models?\n\n3- Once audio speech is utilised in the pertaining together with visual speech units, why using audio speech unit and not complete speech features? is it only to make computation more manageable? how performance and computation would change if audio speech is not discretised? \n\n4- Table 2 contains interesting results, where the multilingual setup does not help English dataset. In addition to what authors mentioned, there could be other reasons like language similarities or available language specific information. This has been the topic of number of works around multilingual models that how to balance between more data Fram more languages or using less data only from fewer similar languages[1][2]. While I believe this paper's topic is not around this issue, I think it would be good to refer  to this point\n\n[1] Cross-entropy training of DNN ensemble acoustic models for low-resource ASR\n[2] Multilingual data selection for low resource speech recognition"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no concern"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786405744,
            "cdate": 1698786405744,
            "tmdate": 1699636320654,
            "mdate": 1699636320654,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Zp7npDhbMN",
                "forum": "M8J0b9gNfG",
                "replyto": "ENSKSoFtQu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer SxNH"
                    },
                    "comment": {
                        "value": "### Firstly, we would like to thank the reviewer for his/her valuable comment and time spent. We took all comments thoughtfully and will reflect them in our future modifications.\n\n> Below is the response to the comment. \n\n**(Q1) Computation aside, the performance if we use continuous features**\n\n**(A1)** Firstly, we would like to kindly remind the performances in Table 5 in the manuscript. If we don't use the continuous features at the inference and directly utilize speech unit features to inference, we cannot achieve enough performance because of the information lost during the quantization process ('-Finetuning' in Table 5). If we don't use speech unit features at all and train the model with continuous features in a standard VSR setting, we can get the performance of '-Unit Pretraining' in Table 5. By pre-training the model with unit features and finetuning the model on continuous features, we can get the best performance, as shown in Table 5 (Proposed Method). Please note that without the incorporation of visual speech units, the pre-training loses its distinctiveness and becomes the same with standard VSR training (continuous video features to text prediction).\n\n---\n\n**(Q2) The advantage of sentence level models**\n\n**(A2)** We would like to thank the reviewer for the valuable suggestion. As lip movements (visual speech) have ambiguous speech information compared to audio and the existence of homophenes (same visual movements but different sounds; ex. ma, ba, pa), context modeling is crucial in the final performance. Therefore, using the sentence-level model brings the advantage of considering the context to predict the correct words and is thus more suitable in practical usage. In contrast, the famous word-level VSR model utilizes a very short video (1.16 sec in LRW [3]) wherein the target word must be centrally located and only can predict 500 words, making it challenging to apply effectively in real-world scenarios. As per the reviewer's suggestion, we will add the advantage of the sentence-level model in the manuscript.\n\n---\n\n**(Q3) About using discrete audio speech units during pre-training**\n\n**(A3)** As the reviewer pointed out, the primary reason for using discrete audio speech units is computational efficiency. If we use raw audio inputs (log filterbank 26-dimension 100fps), the required bit size is 83,200bits for 1-second audio. If we use the speech units (cluster size 1000 25fps), the required bit size is 25bits for 1-second audio. Therefore, we can greatly reduce the computational and storage size by using the speech units. Moreover, we conclude that the performance gain will not be large by using continuous audio speech during the pre-training for the following reasons. 1) A recent study [4] showed that in audio-based speech recognition (ASR), discrete speech units can achieve similar performances with continuous features. 2) The objective of using audio inputs during pre-training is to help our visual-to-text learning, where the pre-trained model is finetuned on continuous visual features without audio. Hence, the more important thing is whether we used audio during pre-training as shown in Table 5.\n\n---\n\n**(Q4) Other reasons why multilingual setup does not help English dataset**\n\n**(A4)** We thank the reviewer for providing valuable insights. We agree there can be other reasons why our multilingual setup does not help the English dataset than the curse of multilinguality. \n\nAs demonstrated in the paper [5], one plausible explanation could be associated with model capacity. In our approach, we employed a model of the same size as the English-only model to handle multilingual speech. Scaling the model size could enhance the performance of high-resource languages as well.\n\nAlso, another reason could be related to what the reviewer said, the similarity between languages. As the suggested references [1,2], using more similar languages will benefit the final speech recognition performances of each other, while not similar languages could deteriorate each other's performances. \n\nAs per the reviewer's suggestion, we will add this discussion to the manuscript.\n\n---\n\n**References**\n    \n    [1] Sahraeian, Reza, and Dirk Van Compernolle. \"Cross-entropy training of DNN ensemble acoustic models for low-resource ASR.\" IEEE/ACM Transactions on audio, speech, and language processing 26.11 (2018): 1991-2001.\n    [2] Thomas, Samuel, et al. \"Multilingual Data Selection for Low Resource Speech Recognition.\" Interspeech. 2016.\n    [3] Chung, Joon Son, and Andrew Zisserman. \"Lip reading in the wild.\" ACCV 2016.\n    [4] Chang, Xuankai, et al. \"Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning.\" Interspeech. 2023.\n    [5] Radford, Alec, et al. \"Robust speech recognition via large-scale weak supervision.\" International Conference on Machine Learning. PMLR, 2023.\n\n---"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700370338109,
                "cdate": 1700370338109,
                "tmdate": 1700370338109,
                "mdate": 1700370338109,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lUbFJKweb4",
                "forum": "M8J0b9gNfG",
                "replyto": "Zp7npDhbMN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3646/Reviewer_SxNH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3646/Reviewer_SxNH"
                ],
                "content": {
                    "comment": {
                        "value": "thanks for the response"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650603046,
                "cdate": 1700650603046,
                "tmdate": 1700650603046,
                "mdate": 1700650603046,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lLIMJ97fo9",
            "forum": "M8J0b9gNfG",
            "replyto": "M8J0b9gNfG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3646/Reviewer_JHaL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3646/Reviewer_JHaL"
            ],
            "content": {
                "summary": {
                    "value": "This work explores sentence-level Multilingual Visual Speech Recognition with a single model based on the pretrained AV-HuBERT and multi-lingual Hubert model. By taking advantages from the previous AV-HuBERT and the multi-lingual Hubert model together with the combination of multiple audio-visual speech datasets of different languages, the work here is able to transform the visual speech features to visual speech units and further perform multilingual visual speech recognition based on these units."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The general structure is clear. The method is simple in general. It\u2019s easy to follow. It\u2019s indeed very appealing in the view of efficiency when trained with discrete units."
                },
                "weaknesses": {
                    "value": "Compared with new methods or other insights, I think this work is more like a case to show the success again of large-scale data and large-scale model. Similar to the audio speech units, the visual speech units are introduced here. But the concept of visual speech units has been introduced since AV-HuBERT and its contemporary works. \nThe method here is totally based on the pre-trained AV-HuBERT and multilingual HuBERT, with a common masking strategy by gradually increasing the masked ratio of audio samples. There seems no new points in the proposed method and training strategy. It is more like a presentation of an example to use existing large-scale models (AV-HuBERT, HuBERT)."
                },
                "questions": {
                    "value": "(1) In Table.2, the worse performance of the proposed method compared with AV-HuBERT is described as the curse of multilinguality. What\u2019s the specific ratio of English data in the whole data? If the English data takes a large ratio in the whole data, and there are also common shared visemes among different languages, considering the larger-scale of the whole data compared with AV-HuBERT, the performance on English should not be worse? \n(2) In Table 3, the proposed work is based on pre-trained AV-HuBERT and HuBERT. The \u201cstandard VSR\u201d is also trained further based on these two models? or from scratch? The duration of \u201c52.5 hours for 8 epochs\u201d is also based on loading the pre-trained models?\n(3) In Table 5, what\u2019s the results of using only \u201cunit pretraining\u201d, whitout both CL and FT? Will it degenerate to the case of AV-HuBERT in Table 6?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699177360270,
            "cdate": 1699177360270,
            "tmdate": 1699636320583,
            "mdate": 1699636320583,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QRHLqkIPTD",
                "forum": "M8J0b9gNfG",
                "replyto": "lLIMJ97fo9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer JHaL"
                    },
                    "comment": {
                        "value": "### Firstly, we would like to thank the reviewer for his/her valuable comment and time spent. We took all comments thoughtfully and will reflect them in our future modifications.\n\n> Below is the response to the comment. \n\n**(Q1) The novelty of the proposed method**\n\n**(A1)** We would like to highlight the novel parts of the paper.\n\n- To the best of our knowledge, there is no prior work explicitly utilizing visual-based discrete speech tokens and analyzed their characteristics. In this paper, we analyzed the characteristics of visual speech units and showed that they contain linguistic content while suppressing the speaker characteristics. By employing the characteristics, we proposed a novel pretraining strategy for multilingual VSR which is very efficient and powerful.\n\n- To the best of our knowledge, **this is the first work exploring multilingual Visual Speech Recognition (VSR) using a single model**. Previous VSR models that explored multiple languages are language-specific, so the same number of models with the number of languages should be prepared to perform VSR for multiple languages. However, with the proposed method, we can perform it with just a single trained model. In this paper, we verified the effectiveness of the proposed method in 5 languages.\n\n- Even though we utilized the same model architecture with AV-HuBERT, **we newly introduced multilingual AV-HuBERT (mAV-HuBERT)**. As we have shown the effectiveness of the mAV-HuBERT in multilingual speech modeling over English-only AV-HuBERT, we can contribute to the research society by providing the pre-trained mAV-HuBERT model. Please note that mAV-HuBERT is trained on about 5k hours of multilingual audio-visual datasets.\n\n---\n\n**(Q2) The reason for worse English performance in Table 2**\n\n**(A2)** We would like to kindly remind the data size of each language is shown in Table 9 (Supplementary). The ratio of English data is  63% \\(3,481hr/5512hr=0.63\\). The curse of multilinguality is observed in diverse prior works exploring multilingual [1,2,3]. Especially, [1] shows that in order to outperform the English performance of the English-only trained speech recognition model with the multilingual trained speech recognition model, we need to scale the model size. [2] also finds the multilingual pretraining of speech recognition model improves the performance of non-English while decreasing the performance of English. \n\nThese outcomes can be attributed to the substantial size of the English dataset in comparison to other languages. Consequently, achieving a performance that surpasses the English-only trained model proves challenging for the multilingual-trained model. In contrast, due to the small data size for low-resource languages, they can get benefits from other language datasets.\n\nIn order to confirm the effectiveness of the mAV-HuBERT on English-only dataset, we finetune the mAV-HuBERT on English-only dataset (LRS3) and compare the performance with AV-HuBERT. The performance is shown in Table R2-1. The table clearly shows if mAV-HuBERT (trained on 5,512 hours multilingual data; English occupies 3,481 hours) is applied to an English-only corpus, we can obtain better performance than AV-HuBERT (trained on 1,759 hours of English data).\n\n* **Table R2-1**. VSR performance (WER %) comparisons on English-only dataset (LRS3)\n| Method | WER (%) |\n| :--------: | :--------: |\n| AV-HuBERT     | 28.6  |\n| **mAV-HuBERT**  | **27.3**  |\n\n---\n\n**(Q3) About Standard VSR in Table 3**\n\n**(A3)** We would like to clarify how the Standard VSR in Table 3 is trained. Standard VSR refers to the model utilizing raw video inputs instead of the proposed visual speech units. Therefore, the Standard VSR is trained by initializing from our pre-trained mAV-HuBERT and then finetuned by attaching a decoder, without the proposed unit-to-text pre-training. In conclusion, the answer to the reviewer's question is \"Yes, it also utilizes mAV-HuBERT which is pre-trained by setting its prediction target with mHuBERT, and the duration is also measured by using the pre-trained models\". We will clarify it in our manuscript.\n\n---\n\n**(Q4) The performance of only Unit Pretraining**\n\n**(A4)** As per the reviewer's comment, we report the performance of using only Unit Pretraining without both CL and FT. The performance is shown in Table R2-2.\n\n* **Table R2-2**. Performance (WER) without both CL and FT.\n| En | It | Fr |\n| :--------: | :--------: | :--------: |\n| 37.5 | 72.2 | 77.3 |\n\nPlease note that if we don't perform finetuning on continuous visual inputs and directly utilize the unit-to-text pre-trained model for VSR, the input of the VSR system is now visual speech units, not raw video. Therefore, it does not degenerate to the case of AV-HuBERT in Table 6 which utilizes raw video inputs. Since the two systems' inputs are different (visual speech unit vs. raw video), AV-HuBERT performs better than the model \"with only Unit Pretraining\".\n\n---"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368294509,
                "cdate": 1700368294509,
                "tmdate": 1700368595321,
                "mdate": 1700368595321,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g5BnY1TsbR",
            "forum": "M8J0b9gNfG",
            "replyto": "M8J0b9gNfG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3646/Reviewer_uTmt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3646/Reviewer_uTmt"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes methods to train a multilingual visual speech recognition model, a first of its kind.  The main contributions of the paper are usage of quantized representation of AV-HuBERT embeddings to reduce the dimensionality of video data, alongside proposing a curriculum learning approach to improve the VSR performance. This approach gradually diminishes the reliance on audio data during training, ultimately leading to the model being trained solely on video embeddings.  The proposed methods significantly reduces training computational costs while enhancing Visual Speech Recognition (VSR) performance across multiple languages."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-> Trains a single model to lip read in multiple languages, which I believe hasn't been done before.\n\n-> There is a clear improvement in the total training time required to train the mAV-Hubert due to the proposed quantization of AV-Hubert embeddings. \n\n-> The proposed multi-lingual model maintains its effectiveness and stands its ground when compared to the monolingual VSR method.\n\n-> The paper has good analyses on the information captured by the discretized visual units which is useful to the VSR community."
                },
                "weaknesses": {
                    "value": "-> The novely of the article is limited. The authors use preexisting blocks such as AV-Hubert and previously proposed methods of curriculum learning to improve the VSR performance. \n\n-> From Table 5, it looks like curriculum learning does not seem to have a significant impact on the VSR performance.\n\n-> The paper contains grammatical errors and requires proofreading."
                },
                "questions": {
                    "value": "In table 6, the authors showed that AV-HuBERT trained on multi-lingual data performs worse that mAV-Hubert for non-English dataset, thereby claiming that their proposed strategy of using visual speech units for training mAV-HuBERT is more effective in building VSR models. The claim would be better substantiated if the authors can apply the same strategy for training mAV-HuBERT on English only train dataset and show that it performs better than AV-HuBERT for English test set."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "As the authors indicate in their paper, lip reading technology has potential to be mis-used in surveillance systems which could violate the privacy of the speaker."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3646/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3646/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3646/Reviewer_uTmt"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699468073877,
            "cdate": 1699468073877,
            "tmdate": 1699636320518,
            "mdate": 1699636320518,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "w8GmwmhUjj",
                "forum": "M8J0b9gNfG",
                "replyto": "g5BnY1TsbR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer uTmt"
                    },
                    "comment": {
                        "value": "### Firstly, we would like to thank the reviewer for his/her valuable comment and time spent. We took all comments thoughtfully and will reflect them in our future modifications.\n\n> Below is the response to the comment. \n\n**(Q1) The novelty of the proposed method**\n\n**(A1)** We would like to highlight the novelty of our paper.\n- To the best of our knowledge, **this is the first work exploring multilingual Visual Speech Recognition (VSR) using a single model**. Previous VSR models that explored multiple languages are language-specific, so the same number of models with the number of languages should be prepared to perform VSR for multiple languages. However, with the proposed method, we can perform it with just a single trained model. In this paper, we verified the effectiveness of the proposed method in 5 languages.\n\n- Even though we utilized the same model architecture with AV-HuBERT, **we newly introduced multilingual AV-HuBERT (mAV-HuBERT)**. As we have shown the effectiveness of the mAV-HuBERT in multilingual speech modeling over English-only AV-HuBERT, we can contribute to the research society by providing the pre-trained mAV-HuBERT model. Please note that mAV-HuBERT is trained on about 5k hours of multilingual audio-visual datasets.\n\n- To the best of our knowledge, **this is the first work exploring visual-only speech units (i.e., visual speech units)**. Through analysis, we showed that visual speech units also correctly contain linguistic information similar to audio speech units. By employing the useful characteristics of the visual speech units (1. efficiency, 2. pseudo text), we first propose a method of pre-training the multilingual VSR model. We show the proposed method is very efficient and powerful through extensive evaluations.\n\n---\n\n**(Q2) The effectiveness of the curriculum learning**\n\n**(A2)** We would like to point out the importance of curriculum learning. In Table 5, when we substitute the curriculum learning from our final model (i.e., -Curriculum) the performance is drastically decreased from the full model. The performances are even worse than those of without pretraining method (i.e., -Unit Pretraining). These results indicate that if we do not use the curriculum learning during our pretraining, we cannot get optimum performances. Therefore, the pretraining should proceed with the proposed curriculum learning to achieve the best performances for all languages. As we discussed in Section 4.3.4, the result coincides with the previous VSR methods highlighting the audio-visual complemental effects in VSR.\n\n---\n\n**(Q3) Gramatical errors**\n\n**(A3)** As per the reviewer's comment, we will proofread the paper and fix the grammatical errors.\n\n---\n\n**(Q4) The effectiveness of the mAV-HuBERT on English only train dataset**\n\n**(A4)** We greatly appreciate for the valuable suggestion. As per the reviewer's suggestion, we compare the VSR performances of AV-HuBERT and mAV-HuBERT on an English-only dataset, LRS3. Therefore, the mAV-HuBERT model is finetuned on LRS3 dataset with the same setting as AV-HuBERT. The performance comparisons can be found in Table R1-1. As the mAV-HuBERT is trained with about 5,500 hours of data (English occupies 3,481 hours), we can achieve better VSR performance than AV-HuBERT (trained on 1,759 hours of English) when it is only finetuned on the English-only dataset.\n\n* **Table R1-1**. VSR performance (WER %) comparisons on English-only dataset (LRS3)\n| Method | WER (%) |\n| :--------: | :--------: |\n| AV-HuBERT     | 28.6  |\n| **mAV-HuBERT**  | **27.3**  |\n\n---"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368264769,
                "cdate": 1700368264769,
                "tmdate": 1700368264769,
                "mdate": 1700368264769,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VZuZTq7HVN",
                "forum": "M8J0b9gNfG",
                "replyto": "g5BnY1TsbR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3646/Reviewer_uTmt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3646/Reviewer_uTmt"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their detailed response. \n\n---\n\n### (Q2) The effectiveness of the curriculum learning\nThanks for the clarification. I seemed to have read Table 5 wrong and I stand corrected. Curriculum learning does show a large impact. \n\n---\n\n### (Q4) The effectiveness of the mAV-HuBERT on English only train dataset\nThe objective of the experiment with English only dataset was to understand the impact of the technique proposed by the author, namely \"use a visual speech unit which is a quantized visual speech feature extracted from a self-supervised model, in modeling visual speech representations.\". The experiment the authors conducted above does not address this issue. It looks like mAV-HUBERT has been trained on larger amount of data, which makes it harder to compare with respect to a strong baseline such as AV-HUBERT. \n\nI would propose the following:\n\n- Start with the same checkpoint for both mAV-HUBERT and AV-HUBERT\n- Use the same amount of English only data to fine tune mAV-HUBERT using the proposed quantized visual speech feature + finetuning as well as to finetune AV-HUBERT model\n- Compare the performance on English only dataset"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652146625,
                "cdate": 1700652146625,
                "tmdate": 1700652633565,
                "mdate": 1700652633565,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]