[
    {
        "title": "Visual Prompting Upgrades Neural Network Sparsification: A Data-Model Perspective"
    },
    {
        "review": {
            "id": "6wzBtJRPsg",
            "forum": "96nX9xIIx2",
            "replyto": "96nX9xIIx2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2675/Reviewer_hiB3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2675/Reviewer_hiB3"
            ],
            "content": {
                "summary": {
                    "value": "To upgrade vision model sparsification, the paper proposed a data-model co-design sparsification paradigm, where integrating input image with the learnable perturbation, and a network tuning strategy is designed to optimize this issue.\n\nThe algorithm has demonstrated excellent performance on CIFAR-10 and CIFAR-100 datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The manuscript exhibits a commendable level of writing proficiency, featuring well-crafted graphics that enhance the overall presentation and a compelling narrative.\n\n2. The algorithm has showcased remarkable efficacy when applied to the CIFAR-10 and CIFAR-100 datasets, achieving good performance."
                },
                "weaknesses": {
                    "value": "1. **Inadequate experiments.** This is a primary concern for the reviewer. The paper only presents experiments on CIFAR-10 and CIFAR-100, which, in the era of big data, are considered insufficient. These experiments do not adequately demonstrate the performance of the proposed method. Conducting experiments on larger datasets, such as ImageNet-1k, is essential.\n\nAdditionally, it's worth noting that the method utilizes ImageNet-1K pre-trained weights, which were trained on a resolution of 224. However, the method is tested on CIFAR data with a resolution of 32. It is evident that padding the data to a resolution of 224 can significantly boost performance. From this perspective, experiments specifically conducted on ImageNet with a resolution of 224, and direct performance comparisons with fine-tuning on this resolution, are crucial.\n\nFigure 10 further substantiates this conclusion, showing that the optimal performance is achieved at a resolution of 224, with diminishing performance as the resolution decreases. Therefore, padding CIFAR data with a resolution of 32 to 224 doesn't necessarily demonstrate the superiority of the method. The gains observed in this case could be attributed to the ImageNet-1K pre-trained weights at a resolution of 224.\n\n2. The reviewer also suggests providing performance comparisons with smaller models, as performance metrics on sparser models, such as using MobileNet, would be more indicative and informative.\n\n\n3. Furthermore, the method exhibits significant limitations, as it necessitates the use of pre-trained weights from a larger dataset. The current version seems to require transforming the ImageNet model to CIFAR. It would be insightful to explore the performance without pre-trained weights. Additionally, conducting comparisons on a larger pre-trained dataset, such as ImageNet, appears necessary for the current version."
                },
                "questions": {
                    "value": "In each figure, the authors have plotted curves labeled as \"our best,\" which may not be entirely necessary as this information can be inferred from the VPNs curve. \n\nMoreover, this plotting style has the potential to cause confusion; upon initial review, it might be perplexing why the curve representing \"ours\" appears as a straight line."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2675/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2675/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2675/Reviewer_hiB3"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2675/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697875808840,
            "cdate": 1697875808840,
            "tmdate": 1699636208594,
            "mdate": 1699636208594,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UYblhu19L8",
                "forum": "96nX9xIIx2",
                "replyto": "6wzBtJRPsg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point-to-point Response to Reviewer hiB3 (Part 1/2)"
                    },
                    "comment": {
                        "value": "Many thanks to reviewer hiB3 for acknowledging that our method achieves \u201cremarkable efficiency\u201d on CIFAR10 and CIFAR100, our writing proficiency is  \u201ccommendable\u201d, our graphics are \u201cwell-crafted\u201d, and our narrative is \u201ccompelling\u201d. We sincerely appreciate all constructive suggestions, which help us to improve our paper further. To address reviewer hiB3\u2019s concerns, we provide pointwise responses below.\n\n**[Cons 1. Only presenting experiments on CIFAR-10 and CIFAR-100 and conducting experiments on large datasets is essential.]** \n\nThis is a possible misunderstanding of our experiment setting. As we state in the Abstract, Line 15; Section 4.1, Paragraph 1; Section 4.2, Paragraph 1; [Figure 4](https://imgur.com/YF7ODNx), we conduct our experiments on eight datasets which are Tiny-ImageNet, Food101, CIFAR100, CIFAR10, DTD, Flowers102, StanfordCars, and OxfordPets. We also display the results of pruning on ImageNet and fine-tuning on Tiny-ImageNet and CIFAR100 in [Figure 6](https://imgur.com/pOVS2DX).\n\n**Additional experiments on ImageNet-1K.** To further alleviate reviewer hiB3\u2019s concern, we conducted additional experiments on ImageNet-1K using our method and some of the best baselines such as HYDRA and OMP on ImageNet-1K pre-trained ResNet-18 network. The empirical results are shown in Table R10, **our method achieves the best accuracy** at 50% and 90% sparsity levels, which illustrates the effectiveness of our method on ImageNet. We also report the results in the revision of our paper in Table A5. Due to the time limitation of rebuttal, we provide outcomes only at two sparsity levels, more sparsity levels are promised in our final version.\n\nTable R10. Performance comparison of our method, HYDRA, and OMP on ImageNet pre-trained ResNet-18 and ImageNet.\n| Method | 50% sparsity | 90% sparsity |\n| :----------: | :----------: | :----------: |\n| HYDRA | 68.91 | 66.62 |\n| OMP | 69.31 | 64.27 |\n| VPNs (ours) | 69.47 | 67.14 |\n\n**[Cons 2. Our method uses a resolution of 32.]** \n\nThanks for pointing out the question. We are aware of the role of resolution in the experiments and our responses to this concern are as follows.\n\n1. [We use a resolution of 224]  We kindly indicate that we unify the resolution of 224 in our method and the baselines in all of our results in Figure [4](https://imgur.com/YF7ODNx) \\ [5](https://imgur.com/7WL9Q9v) \\ [6](https://imgur.com/pOVS2DX) \\ [7](https://imgur.com/zqmXSrW) \\ [8](https://imgur.com/VkoqArm). As displayed in Section 4.1, Paragraph 3, Line 5, we use an input size of 224 and pad size of 16, which corresponds to [Figure 3](https://imgur.com/y1fRg7v) that i = 224 and p = 16. This is also illustrated in our code, the main.py file, line 30, we use an input size of 224. For reviewer hiB3\u2019s concern, we significantly polished our paper in Section 4.1, Paragraph 3 in our revision by specifying both our method and baselines using a resolution of 224 to avoid misunderstanding.\n\n2. [We have results on datasets that have a resolution of 224] We gently state that we present results on Tiny-ImageNet which has an original resolution of 224 in Figure [4](https://imgur.com/YF7ODNx) \\ [5](https://imgur.com/7WL9Q9v) \\ [6](https://imgur.com/pOVS2DX). The findings of these experiments also demonstrate our method is superior to all the baselines at an original resolution of 224 circumstances. \n\n3. [Additional results on ImageNet] The results on ImageNet in Table R10 also illustrate that our method achieves superior performance on ImageNet."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700303344620,
                "cdate": 1700303344620,
                "tmdate": 1700303344620,
                "mdate": 1700303344620,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S8v46gwrrP",
                "forum": "96nX9xIIx2",
                "replyto": "6wzBtJRPsg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point-to-point Response to Reviewer hiB3 (Part 2/2)"
                    },
                    "comment": {
                        "value": "**[Cons 3. Providing performance comparisons with smaller models such as MobileNet.]**\n\nThank you for the great suggestion. We conducted additional experiments on ImageNet pre-trained MobileNet and CIFAR100 using our method, HYDRA, and OMP. **We observe that our method achieves {2.24%, 1.27%} higher accuracy** than {HYDRA, OMP} at 50% sparsity level as shown in Table R11. The results are also included in the revision of our paper in Table A12. Due to the time limitation, we provide outcomes only at two sparsity levels, more sparsity levels are promised in our final version.\n\nTable R11. Performance comparison of our method, HYDRA, and OMP on ImageNet pre-trained MobileNet and CIFAR100.\n| Method | 50% sparsity | 90% sparsity |\n| :----------: | :----------: | :----------: |\n| HYDRA | 79.12 | 70.02 |\n| OMP | 80.09 | 41.82 |\n| VPNs (ours) | 81.36 | 75.5 |\n\n**[Cons 4. It would be insightful to explore the performance without pre-trained weights]**\n\nThank you for the advice. We state that the pruning paradigm of our method which requires first finding masks in a trained model and then tuning the subnetwork to recover the accuracy can\u2019t be used on models without pre-training. This is because a model without pre-training has random weights making the mask-finding stage of our method useless. \n\n**Additional experiments of training from scratch.** To further alleviate reviewer hiB3\u2019s concern, we conducted additional experiments by applying the VPNs pruning paradigm on SynFlow named VPNs w. SynFlow and pruning from scratch on ResNet-18 and CIFAR100. We observe that **VPNs w. SynFlow(with VP) achieves {8.85%, 5.91%} higher accuracy** than the original SynFlow (without VP) at {50%, 90%} sparsity levels as shown in Table R12, which indicates visual prompting significantly enhances models\u2019 sparsification in the setting of training from scratch. We also report the results in the revision of our paper in Table A13. Due to the time limitation, we provide outcomes only at two sparsity levels, more sparsity levels are promised in our final version.\n\nTable R12. Performance comparison of VPNs w. SynFlow and SynFlow pruning from scratch on ResNet-18 and CIFAR100.\n| Method | 50% sparsity | 90% sparsity |\n| :----------: | :----------: | :----------: |\n| SynFlow (without VP) | 66.77% | 64.46% |\n| VPNs w. SynFlow (with VP) | 75.62% | 70.37% |\n\n**[Cons 5. It might be perplexing why the curve representing \"ours\" appears as a straight line.]**\n\nThank you for the great point. We use a similar plot format as in [15]. Regarding reviewer hiB3\u2019s concern, we deleted the \u201cOur Best\u201d dashed lines in all plots in the revision of our paper.\n\n[15] Yihua Zhang, Yuguang Yao, Parikshit Ram, Pu Zhao, Tianlong Chen, Mingyi Hong, Yanzhi Wang, and Sijia Liu. Advancing model pruning via bi-level optimization. Advances in Neural Information Processing Systems, 35:18309\u201318326, 2022a."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700303379959,
                "cdate": 1700303379959,
                "tmdate": 1700303379959,
                "mdate": 1700303379959,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KiS61URIiP",
                "forum": "96nX9xIIx2",
                "replyto": "6wzBtJRPsg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hiB3"
                    },
                    "comment": {
                        "value": "Dear Reviewer **hiB3**,\n\nWe thank reviewer **hiB3** time for the review and constructive comments. We really hope to have a further discussion with the reviewer **hiB3** to see if our response solves the concerns.\n\nIn our response, we have (1) clarified the resolution of our method; (2) conducted additional experiments on ImageNet, MobileNet, and models without pre-trained weights, further indicating the superiority of our method.\n\nWe genuinely hope reviewer **hiB3** could kindly check our response. Thanks!\n\nBest wishes,\n\nAuthors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700473115314,
                "cdate": 1700473115314,
                "tmdate": 1700473115314,
                "mdate": 1700473115314,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "t4StOsr2RU",
                "forum": "96nX9xIIx2",
                "replyto": "KiS61URIiP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Reviewer_hiB3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Reviewer_hiB3"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for the authors' response. \n\nFirstly, a suggestion is to avoid using imgur.com as it seems that many images are now expired. If you make any changes, please include them in the updated draft and indicate in your response. \n\nThis would provide a more straightforward way to align and review the modifications.\n\nOverall, the authors have addressed the majority of my concerns.  The reviewer suggests supplementing additional sparsity levels on ImageNet, such as 20% and 70%, as it appears to be more informative and valuable for reference.\n\nHowever, there is still a concern raised by the reviewer: in the experiments on ImageNet, the authors compared HYDRA and OMP, both of which were published before 2020. This does not make sense, and the authors should consider comparing with more recent and stronger works, such as BiP [1].\n\nFor the experiments on ImageNet with ResNet-18, BiP seems to achieve around 71% and 70% performance at 50% and 90% sparsity (Figure A7 of the paper).\n\n[1] Zhang, Yihua, Yuguang Yao, Parikshit Ram, Pu Zhao, Tianlong Chen, Mingyi Hong, Yanzhi Wang, and Sijia Liu. \"Advancing model pruning via bi-level optimization.\" Advances in Neural Information Processing Systems 35 (2022): 18309-18326."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542918212,
                "cdate": 1700542918212,
                "tmdate": 1700542918212,
                "mdate": 1700542918212,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DLDBrrYCtV",
            "forum": "96nX9xIIx2",
            "replyto": "96nX9xIIx2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2675/Reviewer_eRqj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2675/Reviewer_eRqj"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a joint model pruning and visual prompting learning method. By combining these two methods, it could recover the performance loss by pruning. This method is validated on several pruning methods and datasets to demonstate its universality."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea is easy to follow and effective.It is interesting to see that only a small number of learned parameters could improve the performance."
                },
                "weaknesses": {
                    "value": "1. I think the authors should focus more on the structured sparse case, unstructured pruning is well known that will not contribute to any acceleration in practice.\n2. For structured pruning, the speedup ratio should use latency, not theoritical FLOPs. And more recent methods should be compared.\n3. The visual prompting method essentially uses lower resolution for the input images. It is necessary to compare a baseline that using a lower resolution image as input, and then prune less parameters to maintain the same FLOPs as the proposed VPN. \n4. Following the previous point, I also wonder whether this method will deteriorate some applications that are senstive to resolution, such as object detection or etc."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2675/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2675/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2675/Reviewer_eRqj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2675/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698744864491,
            "cdate": 1698744864491,
            "tmdate": 1700651246869,
            "mdate": 1700651246869,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L6LXQcRHi9",
                "forum": "96nX9xIIx2",
                "replyto": "DLDBrrYCtV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point-to-point Response to Reviewer eRqj (Part 1/3)"
                    },
                    "comment": {
                        "value": "Thank you for recognizing that our idea is \u201ceasy to follow and effective\u201d and our performance is \u201cinteresting\u201d. To address reviewer eRqj\u2019s concerns, we detailed our responses below.\n\n**[Cons 1. Focusing more on the structured sparse case.]** \n\nThank you for the suggestion. We kindly point out that our main contributions focus on introducing the new data-model co-design pruning paradigm and we respectfully argue that focusing on unstructured pruning is highly meaningful. Unstructured pruning is extremely useful as both a mathematical prototype and an empirical testbed for new SNN algorithms and it is also receiving increasingly better support in practice. The evidence is as follows:\n\n1. [Better performance than structured pruning] As the finest-grained and most flexible sparsity level, unstructured sparsity is superior to other more structured forms of sparsity [7].\n\n2. [Widely used on nonGPU hardware] Unstructured sparsity has widely proven its practical relevance on nonGPU hardware, such as CPUs or customized accelerators. For instance, in the range of 70-90% high unstructured sparsity, XNNPACK [8] has already shown significant speedups over dense baselines on smartphone processors. \n\n3. [Receive increasing support] The hardware support of unstructured sparsity may be relatively limited on \u201coff-the-shelf\u201d commodity GPUs/TPUs, but it keeps improving quickly over the years. For example, advanced GPU kernels such as NVIDIA cuSPARSE [9] and Sputnik [10] have built the momentum to better support finer-grained sparsity.\n\n4. [Our method achieves time reduction in mask finding] We demonstrate that our method (unstructured pruning) achieves significant time reduction in terms of mask finding compared to other pruning methods like HYDRA and BiP as shown in [Figure 9](https://imgur.com/t2ysbTc).\n\n**[Cons 2. The speedup ratio should use latency.]** \n\nThank you for the advice. We use FLOPs following [11, 12] and the FLOPs are also widely used in calculating the speedup ratio. To alleviate reviewer eRqj\u2019s concern, we provide the latency speedup ratio of our method on the Quadro RTX 6000 GPU in Table R6. **Our method achieves latency speedup ratios of 1.1\u00d7 and 1.2\u00d7 at 10% and 20% channel-wise sparsity** respectively without compromising the performance relative to the dense network, as indicated in [Figure 8](https://imgur.com/VkoqArm). We kindly point out that our main contributions focus on introducing the new data-model co-design pruning paradigm and unstructured pruning, we provide the results of structured pruning only to indicate a potential research direction. We also include the latency results in the revised paper in Table A9.\n\nTable R6. The latency and FLOPs of VPNs structured pruning on ImageNet pre-trained ResNet-18 and CIFAR100.\n| Sparsity | Latency(ms) | FLOPs(G) |\n| :----------: | :----------: | :----------: |\n| Dense | 2.23 &plusmn; 0.03 | 1.82 |\n| 10% sparsity | 2.02 &plusmn; 0.02 | 1.68 |\n| 20% sparsity | 1.86 &plusmn; 0.02 | 1.45 |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302999633,
                "cdate": 1700302999633,
                "tmdate": 1700304121160,
                "mdate": 1700304121160,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qhwo4GuUIq",
                "forum": "96nX9xIIx2",
                "replyto": "DLDBrrYCtV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point-to-point Response to Reviewer eRqj (Part 2/3)"
                    },
                    "comment": {
                        "value": "**[Cons 3. More recent structured pruning methods should be compared.]** \n\nThank you for the suggestion. We gently indicate that we have considered several recent state-of-the-art methods such as DepGraph which is a very strong baseline published in 2023 in Section 4.2, Paragraph 1, Line 3. \n\n**Additional experiments on structured pruning.** To further address reviewer eRqj\u2019s concern, we conducted additional experiments on ImageNet pre-trained ResNet-18 and CIFAR100 using GReg [13] and LAMP [14] which are two recent structured pruning methods. We observe that **our method has the best performance** as shown in Table R7. We add the new results in our revised paper in Table A10. Due to the time limitation, we provide outcomes only at two sparsity levels, more sparsity levels are promised in our final version. If the reviewer can kindly point out more methods, we will also add them in our final version.\n\nTable R7. Performance comparison of our method, GReg, and LAMP on ImageNet pre-trained ResNet-18 and CIFAR100 at 20% and 50% channel-wise sparsity levels.\n| Method | 20% sparsity | 50% sparsity |\n| :----------: | :----------: | :----------: |\n| GReg | 78.91 | 62.63 |\n| LAMP | 80.7 | 75.46 |\n| VPNs (ours) | 81.21 | 75.58 |\n\n**[Cons 4. Our method essentially uses lower resolution input and needs to compare the baselines with lower resolution.]**\n\nThis might be a misunderstanding of our setting and we respond to the concern from the following aspects:\n\n1. [The resolution of our method and baselines is always 224] In our design, the resolution of our method is always 224 which equals the resolution of the input images in all baselines.  Although the visual prompt as learnable parameters are added to the margin of the image, the full image pixel information is preserved. Evidence can be found in Section 4.1, Paragraph 3, Line 5 that we use an input size of 224 and a pad size of 16, and in [Figure 3](https://imgur.com/y1fRg7v)  that i = 224 and p = 16.\n\n2. [**Additional experiments of baselines using lower resolution and pruning fewer parameters**] To further alleviate reviewer eRqj's concern, we conducted additional experiments on some of the best baselines such as HYDRA and OMP with lower resolution and pruning fewer parameters on ImageNet pre-trained ResNet-18 and CIFAR100. HYDRA and OMP use an input resolution of 192 and prune 13k (the number of parameters in the visual prompt) fewer parameters than our method. We find that **our method achieves {3.94%, 3.69%} higher accuracy** than {HYDRA, OMP} at 90% sparsity level from the results displayed in Table R8, which indicates our method is superior to baselines using lower resolution and pruning fewer parameters. We also add the results in the revision of our paper in Table A11. Due to the time limitation, we provide outcomes only at two sparsity levels, more sparsity levels are promised in our final version.\n\nTable R8. Performance comparison of our method, HYDRA, and OMP on ImageNet pre-trained ResNet-18 and CIFAR100. HYDRA and OMP use 192 as the input resolution and prune 13k fewer parameters than our method. \n| Method | 50% sparsity | 90% sparsity |\n| :----------: | :----------: | :----------: |\n| HYDRA | 80.9 | 76.64 |\n| OMP | 80.89 | 76.89 |\n| VPNs (ours) | 83.18 | 80.58 |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700303097748,
                "cdate": 1700303097748,
                "tmdate": 1700303097748,
                "mdate": 1700303097748,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D9qFMFjoO5",
                "forum": "96nX9xIIx2",
                "replyto": "DLDBrrYCtV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point-to-point Response to Reviewer eRqj (Part 3/3)"
                    },
                    "comment": {
                        "value": "**[Cons 5. Whether our method will deteriorate in applications that are sensitive to resolution such as object detection.]**\n\nThis is an interesting direction to explore. We provide detailed responses below.\n\n1. [Our method is insensitive to resolution] As illustrated above, our method and the baselines all use a resolution of 224 in the original setting and our method surpasses all baselines as shown in [Figure 4](https://imgur.com/YF7ODNx). In the additional experiments displayed in Table R8, our method is still better than the baselines using a resolution of 192. No matter when the baselines use the resolution of 224 or 192, our method is consistently better than the baselines, demonstrating our method is insensitive to resolution.\n\n2. [**Superior performance on object detection**] To further alleviate reviewer eRqj's concern, we conducted supplemental experiments on Pascal VOC 2007, which is a well-known object detection task. We compare our method to HYDRA and OMP on YOLOv4 with ImageNet pre-trained ResNet-18 backbone. **Our method achieves {3.78%, 2.67%} higher AP** than {HYDRA, OMP} at 90% sparsity level as shown in Table R9, which indicates the superiority of our method in resolution-sensitive tasks. The results are also included in the revision of our paper in Table A6. Due to the time limitation, we provide outcomes only at two sparsity levels, more sparsity levels are promised in our final version.\n\nTable R9. AP comparison of our method, HYDRA, and OMP on YOLOv4 with ImageNet pre-trained ResNet-18 backbone and Pascal VOC 2007.\n| Method | 50% sparsity | 90% sparsity |\n| :----------: | :----------: | :----------: |\n| HYDRA | 35.25 | 32.74 |\n| OMP | 35.01 | 33.85 |\n| VPNs (ours) | 38.37 | 36.52 |\n\n[7] Mao, Huizi, Han, Song, Pool, Jeff, Li, Wenshuo, Liu, Xingyu, Wang, Yu & Dally, William J 2017 Exploring the granularity of sparsity in convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 13\u201320.\n\n[8] Elsen, Erich, Dukhan, Marat, Gale, Trevor & Simonyan, Karen 2020 Fast sparse convnets. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 14629\u201314638.\n\n[9] Valero-Lara, Pedro, Mart\u00ednez-P\u00e9rez, Ivan, Sirvent, Ra\u00fcl, Martorell, Xavier & Pena, Antonio J 2018 Nvidia gpus scalability to solve multiple (batch) tridiagonal systems implementation of cuthomasbatch. In Parallel Processing and Applied Mathematics: 12th International Conference, PPAM 2017, Lublin, Poland, September 10- 13, 2017, Revised Selected Papers, Part I, pp. 243\u2013253. Springer.\n\n[10] Gale, Trevor, Zaharia, Matei, Young, Cliff & Elsen, Erich 2020 Sparse gpu kernels for deep learning. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1\u201314. IEEE.\n\n[11] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming. In Proceedings of the IEEE international conference on computer vision, pp. 2736\u20132744, 2017.\n\n[12] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. Depgraph: Towards any structural pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16091\u201316101, 2023.\n\n[13] Huan Wang, Can Qin, Yulun Zhang, and Yun Fu. Neural pruning via growing regularization. arXiv preprint arXiv:2012.09243, 2020.\n\n[14] Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo Shin. Layer-adaptive sparsity for the magnitude-based pruning. arXiv preprint arXiv:2010.07611, 2020."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700303119112,
                "cdate": 1700303119112,
                "tmdate": 1700304496982,
                "mdate": 1700304496982,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "itF8yTOpJt",
                "forum": "96nX9xIIx2",
                "replyto": "DLDBrrYCtV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eRqj"
                    },
                    "comment": {
                        "value": "Dear Reviewer **eRqj**,\n\nWe thank reviewer **eRqj** time for the review and constructive comments. We really hope to have a further discussion with the reviewer **eRqj** to see if our response solves the concerns.\n\nIn our response, we have (1) interpreted the contribution of our method and the significance of unstructured pruning; (2) conducted additional experiments on structured pruning, object detection, and baselines with lower resolution, further indicating the superiority of our method.\n\nWe genuinely hope reviewer **eRqj** could kindly check our response. Thanks!\n\nBest wishes,\n\nAuthors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700473062815,
                "cdate": 1700473062815,
                "tmdate": 1700473062815,
                "mdate": 1700473062815,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hhdU6ITWaP",
                "forum": "96nX9xIIx2",
                "replyto": "DLDBrrYCtV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Last Day Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer **eRqj**, \n\nWe genuinely appreciate your dedicated time and effort in reviewing our work. As the final day of the discussion period approaches, we kindly request that you share any additional questions or concerns you may have. We are eager to engage in further discussions with you. \n\nIf our responses have adequately addressed your concerns, we kindly ask that you consider raising the score of our work. Again, we are truly grateful for your valuable time and efforts. \n\nWarm regards, \n\nThe Authors"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620402302,
                "cdate": 1700620402302,
                "tmdate": 1700620402302,
                "mdate": 1700620402302,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OPI8OvaYHE",
                "forum": "96nX9xIIx2",
                "replyto": "L6LXQcRHi9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Reviewer_eRqj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Reviewer_eRqj"
                ],
                "content": {
                    "comment": {
                        "value": "1. I would like to point out that even the XNNPACK mentioned that it needs block sparsity or certain 1x1 conv block as refered in [link](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/PruneForLatencyOnXNNPack) and [link](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_sparsity_2_by_4). For NVidia GPU, it needs the 2 by 4 sparsity pattern mentioned in previous link. The rebuttal on structured pruning is an obvious sophism to me. The provided latency test is also only for channel pruning, which is a kind of structured pruning."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650835320,
                "cdate": 1700650835320,
                "tmdate": 1700650835320,
                "mdate": 1700650835320,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VEcFgGdPyx",
                "forum": "96nX9xIIx2",
                "replyto": "qhwo4GuUIq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Reviewer_eRqj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Reviewer_eRqj"
                ],
                "content": {
                    "comment": {
                        "value": "I am afraid the authors misunderstand my questions about resolution. What I mean is that the effective content of image is of size 192x192. Thus we need to compare the results of VPN of size 224 (actual size 192) of sparsity x and other methods of size 192 of sparsity y, in which x and y are chosen that the compared methods have similar FLOPs, because the additional content in VPN is not necessary for other methods."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651056064,
                "cdate": 1700651056064,
                "tmdate": 1700651056064,
                "mdate": 1700651056064,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iJXKkA9gnj",
                "forum": "96nX9xIIx2",
                "replyto": "D9qFMFjoO5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Reviewer_eRqj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Reviewer_eRqj"
                ],
                "content": {
                    "comment": {
                        "value": "How can you transfer your method to VOC, since the input size of detection tasks are usually much larger than that of classification. Moreover, VOC2007 IS NOT a valid dataset for evaluating detection tasks nowadays, larger dataset such as COCO is a must."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651186322,
                "cdate": 1700651186322,
                "tmdate": 1700651186322,
                "mdate": 1700651186322,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gmQkzvFBGp",
                "forum": "96nX9xIIx2",
                "replyto": "DLDBrrYCtV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Reviewer_eRqj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Reviewer_eRqj"
                ],
                "content": {
                    "title": {
                        "value": "After rebuttal score"
                    },
                    "comment": {
                        "value": "Above all, the rebuttal has not addressed my concerns at all. I have to lower my score."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651222682,
                "cdate": 1700651222682,
                "tmdate": 1700651222682,
                "mdate": 1700651222682,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3gt9Wuo8Xl",
                "forum": "96nX9xIIx2",
                "replyto": "YEGndxWqX0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Reviewer_eRqj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Reviewer_eRqj"
                ],
                "content": {
                    "comment": {
                        "value": "The authors' emphasis on unstructured pruning in relation to BERT is intriguing. However, it's evident that visual prompting is not applicable to such a model. It also seems challenging for the authors to present a compelling example from the vision community in this context. Their defensiveness about this concept, which is well-acknowledged in the model acceleration community, is perplexing.\n\nRegarding the COCO experiments, I question the rationale behind highlighting a two-page technical report [27]. If the intent is to underscore that these seminal works also utilized the VOC dataset, it's noteworthy to mention that moco v1 [26] also reports COCO results alongside VOC. As a submission to a prestigious conference like ICLR, it's crucial to ensure the adequacy of experimental validation.\n\nIn summary, while I recognize the potential of the ideas presented in this paper, it is imperative that the authors take the reviewers' comments seriously and make substantive improvements to their work."
                    }
                },
                "number": 34,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729965983,
                "cdate": 1700729965983,
                "tmdate": 1700729965983,
                "mdate": 1700729965983,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1Fh5Lr228k",
            "forum": "96nX9xIIx2",
            "replyto": "96nX9xIIx2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2675/Reviewer_sGzb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2675/Reviewer_sGzb"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use visual prompts to improve performance of the pruned model by applying visual prompts earlier in the process, aka, before the model fine-tuning. This effort was motivated by the experiments of applying post-pruning prompt to the sparse models with and without fine-tuning. As post-pruning prompts showed only marginal gains to subnets that went through fine-tuning, authors proposed to apply the visual prompts earlier in the process. The proposed scheme was compared with eight pruning baselines on eight classification tasks. Numerical comparisons show the potential of the proposed scheme."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea of applying visual prompts to identify a subnet which further leads to a better pruning results is interesting.\n- The idea of using visual prompts to control a pretrained vision model is an interesting direction to pursue."
                },
                "weaknesses": {
                    "value": "- The paper is not easy to read. There's a particular emphasis on \"data model co-design\", but it takes quite a while to understand what this refers to concretely.\n- Why is the visual prompts essential? How about learning additional parameters without using the visual prompt?\n- How would visual prompts be different from data augmentation?"
                },
                "questions": {
                    "value": "Please see my questions in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2675/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789990511,
            "cdate": 1698789990511,
            "tmdate": 1699636208441,
            "mdate": 1699636208441,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AAMFlizzlo",
                "forum": "96nX9xIIx2",
                "replyto": "1Fh5Lr228k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point-to-point Response to Reviewer sGzb (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for recognizing that our results and the direction of our work are \u201cinteresting\u201d. To address reviewer sGzb\u2019s concerns, we provide detailed responses below.\n\n**[Cons 1. The statement of \u201cdata-model co-design\u201d is not concrete.]** \n\nThanks for putting forward the point. Specifically, data-model co-design indicates both the input data and the model are optimized in pruning. This paradigm of our method is significantly different from the baselines which are model-centric. \n1. [Model-centric design] Focusing on searching and preserving crucial weights by analyzing network topologies (Abstract, Line 6) with fixed inputs as illustrated in [3].\n2. [Data-model co-design] Optimizing weight masks and visual prompts jointly by combining data-centric design that constructs well-designed prompts (Section 1, Paragraph 3, Line 3) and model-centric design.\nMore explanation of data-model co-design can be found in Section 1, Paragraph 5, Point 2; [Figure 2](https://imgur.com/tcdNFkO); and Section 3, Paragraph 1.\n\nTo further alleviate reviewer sGzb\u2019s concern, we significantly polished our paper in Section 1, Paragraph 5, and Section 3, Paragraph 1 by interpreting the data-model co-design more concretely to avoid misunderstanding in our revision.\n\n**[Cons 2. Why is the visual prompt essential?]** \n\nThank you for the question. The significance of the visual prompt is detailed as follows:\n\n1. [Motivation] \n\n- Xu et al. [4] demonstrate that prompts can recover compressed LLMs which illuminates the efficacy of post-pruning prompts in enhancing the performance of compressed LLMs. Observing that post-pruning prompts bolster the efficiency and performance of LLMs, It is only natural to inquire about the impact of VPs on vision models.\n\n- We observe from [Figure 1](https://imgur.com/37Np8Dh) that post-pruning prompts escalate the performance of the subnetworks before fine-tuning. However, neither of these settings consistently surpasses the standard no-prompting approach (pruning + fine-tuning). We postulate visual prompts are capable of enhancing the sparsification of vision models by utilizing them in a different way.\n\n2. [Empirical evidence] \n- As shown in [Figure 4](https://imgur.com/YF7ODNx) and [Figure 5](https://imgur.com/7WL9Q9v), we demonstrate the essential role of VP by comparing our method (with VP) to HYDRA (without VP) on eight downstream datasets and three architectures.\n\n- As shown in [Figure 7](https://imgur.com/zqmXSrW), we observe that VP combined with existing prunings consistently surpasses their original counterpart (Section 4.2, Paragraph 4), which further illustrates the essential of VP.\n\n3. [Previous literature] \n\n- [5, 6] showcases visual prompting as a parameter-efficient method that substantially improves the generalization and accuracy of vision models over multiple tasks. We believe these advantages brought by visual prompts also benefit pruning."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302578088,
                "cdate": 1700302578088,
                "tmdate": 1700302578088,
                "mdate": 1700302578088,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WiUh1kVpuz",
                "forum": "96nX9xIIx2",
                "replyto": "1Fh5Lr228k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point-to-point Response to Reviewer sGzb (Part 2/2)"
                    },
                    "comment": {
                        "value": "**[Cons 3. How about learning additional parameters without using the visual prompt?]**\n\nThank you for the suggestions. To the best of our understanding, the reviewer suggests comparing our method (with VP) to the baselines which learn more parameters than ours. Here is our response to the questions.\n\n1. [Current results] As displayed in [Figure 4](https://imgur.com/YF7ODNx), our method achieves better performance at 80% sparsity than other baselines at {40%, 50%, 60%, 70%} sparsity levels on CIFAR100, CIFAR10, Food101, Flowers102, DTD, and StanfordCars.\n\n2. [**Additional experiments of the baselines learning additional parameters**] We implemented supplemental experiments on ImageNet pre-trained ResNet-18 on CIFAR100, using our method, HYDRA, and OMP. HYDRA and OMP learn an additional 13k (the number of parameters in the visual prompt)  parameters than our method. We find that **our method obtains {3.46%, 3.06%} higher accuracy** than {HYDRA, OMP} at 90% sparsity level as displayed in Table R4, which manifests that our method is better than the baselines with additional parameters. The results are also reported in our revised paper in Table A7. Due to the time limitation, we provide outcomes only at two sparsity levels, more sparsity levels are promised in our final version. \n\nTable R4. Performance comparison of our method, HYDRA, and OMP on ImageNet pre-trained ResNet-18 and CIFAR100. HYDRA and OMP learn 13k additional parameters than our method. \n| Method | 50% sparsity | 90% sparsity |\n| :----------: | :----------: | :----------: |\n| HYDRA | 81.03 | 77.12 |\n| OMP | 81.36 | 77.52 |\n| VPNs (ours) | 83.18 | 80.58 |\n\n**[Cons 4. How would visual prompt be different from data augmentation?]**\n\nVisual prompt is essentially different from data augmentation, and the reasons are as follows:\n\n1. [Purpose] Visual prompts are usually designed to improve model adaption. While data augmentations are used to avoid overfitting.\n\n2. [Methodology] Visual prompts can be optimized in a data-driven way. However, most data augmentations are data-agnostic.In our method, visual prompting is the key to enabling the data-model exploration of crucial sparsity topologies.\n\n3. [**Additional experiments of more data augmentations**] To further address reviewer sGzb\u2019s concern, we compare our method without mix-ups to HYDRA and OMP with mix-ups on ImageNet pre-trained ResNet-18 and CIFAR10. **Our method achieves {5.46%, 3.50%} higher accuracy** than {HYDRA, OMP} at 90% sparsity level as shown in Table R5, which indicates that visual prompting is better than using more data augmentation. We add the results to the revised paper in Table A8. Due to the time limitation, we provide outcomes only at 50% and 90% sparsity levels, more sparsity levels are promised in our final version.\n\nTable R5. Performance comparison of our method (without mix-up), HYDRA (with mix-up), and OMP (with mix-up) on ImageNet pre-trained ResNet-18 and CIFAR10.\n| Method | 50% sparsity | 90% sparsity |\n| :----------: | :----------: | :----------: |\n| HYDRA (with mix-up) | 92.72 | 90.83 |\n| OMP (with mix-up) | 94.63 | 92.79 |\n| VPNs (without mix-up) | 96.47 | 96.29 |\n\n\n[3] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic flow. Advances in neural information processing systems, 33:6377\u20136389, 2020.\n\n[4] Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, and An-shumali Shrivastava. Compress, then prompt: Improving accuracy-efficiency trade-off of llm inference with transferable prompt. arXiv preprint arXiv:2305.11186, 2023.\n\n[5] Aochuan Chen, Yuguang Yao, Pin-Yu Chen, Yihua Zhang, and Sijia Liu. Understanding and improving visual prompting: A label-mapping perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19133\u201319143, 2023.\n\n[6] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16816\u201316825, 2022a."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302777084,
                "cdate": 1700302777084,
                "tmdate": 1700304150654,
                "mdate": 1700304150654,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BI7sRU4TSh",
                "forum": "96nX9xIIx2",
                "replyto": "1Fh5Lr228k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sGzb"
                    },
                    "comment": {
                        "value": "Dear Reviewer **sGzb**,\n\nWe thank reviewer **sGzb** time for the review and constructive comments. We really hope to have a further discussion with the reviewer **sGzb** to see if our response solves the concerns.\n\nIn our response, we have (1) interpreted the data-model co-design and the importance of visual prompts concretely; (2) conducted additional experiments on baselines learning more parameters and with more data augmentations, further indicating the superiority of our method.\n\nWe genuinely hope reviewer **sGzb** could kindly check our response. Thanks!\n\nBest wishes,\n\nAuthors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472928218,
                "cdate": 1700472928218,
                "tmdate": 1700472928218,
                "mdate": 1700472928218,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bFDXfiKkJD",
                "forum": "96nX9xIIx2",
                "replyto": "1Fh5Lr228k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Last Day Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer **sGzb**, \n\nWe genuinely appreciate your dedicated time and effort in reviewing our work. As the final day of the discussion period approaches, we kindly request that you share any additional questions or concerns you may have. We are eager to engage in further discussions with you. \n\nIf our responses have adequately addressed your concerns, we kindly ask that you consider raising the score of our work. Again, we are truly grateful for your valuable time and efforts. \n\nWarm regards, \n\nThe Authors"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620391036,
                "cdate": 1700620391036,
                "tmdate": 1700620391036,
                "mdate": 1700620391036,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "izPKUiwMba",
            "forum": "96nX9xIIx2",
            "replyto": "96nX9xIIx2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2675/Reviewer_8b5d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2675/Reviewer_8b5d"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new post-pruning method by introducing visual prompts into the pruning pipeline. The authors introduce visual prompts into trained vision models. During pruning, not only weight masks but also visual prompts are optimized. The whole pipeline includes two steps: (1) fixing pretrained weights of vision models, tuning masks and visual prompts; (2) fixing the mask, fine-tuning both weights and visual prompts. The authors adopt the proposed method on several downstream classification tasks and compare the performances of pruned models with other pruning methods. The experiments show that the proposed method can achieve better performance than other methods with the same sparsity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The writing and presentation of this paper are quite good. The logic of the article is very clear, and the choice of words in the writing is also very precise.\n2. The idea of introducing tunable visual prompts into the pruning pipeline is intriguing. The experiments validate the effectiveness of the proposed strategy.\n3. The authors compare the proposed method with other pruning methods and additionally apply the proposed visual prompt pruning strategy to these methods, demonstrating the transferability of their approach."
                },
                "weaknesses": {
                    "value": "1. Introducing visual prompts into vision models seems to boost their performance. However, comparing models with visual prompts (the proposed method) to those without (other baseline methods) might not be entirely fair. What if we apply both the proposed and baseline methods to a model that has already been fine-tuned with visual prompts?\n2. I have some doubts regarding the generality and performance of this paper.\n(1) Why must we conduct experiments on downstream tasks of ImageNet? Why not directly on ImageNet itself, since most pruning work actually focuses more on performance on ImageNet?\n(2) The method proposed in this paper seems to be limited to scenarios where visual prompts can be applied, with their main application currently being in classification tasks. How can the proposed approach be used for other tasks, such as detection, segmentation, etc.?"
                },
                "questions": {
                    "value": "Please refer to the weaknesses. I hope the authors can provide more experiments to demonstrate the effectiveness of the method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2675/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2675/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2675/Reviewer_8b5d"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2675/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816110016,
            "cdate": 1698816110016,
            "tmdate": 1699636208352,
            "mdate": 1699636208352,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aJp6g2PqOa",
                "forum": "96nX9xIIx2",
                "replyto": "izPKUiwMba",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point-to-point Response to Reviewer 8b5d"
                    },
                    "comment": {
                        "value": "Many thanks to reviewer 8b5d for acknowledging that our proposal is \u201cintriguing\u201d, our experiments \u201cvalidate the effectiveness\u201d of our method, our writing and presentation are \u201cquite good\u201d, and we demonstrate the \u201ctransferability\u201d of our approach. We sincerely appreciate all constructive suggestions, which help us to improve our paper further. To address reviewer 8b5d\u2019s questions, we provide pointwise responses below.\n\n**[Cons 1. Comparing models with visual prompts to those without might not be entirely fair.]** \n\nWe respectfully disagree. We have tried our best to guarantee fairness in the empirical results shown in [Figure 7](https://imgur.com/zqmXSrW). Specifically, in the setting of [Figure 7](https://imgur.com/zqmXSrW): \n1. [For our method (VPNs)] We leverage VP in mask finding and subnetwork tuning. \n2. [For baselines (VPNs w. Random, VPNs w. OMP, VPNs w. LTH)] We apply the same VP as our method to Random, OMP, and LTH in mask finding and subnetwork tuning. \n\nBoth our method and baselines use exactly the same fine-tuning of subnetwork weights and VP. But our method apparently surpasses all baselines with visual prompting at all sparsity levels as shown in [Figure 7](https://imgur.com/zqmXSrW), which indicates the superiority of our method in the \u201centirely fair\u201d setting.\n\n**Additional experiments of baselines with VP.** To further address reviewer 8b5d\u2019s concern, we conduct supplemental experiments using our method and some of the best baselines such as LTH and OMP with VP on ImageNet pre-trained ResNet-18 network and Tiny-imageNet. We observe that **our method is superior to baselines with VP** as displayed in Table R1. We also include the results in the revision of our paper in Table A4. Due to the time limitation of rebuttal, we provide outcomes at 50% and 90% sparsity levels, and more sparsity levels are promised in our final version. \n\nTable R1. Performance comparison of our method, VPNs w. LTH, and VPNs w. OMP on ImageNet pre-trained ResNet-18 and Tiny-ImageNet.\n| Method | 50% sparsity | 90% sparsity |\n| :----------: | :----------: | :----------: |\n| VPNs w. LTH | 73.71 | 67.03 |\n| VPNs w. OMP | 73.56 | 64.78 |\n| VPNs (ours) | 73.82 | 67.89 |\n \n**[Cons 2. Why not directly conduct experiments on ImageNet itself?]** \n\nThank you for the great advice. We provided results of pruning on ImageNet and fine-tuning on downstream datasets for transfer study in [Figure 6](https://imgur.com/pOVS2DX). \n\n**Additional experiments on ImageNet.** To further alleviate reviewer 8b5d\u2019s concern, we conducted additional experiments on ImageNet using our method and some of the best baselines such as HYDRA and OMP, on ImageNet pre-trained ResNet-18 network. **Our method has {0.52%, 2.87%} higher accuracy** than {HYDRA, OMP} at 50% sparsity level as shown in Table R2, which illustrates the effectiveness of our method on ImageNet.  We also report the results in the revision of our paper in Table A5. Due to the time limitation of rebuttal, we provide outcomes at 50% and 90% sparsity levels, more sparsity levels are promised in our final version.\n\nTable R2. Performance comparison of our method, HYDRA, and OMP on ImageNet pre-trained ResNet-18 and ImageNet.\n| Method | 50% sparsity | 90% sparsity |\n| :----------: | :----------: | :----------: |\n| HYDRA | 68.91 | 66.62 |\n| OMP | 69.31 | 64.27 |\n| VPNs (ours) | 69.47 | 67.14 |\n\n**[Cons 3. How can the proposed approach be used for other tasks, such as detection?]**\n\nThank you for the great point. Following the reviewer\u2019s suggestion, we carried out additional experiments on Pascal VOC 2007 [1], which is one of the most widely used datasets for object detection tasks. \n\n**Additional experiments on Pascal VOC 2007.**  We compared our method to some of the best baselines such as HYDRA and OMP on YOLOv4 [2] which uses ImageNet pre-trained ResNet-18 as the backbone. The outcomes are presented in Table R3. **Our method achieves {3.78%, 2.67%} higher AP** than {HYDRA, OMP} at 90% sparsity level, which demonstrates the superiority of our method on object detection. We add the results to the revised paper in Table A6. Due to the time limitation, we provide outcomes only at two sparsity levels, more sparsity levels are promised in our final version. \n\nTable R3. AP comparison of our method, HYDRA, and OMP on YOLOv4 with ImageNet pre-trained ResNet-18 backbone and Pascal VOC 2007.\n| Method | 50% sparsity | 90% sparsity |\n| :----------: | :----------: | :----------: |\n| HYDRA | 35.25 | 32.74 |\n| OMP | 35.01 | 33.85 |\n| VPNs (ours) | 38.37 | 36.52 |\n\n[1] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303\u2013338, 2010.\n\n[2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302409575,
                "cdate": 1700302409575,
                "tmdate": 1700304327314,
                "mdate": 1700304327314,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R9B5IkgVP2",
                "forum": "96nX9xIIx2",
                "replyto": "izPKUiwMba",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8b5d"
                    },
                    "comment": {
                        "value": "Dear Reviewer **8b5d**,\n\nWe thank reviewer **8b5d** time for the review and constructive comments. We really hope to have a further discussion with the reviewer **8b5d** to see if our response solves the concerns.\n\nIn our response, we have (1) clarified the fairness of our setting and carried out supplemental experiments on baselines with visual prompting; (2) conducted additional experiments on ImageNet and object detection tasks, further demonstrating the superiority of our method.\n\nWe genuinely hope reviewer **8b5d** could kindly check our response. Thanks!\n\nBest wishes,\n\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472853707,
                "cdate": 1700472853707,
                "tmdate": 1700472853707,
                "mdate": 1700472853707,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9gGnSIKuQq",
                "forum": "96nX9xIIx2",
                "replyto": "izPKUiwMba",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Last Day Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer **8b5d**, \n\nWe genuinely appreciate your dedicated time and effort in reviewing our work. As the final day of the discussion period approaches, we kindly request that you share any additional questions or concerns you may have. We are eager to engage in further discussions with you. \n\nIf our responses have adequately addressed your concerns, we kindly ask that you consider raising the score of our work. Again, we are truly grateful for your valuable time and efforts. \n\nWarm regards, \n\nThe Authors"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620377653,
                "cdate": 1700620377653,
                "tmdate": 1700620377653,
                "mdate": 1700620377653,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cEx98LeAQn",
                "forum": "96nX9xIIx2",
                "replyto": "aJp6g2PqOa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2675/Reviewer_8b5d"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2675/Reviewer_8b5d"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for you response. The additional experiments have addressed my concerns."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630990270,
                "cdate": 1700630990270,
                "tmdate": 1700630990270,
                "mdate": 1700630990270,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]