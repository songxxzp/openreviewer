[
    {
        "title": "Graph Inference Acceleration by Bridging GNNs and MLPs with Self-Supervised Learning"
    },
    {
        "review": {
            "id": "f7IootBKsy",
            "forum": "AhCdJ93Wmi",
            "replyto": "AhCdJ93Wmi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3002/Reviewer_rTyJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3002/Reviewer_rTyJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel method, SSL-GM, for graph inference acceleration. The primary idea is to bridge GNNs and Multi-Layer Perceptrons (MLPs) through Self-Supervised Learning (SSL) to integrate structural information into MLPs. SSL-GM employs self-supervised contrastive learning to align the representations of GNNs and MLPs. Besides, it uses non-parametric aggregation, augmentation, and reconstruction regulation to avoid potential model collapse, improve model training, and prevent representation shift, respectively. The extensive experiments demonstrate SSL-GM's empirical superiority over existing models in terms of both efficiency and effectiveness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* It introduces a novel approach, SSL-GM, which integrates structural information into MLPs by contrastive learning between GNN and MLP outputs.\n* The empirical results over 10 datasets demonstrate the effectiveness of SSL-GM.\n* The paper is well-written and structured, with clear explanations of the methodology and theoretical insights."
                },
                "weaknesses": {
                    "value": "* In this paper, the structure-aware ability of SSL-GM is supported by the objective $\\mathcal{L}_{cont}$. However, its success depends on the quality of the learned representations from the GNN, rather than solely on the capabilities of the student MLP. In other words, $\\mathcal{L}{cont}$ enforces the student MLP to output representations similar to the aggregated representations from the GNN but does not inherently empower the student with structure-aware abilities. For unseen nodes during training, the student MLP may encounter challenges in learning structure-aware representations without the supervision provided by the GNN.\n* It's worth considering whether addressing the representation shift issue is necessary. The absence of the reconstruction strategy only leads to a slight decrease in performance (as shown in Table 1, \"w/o Rec.\" column), indicating that this strategy may not be indispensable in some cases.\n* To enhance reproducibility and ensure accurate results, it would be helpful if the authors could provide detailed information about the experimental environment used for SSL-GM.  I used the Google Colab platform to rerun the source code but obtained $83.80_{\\pm0.46}$ in the Cora dataset compared to  $84.60_{\\pm0.24}$ in the paper."
                },
                "questions": {
                    "value": "Please refer to the Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3002/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3002/Reviewer_rTyJ",
                        "ICLR.cc/2024/Conference/Submission3002/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3002/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697686991111,
            "cdate": 1697686991111,
            "tmdate": 1700534704038,
            "mdate": 1700534704038,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4mY1nRlSH0",
                "forum": "AhCdJ93Wmi",
                "replyto": "f7IootBKsy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rTyj - Part 1"
                    },
                    "comment": {
                        "value": "Thank you for your detailed comments and constructive suggestions. To address your concerns, we provide the following responses.\n\n> Q1. In this paper, the structure-aware ability of SSL-GM is supported by the objective $L_{cont}$. However, its success depends on the quality of the learned representations from the GNN, rather than solely on the capabilities of the student MLP. In other words, $L_{cont}$ enforces the student MLP to output representations similar to the aggregated representations from the GNN but does not inherently empower the student with structure-aware abilities. For unseen nodes during training, the student MLP may encounter challenges in learning structure-aware representations without the supervision provided by the GNN.\n\nThank you for your insightful comment regarding the structure-aware capabilities of our SSL-GM model. We appreciate the opportunity to clarify how our model addresses the limitations of traditional GNN-MLP methods and its independence from GNN supervision.\n\n**Addressing Limitations of GNN-MLP Methods.** Our primary goal with SSL-GM is to overcome the challenges faced by existing GNN-MLP models that depend heavily on knowledge distillation from teacher GNNs to student MLPs. We found that the performance of student MLPs in these models is tightly bound to the quality of pre-trained GNNs, leading to limited generalizability, especially for unseen nodes. traditional methods, SSL-GM injects high-order information into MLPs through self-supervised learning (SSL), negating the need for a well pre-trained GNN. This approach ensures that the GNN representations serve as high-order approximations of the MLP representations.\n\n**Inherent High-Order Information in MLP Representations.** By maximizing the mutual information between MLP representations and the approximated GNN representations, we enable the MLP to inherently contain high-order structural information. This mechanism is key to ensuring the structure-aware capabilities of the student MLP independently of the GNN. To validate this capability, we conducted experiments in inductive and cold-start settings, assessing the model's effectiveness on unseen nodes. The results, as presented in Tables 2 and 3 of our paper, demonstrate the robustness of SSL-GM in these challenging scenarios.\n\n**Inductive and Cold-Start Settings.** The inductive and cold-start experiments provide concrete evidence of SSL-GM's ability to generalize well to unseen nodes, a critical measure of its structure-aware capabilities. These settings were specifically chosen to test the model performance in scenarios where traditional GNN-MLP methods typically struggle. \n\n\n> Q2. It's worth considering whether addressing the representation shift issue is necessary. The absence of the reconstruction strategy only leads to a slight decrease in performance (as shown in Table 1, \"w/o Rec.\" column), indicating that this strategy may not be indispensable in some cases.\n\nThank you for your observation regarding the impact of the reconstruction strategy on our SSL-GM model's performance. We appreciate this opportunity to discuss the conditions under which this strategy is particularly impactful.\n\nWe concur that the significance of the representation shift issue is closely tied to the size and complexity of the graph. In large-scale graphs with extensive and complex structures, addressing representation shift becomes increasingly important. As illustrated in our experiments, particularly on datasets like Arxiv, the absence of the reconstruction regularizer led to a noticeable performance drop (nearly 1 percentage point). This indicates that for large-scale graphs, the reconstruction strategy plays a crucial role in maintaining the integrity and quality of representations.\n\nOur findings highlight the need for a strategic use of the reconstruction strategy in SSL-GM, applying it judiciously based on the characteristics of the graph at hand. For larger graphs, the strategy is indispensable for maintaining high-quality representations and overall performance. We plan to continue exploring the dynamics of representation shift across different graph sizes and types, further refining our understanding of when and how to effectively implement reconstruction strategies in our model."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700073458290,
                "cdate": 1700073458290,
                "tmdate": 1700073600476,
                "mdate": 1700073600476,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JJu9sehz9A",
                "forum": "AhCdJ93Wmi",
                "replyto": "f7IootBKsy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rTyj - Part 2"
                    },
                    "comment": {
                        "value": "> Q3. To enhance reproducibility and ensure accurate results, it would be helpful if the authors could provide detailed information about the experimental environment used for SSL-GM. \n\nThank you for emphasizing the importance of reproducibility in our research. We agree that providing detailed information about our experimental setup is crucial for validating and replicating our results. Below is a comprehensive outline of our experimental environment:\n```\npython==3.8.0\ntorch==2.10\ntorch_geometric==2.3\ntorch_scatter==2.1.2\ntorch_cluster==1.6.3\ntorch_sparse==0.6.18\nogb==1.3.6\n```\n\n\n**Code Availability and Bug Mitigation.** We have meticulously reviewed and updated our code to address potential bugs and ensure consistent performance. The updated code is readily available for download and can be executed in environments such as Google Colab.\n\n**Step-by-Step Guide for Replication in Colab.**\n\n- **Code Setup.** Download and upload the updated code to Google Colab. \n- **Environment Installation.** Execute the following commands in Colab to set up the necessary environment. \n\n```\n!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n!pip install torch_geometric\n!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.1.0+cu118.html\n!pip install ogb\n```\n\n- **Execution of Experiments.** Run the code using the command. \n\n```\n!python main.py --setting trans --dataset cora --use_params --seed 1\n```\n\nFor comprehensive results, average the outcomes over ten different seeds (1 to 10), and make 10 train/validation/test splits for each seed to minimize the impact of randomness. In our experiments on Colab, the accuracy for Cora over 10 seeds typically peaks at epochs 200 or 300. The averaged accuracy is $84.63\u00b10.36$.\n\n**Ensuring Reproducibility.**\n\nWe encourage researchers to follow these guidelines for replicating our experiments. Our approach ensures a high degree of reproducibility and accuracy, allowing others to validate and build upon our work confidently.\n\nWe hope this detailed information assists in the replication and verification of our experimental results. Our commitment to open and reproducible research is paramount, and we are grateful for the opportunity to share these details.\n\n---\n\nIn light of these responses, we hope we have addressed your concerns, and hope you will consider increasing your score. If we have left any notable points of concern unaddressed, please do share and we will attend to these points."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700073493620,
                "cdate": 1700073493620,
                "tmdate": 1700074607925,
                "mdate": 1700074607925,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "krj9OPcXs6",
                "forum": "AhCdJ93Wmi",
                "replyto": "4mY1nRlSH0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3002/Reviewer_rTyJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3002/Reviewer_rTyJ"
                ],
                "content": {
                    "title": {
                        "value": "Questions about Inductive Setting and Cold-start Setting."
                    },
                    "comment": {
                        "value": "Thank you for the clarification. I appreciate the authors' responses in helping me understand the work better. However, I still have concerns regarding the experimental outcomes presented in Table 2 and Table 3, which showcase the primary contribution of SSL-GM in terms of experiments.\n\nIf I understand correctly, the cold-start setting can be seen as a non-structure-in-inductive-set version of the inductive setting. In the case of NOSMOG, which relies on positional encoding, it is expected to experience performance drops compared to the inductive setting, as observed in Table 3 and Table 2. For MLP and SSL-GM, which do not require structural information from the inductive set, slight performance differences between the two settings can be attributed to variations in splits or seeds.\n\nHowever, the performance discrepancy in the cold-start setting for GLNN, which also disregards structural information, is quite significant. For instance, on Cora, the performance is 78.34 in the inductive setting compared to 71.96 in the cold-start setting. Similarly, on Amazon-CS/Amazon-Photo/Arxiv, the performance is 87.04/93.28/63.53 in the inductive setting compared to 83.98/91.05/60.55 in the cold-start setting. It is expected that the GNN teacher should perform similarly in the observed set, which is the same or similar for both settings, indicating comparable inference abilities acquired by GLNN. Regarding Citeseer, Co-CS, and Co-Phy, the results appear to be reasonable.\n\nThese variations in both settings suggest that there might be significant differences when evaluating GLNN in the two settings. It raises concerns about the experimental results of the baselines, as it seems to be an unfair comparison. Additionally, the details of the cold-start setting, including implementation and hyperparameters, are not provided in the source codes, which adds further uncertainty."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700451221056,
                "cdate": 1700451221056,
                "tmdate": 1700451221056,
                "mdate": 1700451221056,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IleiLTtsFB",
                "forum": "AhCdJ93Wmi",
                "replyto": "f7IootBKsy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3002/Reviewer_rTyJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3002/Reviewer_rTyJ"
                ],
                "content": {
                    "title": {
                        "value": "Question about the implications of SSL-GM in practical applications"
                    },
                    "comment": {
                        "value": "Thank you for the authors' response. I appreciate their clarification, which has addressed my concerns regarding the experimental settings.\n\nI would like to further discuss the implications of SSL-GM in practical applications. While SSL-GM offers improvements in addressing the limitations of GNN-MLP methods, it introduces **an additional training process** using a Logistic regression function for test nodes or graphs based on downstream tasks. This approach may hinder the inherited advantage of GNN-MLP techniques, particularly in latency-constrained applications. For example, in node classification tasks where the deployed model needs to handle an arbitrary number of test nodes at any given time, traditional GNN-MLP methods can perform inference on all the nodes simultaneously, while SSL-GM requires additional training time, **resulting in increased latency**. This trade-off between performance and speed could potentially reduce the primary contribution of SSL-GM in addressing the limitations of GNN-MLP methods. \n\nAdditionally, I have noticed the extension of SSL-GM to supervised learning. It is worth mentioning that the performance of SSL-GM drops significantly on the Cora dataset in this extension. Furthermore, the reported results are limited to the semi-supervised setting. I am particularly interested in the evaluation of SSL-GM in the inductive and cold-start settings, which would help validate its value in addressing the limitations of GNN-MLP methods while preserving their practical advantages."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532777231,
                "cdate": 1700532777231,
                "tmdate": 1700532793249,
                "mdate": 1700532793249,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1NbW5axGtZ",
            "forum": "AhCdJ93Wmi",
            "replyto": "AhCdJ93Wmi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3002/Reviewer_eCUd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3002/Reviewer_eCUd"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of accelerating GNN inference by training an MLP on the node features. It proposes to use self-supervised learning to train the MLP and achieves strong empirical results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper is well-written and easy to follow.\n2.\tThe empirical results are rich and significantly outperform the baselines."
                },
                "weaknesses": {
                    "value": "1.\tTheorem 1 is possibly wrong. I check the proof in the appendix, which basically connects each term in (17) to each term in (13) and leads to two problems. (1) The authors seem that minimizing each term in (17) also minimizes the corresponding term in (13). Pls prove these conclusions rigorously instead of only using intuitively explanations. (2) Even if the first point holds, (13) minimizes the summarization of 4 terms, and thus the minimizer may not be the minimizers of the 4 terms. Thus, there is no guarantee that (13) and (16) will have the same minimizer. Nowadays, many machine learning papers have theorems but my opinion is that theorems should be rigorous. Moreover, I failed to follow the mutual information part of Section 5, especially how the authors transform different models into mutual information forms. If this can be done, pls conduct the mathematical transformations in rigorous ways.\n2.\tIt is unclear what are the challenges of using self-supervised learning (SSL) to train MLP and what are the new designs of the paper. SSL is widely used for graph learning, and the author should be very specific in the challenges of using it to train MLP. Currently, descriptions of the weak points of existing works, e.g., \u201cinsufficient exploration of structural information when inferring unseen nodes\u201d, \u201ccannot fully model the structural information\u201d are rather vague. In section 3, the authors propose several techniques and loss terms, e.g., alignment loss, data augmentation, and reconstruction loss. These are not new for SSL, which are also evidenced by the citations provided by the authors. The question is that what are the new things proposed by the authors. The paper will be stronger if the authors can connect the challenges and the proposed new techniques. \n3.\tExperiments can be improved. (1) Pls provide the time for model training, which is also an important practical consideration. (2) Pls run the experiments on large practical datasets, e.g., Papers100M, MAG240M, and IGB.\n====================================================\nI have reade the author response. Instead of addressing my concerns, the author response raises more concerns. As such, I decide to lower my rating."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3002/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3002/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3002/Reviewer_eCUd"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3002/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698114315608,
            "cdate": 1698114315608,
            "tmdate": 1700635475944,
            "mdate": 1700635475944,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PxXVVTVxmp",
                "forum": "AhCdJ93Wmi",
                "replyto": "1NbW5axGtZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eCUd - Part 1"
                    },
                    "comment": {
                        "value": "Thank you for your detailed comments and constructive suggestions. To address your concerns, we provide the following responses.\n\n> Q1. The authors seem that minimizing each term in (17) also minimizes the corresponding term in (13). Pls prove these conclusions rigorously instead of only using intuitively explanations. \n\nThank you for highlighting the need for a more rigorous explanation of the connections between the terms in equations (17) and (13). We appreciate the opportunity to delve deeper into the mathematical underpinnings of our model. First of all, we want to emphasize that we consider Eq. (13) only as an instantiation of Eq. (17). Here we will reinterpret the proof process. \n\n**Information Bottleneck Perspective.** We start by reinterpreting equation (17) from the perspective of the information bottleneck principle. The equation aims to jointly optimize four terms, each representing a different aspect of the information bottleneck:\n\n$$\n\\mathbf{T}^* = argmin_{\\mathbf{H}, \\mathbf{Z}}  \\lambda H(\\mathbf{H} | \\mathcal{G}_\\mathcal{I})  + \\lambda H(\\mathbf{Z} | \\mathbf{H}, {\\mathcal{G}}_I) + H(\\mathbf{Z}) + H(\\mathbf{H} | \\mathbf{Z})\n$$\n\nwhere $\\mathbf{T} = (\\mathbf{H}, \\mathbf{Z})$, $\\mathbf{H}$ and $\\mathbf{Z}$ are the representations learned by MLPs and GNNs, respectively. \n\n**Modeling the Terms.** We model the first term by minimizing the distance between $\\mathbf{H}$ and $\\mathbf{F}$, represented as $\\| \\mathbf{H} - \\mathbf{F}^* \\|$. For the second term, we minimize the distance between $\\mathbf{F}$ and $\\mathbf{Z}$, corresponding to $\\| \\mathbf{Z} - \\mathbf{F}^* \\|$. The third term, $H(\\mathbf{H} | \\mathbf{Z})$, is approached by optimizing the covariance between $\\mathbf{Z}$ and $\\mathbf{H}$, expressed as $\\sum_i \\text{Cov}(\\mathbf{H}_i, \\mathbf{Z}_i)$. To minimize the last term, $H(\\mathbf{Z})$, we make $\\mathbf{Z}$ invariant to a specific variable, using $\\mathbf{Z}$ to reconstruct $\\mathbf{X}$. \n\nCombining these four terms, with some transformation, we derive our final optimization objective:\n$$\n\\mathbb{E}_{\\hat{\\mathbf{A}}, \\hat{\\mathbf{X}}} \\left[ \\| \\hat{\\mathbf{H}} - \\mathbf{F}^* \\|^2 + \\| \\hat{\\mathbf{Z}} - \\mathbf{F}^* \\|^2 +  \\| \\mathcal{D}(\\hat{\\mathbf{Z}}) - \\mathbf{X} \\|^2  | \\hat{\\mathbf{A}}, \\hat{\\mathbf{X}}  \\right] - 2  {\\mathbb{E}} \\left[ \\sum_i \\text{Cov}(\\hat{\\mathbf{H}}_i, \\hat{\\mathbf{Z}}_i) |  \\hat{\\mathbf{A}}, \\hat{\\mathbf{X}}, \\mathbf{F}^* \\right]\n$$\n\n**Clarification on the Connection Between Eq. (13) and Eq. (17).** It is important to note that a direct, rigorous connection between the terms in equations (17) and (13) is not necessary. Our final objective serves as a practical implementation of the information bottleneck concept, tailored to the specific context of our model.\n\nIn summary, our approach to optimizing equation (17) is grounded in the principles of the information bottleneck, with each term contributing to the overarching goal of learning robust and informative representations. We believe this explanation provides a more rigorous understanding of our method's mathematical foundation.    \n\n\n> Q2. Even if the first point holds, (13) minimizes the summarization of 4 terms, and thus the minimizer may not be the minimizers of the 4 terms. Thus, there is no guarantee that (13) and (17) will have the same minimizer. Nowadays, many machine learning papers have theorems but my opinion is that theorems should be rigorous. \n\nThank you for your critical observation regarding the relationship between the minimizers of equations (13) and (17). We appreciate this opportunity to clarify our approach and address the need for theoretical rigor.\n\n**Differentiation Between Eq. (13) and (17).** As discussed, equation (13) represents an implementation of the broader conceptual framework outlined in equation (17). Our approach to minimizing equation (17) through equation (13) is a practical instantiation rather than a direct equivalence. We acknowledge that the minimizer of equation (13) may not necessarily be the same as the individual minimizers of the four terms in equation (17). The composite nature of equation (13) means that its minimization involves a balance among these terms, which may not align perfectly with their individual minimization.\n\n**Future Research Directions.** This realization forms the basis for our future research. We aim to develop more refined implementations that bring the minimization of equation (13) closer to the ideal minimization of equation (17). This will involve exploring methods with lower upper bounds and potentially tighter convergence to the theoretical optimum. We also recognize the importance of theoretical rigor in our assertions and theorems. Moving forward, we will focus on strengthening the mathematical foundations of our model, ensuring that our theoretical claims are robustly supported by rigorous analysis."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700071694651,
                "cdate": 1700071694651,
                "tmdate": 1700073559872,
                "mdate": 1700073559872,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CdrY0jYIpB",
                "forum": "AhCdJ93Wmi",
                "replyto": "1NbW5axGtZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eCUd - Part 2"
                    },
                    "comment": {
                        "value": "**Remark.** The current implementation, as outlined in equation (13), represents the first step in an ongoing process to optimize equation (17). We are committed to continuous improvement and refinement of our methods. Your feedback is invaluable in guiding our efforts towards more rigorous and effective model development. It helps us identify areas for theoretical and practical enhancement in our research.\n\n> Q3. Moreover, I failed to follow the mutual information part of Section 5, especially how the authors transform different models into mutual information forms. If this can be done, pls conduct the mathematical transformations in rigorous ways.\n\nThank you for pointing out the need for a more rigorous and detailed mathematical explanation of the transformations into mutual information forms for various models. We appreciate this opportunity to delve into the specifics of these transformations.\n\n**Transformation Process.** Based on the findings in [1], optimizing the cross-entropy $\\text{CE}(\\mathbf{X}, \\mathbf{Y})$ between $\\mathbf{X}$ and $\\mathbf{Y}$ corresponds to maximizing the mutual information $\\text{MI}(\\mathbf{X}, \\mathbf{Y}) = \\sum_{i\\in \\mathcal{V}} I(\\mathbf{y}_i, \\mathbf{x}_i)$ between $\\mathbf{X}$ and $\\mathbf{Y}$. \n\n[1] Malik Boudiaf, Jerome Rony, Imtiaz Masud Ziko, Eric Granger, Marco Pedersoli, Pablo Piantanida, and Ismail Ben Ayed. A unifying mutual information view of metric learning: cross-entropy vs. pairwise losses. In ECCV, 2020.\n\n**Model-Specific Transformation.** The specific transformation is summarized in the following table. \n\n|               | Objective                                                                      |\n|---------------|------------------------------------------------------------------------------------|\n| MLP           | $\\sum_{i \\in \\mathcal{V}} I(\\mathbf{y}_i, \\mathbf{x}_i)$                           |\n| GNN           | $\\sum_{i \\in \\mathcal{V}} I(\\mathbf{y}_i; \\mathcal{S}_i)$                          |\n| GLNN          | $\\sum_{i \\in \\mathcal{V}} I(\\mathbf{x}_i; \\mathbf{y}_i \\| \\mathcal{S}_i)$           |\n| NOSMOG & GENN | $\\sum_{i \\in \\mathcal{V}} I(\\mathbf{x}_i; \\mathbf{y}_i \\| \\mathcal{S}_i) + I(\\mathbf{y}_i; \\mathbf{A}^{[i]})$    |\n| SSL-GM (ours) | $\\sum_{i \\in \\mathcal{V}} I(\\mathbf{y}_i; \\mathbf{x}_i \\| \\mathcal{S}_i) + I(\\mathbf{x}_i; \\mathcal{S}_i)$ |\n\n- **MLPs.** For MLPs optimizing the cross-entropy between node features and labels, the objective transforms directly into $\\sum_{i\\in \\mathcal{V}} I(\\mathbf{y}_i, \\mathbf{x}_i)$. \n\n- **GNNs.** GNNs consider both structural information and node features. We define the computational graph surrounding node $i$ as $S_i = (\\mathbf{X}^{[i]}, \\mathbf{A}^{[i]})$, where $\\mathbf{X}^{[i]}$ is the node features of neighborhoods of target node $i$ and $\\mathbf{A}^{[i]}$ is the corresponding subgraph structure, transforming the GNN learning objective into $\\sum_{i \\in \\mathcal{V}} I(\\mathbf{y}_i; \\mathcal{S}_i)$. \n\n- **GLNN.** For GLNN that uses MLPs to mimic the predictions of GNNs, we denote the outputs of GNNs as $y_i | S_i$ and thus convert the learning objective $\\sum_{i \\in \\mathcal{V}} I(\\mathbf{x}_i; \\mathbf{y}_i | \\mathcal{S}_i)$.\n\n- **NOSMOG and GENN.** The following works of GLNN further add structural information in the learning objective, denoted as $\\sum_{i \\in \\mathcal{V}} I(\\mathbf{x}_i; \\mathbf{y}_i | \\mathcal{S}_i) + I(\\mathbf{y}_i; \\mathbf{A}^{[i]})$. \n        \n- **SSL-GM (Ours).** Our proposed SSL-GM applies contrastive learning to maximize the mutual information between the node features and the high-order representations first and then use the learned representations to do classification, denoted as $\\sum_{i \\in \\mathcal{V}} I(\\mathbf{y}_i; \\mathbf{x}_i | \\mathcal{S}_i) + I(\\mathbf{x}_i; \\mathcal{S}_i)$. \n\n**Detailed Explanation.** In the context of MLPs, the focus is on maximizing the mutual information solely between the labels and node features. This approach, however, does not account for structural information inherent in the graph data, potentially limiting the depth of understanding that can be derived from the features alone. GNNs extend this concept by maximizing the mutual information between the labels and the corresponding subgraphs. This approach captures both semantic (feature-based) and structural (graph topology) information, offering a more holistic representation of the graph data. GLNNs further enhance this framework by maximizing the mutual information between node features and the soft labels generated by pre-trained GNNs. This approach leverages high-level abstract information retained in GNN outputs, enriching the feature representation with insights gleaned from the graph structure. NOSMOG and GENN incorporate an additional layer of complexity by maximizing mutual information between the subgraph of the target node and its labels. This addition emphasizes fine-grained structural information, further enriching the model understanding of the graph's intricacies."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072650222,
                "cdate": 1700072650222,
                "tmdate": 1700072909160,
                "mdate": 1700072909160,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i5PIzgTLCq",
                "forum": "AhCdJ93Wmi",
                "replyto": "1NbW5axGtZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eCUd - Part 3"
                    },
                    "comment": {
                        "value": "In our SSL-GM approach, the second term of mutual information maximization is directly optimized. We posit that when this second term is maximized, it effectively transforms the first term into $\\sum_{i \\in \\mathcal{V}} I(y_i; x_i | S_i) = \\sum_{i \\in \\mathcal{V}} I(y_i; S_i)$. This transformation aligns our objective with that of traditional GNNs, focusing on maximizing the mutual information between labels and subgraphs, thereby capturing both semantic and structural elements of the graph.\n\nIn summary, each model adopts a unique approach to maximizing mutual information, progressively incorporating more complex layers of graph information. Our SSL-GM method synthesizes these approaches, aiming to capture a comprehensive representation of both node features and graph structure.\n\n> Q4. It is unclear what are the challenges of using self-supervised learning (SSL) to train MLP and what are the new designs of the paper. SSL is widely used for graph learning, and the author should be very specific in the challenges of using it to train MLP. Currently, descriptions of the weak points of existing works, e.g., \u201cinsufficient exploration of structural information when inferring unseen nodes\u201d, \u201ccannot fully model the structural information\u201d are rather vague. In section 3, the authors propose several techniques and loss terms, e.g., alignment loss, data augmentation, and reconstruction loss. These are not new for SSL, which are also evidenced by the citations provided by the authors. The question is that what are the new things proposed by the authors. The paper will be stronger if the authors can connect the challenges and the proposed new techniques.\n\n\nThank you for your insightful comment. We appreciate the opportunity to clarify the innovative aspects of our work, particularly in addressing the limitations of existing GNN-MLP methods in inductive, cold-start, and label sparsity settings.\n\n\n- **Insight on Generalization Issues.** Our core contribution does not lie in new model architectures and new objectives. Instead, our work identifies the lack of generalization in current methods as a key issue, stemming from the limitations of knowledge distillation (KD) that aligns GNN and MLP outputs only at the label level. We propose that aligning these models in the representation space using self-supervised learning (SSL) can yield more generalizable representations. This insight represents a significant shift from existing practices.\n        \n- **Challenges in SSL Alignment.** Different from KD methods, aligning GNNs and MLPs under SSL is challenging due to the absence of a pre-trained GNN. To this end, we convert the objective to jointly train GNN and MLP by aligning the 0-hop representations (MLP output) with high-order representations (GNN output). Our approach is applicable with various objectives like Bootstrap, InfoNCE, or MSE. This flexibility in alignment strategy is a novel contribution to the field.\n\n- **Addressing Representation Inconsistency and Model Collapse.** We observed inconsistency in representations between GNNs and MLPs, potentially leading to model collapse (Appendix D.3). Our solution involves using MLP outputs to approximate GNN outputs, a strategy akin to SGC but applied to a different problem context. \n\n- **Graph Augmentation and Reconstruction Regularizer.** We further incorporate data augmentation to enhance the quality of the learned representations. However, given that structural permutation in augmentation can significantly shift GNN representations, we introduce a reconstruction regularizer to mitigate representation shift. This addition is critical in ensuring the stability and reliability of the learned representations.\n\n\n> Q5. Pls provide the time for model training, which is also an important practical consideration. \n\nThank you for your valuable feedback. In response to your comment, we have presented additional experimental results in Appendix C of our paper to further validate the efficacy of our SSL-GM model in inference acceleration.\n\n- **Accuracy vs Efficiency.** We present a comprehensive comparison between our method and other acceleration methods. The results showcase the inference time and accuracy for various methods, including SGC, APPNP, Quantization (QSAGE), Pruning (PSAGE), and Neighbor Sample on Flickr and OGB-Arxiv. We have intentionally omitted GNN-MLP methods that were compared in Figure 3 for brevity. Please note that the time consumption of SAGE and BGRL are the same since the learning process of SSL does not affect inference. \n\n- **Different Settings.** To ensure a thorough evaluation, we chose three datasets across all node classification settings, including transductive, inductive, and cold-start. Our results, as shown in the additional tables, clearly indicate that SSL-GM outperforms existing methods in terms of efficiency and accuracy. \n\n[Experimental results are following]"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072714518,
                "cdate": 1700072714518,
                "tmdate": 1700072784343,
                "mdate": 1700072784343,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cqCfeOfspu",
                "forum": "AhCdJ93Wmi",
                "replyto": "1NbW5axGtZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eCUd - Part 4"
                    },
                    "comment": {
                        "value": "| Datasets |  | SAGE      | BGRL  | SGC                  | APPNP                | QSAGE                | PSAGE                | Neighbor Sample      | **SSL-GM** |\n|------------|------------------|-----------|-------|----------------------|----------------------|----------------------|----------------------|----------------------|----------------------------|\n| Flickr     | Time (ms) | 80.7  | 80.7 (1.00$\\times$)  | 76.9 (1.05$\\times$)  | 78.1 (1.03$\\times$)  | 70.6 (1.14$\\times$)  | 67.4 (1.20$\\times$)  | 25.5 (3.16$\\times$)        | **0.9 (89.67$\\times$)** |\n|                              | Acc (\\%)  | 47.17 | 49.12                | 47.35                | 47.53                | 47.22                | 47.25                | 47.01                      | **49.27**                |\n| Arxiv       | Time (ms) | 314.7 | 314.7 (1.00$\\times$) | 265.9 (1.18$\\times$) | 284.1 (1.11$\\times$) | 289.5 (1.09$\\times$) | 297.5 (1.06$\\times$) | 78.3 (4.02$\\times$)        | **2.5 (125.88$\\times$)** |\n|                              | Acc (\\%)  | 68.52 | 69.29                | 68.93                | 69.10                | 68.48                | 68.55                | 68.35                      | **70.23**                |\n\n\n\n|           |        | Trans      | Trans          | Ind        | Ind            | Cold-start | Cold-start     |\n|-----------|--------|------------|----------------|------------|----------------|------------|----------------|\n| Dataset   | Models | Time (ms)  | Acc (\\%)       | Time (ms)  | Acc (\\%)       | Time (ms)  | Acc (\\%)       |\n| Pubmed    | SAGE   | 73         | 85.94          | 15         | 85.04          | 15         | 77.98          |\n|           | SGC    | 64         | 85.28          | 14         | 85.22          | 13         | 76.10          |\n|           | NOSMOG | 5          | 86.18          | 3          | 83.84          | 3          | 81.48          |\n|           | SSL-GM | **3** | **86.99** | **3** | **86.47** | **3** | **86.44** |\n| Amazon-CS | SAGE   | 103        | 88.88          | 31         | 87.24          | 25         | 61.01          |\n|           | SGC    | 89         | **89.31** | 26         | 87.12          | 24         | 63.08          |\n|           | NOSMOG | 5          | 87.64          | 4          | 86.61          | 4          | 81.95          |\n|           | SSL-GM | **4** | 88.46          | **3** | **87.65** | **3** | **87.58** |\n| Arxiv     | SAGE   | 485        | **72.05** | 315        | 68.52          | 305        | 43.47          |\n|           | SGC    | 410        | 69.95          | 266        | 68.93          | 250        | 42.08          |\n|           | NOSMOG | 6          | 70.84          | 4          | 69.10          | 4          | 61.64          |\n|           | SSL-GM | **4** | 71.12          | **3** | **70.23** | **3** | **66.13** |\n    \n\n\n> Q7. Pls run the experiments on large practical datasets, e.g., Papers100M, MAG240M, and IGB.\n\nThank you for your valuable suggestion to extend our experiments to include large-scale practical datasets like Papers100M, MAG240M, and IGB. Although these datasets are seldomly used in graph learning experiments, we appreciate your emphasis on the importance of testing our model in diverse and challenging real-world scenarios. \n\nWe fully acknowledge the significance of evaluating our model on these large datasets. Such an expansion in our experimental scope would undoubtedly provide deeper insights into the scalability and robustness of our model. We are committed to incorporating this suggestion into our future research plans. Conducting experiments on these larger datasets will be a key focus in our subsequent work, allowing us to further validate and refine our model.\n\n---\nIn light of these responses, we hope we have addressed your concerns, and hope you will consider increasing your score. If we have left any notable points of concern unaddressed, please do share and we will attend to these points."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072840280,
                "cdate": 1700072840280,
                "tmdate": 1700072872856,
                "mdate": 1700072872856,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bA4N1Fz4uh",
                "forum": "AhCdJ93Wmi",
                "replyto": "PxXVVTVxmp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3002/Reviewer_eCUd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3002/Reviewer_eCUd"
                ],
                "content": {
                    "title": {
                        "value": "Confused by the response"
                    },
                    "comment": {
                        "value": "(1) In the Information Bottleneck Perspective part, you directly translate some entropy expressions into some norm expressions. In my opinion, this only holds when you assume that data (or anything that can be assumed) follow certain distribution. However, the assumption part is missing and the derivation is entirely intuitively.\n\n(2) Clarification on the Connection Between Eq. (13) and Eq. (17). You say that a rigorous connection is not required. However, according to my understanding, in a rigorous proof, every step should be connected in reasonable ways. I do not understand this part.\n\nIf you are actually proving Eq. (17), pls state the theorem in terms of Eq. (17) rather than a stronger statement in  Eq. (13)."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635054885,
                "cdate": 1700635054885,
                "tmdate": 1700635054885,
                "mdate": 1700635054885,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JxqyTrG9Af",
                "forum": "AhCdJ93Wmi",
                "replyto": "i5PIzgTLCq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3002/Reviewer_eCUd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3002/Reviewer_eCUd"
                ],
                "content": {
                    "comment": {
                        "value": "I think you are reporting inference time in the tables below. However, I think training time should be reported for readers to understand the cost."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635277993,
                "cdate": 1700635277993,
                "tmdate": 1700635277993,
                "mdate": 1700635277993,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kWBlXHIenk",
                "forum": "AhCdJ93Wmi",
                "replyto": "cqCfeOfspu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3002/Reviewer_eCUd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3002/Reviewer_eCUd"
                ],
                "content": {
                    "comment": {
                        "value": "I cannot agree with the statement that \"these datasets are seldomly used in graph learning experiments\". A simple search can easily find a dozen graph learning papers that use these datasets."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635343728,
                "cdate": 1700635343728,
                "tmdate": 1700635343728,
                "mdate": 1700635343728,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Kcqsz1XW5o",
            "forum": "AhCdJ93Wmi",
            "replyto": "AhCdJ93Wmi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3002/Reviewer_WTGk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3002/Reviewer_WTGk"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies an important problem: how to accelerate graph inference and improve model generalization. For this paper, the authors propose SSL-GM to bridge graph context-aware GNNs and neighborhood dependency-free MLPs with SSL. In addition, the authors also provide theoretical analysis to prove the generalization capability of SSL-GM. Furthermore, the extensive experimental results show that the solution mentioned in this paper not only accelerates GNN inference but also exhibits significant performance improvements over vanilla MLPs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The problem studied in this paper is fundamental in the graph neural network area.\n2. The solution mentioned in this paper is basic and the performance of the method mentioned in this paper is great. The experimental results are extensive.\n3. This paper develops a theoretical analysis.\n4. The presentation of this paper is so clear that I can follow the paper easily."
                },
                "weaknesses": {
                    "value": "1. The experimental results only contain the performance over node classification and graph classification. Is it possible to evaluate the proposed method over link prediction?\n2. It seems that this paper only borrows some ideas from contrastive learning. Based on contrastive learning, this paper develops a new objective function that can be used to solve the model generalization problem. Therefore, could the authors highlight some contributions here? I think it is a good paper but the contribution of this paper is a little bit marginal. If the authors are able to emphasize their contributions here, I am willing to improve my rate."
                },
                "questions": {
                    "value": "See Strengths and Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3002/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698271146552,
            "cdate": 1698271146552,
            "tmdate": 1699636244665,
            "mdate": 1699636244665,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ppqRhVLhuA",
                "forum": "AhCdJ93Wmi",
                "replyto": "Kcqsz1XW5o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WTGk"
                    },
                    "comment": {
                        "value": "Thank you for your detailed comments and constructive suggestions. To address your concerns, we provide the following responses.\n\n> Q1. The experimental results only contain the performance over node classification and graph classification. Is it possible to evaluate the proposed method over link prediction?\n\nThank you for your insightful question regarding the application of our model to link prediction tasks. We appreciate this opportunity to discuss the adaptability of our method to different graph analysis tasks.\n\n**Applying the Model to Link Prediction.** Our model adopts self-supervised learning (SSL) approach that is inherently flexible and can be applied to link prediction without requiring significant modifications to the graph training process. To represent links, we suggest concatenating the corresponding node representations. This concatenated representation will then serve as the input to the logistic regression model for predicting the presence or absence of a link. For the inference phase in link prediction, we propose adopting a linear protocol where the parameters of the encoder are frozen. This means that during inference, only the logistic regression model, which is used for predicting links, will be trained. The link prediction task can be approached in a standard manner, involving the sampling of some edges from the original graph as positive examples and performing negative sampling for negative edges.\n\n**Future Work.** While our current experimental results do not include link prediction, we acknowledge the importance of this task in graph analysis. Therefore, we are considering extending our experimental evaluation to include link prediction, providing a comprehensive view of the applicability across various graph-related tasks. \n        \n> Q2. It seems that this paper only borrows some ideas from contrastive learning. Based on contrastive learning, this paper develops a new objective function that can be used to solve the model generalization problem. Therefore, could the authors highlight some contributions here? I think it is a good paper but the contribution of this paper is a little bit marginal. If the authors are able to emphasize their contributions here, I am willing to improve my rate.\n\n\nThank you for your insightful comment. We appreciate the opportunity to clarify the innovative aspects of our work, particularly in addressing the limitations of existing GNN-MLP methods in inductive, cold-start, and label sparsity settings.\n\n- **Insight on Generalization Issues.** Our core contribution does not lie in new model architectures and new objectives. Instead, our work identifies the lack of generalization in current methods as a key issue, stemming from the limitations of knowledge distillation (KD) that aligns GNN and MLP outputs only at the label level. We propose that aligning these models in the representation space using self-supervised learning (SSL) can yield more generalizable representations. This insight represents a significant shift from existing practices.\n        \n- **Challenges in SSL Alignment.** Different from KD methods, aligning GNNs and MLPs under SSL is challenging due to the absence of a pre-trained GNN. To this end, we convert the objective to jointly train GNN and MLP by aligning the 0-hop representations (MLP output) with high-order representations (GNN output). Our approach is applicable with various objectives like Bootstrap, InfoNCE, or MSE. This flexibility in alignment strategy is a novel contribution to the field.\n\n- **Addressing Representation Inconsistency and Model Collapse.** We observed inconsistency in representations between GNNs and MLPs, potentially leading to model collapse (Appendix D.3). Our solution involves using MLP outputs to approximate GNN outputs, a strategy akin to SGC but applied to a different problem context. \n\n- **Graph Augmentation and Reconstruction Regularizer.** We further incorporate data augmentation to enhance the quality of the learned representations. However, given that structural permutation in augmentation can significantly shift GNN representations, we introduce a reconstruction regularizer to mitigate representation shift. This addition is critical in ensuring the stability and reliability of the learned representations.\n\n---\n\nIn light of these responses, we hope we have addressed your concerns, and hope you will consider increasing your score. If we have left any notable points of concern unaddressed, please do share and we will attend to these points."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700070228646,
                "cdate": 1700070228646,
                "tmdate": 1700070228646,
                "mdate": 1700070228646,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GhwmkKrn3X",
            "forum": "AhCdJ93Wmi",
            "replyto": "AhCdJ93Wmi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3002/Reviewer_N5QN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3002/Reviewer_N5QN"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of Graph inference acceleration and summarizes two shortcomings in existing work: limited acceleration effectiveness and insufficient generalization performance. Based on insights from existing work, it is suggested that self-supervised learning can be used to infer structural information of unseen nodes from the nodes themselves. The paper introduces the SSL-GM algorithm, primarily aligning the consistency between GNN and MLP representations through self-supervised contrastive learning. This bridges GNN and MLP with self-supervised learning to achieve accelerated graph inference."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problem is novel, the challenge of accelerating graph inference still persists.\n2. The proposed algorithm demonstrates favorable performance in multiple experimental validations."
                },
                "weaknesses": {
                    "value": "1. Although a significant number of experiments were conducted, the novelty and contribution of the proposed method remain limited.\n2. In Section 3.1, the author introduces the Non-Parametric Aggregator to help align the representations of GNN and MLP. While the author explains the differences in the appendix, the aggregation method given in Equation 2 still resemble the form of APPNP. I did not find an explanation for this issue in the experimental section and other where of the paper. The author claims that, in contrast to SGC and APPNP, SSL-GM uses non-linear adjacency matrix aggregation instead of high-order adjacency matrices. So, from the perspective of acceleration effectiveness and improvement in generalization, what is the contribution of non-linear adjacency matrix aggregation to accelerating graph inference?\n3. I cannot understand Formula 4. The author injects randomness by perturbing the structure and features of the original graph, with the expectation that the MLP encoder can capture invariant key features. However, Formula 4 is perplexing. The first part of the formula computes the mutual information between G_1 and G_2, but is it merely a distinction of whether the perturbed structure is included? Why does this part enable the encoder to obtain high-quality representations? It seems more like an optimization of the random augmentation methods q_e and q_f, but the author did not clarify whether they are learnable.\n4. Given the method proposed in the paper, I believe it would be interesting to include GNN methods like SGC and APPNP in the experiments concerning acceleration effectiveness."
                },
                "questions": {
                    "value": "Please see the comments in the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3002/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752513172,
            "cdate": 1698752513172,
            "tmdate": 1699636244593,
            "mdate": 1699636244593,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7B2rwygC5d",
                "forum": "AhCdJ93Wmi",
                "replyto": "GhwmkKrn3X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer N5QN - Part 1"
                    },
                    "comment": {
                        "value": "Thank you for your detailed comments and constructive suggestions. To address your concerns, we provide the following responses.\n\n\n> Q1. Although a significant number of experiments were conducted, the novelty and contribution of the proposed method remain limited.\n\n\nThank you for your insightful comment. We appreciate the opportunity to clarify the innovative aspects of our work, particularly in addressing the limitations of existing GNN-MLP methods in inductive, cold-start, and label sparsity settings.\n\n- **Insight on Generalization Issues.** Our core contribution does not lie in new model architectures and new objectives. Instead, our work identifies the lack of generalization in current methods as a key issue, stemming from the limitations of knowledge distillation (KD) that aligns GNN and MLP outputs only at the label level. We propose that aligning these models in the representation space using self-supervised learning (SSL) can yield more generalizable representations. This insight represents a significant shift from existing practices.\n        \n- **Challenges in SSL Alignment.** Different from KD methods, aligning GNNs and MLPs under SSL is challenging due to the absence of a pre-trained GNN. To this end, we convert the objective to jointly train GNN and MLP by aligning the 0-hop representations (MLP output) with high-order representations (GNN output). Our approach is applicable with various objectives like Bootstrap, InfoNCE, or MSE. This flexibility in alignment strategy is a novel contribution to the field.\n\n- **Addressing Representation Inconsistency and Model Collapse.** We observed inconsistency in representations between GNNs and MLPs, potentially leading to model collapse (Appendix D.3). Our solution involves using MLP outputs to approximate GNN outputs, a strategy akin to SGC but applied to a different problem context. \n\n- **Graph Augmentation and Reconstruction Regularizer.** We further incorporate data augmentation to enhance the quality of the learned representations. However, given that structural permutation in augmentation can significantly shift GNN representations, we introduce a reconstruction regularizer to mitigate representation shift. This addition is critical in ensuring the stability and reliability of the learned representations.\n    \n> Q2. In Section 3.1, the author introduces the Non-Parametric Aggregator to help align the representations of GNN and MLP. While the author explains the differences in the appendix, the aggregation method given in Equation 2 still resemble the form of APPNP. I did not find an explanation for this issue in the experimental section and other where of the paper. The author claims that, in contrast to SGC and APPNP, SSL-GM uses non-linear adjacency matrix aggregation instead of high-order adjacency matrices. \n\nThank you for your comment. We acknowledge the need for a clearer explanation regarding our non-parametric aggregator and its distinction from SGC and APPNP. We aim to elucidate this in the following points:\n\n- **Objective of Non-Parametric Aggregator.** The primary goal of our non-parametric aggregator is to approximate the outputs of GNNs. Our approach does not focus on the detailed architecture of the aggregator but rather on its functional outcome. This perspective allows flexibility in either applying existing methods like SGC or APPNP as non-parametric aggregators or adapting GNNs into a non-parametric form by parameter removal.\n        \n- **Detailed Differences.** To illustrate the differences more clearly, we have included a table in our revised manuscript. This table compares the pre-transformation, aggregation, and update processes of SGC, APPNP, and our SSL-GM.\n\n| Model  | Pre-trans                                                  | Aggregation                                                                                                        | Update                                                                          |\n|--------|------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------|\n| SGC    |$\\mathbf{Z}^{(0)} = f_\\theta(\\mathbf{X})$ | $\\mathbf{Z}^{(k+1)} = \\mathbf{S} \\mathbf{Z}^{(k)}$, $\\mathbf{S}$ could be any propagation matrix. | $\\mathbf{Z}^{(k+1)} = \\text{Identity}(\\mathbf{Z}^{(k+1)})$                      |\n| APPNP  | $\\mathbf{Z}^{(0)} = f_\\theta(\\mathbf{X})$ | $\\mathbf{Z}^{(k+1)} = \\mathbf{S} \\mathbf{Z}^{(k)}$, $\\mathbf{S}$ could be any propagation matrix. | $\\mathbf{Z}^{(k+1)} = (1 - \\alpha)\\mathbf{Z}^{(k+1)} + \\alpha \\mathbf{Z}^{(0)}$ |\n| SSL-GM | $\\mathbf{Z}^{(0)} = f_\\theta(\\mathbf{X})$ | $\\mathbf{Z}^{(k+1)} = \\mathbf{S} \\mathbf{Z}^{(k)}$, $\\mathbf{S}$ could be any propagation matrix. | $\\mathbf{Z}^{(k+1)} = \\sigma(\\mathbf{Z}^{(k+1)})$"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069479576,
                "cdate": 1700069479576,
                "tmdate": 1700069479576,
                "mdate": 1700069479576,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vxo2bZdDwZ",
                "forum": "AhCdJ93Wmi",
                "replyto": "GhwmkKrn3X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer N5QN - Part 2"
                    },
                    "comment": {
                        "value": "- **Distinct Update Process in SSL-GM.** While SGC uses an identity function and APPNP employs a random walk-based message passing for the update process, SSL-GM uniquely utilizes a non-linear function as its update function. This non-linear approach is more akin to the workings of a GCN (Graph Convolutional Network), differentiating our method from both SGC and APPNP.\n\n- **Contribution of Non-Linear Aggregation.** The use of non-linear adjacency matrix aggregation in SSL-GM is a deliberate choice to enhance the model capability in graph inference acceleration. This non-linear approach facilitates more complex and nuanced representation learning, contributing to the overall effectiveness and generalization of our model, especially in scenarios where linear approaches might be limited.\n    \nWe believe this detailed explanation, along with the included table, effectively clarifies the unique aspects and contributions of our non-parametric aggregator in SSL-GM. We appreciate your feedback, which has helped us improve the clarity of our methodology.\n        \n    \n> Q3. So, from the perspective of acceleration effectiveness and improvement in generalization, what is the contribution of non-linear adjacency matrix aggregation to accelerating graph inference?\n\nThank you for your insightful question. We recognize the importance of elucidating the role of non-linear aggregation in our model, particularly regarding its impact on generalization and inference acceleration.\n\n**Improvement in Generalization.** Our non-linear aggregation method, in contrast to linear approaches like SGC, incorporates a non-linear function between each layer. This addition is designed to capture more complex patterns in the dataset, potentially leading to the discovery of more generalizable patterns. Moreover, the non-linear function can act as a form of regularization, helping to prevent overfitting and thereby enhancing the model capability to generalize to new datasets.\n    \nTo substantiate our claims, we have conducted experiments comparing model performance with and without non-linearity. The results, presented in the included table, demonstrate the effect of non-linearity on various datasets such as Cora, Citeseer, Pubmed, and Amazon datasets. These results provide empirical evidence of the non-linear approach impact on model performance.\n\n\n|            | Cora       | Citeseer   | Pubmed     | Amazon-CS  | Amazon-Photo |\n|------------|------------|------------|------------|------------|--------------|\n| linear     | 84.60\u00b10.24 | 73.52\u00b10.53 | 86.99\u00b10.09 | 88.46\u00b10.16 | 94.28\u00b10.08   |\n| non-linear | 84.25\u00b10.40 | 73.29\u00b10.53 | 86.57\u00b10.13 | 88.25\u00b10.10 | 94.12\u00b10.13   |\n\n\n    \n**Acceleration Effectiveness Unaffected by Non-Linearity.** Regarding inference acceleration, it is crucial to note that we utilize the trained MLP for inference. Hence, the non-linearity introduced in the GNNs does not affect the inference acceleration. This is because the non-linear characteristics of the GNNs do not impact the MLPs during the inference phase.\n\n\n> Q4. I cannot understand Formula 4. The author injects randomness by perturbing the structure and features of the original graph, with the expectation that the MLP encoder can capture invariant key features. However, Formula 4 is perplexing. The first part of the formula computes the mutual information between $G_1$ and $G_2$, but is it merely a distinction of whether the perturbed structure is included? \n\nWe apologize for any confusion caused by Equation 4 and appreciate the opportunity to clarify its meaning and significance in our paper.\n\n**Recap.** The augmentation process is aligned with the objective of optimal contrastive learning, which aims to train an augmentation-invariant encoder. This is encapsulated in Equation 4, where $\\mathcal{E}^* = argmin_{\\mathcal{E}} I(\\mathcal{G}_1, \\mathcal{G}_2) - I(\\hat{\\mathbf{H}}, \\hat{\\mathbf{Z}})$. In this equation, $\\mathcal{G}_1 = (\\emptyset, \\hat{\\mathbf{X}})$ and $\\mathcal{G}_2 = (\\hat{\\mathbf{A}}, \\hat{\\mathbf{X}})$ represent two augmented views of the graph - $\\mathcal{G}_1$ as the augmented node-set without edges and $\\mathcal{G}_2$ as the augmented graph. $\\hat{\\mathbf{H}}$ and $\\hat{\\mathbf{Z}}$ are the representations encoded by MLPs and GNNs on these augmented graphs, respectively.\n\n**Detailed Explanation:**\n    \n**Random Augmentation.** The random augmentation is applied in each epoch to create $\\mathcal{G}_1$ and $\\mathcal{G}_2$. The first part of the equation computes the mutual information between these two augmented views.\n\n**Learning Objective.** The equation aims to identify an encoder that can capture all information in the augmented views, regardless of the specific augmentations applied. While achieving this ideal is challenging, the goal is for the encoder to learn the most essential information common to both views, a principle known as mutual information maximization."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069551153,
                "cdate": 1700069551153,
                "tmdate": 1700069880732,
                "mdate": 1700069880732,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uEzrv6KHAe",
            "forum": "AhCdJ93Wmi",
            "replyto": "AhCdJ93Wmi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3002/Reviewer_RVGt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3002/Reviewer_RVGt"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method called SSL-GM, which integrates structural information into MLPs by connecting GNNs and MLPs using SSL. This method can accelerate graph inference and improve the generalization capability, resulting in a favorable balance between accuracy and inference time. Experimental results show that the method performs well in node classification tasks and is effective in accelerating inference. In addition, many theoretical analyses and corresponding experiments flesh out the work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper propose a new method, which can accelerate GNN inference and performs well in node classification tasks over the state-of-art models.\n2. There are detailed theoretical analyses and corresponding experiments of SSL-GM ,which fleshes out the article and provides inspiration for the innovations that follow.\n3. This work has comprehensive and detailed experiments, which validates the performance and efficiency of SSL-GM."
                },
                "weaknesses": {
                    "value": "1. The article lacks sufficient innovation and is merely a combination and application of existing methods, such as Bootstrap loss, SGC, graph augmentation, and reconstruction.\n2. In section 4.3, there is only 'Figure 3' to demonstrate the capability of SSL-GM for inference acceleration. However, it is necessary to provide detailed experimental results regarding accuracy and inference time. These results should be obtained from a wider range of datasets and classification settings.\n3. In section 3.3, it would be better to introduce representation shift in detail and explain how reconstruction helps in mitigating representation shift."
                },
                "questions": {
                    "value": "1. Although this work is a combination of existing methods, it is fascinating to introduce this model from a higher level rather than loss function level. \n2. In B.3, learning rates is misspelled where \u20185e4\u2019 appears twice."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3002/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3002/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3002/Reviewer_RVGt"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3002/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765691475,
            "cdate": 1698765691475,
            "tmdate": 1699636244517,
            "mdate": 1699636244517,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4YLUpOiK3Y",
                "forum": "AhCdJ93Wmi",
                "replyto": "uEzrv6KHAe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RVGt - Part 1"
                    },
                    "comment": {
                        "value": "# Reviewer 1\n\nThank you for your detailed comments and constructive suggestions. To address your concerns, we provide the following responses.\n\n> Q1. The article lacks sufficient innovation and is merely a combination and application of existing methods, such as Bootstrap loss, SGC, graph augmentation, and reconstruction.\n\nThank you for your insightful comment. We appreciate the opportunity to clarify the innovative aspects of our work, particularly in addressing the limitations of existing GNN-MLP methods in inductive, cold-start, and label sparsity settings.\n\n- **Insight on Generalization Issues.** Our core contribution does not lie in new model architectures and new objectives. Instead, our work identifies the lack of generalization in current methods as a key issue, stemming from the limitations of knowledge distillation (KD) that aligns GNN and MLP outputs only at the label level. We propose that aligning these models in the representation space using self-supervised learning (SSL) can yield more generalizable representations. This insight represents a significant shift from existing practices.\n        \n- **Challenges in SSL Alignment.** Different from KD methods, aligning GNNs and MLPs under SSL is challenging due to the absence of a pre-trained GNN. To this end, we convert the objective to jointly train GNN and MLP by aligning the 0-hop representations (MLP output) with high-order representations (GNN output). Our approach is applicable with various objectives like Bootstrap, InfoNCE, or MSE. This flexibility in alignment strategy is a novel contribution to the field.\n\n- **Addressing Representation Inconsistency and Model Collapse.** We observed inconsistency in representations between GNNs and MLPs, potentially leading to model collapse (Appendix D.3). Our solution involves using MLP outputs to approximate GNN outputs, a strategy akin to SGC but applied to a different problem context. \n\n- **Graph Augmentation and Reconstruction Regularizer.** We further incorporate data augmentation to enhance the quality of the learned representations. However, given that structural permutation in augmentation can significantly shift GNN representations, we introduce a reconstruction regularizer to mitigate representation shift. This addition is critical in ensuring the stability and reliability of the learned representations.\n\n> Q2. In section 4.3, there is only 'Figure 3' to demonstrate the capability of SSL-GM for inference acceleration. However, it is necessary to provide detailed experimental results regarding accuracy and inference time. These results should be obtained from a wider range of datasets and classification settings.\n\nThank you for your valuable feedback. In response to your comment, we have presented additional experimental results in Appendix C of our paper to further validate the efficacy of our SSL-GM model in inference acceleration.\n\n- **Accuracy vs Efficiency.** We present a comprehensive comparison between our method and other acceleration methods. The results showcase the inference time and accuracy for various methods, including SGC, APPNP, Quantization (QSAGE), Pruning (PSAGE), and Neighbor Sample on Flickr and OGB-Arxiv. We have intentionally omitted GNN-MLP methods that were compared in Figure 3 for brevity. Please note that the time consumption of SAGE and BGRL are the same since the learning process of SSL does not affect inference. \n\n- **Different Settings.** To ensure a thorough evaluation, we chose three datasets across all node classification settings, including transductive, inductive, and cold-start. Our results, as shown in the additional tables, clearly indicate that SSL-GM outperforms existing methods in terms of efficiency and accuracy. \n\n[We list the experimental results in the next part. ]"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068351308,
                "cdate": 1700068351308,
                "tmdate": 1700068466387,
                "mdate": 1700068466387,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6owq9nq834",
                "forum": "AhCdJ93Wmi",
                "replyto": "uEzrv6KHAe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "| Datasets |  | SAGE      | BGRL  | SGC                  | APPNP                | QSAGE                | PSAGE                | Neighbor Sample      | **SSL-GM** |\n|------------|------------------|------------|-------------|----------------------|----------------------|----------------------|----------------------|----------------------|----------------------------|\n| Flickr     | Time (ms) | 80.7  | 80.7 (1.00$\\times$)  | 76.9 (1.05$\\times$)  | 78.1 (1.03$\\times$)  | 70.6 (1.14$\\times$)  | 67.4 (1.20$\\times$)  | 25.5 (3.16$\\times$)        | **0.9 (89.67$\\times$)** |\n|                              | Acc (\\%)  | 47.17 | 49.12                | 47.35                | 47.53                | 47.22                | 47.25                | 47.01                      | **49.27**                |\n| Arxiv       | Time (ms) | 314.7 | 314.7 (1.00$\\times$) | 265.9 (1.18$\\times$) | 284.1 (1.11$\\times$) | 289.5 (1.09$\\times$) | 297.5 (1.06$\\times$) | 78.3 (4.02$\\times$)        | **2.5 (125.88$\\times$)** |\n|                              | Acc (\\%)  | 68.52 | 69.29                | 68.93                | 69.10                | 68.48                | 68.55                | 68.35                      | **70.23**                |\n\n\n\n|           |        | Trans      | Trans          | Ind        | Ind            | Cold-start | Cold-start     |\n|-----------|--------|------------|----------------|------------|----------------|------------|----------------|\n| Dataset   | Models | Time (ms)  | Acc (\\%)       | Time (ms)  | Acc (\\%)       | Time (ms)  | Acc (\\%)       |\n| Pubmed    | SAGE   | 73         | 85.94          | 15         | 85.04          | 15         | 77.98          |\n|           | SGC    | 64         | 85.28          | 14         | 85.22          | 13         | 76.10          |\n|           | NOSMOG | 5          | 86.18          | 3          | 83.84          | 3          | 81.48          |\n|           | SSL-GM | **3** | **86.99** | **3** | **86.47** | **3** | **86.44** |\n| Amazon-CS | SAGE   | 103        | 88.88          | 31         | 87.24          | 25         | 61.01          |\n|           | SGC    | 89         | **89.31** | 26         | 87.12          | 24         | 63.08          |\n|           | NOSMOG | 5          | 87.64          | 4          | 86.61          | 4          | 81.95          |\n|           | SSL-GM | **4** | 88.46          | **3** | **87.65** | **3** | **87.58** |\n| Arxiv     | SAGE   | 485        | **72.05** | 315        | 68.52          | 305        | 43.47          |\n|           | SGC    | 410        | 69.95          | 266        | 68.93          | 250        | 42.08          |\n|           | NOSMOG | 6          | 70.84          | 4          | 69.10          | 4          | 61.64          |\n|           | SSL-GM | **4** | 71.12          | **3** | **70.23** | **3** | **66.13** |\n\n\n> Q3. In section 3.3, it would be better to introduce representation shift in detail and explain how reconstruction helps in mitigating representation shift.\n\nThank you for pointing out the need for a more detailed discussion on representation shift and its mitigation. We have revised Section 3.3 to address this.\n\n**Defining Representation Shift.** We define representation shift as the alteration in learned representations due to data augmentations. While feature augmentation does not significantly alter the distribution of learned representations, structural augmentation can have a profound impact. This shift, particularly pronounced in structural augmentation, can significantly alter the local structure of the target node. For instance, as depicted in Fig. 2, simple edge permutation can dramatically change the 2-hop neighborhoods of a target node, leading to a substantial shift in the representations.\n        \n**Mitigation Strategy - Reconstruction Regularizer.** To counter this problem, we hypothesize that if GNN representations can preserve more localized information, the impact of representation shift can be minimized. Based on this, we introduce a reconstruction regularizer. This regularizer aims to reconstruct the raw node features based on the GNN representations, thereby mitigating the representation shift. The underlying idea is to anchor the learned representations closer to the original feature space, reducing the impact of structural changes.\n        \n**Effectiveness of Our Approach.** The reconstruction regularizer thus serves as a stabilizing factor, ensuring that the GNN representations remain informative and reliable despite augmentations. This approach is innovative in how it leverages the reconstruction concept to maintain the integrity of learned representations in the face of potential shifts."
                    },
                    "title": {
                        "value": "Response to Reviewer RVGt - Part 2"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068537868,
                "cdate": 1700068537868,
                "tmdate": 1700068615461,
                "mdate": 1700068615461,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vPj2t1RHOU",
                "forum": "AhCdJ93Wmi",
                "replyto": "uEzrv6KHAe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3002/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RVGt - Part 3"
                    },
                    "comment": {
                        "value": "> Q4. Although this work is a combination of existing methods, it is fascinating to introduce this model from a higher level rather than loss function level.\n\nThank you for your insightful suggestion to present our SSL-GM model from a more holistic perspective. We appreciate this opportunity to highlight the integrative approach and overarching objectives of our model, beyond the specifics of the loss function.\n\n**Holistic Approach.** In developing SSL-GM, our focus extends beyond the choice of loss functions to a comprehensive consideration of the model architecture and its ability to generalize effectively. This broader view is essential for understanding the innovative aspects of our approach. A key component of our model is the use of contrastive learning between GNNs and MLPs. This strategy is designed to infuse MLPs with generalizable high-order representations, enhancing their capability to capture complex graph structures and features.\n\n**Strategic Components.** To avoid model collapse, a common challenge when directly minimizing the distance between GNNs and MLPs, we introduce non-parametric aggregation. This component ensures consistency and stability in the relationship between GNNs and MLPs. Further strengthening our model, we apply data augmentation techniques and leverage a reconstruction term. These additions are geared towards improving the model capability and preventing potential representation shifts. Importantly, the choices of augmentations and loss functions are flexible, allowing for adaptability to different graph scenarios.\n\n**Emphasizing Generalization and Adaptability.** The central theme of SSL-GM is to enhance the model generalization capabilities, enabling it to perform effectively across a variety of graph-related tasks and settings. The flexible nature of key components, including the choice of loss functions and augmentations, reflects our commitment to creating a versatile and adaptable model that can address diverse challenges in graph model acceleration.\n\n> Q5. B.3, learning rates is misspelled where \u20185e4\u2019 appears twice.\n\nThank you for bringing this typographical error to our attention. We sincerely apologize for the oversight and appreciate your meticulous reading of our manuscript. We have reviewed Section B.3 and corrected the error where the learning rate '5e4' is incorrectly mentioned twice.\n\n---\n\nIn light of these responses, we hope we have addressed your concerns, and hope you will consider increasing your score. If we have left any notable points of concern unaddressed, please do share and we will attend to these points."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068672079,
                "cdate": 1700068672079,
                "tmdate": 1700070000665,
                "mdate": 1700070000665,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]