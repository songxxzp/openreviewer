[
    {
        "title": "AdaRec: Adaptive Sequential Recommendation for Reinforcing Long-term User Engagement"
    },
    {
        "review": {
            "id": "DYTWTj5uFH",
            "forum": "waeGeAdZUx",
            "replyto": "waeGeAdZUx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission59/Reviewer_TqZE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission59/Reviewer_TqZE"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied the challenges of users\u2019 complicated behavior changes in online recommendation systems. In different periods, users\u2019 behaviors change in terms of preferences, return time and frequencies of immediate user feedback. To handle this challenge, the authors propose an adaptive sequential recommendation method to optimize long-term user engagement. Specifically, the authors utilize a context encoder to encode user\u2019s behavior patterns and regularize the encoder to produce a similar latent representation for states with similar state values. An optimistic exploration is further utilized to encourage exploration. Experiments are carried out using a recommender simulator and an online A/B test."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper is well-organized and easy to follow.\n2.\tExperiments are carried out on both a recommender simulator and an online A/B test, which is comprehensive.\n3.\tThe authors conduct an ablation study to validate the effectiveness of each component."
                },
                "weaknesses": {
                    "value": "1.\tOne major concern about the proposed method is the regularization loss in the proposed context encoder, which seems problematic to me. The motivation of this paper is to handle the distribution shift of the evolving user behavior patterns. However, encouraging states with similar state values to have similar latent encoding representation does not solve the distribution shift issues. The estimated state value function can still face the challenge of user behavior shift, resulting in an inaccurate state value estimation. \n\n2.\tAnother major concern is the novelty of the proposed method. To my knowledge, using context encoder to encoder user behavior patterns is not new in the recommendation context, which is also discussed in the related work section. The novelty of adding regularization loss in the context encoder is limited. The adopted exploration mechanism from RL literature is rather general and it is unclear how it particularly handles the user exploration in the recommendation context, which usually involves large action space. \n\n3.\tAs this paper aims to handle the user behavior evolution challenge in the sequential recommendation setting, some baselines in the Non-RL recommender literature are missing such as [1, 2].\n\n[1] Zhou, G., Zhu, X., Song, C., Fan, Y., Zhu, H., Ma, X., ... & Gai, K. (2018, July). Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining (pp. 1059-1068).\n\n[2] Zhou, G., Mou, N., Fan, Y., Pi, Q., Bian, W., Zhou, C., ... & Gai, K. (2019, July). Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI conference on artificial intelligence (Vol. 33, No. 01, pp. 5941-5948)."
                },
                "questions": {
                    "value": "See the Weaknesses for the questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission59/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698619899591,
            "cdate": 1698619899591,
            "tmdate": 1699635930217,
            "mdate": 1699635930217,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6N1LvAUaGR",
                "forum": "waeGeAdZUx",
                "replyto": "DYTWTj5uFH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission59/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission59/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer TqZE (Part I)"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments. We provide discussions on your concerns as follows.\n\nQ: **One major concern about the proposed method is the regularization loss in the proposed context encoder, which seems problematic to me. The estimated state value function can still face the challenge of user behavior shift, resulting in an inaccurate state value estimation.**\n\nA: As demonstrated in Figure 2, the encoder output $\\phi(s)$ is also used as the input to the state-value function $V$ for more accurate value estimation. In Equation (2) the input $\\phi(s)$ of the $V$-function is omitted for brevity. We add the clarification in Section 4.1 of the revised paper.  To be specific, the optimization of the context encoder and the value functions $V$ follows an iterative procedure. Since the distance-based encoder loss do not directly rely on the accrate output of $V$ (it only uses $V$ to rank the states and assign them into groups),  value functions on the previous timestep is enough to generate an accurate encoder loss.\nWe also exhibit the curve of the value function training loss in Appendix D.5. The numerical results are posted here for your reference. \n\n| Algorithm | TD3 | SAC | RLUR | AdaRec (Ours) |\n| :--- | :---: | :---: | :---: | :---: |\n| Critic loss on 10k steps | 6.88e-3 | 2.81e-3 | 2.52e-3 | **1.85e-3** |\n\nThanks to the encoder output $\\phi(s)$ as additional input of the value function, AdaRec has the lowest and most stable critic loss among selected value-based algorithms. This means that AdaRec has a relatively accurate value estimation during training. TD3 has a higher but stable critic loss mainly because of its insufficient exploration. Although SAC and RLUR have their respective exploration modules, they cannot adapt to environment changes and have an unstable critic loss curve.\n\nQ: **Another major concern is the novelty of the proposed method.  The novelty of adding regularization loss in the context encoder is limited. It is also unclear how the exploration mechanism handles the user exploration in the recommendation context, which usually involves large action space.**\n\nA: Context encoders themselves are only one of the network architechtures that enable more delicated design of state representations. It is the loss functions that differentiate encoder designs. There are many peer-reviewed RL papers that solely focus on proposing new losses for training the context encoders [1,2,3,4]. With regard to our paper, the major concern for designing encoder loss is that a variety of latent factors, including user behavior patterns, user preferences, and users' return time distributions, can collaboratively influence how the environment change. Previous methods that attempt to recover one single latent factor with context encoders will fail in this scenario. Our AdaRec algorithm manages to relate the encoder output with the estimated long-term return, which serves as a universal criterion to detect whether the environment changes: the policy will obtain low rewards with distribution shifts. The performance comparison between ESCP, which is the state-of-the-art context encoder-based algorithm, and AdaRec in both the simulator and the live experiments also demonstrates the effectiveness of our loss for training the encoder.\n\nWith respect to the exploration mechanism, we would like to emphasize that instead of setting the action space to be all candidate items, we use a 7-dimentional continuous action space for RL training. In this setting the action space is not large. The actions are combined with a deep scoring model and generate a ranking score for each candidate items. Please kindly check the updated Section 2.1 for details. In this problem formulation, the major concern is that recommendation tasks can be risk-sensitive: users may disengage from the application and cease the recommendation process if they encounter recommended items that fail to capture their interest. So relying solely on optimistic actions will be harmful to the recommendation quality. In AdaRec, the zero-order action selection procedure guarantees that potentially undesired actions will not lead to improper recommendation."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission59/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699938177501,
                "cdate": 1699938177501,
                "tmdate": 1699938177501,
                "mdate": 1699938177501,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Li0HKaA7bA",
                "forum": "waeGeAdZUx",
                "replyto": "DYTWTj5uFH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission59/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission59/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer TqZE (Part II)"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments. We provide discussions on your concerns as follows.\n\nQ: **Some baselines in the Non-RL recommender literature are missing.**\n\nA: We thank the reviewers for pointing it out. The mentioned papers are discussed in the related work section in Appendix B.1. We also implement DIN and compare its performance with AdaRec. We keep the original input of DIN unchanged and use transformers, which has shown better performance than vanilla fully-connected networks, as the backbone network. This help us make fair comparisons. The comparative results are in Appendix D.3 of the revised paper. We also list them here for your reference.\n\n|  | Average Return Days ( $\\downarrow$ ) | Return Probability at Day 1 $(\\uparrow)$ | Reward $(\\uparrow)$ |\n| :--- | :---: | :---: | :---: |\n| CEM | $1.841 \\pm 0.214$ | $0.720 \\pm 0.067$ | $-0.017 \\pm 0.001$ |\n| DIN | $1.725 \\pm 0.029$ | $0.755 \\pm 0.005$ | $-0.017 \\pm 0.001$ |\n| TD3 | $2.023 \\pm 0.012$ | $0.659 \\pm 0.002$ | $-0.020 \\pm 0.000$ |\n| SAC | $2.023 \\pm 0.012$ | $0.659 \\pm 0.002$ | $-0.020 \\pm 0.000$ |\n| OAC | $1.778 \\pm 0.122$ | $0.738 \\pm 0.040$ | $-0.017 \\pm 0.001$ |\n| RND | $1.704 \\pm 0.131$ | $0.765 \\pm 0.045$ | $-0.016 \\pm 0.001$ |\n| ESCP | $1.719 \\pm 0.098$ | $0.759 \\pm 0.032$ | $-0.017 \\pm 0.000$ |\n| RLUR | $1.910 \\pm 0.066$ | $0.693 \\pm 0.019$ | $-0.019 \\pm 0.000$ |\n| AdaRec (Ours) | $\\mathbf{1.541} \\pm 0.056$ | $\\mathbf{0.819} \\pm 0.017$ | $\\mathbf{- 0.015} \\pm 0.000$ |\n\nAs shown in the reults, DIN performs worse than AdaRec. This is because supervised-learning algorithms can only learn from the immediate response, which is the click-through rate in DIN's formulation. Instead, AdaRec is a RL algorithm that has the ability to capture the long-term effect of the recommended items and is more suitable for optimizing long-term user engagement. \n\n[1] Context-aware Dynamics Model for Generalization in Model-Based Reinforcement Learning. ICML 2020.\n\n[2] Trajectory-wise Multiple Choice Learning for Dynamics Generalization in Reinforcement Learning. NeurIPS 2020.\n\n[3] Learning Invariant Representations for Reinforcement Learning without Reconstruction. ICLR 2021.\n\n[4] Adapt to Environment Sudden Changes by Learning a Context Sensitive Policy. AAAI 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission59/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699938367185,
                "cdate": 1699938367185,
                "tmdate": 1699938367185,
                "mdate": 1699938367185,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XAga3m2Efr",
                "forum": "waeGeAdZUx",
                "replyto": "DYTWTj5uFH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission59/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission59/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Replying to Reviewer TqZE"
                    },
                    "comment": {
                        "value": "Dear Reviewer TqZE,\n\nWe sincerely value your dedicated guidance in helping us enhance our work. We are eager to ascertain whether our responses adequately address your primary concerns, particularly in relation to the effectiveness of the context encoder, the novelty of the proposed approach, and performance comparison with non-rl recommender algorithms. Thank you."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission59/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214866762,
                "cdate": 1700214866762,
                "tmdate": 1700214866762,
                "mdate": 1700214866762,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iACZkgYtYR",
                "forum": "waeGeAdZUx",
                "replyto": "DYTWTj5uFH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission59/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission59/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Replying to Reviewer TqZE"
                    },
                    "comment": {
                        "value": "Dear Reviewer TqZE,\n\nAs the author-reviewer discussion period is approaching the end, we again extend our gratitude for your reviews that help us improve our paper. Any further comments on our rebuttal or the revised paper are appreciated. Thanks for your time!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission59/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458569536,
                "cdate": 1700458569536,
                "tmdate": 1700458569536,
                "mdate": 1700458569536,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IcRruQXVAo",
            "forum": "waeGeAdZUx",
            "replyto": "waeGeAdZUx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission59/Reviewer_dRAr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission59/Reviewer_dRAr"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel paradigm called AdaRec to address the challenge of evolving user behavior patterns in large-scale online recommendation systems. The goal is to optimize long-term user engagement by leveraging Reinforcement Learning (RL) algorithms. By introducing a distance-based representation loss to extract latent information from users' interaction trajectories, AdaRec helps the RL policy identify subtle changes in the recommendation system. To enable rapid adaptation, AdaRec encourages exploration using the idea of optimism under uncertainty. It also incorporates zero-order action optimization to ensure stable recommendation quality in complex environments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The problems studied in this paper exist widely in recommendation systems, and have been ignored by previous researchers, which is a very promising and important research direction.\n2. The paper presents a distance-based representation loss to identify the subtle user behavior patterns changes, which is novel and interesting.\n3. Extensive empirical analyses in simulator-based and live sequential recommendation tasks demonstrates that AdaRec outperforms baseline algorithms in terms of long-term performance."
                },
                "weaknesses": {
                    "value": "1. The writing of the paper needs further improvement. \n(1)\tWhat is the specific meaning of State Space S?\n(2)\tThe paper should give a brief introduction before using some reinforcement learning concepts.\n2. Although I agree with that the user behavior patterns are evolving in recommendation systems, the study in Section 3 does not make sense to me. At different time stamp, user interaction history statistics are different, that is, the distribution of states are different, so different interaction frequency may not reflect the change of user behavior patterns. There is no evidence to support the argument \u201cAs previously discussed, given the same distribution of states s_t and actions a_t, the users\u2019 return time exhibits fluctuations across different weeks.\u201d \n3. In reinforcement learning, there are many exploration strategies. What are the advantages of the schemes mentioned in the paper, and the comparison experiments with other schemes need to be provided."
                },
                "questions": {
                    "value": "1. What is the specific meaning of State Space S?\n2. The comparison experiments with other exploration schemes need to be provided."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission59/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission59/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission59/Reviewer_dRAr"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission59/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827266911,
            "cdate": 1698827266911,
            "tmdate": 1700408915648,
            "mdate": 1700408915648,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OPoHrpAA35",
                "forum": "waeGeAdZUx",
                "replyto": "IcRruQXVAo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission59/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission59/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer dRAr"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments. We provide discussions on your concerns as follows.\n\nQ: **What is the specific meaning of State Space S? The paper should give a brief introduction before using some reinforcement learning concepts.**\n\nA: The State Space $\\mathcal{S}$ refers to a set of all possible states that may occur in the trajectory of an MDP. In Section 2.1, we add RL concepts such as the accumulated return $\\eta_T$, the state-action value function $Q_T^\\pi(s, a)$, and the state value function $V_T^\\pi(s)$. Please check the revised paper for details. We hope the additional RL concepts can make readers easier to understand our proposed algorithm.\n\nQ: **The study in Section 3 does not make sense to me. The distribution of states are different, so different interaction frequency may not reflect the change of user behavior patterns.**\n\nA: We thank the reviewer for pointing it out. To remove the influence of various interaction histories, we select interactions that happen at the start of each recommendation session, i.e., the $s_0$ in the trajectory. We exhibit the updated analysis results in Figure 1 (lower right) of the revised paper. According to the analysis, the user interaction frequency still exhibits large variations in different time periods. The mode of changes in user behavior patterns is similar to those in the original analysis. The return distribution in Figure 1 (upper right) is computed trajectory-wise, and depends on $(s_0,a_0,a_1,\\cdots,a_n,\\cdots)$ without interaction history. Its evolvement can also reflect the changes in user behavior patterns. We believe the updated data-driven study can better demonstrate the existence of distribution shift in recommender systems.\n\nQ: **In reinforcement learning, there are many exploration strategies. What are the advantages of the schemes mentioned in the paper? The comparison experiments with other exploration schemes also need to be provided.**\n\nA: Among exploration strategies in RL, there are two major mechanisms that are relatively simple and suitable for deployment in real-world applications. One is the optimism under uncertainty [1,2]. It encourages the learning agent to choose optimistically when it is uncertain about the outcome of certain actions. However, as clarified in Section 4.2, recommendation tasks can be risk-sensitive: users may disengage from the application and cease the recommendation process if they encounter recommended items that fail to capture their interest. So relying solely on optimistic actions will be harmful to the recommendation quality. In AdaRec, the exploration module is also based on the idea of optimism under uncertainty. But the zero-order action selection procedure guarantees that potentially undesired actions will not lead to improper recommendation.\n\nAnother mechanism is curiosity-driven exploration [3]. It usually involves a random encoding network $N$ that maps the state $s$ into low-dimentional vectors $z$. Then another separate encoding network $N^{\\prime}$ is trained to reconstruct the mapping from $s$ to $z$. If the visitation count to the state $s$ is small, the reconstruction loss of $N^{\\prime}$ on state $s$ will be relatively large and $s$ will be more likely to be explored. But in our setting of adaptive recommendation, the outcome of the same state $s$ can be different with different user behavior patterns. So even when $s$ has been visited for many times, it is possible that with new behavior patterns the state $s$ still requires considerate exploration. Instead, the exploration mechanism in AdaRec relies on the optimistic Q-values that are more accurate than intuitive visitation counts.\n\nFor comparative analysis, we consider OAC [1] and RND [3] as additional baselines. These two algorithms are representative approaches of the two exploration mechanisms respectively. We add the results in the revised paper, and also post them here for your reference:\n\n|  | Average Return Days $(\\downarrow)$ | Return Probability at Day 1 $(\\uparrow)$ | Reward $(\\uparrow)$ |\n| :--- | :---: | :---: | :---: |\n| CEM | $1.841 \\pm 0.214$ | $0.720 \\pm 0.067$ | $-0.017 \\pm 0.001$ |\n| TD3 | $2.023 \\pm 0.012$ | $0.659 \\pm 0.002$ | $-0.020 \\pm 0.000$ |\n| SAC | $2.023 \\pm 0.012$ | $0.659 \\pm 0.002$ | $-0.020 \\pm 0.000$ |\n| OAC | $1.778 \\pm 0.122$ | $0.738 \\pm 0.040$ | $-0.017 \\pm 0.001$ |\n| RND | $1.704 \\pm 0.131$ | $0.765 \\pm 0.045$ | $-0.016 \\pm 0.001$ |\n| ESCP | $1.719 \\pm 0.098$ | $0.759 \\pm 0.032$ | $-0.017 \\pm 0.000$ |\n| RLUR | $1.910 \\pm 0.066$ | $0.693 \\pm 0.019$ | $-0.019 \\pm 0.000$ |\n| AdaRec (Ours) | $\\mathbf{1.541} \\pm 0.056$ | $\\mathbf{0.819} \\pm 0.017$ | $\\mathbf{-0.015} \\pm 0.000$ |\n\nOur AdaRec algorithm can outperform OAC and RND in all metrics, demonstrating its superior ability to make safe explorations.\n\n[1] Better Exploration with Optimistic Actor-Critic.\n\n[2] Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic.\n\n[3] Exploration by Random Network Distillation."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission59/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699871306009,
                "cdate": 1699871306009,
                "tmdate": 1699871458749,
                "mdate": 1699871458749,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4SALfNBDm3",
                "forum": "waeGeAdZUx",
                "replyto": "IcRruQXVAo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission59/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission59/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Replying to Reviewer dRAr"
                    },
                    "comment": {
                        "value": "Dear Reviewer dRAr,\n\nWe sincerely value your dedicated guidance in helping us enhance our work. We are eager to ascertain whether our responses adequately address your primary concerns, particularly in relation to the new empirical analysis in Section 3 and the additional performance comparison with other exploration schemes in the experiments."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission59/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214741680,
                "cdate": 1700214741680,
                "tmdate": 1700214759662,
                "mdate": 1700214759662,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DRmujzJr5V",
                "forum": "waeGeAdZUx",
                "replyto": "IcRruQXVAo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission59/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission59/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Replying to Reviewer dRAr"
                    },
                    "comment": {
                        "value": "Dear Reviewer dRAr,\n\nWe noticed that you have raised the score to 6. Thank you for the acknowledgement!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission59/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458308677,
                "cdate": 1700458308677,
                "tmdate": 1700458308677,
                "mdate": 1700458308677,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AV5yAWZjRE",
            "forum": "waeGeAdZUx",
            "replyto": "waeGeAdZUx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission59/Reviewer_kKUq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission59/Reviewer_kKUq"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel paradigm to tackle the challenge of distribution shift in large-scale online recommendation systems. In these systems, the dynamics and reward functions are continuously affected by changes in user behavior patterns, making it difficult for existing reinforcement learning (RL) algorithms to adapt effectively.\n\nAdaRec proposes a multi-faceted approach to address this issue. It introduces a distance-based representation loss, which extracts latent information from users' interaction trajectories. This information reflects how well the RL policy aligns with current user behavior patterns, allowing the policy to detect subtle changes in the recommendation system.\n\nAdaRec's approach to addressing distribution shift in recommendation systems appears promising and aligns with current challenges in the field. The full paper's empirical results and detailed methodology will be necessary to assess the significance and practical applicability of this novel paradigm in real-world recommendation systems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. AdaRec introduces a novel paradigm for addressing distribution shift in large-scale online recommendation systems, which is a significant and challenging problem in the field.\n\n2. The use of zero-order action optimization to ensure stable recommendation quality in complicated environments is a strong point, as it addresses the need for robustness in real-world recommendation systems.\n\n3. The claim of superior long-term performance is supported by extensive empirical analyses in both simulator-based and live sequential recommendation tasks, indicating a commitment to evaluating the proposed solution rigorously."
                },
                "weaknesses": {
                    "value": "1. The use of \"optimism under uncertainty\" and zero-order action optimization may introduce additional complexity to the approach, which could be a drawback in terms of implementation and computational cost."
                },
                "questions": {
                    "value": "Does the computational cost of reinforcement learning need to be analyzed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission59/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699116951602,
            "cdate": 1699116951602,
            "tmdate": 1699635930014,
            "mdate": 1699635930014,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YrN9outK2r",
                "forum": "waeGeAdZUx",
                "replyto": "AV5yAWZjRE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission59/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission59/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer kKUq"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments. We provide discussions on your concerns as follows.\n\nQ: **The use of \"optimism under uncertainty\" and zero-order action optimization may introduce additional complexity to the approach, which could be a drawback in terms of implementation and computational cost.**\n\nA: In terms of implementation, the major concern may be the gradient $\\left[\\nabla_a Q_{\\mathrm{UB}}(s, a)\\right]_{a=a_T}$. A similar form of gradient is computed when updating the Q-networks, so the gradient computation will not lead to heavy implementation cost. \n\nIn terms of computational cost, AdaRec requires addtional steps in action selection, computing the gradient of the Q-function and sampling among candidate actions. But apart from action selection, RL training involves interacting with the environment and updating the policy with gradient decent. These two parts will take up more time than action selection. We conduct empirical studies and exhibit the average time cost of action selection in one training step. The total time cost of one training step is also shown for comparison. According to the results, although the exploration module lead to an addtional 129\\% of computation cost, it only costs less than 10\\% more total time. Also, during deployment the exploration module is not included, so it adds no more computation cost.\n\n|  | Action Selection (s) | Total Time (s) |\n| :--- | :---: | :---: |\n| AdaRec | 0.259 | 1.738 |\n| AdaRec (no exploration) | 0.113 | 1.595 |\n| Exploration Time Cost | 129 \\% | 8.966 \\% |\n\nThe discussions are added to Appendix D of the revised paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission59/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699870146199,
                "cdate": 1699870146199,
                "tmdate": 1699870146199,
                "mdate": 1699870146199,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tmOMPIEAOw",
                "forum": "waeGeAdZUx",
                "replyto": "YrN9outK2r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission59/Reviewer_kKUq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission59/Reviewer_kKUq"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer kKUq"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. I have read the rebuttal and most of my concerns have been well addressed. Overall, I am towards acceptance and will maintain my score.\n\nBest, The reviewer kKUq"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission59/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736580658,
                "cdate": 1700736580658,
                "tmdate": 1700736580658,
                "mdate": 1700736580658,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]