[
    {
        "title": "Do Current Large Language Models Master Adequate Clinical Knowledge?"
    },
    {
        "review": {
            "id": "9agLWa56ez",
            "forum": "gYcft1HIaU",
            "replyto": "gYcft1HIaU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9466/Reviewer_h42w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9466/Reviewer_h42w"
            ],
            "content": {
                "summary": {
                    "value": "The paper evaluates the current performance of medical LLMs by creating a benchmark and testing existing said LLMs against it. This work creates MedDisK, a database designed to test the medical knowledge of LLMs on different \"clinical knowledge aspects\". These properties are not limited to those used just for diagnosis; example properties include patient population, treatment principles, departments (relevant medical departments), etc. This work also introduces MedDisKEval, a method that includes automated and clinical-expert-dependent steps to grade the performance of LLMs. Notably, the paper concludes that most current medical LLMs do not perform better than the base LLMs they are built upon."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The development of a medical knowledge benchmark involved consulting 20 clinical experts over 10 months is good. This paper focuses largely on Chinese data/expert consult, but the presentation itself features relevant English translation.\n* Creating a clear evaluation method combining automated/expert consultation is also useful. \n* The conclusions of the evaluation point out specific flaws in existing medical LLMs; certain models evaluate different features poorly, for example. This provides a concrete criticism/evaluation of those methods that can be built upon."
                },
                "weaknesses": {
                    "value": "* The creation of a medical LLM benchmark itself does not make fundamental improvements over existing benchmarks developed in medical LLM research. As an example, the Singhal et al. 2023a paper also tested modern LLMs with human evaluation (MultiMedQA). Creating another benchmark by itself is not a conceptually novel improvement, and this work did not sufficiently argue for its improvement above these existing models/evaluations.\n* This work does not go into as much detail about the representation of medical knowledge in LLMs, providing only a benchmark without technical insight of what the LLMs might be doing or how they encode medical information.\n* Using MedDisKEval seems expensive or possibly unreliable. Someone seeking to use this evaluation method may need to consult expert opinion themselves, just to calibrate the alignment scores. The motivation behind the linear combination of BLEU-1, ROUGE-1 and cosine sim is empirically driven and is not inherently convincing as a metric."
                },
                "questions": {
                    "value": "* The database MedDisK was constructed with \"clinical experts and machine assistance.\" Further clarification is required; what was the exact process of constructing the database, and how was machine assistance used? \n* This work focused on a set of LLMs that is somewhat disjoint from existing popular medical LLMs. For example, MedPaLM?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777580102,
            "cdate": 1698777580102,
            "tmdate": 1699637191136,
            "mdate": 1699637191136,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sMNZEjcx34",
                "forum": "gYcft1HIaU",
                "replyto": "9agLWa56ez",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9466/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9466/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h42w --- Part 1"
                    },
                    "comment": {
                        "value": "**1. Novelty of MedDisKEval**: Thank you for your thoughtful concern on the novelty of our proposed LLM benchmark. The MultiMedQA dataset proposed by Singhal et al. is actually a set of medical QA datasets, some of them (MMLU, MedQA, MedMCQA, etc.) have already been proposed in other literature. These QA-based datasets are able to evaluate LLMs\u2019 medical capabilities to some extent, while they face limitations in evaluating clinical knowledge mastery. Firstly, existing QA-based evaluation sets have a limited coverage of diseases and disease-related knowledge, which are crucial for clinical practice. To clarify our claim, we have compared them with our proposed benchmark MedDisK in terms of the coverage of diseases, and 7 types of disease-knowledge-related entities: patient population (Popu), symptom (Symp.), body parts (Part.), body systems (Syst.) therapeutic procedure (Proc.), medication (Medi.), and departments (Dept.).We have found that these datasets cover limited number of diseases as well as disease-related knowledge: \n\n| Datasets           |      Type      | \\# diseases |    Publicly available?    |\n| ------------------ | :------------: | :---------: | :-----------------------: |\n| MedQA              |   QA dataset   |    1391     |            Yes            |\n| MedMCQA            |   QA dataset   |    3475     |            Yes            |\n| MMLU (medical)     |   QA dataset   |     383     |            Yes            |\n| MedicationQA       |   QA dataset   |     172     |            Yes            |\n| LiveQA             |   QA dataset   |     480     |            Yes            |\n| HealthSearchQA     |   QA dataset   |     262     |            Yes            |\n| **Total of Above** |  QA datasets   |    3907     |            Yes            |\n| **MedDisK (Ours)** | Knowledge base |    10632    | Yes (evaluation platform) |\n\n| Dataset         | \\#Popu. | \\#Symp. | \\#Part. | \\#Syst. | \\#Proc. | \\#Medi. | \\#Dept. |\n| --------------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- |\n| MedQA           | 197     | 377     | 574     | 15      | 429     | 62      | 36      |\n| MedMCQA         | 241     | 452     | 1245    | 33      | 811     | 56      | 54      |\n| MMLU  (medical) | 92      | 114     | 250     | 10      | 111     | 6       | 9       |\n| MedicationQA    | 27      | 64      | 52      | 7       | 70      | 67      | 3       |\n| LiveQA          | 75      | 108     | 141     | 9       | 166     | 14      | 19      |\n| HealthSearchQA  | 10      | 63      | 40      | 3       | 4       | 2       | 2       |\n| **Total  of Above** | 349     | 570     | 1362    | 34      | 997     | 183     | 83      |\n| **MedDisK  (Ours)** | 701     | 18737   | 1585    | 89      | 5097    | 3826    | 89      |\n\nThe comparison results indicate that the proposed benchmark covers a much broader range of disease knowledge compared to the previous QA datasets. We have also updated this comparison results in the appendix A2 of the revised paper. Moreover, several recent studies ([1], Table 8 in [2]) highlight that **data contamination** in existing LLMs adversely affects the fairness of contemporary open-source QA-based evaluation datasets. In contrast, our knowledge base is crafted by clinical experts and cannot be directly accessed on the Internet; thus, the proposed evaluation benchmark does not suffer from data contamination. An evaluation platform will be released to ensure the availability our proposed benchmark.\n\n[1] Zhou K, Zhu Y, Chen Z, et al. Don't Make Your LLM an Evaluation Benchmark Cheater[J]. arXiv preprint arXiv:2311.01964, 2023.\n\n[2] Wei T, Zhao L, Zhang L, et al. Skywork: A More Open Bilingual Foundation Model[J]. arXiv preprint arXiv:2310.19341, 2023.\n\n**2. Research value**: We are sincerely grateful for your insightful comments. For a considerable period, we have been researching how general LLMs can truly serve as foundational models in the medical domain and be applicable in real clinical scenarios. We have observed that current general/medical LLMs cannot be directly applied in real-world clinical applications, despite achieving considerable performance on specific medical tasks. The motivation of this work is to find out how far current LLMs are to the real medical foundation models in the aspect of clinical knowledge mastery. We humbly believe that the proposed evaluation benchmark could offer valuable insights to developers of medical LLMs, ultimately promoting the advancement of foundational models in the medical domain. We will further delve into how LLMs can better encode medical knowledge to be applicable in real-world clinical scenarios."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216500599,
                "cdate": 1700216500599,
                "tmdate": 1700217172764,
                "mdate": 1700217172764,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "K0w3kEaTHx",
            "forum": "gYcft1HIaU",
            "replyto": "gYcft1HIaU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9466/Reviewer_tVw5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9466/Reviewer_tVw5"
            ],
            "content": {
                "summary": {
                    "value": "To evaluate whether LLMs have mastered sufficient clinical knowledge, the authors first propose a large-scale medical disease-based knowledge base named MedDisK. They then develop MedDisKEval, a method that prompts LLMs to retrieve information on clinical knowledge aspects and measures the similarity between LLM-generated information and MedDisK. Results show that most of the current LLMs do not have sufficient clinical knowledge."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation is clear, and it is interesting to know whether current LLMs have mastered sufficient domain knowledge to help in the medical domain.\n2. The authors conduct extensive experiments with 12 LLMs, which include general LLMs and medical LLMs.\n3. The authors provide sufficient examples of prompt instructions, knowledge aspects, and LLM responses, which make it easier for readers to grasp the basic idea of the paper."
                },
                "weaknesses": {
                    "value": "1. One significant issue with this paper is that the authors may overstate the implications of their evaluation results. The experiments are exclusively conducted in Chinese. However, this critical detail is not adequately emphasized in the main paper, particularly in their conclusion that \"none of the evaluated LLMs have mastered sufficient knowledge to handle real clinical problems effectively.\" Based on their evaluation, the valid conclusion should be that LLMs do not possess adequate clinical knowledge **in the Chinese language**, and this finding cannot be generalized to other languages.\n\n2. In the second paragraph of the introduction, the authors claim that current QA-based medical evaluation datasets cannot evaluate whether LLMs have mastered sufficient medical knowledge because those datasets cover only some common diseases. It would be more robust if the authors could further justify this statement with some analysis (e.g. to quantitatively show the coverage of diseases in the existing benchmarks).\n\n3. For the proposed knowledge base MedDisK, it would be better for authors to include more details of the construction process. For example, how is the agreement among the clinical experts, is there any strategy used to tackle disagreement, and will this process introduce any additional human bias?\n\n4. In section 3.2.1, the authors \"employ a specialized NER model to identify and extract medical entities from the text\". However, the exact name and citation of the used NER model are missing, and it will be more convincing to include an analysis of the accuracy of the NER model as incorrectly recognized entities could impact the evaluation results of LLMs."
                },
                "questions": {
                    "value": "1. The authors claim that current QA-based medical evaluation datasets cover only some common diseases. However, in section 3.1 where the authors introduce their proposed knowledge base, it is said that \"We first select a subset from the ICD10 database according to whether the diseases are common in clinical (determined by clinical experts) and are statistically frequent in EHR (Electronic Health Record), resulting in 10,632 common diseases.\" I wonder why they also consider common diseases in their knowledge base?\n\n2. In the section of \"Disease-Knowledge-based Automated Scoring\", are there any better metrics to evaluate the similarity? The token-level BLEU-1 and ROUGE-1 cannot consider semantic meaning, and the M3E model is described as a sentence-level metric, whereas the evaluation in this context focuses on the meaning of individual tokens.\n\n3. In Table 3, one completely wrong example of LLM response is \"ok, I see\". Since the authors mention that they employ a specialized NER model to identify and extract medical entities from the text, I wonder why the NER model could extract such words from the responses.\n\n4. In section 4.2.2, the authors assign scores of 0, 5, and 10 to \u201dCompletely Wrong,\u201d \u201dPartially Correct,\u201d and \u201dBasically Correct,\u201d respectively, to calculate a total score. It's important to clarify how they arrived at the values \"0, 5, 10\" for this scoring system."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783566597,
            "cdate": 1698783566597,
            "tmdate": 1699637191012,
            "mdate": 1699637191012,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x9BIj5Uyxx",
                "forum": "gYcft1HIaU",
                "replyto": "K0w3kEaTHx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9466/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9466/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tVw5 --- Part 1"
                    },
                    "comment": {
                        "value": "**1. Language issue**: Thank you for your thoughtful comments. It is worth noting that foundational medical knowledge is relatively stable and universally applicable, unaffected by specific languages, because it includes fundamental concepts in physiology, anatomy, pharmacology, and other fields. These concepts have similar meanings across different linguistic contexts. Moreover, according to current works on LLM evaluation, foundation models such as GPT-3.5-turbo, GPT-4 also achieves considerable performance on **Chinese** general [1] and medical [2] datasets. This is because current LLMs have already possessed strong multilingual capabilities, and language is no longer a key factor influencing their performance. \n\nWe have also conducted a preliminary experiment to study the influence of language in our evaluation. Specifically, we randomly extract 500 diseases in our knowledge base, and translate all the related content into English with GPT-4. We have asked clinical experts to validate the correctness of the translation. Subsequently, we evaluate GPT-3.5-turbo on this small-scale English dataset by replacing the NER model with an English medical NER model MedCAT [3] and M3E model with MPNet model [4]. We find that the total score of GPT-3.5-turbo on these 500 diseases is **4.56** in English and **4.07** in Chinese, indicating that language has limited influence to our proposed evaluation method.\n\n[1] Huang Y, Bai Y, Zhu Z, et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Advances in neural information processing systems, 2023.\n\n[2] Zhu W, Wang X, Zheng H, et al. PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain[J]. arXiv preprint arXiv:2310.14151, 2023.\n\n[3] Kraljevic Z, Searle T, Shek A, et al. Multi-domain clinical natural language processing with medcat: the medical concept annotation toolkit[J]. Artificial intelligence in medicine, 2021.\n\n[4] Song K, Tan X, Qin T, et al. Mpnet: Masked and permuted pre-training for language understanding[J]. Advances in Neural Information Processing Systems, 2020.\n\n**2. Coverage of existing benchmarks**: We are grateful for your constructive suggestions. We compare MedDisK with six well-known medical QA evaluation datasets: MedQA, MedMCQA, MMLU (medical part), MedicationQA, LiveQA, and HealthSearchQA. We estimate the coverage of diseases in theses benchmarks by leveraging an English medical NER model MedCAT. The results are presented below, and we have added this comparison in the appendix A2 of our revised paper as well:\n\n| Datasets           |      Type      | \\# diseases |    Publicly available?    |\n| ------------------ | :------------: | :---------: | :-----------------------: |\n| MedQA              |   QA dataset   |    1391     |            Yes            |\n| MedMCQA            |   QA dataset   |    3475     |            Yes            |\n| MMLU (medical)     |   QA dataset   |     383     |            Yes            |\n| MedicationQA       |   QA dataset   |     172     |            Yes            |\n| LiveQA             |   QA dataset   |     480     |            Yes            |\n| HealthSearchQA     |   QA dataset   |     262     |            Yes            |\n| **Total of Above** |  QA datasets   |    3907     |            Yes            |\n| **MedDisK (Ours)** | Knowledge base |    10632    | Yes (evaluation platform) |\n\nThe table above shows that the proposed MedDisK covers much more diseases (>10k) than existing medical QA datasets (~3k). The results suggest that MedDisKEval significantly surpasses existing benchmarks in terms of the disease knowledge coverage."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700215828322,
                "cdate": 1700215828322,
                "tmdate": 1700215828322,
                "mdate": 1700215828322,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EwWKiU8GWB",
            "forum": "gYcft1HIaU",
            "replyto": "gYcft1HIaU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9466/Reviewer_rk49"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9466/Reviewer_rk49"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a large-scale medical disease-based knowledge base MedDisK, covering 10,632 common diseases and 18 clinical knowledge to evaluate LLMs. The purpose of the dataset is to  (a) include common diseases (b) involve disease base knowledge and (c) ensure that the sourcing of the dataset is such that it remains publicly inaccessible to prevent leaks during testing.\t\n\nFirst filter common diseases (determined by clinical experts based on ICD10 databases and frequency in EHR)  resulting in 10,632 common diseases. Then  employ clinical experts to define 18 disease-based clinical knowledge aspects that are crucial to medical decision-making (diagnoses, examinations, treatments) for each of the diseases. They use this database to probe LLMs and evaluate the mastery of clinical knowledge. They show that their scoring measures are in high agreement with clinical experts' subjective evaluation.  \n\nUsing the evaluation metrics they show that existing LLMs have not mastered adequate knowledge for clinical practice (showing that over 50%  of the generated information is not consistent with their KB) and are not ready to be foundation models for clinical domain."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper does a good job in communicating the ideas. I agree with the author's motivation that for the LLMs to be accepted as foundation models they need to have mastered adequate clinical knowledge. This is an important question and needs comprehensive evaluation."
                },
                "weaknesses": {
                    "value": "The paper could provide a more thorough justification for the introduction of the new medical dataset, especially in the context of existing evaluation datasets. The paper mentions that the existing evaluation datasets cover only some common diseases and lack extensive coverage of knowledge across various aspects of diseases. This reviewer feels that this needs to be substantiated with more thorough comparison. \n\nWhile it is surprising that most of the LLMs perform poorly (with over 50%) predicted to be completely wrong. The evaluation procedure used to arrive at this conclusion requires further elaboration.\n\nOverall I am not fully convinced that this dataset MedDisK  and the outlined evaluation procedure is robust for determining LLMs clinical knowledge yet. \n\nThis reviewer has listed all the concerning questions in detail below."
                },
                "questions": {
                    "value": "What is the source of the EHR resource used in the preliminary making of the dataset?\n\nThe authors state \u201cThe existing medical evaluation benchmarks are predominantly based on question-answering (QA) tasks. These benchmarks collect questions from diverse sources, including medical examinations, electronic health records, online resources, and expert crafting\u2026\u2026cover only some common diseases and lack extensive coverage of knowledge across various aspects of diseases. \u201d  Can you compare each of these resources the paper is referring to in this sentence with MedDisK in terms of coverage of common diseases, disease base knowledge and public availability?  How does this compare with other existing medical relational databases - MIMIC, i2b2, iBKH KG etc?\n\nI understand that the paper does interval sampling (10 examples from each interval) and engages clinical experts to provide a categorical standard - wrong, correct or partially correct.  And this resulted in the following standard (0-0.3 is wrong) and (0.3 to 0.8 is partially correct) and (0.8 to 1.0 is correct). How representative are these categories? Did the experts find that all the samples in 0.8 to 1.0 are correct and correspondingly all in 0-0.3 are wrong? Can you provide more representative examples or more thorough classification of the \u201cCompletely Wrong\u201d category?\t\n\nSince LLMs response is post-processed using the NER model I think the NER model's performance is extremely crucial to evaluation. How well does it perform in identifying medical entities? From the analysis conducted in Table 8, it appears that all the LLMs are underperforming in identifying symptoms, affected sites, etc., while they generally perform well in recognizing population ages involving numeric entities. \n\nWould it be considered a correct hit if the model predicts 'GI tract' instead of 'digestive system' in the examples from Table 3? What kind of standardization was performed in evaluating LLMs response with the experts output?\n\nWhat according to the authors are the limitations of the dataset and the evaluation procedure outlined here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698974395956,
            "cdate": 1698974395956,
            "tmdate": 1699637190864,
            "mdate": 1699637190864,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "c155UwXIeo",
                "forum": "gYcft1HIaU",
                "replyto": "EwWKiU8GWB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9466/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9466/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rk49 --- Part 1"
                    },
                    "comment": {
                        "value": "**1. About MedDisK construction**: Thank you for your thoughtful concerns on the construction of our proposed database. As we introduced in our paper, the ICD-10 diseases coding system includes around 27,000 diseases. First, we conducted a statistical analysis of the occurrence frequencies of these ICD-10 diseases based on ~**4 million** highly de-identified EHRs from > 100 hospitals across 5 cities. We then filtered out high-frequency diseases with an occurrence rate exceeding 1/10000 to build the disease knowledge base. This process results in 1,048 diseases. To expand MedDisK's coverage of diseases, we asked clinical experts to choose an additional 9,584 diseases from the remaining low-frequency cases based on their clinical significance. We have included additional construction details of MedDisK in appendix A1 of the revised paper.\n\n**2. MedDisKEval versus Other datasets**: We are sincerely grateful to your constructive comments on the comparison between MedDisKEval and other medical QA datasets. We choose a total of six medical QA datasets for comparison: MedQA, MedMCQA, MMLU (medical part), MedicationQA, LiveQA, and HealthSearchQA. We use an English medical NER tool called MedCAT [1] to extract diseases from the questions, options, and answers. In assessing disease-based knowledge coverage, we find it challenging to precisely quantify the number of knowledge aspects covered by the QA dataset, given its potential to encompass distinct types of knowledge for various diseases. For instance, these QA datasets may only examine symptoms of one disease and anatomical sites of another. Instead, we extract and analyze the occurrence of 7 common disease-knowledge-related entities within these datasets, including patient population (Popu), symptom (Symp.), body parts (Part.), body systems (Syst.) therapeutic procedure (Proc.), medication (Medi.), and departments (Dept.). The experimental results are presented below, respectively. We have also updated the results and analysis in appendix A2 of the revised paper.\n| Datasets           |      Type      | \\# diseases |    Publicly available?    |\n| ------------------ | :------------: | :---------: | :-----------------------: |\n| MedQA              |   QA dataset   |    1391     |            Yes            |\n| MedMCQA            |   QA dataset   |    3475     |            Yes            |\n| MMLU (medical)     |   QA dataset   |     383     |            Yes            |\n| MedicationQA       |   QA dataset   |     172     |            Yes            |\n| LiveQA             |   QA dataset   |     480     |            Yes            |\n| HealthSearchQA     |   QA dataset   |     262     |            Yes            |\n| **Total of Above** |  QA datasets   |    3907     |            Yes            |\n| **MedDisK (Ours)** | Knowledge base |    10632    | Yes (evaluation platform) |\n\n| Dataset         | \\#Popu. | \\#Symp. | \\#Part. | \\#Syst. | \\#Proc. | \\#Medi. | \\#Dept. |\n| --------------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- |\n| MedQA           | 197     | 377     | 574     | 15      | 429     | 62      | 36      |\n| MedMCQA         | 241     | 452     | 1245    | 33      | 811     | 56      | 54      |\n| MMLU  (medical) | 92      | 114     | 250     | 10      | 111     | 6       | 9       |\n| MedicationQA    | 27      | 64      | 52      | 7       | 70      | 67      | 3       |\n| LiveQA          | 75      | 108     | 141     | 9       | 166     | 14      | 19      |\n| HealthSearchQA  | 10      | 63      | 40      | 3       | 4       | 2       | 2       |\n| **Total  of Above** | 349     | 570     | 1362    | 34      | 997     | 183     | 83      |\n| **MedDisK  (Ours)** | 701     | 18737   | 1585    | 89      | 5097    | 3826    | 89      |\n\nThe experimental results show that MedDisK covers significantly more diseases and disease-knowledge-related entities than existing QA datasets, which may demonstrate our claim. For medical knowledge graphs like iBKH, they contain various biomedical entities (disease, gene, molecule, etc.) and their corresponding relations. Generally, these medical KGs cover only a limited number of disease-related relations; for instance, iBKH includes only 6 disease-related relations. In contrast, MedDisK is a disease-centered knowledge base that involves **18** essential knowledge types crucial for clinical practice, including diagnosis and treatment. Additionally, none of these medical knowledge graphs have been utilized in the construction of mainstream medical evaluation datasets yet, making them incomparable to our evaluation dataset. \n\n[1] Kraljevic Z, Searle T, Shek A, et al. Multi-domain clinical natural language processing with medcat: the medical concept annotation toolkit[J]. Artificial intelligence in medicine, 2021, 117: 102083."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214848222,
                "cdate": 1700214848222,
                "tmdate": 1700217517782,
                "mdate": 1700217517782,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]