[
    {
        "title": "Evaluating Representation Learning on the Protein Structure Universe"
    },
    {
        "review": {
            "id": "Qa9TsGua2r",
            "forum": "sTYuRVrdK3",
            "replyto": "sTYuRVrdK3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6130/Reviewer_keJ2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6130/Reviewer_keJ2"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a large suite of benchmarks for evaluating learned embeddings of proteins. Included are node-level evaluations (at the level of individual residues, e.g. inverse folding, metal binding site prediction) and graph-level evaluations (at the level of the entire protein, e.g. fold classification). The authors also provide a number of software tools, including dataloaders for various pretraining tasks. Finally, they evaluate selected architectures on various pretraining task/benchmark combinations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "New benchmarks are always welcome, and there is also great value in consolidating existing benchmarks. This paper does that well, as the selection here is quite broad, with good coverage. I appreciate the inclusion of both node-level and graph-level evaluations. Within each category, each choice of benchmark is accompanied by a specific rationale, which is also a strength. The documentation of the codebase also seems clear and easy to follow."
                },
                "weaknesses": {
                    "value": "It would have been nice to see baselines for all of the downstream tasks in the benchmark using the tools in this software suite (or, lacking that, even scores copied from their respective papers if needs be). Certain tasks like \"reaction class prediction\" are currently missing. Part of the value-add here is the ease of running experiments on all of these tasks, and that isn't currently demonstrated in the current version of the manuscript. Also, consolidated baseline scores are useful sanity checks for reproduction experiments down the line.\n\nMiscellaneous stuff:\n\n- Please provide details about the confidence intervals in Table 2.\n> whereas pLDDT prediction and structure denoising benefit invariant models the most\n- I don't really understand what the basis of this claim (^) is. All four models do approximately as well in the pLDDT column, e.g."
                },
                "questions": {
                    "value": "In Table 3, inverse folding (a downstream node-level task) is listed alongside four other tasks explicitly identified as pretraining tasks above. Is this intentional? \n\nAre the experiments in Table 2 at all realistic? I don't think these tasks ever be attempted using models with no pretraining at all in practice. Also, the ESM model without any pretraining can hardly be called an ESM model at all.\n\nDid you consider adding any restrictions on the interfaces between model embeddings and the downstream tasks? If the goal is to evaluate the embeddings themselves, it seems to me like there should be an attempt to standardize e.g. the size of the task-specific MLPs used."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6130/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6130/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6130/Reviewer_keJ2"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6130/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697846385907,
            "cdate": 1697846385907,
            "tmdate": 1699636664392,
            "mdate": 1699636664392,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bVCwxpFJsK",
                "forum": "sTYuRVrdK3",
                "replyto": "Qa9TsGua2r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer keJ2"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their careful consideration of our work. We are especially glad the reviewer found the coverage of our benchmark to be broad and the codebase to be well documented -- we have put a lot of effort into the engineering! \n\n---\n> It would have been nice to see baselines for all of the downstream tasks in the benchmark using the tools in this software suite...\n\nPlease stay tuned for the updated manuscript containing more downstream results. We will upload this ASAP.\n\n---\n> Please provide details about the confidence intervals in Table 2.\n\nWe report the standard deviation over three runs across different seeds. We will mention this in the revised paper.\n\n---\n> In Table 3, inverse folding (a downstream node-level task) is listed alongside four other tasks explicitly identified as pretraining tasks above. Is this intentional?\n\nThank you for raising this ambiguity. We use the inverse folding task both as a downstream evaluation task (on the CATH-derived dataset by [Ingraham et al](https://www.mit.edu/~vgarg/GenerativeModelsForProteinDesign.pdf)), which is commonly used in the field for this task. We also investigate an inverse folding pre-training objective on the afdb_rep_v4 dataset which we subsequently finetune on the CATH dataset.\n\n---\n> Are the experiments in Table 2 at all realistic? I don't think these tasks ever be attempted using models with no pretraining at all in practice. \n\nWe thank the reviewer for raising this point. We generally agree with the reviewer on this point. These tasks are generally (but not exclusively) attempted with pre-trained models. However, they serve as an important point of reference for understanding the performance gains through pre-training, and investigating whether simpler strategies can achieve comparable results.\n\n---\n> Also, the ESM model without any pretraining can hardly be called an ESM model at all.\n\nWe in fact used a frozen pre-trained ESM2 650M parameter model. We will amend the caption for Table 2.\n\nWe chose the 650M variant of ESM2 because this is the parameter count/scale at which the ESM2 authors observed significant improvements in the model\u2019s ability at structure-related tasks. Our results further reinforce their observation \u2013 we find that combining ESM2 650M with our structural featurisation yields extremely strong results on Fold classification tasks. \n\n---\n> Did you consider adding any restrictions on the interfaces between model embeddings and the downstream tasks? If the goal is to evaluate the embeddings themselves, it seems to me like there should be an attempt to standardize e.g. the size of the task-specific MLPs used.\n\nWe used task-specific MLPs of a consistent size (3 layers with the same width as the GNN encoder, with skip connections), tuning the dropout on the FoldClassification task. We fixed a consistently large hidden dimensionality across GNN encoders (at most 512). See Appendix B5 for more details."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700339636923,
                "cdate": 1700339636923,
                "tmdate": 1700339988784,
                "mdate": 1700339988784,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Fr6L2UTJhY",
                "forum": "sTYuRVrdK3",
                "replyto": "bVCwxpFJsK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6130/Reviewer_keJ2"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewer_keJ2"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply. I look forward to the additional evaluations."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538895092,
                "cdate": 1700538895092,
                "tmdate": 1700538895092,
                "mdate": 1700538895092,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AiZm1pdoRA",
            "forum": "sTYuRVrdK3",
            "replyto": "sTYuRVrdK3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6130/Reviewer_foh9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6130/Reviewer_foh9"
            ],
            "content": {
                "summary": {
                    "value": "The authors have introduced a novel framework aimed at curating datasets sourced from public repositories like PDB. Their objective is to construct benchmark datasets that facilitate the evaluation of protein structure representation. This framework takes into account various settings, including different backbone models such as various GNN-based models and diverse feature representations, to enhance our understanding of protein structure presentation. Additionally, the authors explore a wide range of pretraining tasks, including sequence/structure denoising and inverse folding, before subjecting these models to a battery of diverse downstream tasks, operating at the Alpha carbon, residue, or overall protein levels.\n\nTo support this work, the authors have generously shared an anonymous GitHub link containing scripts for data preprocessing, which enables the creation of datasets for both pretraining and benchmarking. This contribution is particularly noteworthy as it addresses a critical gap in the field of protein structure representation learning. Historically, the lack of systematically curated benchmarking datasets covering aspects such as featurization, pretraining, and downstream task evaluation has hindered progress. The absence of such a framework has made it exceedingly challenging to compare or reproduce research findings in this specialized domain.\n\nHowever, there are some concerns regarding the paper's completeness. Notably, the authors have outlined a set of downstream tasks in Figure 1, yet a significant portion of these tasks remains unreported in the results section."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The unveiling of this framework, designed for assembling public datasets in order to generate pretraining and downstream benchmark datasets for the study of protein structure representation, is a noteworthy development.\n\nThe examination of how pretraining and featurization affect various downstream architectures and tasks proves to be engaging and insightful.\n\nThe timeliness and significance of this research topic cannot be understated, as it addresses the pressing need for a standardized framework that enables the comparison of various state-of-the-art methods using the same benchmark datasets."
                },
                "weaknesses": {
                    "value": "Addressing the Issue of Potential Leakage:\n\nEfforts to mitigate potential data leakage are crucial to ensuring the integrity of benchmarking results, as such leakage could introduce misleading elements into the research findings. Have you considered the removal of overlapping sequences between the pretraining datasets and the downstream testing datasets to further safeguard against such issues?\n\nExpanding Featurization Methods:\n\nIn terms of featurization, the paper seems to primarily focus on simple feature extraction methods. It might be valuable if the authors explored the integration of the following additional featurization techniques:\n\n    Incorporating 3D presentations, such as Uni-Mol (available at https://github.com/dptech-corp/Uni-Mol).\n    Utilizing pretrained models trained on 2D data, like ESM-1b and ESM-2. Considering that Table 1 showcases results using ESM as a backbone model, which produced superior outcomes on the fold dataset, it could be beneficial to include the results for other ESM models in the table.\n\nDiverse Downstream Tasks:\n\nTable 1 predominantly presents results for two specific tasks. However, it appears that several other downstream tasks have not been included in this experiment. It is worth noting that many of these unreported downstream tasks are of substantial importance and are conspicuously absent from the paper. Is there a particular reason for not considering these tasks, and could they potentially be included to provide a more comprehensive view of the research findings?"
                },
                "questions": {
                    "value": "1. Could you please address the potential leakage as discussed in the weakness comments?\n\n2. Could you please consider additional featurization as pointed out in the weakness comments?\n\n3. Could you please complete the experiments and provide the results with the downstream tasks that are missing in the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6130/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698733590537,
            "cdate": 1698733590537,
            "tmdate": 1699636664212,
            "mdate": 1699636664212,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X1ApnYfRai",
                "forum": "sTYuRVrdK3",
                "replyto": "AiZm1pdoRA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6130/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer foh9 (Part 1/n)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback and for finding our work timely and significant!\n\n> Efforts to mitigate potential data leakage are crucial to ensuring the integrity of benchmarking results, as such leakage could introduce misleading elements into the research findings. Have you considered the removal of overlapping sequences between the pretraining datasets and the downstream testing datasets to further safeguard against such issues?\n\nThank you for raising this point. Please consider our counter argument against why using a part of the downstream test dataset for pre-training neural networks is okay.\n\nWe believe that utilizing a portion of the downstream data, **without labels**, to find a suitable model initialization that improves performance on the end task is acceptable in our setting. Our primary goal with using a large corpus of AlphaFold DB structures is to boost downstream task accuracy, and not directly use the label information, which contains key details. This is in line with models in natural language processing like BERT (and even SciBERT), the full Wikipedia datasets were utilized for BERT pre-training without carefully removing any overlap with fine-tuning sets. \n\nNow, protein data may be a special case since exactly identical sequences and structures exist across datasets, unlike in language data. We somewhat mitigate this by using the non-redundant set of AFDB which are predicted structures which consider idealised geometry. There is still a semantic gap between crystal structures from the PDB (our downstream tasks) and AFDB. They are never identical.\n\nHowever, we do agree that when it comes to the domain generalization properties of pre-training models, it might be better to explicitly exclude certain samples in the pre-training phases.\n\nAnother counterargument is that it\u2019s fairly straightforward to obtain and utilize predicted protein structure information from tools like AlphaFold during pre-training in real-world settings. Researchers working on real-world protein modelling and design problems are actively starting to use AlphaFold-predicted structures to augment their datasets and are obtaining powerful gains from it. Two notable examples include:\n- [Learning inverse folding from millions of predicted structures](https://www.biorxiv.org/content/10.1101/2022.04.10.487779v1), Hsu et al. (ICML oral, 2022)\n- [RosettaFold2](https://www.biorxiv.org/content/10.1101/2023.05.24.542179v1), which uses AlphaFold-predicted structures to train the latest RosettaFold version.\n\nIn sum, as long as the pre-training data does not include labels and is consistent across models, we believe it is a reasonably fair methodology for improving downstream performance, and the particular scenario of AlphaFold-predicted structures is already being used in real-world scenarios to the best of our knowledge.\n\n---\n> It might be valuable if the authors explored the integration of the following additional featurization techniques: Incorporating 3D presentations, such as Uni-Mol\n\nUni-Mol is focused on protein binding pocket and ligand (small molecule) interactions, which is **not directly applicable** to our setting. Quoting the repository, \u201cThe framework comprises two models: a molecular pretraining model that has been trained using 209M molecular 3D conformations, and a pocket pretraining model that has been trained using 3M candidate protein pocket data.\u201d\n\nProteinWorkshop focuses on protein-only representation learning and self-supervised pre-training at the node and graph level. Critically, we are interested in tasks that consider the **entire** protein structure, as opposed to the setting of considering a cropped patch (the protein pocket) and its interaction with a ligand.\n\nIn the future, it would be interesting to extend our framework to biomolecular complexes, as the PDB remains the main data source. (See [related response](https://openreview.net/forum?id=sTYuRVrdK3&noteId=Ha3Lo64kpL) to Reviewer W2RU.)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338891041,
                "cdate": 1700338891041,
                "tmdate": 1700339386367,
                "mdate": 1700339386367,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MhB3vYxS0j",
                "forum": "sTYuRVrdK3",
                "replyto": "AiZm1pdoRA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer foh9 (Part 2/n=2)"
                    },
                    "comment": {
                        "value": "> Utilizing pretrained models trained on 2D data, like ESM-1b and ESM-2....it could be beneficial to include the results for other ESM models in the table.\n\nAt present, we chose the 650M variant of ESM2 because this is the parameter count/scale at which the ESM2 authors observed significant improvements in the model\u2019s ability at structure-related tasks. (See Figure 1 in [the Science paper](https://www.science.org/doi/10.1126/science.ade2574) introducing ESM2.) Our results further reinforce their observation \u2013 we find that combining ESM2 650M with our structural featurisation yields extremely strong results on Fold classification tasks. \n\nOur goal with the benchmark was to demonstrate the utility of our ProteinWorkshop framework and to provide new insights from fairly comparing standard architecture choices. It should be fairly straightforward to swap the exact version of the ESM language model used for featurisation within ProteinWorkshop, but we believe we have already used one of the best ESM variant for structural tasks. While we cannot be certain without having done the experiment, it is a reasonable guess that a variant of ESM2 with fewer parameters, or the older generation of ESM1b models, will not perform any better than ESM2 650M nor would this change the outcome of our benchmarks.\n\nWe included ESM2 as a reference, but our focus is on structure-based geometric GNN models and scaling them up.\n\n---\n\n> Table 1 predominantly presents results for two specific tasks. However, it appears that several other downstream tasks have not been included in this experiment. It is worth noting that many of these unreported downstream tasks are of substantial importance and are conspicuously absent from the paper. Is there a particular reason for not considering these tasks, and could they potentially be included to provide a more comprehensive view of the research findings?\n\nThank you for raising this point. Please stay tuned for the updated manuscript containing more downstream results. We will upload this ASAP."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700339179832,
                "cdate": 1700339179832,
                "tmdate": 1700339502684,
                "mdate": 1700339502684,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bf66sveWit",
                "forum": "sTYuRVrdK3",
                "replyto": "MhB3vYxS0j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6130/Reviewer_foh9"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewer_foh9"
                ],
                "content": {
                    "title": {
                        "value": "Still missing downstream tasks in Figure 1"
                    },
                    "comment": {
                        "value": "Thank you for providing your responses to my comments. I have carefully reviewed both the comments from other reviewers and the author's response. However, I maintain the perspective that the paper remains incomplete. Specifically, the handling of leakage lacks thorough attention, essential baselines related to the ESM models have been ignored, and a critical shortcoming is the absence of a significant portion of the promising downstream tasks highlighted in Figure 1 within the experimental framework.\n\nGiven that this paper focuses on a benchmark framework, completeness is crucial for its utility within the research community. It is my belief that addressing these concerns would significantly enhance the overall quality and applicability of the paper.\n\nBest regards,"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633942881,
                "cdate": 1700633942881,
                "tmdate": 1700633942881,
                "mdate": 1700633942881,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zWVL77A7Qy",
            "forum": "sTYuRVrdK3",
            "replyto": "sTYuRVrdK3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6130/Reviewer_5uRv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6130/Reviewer_5uRv"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the task of protein structure representation learning and aims to provide a robust and standardized benchmark for this task. \n\nIn this paper, the authors provide different pretraining datasets, downstream datasets, pretraining tasks, auxiliary tasks, featurisation schemes, and model architectures. They cover most of the widely used training strategies, datasets, and model architectures. \n\nIn addition, the authors run experiments using the provided code base and provide some observations and insights."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is well-written and easy to follow.\n\nThe provided datasets, GNN models, and training strategies are comprehensive."
                },
                "weaknesses": {
                    "value": "1. In addition to datasets etc, I think a good benchmark should also provide experimental results with well-searched hyperparameters. In such case, future researchers can directly take results for a fair comparison. \n - However, in the current version, the authors didn\u2019t provide results on all downstream tasks. \n - In addition, I am not sure whether the hyperparameters are well-searched, since the best results reported here are still worse than some existing methods. For example, the best results on Fold (considering both with and without auxiliary tasks) are still worse than ProNet [1] and CDConv [2] which don\u2019t use any auxiliary tasks.\n\n2. Do ESM results in the table use pre-trained ESM weights?\n\n3. Some other pretraining strategies are used in Geom3D [3] and ESM-GearNet [4].  \n\n[1] Learning Hierarchical Protein Representations via Complete 3D Graph Networks.  \n[2] Continuous-Discrete Convolution for Geometry-Sequence Modeling in Proteins.  \n[3] Symmetry-Informed Geometric Representation for Molecules, Proteins, and Crystalline Materials.  \n[4] A Systematic Study of Joint Representation Learning on Protein Sequences and Structures"
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6130/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698963631589,
            "cdate": 1698963631589,
            "tmdate": 1699636664071,
            "mdate": 1699636664071,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8hbJQhYigh",
                "forum": "sTYuRVrdK3",
                "replyto": "zWVL77A7Qy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5uRv (Part 1/n)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback and constructive comments. We are glad the reviewer found our benchmark comprehensive and the manuscript clear.\n\n> However, in the current version, the authors didn\u2019t provide results on all downstream tasks.\n\nPlease stay tuned for the updated manuscript containing more downstream results. We will upload this ASAP.\n\n---\n> In addition, I am not sure whether the hyperparameters are well-searched, since the best results reported here are still worse than some existing methods. For example, the best results on Fold (considering both with and without auxiliary tasks) are still worse than ProNet and CDConv\n\nGiven the large number of models and featurisation schemes, we did try our best to do a consistent and fair hyperparameter search. See Appendix B5 for details -- essentially, we used the fold classification task to select the best learning rate and dropout per model and featurisation scheme. We fixed a consistently large hidden dimensionality across models, as we also wanted to focus on scaling model size and dataset size via pre-training.\n\nWhile our best performing models do not outperform the best reported results on Fold, we have obtained these results in a consistent and fair experimental setup. Our goal was to demonstrate the utility of our benchmarking framework and uncover the impact of architectural considerations such as featurisation schemes, geometric GNN models, and pre-training/auxiliary tasks. We are confident that the insights from our benchmarking study will subsequently help us build models that outperform the state of the art across both node and graph level representation learning tasks. \n\nFinally, it is worth stating that both CDConv and ProNet are multi-scale architectures with a **highly specialised design tailored to graph/global-level prediction tasks**. It is not trivial to generalise such architectures for node-level tasks, unlike the suite of generic as well as protein-specific geometric GNNs considered in this work.\n\nIn summary, while the subject of this particular paper is to introduce ProteinWorkshop as a platform for evaluating protein representation learning, we are now actively working on building state of the art models for node and graph level protein tasks based on the insights gained!\n\n---\n> Do ESM results in the table use pre-trained ESM weights?\n\nYes, we use a frozen pre-trained ESM2 650M parameter model. We apologize for the oversight and have added this point to the paper. We chose the 650M variant of ESM2 because this is the parameter count/scale at which the ESM2 authors observed significant improvements in the model\u2019s ability at structure-related tasks. Our results further reinforce their observation \u2013 we find that combining ESM2 650M with our structural featurisation yields extremely strong results on Fold classification tasks."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700337700884,
                "cdate": 1700337700884,
                "tmdate": 1700340072095,
                "mdate": 1700340072095,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LLr2v6LA6p",
                "forum": "sTYuRVrdK3",
                "replyto": "zWVL77A7Qy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission6130/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5uRv (Part 2/n=2)"
                    },
                    "comment": {
                        "value": "> Some other pretraining strategies are used in Geom3D [3] and ESM-GearNet [4].\n\nRe. Geom3D and ESM-GearNet: We will cite these papers in the revised version. Do note that Geom3D strategies seem to be specialised for small molecules (as stated in section 4.7 of their paper) but some of them are reasonably close to those studied by us, distance/angle prediction. \n\nLet us provide further justification of how we selected our pre-training tasks.\n\nWe focussed on pre-training tasks that roughly fall under the category of denoising (eg. sequence, coordinates). We were particularly interested in self-supervised objectives that were:\n1. Extremely scalable, so as to pre-train on the large scale AlphaFold Protein Structure Database (AFDB) of 2.4M structures; and \n2. Train protein representations at the fine-grained node level, so as to be general-purpose across the downstream tasks considered. \n\nNaturally, we would have liked to continue exploring more pre-training strategies, but could not do so due to the significant cost of running each pre-training experiment on 2.4M AFDB structures (total cost of GPU hours roughly being $50,000). Notably, the structural clustering of AFDB we use results in a dataset approx. 3x larger than what is used for pre-training GearNet and its variants. Thus, the scalability of the pre-training objectives was a critical consideration in selecting them for our benchmarking study.\n\nWe did not yet consider other possible tasks that have also been used in the literature, such as contrastive learning and generative modelling-inspired objectives. Most of these objectives are (1) computationally heavier and more cumbersome to set up than corruption-type objectives, making them harder to scale up, and (2) only train protein representations for the global/graph level and do not operate at the node level.\n\nWe would also like to highlight that, ultimately, ProteinWorkshop is a platform through which we certainly hope to integrate future pre-training objectives such as those from complementary studies like Geom3D. The goal of the benchmarking study in this paper was to **demonstrate the utility of our framework** and making pre-training on the largest non-redundant corpus of protein structures accessible to the community. We believe this makes ProteinWorkshop an ideal framework to advance self-supervised protein representation learning in the future."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700337864353,
                "cdate": 1700337864353,
                "tmdate": 1700340114030,
                "mdate": 1700340114030,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5jdwAVnb99",
                "forum": "sTYuRVrdK3",
                "replyto": "LLr2v6LA6p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6130/Reviewer_5uRv"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission6130/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewer_5uRv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' response.\n\nOverall, I agree with the contribution. \nHowever, my main concern **results for all downstream tasks are not currently available** is not well addressed. As someone in this research area, I really hope this tool can be used to fairly benchmark different methods and I can directly compare to the results listed in this paper in my future research.\n\nLooking forward to further experimental results, for now, I will keep my score as 5."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674254523,
                "cdate": 1700674254523,
                "tmdate": 1700674254523,
                "mdate": 1700674254523,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u4pm8v6N7C",
            "forum": "sTYuRVrdK3",
            "replyto": "sTYuRVrdK3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6130/Reviewer_W2RU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6130/Reviewer_W2RU"
            ],
            "content": {
                "summary": {
                    "value": "This work presents an open benchmark for evaluating protein structure representation-learning methods. The benchmark includes a diverse set of pre-training methods, downstream tasks, and corpora and includes experimental and predicted protein structures. The structure-based pre-training and fine-tuning datasets and tasks emphasize tasks that enable structural annotation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Modular benchmark enabling rapid evaluation of protein representation learning methods across various tasks, models, representations, and pre-training setups.\n\n2. Analysis of model performance across these different representations and architectures.\n\n3. Using auxiliary tasks to improve the performance of both invariant and equivariant models.\n\n4. Providing tools and procedures for training and evaluating models."
                },
                "weaknesses": {
                    "value": "1. The work is missing an explanation of the limitations of the featurization schemes and pre-training tasks.\n\n2. Would be beneficial to include a discussion about the generalizability of the benchmark results \nto the overall protein structure space, and how this translates to proteins not included in the current dataset.\n\n3. Missing a discussion about how geometric models may be improved to surpass sequence-based models.\n\n4. Missing information about the ease of use of the tools, and details about the computational resources required for using the benchmark.\n\n5. Missing (i) aggregation of methods for improving model performance; and (ii) computation of uncertainties in evaluations."
                },
                "questions": {
                    "value": "Can the work be adapted for other biological macromolecules beyond protein structures?\nSee, for example, the recent reference:\nPerformance and structural coverage of the latest, in-development AlphaFold model, 2023.\nPredicting structure of proteins, nucleic acids, small molecules, ions, and modified residues. Providing a quantitative benchmark, and improving accuracy of protein-ligand structure prediction, protein-DNA and protein-RNA interface structure prediction, and protein-protein interfaces."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6130/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699386363499,
            "cdate": 1699386363499,
            "tmdate": 1699636663970,
            "mdate": 1699636663970,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OY4MVhbOQX",
                "forum": "sTYuRVrdK3",
                "replyto": "u4pm8v6N7C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Reviewer W2RU (Part 1/n)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments on our benchmarking framework as well as analysis! We will improve the work based on all your suggestions which were very useful.\n\n> The work is missing an explanation of the limitations of the featurization schemes and pre-training tasks.\n\nThank you for the suggestion \u2013 we will now include a paragraph each on the limitations of current featurization schemes as well as pre-training tasks in the revised version (which we are preparing with results on new datasets). \n\n**Limitations of featurization schemes:**\n- All of our featurization schemes are at the residue level, with the position of the alpha Carbon atom being assigned the position of a particular node. This is conventional practice in the field due to efficiency, as all atom representations require very high GPU memory/make models extremely slow.\n- The extent to which each scheme is a \u2018complete\u2019 representation of the geometry and structure of the protein residue it represents is variable. For instance, backbone-only featurizations simply ignore the orientations of the side chain atoms in the residue, so the geometric GNN must account for this information implicitly.\n- However, the extent to which providing complete information about all atoms and side chain orientations is debatable, as the exact coordinates from PDB files are known to contain artifacts from structure determination via crystallography (see [ProteinMPNN](https://www.science.org/doi/10.1126/science.add2187)). This is also one of the interesting outcomes of our benchmark \u2013 letting the model implicitly learn about side chain orientation performs better or equally well as explicitly providing complete side chain atomic information.\n\n**Limitations of pre-training tasks:**\n- We only focussed on pre-training tasks that roughly fall under the category of corrupting information in the input (eg. sequence, coordinates) and tasking the model with producing the uncorrupted input. We were particularly interested in self-supervised objectives that were (1) extremely scalable, so as to pre-train on the large scale AlphaFold Protein Structure Database (AFDB) of 2.4M structures; and (2) train protein representations at the fine-grained node level, so as to be general-purpose across the downstream tasks considered.\n- We did not consider other possible tasks that have also been used in the literature, such as contrastive learning and generative modelling-inspired objectives. Most of these objectives are (1) computationally heavier and more cumbersome to set up than corruption-type objectives, making them harder to scale up, and (2) only train protein representations for the global/graph level and do not operate at the node level.\n\n---\n\n> Would be beneficial to include a discussion about the generalizability of the benchmark results to the overall protein structure space, and how this translates to proteins not included in the current dataset.\n\nIndeed, we will elaborate on this further. By using the AFDB as a pre-training corpus, models trained using ProteinWorkshop should, in principle, exhibit strong generalisation to the currently known (and predicted) natural protein structure universe. The reason for this is that afdb_rep_v4, the FoldComp-curated clustering of the AFDB we used for pre-training, contains a non-redundant set of 2.4M structures, ie. each of the structures in the dataset has below 50% TM score with one another, indicating that these are unique folds. Any model trained on afdb_rep_v4 would have \u2018seen\u2019 any new natural protein fold in its pre-training corpus.\n\n---\n\n> Missing a discussion about how geometric models may be improved to surpass sequence-based models.\n\nWe ultimately believe that future protein representation learning models may blur the boundaries of sequence-based vs geometric/structure-based modelling. Concurrent works submitted to ICLR have already been to explore combining geometric graph neural networks with pre-trained protein language models for improved performance on specific downstream tasks, and we believe this trend will continue for the foreseeable future. Moreover, another avenue worth considering for future research on geometric models, one particularly suited to such models, is generative modeling-based pre-training objectives. Some early works have begun to demonstrate the usefulness for downstream tasks of incorporating diffusion-based objectives as a pre-training regime for geometric graph neural networks. As more flexible generative modeling frameworks emerge, we anticipate this trend will also continue, as we will likely see methods such as geometric flow matching begin to supersede diffusion pre-training by instead pre-training models to learnably interpolate between one relevant biomolecular data domain to another. Since our benchmark is built with modularity in mind, our work is poised to quickly take advantage of new generative modeling advances for enhanced geometric pre-training."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334718749,
                "cdate": 1700334718749,
                "tmdate": 1700340046152,
                "mdate": 1700340046152,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ha3Lo64kpL",
                "forum": "sTYuRVrdK3",
                "replyto": "u4pm8v6N7C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6130/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6130/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Reviewer W2RU (Part 2/n=2)"
                    },
                    "comment": {
                        "value": "> Missing information about the ease of use of the tools, and details about the computational resources required for using the benchmark.\n\nWe thank the reviewer for raising this point. We have added a discussion on usability and computational resources to the appendix (pending upload of revised paper). In short, for benchmarking, all models are trained on 80Gb NVIDIA A100 GPUs. Moreover, all baseline and finetuning results are performed using a single GPU while pre-training is performed using four GPUs.\n\nThe modular design of our benchmark means it can be readily adapted into different workflows easily. Firstly, the benchmark is pip-installable from PyPI and contains several importable modules, such as dataloaders, featurisers and models, that can be imported into new projects. This will aid in standardising the datasets and workflows used in protein representation learning. Secondly, the benchmark serves as an easily extendable template, which users can fork and work directly in, thereby reducing implementation burden. Lastly, we provide a CLI that can be used to quickly run single experiments and hyperparameter sweeps with minimal development time overhead.\n\n---\n> Missing (i) aggregation of methods for improving model performance; and (ii) computation of uncertainties in evaluations.\n\n(i) In order to extract the best performance out of models, it is natural that end-users of ProteinWorkshop may want to perform ensembling of the predictions of various methods. However, our goal with this paper was to rigorously compare architectural choices around featurization, GNN layers, and pre-training/auxiliary tasks in a fair manner, as a way of demonstrating the utility of ProteinWorkshop. We did not set out with the goal of obtaining SOTA performance on all datasets. \n\n(ii) Could the reviewer please clarify what they meant by uncertainties in evaluation? We did tune the model hyperparameters fairly across all models, and performed each experiment with multiple random seeds in order to obtain standard deviations across our results. This allows us to make statistically significant takeaways from our benchmarking experiments.\n\n---\n> Can the work be adapted for other biological macromolecules beyond protein structures? ...\n\n- Yes, advances in geometric GNN modelling and methodology should, in principle, be adaptable and translate well to modelling biomolecular complexes among combinations of proteins, small molecules, nucleic acids, and other elements. \n- While the architectural details of the latest version of AlphaFold are not known to the public, all previous architectures which successfully model biomolecular complexes also represent these systems as geometric graphs with atoms/residues embedded as nodes in 3D Euclidean space. Hence, geometric GNNs are the natural architecture for representation learning across biomolecular systems.\n- We currently focus on protein representation learning because: (1) large scale datasets for self-supervised learning, as well as well-defined downstream tasks, are readily available and accepted by the community; and (2) we see protein representation learning as a fundamental or **foundational task**, improving upon which should also advance applications in protein + X complexes. For instance, pre-trained node embeddings from ProteinWorkshop models can readily be ported as initial features for protein + X models.\n- Comparatively, the scale of data for biomolecular complexes is significantly lesser, and there is less consensus among the community on how to perform evaluation; see [PoseCheck](https://arxiv.org/abs/2308.07413) and [PoseBusters](https://arxiv.org/abs/2308.05777) which was also referenced in the new AlphaFold version.\n\nWe will include a discussion on biomolecular complexes in the revised version, too. Indeed, we envision the future of protein representation learning to consider both independent proteins as well as their interactions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700335244665,
                "cdate": 1700335244665,
                "tmdate": 1700335257813,
                "mdate": 1700335257813,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]