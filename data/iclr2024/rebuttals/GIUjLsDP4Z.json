[
    {
        "title": "Effective Structural Encodings via Local Curvature Profiles"
    },
    {
        "review": {
            "id": "c1iiND9Ryu",
            "forum": "GIUjLsDP4Z",
            "replyto": "GIUjLsDP4Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2870/Reviewer_pewF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2870/Reviewer_pewF"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores the efficacy of various structural encodings, along with their integration with global positional encodings, to enhance the performance of Graph Neural Networks (GNNs) in downstream tasks. It introduces a novel structural encoding known as LCP, derived from discrete Ricci curvature, which demonstrates superior performance compared to existing encoding methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The article introduces LCP encoding which presents how curvature information may affect GNN performance, contributing to a better understanding of the research context and significance\n2. The article conducts comprehensive experiments on various datasets; however, it could benefit from additional experiments to investigate the underlying reasons why LCP is effective"
                },
                "weaknesses": {
                    "value": "1. The article lacks explanations for some crucial implementation steps, making it confusing to read. I suggest the author improve the presentation and logic of the article to enhance clarity (see question 1 and 2)\n2. Please provide definitions for variable names, such as 'd_max.' Currently, there are many variables in the article that are not explained, which can be challenging for newcomers to understand\n3. In spite of keeping settings and optimization hyperparameters consistent among different settings, the authors should still provide the corresponding parameter configurations. This would aid in experiment reproducibility and, as a result, make the results more robust.\n4. In Section 3.1, the authors mention, 'We believe that the curvature information of edges away from the extremes of the curvature distribution, which is not being used by curvature-based rewiring methods, can be beneficial to GNN performance.' I consider this assertion somewhat speculative, and I did not find any subsequent experiments that substantiate this claim. Would it be possible to include relevant ablation experiments to support this hypothesis?\n5. In Section 3.1, the authors define LCP as 'five summary statistics of the CMS.' I would appreciate a more detailed motivation for this particular definition. Additionally, it would be beneficial to include relevant ablation experiments that showcase the impact of removing specific summary statistics to demonstrate their significance in influencing the final results."
                },
                "questions": {
                    "value": "1. Could the authors please provide a detailed explanation of the specific approach referred to as 'no encodings (NO)'?\n2. Could the authors please elaborate on how the combination of LCP encoding and position encoding is implemented in Section 4.2.2? I couldn't find any details regarding the actual implementation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2870/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2870/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2870/Reviewer_pewF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2870/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698537545765,
            "cdate": 1698537545765,
            "tmdate": 1699636230552,
            "mdate": 1699636230552,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q7wbWE2CMp",
                "forum": "GIUjLsDP4Z",
                "replyto": "c1iiND9Ryu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2870/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2870/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for the detailed feedback.\n\n> Could the authors please provide a detailed explanation of the specific approach referred to as 'no encodings (NO)'?\n\nBy \u2018no encodings (NO)\u2019, we refer to the setting where only the original node features are available to the GNN, no additional positional or structural encodings were added. We have added a short clarification of this in section 4.2.\n\n> Could the authors please elaborate on how the combination of LCP encoding and position encoding is implemented in Section 4.2.2? I couldn't find any details regarding the actual implementation.\n\nFor the experiments in the main text, we concatenated the positional or structural encodings to the node feature vectors in the original datasets as a preprocessing step. When using two different encodings together, we concatenated both to the original feature vector. We have added a clarification on this in section 4.1. We also ran ablations on some of the datasets considered where instead of only concatenating encodings and thereby increasing the dimension of the node features, we linearly projected the concatenated node features down to the original node feature dimension (following [1]). This had no clear effects on performance, however.\n\n> Please provide definitions for variable names, such as 'd_max.' Currently, there are many variables in the article that are not explained, which can be challenging for newcomers to understand.\n\nWe thank the reviewer for pointing this out. We have added explanations for all previously unintroduced variables.\n\n> In spite of keeping settings and optimization hyperparameters consistent among different settings, the authors should still provide the corresponding parameter configurations. This would aid in experiment reproducibility and, as a result, make the results more robust.\n\nWe have extended appendix section A.3 and included relevant hyperparameter choices to make our results reproducible without considering other papers. \n\n> In Section 3.1, the authors mention, 'We believe that the curvature information of edges away from the extremes of the curvature distribution, which is not being used by curvature-based rewiring methods, can be beneficial to GNN performance.' I consider this assertion somewhat speculative, and I did not find any subsequent experiments that substantiate this claim. Would it be possible to include relevant ablation experiments to support this hypothesis?\n\nWe consider the experiments in section 4.2 to be very strong evidence for the idea that the curvature information of edges away from the extremes of the curvature distribution can be useful for GNN performance. Using the LCP clearly outperforms methods that use the ORC to rewire the graph on all datasets.\n\n> In Section 3.1, the authors define LCP as 'five summary statistics of the CMS.' I would appreciate a more detailed motivation for this particular definition. Additionally, it would be beneficial to include relevant ablation experiments that showcase the impact of removing specific summary statistics to demonstrate their significance in influencing the final results.\n\nOur use of these summary statistics is motivated by their use in the Local Degree Profile (LDP) [2]. As our ablations in Appendix A.4 show, min and max seem to be the most useful summary statistics. \n\n[1] Ramp\u00e1\u0161ek, Ladislav, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. \"Recipe for a general, powerful, scalable graph transformer.\" Advances in Neural Information Processing Systems 35 (2022): 14501-14515.\n\n[2] Cai, Chen, and Yusu Wang. \"A simple yet effective baseline for non-attributed graph classification.\" arXiv preprint arXiv:1811.03508 (2018)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072177156,
                "cdate": 1700072177156,
                "tmdate": 1700072177156,
                "mdate": 1700072177156,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MoBWdhABQn",
            "forum": "GIUjLsDP4Z",
            "replyto": "GIUjLsDP4Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2870/Reviewer_GHx9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2870/Reviewer_GHx9"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to use local curvature profile (LCP) for structural encoding in graph neural networks. Several notions of local curvatures are investigated and superior experimental results are shown on several datasets as compared to the baseline."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The introduction of the local curvatures for structural encoding in graph neural networks is the key contribution of the paper.  A theoretical result (Theorem 1) is also established suggesting improved expressivity due to LCP. However the result is rather qualitative without a quantitative characterization of the extent to which the expressivity is improved. Thus the theoretical development is rather light.\n\nOverall the paper is very well written and, for most parts, easy to read. \n\nThe idea is sound and the experiments look convincing to this reviewer."
                },
                "weaknesses": {
                    "value": "I do not see an obvious weakness in the paper, just like I do not see its development particularly striking.  To me, the paper falls into those works that have a sound intuitive idea, which is validated via empirical evaluation. The paper does not appear to touch on the studied problem (i.e., the issues of over-smoothing and over-squashing) at a fundamental level or at depth. But it is perhaps above the acceptance threshold."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2870/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804216133,
            "cdate": 1698804216133,
            "tmdate": 1699636230477,
            "mdate": 1699636230477,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vKj9smQwUa",
                "forum": "GIUjLsDP4Z",
                "replyto": "MoBWdhABQn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2870/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2870/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the encouraging feedback.\n\n> A theoretical result (Theorem 1) is also established suggesting improved expressivity due to LCP. However the result is rather qualitative without a quantitative characterization of the extent to which the expressivity is improved. Thus the theoretical development is rather light.\n\nWe would like to note that the WL hierarchy is by now a standard tool for measuring expressivity, which is widely used in the Graph Machine Learning literature. Theorem 1 allows for categorizing LCP in terms of the WL hierarchy.\n\n> The paper does not appear to touch on the studied problem (i.e., the issues of over-smoothing and over-squashing) at a fundamental level or at depth.\n\nWe have added additional plots depicting the (normalized) Dirichlet energy, which is commonly used to measure over-smoothing, in Appendix A.9. We find that using the LCP increases the Dirichlet energy almost as much as ORC-based rewiring (BORF), which was explicitly designed to deal with over-smoothing."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700071790553,
                "cdate": 1700071790553,
                "tmdate": 1700071790553,
                "mdate": 1700071790553,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FItakfdsyy",
                "forum": "GIUjLsDP4Z",
                "replyto": "vKj9smQwUa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2870/Reviewer_GHx9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2870/Reviewer_GHx9"
                ],
                "content": {
                    "title": {
                        "value": "I have read your comments"
                    },
                    "comment": {
                        "value": "Thank you for the additional information."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532469126,
                "cdate": 1700532469126,
                "tmdate": 1700532469126,
                "mdate": 1700532469126,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ANRyw7FKun",
            "forum": "GIUjLsDP4Z",
            "replyto": "GIUjLsDP4Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2870/Reviewer_JWiJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2870/Reviewer_JWiJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes improving graph neural networks, e.g. graph convnets. The idea is to encode structural information through Local Curvature Profiles, enabling each node to better characterize the geometry of its neighborhood. Instead of rewriting the graph, the proposed approach adds summary statistics about each node's local curvature to the features of each node.\n\nOn a variety of different tasks, this approach improves the performance of the resulting graph neural nets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper seems reasonable to this reviewer, outperforming baseline encoding approaches or approaches that require rewiring. \n\nPerhaps most surprising to this reviewer is that it improves performance of GATs (seemingly similar to transforms in that they use self-attention?) as it would seem reasonable that such a network would be able to dynamically compute something similar to these statistics.\n\nThe experiments seem reasonably done at least to this reviewer (not an expert in this area at all), involving both LCP itself as well as combining it with positional encoding, and then later rewiring."
                },
                "weaknesses": {
                    "value": "It's not obvious to this reviewer what the weaknesses are. The main concern to this reviewer is that some large pretrained transformer could do better than any of the proposed methods, but that's a very general concern these days. Possibly this approach or GNNs in general could work better on more specialized tasks where there are a very large number of nodes."
                },
                "questions": {
                    "value": "How do the results compare versus model size? E.g. could making a GCN deeper allow it to implicitly compute these kinds of features itself? What's stopping it from doing that?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2870/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808214789,
            "cdate": 1698808214789,
            "tmdate": 1699636230400,
            "mdate": 1699636230400,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BHAeE0QIwa",
                "forum": "GIUjLsDP4Z",
                "replyto": "ANRyw7FKun",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2870/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2870/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for the careful read of our submission and for the very positive feedback.\n\n> How do the results compare versus model size? E.g. could making a GCN deeper allow it to implicitly compute these kinds of features itself? What's stopping it from doing that?\n\nAs shown in [1], message-passing GNNs are generally unable to detect whether a graph contains a cycle of a specific length. The number of cycles of lengths 3-5 is, however, crucial when computing Augmentations of Forman\u2019s Ricci Curvature (see Appendix A.1), which according to [2] can be thought of as low-level approximations of the Ollivier-Ricci Curvature. As such, we would not expect a message-passing GNN to be able to implicitly compute curvature-based features, even if we increased the model size. Even if this were possible, increasing the model size - especially the depth - to such a degree would most likely result in over-smoothing, thus degrading performance.\n\n> Perhaps most surprising to this reviewer is that it improves performance of GATs (seemingly similar to transforms in that they use self-attention?) as it would seem reasonable that such a network would be able to dynamically compute something similar to these statistics.\n\nIn general, our reasoning in the previous comment still applies as GAT is a message-passing GNN. However, we agree with the reviewer that it is surprising that GAT does not at least compute some form of approximation. This phenomenon has also been observed in recent works on over-squashing [3] and presents a promising avenue for future research.\n\n[1] Loukas, Andreas. \"What graph neural networks cannot learn: depth vs width.\" arXiv preprint arXiv:1907.03199 (2019).\n\n[2] Jost, J\u00fcrgen, and Florentin M\u00fcnch. \"Characterizations of Forman curvature.\" arXiv preprint arXiv:2110.04554 (2021).\n\n[3] Di Giovanni, Francesco, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Lio, and Michael M. Bronstein. \"On over-squashing in message passing neural networks: The impact of width, depth, and topology.\" In International Conference on Machine Learning, pp. 7865-7885. PMLR, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700071613929,
                "cdate": 1700071613929,
                "tmdate": 1700071613929,
                "mdate": 1700071613929,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xVTzgUWIzk",
                "forum": "GIUjLsDP4Z",
                "replyto": "BHAeE0QIwa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2870/Reviewer_JWiJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2870/Reviewer_JWiJ"
                ],
                "content": {
                    "title": {
                        "value": "thanks! will keep my score"
                    },
                    "comment": {
                        "value": "thanks for the response! I'm still on board with accepting this paper so I'd like to keep my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723064726,
                "cdate": 1700723064726,
                "tmdate": 1700723064726,
                "mdate": 1700723064726,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "seIrtSjaR6",
            "forum": "GIUjLsDP4Z",
            "replyto": "GIUjLsDP4Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2870/Reviewer_hevh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2870/Reviewer_hevh"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the crucial issue of improving the performance of GNNs through structural encodings. The authors present a novel approach based on discrete Ricci curvature, termed Local Curvature Profiles (LCP), and demonstrate its significant effectiveness in enhancing GNN performance. They also investigate the combination of local structural encodings with global positional encodings and compare these encoding types with curvature-based rewiring techniques. The paper makes important contributions to the field of Graph Machine Learning and provides valuable insights into the potential of curvature-based encodings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- LCP provides a unique way to encode the geometry of a node's neighborhood, and the paper convincingly demonstrates its superior performance in node and graph classification tasks.\n\n- The paper investigates the combination of local structural encodings with global positional encodings, showing that they capture complementary information about the graph. This finding is valuable as it suggests that using a combination of different encoding types can result in enhanced downstream performance. The authors provide empirical evidence to support this claim.\n\n- A theoretical analysis of LCP's computational efficiency and its impact on expressivity is included in the paper."
                },
                "weaknesses": {
                    "value": "- Some parts of the introduction are a bit dense and may be challenging for readers not deeply familiar with the field. A clearer presentation of the background and motivation could benefit a wider audience.\n\n- Including experiments on a more diverse set of datasets and domains would be better."
                },
                "questions": {
                    "value": "How well does LCP generalize across different domains, and what factors might influence its applicability in practical scenarios?\n\nAre there any computational bottlenecks when implementing LCP in large-scale graph datasets, and what strategies or optimizations could be considered to address these issues?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2870/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823974652,
            "cdate": 1698823974652,
            "tmdate": 1699636230296,
            "mdate": 1699636230296,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Eyi1Rj3iKz",
                "forum": "GIUjLsDP4Z",
                "replyto": "seIrtSjaR6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2870/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2870/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for the encouraging feedback.\n\n> Including experiments on a more diverse set of datasets and domains would be better.\n\nWe have added four additional datasets in appendix sections A.7 and A.8: three heterophilious node classification datasets [1] and one graph regression task [2]. These datasets contain both social networks (e.g. Amazon ratings) and molecules (Zinc), i.e. vary in the domain and graph topology. We find that the LCP outperforms all other encodings considered and increases performance for all three models (GCN, GIN, GAT).\n\n> How well does LCP generalize across different domains, and what factors might influence its applicability in practical scenarios?\n\nOur experiments include social networks and networks relevant to the natural sciences (e.g. molecules). While we find that the LCP leads to considerable performance gains across domains, it seems that it is generally more useful for graph-level tasks (classification, regression) than for node-level tasks.\n\n> Are there any computational bottlenecks when implementing LCP in large-scale graph datasets, and what strategies or optimizations could be considered to address these issues?\n\nThe main computational bottleneck that comes with using the LCP is that it scales cubically with the maximum degree of a node in the graph. For example, for the Tolokers dataset that we have added, computing the LCP is infeasible because of the high average degree in the graph (it takes more than six hours, which we consider a timeout).\n\nWe can address this issue by using the combinatorial approximations presented in appendix A.1.3 instead of the ORC itself. These approximations scale linearly in the max degree, so computing the (approximate) LCP for Tolokers, for example, now only takes a few seconds. In addition, the LCP can be implemented with a different curvature notion (Forman\u2019s curvature, short: FRC), which is more scalable than ORC. Experiments for this can be found in Table 5.\n\n> Some parts of the introduction are a bit dense and may be challenging for readers not deeply familiar with the field. A clearer presentation of the background and motivation could benefit a wider audience.\n\nWe will improve the clarity of the writing in the introduction and background and related works sections. If the reviewer has concrete suggestions on which concepts a wider audience might not be familiar with, we would be grateful.\n\n[1] Platonov, Oleg, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. \"A critical look at the evaluation of GNNs under heterophily: are we really making progress?.\" arXiv preprint arXiv:2302.11640 (2023).\n\n[2] G\u00f3mez-Bombarelli, Rafael, Jennifer N. Wei, David Duvenaud, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Benjam\u00edn S\u00e1nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams, and Al\u00e1n Aspuru-Guzik. \"Automatic chemical design using a data-driven continuous representation of molecules.\" ACS central science 4, no. 2 (2018): 268-276."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700071300035,
                "cdate": 1700071300035,
                "tmdate": 1700071300035,
                "mdate": 1700071300035,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0LLSs56Ib8",
                "forum": "GIUjLsDP4Z",
                "replyto": "BHAeE0QIwa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2870/Reviewer_hevh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2870/Reviewer_hevh"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for the response to my comments. The rebuttal has addressed my concerns and I'll keep my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709139782,
                "cdate": 1700709139782,
                "tmdate": 1700709139782,
                "mdate": 1700709139782,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]