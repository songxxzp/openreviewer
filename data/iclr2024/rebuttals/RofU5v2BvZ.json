[
    {
        "title": "Efficient Human-AI Coordination via Preparatory Language-based Convention"
    },
    {
        "review": {
            "id": "uPrJaqnU45",
            "forum": "RofU5v2BvZ",
            "replyto": "RofU5v2BvZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3413/Reviewer_fw9C"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3413/Reviewer_fw9C"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the use of LLMs to facilitate human-AI coordination more intuitively. Specifically, the paper considers the possibility of using the LLMs as a way to set up a convention of how the task will be carried out. In the context of the paper, the convention takes the form of a task-level plan which lays out the different roles and actions to be carried out by each participant. To allow their method to support more complex scenarios, they use an approach to decompose the overall problem into sub-problems and then solve each one sequentially. The method is evaluated both using human proxies and with real human participants. The method is compared against multiple learning-based baselines.  Additionally, they performed an ablation study on their approach, showed the effectiveness of their method on other reasoning benchmarks, and showed how their method can achieve higher levels of human-AI value alignment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I think the paper tackles an important problem, namely coordination in the MARL setting. The paper proposes an interesting approach to using LLMs in this context, i.e., coming up with a coordination plan. I also appreciate the fact that the authors took the trouble to run actual user studies, which are very important in validating such systems. Finally, I also like the fact that the authors acknowledge potential limitations the LLM-based reasoning systems might have with respect to reasoning and propose a method to address it."
                },
                "weaknesses": {
                    "value": "In terms of the weaknesses, I had issues with clarity formulation \u2013 multiple terms were loosely used and not clearly formalized. There is some important information missing about the user studies. Next, there seem to be certain assumptions being made about the work whose implications are never clearly spelled out, and finally, I had some questions about the comparison between the current methods and the RL baselines. Below, I have provided a more detailed discussion of the specific concerns I had.\n\n1. Clarity Issues: First off, there are multiple terms and concepts being referred to or introduced in the paper that are never fully formalized. Two specific ones that confused me were user preference and sequencing strategy. The paper talks about inputting human preferences in multiple places, and some instances of it are included in the example interactions. However, at no point is it made clear what exactly is human-preferences with respect to the underlying game formulations. Are these constraints over policies/total utility received by the user? Is this information already contained in the human reward function but just made more explicit in the input to the LLM? Secondly, the paper talks about problem decomposition and sequencing and how the one used in the paper is a general form that can be used in any task (Limitation section). But apart from some examples given in the figures, there is no exact formalization of this process. How did the authors prove that this decomposition is applicable to all possible extended-form Markov games? What happens when there is concurrency required between different tasks? Can each task be refined in isolation? How does stochasticity affect this process, and how do you account for task failure?\n\n2. User Studies - While I really think it is great that the authors chose to do user studies, I have some concerns about the current user study. First off, I don\u2019t see any mention of the user study being approved or granted exemption by an institutional review board. Similarly, there is no mention of the demographics and background of the participants, how they were recruited, and what the compensation was. Also, since there were only 12 participants and 16 conditions (method*setting), how were the studies counterbalanced? Finally, in the LLM setting, creation and validation of convention introduce new steps. Were any subjective evaluations performed on how much they liked the system? Also, the authors should have measured the cognitive load placed on the participants. \n\n3. Assumptions: Two assumptions that are central to the success of the task are the ability to find high-level actions that can be executed by the low-level controller robustly and also make sense to the user. Here, both components are equally important because a high-level plan is shown to the user that is specified in terms of these high-level actions. If the human cannot make sense of or exactly predict how the agent will carry out these actions, the entire convention process will fall apart. While systems like SayCan make the former assumption (high-level skills), I believe the latter assumption is not crucial to the success of that system. Additionally, the method also expects access to a high-level abstraction of human actions (which is symmetric in Overcooked but may not be so in the most general case). Finally, there seems to be an additional assumption that even though the actions are abstract, any refinement of high-level conventions to low-level actions will be free of conflicts. This is usually hard to guarantee for more general coordination settings.\n\n4. RL Action Space: Finally, it seems that the RL agent is learning policies and directly acting in the low-level action space. How does this allow for a fair comparison? Especially since the skills were learned from human demonstration, wouldn\u2019t it naturally align with how humans would act? Also, did the same 12 participants provide the data from which the skills were learned?"
                },
                "questions": {
                    "value": "Please respond to the questions mentioned as part of the four points mentioned in the earlier section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "There is no mention of an IRB approval for the study discussed in the paper."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3413/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698559933641,
            "cdate": 1698559933641,
            "tmdate": 1699636292768,
            "mdate": 1699636292768,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LHrZNs5xwZ",
                "forum": "RofU5v2BvZ",
                "replyto": "uPrJaqnU45",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely thanks for your thoughtful comments! Response to Reviewer fw9C (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely thanks for you thoughtful comments. Below we have provided some clarifications to your main concerns. Further questions are always welcome!\n\n**Concern 1 Clarity Issues: Unclear usage of items and concepts, especially regarding user preference and sequencing strategy.**\n\n> **C1-1  Unclear about what exactly is human-preference**\n\n**A:** We sincerely apologize for any confusion caused by our unclear statements. Actually, the human-preference here means that the humans can have certain preference priorities regarding how to collaboratively accomplish the task and what specific roles they undertake. The same concept has also been adopted in existing works, such as [1, 2]. \n\nMore specifically, on the Overcooked, human-preference can be that the human prefers to utilize the left pot instead of the right pot, or the human prefers to make onion soup instead of tomato soup. We have added explanations about this concept in our revised version.\n\n> **C1-2 The paper talks about problem decomposition \u2026 is applicable to all possible extended-form Markov games?**\n\n**A:** We apologize for the unclear statement. In original statement, we intended to explain that our approach to task decomposition is relatively abstract and not strongly coupled to the Overcooked task, instead of claiming that it is applicable to all possible extended-form Markov games. We have modified our statement and incorporated it in Appendix F.\n\n> **C1-3 What happens when there is concurrency required between different tasks? Can each task be refined in isolation?**\n\n**A:** Although the Overcooked environment does not involve concurrency between different tasks, for new environment with this requirement, it is entirely feasible for HAPLAN to support this functionality. Actually, we only need to design the prompts to make the derived convention express between which tasks there exist concurrency.\n\n> **C1-4 How does stochasticity affect this process?**\n\n**A:** The planning of LLMs disregards stochasticity as the resulting high-level conventions are insensitive to it.  Instead, we expect the low-level skill policies can be robust to the stochasticity.\n\nOn Overcooked environment, there does not exist stochasticity in the transition and reward functions. However, there can exist stochasticity in the agent\u2019s observation because of the uncertainty in the partner\u2019s state. We address this issue by collecting the demonstration segments with random-policy partner. How to train low-level skill policies that are robust to the stochasticity of the environment itself is an important future research direction.\n\n> **C1-5 How do you account for task failure?** \n\n**A:** When encountered with task failure, we would inspect the high-level convention and low-level skill policies respectively. Firstly, we can check whether there exist issues in the high-level convention. If so, we will regenerate the convention through providing human feedback. Similar adaptive replanning methods can be referenced in [3, 4]. If not, it indicates that there is something wrong with the low-level skill policies, and we will re-train the low-level skills to address this issue.\n\n**Concern 2 User Studies: Concerns about the user study**\n\n**A:** We sincerely appreciate this detailed feedback. We have provided clarification in our **Common Response to Q1**.\n\n**Concern 3 Assumptions: Some assumptions should be stated clearly**\n\n**A:** Thanks to your comment.\n\n- **(Two assumptions are central to the success of the task)** Thanks for this kind reminder. We acknowledge that these two assumptions are significant for HAPLAN and we have added this discussion in our revision. As mentioned by the reviewer, systems like SayCan [5] also requires the first assumption, which is kindly feasible in many scenarios. For the second assumption, despite the skill actions in SayCan are not conveyed to humans, they are also represented in the form of natural language, which implicitly indicates that humans are likely to be able to understand these skills.\n- **(Access to a high-level abstraction of human actions)** The high-level abstraction of human actions is related to the task itself. Since we focus on human-AI coordination tasks, we assume that we can have access to this portion of information and provide it in the examples preceding the prompt. We have stressed this assumption in our revision.\n- **(Refinement of high-level conventions will be free of conflicts)** We appreciate your thoughtful comment. However, we actually do not require this assumption. It is possible that conflicts arise during the refinement of high-level conventions. When such situation occurs, we consider these conventions as undesirable and expect to regenerate them through human feedback. For complex scenarios beyond immediate human recognition, humans can also avoid similar situations after an episode of failure."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461894350,
                "cdate": 1700461894350,
                "tmdate": 1700462122853,
                "mdate": 1700462122853,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SHV0ulF0hx",
                "forum": "RofU5v2BvZ",
                "replyto": "uPrJaqnU45",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fw9C (2/2)"
                    },
                    "comment": {
                        "value": "**Concern 4 RL Action Space: Unfair comparison with RL agent**\n\n**A:** Sorry for our unclear statements. To make sure that the comparison with RL agents is fair, the data utilized in our experiments to learn low-level skills are not complete task trajectories; rather, it only consists of trajectory segments that accomplish specific skills, such as delivering onions to the pot in the top left corner. Thus, these demonstration data is purely functional and does not reflect any human behavior preference. Besides, these trajectory segments are actually collected by our authors, and not provided to the participants.\n\n**References**\n\n[1] Hu, Hengyuan, and Dorsa Sadigh. Language instructed reinforcement learning for human-ai coordination[J].\u00a0arXiv preprint arXiv:2304.07297, 2023.\n\n[2] Wang, Xihuai, et al. Quantifying Zero-shot Coordination Capability with Behavior Preferring Partners[J].\u00a0arXiv preprint arXiv:2310.05208, 2023.\n\n[3] Zhou, Siyuan, et al. Adaptive Online Replanning with Diffusion Models[C].\u00a0NeurIPS, 2023.\n\n[4] Guo, Yanjiang, et al. DoReMi: Grounding language model by detecting and recovering from plan-execution misalignment[J].\u00a0arXiv preprint arXiv:2307.00329, 2023.\n\n[5] Ahn, Michael, et al. Do as i can, not as i say: Grounding language in robotic affordances[J].\u00a0arXiv preprint arXiv:2204.01691, 2022."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462173139,
                "cdate": 1700462173139,
                "tmdate": 1700464576512,
                "mdate": 1700464576512,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GBl3I3M4P2",
                "forum": "RofU5v2BvZ",
                "replyto": "uPrJaqnU45",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer fw9C, have our responses addressed your questions?"
                    },
                    "comment": {
                        "value": "Dear Reviewer fw9C:\n\nWe thank you again for your comments and hope our responses could address your questions. As the response system will end soon, please let us know if we missed anything. More questions on our paper are always welcomed. If there are no more questions, we will appreciate it if you can kindly raise the score.\n\nSincerely yours,\n\nAuthors of Paper3413"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613532533,
                "cdate": 1700613532533,
                "tmdate": 1700613532533,
                "mdate": 1700613532533,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l4EpLmkfJb",
                "forum": "RofU5v2BvZ",
                "replyto": "GBl3I3M4P2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3413/Reviewer_fw9C"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3413/Reviewer_fw9C"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewers"
                    },
                    "comment": {
                        "value": "I really appreciate the detailed response. Correct me if I am wrong, but it seems the decomposition method is pretty ad hoc. The questions like concurrency, task failures, and stochasticity, were aimed at testing how the current decomposition method would handle such issues. From your response, it mostly seems that the decomposition method would need to be updated to account for such cases."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614951088,
                "cdate": 1700614951088,
                "tmdate": 1700614951088,
                "mdate": 1700614951088,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tNYZM9Qzig",
                "forum": "RofU5v2BvZ",
                "replyto": "uPrJaqnU45",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We sincerely appreciate your valuable feedback!"
                    },
                    "comment": {
                        "value": "We are really delighted to see your new feedback! Firstly, we would like to clarify that the purpose of our work is NOT to provide a general task decomposition approach for various problems. Our main contribution is to propose a new paradigm for human-AI coordination through the integration of LLMs, as also pointed out by Reviewer nsKW. (Sorry for not highlighting it to you earlier)\n\nWhile the current decomposition method applies to all layouts on Overcooked, it is true that we would need to modify it when facing some new cases, like cases with concurrency requirement or stochasticity. However, we do not see this as a fundamental weakness but as an opportunity to enhance task coordination performance by incorporating our task priors. This capability is a direct result of introducing LLMs, which is clearly not allowed in the traditional Zero-Shot Coordination methods [1, 2, 3].\n\nOnce again, we extend our gratitude for your feedback.\n\n**References**\n\n[1] Johannes Heinrich, Marc Lanctot, and David Silver. Fictitious self-play in extensive-form games. ICML, 2015.\n\n[2] Rui Zhao, Jinming Song, Yufeng Yuan, Haifeng Hu, Yang Gao, Yi Wu, Zhongqian Sun, and Wei\nYang. Maximum entropy population-based training for zero-shot human-ai coordination. AAAI, 2023.\n\n[3] Chao Yu, Jiaxuan Gao, Weilin Liu, Botian Xu, Hao Tang, Jiaqi Yang, Yu Wang, and Yi Wu. Learning zero-shot cooperation with humans, assuming humans are biased. ICLR, 2023."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626696365,
                "cdate": 1700626696365,
                "tmdate": 1700627513751,
                "mdate": 1700627513751,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Mfn9kSjSp2",
            "forum": "RofU5v2BvZ",
            "replyto": "RofU5v2BvZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3413/Reviewer_i2dJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3413/Reviewer_i2dJ"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose the use of large language models (LLMs) to facilitate coordination between humans and AI within human-AI teams through language-based conventions. Their findings indicate that LLMs are adept at aligning with humans who exhibit various coordination patterns, outperforming other baseline methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well written with the exception of two subsections (5.3 and 5.4). \n- Utilizing LLMs to come up with coordination strategies taking human preferences into account is an interesting direction. \n- Experiments on the ability of the approach to coordinate with humans of different coordination patterns have been done using both real humans and proxy human models."
                },
                "weaknesses": {
                    "value": "- The requirement for humans to decompose problems is a significant precondition. Additionally, humans are tasked with evaluating the LLM-generated plans and executing the tasks, which could be problematic in complex domains.\n- The findings regarding the use of multiple sessions are somewhat expected, given that the problem decomposition is already done, significantly reducing the difficulty of the problem for the LLM. \n- The details regarding the additional benchmark results are vague, particularly whether they refer to the coordination aspect or the use of multiple sessions.\n- Depending on human evaluations to assess plans may be unreliable in complex domains, even when the evaluator is an expert."
                },
                "questions": {
                    "value": "- Were there any cases where the LLM provided actions that were not at all executable in the environment? If so, how were they dealt?\n- Figure 2 is hard to comprehend. What are the boxes in blue supposed to convey?\n- Do the additional benchmark results have human-AI coordination as well? How are the respective tasks being divided between humans and the AI? It is unclear from the write-up."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3413/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698924068078,
            "cdate": 1698924068078,
            "tmdate": 1699636292687,
            "mdate": 1699636292687,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QitdA2T5jU",
                "forum": "RofU5v2BvZ",
                "replyto": "Mfn9kSjSp2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Truly appreciate your valuable feedback! Below we have provided some clarifications in hopes that they can be helpful."
                    },
                    "comment": {
                        "value": "### **Weaknesses**\n\n**W1 Unclear writing of Section 5.3 and 5.4**\n\n**A:** We apologize for the unclear writing of two subsections of 5.3 and 5.4. Actually, the experiments in these two subsections are both aimed to validate the effectiveness of utilizing multiple LLM sessions. Detailed clarification is provided in the response to **Q3**. We have improved the writing in our revision. \n\n**W2 Concern about the requirement for human to decompose problems and to evaluate & execute the plans generated by LLM**\n\n**A:** We acknowledge that it is a practical problem. Currently, we follow the common mechanisms using LLMs like in [1, 2], which have a reliance on human manual work. In the future, we will explore utilizing AI to decompose problems automatically and help evaluate the generated plans. Recently, there have been some efforts [3, 4] to explore related problems.\n\n**W3 The usage of multiple sessions are expected**\n\n**A:** Firstly, we sincerely appreciate the reviewer\u2019s acknowledgement of the feasibility of utilizing multiple LLM sessions. Although this finding is somewhat expected, we have not come across any statements regarding this conclusion in the relevant literature. To our knowledge, we are the first to formally propose this finding. We hope it can inspire more research in the future. \n\n**W4 Vague details regarding the additional benchmark results**\n\n**A:** Apologize for the vague descriptions in this part. Actually, the additional benchmark results have nothing to do with the coordination aspect but to validate that utilizing multiple sessions can also help enhance the planning capability of LLMs in the reasoning tasks. We have made it clearer in our revision.\n\n**W5 Human evaluation may be unreliable**\n\n**A:** We acknowledge that this may be a limitation of our approach at this stage. As stated in our response to **W2**, in the future we will explore to utilize AI to help evaluate the plans in complex domains, as [3, 4] did.\n\n**References**\n\n[1]  Zhou D, Sch\u00e4rli N, Hou L, et al. Least-to-most prompting enables complex reasoning in large language models[J]. arXiv preprint arXiv:2205.10625, 2022.\n\n[2] Yang, Xinyi, et al. Human-in-the-loop Machine Translation with Large Language Model[J].\u00a0arXiv preprint arXiv:2310.08908, 2023.\n\n[3] Bai, Yuntao, et al. Constitutional ai: Harmlessness from ai feedback[J].\u00a0arXiv preprint arXiv:2212.08073, 2022.\n\n[4] Lee, Harrison, et al. RLAIF: Scaling reinforcement learning from human feedback with ai feedback[J].\u00a0arXiv preprint arXiv:2309.00267*,* 2023.\n\n### **Questions**\n\n**Q1 Any cases that the LLM provided non-executable high-level actions?**\n\n**A:** Yes, there exist this kind of cases. Such as in the case shown in Figure 5(b), the initial plan generated by LLMs instructs the human player to fetch the onions from the bottom left corner, but this is not achievable due to the obstacle in the middle. When encountered this situation, the human can provide further feedback to help LLMs modify the plans. \n\n**Q2 Figure 2 is hard to comprehend. What are the boxes in blue supposed to convey?**\n\n**A:** We are sorry for any confusion in Figure 2. The boxes in blue means the questions that the LLM is expected to answer. We appreciate your feedback and we have provided a clearer depiction in the revised version.\n\n**Q3 Do the additional benchmark results have human-AI coordination as well? How are the respective tasks being divided between humans and the AI?**\n\n**A:**  Actually, the additional benchmark results are unrelated to the human-AI coordination, thus without need to divide the tasks between humans and AI. We apologize for this unclear statement again, and we have made modifications in our revised manuscript."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461687463,
                "cdate": 1700461687463,
                "tmdate": 1700462326942,
                "mdate": 1700462326942,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vNfvojRl0j",
                "forum": "RofU5v2BvZ",
                "replyto": "Mfn9kSjSp2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer i2dJ, have our responses addressed your questions?"
                    },
                    "comment": {
                        "value": "Dear Reviewer i2dJ:\n\nWe thank you again for your comments and hope our responses could address your questions. As the response system will end soon, please let us know if we missed anything. More questions on our paper are always welcomed. \n\nSincerely yours,\n\nAuthors of Paper3413"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613740096,
                "cdate": 1700613740096,
                "tmdate": 1700613740096,
                "mdate": 1700613740096,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UVWW78YG8V",
            "forum": "RofU5v2BvZ",
            "replyto": "RofU5v2BvZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3413/Reviewer_nsKW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3413/Reviewer_nsKW"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents HAPLAN, and LLM-based approach to planning for ad hoc human-AI cooperation.  At a high-level, HAPLAN passes a textual description of the task (various scenarios in the Overcooked! environment in this case) to an LLM (ChatGPT), requesting that it return a detailed plan with instructions for both the human and the AI.  The AI's component of the plan is executed by a set of pre-trained, goal-conditioned policies for the various sub-tasks, specifically \"fetching\" and \"delivering\" items around the kitchen.  The human observes the plan before performing the task, and has the opportunity to provide corrective, natural language feedback to correct defects in the original plan.  Their experiments show that HAPLAN achieves significantly higher scores with real human partners and scripted AI partners than pervious methods proposed for ad hoc cooperation in Overcooked!.\n\nThe authors argue that one of the key contributions of the work is the idea of processing different stages of the planning process (high level planning, timing calculations, subtask ordering) in separate GPT sessions, which helps overcome the difficulties in reasoning about long conversation histories.  They support this argument with experiments (unrelated to ad hoc cooperation) on a set of benchmark reasoning tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The main strength of the paper in my opinion is that it proposes (and experimentally validates) a new paradigm for practical human-AI cooperation that leverages the strengths of modern large language models to enable natural language interaction as part of the coordination process itself.  I could imagine that with further development, this approach could be useful for applications such as customer-support chatbots, for tasks requiring close coordination between the human and the chatbot to resolve a technical issue."
                },
                "weaknesses": {
                    "value": "My main concern with the work is that the improved performance of HAPLAN relative to the baseline algorithms may have much less to do with its ability to cooperate with a variety of partners, and more to do with the superiority of the LLM-based hierarchical planning approach over the \"flat\" and \"uninformed\" RL algorithms used by the baseline approaches.\n\nHAPLAN incorporates a great deal of human task knowledge, some of it through the natural language interface itself, but also through the use of pre-trained low-level policies that implicitly describe a decomposition of the high-level task into considerably simpler sub-tasks.  As Overcooked! can be challenging even without the ad hoc component (due to sparse rewards and the complexity of the policie needed to achieve them), it seems likely that the additional information available to HAPLAN would make a substantial difference in task performance against any partner.\n\nTo test this alternative explanation for the results, it would be useful to see how well HAPLAN compares to joint policies trained together for the overcooked task (such that each policy is a best-response to its partner).  It would also be helpful to provide more information about the \"diversity\" of strategies observed during human subjects experiments.\n\nA related issue is that some important details of the experimental setup have been omitted.  Most significantly, it is unclear how plans are \"stepped\" during the interaction, that is, how the agent implementing the HAPLAN plan knows when an item has been fetched and delivered, and decides to move on to the next step?\n\nFinally, while not a weakness of the HAPLAN approach itself, it is important to clarify for the reader that HAPLAN operates in a very different cooperation paradigm than the baselines.  While this paradigm, with a detailed conversational coordination phase prior to any physical interaction, may be suitable in some settings, it may not be useful in others (such as real time human-robot shared autonomy)."
                },
                "questions": {
                    "value": "1. How was the textual plan generated by HAPLAN \"stepped\" during execution?\n2. Was there any real-time synchronization between the human and the AI?  For example, would the AI wait for the human if they were delayed in completing a prerequisite task?\n3. How long were individual episodes of interaction (how many time steps, were the agents allowed to complete multiple dishes?)\n4. For the scripted agents (Table 1), what information about the specific scripted policy was provided via the prompts?\n5. Were humans allowed to construct their own initial prompts?  Rather than providing the instruction given to the human, could the human provide their own description of the role they planned to take?\n6. How closely did human's observed behavior match the joint strategy they finally agreed upon?\n7. It wasn't immediately clear, but were humans allowed to provide text-based feedback between episodes, or only in advance of episodes?\n8. How much variance was there in the types of feedback humans provided?\n9. How would a plan generated by HAPLAN compare to a joint policy trained via a cooperative MARL method?\n10. How much prompt engineering was required here?  The prompts themselves are quite complex; are any results available with elements of these prompts removed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3413/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3413/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3413/Reviewer_nsKW"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3413/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698955972615,
            "cdate": 1698955972615,
            "tmdate": 1700683435060,
            "mdate": 1700683435060,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VeNzT5SibN",
                "forum": "RofU5v2BvZ",
                "replyto": "UVWW78YG8V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your interest in our work and valuable comments! Response to Reviewer nsKW (1/3)"
                    },
                    "comment": {
                        "value": "### **Weaknesses**\n\n**W1 Analysis on performance improvement of HAPLAN**\n\n**A:** Due to the strong hierarchical planning capabilities of LLMs and our proposed establishing conventions via feedback, HAPLAN can coordinate with diverse partners. To further analyze the performance improvement of HAPLAN, we additionally conducted two experiments:\n\n- **Performance of jointly learned policy**: We train a joint policy on Overcooked via MAPPO [1], the performance of which we denote as \u201cOracle\u201d since both sides are best responses to each other. The results are listed in the following table. We can see that HAPLAN can achieve 75%~89% of the oracle performance thanks to planning by LLMs and human feedback. Although it is currently slightly less than 90%, HAPLAN still outperforms other baselines significantly. We hypothesize a significant factor affecting the overall performance is that the human participants are new to Overcooked task. We will conduct further research in the future to enhance performance.\n\n\n  |                       | Counter Circle   | Asymmetric Advantages | Soup Coordination | Distant Tomato   | Many Orders      |\n  | --------------------- | ---------------- | --------------------- | ----------------- | ---------------- | ---------------- |\n  | HAPLAN (First Round)  | 135.00$\\pm$8.66  | 345.00$\\pm$16.58      | 195.00$\\pm$8.66   | 325.00$\\pm$29.58 | 350.00$\\pm$53.85 |\n  | HAPLAN (Second Round) | 160.00$\\pm$14.14 | 360.00$\\pm$24.49      | 205.00$\\pm$8.66   | 355.00$\\pm$16.58 | 380.00$\\pm$50.99 |\n  | HAPLAN (Real Human)   | 170.00$\\pm$17.32 | 385.00$\\pm$21.79      | 215.00$\\pm$16.58  | 370.00$\\pm$22.36 | 410.00$\\pm$51.96 |\n  | Oracle                | 225.19$\\pm$18.31 | 447.63$\\pm$8.11       | 240.67$\\pm$4.13   | 481.51$\\pm$54.08 | 462.76$\\pm$12.71 |\n\n- **``diversity'' of strategies:**  We also visualize the human behaviors from human subject study. The result is shown in Figure 10 of our revised manuscript, where we can see that the participants encompass enough diversity. Considering that HAPLAN achieves the best performance in experiments, we conclude that our method can coordinate with diverse teammates.\n\n**W2 Constraints under certain problem setting**\n\n**A:** We greatly appreciate your suggestion and have made corresponding clarifications in the newest revision.\n\n### **Questions**\n\n**Q1 How was the textual plan generated by HAPLAN ``stepped'' during execution?**\n\n**A:** The textual plan generated by HAPLAN is a sequence of skills for both human and AI. Humans can act seamlessly based on their natural language understanding of the plan. For AI, the execution requires to implement two additional functions, namely, **index function** and **completion function**. Here, the index function determines which low-level skill policy to call according to the text, while the completion function determines whether the current skill has been completed.\n\nWe implement both the index function and complete function as rules in Overcooked, with the details provided in the supplementary materials. Investigating learning based implementation is an interesting direction, and we plan to leave it for future work.\n\n**Q2 Was there any real-time synchronization between the human and the AI?**\n\n**A:** Yes. Overcooked is a real-time synchronous environment, which means that the feedback ($s', r)$ for the next time step is provided only when both human and AI\u2019s actions are determined.\n\n**Q3 How long were individual episodes of interaction?**\n\n**A:** The horizon of the individual episodes of interaction is 400 timesteps, and the agents are allowed to complete multiple dishes."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461077339,
                "cdate": 1700461077339,
                "tmdate": 1700462836392,
                "mdate": 1700462836392,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "02QucR4RkI",
                "forum": "RofU5v2BvZ",
                "replyto": "UVWW78YG8V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nsKW (2/3)"
                    },
                    "comment": {
                        "value": "**Q4 For the scripted agents (Table 1), what information about the specific scripted policy was provided via the prompts?**\n\n**A:**   The prompts will convey the behavioral preference of each script agent to the LLMs. For example, for `Onion Placement` scripted agent, we will mention ``I will continuously place onions or tomatoes into the pot'' in the prompt.\n\n**Q5 Were humans allowed to construct their own initial prompts? Rather than providing the instruction given to the human, could the human provide their own description of the role they planned to take?**\n\n**A:** Yes, humans are allowed to construct their own initial prompts and provide their own description of the role they planned to take, all of which will be part of the prompts to the LLMs. \n\n**Q6 How closely did human\u2019s observed behavior match the joint strategy they finally agreed upon?**\n\n**A:** Since both human and AI strictly follow the convention to take their actions, the human\u2019s observed behavior fully match the convention (or the so-called joint strategy here) they finally agreed upon. The supplementary materials also provide a video demo of human-AI coordination.\n\n**Q7 It wasn\u2019t immediately clear, but were humans allowed to provide text-based feedback between episodes, or only in advance of episodes?**\n\n**A:** Text-based feedback between episodes is allowed. Actually, in our real human study, we had each human participant cooperate with HAPLAN for three episodes, requiring human feedback at the beginning of each episode. To help humans focus more on the task, we currently do not allow them to provide feedback in an episode.\n\n**Q8 How much variance was there in the types of feedback humans provided?**\n\n**A:** Actually, the human feedbacks can reflect quite different human preferences. Taking the layout of Many Orders as an example, we can list the following different types of human preferences:\n\n| Human Preferences |\n| ------------------------------------------------------------ |\n| Responsible for placing ingredients in the left pot, not in charge of delivering the dishes. |\n| Responsible for placing ingredients in the right two pots, not in charge of delivering the dishes. |\n| Responsible for placing ingredients in all these three pots, not in charge of delivering the dishes. |\n| Responsible for delivering the dishes from the middle pot, not in charge of placing ingredients in the pots. |\n| Responsible for delivering the dishes from the left pot and right pot, not in charge of placing ingredients in the pots. |\n| Responsible for delivering the dishes from the three pots, not in charge of placing ingredients in the pots. |\n| Responsible for placing ingredients in the left pot and delivering the soup from the left pot. |\n| Responsible for placing ingredients in the left pot and delivering the soup from the left and middle pot. |\n| Responsible for placing ingredients in the middle and right pot and delivering the soup from the right pot. |\n| Responsible for placing ingredients in the right pot and delivering the soup from the three pots. |\n\nTaking the first type of human preference as an example, the human feedback can take various forms. We have compiled the occurrences observed in our experiments, which are as follows:\n\n| Human Feedback |\n| ------------------------------------------------------------ |\n| Please use the two pots on the right to make tomato soup and, by the way, take care of delivering the vegetables to the pot on the left. |\n| Use the two pots on the right to make tomato soup and also be responsible for delivering vegetables to the pot on the left. |\n| Cook tomato soup with the two pots on the right and, at the same time, be in charge of delivering vegetables to the pot on the left. |\n| Make tomato soup with the two pots on the right, and make sure to handle the vegetable delivery for the pot on the left as well. |\n| The two pots on the right are for making tomato soup, and you also need to take care of delivering vegetables to the pot on the left. |\n| Please use the two pots on the right to cook tomato soup and, in the meantime, take care of delivering vegetables to the pot on the left. |\n| Cook tomato soup with the two pots on the right, and also be responsible for delivering vegetables to the pot on the left. |\n| Please use the two pots on the right to make tomato soup, and also be responsible for delivering vegetables to the pot on the left. |\n| Simmer tomato soup with the two pots on the right, and also complete the vegetable delivery for the pot on the left. |\n| Please use the two pots on the right to make tomato soup and take care of the vegetable delivery task for the pot on the left. |"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461228266,
                "cdate": 1700461228266,
                "tmdate": 1700464312755,
                "mdate": 1700464312755,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YFvcgIigYj",
                "forum": "RofU5v2BvZ",
                "replyto": "UVWW78YG8V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nsKW (3/3)"
                    },
                    "comment": {
                        "value": "**Q9 How would a plan generated by HAPLAN compare to a joint policy trained via a cooperative MARL method?**\n\n**A:** Overall, HAPLAN can attain a performance ranging from 75% to 89% compared to a jointly trained policy using cooperative MARL methods. More details are provided in our response to **W1**.\n\n**Q10 How much prompt engineering was required here? The prompts themselves are quite complex; are any results available with elements of these prompts removed?**\n\n**A:** We thanks for your valuable feedback. However, we believe we have already used the minimal possible version of prompts and it is hard to further reduce them.\n\nConsidering the complexity of the problem itself, we decompose the whole problem into several sub-problems and design specific prompts for a separate LLM session to solve each sub-problem. This kind of task decomposition has been demonstrated effective to improve the reasoning quality of the LLMs in Least-to-Most [2]. The prompts for each session contributes to the overall completion of the entire task and are indispensable.\n\nBesides, it is also inappropriate to remove any element in the prompts for one specific session. Taking the prompts for Session 1 in our Appendix G as an example, the first part `In a collaborative cooking game, ... the pot is onion` introduces the task background knowledge to the LLM, the second part `For Example 1: The instructions ... the middle pot` provides some demonstrations for the subproblem, while the third part `Now, the instructions ... examples` proposes the concrete question to the LLM.\n\nActually, part 1 and part 3 are obviously necessary for the LLM session to solve the subproblem, while part 2 as demonstrations have already been proved effective in OpenAI\u2019s official document [https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions](https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions). Moreover, we claim that we have already made the prompts design as concise as possible while ensuring the functionality of these prompts, as redundancy is unnecessary.\n\n**References**\n\n[1] Yu C, Velu A, Vinitsky E, et al. The surprising effectiveness of ppo in cooperative multi-agent games[C]. NeurIPS, 2022.\n\n[2] Zhou D, Sch\u00e4rli N, Hou L, et al. Least-to-most prompting enables complex reasoning in large language models[J]. arXiv preprint arXiv:2205.10625, 2022."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461308627,
                "cdate": 1700461308627,
                "tmdate": 1700461339984,
                "mdate": 1700461339984,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rgCnZigHgX",
                "forum": "RofU5v2BvZ",
                "replyto": "UVWW78YG8V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer nsKW, have our responses addressed your questions?"
                    },
                    "comment": {
                        "value": "Dear Reviewer nsKW:\n\nWe thank you again for your comments and hope our responses could address your questions. As the response system will end soon, please let us know if we missed anything. More questions on our paper are always welcomed. \n\nSincerely yours,\n\nAuthors of Paper3413"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613803748,
                "cdate": 1700613803748,
                "tmdate": 1700613803748,
                "mdate": 1700613803748,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6qIyxNffZ6",
                "forum": "RofU5v2BvZ",
                "replyto": "rgCnZigHgX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3413/Reviewer_nsKW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3413/Reviewer_nsKW"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttals and other reviews"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to respond to my questions in detail.  I believe that you have addressed one of my concerns, which was that in your experiments the HAPLAN agent did not face a significant coordination challenge once a human-AI plan had been generated.  The apparent variety of human preferences indicates that there was in fact significant diversity in the strategies followed by different human partners.\n\nLooking at the other three reviews, however, I tend to agree with the assessment that the HAPLAN approach would require significant effort to generalise to other tasks beyond overcooked.  I believe there are two things that future work could do make a stronger case that HAPLAN-like agents could be a practical solution to real-world human-AI coordination problems:\n1. First, formalise HAPLAN in domain-independent terms, and make it clear what would be required to apply the same approach in a different environment.  It appears to me that this would require designing (or training) a new \"plan execution module\" that translates textual plans to low-level actions, as well as a new set of prompts to describe the new planning problem itself.  Little of the engineering work that has gone into HAPLAN could be directly transferred to a new task as is.  What remains is the abstract process of plan generation and interactive plan refinement, which could serve as a template for new agents in other domains.\n2. Second, implement a HAPLAN agent in a second domain, preferably one that is closer to \"real world\" conditions than the benchmark Overcooked domain.  The goal here would be to show that not only do the planning abilities of LLMs generalise, but that humans can (and are willing) to follow AI-generated plans in practical settings. \n\nAt present , I view the work as a preliminary proof-of-concept of the use of LLMs to support ad hoc cooperation, and have adjusted my score to reflect this."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683414680,
                "cdate": 1700683414680,
                "tmdate": 1700683414680,
                "mdate": 1700683414680,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YjGZybM1xB",
            "forum": "RofU5v2BvZ",
            "replyto": "RofU5v2BvZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3413/Reviewer_nmM1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3413/Reviewer_nmM1"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors present an approach to have humans coordinate with large language models via giving instructions to the LLM prior to the interaction (referred to as establishing a convention in the paper). These instructions are then iterated on during a sequence of interactions. The LLM is controlled via complex prompts (given in the supplementary materials) for each task which are translated into in-game actions via developer-specified low-level skills learned through demonstrations. The results indicate that the authors' approach (HAPLAN) meets or slightly exceeds the performance of the baselines, with greater effects on later rounds."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "There is certainly a need to explore better ways for human users to interact with and controller Transformer-based models. This is also, to the best of my knowledge, an original approach for the Overcooked task specifically. The quality in terms of the amount of engineering work and time cost for the human subject study is also impressive. The results are likely to be of interest to those working with the Overcooked tasks or those with the resources to replicate the work HAPLAN requires for each domain."
                },
                "weaknesses": {
                    "value": "This paper has three major weaknesses. \n\nFirst is the approach itself. This is not clearly conveyed in the text of the paper, certainly not in sufficient detail for replication. As far as I can tell, the process is that the authors come up with a domain-specific sequence of prompts for the sequence of prompts in a domain. This includes coming up with a set of low-level skills to refer to in these prompts. They then train these low-level skills via imitation learning. Then, during an interaction, they use an LLM to interact with the user and based on the user's prompts/conventions, the LLM produces an output plan referencing the low-level skills, which is then executed. If this is the case, this is an approach that has a lot of barriers to generalization, requiring human expertise and significant development time. It's also not guaranteed to generalize to every human-AI interaction domain, as it may not always be possible to break a problem into sequences or tasks or into low-level skills. The novelty of this approach is also fairly low, relying on putting together existing approaches.\n\nSecond is the human subject study methodology. There's no clarity in the paper in terms of what this methodology was. While the supplementary materials indicate that efforts were put forth to attempt to decrease bias, it's unclear what this means exactly. There are many possible sources of bias in terms of what population was recruited from, how they were compensated, what instructions they were given, and so on. Clarification on these points, ideally through a complete breakdown of the methodology, is necessary in order to avoid any potential that the results might be tainted by bias. \n\nThird is the results. The improvement in terms of the results is fairly marginal for what appears to be a much more complex and engineering-intense approach. Further, almost all the baselines see improvement over the three rounds, so it's unclear to what extent the convention is helping. The inclusion of a version of HAPLAN without the convention might have helped clarify this."
                },
                "questions": {
                    "value": "1. Am I correct in my understanding of HAPLAN?\n2. Am I correct in my understanding of the development/design and engineering work needed to adapt HAPLAN to a new domain?\n3. What was the methodology of the human subject study?\n4. Is the improvement of HAPLAN significant?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Other reasons (please specify below)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "My concerns have been addressed with additional details."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3413/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3413/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3413/Reviewer_nmM1"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3413/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698986947142,
            "cdate": 1698986947142,
            "tmdate": 1700534404786,
            "mdate": 1700534404786,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RYoEOCqSu3",
                "forum": "RofU5v2BvZ",
                "replyto": "YjGZybM1xB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your detailed comments! Response to Reviewer nmM1 (1/3)"
                    },
                    "comment": {
                        "value": "### **Weaknesses**\n\n**W1 Concerns about the development and engineering work needed to adapt HAPLAN to a new domain**\n\n**A:** Thanks for your insightful comment. We admit that it may be inevitable to design new prompts when facing a brand-new domain. However, we claim that it is not a fatal drawback of HAPLAN:\n\n- Firstly, we would like to re-claim that the purpose of our work is NOT to provide a general prompt suitable for every human-AI coordination task. Instead, just as pointed out by Reviewer nsKW, our main contribution is that we propose a new paradigm for human-AI coordination that leverages the strengths of LLMs.\n- Secondly, we agree that to guide LLMs to perform in a manner aligned with human expectations needs carefully designed prompts. However, with the publications of some recent works on prompting LLMs, such as Chain-of-Thought [1], Least-to-Most [2], Tree-of-Thought [3], etc. and tutorials on the Internet, such as [4,5,6], we can anticipate that prompt engineering is becoming progressively easier.\n- Moreover, we have provided the complete prompts in Appendix F used in our experiments, which we believe can serve as good references for new domains. Providing clear descriptions of task details and rules, incorporating human preferences, and leveraging techniques like Chain-of-Thought and using multiple sessions proposed in our work can all be helpful.\n\n**W2 Whether it is always possible to break a problem into sequences or tasks or into low-level skills**\n\n**A:** We appreciate your thoughtful comment. We assume that the task can be divided into sequences due to the sequential property of Dec-POMDPs. Moreover, we argue that the task can be divided into sub-tasks or low-level skills is not a strong assumption. Many problems could be divided into sub-tasks [7] or hierarchical structure [8], as the complex world is decomposable or nearly decomposable[9, 10]. Below are more evidences:\n\n- On one hand, most existing collaborative problems can be structurally decomposed, such as SMAC [11] which can be divided based on which specific target to attack; Google Research Football [12] which can be divided on multiple subtasks or skills, like interception, passing and shooting; not to mention the Overcooked we use.\n- On the other hand, this assumption is commonly used in existing studies. In SayCan [13], as pointed out by Reviewer fw9C, the authors assume that we can pre-train low-level reusable skills, like \u201cpick up the apple\u201d. Least-to-most [2] decomposes a hard problem into several easier subproblems to facilitate the reasoning of LLMs. Moreover, a large body of existing multi-agent hierarchical reinforcement learning methods, such as [14], dealing with coordination tasks also make similar assumptions.\n\nFor some non-decomposable setting, we leave it as an significant future work.\n\n**W3 Novelty of the approach**\n\n**A:** Our **main contribution**, as mentioned by Reviewer nsKW, lies in proposing a novel paradigm for human-AI coordination. To our best knowledge, this is the first work that utilizes LLMs to enable natural language interaction as part of the coordination process itself. \n\nThrough our proposed decomposing the problem, solving subproblems via multiple LLM sessions, and constructing conventions via human feedback, HAPLAN significantly enhances the efficiency of human-AI coordination.\n\n**W4 Clarification on the methodology of the human subject study?**\n\n**A:** We apologize for any confusion caused by our unclear statement. We have provided clarification to human subject study in **Common Response Q1**. Hope it will help."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459448340,
                "cdate": 1700459448340,
                "tmdate": 1700462791712,
                "mdate": 1700462791712,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XiqKc10YWr",
                "forum": "RofU5v2BvZ",
                "replyto": "YjGZybM1xB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nmM1 (2/3)"
                    },
                    "comment": {
                        "value": "**W5 To what extent do the conventions help?**\n\n**A:** We appreciate your thoughtful comment. Our response is two-fold:\n\n- First, the baselines in our experiments do not utilize conventions, as they are unable to explicitly incorporate feedback. The performance improvement of them across three rounds is mainly because that humans become more familiar with the task and optimize their own decision-making in the game.\n- To further check **to what extent** the convention is helping, we externally include one baseline denoted as HAPLAN w/o convention, which disables developing conventions via human feedback. The result is shown in the subsequent table, where we can see that HAPLAN w/o convention suffers from a significant performance decline especially at the final round, indicating the effectiveness of convention.\n\n|              |                       | Counter Circle | Asymmetric Advantages | Soup Coordination | Distant Tomato | Many Orders  |\n| ------------ | --------------------- | -------------- | --------------------- | ----------------- | -------------- | ------------ |\n| First Round  | HAPLAN w/o convention | 135.00$\\pm$21.79 | 340.00$\\pm$14.14        | 195.00$\\pm$16.58    | 325.00$\\pm$25.98   | 355.00$\\pm$45.55 |\n|              | HAPLAN                | 135.00$\\pm$8.66  | 345.00$\\pm$16.58        | 195.00$\\pm$8.66     | 325.00$\\pm$29.58   | 350.00$\\pm$53.85 |\n| Second Round | HAPLAN w/o convention | 145.00$\\pm$16.58 | 350.00$\\pm$10.00        | 205.00$\\pm$16.58    | 335.00$\\pm$16.58   | 365.00$\\pm$29.58 |\n|              | HAPLAN                | 160.00$\\pm$14.14 | 360.00$\\pm$24.49        | 205.00$\\pm$8.66     | 355.00$\\pm$16.58   | 380.00$\\pm$50.99 |\n| Third Round  | HAPLAN w/o convention | 150.00$\\pm$22.36 | 355.00$\\pm$8.66         | 205.00$\\pm$8.66     | 340.00$\\pm$28.28   | 370.00$\\pm$33.16 |\n|              | HAPLAN                | 170.00$\\pm$17.32 | 385.00$\\pm$21.79        | 215.00$\\pm$16.58    | 370.00$\\pm$22.36   | 410.00$\\pm$51.96 |\n\n**References**\n\n[1] Wei J, Wang X, Schuurmans D, et al. Chain-of-thought prompting elicits reasoning in large language models[J]. Advances in Neural Information Processing Systems, 2022, 35: 24824-24837.\n\n[2] Zhou D, Sch\u00e4rli N, Hou L, et al. Least-to-most prompting enables complex reasoning in large language models[J]. arXiv preprint arXiv:2205.10625, 2022.\n\n[3] Yao S, Yu D, Zhao J, et al. Tree of thoughts: Deliberate problem solving with large language models[J]. arXiv preprint arXiv:2305.10601, 2023.\n\n[4] OpenAI. Prompt engineering. [https://platform.openai.com/docs/guides/prompt-engineering](https://platform.openai.com/docs/guides/prompt-engineering).\n\n[5] Isa Fulford, Andrew Ng. ChatGPT Prompt Engineering for Developers**.** [https://learn.deeplearning.ai/chatgpt-prompt-eng/](https://learn.deeplearning.ai/chatgpt-prompt-eng/)\n\n[6] Lilian Weng. Prompt Engineering. [https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)\n\n[7] Guestrin, Carlos, et al. Efficient solution algorithms for factored MDPs[J].\u00a0Journal of Artificial Intelligence Research*,* 2003.\n\n[8] Pateria, Shubham, et al. Hierarchical reinforcement learning: A comprehensive survey[J].\u00a0ACM Computing Surveys, 2021.\n\n[9] Zhang, Chongjie.\u00a0Scaling multi-agent learning in complex environments. University of Massachusetts Amherst, 2011.\n\n[10] Yuan, Lei, et al. Multi-Agent Concentrative Coordination with Decentralized Task Representation[C]. IJCAI, 2022.\n\n[11] Samvelyan, Mikayel, et al. The starcraft multi-agent challenge[J].\u00a0arXiv preprint arXiv:1902.04043, 2019.\n\n[12] Kurach, Karol, et al. Google research football: A novel reinforcement learning environment[C].\u00a0AAAI, 2020.\n\n[13] Ahn, Michael, et al. Do as i can, not as i say: Grounding language in robotic affordances[J].\u00a0arXiv preprint arXiv:2204.01691, 2022.\n\n[14] Zhang F, Jia C, Li Y C, et al. Discovering generalizable multi-agent coordination skills from multi-task offline data[C], ICLR, 2022."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459495056,
                "cdate": 1700459495056,
                "tmdate": 1700460222748,
                "mdate": 1700460222748,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mYIg0e5kqF",
                "forum": "RofU5v2BvZ",
                "replyto": "YjGZybM1xB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nmM1 (3/3)"
                    },
                    "comment": {
                        "value": "### **Questions**\n\n**Q1, 2 Am I correct in my understanding of HAPLAN? Am I correct in my understanding of the development/design and engineering work needed to adapt HAPLAN to a new domain?**\n\n**A:** Please refer to our reponse to W1.\n\n**Q3 What was the methodology of the human subject study?**\n\n**A:** Please refer to our common response to human subject study.\n\n**Q4 Is the improvement of HAPLAN significant?**\n\n**A:** To check whether the improvement of HAPLAN is significant, we compare the percentage of performance improvement of HAPLAN over the second-best baseline with both human proxy models and real humans. The results are presented in the following two tables.\n\n| (Human Proxy) Layout | Partner                     | Performance improvement to the second-best baseline |\n| -------------------- | --------------------------- | --------------------------------------------------- |\n| Soup Coordination    | Onion Placement & Delivery  | 10.03%                                              |\n| Soup Coordination    | Tomato Placement & Delivery | 14.61%                                              |\n| Distant Tomato       | Tomato Placement            | 41.18%                                              |\n| Distant Tomato       | Tomato Placement & Delivery | 26.82%                                              |\n| Many Orders          | Delivery                    | 15.89%                                              |\n\n| (Real Human) Layout                                  | Counter Circle | Asymmetric Advantages | Soup Coordination | Distant Tomato | Many Orders |\n| ---------------------------------------------------- | -------------- | --------------------- | ----------------- | -------------- | ----------- |\n| Performance improvement to the second-based baseline | 3.03%          | 4.05%                 | 7.50%             | 5.71%          | 9.33%       |\n\nFrom the above tables, we see that: When tested with human proxy models, HAPLAN achieved a performance improvement of **more than 10%** compared to the second-best baseline in **half of the scenarios**, and a significant performance improvement of **more than 40%** in Distant Tomato; when tested with real humans, HAPLAN surpassed all other baselines in all of the layouts."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460307702,
                "cdate": 1700460307702,
                "tmdate": 1700462810203,
                "mdate": 1700462810203,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u4tPXBbCYg",
                "forum": "RofU5v2BvZ",
                "replyto": "mYIg0e5kqF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3413/Reviewer_nmM1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3413/Reviewer_nmM1"
                ],
                "content": {
                    "title": {
                        "value": "Re: Response to Reviewer nmM1"
                    },
                    "comment": {
                        "value": "I thank the reviewers for their thoughtful and exhaustive response. In summary, I had identified three initial weaknesses. First was the approach. I appreciate the author's argument that prompt engineering will become easier and that most tasks can be broken into subtasks. In addition, while I am still concerned about the novelty of the approach in terms of explicitly setting a convention being a relatively minor variation of existing LLM agent interaction, the inclusion of a baseline does help to address this.\n\nSecond, I had concerns with the lack of human subject study methodology description. I appreciate the author's explanation of the study methodology. However, I still retain two concerns. First, \"we ensured similarities in age and gender distribution among these groups\" makes it sound as though the researchers intentionally placed certain subjects in certain groups, which is a clear bias risk. Second, that the explanation seems to suggest that only 5 participants were in each condition, which is insufficient for statistical significance. \n\nThird was the concern around the results. I very much appreciate the inclusion of the additional baseline. An increase in 10% may not be significant in terms of an increase, particularly with such large std dev. If there are at least 10 participants in each condition a statistical test would be helpful here. \n\nOverall, all of my concerns have been at least partially addressed but I retain some questions. I'm upgrading my review to reflect this."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534347364,
                "cdate": 1700534347364,
                "tmdate": 1700534347364,
                "mdate": 1700534347364,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]