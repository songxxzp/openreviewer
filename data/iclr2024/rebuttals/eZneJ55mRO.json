[
    {
        "title": "G$^2$N$^2$ : Weisfeiler and Lehman go grammatical"
    },
    {
        "review": {
            "id": "Op2doEGzQL",
            "forum": "eZneJ55mRO",
            "replyto": "eZneJ55mRO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1280/Reviewer_giVQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1280/Reviewer_giVQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new architecture for GNNs that captures precisely the expressive power of 3WL. This architecture is based on a grammatical representation of a language over graphs that has the same expressive power as 3WL. The idea is that this new architecture permits a more efficient implementation than the 3WL-based GNNs, which are known not to scale well in practical scenarios."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is very polished and easy to follow\n- The topic is timely and the problem practically relevant\n- Experiments confirm the suitability if the approach"
                },
                "weaknesses": {
                    "value": "There is only one criticism I make to the paper and it is the lack of search for a principled explanation of why the GNNs based on MATLANG are more efficient than the ones based on 3WL."
                },
                "questions": {
                    "value": "Could you please comment further on the main criticism I posed above: what do you think is the main reason the MATLANG-based GNNs are more practically suitable than the standard ones based on 3WL? This came as a big surprise to me, and it is a bit dissatisfying to stay with an explanation based on experiments only. I feel that a more principled, perhaps theoretical explanation, is lacking."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1280/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697659108250,
            "cdate": 1697659108250,
            "tmdate": 1699636054783,
            "mdate": 1699636054783,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ykItOSZciW",
                "forum": "eZneJ55mRO",
                "replyto": "Op2doEGzQL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1280/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1280/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Global reply to the Reviewer giVQ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for his comments and question. We are very pleased that he appreciates the writing of our paper, its relevance and the suitability of our approach. Below, we reply to his question, hoping that these responses will clarify the positioning of MATLANG-based GNNs in the 3-WL literature."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1280/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236651817,
                "cdate": 1700236651817,
                "tmdate": 1700236651817,
                "mdate": 1700236651817,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wNIZtdwcmz",
                "forum": "eZneJ55mRO",
                "replyto": "Op2doEGzQL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1280/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1280/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the question about the reason why MATLANG-based GNNs are more practically suitable that the standard ones based on 3WL"
                    },
                    "comment": {
                        "value": "*Reviewer question : what do you think is the main reason the MATLANG-based GNNs are more practically suitable than the standard ones based on 3WL? This came as a big surprise to me, and it is a bit dissatisfying to stay with an explanation based on experiments only. I feel that a more principled, perhaps theoretical explanation, is lacking.*\n\nIn the experiments of section 4, PPGN is the only model to have a provable 3-WL expressive power Maron et al. (2019). Indeed, if 3-GNN has a 3-WL expressive power, its relaxed version 1-2-3-GNN does not provably reach this expressiveness.     \n\nIn the subsection 3.5 of the paper, we present some theoretical explanation concerning the superiority of G$^2$N$^2$ on PPGN for downstream tasks. These explanations take their root in the CFG we build from the PPGN model (please see the appendix A.3). We argue that the CFG of PPGN misses a variable ($V_c$) and a lot of rules of r-$G(L_3)$. \n\nDuring the rebuttal period, thanks to the questions of the other reviewers, we investigate the expressive power of our architecture at a fixed number of layer. This investigation leads to the following proposition, which we add to appendix A.5. \n\n**Proposition[Graph isomorphism (WL) expressiveness at fixed depth]**\n\n*Let $k \\in \\{1,2,3 \\}$, G$^2$N$^2$ and E-G$^2$N$^2$ have the same separative power after $k$ layers.*\n\n\nThe proof of this proposition, also available in appendix A.5, has highlighted the benefit of the $V_c$ variable and in particular of the rule $MV_c$. Since PPGN needs more layers to compute a matrix carrying this rule its expressive power at a fixed number of layer is lower. \n\nAnother important aspect is that PPGN needs to approximate some of these missing rules using MLPs. To guarantee such  an approximation, a certain width and depth for MLP are needed. G$^2$N$^2$ does not suffer from these computational constraints since it only needs to provide linear combinations as arguments of the operations.  \n\nH. Maron, H. Ben-Hamu, H. Serviansky, and Y. Lipman. Provably powerful graph networks.\nAdvances in neural information processing systems, 32, 2019."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1280/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236833212,
                "cdate": 1700236833212,
                "tmdate": 1700236833212,
                "mdate": 1700236833212,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mfRaBGTsCx",
            "forum": "eZneJ55mRO",
            "replyto": "eZneJ55mRO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1280/Reviewer_3x4U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1280/Reviewer_3x4U"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a framework to convert context-free rules over an algebraic matrix language into a GNN architecture. Using this framework, they produce a WL-3 GNN as follows: (1) they write down a set of context-free rules producing a language that is just as expressive as 3-WL, (2) they reduce this set of rules into a smaller set of rules, and (3) they translate these rules directly into a GNN architecture. The resulting architecture performs competitively in practice, outperforming various existing GNNs on a variety of tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "(1) While this is not my area, the contribution of the paper seems strong in that it presents a framework for designing GNN architectures that implement a given CFG.\n\n(2) The experiments seem strong, and the proposed GNN is both provably expressive and performs competitively compared to existing architectures.\n\n(3) The paper is well-written, clear, and well-organized."
                },
                "weaknesses": {
                    "value": "Some minor weaknesses are discussed in the questions section."
                },
                "questions": {
                    "value": "(1) While many CFGs are equally expressive if we can apply their rules an arbitrary number of times, it seems like what we actually care about is the expressiveness of the grammar after L rule applications, given that in practice our GNNs are finite depth. In light of this intuition, the paper might benefit from some discussion of which CFGs are preferable, given that they have the same expressive power.\n\n(2) From my understanding, it seems that this architecture outperforms other architectures in some datasets but not others (table 6), but has the advantage of being the strongest GNN out of those with provable expressivity (since it dominates PPGN). Is this understanding correct? If so, what is the utility of having provable expressivity, beyond the model's performance on the datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1280/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1280/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1280/Reviewer_3x4U"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1280/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807517080,
            "cdate": 1698807517080,
            "tmdate": 1699636054717,
            "mdate": 1699636054717,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qIZzRhab7d",
                "forum": "eZneJ55mRO",
                "replyto": "mfRaBGTsCx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1280/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1280/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Global reply to the Reviewer 3x4U"
                    },
                    "comment": {
                        "value": "We thank the reviewer for his constructive comments and questions. We are pleased that he appreciates the writing of our paper and that he considers our contributions and experiments strong. Below, we reply to his two questions in two separated comments, hoping that these responses will clarify some aspects of our work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1280/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236124619,
                "cdate": 1700236124619,
                "tmdate": 1700236124619,
                "mdate": 1700236124619,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8J5PUM7tCp",
                "forum": "eZneJ55mRO",
                "replyto": "mfRaBGTsCx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1280/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1280/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the question about the impact of the number of GNN layers on the expressiveness"
                    },
                    "comment": {
                        "value": "*Reviewer question : While many CFGs are equally expressive if we can apply their rules an arbitrary number of times, it seems like what we actually care about is the expressiveness of the grammar after L rule applications, given that in practice our GNNs are finite depth. In light of this intuition, the paper might benefit from some discussion of which CFGs are preferable, given that they have the same expressive power.*\n\nThe reviewer is right, many CFGs share the same expressive power. As shown by Theorem 3.2. of the paper, it is the case for r-G(L3) and G(L3). We would like to emphasize that this theoretical result assumes an arbitrary depth, as usually done in the literature when considering the expressive power of a GNN model (Zhang et al. (2023)).\n\nDespite this statement, it is true that the number of rule applications (i.e. the number of layers in the GNN) has an impact on the expressiveness. Thus, questioning the differences between the different CFGs and the impact of their depth is very relevant. Please note that this question has also been raised by reviewer HFhT.    \n\nWe have examined this aspect from a theoretical point of view during the rebuttal period. In this limited time, we manage to provide the following proposition showing that both models have the same separative power for a fixed number of layers $k$ such that $k \\leq 3$. \n\n**Proposition[Graph isomorphism (WL) expressiveness at fixed depth]**\n\n*Let $k \\in \\{1,2,3 \\}$, G$^2$N$^2$ and E-G$^2$N$^2$ have the same separative power after $k$ layers.*\n\n\nThis result is mentioned in Section 4, question **Q1**. The proposition and its full proof have been added in appendix A.5. The rationale behind the proof is that graph isomorphism (i.e. the WL hierarchy) measures expressiveness at graph-level, as mentioned in section 2 of our paper and in Zhang et al. (2023). As a consequence, for measuring the expressive power at a given depth, we are interested in strings that compute a scalar.\n\nGeneralizing this result to deeper models, with $k>3$, needs further investigations that will be led in our future work.   \nWe thank the reviewer again for this question which led us to find this new proposition which theoretically strengthens our paper since all our experimental results are obtained with $k=3$. \n\nB. Zhang, C. Fan, S. Liu, K. Huang, X. Zhao, J. Huang, and Z. Liu. The expressive power\nof graph neural networks: A survey. arXiv preprint arXiv:2308.08235, 2023."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1280/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236339056,
                "cdate": 1700236339056,
                "tmdate": 1700236339056,
                "mdate": 1700236339056,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MjisTrqOd7",
                "forum": "eZneJ55mRO",
                "replyto": "mfRaBGTsCx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1280/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1280/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the question on the results of G$^2$N$^2$ on some datasets"
                    },
                    "comment": {
                        "value": "*Reviewer question : From my understanding, it seems that this architecture outperforms other architectures in some datasets but not others (table 6), but has the advantage of being the strongest GNN out of those with provable expressivity (since it dominates PPGN). Is this understanding correct? If so, what is the utility of having provable expressivity, beyond the model's performance on the datasets?*\n\nIt is true that although G$^2$N$^2$ has a provable expressiveness, it does not outperform all other architectures for all the datasets in Table 6. Our model is slightly outperformed by some architectures on the IMDB and NCI1 datasets. [Please note that during the rebuttal we realised that there was an error in table 6, as G$^2$N$^2$ is not ranked second but third on the IMDB-B dataset. We apologise for this typo which has been corrected in the new version].\n\nIn the case of the IMDB datasets and the NCI1 dataset, our model is outperformed by the GSN model, proposed in Bouritsas et al. (2022). This is due to the specific characteristics of these datasets since graphs in IMDB consist of a union of dense subgraphs (cliques). Hence, GSN is based on feature augmentation with subgraph counting both at edge- and node-level. More precisely, their best model on IMDB computes 5-clique at edge-level, which is not computable by 3-WL GNNs. In the same vein, for the NCI1 dataset, they compute the 15-cycle at edge level when achieving their best results, which again is not computable by 3-WL GNNs. \n\nSuch results illustrate the importance of the expressive power of models on the downstream tasks performance. They also highlight that the WL hierarchy is not the only way to assess expressive power, since substructure counting abilities also impacts these performance. This aspect has been highlighted in many recent papers such as Frasca et al. (2022); Huang et al.\n(2023). These considerations open the door to the building of grammars based on subgraph counting. We plan to investigate this aspect in our future works.  \n\nConcerning the performance of GNTK on IMDB which are comparable with ours, as mentioned in Chami et al. (2022), GNTK is a graph Kernel based method. One can conjectures that the kernel used by GNTK well fit on the specificity of IMDB dataset. Further investigations are needed to bridge the gap between kernel based methods and expressiveness issues.\n\nG. Bouritsas, F. Frasca, S. Zafeiriou, and M. M. Bronstein. Improving graph neural network\nexpressivity via subgraph isomorphism counting. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 45(1):657\u2013668, 2022.\n\nI. Chami, S. Abu-El-Haija, B. Perozzi, C. R\u00b4e, and K. Murphy. Machine learning on graphs:\nA model and comprehensive taxonomy. The Journal of Machine Learning Research, 23(1):\n3840\u20133903, 2022.\n\nF. Frasca, B. Bevilacqua, M. M. Bronstein, and H. Maron. Understanding and extending\nsubgraph gnns by rethinking their symmetries. In Advances in Neural Information\nProcessing Systems, 2022.\n\nY. Huang, X. Peng, J. Ma, and M. Zhang. Boosting the cycle counting power of graph\nneural networks with I2\n-GNNs. In The Eleventh International Conference on Learning\nRepresentations, 2023."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1280/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236598725,
                "cdate": 1700236598725,
                "tmdate": 1700236598725,
                "mdate": 1700236598725,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jBgVhiW9z7",
                "forum": "eZneJ55mRO",
                "replyto": "MjisTrqOd7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1280/Reviewer_3x4U"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1280/Reviewer_3x4U"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your comprehensive answers."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1280/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713525029,
                "cdate": 1700713525029,
                "tmdate": 1700713525029,
                "mdate": 1700713525029,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EgTtBtVB80",
            "forum": "eZneJ55mRO",
            "replyto": "eZneJ55mRO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1280/Reviewer_HFhT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1280/Reviewer_HFhT"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the expressive power of 3-WL from the aspect of formal language. The authors show that 3-WL is equivalent to a context-free grammar (CFG), and propose a reduced CFG that preserves the same expressiveness. Based on the reduced CFG, the authors develop a new WL algorithm and GNN model that match the expressiveness of 3-WL. The new GNN model achieves competitive performance and efficiency on downstream tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "There are some positive points of this paper. \n* The paper is well-written and easy to follow.\n* The paper exploits the formal language equivalence to investigate the GNN model and design a new GNN model, which I think is a promising direction for future research."
                },
                "weaknesses": {
                    "value": "I have some concerns about the paper as follows:\n* I am not convinced by the novelty and the contribution of the paper, as the CFG $G_\\mathcal{L_3}$\u200b\u200b seems to be a straightforward derivation of the MATLANG.\n* The validation and discussion of the reduced CFG and the corresponding GNN may be insufficient, both empirically and theoretically. I have some questions for the authors below."
                },
                "questions": {
                    "value": "* In the theoretical aspect, although the two CFGs r-$G_{\\mathcal{L}_3}$ and $G_{\\mathcal{L}_3}$ have the same expressive power, it may take more steps for r-$G_{\\mathcal{L}_3}$ than $G_{\\mathcal{L}_3}$ to generate the same string. Therefore, how can $G^2N^2$ match the expressiveness of the ordinary 3-WL GNN with a fixed number of layers?\n* I would also like to see how the new GNN model performs on the ZINC-12k and ZINC-full datasets, which are widely used benchmarks for molecular property prediction."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1280/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1280/Reviewer_HFhT",
                        "ICLR.cc/2024/Conference/Submission1280/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1280/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817281336,
            "cdate": 1698817281336,
            "tmdate": 1700366067267,
            "mdate": 1700366067267,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qGJe5clqxt",
                "forum": "eZneJ55mRO",
                "replyto": "EgTtBtVB80",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1280/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1280/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Global reply to the Reviewer HFhT:"
                    },
                    "comment": {
                        "value": "We thank the reviewer for his constructive comments and questions. We are pleased that he appreciates the writing of our paper and that he thinks our contributions offer promising directions for future research. Below, we respond to his two questions in two separated comments, hoping that these responses will also address the weaknesses he identifies."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1280/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235590501,
                "cdate": 1700235590501,
                "tmdate": 1700235590501,
                "mdate": 1700235590501,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ptSaaEhQrR",
                "forum": "eZneJ55mRO",
                "replyto": "EgTtBtVB80",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1280/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1280/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the question about the impact of the grammar reduction and the number of GNN layers on the expressiveness"
                    },
                    "comment": {
                        "value": "*Reviewer question : In the theoretical aspect, although the two CFGs $r-G(L3)$ and $G(L3)$ have the same expressive power, it may take more steps for $r-G(L3)$ than $G(L3)$ to generate the same string.  Therefore, how can $G^2N^2$ match the expressiveness of the ordinary 3-WL GNN with a fixed number of layers ?*\n\nThe reviewer is right : r-G(L3) and G(L3) have the same expressive power, as shown by theorem 3.2. of the paper. It is also true that r-G(L3) is not able to generate all the strings that can be generated by G(L3) since more operations are available in G(L3). So, questioning the necessity of using more GNN layers with G$^2$N$^2$ than with the GNN built from G(L3) (called E-G$^2$N$^2$ in this reply) is very relevant. \n\nWe have examined this aspect from a theoretical point of view during the rebuttal period. In this limited time, we manage to provide the following proposition showing that both models have the same separative power for a fixed number of layers $k$ such that $k \\leq 3$. \n\n**Proposition[Graph isomorphism (WL) expressiveness at fixed depth]**\n\n*Let $k \\in \\{1,2,3 \\}$, G$^2$N$^2$ and E-G$^2$N$^2$ have the same separative power after $k$ layers.*\n\n\nThis result is mentioned in Section 4, question **Q1**. The proposition and its full proof have been added in appendix A.5. The rationale behind the proof is that graph isomorphism (i.e. the WL hierarchy) measures expressiveness at graph-level, as mentioned in section 2 of our paper and in Zhang et al. (2023). As a consequence, for measuring the expressive power at a given depth, we are interested in strings that compute a scalar. \n\nGeneralizing this result to deeper models, with $k>3$, needs further investigations that will be led in our future work.   \n\nWe thank the reviewer again for this question which led us to find this new proposition which theoretically strengthens our paper since all our experimental results are obtained with $k=3$.\n\nB. Zhang, C. Fan, S. Liu, K. Huang, X. Zhao, J. Huang, and Z. Liu. The expressive power\nof graph neural networks: A survey. arXiv preprint arXiv:2308.08235, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1280/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236028065,
                "cdate": 1700236028065,
                "tmdate": 1700236028065,
                "mdate": 1700236028065,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RONMcNX3ZB",
                "forum": "eZneJ55mRO",
                "replyto": "EgTtBtVB80",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1280/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1280/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the question about the performance of G$^2$N$^2$ on ZINC datasets"
                    },
                    "comment": {
                        "value": "*Reviewer question : I would also like to see how the new GNN model performs on the ZINC-12k and ZINC-full datasets, which are widely used benchmarks for molecular property prediction.*\n\nWe have launched some experiments on these two datasets. The results will be provided and discussed in a new comment as soon as they are available."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1280/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236084176,
                "cdate": 1700236084176,
                "tmdate": 1700236084176,
                "mdate": 1700236084176,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "06XGaSiUOG",
                "forum": "eZneJ55mRO",
                "replyto": "ptSaaEhQrR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1280/Reviewer_HFhT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1280/Reviewer_HFhT"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply. I think this point is compelling. It would be nice to see this incorporated into the revision. I will raise my score to a 6 regardless of the results of the experiments."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1280/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700365992923,
                "cdate": 1700365992923,
                "tmdate": 1700365992923,
                "mdate": 1700365992923,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]