[
    {
        "title": "Fooling Contrastive Language-Image Pre-Training with CLIPMasterPrints"
    },
    {
        "review": {
            "id": "rHLCuLXagC",
            "forum": "IDDiv29paY",
            "replyto": "IDDiv29paY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2771/Reviewer_1X8T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2771/Reviewer_1X8T"
            ],
            "content": {
                "summary": {
                    "value": "This paper examines the possibility of attacking pre-trained CLIP models, i.e. generating a fooling image to maximize its embedding cosine similarity with some given prompts. Three possible methods of attacking including stochastic gradient descent, latent variable evolution, and projected gradient descent are considered. Experiments are done on imagenet showing the effectiveness of proposed methods. It is also found that fooling images trained on a small number of image captions potentially generalize to a much larger number of semantically related captions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Measuring the similarities between images and text is an important topic and it is surprising to see that with simple adversarial attacks, we are able to generate a fooling image that can simultaneously maximize its cosine similarity across multiple different prompts.\n\nThe use of latent variable evolution in the context of adversarial attack is interesting. With LVE, it does not require access to the model weights and thus overcoming the limitations of common gradient-based methods such as SGD and PGD.\n\nThe paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "I am not very convinced by the setting of the problem. In particular, I am not convinced why we need a fooling image that maximizes its possibility with many other prompts. What are the potential concerns of this vulnerability?\n\nIt seems that the methodology is not different enough with the existing literature. In particular, as admitted in the paper, none of SGD, LVE, or PGD is a technical contribution of this paper, and it seems that this paper is just evaluating those methods in the context of fooling CLIP.\n\nOnly original CLIP models are evaluated in the paper but there are many later improvement such as TCL (https://arxiv.org/pdf/2202.10401.pdf), ALBEF (https://arxiv.org/abs/2107.07651), BLIP (https://arxiv.org/abs/2201.12086), and so on. Do they have the same vulnerability? How does this vulnerability scale with the size of pre-training data (https://arxiv.org/abs/2210.08402)?"
                },
                "questions": {
                    "value": "The title is misleading. If I understand correctly, the paper does not have pre-training experiments but only carry out evaluations of pre-trained CLIP.\n\nPlease see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2771/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2771/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2771/Reviewer_1X8T"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2771/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698177539364,
            "cdate": 1698177539364,
            "tmdate": 1699636220214,
            "mdate": 1699636220214,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yClfxEkoH7",
                "forum": "IDDiv29paY",
                "replyto": "rHLCuLXagC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2771/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2771/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1X8T"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback and questions. We address them in a point-by-point manner:\n\n> I am not very convinced by the setting of the problem. In particular, I am not convinced why we need a fooling image that maximizes its possibility with many other prompts. What are the potential concerns of this vulnerability?\n\nThe reviewer makes an excellent point for classification settings: Indeed, we find that in a classification setting, where the similarity scores are passed through a softmax, the attack has the same effect as a regular adversarial attack (i.e. the adverserial class with the incidentally highest similarity score becomes the predicted output class). \n\nNevertheless, as emphasised by the authors of CLIP (Radford et al., 2021), a further highly relevant application is zero-shot image retrieval, which offers plenty of attack surface by means of CLIPMasterPrints. In more detail, inserting a single CLIPMasterprint into an existing database of images could potentially disrupt the system\u2019s functionality for a wide range of search terms, as for each targeted search term the inserted fooling master image is likely to be the top result.\n\nWhen inserting several CLIPMasterPrints into the database, even the top n results could consist entirely of these adversarial images rather than the true results. While this is also possible when inserting \u201cregular\u201d adversarial examples, the amount of examples needed for an attack using fooling master images is orders of magnitude lower than for regular adversarial examples. Practical malicious applications of this vulnerability could be\n- censorship of images related to a list of censored topics\n- adversarial product placement: targeting a variety of searched brands to advertise a different product as the top result\n- disruption of service: introducing a larger number of unrecognizable CLIPMasterPrints for a wide range of topics, resulting in unintelligible results for many queries, reducing the quality of service of an image retrieval system.\n\nMechanisms of introducing CLIPMasterprints into a database depend on the application but could be as simple as putting images online to be crawled by search engines or uploading them through webforms.  \nTo make this point more clear, we have added a Section 5 on \u201cPotential Attack Scenarios\u201d to revised manuscript.\n\n>It seems that the methodology is not different enough with the existing literature. In particular, as admitted in the paper, none of SGD, LVE, or PGD is a technical contribution of this paper, and it seems that this paper is just evaluating those methods in the context of fooling CLIP.\n\nAs we also point out in the response to Reviewer zgK9, It is important to note that LVE is a family of approaches and that our approach is not very similar to previous work, which we have also clarified again in the revised manuscript. For example, it differs in the evolved solutions, generative model, optimized loss function, and application domain.\n\n>Only original CLIP models are evaluated in the paper but there are many later improvement such as TCL (https://arxiv.org/pdf/2202.10401.pdf), ALBEF (https://arxiv.org/abs/2107.07651), BLIP (https://arxiv.org/abs/2201.12086), and so on. Do they have the same vulnerability? How does this vulnerability scale with the size of pre-training data (https://arxiv.org/abs/2210.08402)?\n\nGiven the much more common use of CLIP compared to its reiterations we find that the vulnerability of the original model to be of high significance even as newer approaches exist. Nevertheless, we agree that an evaluation on these improved models is desirable, and have therefore mined additional CLIPMasterPrints for BLIP (Li et al.,2022) and SigLIP (Zhai et al., 2023) models and find that both approaches are vulnerable to the attack as well. The corresponding results can be found in Figure 5 of the revised manuscript. Investigating how the vulnerability scales w.r.t. dataset size would be a valuable addition to the manuscript which we plan to add in the future by comparing e.g. the performance of mined CLIPMasterPrints on OpenCLIP models trained on LAION datasets of different sizes.\n\n> The title is misleading. If I understand correctly, the paper does not have pre-training experiments but only carry out evaluations of pre-trained CLIP.  \n\nWe decided to spell out CLIP in the title rather as we found \u201cFooling CLIP with CLIPMasterPrints\u201d, to be a poor choice in terms of style. But we agree with the reviewer that it could be misleading and therefore renamed the paper to \"Fooling Contrastive Language-Image Pre-trained Models with CLIPMasterPrints\" \n\nConcerning the reviewer\u2019s remark not conducting any pre-training: We propose a mitigation approach where we refine the original ViT weights in order to push the manifold of text embeddings away from mined CLIPMasterPrints. However this approach has been moved to the Appendix due to limited space."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2771/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565866621,
                "cdate": 1700565866621,
                "tmdate": 1700565951259,
                "mdate": 1700565951259,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "04YmcPhiDg",
                "forum": "IDDiv29paY",
                "replyto": "yClfxEkoH7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2771/Reviewer_1X8T"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2771/Reviewer_1X8T"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "Thank the authors for their rebuttal. Thanks for explaining the potential concerns of the vulnerability of CLIP and I also appreciate the commitment of adding more results. However, I am still not entirely convinced by the use case and novelty of CLIPMasterPrint. It seems that the novelty of the paper is a specific variant of LVE, then it seems the comparison with vanilla LVE is missing. As regards to the use case, on one hand white box attacks seem impractical as it assumes access to the weights of the retrieval model while black box attacks like LVE seem very easy to detect due to its difference from natural language. I am not convinced that CLIPMasterPrint can be a real threat."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2771/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698607866,
                "cdate": 1700698607866,
                "tmdate": 1700698607866,
                "mdate": 1700698607866,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Zw16aeToMQ",
            "forum": "IDDiv29paY",
            "replyto": "IDDiv29paY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2771/Reviewer_zgK9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2771/Reviewer_zgK9"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the adversarial attacks of CLIP models and proposes CLIPMasterPrints, a type of images that can maximize the CLIP scores with a wide range of prompts. This paper proposes three ways to mine these CLIPMasterPrints, SGD, PGD and gradient-free optimization when the model weights are inaccessible. Details experiments on image recognition tasks to show that extracted CLIPMasterPrints can fool the pretrained CLIP model on wide categories. The authors also study how to mitigate the attack risks from CLIPMasterPrints by mitigating the modality gap between text and image encoder in CLIP"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper proposes a new type of adversarial attacks for CLIP models CLIPMasterPrints. Given the fact that CLIP is the foundational vision-language models that have wide application, this topic plays an important role in mitigating the risks of misusing CLIP.\n\n2. This paper proposes several technical ways to mine the CLIPMasterPrints, from gradient based methods to non-gradient methods, which could cover diverse scenarios based on if the CLIP weights are accessible or not.  \n\n3. This paper also studies the way to reduce the risks of CLIPMasterPrints. Although the solution points to the existing finding (multimodality gap), it is still good to see the solution."
                },
                "weaknesses": {
                    "value": "1. Limited experiments: this paper also conducts experiments on ImageNet. It is interesting to see if the conclusion still holds for other recognition dataset ( note CLIP is evaluation on dozens of datasets). Moreover, this paper also uses CLIP ViT-L models. It also interesting to see the performance on CLIP ResNet models."
                },
                "questions": {
                    "value": "See Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Not applicable."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2771/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2771/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2771/Reviewer_zgK9"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2771/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698600198532,
            "cdate": 1698600198532,
            "tmdate": 1700689247610,
            "mdate": 1700689247610,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EmMMNFu36Z",
                "forum": "IDDiv29paY",
                "replyto": "Zw16aeToMQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2771/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2771/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zgK9"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insights and kind words regarding our work. Figure 4 in the revised manuscript shows the performance of PGD-mined CLIPMasterPrints for an ensemble of 64 ResNet50 models as an image encoder on Imagenet. Additionally, we ran more experiments to compare to more recent improvements over CLIP, such as BLIP (Li et al.,2022) and SigLIP (Zhai et al., 2023). Our results show that the model using a ResNet ensemble is somewhat less vulnerable to CLIPMasterPrints, yet the obtained scores are still on par with the baseline. Furthermore, BLIP and SigLIP seem to be vulnerable to CLIPMasterPrints as well. We agree that results on different datasets would add to the overall picture and plan to add these (e.g. Flickr30k) in the future."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2771/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700564276584,
                "cdate": 1700564276584,
                "tmdate": 1700564276584,
                "mdate": 1700564276584,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QkeefbErtP",
                "forum": "IDDiv29paY",
                "replyto": "EmMMNFu36Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2771/Reviewer_zgK9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2771/Reviewer_zgK9"
                ],
                "content": {
                    "title": {
                        "value": "Addressed all my questions"
                    },
                    "comment": {
                        "value": "Hi, thanks for your rebuttal. You've addressed all my questions, I'll keep the score as 6 but raise my confidence to 5."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2771/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689223730,
                "cdate": 1700689223730,
                "tmdate": 1700689223730,
                "mdate": 1700689223730,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uGOlB8OAIg",
            "forum": "IDDiv29paY",
            "replyto": "IDDiv29paY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2771/Reviewer_W2Qv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2771/Reviewer_W2Qv"
            ],
            "content": {
                "summary": {
                    "value": "The paper focus on mining images referred as \u201cfooling master images\u201d or \u201cCLIPMasterPrints\u201d that fool the CLIP model by obtaining higher image-text similarity scores compared to clean image-text scores. These CLIPMasterPrints are optimized to obtain hight similarity scores across different text embeddings. Authors show that these CLIPMasterPrints also generalize to semantically related text prompts that are not directly considered in the optimization process. They attribute the rationale for existence of such CLIPMasterPrints  to the not well aligned CLIP image-text embeddings. They countermeasure CLIPMasterPrints  by adjusting the CLIP alignment i.e. by shifting the centroids of image and text embeddings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-\tSimple approach to successfully fool against different text embeddings.\n-\tPaper is easy to read and understand.\n-\tIllustrations are clear."
                },
                "weaknesses": {
                    "value": "Prior works have already shown that CLIP is vulnerable to adversarial attacks (Noever & Miller Noever, 2021; Daras &Dimakis, 2022; Goh et al., 2021). This work on fooling master images is a variant of such adversarial attacks utilizing already existing optimization algorithms like SGD, PGD and, LVE (Latent Variable Evolution) to craft an image with perturbations. The difference here is that adversarial objective aims to fool different text embeddings. Furthermore, countermeasuring CLIPMasterPrints is performed by shifting the centroids of image and text embeddings that is proposed in prior work Liang et al. (2022) (cited in the paper). Therefore, this limits the originality, novelty and technical contributions of this work."
                },
                "questions": {
                    "value": "Despite the concept of CLIPMasterPrints for CLIP is interesting, the paper does not meet the criteria for conference acceptance. I suggest this paper to be a fit for workshop."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2771/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698872156853,
            "cdate": 1698872156853,
            "tmdate": 1699636220065,
            "mdate": 1699636220065,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]