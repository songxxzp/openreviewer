[
    {
        "title": "Adversarial Adaptive Sampling: Unify PINN and Optimal Transport for the Approximation of PDEs"
    },
    {
        "review": {
            "id": "hLriYdiTgt",
            "forum": "7QI7tVrh2c",
            "replyto": "7QI7tVrh2c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5132/Reviewer_CyTK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5132/Reviewer_CyTK"
            ],
            "content": {
                "summary": {
                    "value": "In this article, authors raise a significant problem that linked with numerical difficulty is that random samples in the training dataset introduce statistical errors into the functional loss which may become the dominant error in the approximation, and therefore overshadow the modeling capability of the neural network. A new approach was proposed to optimize both the approximate solution and random samples in the training set by using a min max formulation. This approach is called adversarial adaptive sampling (AAS). The main idea of AAS is minimizing the residuals and meanwhile push the residual-induced distribution to a uniform one.  AAS can be divided into two parts. In the maximization part, the deep generative model helps define the difference between the residual-induced distribution and a uniform one using Wasserstein distance. In the minimization one, this difference is minimized together with the residuals.   Also, they used some benchmark test problems of comparison AAS algorithm with another state-of-the-art algorithms as DAS and RAR."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The motivation is clear, and the method is novel and interesting. Authors clearly described the theory of proposed method and the algorithm."
                },
                "weaknesses": {
                    "value": "A numerical result is given for one type of PDEs. It would be interesting to see how AAS algorithm performs for other types of PDEs. There is no discussion of further development or limitations of the proposed method."
                },
                "questions": {
                    "value": "What other types of PDEs can the proposed algorithm be applied to? \nWhat are restrictions of the proposed method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5132/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5132/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5132/Reviewer_CyTK"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5132/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698748160292,
            "cdate": 1698748160292,
            "tmdate": 1699636506239,
            "mdate": 1699636506239,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6MnaKxTXt3",
                "forum": "7QI7tVrh2c",
                "replyto": "hLriYdiTgt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5132/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5132/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CyTK"
                    },
                    "comment": {
                        "value": "1. A numerical result is given for one type of PDEs. It would be interesting to see how AAS algorithm performs for other types of PDEs. There is no discussion of further development or limitations of the proposed method. \n\nWe agree with this reviewer that the performance of AAS needs to be further investigated. The next step is to make AAS more robust by improving the training strategy, and more problems will be considered such as singularly perturbed equations, parametric equations, etc.  We have added the numerical results of parametric Burgers' equation, see Appendix A.5.\n\n2. What other types of PDEs can the proposed algorithm be applied to? What are restrictions of the proposed method?\n\nAAS is formulated in the framework of the least-squares method. For a certain type of PDE, if the least-squares method is effective, AAS should help to reduce the statistical error. The main restriction comes from the regularization term since it is quite difficult to control the constraint on the Lipschitz norm. More attention needs to be paid to this issue."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5132/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700227329835,
                "cdate": 1700227329835,
                "tmdate": 1700227329835,
                "mdate": 1700227329835,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4jHpgpHaUs",
            "forum": "7QI7tVrh2c",
            "replyto": "7QI7tVrh2c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5132/Reviewer_3BKL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5132/Reviewer_3BKL"
            ],
            "content": {
                "summary": {
                    "value": "A uniform approximation error is crucial in representing functions in a given vector base.  The same applies to compound function representations like neural networks. It also crucial where the errors are measured. The Authors combine both aspects, a  Wasserstein distance between a uniform and error distributions,  and an adversarial adaptive sampling of points for the PINN collocation.\n\nThe loss is divided to an analytical part that reflects the difference of the neural network from the solution and a statistical part that comes from using a finite sample.  By tuning the residual to become uniform, the density of samples becomes optimally non-uniform.\n\nThe method is used for three different PDEs with encouraging results compared to other adaptive learning techniques."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "An excellent way to bring confidence to the inference with good proofs of convergence for the adaptive learning distributions.\n\nPresentation is clear and not hard to follow."
                },
                "weaknesses": {
                    "value": "Looking at the point clouds one visually can guess that the Delaunay triangulation link length distribution of  the point cloud contains very short distances i.e the point cloud is not locally smooth. Usually building meshes for FEM one prefers same size of elements locally and smaller elements in those areas do not provide more accuracy. \n\nI suppose this is the same for collocation points in PINNs. Currently the point clouds do not look like node distributions for FEM calculus. This is increasingly so, when creating points in the higher dimensions. The amount of wasted calculations increases and may drive this to a curse of high dimensionality."
                },
                "questions": {
                    "value": "Any methods to solve the above problem? A possible remedy could introduce quasi random point distributions where the points do follow the error distribution, but are smoothly enough distributed in oder to avoid computation of the the error on points that are close to each other already without an expectation of significant change in the error distribution.\n\nCould the problem above be solved with couple of iteration steps of repulsive force in the point cloud?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5132/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767300102,
            "cdate": 1698767300102,
            "tmdate": 1699636506143,
            "mdate": 1699636506143,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QaMPwUICER",
                "forum": "7QI7tVrh2c",
                "replyto": "4jHpgpHaUs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5132/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5132/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3BKL"
                    },
                    "comment": {
                        "value": "1. Looking at the point clouds one visually can guess that the Delaunay triangulation link length distribution of the point cloud contains very short distances i.e the point cloud is not locally smooth. Usually building meshes for FEM one prefers same size of elements locally and smaller elements in those areas do not provide more accuracy.\nI suppose this is the same for collocation points in PINNs. Currently the point clouds do not look like node distributions for FEM calculus. This is increasingly so, when creating points in the higher dimensions. The amount of wasted calculations increases and may drive this to a curse of high dimensionality.\n\nCurrently the collocation points rely on the best possible PDF that helps smooth the residual profile such that the adaptive sampling can be understood from the viewpoint of variance reduction. We do not have any direct  specific control over the skewness if we relate the points to a mesh. The collocation points are random rather than deterministic. The idea of Delaunay triangulation makes more sense in low-dimensional spaces since points become sparse in high-dimensional spaces and the geometry is also fundamentally different.\n\n2. Any methods to solve the above problem? A possible remedy could introduce quasi random point distributions where the points do follow the error distribution, but are smoothly enough distributed in oder to avoid computation of the the error on points that are close to each other already without an expectation of significant change in the error distribution.\nCould the problem above be solved with couple of iteration steps of repulsive force in the point cloud?\n\nWe agree that quasi-random points can be better if we are able to generalize the intuition of Delaunay triangular to high-dimensional spaces. The idea of repulsive force sounds very interesting, especially for low-dimensional spaces. We need to figure out how the repulsive force affects the variance reduction. Furthermore, the repulsive force will introduce a correlation between random samples, which makes the sample generation more challenging."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5132/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700227070770,
                "cdate": 1700227070770,
                "tmdate": 1700227070770,
                "mdate": 1700227070770,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yqiDC5nJBW",
                "forum": "7QI7tVrh2c",
                "replyto": "QaMPwUICER",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5132/Reviewer_3BKL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5132/Reviewer_3BKL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank for your clarifications. My remains  that this is an excellent paper. Having similar ideas around is understandable, but the point is that one has found a procedure that is clear and works in practice.\n\nOn the skewness of the \"effective\" mesh in PINNS, I would argue that the problem in FEM comes from discretisation of the  derivatives and does not have a counterpart in PINNS. In PINNS the points that are \"too\" close provide less information (i.e. in the neighbourhood where the Jacobian describes the behaviour of the solution as this information is already provided by a single point).  Then increasing the weight of the loss term would have the same effect than adding points nearby.\n\nBTW. Similar considerations could be used to prune out the points too close from a generated i.i.d  random sample."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5132/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646055670,
                "cdate": 1700646055670,
                "tmdate": 1700646055670,
                "mdate": 1700646055670,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NWYbyiVild",
            "forum": "7QI7tVrh2c",
            "replyto": "7QI7tVrh2c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5132/Reviewer_ZtMG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5132/Reviewer_ZtMG"
            ],
            "content": {
                "summary": {
                    "value": "In this paper the authors introduce an adaptive sampling technique that for sampling the input points (also referred to as collocation points) for training a PINN architecture on a domain. The main motivation behind it is that for highly irregular PDEs, uniformly sampling collocation points will not work well and result in high residual error for PINNs. \n\nThe idea is similar to the adaptive sampling techniques that have been introduced in previous work by Tang et al (2023) and Gao et al (2023), however instead of directly minimizing a pushforward or adaptive sampling, the authors in this paper enforce a wasserstein loss to ensure that the residual error is uniformly distributed across the grid. \n\nThat is, the training is done such that the error induced by PINNs is uniformly distributed across the grid, and hence the points are sampled accordingly. The authors operationalize this using a WGAN setup, wherein they train a pushforward map to learn p(x), such the wasserstein-1 loss between the the density of the residual and uniform distribution. This is similar in vein of the adaptive sampling techniques introduced in Gao et al (2023) but the weights are instead learned using a GAN type architecture."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea to use a GAN type loss with to ensure that the density of the residual is uniform across the domain is very interesting. \n\nIn their experiments, the authors are able to do well on very sparse data, i.e., PDEs that have two-peaks, something that PINNs sometimes don\u2019t do well in, and are able to get better results than the previous baselines by Tang et al (2023), thus showing the benefits of using the wasserstein formulation for learning $p_\\alpha$."
                },
                "weaknesses": {
                    "value": "In general the work is very similar to that by Tang et al (2023), there the authors are using a network defined using a normalizing flows type of an architecture whereas here the authors are using a GAN instead. \n\nThe paper is very hard to read with sometimes the notation and the terms used by the authors for different quantities is not clear. For example\n\n- The authors mention that this methodology achieves variance reduction, what is the proof for that? is it similar in flavor of Tang, et al (2023)\n- In Assumption 1, the authors refer to $r$ as an operator. Is that an operator, or the residual between the loss. If the authors mean that $r$ is an operator, then from which function space to what other function space? (I presume it is U x F \u2192 U).\n- In theorem 1, $\\nu$ is used to define the density of the residuals, whereas in the derivation under equation 6, it is defined by $\\mu_r$. I think that they are the same quantity, however if not, what is $\\nu$ and what is $\\mu_r$?\n- It is unclear as to what is being approximated by a KRNet?, since the authors first use $p_\\alpha$ to define the density that they are trying to train, however, there is no mention of it after section 3.2\n- Also, KRNet is not cited in the last paragraph of page 3.\n- In equation 9, what is p*?\n\nFew other questions that I think would help with the understanding of the usability of the techniques are: \n\n- What are the implementation details (in terms of number of parameters) etc of the networks approximating the solution $u_\\theta$ and approximating the density network, i.,e $p_\\alpha$.\n- Since the authors are doing a min-max optimization, that is usually hard to train and get right, esp for for PDEs that may have some advective terms.\n\nWhile the methodology provided by the authors seems to do better than the baselines, given the presentation and the lack of clarity of the precise steps, I think the paper would benefit from a revision"
                },
                "questions": {
                    "value": "I have asked most of the questions in the previous sections."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5132/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5132/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5132/Reviewer_ZtMG"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5132/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792169859,
            "cdate": 1698792169859,
            "tmdate": 1700682736218,
            "mdate": 1700682736218,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Gi9JnUek9I",
                "forum": "7QI7tVrh2c",
                "replyto": "NWYbyiVild",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5132/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5132/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZtMG"
                    },
                    "comment": {
                        "value": "1. In general the work is very similar to that by Tang et al (2023), there the authors are using a network defined using a normalizing flows type of an architecture whereas here the authors are using a GAN instead.\n\nFirst, we do not use a GAN in this work. The formulation of AAS is a min-max problem, but not every min-max problem is GAN. This work is similar to the work DAS-PINNs in Tang et al (2023) only in the sense that both consider to use adaptive sampled collocation points to construct a residual-based PDE solver. The adaptive sampling framework of DAS consists of two parts: solving PDEs and refining the collocation points. The two parts in DAS-PINNs are trained separately and not integrated into one objective function, which shares more similarities to adaptive finite element methods. \n\nIn this work, we directly put the sampling distribution into the loss functional. Adversarially, if this sampling distribution is the residual distribution, the residual itself will be uniformly distributed, which means the error is evenly spread and thus achieves a variance reduction with respect to the uniform distribution. One big advantage of this work versus DAS is that the training of PDE (residual) and sampling density are unified into one minimax procedure. In the training procedure, we don't have to sample from residual to train the KRnet approximation of sampling density, which could generate additional computational error particularly for high-dimensional problems. \n\nIn addition, we also explain the work Gao et al (2023) mentioned by the reviewer. That work uses the same loss and strategy as in Tang et al (2023). The only difference is they use a MCMC technique, instead of a normalizing flow model of KRnet.\n\n2. The authors mention that this methodology achieves variance reduction, what is the proof for that? is it similar in flavor of Tang, et al (2023)\n\nIn AAS, in the training procedure, the residual will approach the uniform distribution, with the balance from the growth of the adversarial sampling density function $p$. This will guarantee a variance reduction of residual, because if the residual is uniformly distributed, it is constant in the domain, and a constant function has $0$ variance with respect to any probability measure. This is why we didn't explain it in the paper. \n\n3. In Assumption 1, the authors refer to $r$ as an operator. Is that an operator, or the residual between the loss. If the authors mean that $r$ is an operator, then from which function space to what other function space? (I presume it is $U \\times F \\rightarrow U$).\n\nNote that $r$ is the residual of PDEs. Given the partial differential operator $\\mathcal{L}$, and the right-hand term $s$, for any function $u \\in C^{\\infty}(\\Omega)$, then the residual defines an operator, which is interpreted as\n    $r(u) = \\mathcal{L}u - s$.\n\n4. In theorem 1, $\\mu$ is used to define the density of the residuals, whereas in the derivation under equation 6, it is defined by $\\mu_r$. I think that they are the same quantity, however if not, what is $\\mu$ and what is $\\mu_r$?\n\nIn theorem 1, $\\mu$ represents the uniform distribution. $\\nu$ represents the residual-induced distribution. To avoid confusion, we have revised the manuscript, where $\\nu$ is replaced by $\\mu_r$.\n\n5. It is unclear as to what is being approximated by a KRNet?, since the authors first use $p_\\alpha$ to define the density that they are trying to train, however, there is no mention of it after section 3.2\n\nHere, $p_{\\boldsymbol{\\alpha}}$ is a PDF modeled by KRnet. Solving PDEs with neural networks needs to generate collocation points in the computational domain for training. These collocation points are drawn from $p_{\\boldsymbol{\\alpha}}$ defined by KRnet\u2014different sets of collocation points correspond to different KRnets. So, the maximization step in equation 8 is the training procedure for the KRnet modeled pdf, see section 3.3.\n\n6. Also, KRnet is not cited in the last paragraph of page 3.\n\nWe here use a bounded KRnet, and the citation is on page 4: ``The bounded KRnet can be achieved by adding a logistic transformation layer (Tang et al., 2023) or a new coupling layer proposed in (Zeng et al., 2023)\". We have also added a reference on page 3 in the revised manuscript.\n\n7. In equation 9, what is $p^*$?\n\nFor a fixed $u_{\\boldsymbol{\\theta}}$, equation (9), the Euler\u2013Lagrange equation for the functional in equation (8), gives the necessary condition for the existence of optimal $p*$ of the maximization step in equation (8), which is a standard technique called the calculus of variations in analysis. We write this equation to illustrate that the maximization step is well-posed after adding a gradient penalty term (see equation 8), which is a weakened condition for the Lipschitz norm condition. The regularization of it is then guaranteed, for otherwise, the maximization step will yield a delta measure (see section 3.1)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5132/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700226728713,
                "cdate": 1700226728713,
                "tmdate": 1700724291530,
                "mdate": 1700724291530,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dt6P7J0Zk5",
                "forum": "7QI7tVrh2c",
                "replyto": "CY2xhqWWWc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5132/Reviewer_ZtMG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5132/Reviewer_ZtMG"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "The authors have answered a lot of my questions well in their response. I would recommend adding the appropriate clarifications in the final version of the paper. Thanks!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5132/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682709844,
                "cdate": 1700682709844,
                "tmdate": 1700682709844,
                "mdate": 1700682709844,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Rek3puFWf2",
            "forum": "7QI7tVrh2c",
            "replyto": "7QI7tVrh2c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5132/Reviewer_YsSo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5132/Reviewer_YsSo"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new objective to sample collocation points for PINNs adaptively. Specifically, the collocation points for training the PINN are sampled from a normalizing flow with soft Lipschitz constraint, enforced by Sobolev regularization. The PINNs and normalizing flows are optimized in an alternating fashion, where the normalizing flow is trained to maximize the expected PINN residual (w.r.t. to the distribution given by the pushforward of the normalizing flow). This formulation is then connected to the dual form of an optimal transport problem. In this context, it is shown that there exists an optimal solution to the proposed min-max problem where the (normalized) squared residual converges to a uniform distribution in the Wasserstein distance. The approach's effectiveness is further demonstrated on two toy problems and one high-dimensional nonlinear PDE and compared to three related methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) Tackling adaptive sampling for PINNs using optimal transport seems to be a promising direction.\n2) The proposed objective seems to be novel and can bring numerical benefits on the considered examples."
                },
                "weaknesses": {
                    "value": "The theoretical as well as numerical contributions need to be significantly improved:\n\n1) Further challenging problems (as in Subsection 5.3), as well as baselines (as enumerated in the section on related works), are needed to judge the performance of the proposed algorithm. \n2) \"[...] which is the first time to minimize the residual and seek the optimal training set simultaneously for PINN.\": Analogous to DAS-PINNs, it seems that the final algorithm is still alternating between optimizing the two networks, see also Question 2) below. In general, it should be made more explicit what the novelty of the present work is and how it theoretically compares to related work.\n3) For the numerical results, training times and standard deviations (w.r.t. different seeds) are missing. Especially given that the adversarial training slows down training.\n4) The loss for learning the normalizing flow seems to use the REINFORCE trick, which, however, is known to suffer from high variance.\n5) Since the residual continuously changes over the course of optimization, it should be better motivated what the advantage of learning a generative modeling is (as compared to just using a method to sample, e.g., according to squared residual)?\n6) It seems that the precise connection to optimal transport remains a bit unclear:\n\t* \"So the minimization step will reduce not only the residual but also the Wasserstein distance between $\\mu_r$ and the uniform distribution\". Since there is only an upper bound shown on page 6, it is unclear why the minimization step is guaranteed to decrease the Wasserstein distance.\n\t* \"The evolution of the residual-induced distribution has a clear path\": According to the theorem, there only *exists* such a path, and it is not clear whether this path is taken by GD. It would be good to have at least plots of the residual evolution for all experiments.\n\n**Minor issues:** \n1) use `\\citep`.\n2) In Eq. (4), min-max should be written on both sides of the equation.\n3) What is a 'proper' constraint?\n4) The last paragraph in Section 3 seems a bit convoluted. The choice for minimal variance could just be motivated by the optimal choice for importance sampling. \n5) It is unclear why we would consider $I^D$ instead of $\\Omega$ when the KRnet is first introduced.\n6) The text for the figures of the training set is barely readable.\n7) typo: resultd"
                },
                "questions": {
                    "value": "1) Why is RAR not tested for the PDEs in (12) and (13)?\n2) Why can we not use a single backward pass on the loss in (11) and update both $\\theta$ and $\\alpha$ (using decent and ascent, respectively)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5132/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5132/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5132/Reviewer_YsSo"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5132/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805379513,
            "cdate": 1698805379513,
            "tmdate": 1700720358723,
            "mdate": 1700720358723,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yJpzoDZ1TN",
                "forum": "7QI7tVrh2c",
                "replyto": "Rek3puFWf2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5132/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5132/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YsSo"
                    },
                    "comment": {
                        "value": "1. Further challenging problems (as in Subsection 5.3), ... are needed to judge the performance of the proposed algorithm. \n\nWe have added the additional numerical experiments (parametric Burgers' equations) to Appendix A.5 when we submitted the original manuscript. We do not claim that the AAS method can beat classical methods. We use the benchmark problems to illustrate one thing: one should pay attention to the sampling method when training PINN, especially for low-regularity or high-dimensional problems. However, we do not claim that adaptive sampling can resolve every issue. Adaptive sampling for PINN is a new direction, where DAS-PINNs is a recent work, and we have already used DAS-G and DAS-R as baselines. It is not a computer vision or NLP task, which requires a lot of old baselines.\n\n2. Analogous to DAS-PINNs, it seems that the final algorithm is still alternating between optimizing the two networks, see also Question 2) below. In general, it should be made more explicit what the novelty of the present work ...\n\nThe current algorithm allows the two models (PINN and KRnet) to evolve at the same time. In the work of DAS-PINNs, the style of adaptive sampling is like that of the classical methods such as adaptive finite element methods. That is, the procedure of adaptive sampling in DAS-PINNs is like $\\mathsf{solve} \\rightarrow \\mathsf{refine} \\rightarrow \\mathsf{solve} \\rightarrow \\ldots$. Simply speaking, given a training set, we need to train PINN with a large number of epochs to ensure that we have ``solved\" the PDE, and then we use the KRnet-induced distribution to approximate the residual-induced distribution by minimizing the KL divergence between them. However, in this work, we do not require the minimization step to have a large number of epochs. Moreover, the min-max formulation in AAS implicitly pushes the residual-induced distribution to a uniform one without any KL divergence-like formulation, while there is an explicit minimization step about KL divergence in DAS-PINNs. In summary, AAS's formulation and training style are different from DAS-PINNs. \n\n3. For the numerical results, training times and standard deviations (w.r.t. different seeds) are missing. Especially ...\n\nThanks for the suggestion. Based on our available computational resources, we have taken three runs with different random seeds for the proposed AAS method and computed the mean error and the standard deviation (std) of the three runs. Also, we record the mean training time of the three runs for the three test problems presented in the manuscript. The results are as follows.\n|   | Mean error with std   | Training time |\n|  ----  | ----  | ----  | \n| Equation 12  | 3.1754e-05 $\\\\pm$ 1.1272e-05 |   4.82 hours |\n| Equation 13  |  6.4194e-04 $\\\\pm$ 4.6891e-04 |  5.41 hours |\n| Equation 14  |  0.0024 $\\\\pm$ 0.0015 |  15.74 hours |\n\n4. The loss for learning the normalizing flow seems to use the REINFORCE trick, ... suffer from high variance. \n\nWe are not quite sure about what the reviewer means by ``use the REINFORCE trick\". The normalizing flow model generates samples for training PINN. Still, we do not require the flow model to have a high-accuracy performance since our goal is to solve PDEs instead of density estimation. It is enough if the normalizing flow model can capture where the low regularity of the solution is. In addition, other deep generative models with an explicit PDF can be applied here.\n\n5. Since the residual continuously changes over the course of optimization, it should be better motivated what the advantage of learning a generative modeling is (as compared to just using a method to sample, e.g., according to squared residual)?\n\nSampling data points according to the residual is suitable for low-dimensional problems. For high-dimensional problems, using sampling methods such as MCMC is not efficient, and this is why deep generative models (such as GAN, VAE, Flow, and diffusion models) are proposed.\n\n6. Since there is only an upper bound shown on page 6, it is unclear why the minimization step is guaranteed to decrease the Wasserstein distance. \n\nThis is just an illustrative analysis to show how the idea works. For more details, see Appendix A.1-A.4. We have provided the detailed analysis about this question."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5132/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225913277,
                "cdate": 1700225913277,
                "tmdate": 1700225913277,
                "mdate": 1700225913277,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PwqWYhVfkx",
                "forum": "7QI7tVrh2c",
                "replyto": "MPYE21cJPc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5132/Reviewer_YsSo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5132/Reviewer_YsSo"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response, the clarifications, and the additional experiments. I raised my score accordingly; however, I believe the empirical validation and presentation still need to be improved. Let me specifically elaborate on the empirical validation:\n\n*Empirical evidence:* Thank you for providing runtimes for the experiments; as hinted at in my review, it would be good to compare to the runtimes of the baselines. Further experiments could have validated the statements in the rebuttal as well:\n \n1. \"For high-dimensional problems, using sampling methods such as MCMC is not efficient, and this is why deep generative models (such as GAN, VAE, Flow, and diffusion models) are proposed.\": I agree that DL models provide promising alternatives; however, in most applied fields, MCMC methods (and extensions such as SMC, AIS, ...) are still the gold standard, even for high-dimensional problems.\n\n2. \"However, for high-dimensional problems, generating the candidate points by the uniform distribution is not efficient, sometimes failed, but AAS and DAS can efficiently generate the data points to refine the collocation points thanks to the deep generative model.\"\n\nIn both cases, the argument evolves around high-dimensional problems, but there seems to be only a single high-dim. (10d) problem in the current set of experiments. Given the claim that these baselines perform worse, including them as (weak) baselines would have been easy. I do not think the statement \"It is not a computer vision or NLP task, which requires a lot of old baselines.\" justifies not presenting further experiments and baselines. Based on the lack of ablation studies or further high-dimensional problems, it is hard to judge the contribution of the new method."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5132/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720326397,
                "cdate": 1700720326397,
                "tmdate": 1700720326397,
                "mdate": 1700720326397,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "de9zI8Of6q",
                "forum": "7QI7tVrh2c",
                "replyto": "Rek3puFWf2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5132/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5132/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply. We agree with this reviewer that MCMC methods (extensions such as SMC, AIS, etc.) are still the gold standard. However, our goal here is to solve PDEs instead of sampling. So, we need to generate samples quickly during the adaptive sampling procedure."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5132/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723303762,
                "cdate": 1700723303762,
                "tmdate": 1700723894564,
                "mdate": 1700723894564,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]