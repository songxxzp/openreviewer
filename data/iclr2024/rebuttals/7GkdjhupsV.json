[
    {
        "title": "InfoAug: Mutual Information Informed Augmentation for Representation Learning"
    },
    {
        "review": {
            "id": "a7VttbC4kg",
            "forum": "7GkdjhupsV",
            "replyto": "7GkdjhupsV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4826/Reviewer_A3xm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4826/Reviewer_A3xm"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a new method for augmenting the pool of positive samples in self-supervised learning (SSL). In particular, the authors make use of videos, that if two patches p1, p2 in the first frame of a video consistently appear together in later frames, they are considered to be a pair of positive samples. The consistency between p1, p2 is measured by the mutual information between their locations in different frame, which itself can be estimated accurately using classic methods. The newly generated positive samples will eventually be used in addition to conventional positive samples in SSL learning. Different projection heads are used for the old and the new positive samples respectively. Experiments on 3 datasets show that the proposed method improves the performance of 7 SSL methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is overall well written and easy to follow;\n- The method to generate additional positive samples from videos is novel. The idea to treat the whole video as the empirical population of image patches (and to further extract statistical dependence from this population) is also quite interesting;\n- The empirical evaluation covers most mainstream SSL methods (up to seven) and three widely-used datasets (CIFAR-10, CIFAR-100, STL-100). Ablation studies regarding the effect of the newly introduced video-based positive samples, the number of training samples and the number of training epochs are conducted and reported carefully;"
                },
                "weaknesses": {
                    "value": "- **On twin pairs of video patches** There is, in my opinion, some room for simplification/improvement in the proposed approach. According to the proposed method, patches that are most statistically dependent will be considered to be a pair. Then why not simply take adjacent patches to form a pair, as adjacent patches are more likely to be dependent. Also adjacent patches are visually more consistent/similar. This is conceptually much simpler than your MI-based approach. In this regards, I am also curious about what are the twin patches found in the method. Are they indeed adjacent patches? If not so, an example or a remark will be very helpful and insightful;\n\n- **Applicability of the method**. While the proposed method consistently brings improvement in diverse SSL methods, it relies on high-quality video datasets to work well. As the authors themselves mention, suitable video dataset may not always be available, and in fact this is the reason behind not testing on larger dataset widely used in other SSL literature e.g. ImageNet. This to some extent makes the method not so general as compared to other methods, despite its novel idea to take additional samples from multiple dataset.\n\n- **Complexity**. Due to the use of additional dataset and video processing model (e.g. TAPIR), the overall complexity of the method is also high compared to some other well-known baselines (e.g. SimCLR, Barlow Twins), which is conceptually simpler. It also gives me a sense that the supreme performance of the method is due to the use of external dataset rather than algorithmic/theoretic innovation."
                },
                "questions": {
                    "value": "- See the section `On twin pairs of video patches\u2019 above.\n- How do one typically determine the size of the patches in the video framing processing stage? Is it the same as in the main dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4826/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4826/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4826/Reviewer_A3xm"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4826/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698251697643,
            "cdate": 1698251697643,
            "tmdate": 1699636466224,
            "mdate": 1699636466224,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mfRSRnLtaU",
                "forum": "7GkdjhupsV",
                "replyto": "a7VttbC4kg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4826/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer A3xm"
                    },
                    "comment": {
                        "value": "First of all, thank you very much for your acknowledgement of the article, including the novelty of our ideas and the validity of the method. We also thank you for your time in reviewing the article. In response to your questions, we provide the following answers.\n\n\n1. First, for the first question, using only neighboring patches as positive samples, although simple in idea, is not rigorous in principle, and will greatly reduce the performance of the model. For example, imagine a dog standing on the edge of a flowerpot, due to their physical proximity, using neighboring patches as positive samples would force the model to push the representation of the dog and the flowerpot closer to each other, however in reality they are not objects of the same class, thus posing significant damage to the model.\n\n2. For the second question, your suggestion is exactly one of our considerations in this article, we confirmed the effectiveness of InfoAug with great certainty through a large number of comparative and ablation experiments, and we would prefer that subsequent work continues to explore how to manipulate the large in-the-wild dataset to achieve an organic fusion with InfoAug, and to gain the ability to learn from the video dataset with a large amount of unsupervised learning capability from video datasets.\n\n3. For the third problem, first, we give the following specific data as a reference, for a video data of 80 frames, we only need one second to complete the tracking of nearly 100 points on the whole video. For mutual information estimation, even with the most common algorithm, we only need 0.0025 seconds to complete the mutual information estimation for a pair of random variables with nearly 50 observations. Further, in order to demonstrate the effectiveness of InfoAug, we conducted comparison experiments using randomly selected positive samples, and found that merely increasing the number of iterations (amount of data) does not lead to effective performance improvement, thus illustrating the absolute superiority of mutual information as a selection metric.\n\n4. Finally, on the question of how to determine the patch size, we are not that clear on what you mean by \"main dataset\". In any case, patch size is an hyperparameter that should be determined by roughly estimating the size of individual objects contained in the video dataset. Although we found in our experiments that the choice of this hyperparameter does not bring much effect variation, it is still a case-by-case (dataset) choice, and the ultimate goal is to expect the patch size to match the majority of the objects in the scene to get the best possible cover effect.\n\n\nFinally, thank you for your questions, especially the first one which is intriguing. We hope our answers have solved your confusion."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593741380,
                "cdate": 1700593741380,
                "tmdate": 1700593741380,
                "mdate": 1700593741380,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ic107nE7lO",
                "forum": "7GkdjhupsV",
                "replyto": "mfRSRnLtaU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4826/Reviewer_A3xm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4826/Reviewer_A3xm"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "Many thanks for your detailed rebuttal, which addresses most of my confusion. I'll keep my already positive score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689514001,
                "cdate": 1700689514001,
                "tmdate": 1700689514001,
                "mdate": 1700689514001,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wwIPqHgwPx",
            "forum": "7GkdjhupsV",
            "replyto": "7GkdjhupsV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4826/Reviewer_xZ59"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4826/Reviewer_xZ59"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces InfoAug, a novel data augmentation technique that identifies positive pairs of patches in a video frame by estimating their mutual information using off-the-shelf tracking models. A dual branch is utilized to handle the mutual-information-guided positive pairs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The concept of identifying positive patches is interesting.\n- Using estimated mutual information to find positive patches seems reasonable.\n- Experiments show the effectiveness of the positive patches selection and the dual branch."
                },
                "weaknesses": {
                    "value": "- The experiments are primarily conducted on small datasets and employ small backbones like ResNet-18, which may limit the generalizability of the results.\n- Fairness of Comparisons: The comparison with other methods could be improved. Specifically, the use of a 3-layer head instead of the standard single-layer might lead to skewed results. Additionally, the direct comparison of InfoAug with other methods is not fair due to the employment of 2 branches in InfoAug. The improvements over the \"random twin patch\" are marginal and sometimes worse, as seen in Tables 1 and 2.\n- Training Epochs: The models are trained for only 100 epochs, which might not be sufficient for convergence. When more epochs are used, as shown in Table 5, the improvements seem to diminish. Again, it is not fair to compare them directly."
                },
                "questions": {
                    "value": "- Patch Extraction: Could the authors clarify how patches are extracted? Are they overlapping or non-overlapping? Are they selected from specific regions or chosen randomly?\n- Application to Image Datasets: How does InfoAug select positive pairs from video datasets and apply this knowledge to image datasets? Is the model first pre-trained on video datasets and then fine-tuned on image datasets, or are twin patches retrieved from video datasets during training on image datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4826/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4826/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4826/Reviewer_xZ59"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4826/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804726659,
            "cdate": 1698804726659,
            "tmdate": 1699636466131,
            "mdate": 1699636466131,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Cw7wpg2UoT",
                "forum": "7GkdjhupsV",
                "replyto": "wwIPqHgwPx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4826/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer xZ59"
                    },
                    "comment": {
                        "value": "First of all, we sincerely appreciate the valuable time of the reviewer and thank you for your questions and corrections. We also appreciate your recognition of the novelty and the acknowledgment of the use of mutual information and dual branch formulation in our article.\n\nNow, I will respond to your corrections and questions in a sequential order:\n\n1. Regarding the first concern, we appreciate that you noticed our argument about the size of the pretraining dataset. Since the existing InfoAug is well-suited for learning from small to medium-sized datasets by imitating the learning process of humans through mutual information estimation, we considered that using a larger model such as ResNet-50 for training would lead to overfitting, as ResNet-18 is already a large model for the existing dataset scale. This way, we could not effectively showcase the performance of InfoAug itself.\n\n2. For the second concern, we agree with your suggestion, and thus, we examined prominent state-of-the-art models such as SimCLR, BYOL, MoCoV2, which indeed employ a two-layer fully connected head as the projection head. Consequently, we conducted experiments based on this suggestion, and the results are put into Appendix A.3. The results still, under standard 2-layer projection head, show a consistent improvement over all the SOTA models. As for your concern about \"random-twin-patch,\" I believe there might be a misunderstanding regarding the purpose of that experiment. We intentionally used randomly selected patches to illustrate the importance of mutual information as the metric for selection. Randomly selecting patches as positive samples without considering mutual information would not yield good results. This experiment was intentionally done to provide a comprehensive demonstration of the significance of mutual information in InfoAug, as a comparative study to only adopt random selection.\n\n3. Regarding the third concern, we chose 100 epochs as the main experiment, as we found that it almost reached convergence within that timeframe. The reason behind this is that we used patches as training units instead of whole images, which increased the number of iterations per epoch by a factor of N (where N is the number of patches in an image).\n\n\nRegarding your questions:\n\n1. Certainly, we would be pleased to explain the specific process of patch selection. Each image is divided into N non-overlapping patches. Although using overlapping patches may further improve performance, we intentionally avoided it to demonstrate the true effectiveness of InfoAug. Overlapping patches may contain the same object, which would not clearly illustrate the necessity of mutual information guidance. Additionally, the patches are uniformly divided without any artificial design, intended to not hurt its generalizability.\n\n2. For the second question, yes, the model is first pretrained on a video dataset without labels and then fine-tuned and tested on an image dataset. This aligns with the designed workflow of InfoAug and resembles the way humans learn.\n\nWe hope our additional experiment and explanation shall address your concern, and could be seen by more, allowing for further consideration of integrating mutual information into contrastive learning."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661793174,
                "cdate": 1700661793174,
                "tmdate": 1700661793174,
                "mdate": 1700661793174,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HR169GwfzB",
            "forum": "7GkdjhupsV",
            "replyto": "7GkdjhupsV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4826/Reviewer_m3jm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4826/Reviewer_m3jm"
            ],
            "content": {
                "summary": {
                    "value": "The rough idea of the proposed method is to construct a video-based dataset that can be applied to unsupervised image representation learning. In specific, for a given patch, mutual information is estimated between patches via applying a pre-trained tracking module. Then, for every patch, the similar patch with the highest mutual information is selected as a twin patch. Then, the assigned twin patch is regarded as a pseudo-positive pair on the contrastive learning scheme. Authors experiment the efficacy of proposed method on the CIFAR-10/100 and STL dataset with integration to existing self-supervised learning algorithm."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The idea of extracting patch-wise information in the video dataset and applying to benefit 2d dataset training is intruiging.\n2. The proposed loss function is simple"
                },
                "weaknesses": {
                    "value": "1) My biggest issue in this paper is the significance of the result. The linear probing result on CIFAR-10/100 is strictly underwhelming, given that conventional self-supervised learning result on the given dataset approaches around 92.6% in CIFAR-10 and 70.5% in CIFAR-100 (see [1]). In contrast, BYOL in the author's code shows 60.5% in CIFAR-10 and 30.3% in CIFAR-100.\\\n2) Due to this result, I am puzzled as to why we should look at this \"pre-training on the video dataset\". The method seems like an underwhelming training strategy albeit using additional data.\\\n3) Thereby, I suggest the authors redesign the experiment by pre-training on a much larger dataset and show better performance on such CIFAR-10/100.\\\n4) The paper has some typos (utual information, (z1,z1) before equation (4), etc...).\\\n5) Furthermore, consider incorporating other models (e.g. CLIP) that can be applied in a zero-shot manner. \n\nIn summary, I am severely concerned about the validity of the proposed pre-training approach since the result in Table 1 is very underwhelming and far from the recent numbers. Thereby, I lean to rejection.\n\n\n\n***References***\n[1] Unsupervised Visual Representation Learning via Mutual Information Regularized Assignment, NeurIPS 2022"
                },
                "questions": {
                    "value": "See Weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4826/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820729989,
            "cdate": 1698820729989,
            "tmdate": 1699636466059,
            "mdate": 1699636466059,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qMp2Ntbkfk",
                "forum": "7GkdjhupsV",
                "replyto": "HR169GwfzB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4826/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer m3jm"
                    },
                    "comment": {
                        "value": "First of all, thank you very much for your time devoted in giving us the valuable feedback and your acknowledgement on the novelty of our method and design of loss function.\n\nWith respect to the questions you raised, we would like to provide the following responses:\n\n1. Regarding the doubts about the first, second, and third points, we offer the following explanation: We aim to provide a more rigorous method for defining positive samples, inspired by human learning. In a given environment, we observe various objects and obtain a rough estimate of the mutual information between objects. One implicit requirement of this method is that we have a sufficient number of observations of objects in an environment. This requirement is naturally fulfilled in human learning, but it is severely violated in large-scale in-the-wild video datasets. These datasets often suffer from significant camera movements and occasional scene changes. Sometimes, an object only appeared for first frame and quickly left the camera in subsequent frames. This setup is not consistent with our human learning perception and does not clearly demonstrate the intended concept of InfoAug, which aims to judge positive and negative samples based on the moderate perception of information sharing between objects. Although we have conducted corresponding experiments on large-scale datasets, even achieving results comparable to traditional SOTA models, we did not focus solely on this aspect. It is unrelated to the experimental results but only mismatched with the method we wanted to convey. Therefore, we explicitly explained this mismatch with large in-the-wild datasets in our paper, hoping that future work can further expand this method and find unrestricted learning approaches for InfoAug on large-scale datasets. Nevertheless, we believe this should not hinder the demonstration of InfoAug's powerful features. Through a combination of 21 models and datasets, 3 comparative experiments, and ablative experiments across 5 scenarios, we demonstrated the improvement of InfoAug over the existing SOTA baseline, proving the interesting connection between mutual information and positive/negative samples. We hope this method of defining positive and negative samples can be further popularised.\n\n2. Regarding your fourth point, we sincerely appreciate you pointing out the issue, and we have corrected the errors in the paper. To present more clearly the improvement of InfoAug over the existing SOTA and provide more detailed usage instructions, we have supplemented the appendix with experiments and calculations of the computational cost of mutual information on different projection heads. Additionally, we have made modifications to two figures in the paper to enhance readers' understanding of the natural workings of InfoAug. As for your fifth suggestion, we greatly appreciate your proposal and assistance. However, since CLIP is a multimodal model for language and image embedding, and our method does not involve the language modality, we would appreciate it if you could provide further clarification and explanation.\n\nThanks!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662886164,
                "cdate": 1700662886164,
                "tmdate": 1700662886164,
                "mdate": 1700662886164,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c9ExI7uD1Q",
                "forum": "7GkdjhupsV",
                "replyto": "HR169GwfzB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4826/Reviewer_m3jm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4826/Reviewer_m3jm"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. \nHowever, I think the author's response does not clarify any of my concerns, especially on the comparison against existing methods. It is hard to understand the rebuttal despite the simple question of why the baseline method's performance does not match the reported numbers (I believe reviewer LYQ4 asks similar questions). I'll stick to my current score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730562666,
                "cdate": 1700730562666,
                "tmdate": 1700730592460,
                "mdate": 1700730592460,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YEKRoC1Htk",
            "forum": "7GkdjhupsV",
            "replyto": "7GkdjhupsV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4826/Reviewer_LYQ4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4826/Reviewer_LYQ4"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes InfoAug, a contrastive learning framework that employs a novel way of selecting positive pairs. Besides positive pairs that are generated by augmentations, InfoAug also selects the \"twin patch\" which maximizes the mutual information of the original patch as another positive patch. Experiments show that when trained on a video dataset and evaluated on CIFAR-10, STL-10, and CIFAR-100, the proposed InfoAug brings 1-2% improvement over existing contrastive learning methods."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper proposes a novel way of selecting positive pairs in contrastive learning, which is to select a twin patch that maximizes mutual information between the two pairs. Such a design proposes a new direction in utilizing video datasets in contrastive learning."
                },
                "weaknesses": {
                    "value": "Overall, the major problem of this paper is the weak experimental results. The experiment results are weak in two aspects:\n1. The accuracy of baselines on evaluated datasets is low. For example, when trained and tested on CIFAR-10, SimCLR can achieve >90% accuracy. However, in this paper, when trained on DAVIS20+GMOT40 and tested on CIFAR-10, the performance is only 60%-70%. I would suggest including CIFAR-10 as a pseudo-video dataset in the pre-training stage so that the paper can make a fair comparison with current methods on these datasets.\n2. The improvement of the proposed method is marginal. For example, when the accuracy on CIFAR-10 is 60%-70%, the standard deviation can be large, but the proposed method only improves the performance by 1-2% on each dataset. This makes the improvement shown in the paper not convincing enough.\n3. The paper does not include an analysis of computation overhead over baseline methods. Estimating the mutual information between two patches can introduce some computation overheads, making the framework slower than baseline methods.\n\nThe presentation of the paper is also not very clear and needs further polishing. There are multiple typos and grammar errors, and the font in the figures is too small."
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4826/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699117438669,
            "cdate": 1699117438669,
            "tmdate": 1699636465957,
            "mdate": 1699636465957,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IB0zM3xqx7",
                "forum": "7GkdjhupsV",
                "replyto": "YEKRoC1Htk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4826/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer LYQ4"
                    },
                    "comment": {
                        "value": "Thank you very much for your time in reviewing our work, giving valuable feedback. We also thank you for giving credit that it serves as novel for contrastive learning in video dataset.\n\nNow, we will address your concern down below in a sequential order:\n\n1. Regarding your concern 1 and 2, yes, we use a small-scale dataset for pretraining, which leads to a downscaled accuracy on the downstream task. The reason behind that lies in what we would like to contribute with this paper: To demonstrate and elicit a natural definition of \u201cpositive samples\u201d in contrastive learning. The inspiration lies in how we human recognizes positive samples: we learn in an environment with necessary amount of observation (you cannot see two cats walking together for one millisecond and learning they are of same species). The problem with current large scale video dataset is that, the scene is dynamically moving, leading to extremely insufficient observation for almost any object, which greatly differs from human-learning settings. So, although some of the result looks competitive with the traditional contrastive learning paradigm on those in-the-wild datasets, which exhibits high camera dynamic, the result is a compromised one in that the advantage of InfoAug is offset by the divergence of the intentional learning setting and that of large video dataset shoot casually, which might be misleading to the question of \u201cwhat is the strength and how to apply it\u201d. Therefore, we make clear our intention and proposed learning setting explicitly in the paper to avoid the misunderstanding of the suitable application of InfoAug, and test its performance on video dataset that do offers similar level of observation for a group of objects. The main results on three major downstream dataset, ablation study on loss weight and training epoch, all consistently shows the benefit of InfoAug as a valuable approach for defining positive samples. We notice the fact that the improvement ranges from 1% to 4% and we hope the improvement will be larger if the scale can be further expanded by future work. To this extent, we also examined if InfoAug really bring significant result by doing various comparison experiment to ablate out all the possible contributing factor, including the increased training sample, dual-branch formulation. and it shows that all the consistent contribution vanish, without using InfoAug to provide mutual-information awareness, demonstrating the absolute value in defining \u201cpositive sample\u201d by InfoAug.\n\n2. For your third concern, we attach the relevant diagram at the end of appendix for your valid concern. For a robot to determine the positive samples in a video, where we assume it focus simultaneously on as much as 40 objects, for an interpolation of 50 frames, it only took 2 seconds on a single CPU core. If, as we did by multi-thread it with 16 working thread, it is reduced to less that 0.2 second. For the most updated MI estimation technique, it could be further reduced to 0.1 second without any multi-threading, without GPU-acceleration.\n\nIn summary, we show by multiple comparative experiment (please see more in section 4.1, 4.2 and 4.3)that the improvement is significant and consistent over any SOTA model-dataset combination, that the way of defining positive samples by mutual information is both natural and of great potential, we hope such an idea could be seen by others to together expand it towards to more unified contrastive learning approach."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643629402,
                "cdate": 1700643629402,
                "tmdate": 1700643629402,
                "mdate": 1700643629402,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]