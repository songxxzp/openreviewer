[
    {
        "title": "Implicit bias of SGD in $L_2$-regularized linear DNNs: One-way jumps from high to low rank"
    },
    {
        "review": {
            "id": "JuuESbN1Ic",
            "forum": "P1aobHnjjj",
            "replyto": "P1aobHnjjj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3990/Reviewer_Gs3p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3990/Reviewer_Gs3p"
            ],
            "content": {
                "summary": {
                    "value": "This papers considers the problem of minimizing parameters of a deep linear network with the matrix completion loss. Specifically the authors considered an $\\ell_2$ regularized version of this problem and show that with certain conditions on the learning rate and the regularization parameter, SGD can jump from a high-rank local min to a low-rank local min, while it cannot jump from a low-rank local min to a high-rank local min."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think the theoretical results of this paper are interesting because it gives a nice characterization of the implicit bias of GD/SGD for deep linear networks. The proof that SGD can \"jump\" from high-rank to low-rank local minima is new to me and I think its a good step towards understand the training dynamics for deep neural networks."
                },
                "weaknesses": {
                    "value": "I think the main weakness of this paper is that both the theoretical results and the experiments require specific conditions on the $\\ell_2$ regularization parameter $\\lambda$ and the learning rate $\\eta$, which seem to be a bit artificial. For example, the requirement that $\\lambda$ is large in Theorem 3.2 seem to artificially cause $\\|\\theta\\|$ to decay more quickly, thus biasing it towards low numerical rank. In the numerical experiments, a similar annealing technique is used run SGD with a large $\\lambda$ and $\\eta$, before switching to smaller parameters. \n\nMy main point is that the implicit bias observed in this paper could be a result of a deliberate choice of parameters, instead of a natural property of SGD and GD. I hope the authors can clarify this point."
                },
                "questions": {
                    "value": "Please see previous section. Also, I wonder if the proof in this paper for Theorem 3.2 also works for GD? In other words, is $B_r$ also absorbing for GD?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3990/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3990/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3990/Reviewer_Gs3p"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3990/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698648863747,
            "cdate": 1698648863747,
            "tmdate": 1700206854635,
            "mdate": 1700206854635,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X2ioxk2Rfx",
                "forum": "P1aobHnjjj",
                "replyto": "JuuESbN1Ic",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3990/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3990/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thoughtful review.\n\nFirst note that we actually do not need $\\lambda$ to be large for our theorem, only $C$ needs to be large. We apologize for the confusion, we did not realize that our statement was confusing, and we have replaced it with \"for $r\\geq0$, any $\\lambda\\geq 0$, any large enough $C$ and small enough $\\alpha,\\epsilon_1,\\epsilon_2,\\eta$\" which can only be understood in the correct way.\n\nThis should resolve your concern that the effect we observe is only a result of a very large regularization, since our result applies to arbitrarily small ridge $\\lambda$.\n\nNote also that an upper bound on $\\eta$ is unavoidable, since large $\\eta$ could lead to divergence. Now it seems likely that the bound we require on $\\eta$ is not tight, especially w.r.t. its dependence on $\\lambda$, and this could be improved in future work.\n\nFrom our theorem, one can identify settings (for $\\lambda,\\eta$ small enough) where GD can get stuck at a rank-overestimating minima, while SGD will be able to escape it given enough time, showing a fundamental difference between the two regimes.\n\nRegarding your question. Yes, the sets $B_r$ are absorbing for GD too, only the jumps are specific to SGD. But both are needed to guarantee a gradual decrease of the rank."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3990/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093354752,
                "cdate": 1700093354752,
                "tmdate": 1700093354752,
                "mdate": 1700093354752,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dZUl4HwsSZ",
                "forum": "P1aobHnjjj",
                "replyto": "X2ioxk2Rfx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3990/Reviewer_Gs3p"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3990/Reviewer_Gs3p"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for addressing my concerns. I believe that this is a good paper that should be accepted. I have raised my score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3990/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700206845025,
                "cdate": 1700206845025,
                "tmdate": 1700206845025,
                "mdate": 1700206845025,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eVTpeBHrwM",
            "forum": "P1aobHnjjj",
            "replyto": "P1aobHnjjj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3990/Reviewer_fYKu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3990/Reviewer_fYKu"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the optimization landscape of regularized stochastic gradient descent applied to matrix completion with linear networks problems (which is equivalent to matrix completion with a $2/L$ Shatten norm regularizer). Several properties of the optimization landscape are proved, including the fact that the only critical points of the optimization problem over the factor matrices (minimizing $\\mathcal{L}_{\\lambda}(\\theta)$ must be local minima of the optimization problem over the full matrix $A$, unless they are strict saddle points in the original problem.\n\n In addition, it is shown that if gradient flow converges to a global minimum, then a version of gradient flow with a sufficiently small regularization parameter will converge to a minimum with a larger rank than the ground truth. Arguably the most significant final result is that stochastic gradient descent jumps from high rank local minima to lower rank local minima, with the jumps being one directional: one cannot return to a higher rank region after entering a lower rank region. Here, the lower rank regions should be understood as defined on page 5, in an approximate sense. Throughout the proofs, the fact that the local minima of the optimization problem over $\\mathcal{L}_\\lambda(\\theta)$ must be balanced (cf. Proposition A.1). An approximate version of this condition is also present in the definition of the low absorbing low rank spaces in the main results of the paper."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The main paper is well-written and the results appear generally sound. The results are of great importance to the field and interest to the community. **This is highly non trivial and important work**."
                },
                "weaknesses": {
                    "value": "Although the main paper is well written, the **proofs are not reader-friendly** at all. \n\nThe writing of the proofs is very terse and laconic, omitting many details. Although this is reminiscent of some great pure mathematics papers that were ahead of their time and I enjoyed the challenge some of the time, I strongly believe this style should only be considered acceptable if there is absolutely zero tolerance for any errors or inaccuracies whatsoever. I don't think the proofs actually stand up to this amount of scrutiny: there are **at least a few typos, minor errors and imprecisions** in the subset of the proofs I was able to look at, and since a lot of information is left out for the reader to figure out, the additional presence of even a small number of actual errors dramatically expands the \"search space\" from the point of view of the reader.  I would really like to see a substantial revision of the paper with more detailed and careful proofs (and maintaining my score is conditional on that). \n\nFor instance, in page 12, point \"(0)\", the definition of the $U_i, V_i$ is not really consistent: the index under the $U,V,S$ is used both to mean the iteration step in the sequence and the position in the product $W^L...W^1$. \n\nIn addition, in page 13, consider the following statement the authors make \" as $\\lambda\\rightarrow 0$, the critical points of the loss move continuously. Consider a continuous path of critical points, as $\\lambda\\rightarrow 0$, it converges to...\"\nAlthough the argument makes sense intuitively, filling in the gaps with rigorous proofs is definitely beyond the scope of what can be expected of the reader to do. At least some citations are a minimum. I doubt that simple continuity is enough to guarantee convergence (even if a subsequence converges, the path could oscillate widely), probably the only way to rigorously prove the statement is to use a quantitative version of the statement relying on calculus of variations. \n\nThis is not the only example. In point (1) in page 14, the authors say \"the singular value ..... must converge to a non zero eigenvalue\".  It is not clear **why this is the case**, or why the the *singular value* turns into an *eigenvalue* after convergence. Far more details are required. \n\nIn the middle of page 14, it is hard to imagine that the equation $U_{\\ell,i}(\\lambda)U_{\\ell-1,i}(\\lambda)$ can be correct without **at least a transpose missing**. Of course, the lack of a rigorous and consistent definition of $U_{\\ell,i}$ does not help here. \n\nAt the bottom of page 14: the line starting with \"other directions\" ends with \" $L-1,)$\" and a few lines below we have the equation $U_\\ell^\\top dU_{\\ell}+ = -dU_\\ell^\\top U_\\ell$. What does \"+=\" mean here? The same issue is present in many other parts of the paper, including in the third line of text on page 15.\n\nTowards the end of Appendix A in page 17, the term \"saddle to saddle\" is mentioned with absolutely no explanation or citation. \n\nIn the middle of page 13, the authors use the fact that \"a matrix cannot be approached with matrices of strictly lower rank\", which is true but should probably warrant a citation since the equivalent statement is not true for tensors. \n\n\nThe proof of Proposition A4 is very hard to make sense of without further information: the first sentence is \"\"let A(\\lambda) be path of global minima restricted to the set of matrices of rank $r^*$ or less.\" how do you construct the path? Even for $L=2$, there can be a continuous set of global minima of local intrinsic dimension higher than 2, how do you use the axiom of choice to construct a \"path\"? \nSentences such as \"going along directions that increase the rank of $A(\\lambda)$, the regularization term increases at a rate of $d^{2/L}$ for $d$ the distance\" definitely need more mathematical details. \n\nSimilarly, the statement about $\\phi$ being differentiable in the directions which do not change the rank should be made more precise (although I agree with it, probably at least a citation to [1] is a minimum) \n\nFor proposition A.5, the proof starts with the following sentence \"We know that L2 regularized GF $\\theta_\\lambda(t)$ converges to unregularized GF $\\theta(t)$ as $\\lambda\\rightarrow 0$\". There are two parameters here, $\\lambda$ and $t$, is the convergence uniform over all $t$?\n\n\n\n\n\n\n\n\n\n\n\n\n\n========more minor points:=====\n\nMany apologies if I am being picky but as a relative outsider to optimization literature, even the statement that the point 0 is a critical point was not immediately  obvious to me (perhaps either a calculation of the gradient or a mention of the fact that $L>1$ would help). \n\nIn the bottom of page 13, the equation before equation (1) is presumably the end of a sentence, thus the next line should be rewritten. Below, that \"no such thing happen\" should be \"no such thing happens\"\n\nSome citation for Fact C.4 (Ky Fan?) would be nice. \n\n\nIn page 19, just before the beginning of Section D.1. Do the authors mean $G_{\\theta,ij}$ instead of $G_{\\theta,j}$?\n\n\nJust above equation (6), $\\|W_\\ell|^2$ should be $\\|W_\\ell\\|^2$ and the sentence is missing a period. \n\n\n\n\n\n\n\n\n[1] Characterization of the subdifferential of some matrix norms, G.A. Watson. 1992, linear algebra and its applications."
                },
                "questions": {
                    "value": "1. In the third line of page 13, ou mention that the quantity in the limit is strictly positive but possibly infinite. Apologies if I  lack some background knowledge but could you explain your reasoning there? It is not at all obvious to me. \n\n2 At the beginning of the proof of proposition A5 in the first equation, should the infimum run over  $Rank A>r$ instead of $Rank A<r $ as written?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3990/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777130384,
            "cdate": 1698777130384,
            "tmdate": 1699636360817,
            "mdate": 1699636360817,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fW5RZTRhSE",
                "forum": "P1aobHnjjj",
                "replyto": "eVTpeBHrwM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3990/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3990/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thorough and detailed review. \n\nYour remarks were really valuable to improve the readability of our proofs. We have made many changes that should address your issues and we are still working on improving readability.\n\nWe have replaced $U_i,V_i$ by $V_i,W_i$ to avoid the conflict in notation with the $U_\\ell$s.\n\nWe have removed the reliance of the proofs on a continuous path of minima as $\\lambda \\searrow 0$. As you said, some work needs to be done to make such statement rigorous and it turns out to not be necessary.\n\nThe \"eigenvalue\" was a typo, it should have been \"singular value\", and we have changed this part of the proof in the newer version anyway, since we do not rely on a path of minimizers.\n\nYou were right that a transpose was missing and we have added a definition of $U_{\\ell,i}$. \n\nThe \"+=\" was a typo, we replaced it with \"=\".\n\nThe saddle-to-saddle regime refers to the two papers cited at the end of the sentence, we have moved the citations to earlier in the sentence to clarify this.\n\nWe have added a short explanation for why a matrix cannot be approached by matrices of lower rank (the error will always be at least the $r+1$ singular value of $A$).\n\nWe have removed the reliance on a path of minima, instead we just take $A(\\lambda)$ to be a global minimum of $C_\\lambda$ amongst matrices of rank $r^*$ or less.\n\nThanks for the reference you provided for the differentiability along directions that do not change the rank, we will add it as a justification.\n\nThe convergence is not uniform in $t$, but we only need sequential convergence:  for all fixed $t$, we have convergence as $\\lambda \\searrow 0$. We have made this clearer.\n\n-----------\n\nWe will add a short explanation: the first $L-1$ derivatives of the unregularized loss $\\mathcal{L}$ vanish at $\\theta=0$, thus adding a regularization terms $\\lambda\\Vert\\theta\\Vert^2$ leads to a local minimum.\n\nWe fixed the typos and error you mentioned and added a citation for Fact C.4.\n\nRegarding your questions:\n1. In general you get an explosion if $A_{\\theta_i}$ approaches $A_{\\hat{\\theta}}$ from a higher rank direction. Assume that $A_{\\theta_i}$ equals to $A_{\\hat{\\theta}}$ with a small additional singular value: $A_{\\theta_i} = A_{\\hat{\\theta}} + uv^T / i$, then for balanced parameters $\\Vert \\theta_i - \\hat{\\theta} \\Vert^2 = i^{-\\frac{2}{L}}$, leading to an infinite limit.\n\n2. No, we do want the infimum over $\\mathrm{Rank}A<r^*$. Since we know it will be strictly larger than $0$, we can always choose $t_0$ large enough and $\\lambda$ small enough to be below this threshold. Once we are below this threshold, we know that the rank of $A_{\\theta(t)}$ cannot be strictly smaller than $r^*$."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3990/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093219878,
                "cdate": 1700093219878,
                "tmdate": 1700093219878,
                "mdate": 1700093219878,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KG6YlyIYXG",
                "forum": "P1aobHnjjj",
                "replyto": "fW5RZTRhSE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3990/Reviewer_fYKu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3990/Reviewer_fYKu"
                ],
                "content": {
                    "title": {
                        "value": "More polishing"
                    },
                    "comment": {
                        "value": "Thanks for your answers. I agree the deadline is a bit tight for such a technical paper. I am trying to make my way through your revision since I might not be able to read everything before the deadline I will leave a quick comment here already.\n\nI think the notation in the proof of proposition A.2. is still not ideal: you are still using the same symbol ($W_i$) to mean two things: both the matrix of right singular vector of $A_i$ (**ith iteration**) **and** the weight matrix at the ith **layer**. The first sentence of point (0) is also not a sentence.  \n\nI agree with reviewer NbUi that you absolutely should not make statements about GD if you prove something for GF. \n\nCan I ask if there is actually a **proof of Theorem 3.2** (not a sketch) somewhere in the appendix? What about **Theorems B.1 and B.2**? \n\n\nYou might want to sit down and check the notations of the whole paper.  Also can we have a revision with the modified parts in a different color for ease of reference? \n\n\nLike I said before, you should add more details for some of the proofs. It's a nice paper but it's hard to parse. One more example: second to last paragraph of the proof of proposition A.3:  \"one can guarantee that the second term dominates the third one since C is differentiable.\" I agree the statement holds, but it is always nice to add details (especially in a highly technical paper with errors). I think the point is that this statement feels counter intuitive because of the lack of incoherence assumptions: I think the size of the neighborhood in which the statement holds can be small in a way that depends on the size of the matrix, which fortunately is not a problem. \n\nRegarding my question 1: thanks for the explanation. I agree with it intuitively and it helps already, but I think it's no substitute for a rigorous proof (not just in the case where we add a rank 1 matrix which is the illustrative case you gave in this answer), which should be both here and in the paper. This question is actually key to the proof of Part 2.a of Proposition A.2 as well"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3990/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619035273,
                "cdate": 1700619035273,
                "tmdate": 1700619035273,
                "mdate": 1700619035273,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HC6Wv0wKOo",
            "forum": "P1aobHnjjj",
            "replyto": "P1aobHnjjj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3990/Reviewer_NbUi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3990/Reviewer_NbUi"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes the matrix completion task with deep linear neural networks. It shows that the critical points that are not local minima can be avoided with a small ridge. And it shows that GD cannot avoid overestimating minima but  SGD can jump from any minimum to a lower rank minimum."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I think analyzing the training dynamics of the deep linear neural networks on the matrix completion task is a very interesting problem. This paper provides insights on the advantages of using SGD to get a low-rank minima and provides experimental results."
                },
                "weaknesses": {
                    "value": "1. I feel the statement of theorems is not very clear. It uses a lot of ''small enough'', \"large enough\".  I think the statement should be more rigorous. \n\n2. This paper claims that it shows GD can avoid rank-underestimating minima by taking a small enough ridge $\\lambda$. But Proposition 3.2 is for Gradient Flow with a very strong assumption. I believe there is a gap. \n\n3. For the function $f_\\alpha$, it takes a very specific form. The authors claim that changing $f_\\alpha$ with similar properties should not affect the results. I don't see the reason why the condition cannot be extended to more general functions. I believe it could improve the results.\n\n4. A small suggestion is that the proof in the appendix is not easy to read. I think it can be more organized and add more explanation."
                },
                "questions": {
                    "value": "1. In the remark before section 3.2, it's said that it's possible that GD can recover the minimal rank solution easily. Can you say something about this case? \n\n2. I have a concern about the constant in Theorem 3.2. It is said $\\lambda$ and $C$ are large enough, but an example of acceptable rates in terms of $\\lambda$ is $C \\sim \\lambda^{-1}$. Is it contradictory?\n\n3. In the proof of Theorem 3.2, r columns with the most observed entries are taken.  What if all the columns have the same number of observed entries? Will the $d_{out}-r$ other columns of $W_L$ decay exponentially? I don't see the relation between rank $r$ and the number of observed entries here."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3990/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3990/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3990/Reviewer_NbUi"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3990/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788671540,
            "cdate": 1698788671540,
            "tmdate": 1699636360729,
            "mdate": 1699636360729,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EOZJ3ls8mS",
                "forum": "P1aobHnjjj",
                "replyto": "HC6Wv0wKOo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3990/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3990/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thoughtful review. Regarding your points:\n1. The statement \"for small enough $x$\" is simply a (commonly used) shortcut for \"there is a $x_0$ such that for all $x\\leq x_0$\". They are both rigorous formal statements. Note that exact bounds are given in the appendix, we did not put them in the main to improve readability.\n\n2. Since the loss $\\mathcal{L}_\\lambda$ is smooth, GD (and SGD) converges to GF as $\\eta \\searrow 0$. We stated the Proposition for GF to simplify the statement.\nWe preferred to assume an initialization that converges to a global minimum directly for the following reason. There exist many papers that can guarantee convergence to a global minimum (e.g. in the NTK regime, or with specific initialization, we will add references to such results in the main after the statement of this Proposition), and since it seems that we have not yet fully answered the question of convergence of DLNs, we expect more result to appear in the next years. Our Proposition can be applied to any such settings.\n\n3. We feel that our proofs are already quite complex and technical, and considering a general $f_\\alpha$ would lead to even more complexity for only a small increase in generality.\n\n4. We have clarified the proofs thanks to the remarks of the reviewers. Please tell us if there is one section in particular that you find difficult to understand, so that we may focus our efforts there.\n\n\nRegarding your questions:\n1. We say that there might not be any rank-overestimating minima (in contrast to the fact that there always exist rank-underestimating minima, e.g. the origin $\\theta=0$), in which case GD with a small enough ridge $\\lambda$ and learning rate $\\eta$ will converge to a minimum with the right rank by Proposition 3.2. Of course we do not know under what condition this happens, and it might be difficult to characterize, a trivial example would be if we observe all entries of the matrix.\n\n2. Sorry, we did not realize that our statement was confusing, we replaced it with \"for $r\\geq0$, any $\\lambda\\leq 0$, any large enough $C$ and small enough $\\alpha,\\epsilon_1,\\epsilon_2,\\eta$\" which can only be understood in the correct way. Namely, we do not need $\\lambda$ to be large for our theorem, only $C$ needs to be large.\n\n3. We can guarantee a jump under the event that SGD randomly selects observations only from the same $r$ rows/columns over a large period of time. This could happen for any set of $r$ rows/columns, but we only consider the $r$ rows/columns with the most observed entries since it is most likely to happen to these rows entries. When all rows have the same amount of observed entries, this event is as likely for all subsets of rows/columns (with probability equal to the lower bound given in the statement of the Theorem)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3990/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093087339,
                "cdate": 1700093087339,
                "tmdate": 1700093087339,
                "mdate": 1700093087339,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wg7634tods",
                "forum": "P1aobHnjjj",
                "replyto": "EOZJ3ls8mS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3990/Reviewer_NbUi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3990/Reviewer_NbUi"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the reply.  I have a few more comments on your response.\n\n1. Thanks for clarifying \"small enough\" and \"large enough\".  Personally, I would prefer to have explicit bounds in the statement, or at least in the main text. This improves clarity and avoids misinterpretation.\n\n2.  I  don't think it is proper to make claims on GD while the results are for GF. Though GD converges to GF with the learning rate going to $0$, they are different in principle. \n\n3. I am not sure a general $f_\\alpha$ would make the proof much more complicated. I think it is an important quantity however currently it is unclear why such $f_\\alpha$ is chosen. If a proof for general $f_\\alpha$ is not possible, at least the paper should discuss what kind of $f_\\alpha$ is reasonable and how it affects the proof.\n\n4. I read the updated draft but still find many arguments in the proofs are vague and handwavy.  E.g., in the proof for Proposition A.5, it is unclear how to apply Lemma A.1 to get the results given Lemma A.1 is a bit complicated, and what $\\lambda_n \\rightarrow 0$ means (as $n$ goes to infinity?). There are some more.  I believe the proofs should be correct, but my point is that more explanation and details should be included for readability and reproducibility."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3990/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534390506,
                "cdate": 1700534390506,
                "tmdate": 1700534390506,
                "mdate": 1700534390506,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vGeIJn81sO",
            "forum": "P1aobHnjjj",
            "replyto": "P1aobHnjjj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3990/Reviewer_M8aA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3990/Reviewer_M8aA"
            ],
            "content": {
                "summary": {
                    "value": "This work shows that when applied to matrix completion with deep linear networks, SGD can transition from local minima with higher ranks to solutions with lower ranks, while transitions in the opposite direction are zero. Crucially, this results depends on the gradient distribution of SGD which leads to drastically different outcomes than what common SDE-based models for SGD exhibit. The authors further provide numerical experiments that exhibit the predicted transitions in practice."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This work provides an interesting theoretical insight on the distinction between stochastic and deterministic gradient descent. I find it especially exciting that it provides a concrete example how the gradient distribution of SGD enables phenomena that are not apparent from SDE-based models."
                },
                "weaknesses": {
                    "value": "I can not think of a weakness. It's a very nice paper in my opinion."
                },
                "questions": {
                    "value": "As you point out, the common Langevin-based models predict a transition among each pair of points and thus fall short to capture the phenomenon shown by your result. Do you know if they would nevertheless exhibit a systematic low-rank bias (i.e. the transition toward lower-rank solutions being much more likely than toward higher-rank solutions)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3990/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801155274,
            "cdate": 1698801155274,
            "tmdate": 1699636360647,
            "mdate": 1699636360647,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5tUQ3Pxo36",
                "forum": "P1aobHnjjj",
                "replyto": "vGeIJn81sO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3990/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3990/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the review!\n\nRegarding your question, we would also observe a low-rank bias with Langevin dynamics since minima with a lower rank generally have a smaller parameter norm and thus a smaller regularized loss. It should therefore be more likely to jump from a high loss to a low loss minima than the other way round. Now the relation between parameter norm and rank is not exact and there can be example where the minimal parameter norm solution has a higher rank than another solution with a larger parameter norm (this happens for $\\epsilon$ small enough in the setup of our numerical experiments). So the low-rank bias would be in some sense weaker.\n\nBut Langevin dynamics do not capture the covariance of the noise of SGD. One can instead study an SDE with matching covariance. But this covariance is not full rank especially inside and around the $B_r$ sets, our intuition is that there is a lot noise along direction that keep or lower the rank, but (almost) no noise along direction that increase the rank. It is still unclear whether a similarcould be proven in this setting (e. .whether the limiting distribution is also supported inside $B_1$)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3990/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700092949814,
                "cdate": 1700092949814,
                "tmdate": 1700092949814,
                "mdate": 1700092949814,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]