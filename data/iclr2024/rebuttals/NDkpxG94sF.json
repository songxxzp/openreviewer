[
    {
        "title": "V-DETR: DETR with Vertex Relative Position Encoding for 3D Object Detection"
    },
    {
        "review": {
            "id": "oyfZW4ziaw",
            "forum": "NDkpxG94sF",
            "replyto": "NDkpxG94sF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2646/Reviewer_fSn3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2646/Reviewer_fSn3"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an effective enhancement to DETR-based indoor 3D object detection. The key idea is to add a relative positional embedding between the queries and points. The positional embedding, which is called 3D Vertex Relative Position Encoding (3DV-RPE), calculates the relative positional embedding under the coordinate system of each 3D bounding box generated from the query. After incorporating the positional embedding, the performance significantly increases on both ScanNetv2 and Sun-RGBD."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper has demonstrated the following strengths:\n\n* The approach of the paper has good support from their performance improvement on ScanNetv2 and Sun-RGBD.\n* The vertex relative position encoding (3DV-RPE) has the reasonable intuition of embedding position information for 3D detection and will likely inspire other readers.\n* The qualitative results like Figure 1 clearly illustrate the implication of the method in the paper in guiding models' attention."
                },
                "weaknesses": {
                    "value": "* I suggest improving the order of presentation in Sec. 3.2 and Sec. 3.2. For example, I suggest moving the paragraph of \"3DV-RPE\" before talking about \"canonical object space\" and other details. When I read this part, I was quite confused by Sec. 3.2, not knowing how $R$ is generated, what is $P_i$, etc.\n\n* As position encoding is the focus of this paper, I expect the authors to analyze or conduct ablation studies on more position encoding algorithms. Details are in the \"questions\" section below. \n\n* I also haven't found the performance of the baseline without relative position encoding. In case I missed it, I suggest the authors put it into Sec. 4.3 or Sec. 4.4 for a clear ablation study.\n\nTypo on page 5, line 1: fig:rotatedRPE"
                },
                "questions": {
                    "value": "1. **Baseline performance.** As mentioned in the weakness section, could you remind me where you have put the baseline performance? It is critical to recognize the improvement of 3DV-RPE. Technically, I wish to see that under the same normalization and other tricks, 3DV-RPE is indeed helpful.\n\n\n2. **Additional analysis.** With position encoding being the center of this paper, I think it necessary to conduct ablation studies on other common formats of position encoding, such as:\n*  Absolute position encoding, in both the formats you proposed like Eqn. 3 and Eqn. 4, or common sin-cos position encoding.\n* More justifications of hyper-parameters. For example, where does 10 come from in $T$'s shape? May I use another number to replace 10?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2646/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2646/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2646/Reviewer_fSn3"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698599702299,
            "cdate": 1698599702299,
            "tmdate": 1699636204943,
            "mdate": 1699636204943,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vjXe0kLKFB",
                "forum": "NDkpxG94sF",
                "replyto": "oyfZW4ziaw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fSn3 (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the careful reviews and constructive suggestions. We answer the questions as follows.\n\n> **Q1. I suggest improving the order of presentation in Sec. 3.2 and Sec. 3.2. For example, I suggest moving the paragraph of \"3DV-RPE\" before talking about \"canonical object space\" and other details. When I read this part, I was quite confused by Sec. 3.2, not knowing how $\\mathbf{R}$ is generated, what is $\\mathbf{P}_{i}$, etc.**\n\nGreat suggestion! Thanks for pointing out this issue. **We have revised the organization of Sec. 3.1 and Sec. 3.2 to make it easier to follow. Please check the blue text in the revised PDF.**\n\n> **Q2. As position encoding is the focus of this paper, I expect the authors to analyze or conduct ablation studies on more position encoding algorithms. Details are in the \"questions\" section below.**\n\nGreat suggestions! First, we would like to invite you to refer to Table 13 and the related discussion in our submission. We guess the comparison results with the latest position encoding algorithms (in the first two rows) might be the mentioned ablation studies on more position encoding algorithms. Besides, we also answer your other detailed concerns in the following response.\n\nFor your convenience, we also attached Table 13 in the following:\n\n| method                                   | epoch | AP25 | AP50 |\n| ---------------------------------------- | ----- | ---- | ---- |\n| Baseline (w/o RPE)                       | 540x   | 71.4 | 47.6 |\n| Baseline + CRPE (Stratified Transformer[1]) | 540x   | 74.7 | 58.1 |\n| Baseline + CRPE (EQNet[2])                  | 540x   | 73.1 | 54.4 |\n| Baseline + 3DV-RPE                       | 540x   | 77.8 | 66.0 |\n\nAccordingly, our 3DV-RPE significantly outperforms these two advanced RPE schemes, especially on AP50 metrics (with gains of +7.9 and +11.6 respectively). We observe that both the CRPE (Stratified Transformer) and CRPE (EQNet) can achieve performance gains similar to a special case of our 3DV-RPE that only considers the 3D bounding box center (AP25=73.4/AP50=54.8). Therefore, it's clear that the key to effectiveness is to explicitly consider the relative position information between each point and the 8 vertex points of a given 3D bounding box. We will add the above information in the final revision.\n\n[1] Stratified Transformer for 3D Point Cloud Segmentation, CVPR 2022\n\n[2] A Unified Query-based Paradigm for Point Cloud Understanding, CVPR 2022\n\n> **Q3. I also haven't found the performance of the baseline without relative position encoding. In case I missed it, I suggest the authors put it into Sec. 4.3 or Sec. 4.4 for a clear ablation study.**\n\nGood point! In fact, we have reported the performance of the baseline without relative position encoding in Table 6 (1st row and 5th row). We would like to reorganize Sec. 4.4 to place Table 6 and the related discussion at the forefront to make it clearer.\n\nWe also provide the detailed comparison results on ScanNet for reference:\n\n|                        | \\#epoch | AP25 | AP50 |\n| ---------------------- | ------- | ---- | ---- |\n| Our Baseline           | 540x    | 71.4 | 47.6 |\n| Our Baseline + 3D-VPRE | 540x    | 77.8 | 66.0 |\n\n> **Q4. Baseline performance. As mentioned in the weakness section, could you remind me where you have put the baseline performance? It is critical to recognize the improvement of 3DV-RPE. Technically, I wish to see that under the same normalization and other tricks, 3DV-RPE is indeed helpful.**\n\nAs answered in Q3, we have reported the baseline performance in Table 6 (1st row and 5th row) and attached the results for reference. Accordingly, we observe that the 3DV-RPE can boost the AP25 from 71.4 to 77.8 and AP50 from 47.6 to 66.0. We will explicitly add the above discussion in the final revision."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555557165,
                "cdate": 1700555557165,
                "tmdate": 1700619771780,
                "mdate": 1700619771780,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5BEfKMxNW8",
                "forum": "NDkpxG94sF",
                "replyto": "oyfZW4ziaw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fSn3 (2/2)"
                    },
                    "comment": {
                        "value": "> **Q5. Additional analysis. With position encoding being the center of this paper, I think it necessary to conduct ablation studies on other common formats of position encoding, such as: Absolute position encoding, in both the formats you proposed like Eqn. 3 and Eqn. 4, or common sin-cos position encoding.**\n\nGreat point! First, as answered in Q2, we have reported the comparisons with two advanced RPE schemes specially designed for the 3D object detection task, including the CRPE (Stratified Transformer) and CRPE (EQNet). Second, we have also conducted the ablation experiments on the mentioned absolute position encoding in both the formats you proposed, like Eqn. 3 (in the updated PDF Eqn. 5) and Eqn. 4 (in the updated PDF Eqn. 2).\n\n| method                                                        | epoch | AP25 | AP50 |\n| ------------------------------------------------------------- | ----- | ---- | ---- |\n| Baseline (w/o RPE)                                            | 540x   | 71.4 | 47.6 |\n| Baseline + APE w/ Sin-Cos                                     | 540x   | 71.8 | 47.9 |\n| Baseline + APE w/ MLP + NonLinear\uff08Eqn. 4, update to Eqn. 2\uff09                    | 540x   | 72.1 | 48.7 |\n| Baseline + APE w/ MLP + NonLinear + Predefined Table\uff08Eqn. 3, update to Eqn. 5 \uff09 | 540x   | 72.0 | 48.5 |\n| Baseline + 3DV-RPE                                             | 540x   | 77.8 | 66.0 |\n\nAccording to the above results, we can observe that\n\n- Our 3DV-RPE significantly outperforms all the APE variants by a large margin, especially on AP50 which requires more accurate localization capability.\n- By comparing the 3rd row to the 2nd row, we observe that the MLP and non-linear transformation can slightly improve the performance of APE.\n\n> **Q6. More justifications of hyper-parameters. For example, where does 10 come from in $\\mathbf{T}$'s shape? May I use another number to replace 10?**\n\nGood point! We chose $10$ considering the trade-off between performance and memory cost. We conducted ablation experiments on the size of $\\mathbf{T}$ and reported the detailed comparison results in Table 12. We also included the results below for reference:\n\n| 3DV-RPE table shape    | \\#epoch | AP25 | AP50 |\n| ---------------------- | ------- | ---- | ---- |\n| $5\\times5\\times5$    | 360x    | 76.7 | 64.7 |\n| $10\\times10\\times10$ | 360x    | 76.7 | 65.0 |\n| $25\\times25\\times25$ | 360x    | 76.7 | 64.2 |\n\nAccordingly, we have observed that our 3DV-RPE remains robust when faced with different choices for the shape of $\\mathbf{T}$."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555584465,
                "cdate": 1700555584465,
                "tmdate": 1700621263282,
                "mdate": 1700621263282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c1Sao1E8wY",
                "forum": "NDkpxG94sF",
                "replyto": "oyfZW4ziaw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2646/Reviewer_fSn3"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewer_fSn3"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response!\n\nYour answers address my concerns. It looks to me like most of my questions were caused by the presentation issue. Still, I suggest the authors make (1) Table 6 more explicit, maybe adding a \"(Baseline)\" after None; (2) mentioning Table 13 somewhere in the main paper, I personally think it very important.\n\nI worked on 3D detection 1-2 years ago so I may not be the right person to justify the novelty as TFsP or YhUd. I tend to accept this paper for now, and will be willing to increase my score if the authors can convince them of the technical novelty."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671617158,
                "cdate": 1700671617158,
                "tmdate": 1700671635391,
                "mdate": 1700671635391,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qmseYbEiCv",
                "forum": "NDkpxG94sF",
                "replyto": "oyfZW4ziaw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the Response of Reviewer fSn3"
                    },
                    "comment": {
                        "value": "We would like to extend our sincere appreciation to the reviewer for your swift response and beneficial suggestions aimed at enhancing our presentation.\n  \nPursuant to your additional recommendations, we have implemented the following changes:  \n   \n- To enhance clarity, we replaced \"None\" with \"Our Baseline\" in Table 6 in the latest PDF revision.  \n    \n- We would like to apologize for the oversight in our previous response to Q2, where we failed to specify that Table 13 can be found on page 13.\n  \nWe are grateful for your professional integrity and research expertise. We've greatly benefited from your meticulous reviews. With respect to the technical novelty of our work, we kindly invite you to review the comments from Reviewer c9AZ, and our supplementary response regarding the additional inference advantage of our methodology compared to the previous SOTA systems.   \n  \nOnce again, thank you for offering such invaluable feedback and for your support in accepting this paper.\ud83e\udd17"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704266394,
                "cdate": 1700704266394,
                "tmdate": 1700704349338,
                "mdate": 1700704349338,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7gC9krwExR",
            "forum": "NDkpxG94sF",
            "replyto": "NDkpxG94sF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2646/Reviewer_TFsP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2646/Reviewer_TFsP"
            ],
            "content": {
                "summary": {
                    "value": "For the task of indoor 3D object detecion from point clouds, previous sparse detectors neglects to tackle 3D queries outside the bounding box, so the author proposed a 3D vertex positional encoding module (3DV-RPE) to guarantee the locality principle in object detection. Specifically, the proposed method encode relative position information for each query towards its assigned / predicted bounding boxes to provide clear information to guide the model to focus on points near the objects. Moreover, it utilized many widely-adopted tricks to generally improve the performance of the detector (custom backbone / loss normalization / TTA / one-to-many auxillary loss, etc). 3DV-RPE shows competitive results on ScanNetv2 and SUN-RGBD compared to previous methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* this paper pinpoint a interesting gap in previous sparse 3D indoor detectors that how to deal with queries outside the predicted bouding box, and try to prove that whether it is benefical to take this into account for 3D object detection.\n* this paper does a lot of work to incorporate modern network architectures (e.g., ResNet34 + FPN), training strategy improvements to make the detector performs better."
                },
                "weaknesses": {
                    "value": "* Unfair comparision:\n    1) The author combines many technical improvements including normalizing box according to object size, one-to-many assignment as auxillary loss, a modified resnet34-fpn backbone, and even TTA. They're all irrelavant to the claimed core contribution 3DV-RPE. So to prove the proposed module is effective, the most convincing way could be adding 3DV-RPE directly onto the baseline GroupFree3D.  Considering that the 3DV-RPE  module can work in a plug-and-play manner in theory, I would expect more results based on other methods such as GroupFree3D, 3DETR or CAGroupFree3D.\n    2) in table 6, the author reports the one with 3DV-RPE + TTA (77.8 / 66.0), does all other ablation attention results are also reported with TTA?\n    3) the author reported the best results for the proposed method, how about the ablations? how many times have you run for each ablation choice in all tables? Does the fairness of the comparison is guaranteed?\n    4) Given the best set of paraters for the authors final model, change the choices in loss functions can affect the hungarian matching cost matrix, thus it's hard to say the improvement / performance drop comes from the module / improper cost weights.\n    5) Why some ablations are done with ScanNet while others use SUN-RGBD (e.g., Table 4)? Does it means the coordinates normalization works similarly on ScanNetv2?\n\n* minor contributions:\n    1) Actually I think the RPE and normalized coords are designed in similar ways: Point-RCNN has adopted to convert box to  canonical coords and do normalizations on oritentaion. Moreover, in anchor-based detectors, they already use the anchor boxes' W and H to normalize the regression targets. Here the author uses dynamic bounding boxes from predictions, which has also been explored in methods like MetaAnchor, etc.\n    2) So many un-relavent tricks to improve the detection performance. I don't like the way to do whatever it can to improve the results. Rather, the author should focus on the main contribution. After the core module being sufficiently discussed, one can further improve its results with more tricks. Here the author put all stuff together, which makes me doubt where the improvement come from.\n\n* Most of the references are before year 2023, so I think more recent works in year 2023 should be included.\n\n* I recommend against reporting results using TTA, as this leads to cutthroat competition and more potentially unfair comparisons; for example, TTA may be different in different papers, but is always written as \"TTA\"."
                },
                "questions": {
                    "value": "* How much fraction does the queries outside predicted bounding boxes account for with respect to the total number of queries? 10%? 20%? The author should provide a investigation to this problem.\n* Why the non-linear functions is designed in this way? how it is derived? is their any insights to do so?\n* Why does the PE is added in the way in Eq. 1? I think the form of matmul(Q, K) + R does not match the intended aim of the paper. Instead, I think matmul(Q+R, K) should be more proper? or add a relative PE to Q and a global PE to K?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2646/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2646/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2646/Reviewer_TFsP"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763359994,
            "cdate": 1698763359994,
            "tmdate": 1699636204875,
            "mdate": 1699636204875,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SRTrXvxx8D",
                "forum": "NDkpxG94sF",
                "replyto": "7gC9krwExR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TFsP (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the careful reviews and constructive suggestions. Above all, **please refer to the general response regarding concerns about unfair comparisons and minor contributions**. We address the other questions as follows:\n\n> **Q1. The author combines many technical improvements including normalizing box according to object size, one-to-many assignment as auxillary loss, a modified resnet34-fpn backbone, and even TTA. They're all irrelavant to the claimed core contribution 3DV-RPE. So to prove the proposed module is effective, the most convincing way could be adding 3DV-RPE directly onto the baseline GroupFree3D. Considering that the 3DV-RPE module can work in a plug-and-play manner in theory, I would expect more results based on other methods such as GroupFree3D, 3DETR or CAGroupFree3D.**\n\nPlease refer to the general response for more details. In summary, our 3DV-RPE can significantly enhance the performance of both 3DETR and GroupFree3D. We will include these detailed comparison results in the final revision.\n\n> **Q2. in table 6, the author reports the one with 3DV-RPE + TTA (77.8 / 66.0), does all other ablation attention results are also reported with TTA?**\n\nYes, we report all the ablation attention experiment results with TTA in Table 6. Furthermore, we presume you might also be interested in **the comparison results without TTA**, for which we provide the details as follows:\n\n| method | # Epochs | AP25 | AP50 |\n| ---------------------------- | -------- | ---- | ---- |\n| None                         | 360x      | 67.9 | 43.5 |\n| 3D Box Mask                  | 360x      | 72.9 | 58.3 |\n| 3DV-RPE                      | 360x      | 76.2 | 64.2 |\n| 3D Box Mask + 3DV-RPE        | 360x      | 75.3 | 61.5 |\n| None                         | 540x      | 70.6 | 46.7 |\n| 3D Box Mask                  | 540x      | 74.2 | 59.6 |\n| 3DV-RPE                      | 540x      | 77.4 | 65.0 |\n| 3D Box Mask + 3DV-RPE        | 540x      | 76.5 | 62.4 |\n\nIn conclusion, even without TTA, our 3DV-RPE continues to significantly enhance performance, especially the AP50 metrics, which necessitates precise 3D localization capabilities.\n\n> **Q3. the author reported the best results for the proposed method, how about the ablations? how many times have you run for each ablation choice in all tables? Does the fairness of the comparison is guaranteed?**\n\nYes, we have reported the best results from the ablation experiments.\n\nFollowing the previous GroupFree3D methodology, we trained each setting $5\\times$ times and tested each training trial $5\\times$ times. We have reported the maximum performance from these $25\\times$ trials to ensure fairness. Additionally, we are prepared to provide the average performance, if necessary.\n\n> **Q4. Given the best set of paraters for the authors final model, change the choices in loss functions can affect the hungarian matching cost matrix, thus it's hard to say the improvement / performance drop comes from the module / improper cost weights.**\n\nGood point! We attempt to address your concerns about the impact of the loss function and Hungarian matching cost choices by replacing both with the original versions used in 3DETR, and we've reported **the comparison results without using TTA**:\n\n| method | loss weights                                                                                                      | cost weights                                                                                                      | # Epochs | AP25 | AP50 |\n| ------ | ----------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- | -------- | ---- | ---- |\n| Ours   | $\\lambda_{1}=2$,$\\lambda_{2}=1$,$\\lambda_{3}=0.5$,$\\lambda_{4}=3$,$\\lambda_{5}=0.1$,$\\lambda_{6}=0.5$ | $\\lambda_{1}=2$,$\\lambda_{2}=1$,$\\lambda_{3}=0.5$,$\\lambda_{4}=3$,$\\lambda_{5}=0.1$,$\\lambda_{6}=0.5$ | 540x     | 77.4 | 65.0 |\n| 3DETR  | $\\lambda_{1}=1$,$\\lambda_{2}=5$,$\\lambda_{3}=1$,$\\lambda_{4}=2$,$\\lambda_{5}=0.1$,$\\lambda_{6}=0.5$   | $\\lambda_{1}=2$,$\\lambda_{2}=0$,$\\lambda_{3}=0$,$\\lambda_{4}=1$,$\\lambda_{5}=0$,$\\lambda_{6}=0$       | 540x     | 77.0 | 64.7 |\n\nAccording to the above comparison experiments, we can see that **the significant differences in the choices of loss functions and matching cost functions only bring slight performance gains**. We will include these results in the final revision.\n\n> **Q5. Why some ablations are done with ScanNet while others use SUN-RGBD (e.g., Table 4)? Does it means the coordinates normalization works similarly on ScanNetv2?**\n\nThe primary reason is that the ground-truth rotation angle annotations for all 3D bounding boxes in ScanNet are set to zero. Hence, the normalization of coordinates does not change the relative coordinates. We will incorporate this information into the final revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555347748,
                "cdate": 1700555347748,
                "tmdate": 1700621390173,
                "mdate": 1700621390173,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pIHnt3V0E3",
                "forum": "NDkpxG94sF",
                "replyto": "7gC9krwExR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TFsP (2/3)"
                    },
                    "comment": {
                        "value": "> **Q6. Actually I think the RPE and normalized coords are designed in similar ways: Point-RCNN has adopted to convert box to canonical coords and do normalizations on oritentaion. Moreover, in anchor-based detectors, they already use the anchor boxes' W and H to normalize the regression targets. Here the author uses dynamic bounding boxes from predictions, which has also been explored in methods like MetaAnchor, etc.**\n\nPlease refer to the general response. We will add references and discussions on connections to acknowledge related efforts in the final revision. We sincerely hope that our work will not be rejected solely because some aspects of our V-DETR share similar insights with previous studies.\n\n> **Q7. So many un-relavent tricks to improve the detection performance. I don't like the way to do whatever it can to improve the results. Rather, the author should focus on the main contribution. After the core module being sufficiently discussed, one can further improve its results with more tricks. Here the author put all stuff together, which makes me doubt where the improvement come from.**\n\nGood point! We would like to improve the organization of the experiments for clarity. Additionally, we have applied our 3DV-RPE to several other clean baselines, including 3DETR and GroupFree3D. Please refer to the detailed results in the general response\n\n> **Q8. Most of the references are before year 2023, so I think more recent works in year 2023 should be included.**\n\nGreat point! We have carefully revisited the related works from 2023 and summarized a list of their results as follows. We would appreciate any additional valuable comments on the related works from 2023 that we may have missed. **We have added references and comparisons with these works in Table 1 of the revised PDF.**\n\n| results in ScannetV2    | AP25 | AP50 |\n| ----------------------- | ---- | ---- |\n| VDETR(no TTA)           | 77.4 | 65.0 |\n| Point-GCC * [1]         | 73.1 | 59.6 |\n| Uni3DETR [2]            | 71.7 | 58.3 |\n| AShapeFormer[3]         | 71.1 | 56.6 |\n| Swin3D(no pretrain) [4] | 73.3 | 58.6 |\n| Swin3D**                | 76.4 | 63.2 |\n\n| results in SUN RGB-D | AP25 | AP50 |\n| -------------------- | ---- | ---- |\n| VDETR(no TTA)        | 67.5 | 50.4 |\n| Point-GCC* [1]       | 67.7 | 51.0 |\n| Uni3DETR [2]         | 67.0 | 50.3 |\n| AShapeFormer[3]      | 62.2 | -    |\n| OctFormer [5]        | 66.2 | 50.6 |\n| ConDaFormer[6]       | 67.1 | 49.9 |\n\n*use self-supervised pretrainning\n\n**use Extra data pretrainning\n\n[1] Point-GCC: Universal Self-supervised 3D Scene Pre-training via Geometry-Color Contrast, arXiv 2023\n\n[2] Uni3DETR: Unified 3D Detection Transformer, NeurIPS 2023\n\n[3] AShapeFormer: Semantics-Guided Object-Level Active Shape Encoding for 3D Object Detection via Transformers, CVPR 2023\n\n[4] Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding, arXiv 2023\n\n[5] OctFormer: Octree-based Transformers for 3D Point Clouds, arXiv 2023\n\n[6] ConDaFormer: Disassembled Transformer with Local Structure Enhancement for 3D Point Cloud Understanding, NeurIPS 2023\n\n> **Q9. How much fraction does the queries outside predicted bounding boxes account for with respect to the total number of queries? 10%? 20%? The author should provide a investigation to this problem.**\n\nWe guess you're referring to the fraction of queries localized outside the ground-truth bounding boxes, rather than within the predicted bounding box. Each query will predict a center and dimensions, ensuring that it is localized at the center position of the predicted bounding box.\n\nWe compile statistics on the fraction of queries that fall outside the predicted bounding boxes in relation to all queries as follows:\n\n|                                                           | \\#8-th decoder layer | \\#6-th decoder layer | \\#4-th decoder layer | \\#2-th decoder layer | first-stage |\n| --------------------------------------------------------- | -------------------- | -------------------- | -------------------- | -------------------- | ----------- |\n| \\# of the queries outside ground-truth box/\\# all queries | 24.77%               | 25.62%               | 27.53%               | 32.79%               | 38.21%      |\n\nIn addition, we also compile statistics on the fraction of queries localized outside the ground-truth bounding boxes, considering only the matched queries selected by Hungarian matching as follows:\n\n|                                                                       | \\#8-th decoder layer | \\#6-th decoder layer | \\#4-th decoder layer | \\#2-th decoder layer | first-stage |\n| --------------------------------------------------------------------- | -------------------- | -------------------- | -------------------- | -------------------- | ----------- |\n| \\# of the matched queries outside ground-truth box/\\# matched queries | 1.14%                | 1.77%                | 2.64%                | 4.26%                | 5.92%       |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555417677,
                "cdate": 1700555417677,
                "tmdate": 1700619712803,
                "mdate": 1700619712803,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n0PFdFkf2A",
                "forum": "NDkpxG94sF",
                "replyto": "7gC9krwExR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TFsP (3/3)"
                    },
                    "comment": {
                        "value": "> **Q10. Why the non-linear functions is designed in this way? how it is derived? is their any insights to do so?**\n\nWe have reported the detailed comparison results of different non-linear functions in Table 2. Accordingly, we observe consistent gains with different non-linear functions.\n\nThe key insight is that, due to the points being primarily distributed on the surface of the 3D object, the points near the surface of the object tend to hold the most important semantic information. This information can determine the category and size of a prediction box. Therefore, **relative position encoding should be more sensitive to smaller spatial changes than to larger changes in the 3D point cloud space**.\n\nTherefore, **the non-linear function should be capable of magnifying small changes in relative coordinates**. We will include this information in the final revision.\n\n> **Q11. Why does the PE is added in the way in Eq. 1? I think the form of matmul(Q, K) + R does not match the intended aim of the paper. Instead, I think matmul(Q+R, K) should be more proper? or add a relative PE to Q and a global PE to K?**\n\nWe address your concerns on the PE scheme as follows:\n\n- First, we need to clarify that relative position encoding should explicitly encode the relative spatial relationships between each query and each key. Therefore, by default, the shape of the RPE should be $nQ\\times nK$. Our implementation that directly adds RPE to matmul(Q, K) is the most intuitive one.\n- Second,  the motivation for adding the RPE to matmul(Q, K) is to explicitly modulate the pairwise cross-attention map values between the 3D bounding box query and each of the 3D point clouds. In other words, the key (and value) point clouds localized in the query 3D boxes will receive larger values and smaller values otherwise.\n\nTherefore, simply adding the PE to a query or the key essentially results in absolute PE instead of relative PE. We also present the results **(w/o TTA)** with absolute PE, which adds PE to both query and key, in the following table. As you can see, the absolute PE does not perform well.\n\n| method                  | epoch | AP25 | AP50 |\n| ----------------------- | ----- | ---- | ---- |\n| Baseline + abosolute PE | 540x   | 70.7 | 47.1 |\n| Baseline + 3DV-RPE      | 540x   | 77.4 | 65.0 |\n\nIn summary, the proposed RPE scheme provides a natural and effective implementation. We will include the above information in the final revision if necessary.\n\nWe would also like to invite the reviewer to take another look at an insightful study[1] on the relative position encoding scheme for Vision Transformers. Their experiments indicate superior performance of both the bias and contextual modes. **Our 3DV-RPE can be considered a effective designed version of the bias mode, specifically tailored for 3D object detection tasks.**\n\n[1] Rethinking and Improving Relative Position Encoding for Vision Transformer, ICCV 2021"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555485090,
                "cdate": 1700555485090,
                "tmdate": 1700584649426,
                "mdate": 1700584649426,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BRIjQqk5VD",
                "forum": "NDkpxG94sF",
                "replyto": "7gC9krwExR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2646/Reviewer_TFsP"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewer_TFsP"
                ],
                "content": {
                    "title": {
                        "value": "Final"
                    },
                    "comment": {
                        "value": "Thank authors for the detailed responses. \n\nAfter reviewing the rebuttal and the revised paper, it still cannot convince me on questions Q4, Q6, Q7 and Q9. Besides, the novelty of this paper is not enough to me: similar operations has been introduced in previous methods.\n\nI suggest the authors to re-orgainze the paper to foucs more on the main contribution, and do more work to make it distinct from previous methods. \n\nI strongly recommend that authors remove TTA and some very tricky operations, because this will cause other authors to have to spend more effort on doing everything possible to improve performance, rather than seek for what really matters.\n\nAs a result, I'd like to keep my score unchanged."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643073099,
                "cdate": 1700643073099,
                "tmdate": 1700643178435,
                "mdate": 1700643178435,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FAAjUOsPZg",
                "forum": "NDkpxG94sF",
                "replyto": "7gC9krwExR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the Response of Reviewer TFsP"
                    },
                    "comment": {
                        "value": "We are grateful for the reviewer's swift response. \ud83e\udd17\n\n\ud83d\udc49 First, we are seeking further clarification on the \"similar operations\" introduced in previous methods. As highlighted in our general response, the fundamental contribution of our work is the novel 3DV-RPE and its efficient variant, which has significantly enhanced the AP50 performance across an array of baselines. **To the best of our knowledge, the 3DV-RPE scheme, which explicitly models the spatial relations between arbitrary points and the eight vertices of a 3D bounding box, is an innovative approach unexplored in prior studies.**\n\n\ud83d\udc49 Second, we welcome any specific suggestions that could assist us in addressing questions Q4, Q6, Q7, and Q9 more effectively.\n\n\ud83d\udc49 Last, we would like to emphasize that **our V-DETR maintains superior performance even without the application of TTA**. Other operations are primarily inspired by well-established practices in the DETR-based 2D object detection systems. We are committed to releasing the source code, aiming to aid other researchers in focusing their efforts on elements of true significance.\n\nWe greatly appreciate your invaluable and insightful comments once again!\ud83e\udd17"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645355640,
                "cdate": 1700645355640,
                "tmdate": 1700645433541,
                "mdate": 1700645433541,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kecfdT7r8R",
            "forum": "NDkpxG94sF",
            "replyto": "NDkpxG94sF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2646/Reviewer_c9AZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2646/Reviewer_c9AZ"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript proposes a 3d object-detection-specific position encoding method that significantly improves performance for 3d DETR-like object detection.\nThe 3d position encoding encodes the relative position of key, value points to the object query points to allow the transformer to learn to attend to points inside the object bounding box more easily.\nWith the addition of the position encoding and some other tweaks, the proposed method, V-DETR, outperforms CNN-based methods. A first for transformer-based detectors in 3d."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The proposed approach for relative position encoding is intuitive (and illustrated well in Fig 1), and experiments clearly show that it leads to a big improvement for Transformer-based methods and leads to a new state of the art wrt. to CNN-based methods as well. \n\nOverall the quality of writing and illustration is very high. The detailed pipeline visualization clearly shows the recurrent nature of the approach. The visualization of the attention for each of the corners is also very illustrating. \n\nThe manuscript pays attention to practical aspects as well: The use of a precomputed lookup table for the relative PE is a nice and practical way to safe valuable GPU memory.\n\nThe experiments are expansive and convincing. The ablations do help clarify the different choices of the hyperparameters."
                },
                "weaknesses": {
                    "value": "The precomputed lookup table was the hardest to follow (Eq 3) since the connection to Eq 4 was not immediately obvious. One more sentence there to explicitly connect the two would be helpful. I.e. T represents a discretized set of possible \\Delta P that we interpolate into."
                },
                "questions": {
                    "value": "Page 5 has a broken figure reference.\n\nI dont understand how T in Eq(3) is initialized/set? What range do the T values take? -5 to 5 as indicated by the signed-log function?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794577609,
            "cdate": 1698794577609,
            "tmdate": 1699636204805,
            "mdate": 1699636204805,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Sgsz3qZbt3",
                "forum": "NDkpxG94sF",
                "replyto": "kecfdT7r8R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission2646/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2646/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer c9AZ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the careful reviews and constructive suggestions. We answer the questions as follows.\n\n> **Q1. The precomputed lookup table was the hardest to follow (Eq 3) since the connection to Eq 4 was not immediately obvious. One more sentence there to explicitly connect the two would be helpful. I.e. T represents a discretized set of possible \\Delta P that we interpolate into.**\n\nThanks for pointing out this issue and providing great suggestions! **We have followed your suggestions to add the mentioned sentence between Eq. 3 and Eq. 4 (in the updated PDF: between Eq. 2 and Eq. 5) to make it clearer in the revised PDF.**\n\nAdditionally, following the comments from Reviewer fSn3, we've noted that the explanation could be further clarified by discussing \"3DV-RPE\" prior to introducing the concept of the \"canonical object space\".\n\n> **Q2. Page 5 has a broken figure reference.**\n\nThanks for pointing out this issue! We will fix the typo \"fig:rotatedRPE\" (refers to Figure 4) in the final revision.\n\n> **Q3. I dont understand how T in Eq(3) is initialized/set? What range do the T values take? -5 to 5 as indicated by the signed-log function?**\n\n\ud83d\udc49 The $\\mathbf{T}$ is initialized/set with a 3D grid of size $10\\times10\\times10$. Each value is a 3-d vector and the values at position $(i,j,k)$ is computed by $\\mathbf{T}[i, j, k] = (\\frac{\\mathrm{max value}(2i-(\\mathrm{table size}-1))}{\\mathrm{tablesize}-1},\\frac{\\mathrm{maxvalue}(2j-(\\mathrm{tablesize}-1))}{\\mathrm{tablesize}-1},\\frac{\\mathrm{maxvalue}(2k-(\\mathrm{tablesize}-1))}{\\mathrm{tablesize}-1})$.  (Eq(3) has been reordered to Eq(5) in the updated PDF)\n\n\ud83d\udc49 Yes, the $\\mathbf{T}$ values are within the range of -5 to 5. We also provide a concise python implementation of the $\\mathbf{T}$ initialization as follows:\n\n```python\nMAX_VALUE = 5\nTABLE_SIZE = 10\n\n# the initialization of T\nrelative_coords_table = torch.stack(torch.meshgrid(\n    torch.linspace(-MAX_VALUE, MAX_VALUE, TABLE_SIZE, dtype=torch.float32),\n    torch.linspace(-MAX_VALUE, MAX_VALUE, TABLE_SIZE, dtype=torch.float32),\n    torch.linspace(-MAX_VALUE, MAX_VALUE, TABLE_SIZE, dtype=torch.float32),\n), dim=-1).unsqueeze(0)\n\nself.register_buffer(\"relative_coords_table\", relative_coords_table)          \n```\n\nWe create and initialize $\\mathbf{T}$ with the meshgrid function and linspace function in pytorch. **We would like to add the above source code of the efficient implementation of 3DV-RPE in the final revision to make it clearer. We also ensure to release the source code of our approach after the paper is accepted.**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554970589,
                "cdate": 1700554970589,
                "tmdate": 1700621768380,
                "mdate": 1700621768380,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hP0BnMft6o",
                "forum": "NDkpxG94sF",
                "replyto": "Sgsz3qZbt3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2646/Reviewer_c9AZ"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission2646/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2646/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewer_c9AZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifications by the Authors. They address my questions well.\nI have also read the reviews of the other reviewers. I think even though the local coordinate system encoding may have been used in other works, it has not been used and shown effective for 3D-DETR.\nThe modification to the original DETR may be \"small\" but the effect is significant. To me this is actually an advantage rather than a reason for reject. \nIll stick with accept for the paper."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679498169,
                "cdate": 1700679498169,
                "tmdate": 1700679498169,
                "mdate": 1700679498169,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vnOtCGrbh9",
            "forum": "NDkpxG94sF",
            "replyto": "NDkpxG94sF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2646/Reviewer_YhUd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2646/Reviewer_YhUd"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents one improvement over DETR-based methods for 3D object detection. The idea is first to predict a coarse bounding box and then only allow attention weights to be learned within the bounding box for better-using locality as one important inductive bias for 3D object detection. Experiments are done on the ScanNetV2 and Sun RGB-D benchmarks. Results show improved performance over baselines. Extensive ablation studies are also presented."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- the proposed method improves over state-of-the-art methods on ScanNetV2 and Sun RGB-D.\n- the proposed modification to the DETR-based method is valid and reasonable.\n- ablation studies are solid."
                },
                "weaknesses": {
                    "value": "- the proposed method is more like just a small fix to DETR-based method.\n- the proposed fix is also specific to DETR-based backbones.\n- the proposed fix may be vulnerable if the first stage predicting the coarse bounding boxes fail. For example, if the bounding boxes are very off, then preventing later layers to attend to out-of-the-box regions may make it impossible to recover. \n- the paper writing can be improved. For example, the figure layouts are quite messy. The organization for Sec. 3.1 and 3.2 is a bit hard to follow. It looks like Sec. 3.1 focuses on laying out the basic pipeline of DETR and Sec. 3.2 discusses more into the contributions of the paper, but actually the content are mixed together.\n-  there are also claims that are unsupported in the paper. For example, the sentence in the introduction section \"We attribute the discrepancy to the limited scale of training data available for 3D object detection\" is not well supported. Can you use less data to train 2D detectors to show it's really the data scale issue?"
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698875006425,
            "cdate": 1698875006425,
            "tmdate": 1699636204734,
            "mdate": 1699636204734,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eEuMYl5dLL",
                "forum": "NDkpxG94sF",
                "replyto": "vnOtCGrbh9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2646/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2646/Authors",
                    "ICLR.cc/2024/Conference/Submission2646/Area_Chairs",
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YhUd"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the careful reviews and constructive suggestions. We answer the questions as follows.\n\n> **Q1. the proposed method is more like just a small fix to DETR-based method.**\n\nRefer to the general response.\n\n> **Q2. the proposed fix is also specific to DETR-based backbones.**\n\nGood point! We want to point out that DETR-based methods have dominated the SOTA in 2D detection and 3D outdoor detection. Therefore, the proposed fix shows that the 3DV-RPE is key to building the SOTA 3D indoor detection system with the DETR-based method. We would also like to explore how to apply the proposed method to non-DETR-based methods in the future, based on your further valuable comments.\n\n\n> **Q3. the proposed fix may be vulnerable if the first stage predicting the coarse bounding boxes fail. For example, if the bounding boxes are very off, then preventing later layers to attend to out-of-the-box regions may make it impossible to recover.**\n\nGood point! However, we have empirically found that this concern might not be valid, as we explain in detail in the following response.\n\nFirst, we have compiled statistics on the ratio of (matched) first-stage predictions that fall outside the ground-truth bounding boxes. We found that **only 4.35% of the matched proposal boxes have their center points outside the target box, and a mere 0.06% have zero intersection over union (IoU) with the target box at all**. This is a relatively rare occurrence. **The reason is that DETR-based methods apply the Hungarian matching technique to select the top-ranking high-quality predictions based on cost functions that measure the accuracy of bounding box localization.** Therefore, the (matched) first-stage predictions are generally accurate enough to be used as input for the second stage.\n\nSecond, we have also compiled statistics on the ratio of (matched) predictions after refinement through subsequent transformer decoder layers. The proportion of refined predictions with their center points outside the target box decreases from 4.35% to 1.14%, and those with zero IoU at all drop to 0.02%. This indicates that our 3DV-RPE is capable of recovering low-quality first-stage predictions. The key reason is that **our 3DV-RPE can also capture useful long-range contextual information outside the box.** We have conducted a detailed analysis in the Section \"Comparison with 3D box mask\" (page 7) and in Table 6 (page 9).\n\nWe will conduct further studies if you can provide additional valuable comments.\n\n> **Q4. the paper writing can be improved. For example, the figure layouts are quite messy. The organization for Sec. 3.1 and 3.2 is a bit hard to follow. It looks like Sec. 3.1 focuses on laying out the basic pipeline of DETR and Sec. 3.2 discusses more into the contributions of the paper, but actually the content are mixed together.**\n\nGreat suggestion! We would like to improve the layout of the figures and restructure the organization of Sec. 3.1 and Sec. 3.2 to make it clearer.\n\n> **Q5. there are also claims that are unsupported in the paper. For example, the sentence in the introduction section \"We attribute the discrepancy to the limited scale of training data available for 3D object detection\" is not well supported. Can you use less data to train 2D detectors to show it's really the data scale issue?**\n\nThanks for pointing out this issue! We agree this claim is not well supported and we would like to clarify it as follows.\n \n\ud83d\udc49 Our primary point is that **the scale of 3D object detection data is significantly smaller compared to that of 2D object detection tasks.** For instance, the number of different scenes (representing valid training samples) in ScanNetv2 is only 1,513, while the number of different images in COCO exceeds 110,000, rendering it approximately 70 times larger. We hypothesize that the limited amount of training data may complicate the learning of inductive biases, including locality.\n \n\ud83d\udc49 Furthermore, we conduct experiments on the 2D detector DETR, using approximately 1% of the training data (1,200 images), following your suggestion. We train DETR[1] for the same number of iterations (2,217,881 iterations) as the original DETR, which was trained on the full dataset for 300 epochs, while maintaining a batch size of 16. First, we observe that the mAP of the validation set drops from 44.9% to 10.4%, which closely aligns with the performance reported in Table 5 of [2]. Second, in accordance with Figure 6 of the DETR paper, **we visualize the cross-attention maps for the predicted object in Figure 11 of the revised PDF**. **We note that these attention maps fail to focus on local object regions, especially the object extremities. Therefore, it is evident that the scale of data has a substantial impact on the model's ability to effectively learn the locality inductive biases.**\n\n[1] End-to-End Object Detection with Transformers, ECCV 2020\n\n[2] DETReg: Unsupervised Pretraining with Region Priors for Object Detection, CVPR2022"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554534529,
                "cdate": 1700554534529,
                "tmdate": 1700660296429,
                "mdate": 1700660296429,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]