[
    {
        "title": "Tangent Transformers for Composition,Privacy and Removal"
    },
    {
        "review": {
            "id": "0BKQDybnpp",
            "forum": "VLFhbOCz5D",
            "replyto": "VLFhbOCz5D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6437/Reviewer_ZQ32"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6437/Reviewer_ZQ32"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a method for fine-tuning linearized transformers, called Tangent Attention Fine-Tuning (TAFT). \n\nThe paper claims that TAFT can achieve comparable performance to non-linear fine-tuning on various downstream tasks while enjoying the benefits of linearity such as compositionality, parallel training, machine unlearning, and differential privacy. \n\nThe paper also introduces Tangent Transformers, which are linearized versions of pre-trained transformer models, and shows how to compute their Jacobian-Vector products efficiently in a single forward pass. \n\nThe paper demonstrates the advantages of TAFT and Tangent Transformers in several experiments using vision transformer models. \n\nTo summarize, the paper's main contributions are:\n\n- A novel method for fine-tuning linearized transformers that is computationally efficient and competitive with non-linear fine-tuning.\n\n- A theoretical analysis of the benefits of linearity for model composition, parallel training, machine unlearning, and differential privacy.\n\n- An empirical evaluation of TAFT and Tangent Transformers on various downstream visual classification tasks using vision transformer models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper proposes a novel method for fine-tuning linearized transformers, called Tangent Attention Fine-Tuning (TAFT), which is computationally efficient and competitive with non-linear fine-tuning. The paper also introduces Tangent Transformers, which are linearized versions of pre-trained transformer models, and shows how to compute their Jacobian-Vector products efficiently in a single forward pass. The paper demonstrates the advantages of TAFT and Tangent Transformers on several experiments using vision transformer models.\n\nIn terms of originality, the paper introduces a new method for fine-tuning linearized transformers that is computationally efficient and competitive with non-linear fine-tuning. The paper also introduces Tangent Transformers, which are linearized versions of pre-trained transformer models, and shows how to compute their Jacobian-Vector products efficiently in a single forward pass. These contributions are novel and have not been explored before.\n\nIn terms of quality, the paper provides a theoretical analysis of the benefits of linearity for model composition, parallel training, machine unlearning, and differential privacy. The paper also provides an empirical evaluation of TAFT and Tangent Transformers on various downstream visual classification tasks using vision transformer models. The experiments are well-designed and the results are presented clearly.\n\nIn terms of clarity, the paper is well-written and easy to follow. The authors provide clear explanations of their methods and results. The paper also includes visualizations that help to illustrate the concepts presented.\n\nIn terms of significance, the paper\u2019s contributions have important implications for the field of machine learning. The proposed method for fine-tuning linearized transformers is computationally efficient and competitive with non-linear fine-tuning. This has important implications for large-scale applications where computational efficiency is critical. Additionally, the theoretical analysis of the benefits of linearity has important implications for model composition, parallel training, machine unlearning, and differential privacy.\n\nOverall, this is a well-written paper that makes significant contributions to the field of machine learning."
                },
                "weaknesses": {
                    "value": "(1) While the paper is well-written and easy to follow, it would be helpful to provide more detailed explanations of some of the concepts presented. For example, the paper could provide more details on how Tangent Transformers are computed and how they are used in practice (especially from Eq 5 to Eq 6).\n\n(2) While the paper provides an empirical evaluation of TAFT and Tangent Transformers on various downstream visual classification tasks using vision transformer models, it would be helpful to see more experiments that compare TAFT with other fine-tuning methods on these tasks. This would help to establish the competitiveness of TAFT more clearly.\n\n(3) While the paper provides a theoretical analysis of the benefits of linearity for model composition, parallel training, machine unlearning, and differential privacy, it would be beneficial to provide more empirical evidence to support these claims. Specifically, it would be helpful to see more experiments that demonstrate the advantages of TAFT and Tangent Transformers on a wider range of tasks and datasets.\n(e.g. DTD and UCF101).\n\nOverall, this is a well-written paper that makes some contributions to the field. However, there is room for improvement in terms of providing more empirical evidence to support the claims made in the paper and providing more detailed explanations of some of the concepts presented."
                },
                "questions": {
                    "value": "(1) Please give a detailed derivation process from Eq 5 to Eq 6 in the appendix.\n\n(2) The selected datasets (Caltech-256 (Griffin et al., 2007), MIT-67 (Quattoni & Torralba, 2009), Oxford Pets (Parkhi et al.,\n2012), Stanford Dogs (Khosla et al., 2011), CUB-200 (Wah et al., 2011), FGVC-Aircrafts (Maji\net al., 2013), and Stanford Cars (Krause et al., 2013)) are object-oriented. What about the performance of the proposed method on Describable Textures (DTD) (Cimpoi et al., 2014)\uff1f \n\n(3) (Optional to reply) Could the proposed method be applied to temporal classification tasks, such as activity classification on UCF101?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6437/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698606752097,
            "cdate": 1698606752097,
            "tmdate": 1699636718559,
            "mdate": 1699636718559,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eD0pYDShNR",
                "forum": "VLFhbOCz5D",
                "replyto": "0BKQDybnpp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6437/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZQ32"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful review of our paper. We address each concern in detail below.\n\n> **While the paper is well-written and easy to follow, it would be helpful to provide more detailed explanations of some of the concepts presented.**\n\n> **Please give a detailed derivation process from Eq 5 to Eq 6 in the appendix.**\n\nThank you for the suggestion, we clarified some of the notation in Sec 3.1, and added the detailed derivation of the linearized attention function in Appendix B as suggested. We also plan to release code with the full implementation with the final version of the paper.\n\n> **While the paper provides an empirical evaluation of TAFT and Tangent Transformers on various downstream visual classification tasks using vision transformer models, it would be helpful to see more experiments that compare TAFT with other fine-tuning methods on these tasks. This would help to establish the competitiveness of TAFT more clearly.**\n\nThank you for the suggestion. We initially compared only to the best performing fine-tuning method \u2013 non-linear fine-tuning \u2013 since parameter-efficiency is not necessary in our present application. Following the reviewer's suggestion, we added comparisons against adapter and low-rank fine-tuning in Appendix C.7, Table 11, and show that as a result of the trade-off in number of parameters, their performance on downstream tasks are consistently lower than that of non-linear fine-tuning and TAFT.\n\n> **Specifically, it would be helpful to see more experiments that demonstrate the advantages of TAFT and Tangent Transformers on a wider range of tasks and datasets. (e.g. DTD and UCF101).**\n\n> **What about the performance of the proposed method on Describable Textures (DTD) (Cimpoi et al., 2014)\uff1f**\n\nWe believe that our experiments have provided sufficient evidence across 7 different datasets across multiple tasks - composition, unlearning, privacy, and standard fine-tuning. However, we agree that evaluating on non-object-oriented datasets can better elucidate the advantages of TAFT. As such, we have evaluated our proposed method on the DTD dataset in Appendix C.6 and Table 10, where we show TAFT convincingly outperforms the non-linear fine-tuning paragon.\n\n> **(Optional to reply) Could the proposed method be applied to temporal classification tasks, such as activity classification on UCF101?**\n\nWhile indeed interesting, extension to video-based tasks such as UCF101 is beyond the scope of our paper. However, we do not see an immediate reason why our method would not extend to temporal classification (as long as architecture remains transformer-style), and would love to see future work exploring this application."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699988366713,
                "cdate": 1699988366713,
                "tmdate": 1700019364187,
                "mdate": 1700019364187,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yETZQstXN5",
            "forum": "VLFhbOCz5D",
            "replyto": "VLFhbOCz5D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6437/Reviewer_QE2Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6437/Reviewer_QE2Q"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce Tangent Attention Fine-Tuning (TAFT) for fine-tuning linearized transformers. It can perform comparably with fine-tuning the original non-linear network in various downstream visual classification tasks. It enjoys several advantages compared to non-linear fine-tuning when it comes to model composition, parallel training, machine unlearning, and differential privacy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is the first work to propose an efficient method to linearize models in the Transformer family of architectures, which is meaningful. The paper is clearly written with many experiments."
                },
                "weaknesses": {
                    "value": "1.since this is a fine-tuning method, please provide more fine-tuning methods for comparison (lora,adapter...) in table 3. \n2.I wonder how TAFT works when applied to LLM? maybe some experiments can be added.\n3.please derive in detail how to get the closed form expression in equation 5 and 6."
                },
                "questions": {
                    "value": "see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6437/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762038607,
            "cdate": 1698762038607,
            "tmdate": 1699636718412,
            "mdate": 1699636718412,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CDunaHkZjF",
                "forum": "VLFhbOCz5D",
                "replyto": "yETZQstXN5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6437/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QE2Q"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback and suggestions. We respond to each comment below.\n\n> **since this is a fine-tuning method, please provide more fine-tuning methods for comparison (lora,adapter...) in table 3**\n\nSince we are not constrained by parameter usage, we used the best fine-tuning method available for comparison - full non-linear fine-tuning. The strengths of the suggested methods lie in parameter efficiency and training speed, and comparing their performance on downstream tasks would not be completely fair for them. Furthermore, since Table 3 is meant to illustrate the advantages for differential privacy, we believe that comparing to the suggested fine-tuning methods in a more general application such as model composition can better address the reviewer\u2019s concern. Following the reviewer\u2019s suggestion, we added a comparison in Appendix C.7 where we show that non-linear fine-tuning and TAFT both significantly outperform parameter-efficient variants.\n\n> **I wonder how TAFT works when applied to LLM? maybe some experiments can be added.**\n\nWhile we recognize the potential of extending our work to large language models (LLMs), we believe it falls outside the scope of our current research, which focuses on the composition and decomposition of vision transformer networks. Nevertheless, we appreciate the reviewer's suggestion and agree that applying TAFT to LLMs and other modalities would be a valuable area of future research.\n\n> **please derive in detail how to get the closed form expression in equation 5 and 6.**\n\nThank you for the suggestion, we added the derivation to Appendix B. Along with this, we will also release the code for TAFT with the final version of the paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699987742981,
                "cdate": 1699987742981,
                "tmdate": 1699987742981,
                "mdate": 1699987742981,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "N7SJCz1jaB",
            "forum": "VLFhbOCz5D",
            "replyto": "VLFhbOCz5D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6437/Reviewer_Ef2Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6437/Reviewer_Ef2Q"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning linearized transformers that are derived by computing a First-order Taylor Expansion around a pre-trained initialization. The key contributions of the paper are as follows:\n\nEfficient Jacobian-Vector Product Calculation: The authors demonstrate that the Jacobian-Vector Product resulting from linearization can be efficiently computed in a single forward pass. This reduces the training and inference costs of linearized transformers to a similar order of magnitude as the original non-linear models, all while maintaining the same number of parameters.\n\nTAFT presents an efficient and effective method for fine-tuning linearized transformers obtained through Taylor Expansion. It maintains performance parity with non-linear models while providing advantages related to model composition, training efficiency, unlearning, and privacy. This has significant implications for the practical use of transformers in various downstream tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Efficient Jacobian-Vector Product Calculation: The authors demonstrate that the Jacobian-Vector Product resulting from linearization can be efficiently computed in a single forward pass. This reduces the training and inference costs of linearized transformers to a similar order of magnitude as the original non-linear models, all while maintaining the same number of parameters.\n\n2. Comparable Performance: When applied to various downstream visual classification tasks, the Tangent Transformer fine-tuned with TAFT performs on par with fine-tuning the original non-linear network. This suggests that the linearized version is a viable alternative without compromising performance.\n\n3. Convex Fine-Tuning Loss: The paper highlights that Tangent Transformers are linear concerning a new set of weights, resulting in a convex fine-tuning loss. This convexity offers several advantages over non-linear fine-tuning, particularly in terms of model composition, parallel training, machine unlearning, and differential privacy."
                },
                "weaknesses": {
                    "value": "1. In Section 3.4, the author mentions basically the same things as in Section 2 Related work-Pravicy with no new theoretical analysis about differential privacy."
                },
                "questions": {
                    "value": "1. How about interpretability? A detailed analysis of how a given training sample affects the learned model and the predicted results is given in LQF. Is it possible for the authors to provide an analysis of the interpretability?\n2. The authors detail the advantages of linear models in the introduction, but these advantages don't seem to be relevant to transformer, what are the advantages and disadvantages of linearizing the transformer model compared to linearizing ResNet?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6437/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6437/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6437/Reviewer_Ef2Q"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6437/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768476726,
            "cdate": 1698768476726,
            "tmdate": 1699636718301,
            "mdate": 1699636718301,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kJfT7bcB5B",
                "forum": "VLFhbOCz5D",
                "replyto": "N7SJCz1jaB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6437/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Ef2Q"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback and suggestions. We would like to provide a detailed response to each comment.\n\n> **In Section 3.4, the author mentions basically the same things as in Section 2 Related work-Privacy with no new theoretical analysis about differential privacy.**\n\nSection 2 provides comprehensive references to related work on differential privacy relevant to our paper. In Section 3.4, we establish a connection between differential privacy and TAFT. Existing theoretical works have proved (with tight theoretical bounds) that linear models (especially strongly convex, which TAFT is) enjoy much better utility guarantees compared to their non-linear counterparts when trained with DP-SGD. This makes TAFT an ideal candidate for DP fine-tuning, as it ensures that the model is differentially private along with strong utility guarantees. Since the theoretical results for strongly convex models are already tight, our focus is on developing such an architecture which can improve the utility of transformer models for challenging downstream tasks, such as fine-grained image classification.\n\n> **How about interpretability? A detailed analysis of how a given training sample affects the learned model and the predicted results is given in LQF. Is it possible for the authors to provide an analysis of the interpretability?**\n\nThank you for the great suggestion! LQF requires a quadratic loss function to derive a closed form solution (via newton update) with and without the given training sample. This requires the computation of the inverse Hessian, which is often infeasible for large networks and requires approximation. Instead, our method of linearly composing disjoint component models to form the final model allows computing the influence of any particular component model (subset of the training samples) simply via measuring the difference in weights between the target component model and the composition (weight average) of the remaining models. \n\nAs suggested, we added Appendix C.5, Figure 4, which demonstrates that as the number of model shards increase, the influence of each individual component model decreases. Due to linearity, this effect is also reflected in the model output space as seen in Figure 1(a). \n\n> **The authors detail the advantages of linear models in the introduction, but these advantages don't seem to be relevant to transformer, what are the advantages and disadvantages of linearizing the transformer model compared to linearizing ResNet?**\n\nOur comparison with linearized ResNet-50 (Appendix C.1, Table 7) demonstrates TAFT's superior performance, outperforming ResNet-50 by 7.3% and 9.0% for standard fine-tuning and composition tasks across three datasets, respectively. The advantages of linearization stem from the effectiveness of the inductive prior acquired during pre-training. Since vision transformers are shown to learn better inductive priors than convolutional architectures as the scale of training data increases, we believe that linearized transformers yield a clear advantage over linearized ResNets by being able to leverage the better inductive priors learnt from pre-training.\n\nFurther investigation reveals that linearizing transformer models around the full pre-training weights can introduce strong feature biases towards the source pre-training dataset, potentially hindering transferability to downstream tasks, particularly in later layers (Figure 2(b)). Additionally, vision transformers often incorporate a [CLS] token for classification tasks. For downstream tasks significantly further from the pre-training initialization, we found that linearizing the [CLS] token itself can lead to improved performance (Figure 2(c))."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699987439344,
                "cdate": 1699987439344,
                "tmdate": 1699987540483,
                "mdate": 1699987540483,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9npIYMWJ5j",
                "forum": "VLFhbOCz5D",
                "replyto": "kJfT7bcB5B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6437/Reviewer_Ef2Q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6437/Reviewer_Ef2Q"
                ],
                "content": {
                    "title": {
                        "value": "Excellent response"
                    },
                    "comment": {
                        "value": "The author addressed all my questions point by point. After viewing other reviewer's comments and the author's responses, I think this paper deserves to be accepted."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634165749,
                "cdate": 1700634165749,
                "tmdate": 1700634165749,
                "mdate": 1700634165749,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]