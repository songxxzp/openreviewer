[
    {
        "title": "Mitigating Estimation Errors By Twin TD-Regularized Actor and Critic for Deep Reinforcement Learning"
    },
    {
        "review": {
            "id": "yvMKR2j1GO",
            "forum": "Ydlfehfvge",
            "replyto": "Ydlfehfvge",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5958/Reviewer_SnPL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5958/Reviewer_SnPL"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the \"twin TD-regularized actor- critic (TDR) method\" which chooses the target Q-values in temporal difference (TD) learning based on the Q-function attaining the lower TD error, and penalizes the actor network based on the chosen actions' TD errors. This method is combined with priorly proposed practices such as distributional critic functions and LNSS, a slight modified version of n-step returns. The authors evaluate these combined methods on top of  the TD3, SAC, and D4PG algorithms in six environments from the DeepMind Control suite."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) Finding the right balance between pessimism and optimism in TD-based algorithms is a relevant problem that has been the focus of much recent work.\n\n2) The methodological contribution is simple and easy to understand.\n\n3) The experiments are conducted with a fair number of random seeds (10) and include an ablations showing that each of the introduced components contributed positively to the reported performance."
                },
                "weaknesses": {
                    "value": "Major:\n\n1) The experiments are limited to small set of 6 of the easier environments from the DeepMind Control suite. Hence, I believe the empirical results are very far from sufficient for asserting general effectiveness, and appear very much in contrast to the claims made in the paper, e.g., the authors state that one of the main contribution is 'extensive experiments' showing \"TDR enables SOTA performance [...] across a wide variety of control tasks\" (end of Page 2-Page 3). As a minimum, to make any sort of such claims, I would expect to see results on the full set of DeepMind Control environment, including for the more complex humanoid stand/walk/run tasks. \n\n2) The employed baselines use implementations that have not been optimized for the DeepMind Control suite (nor for the sample-efficiency setting considered as the authors only train for 1000 steps). This makes the reported results notably inferior than what considered standard for what these algorithms can achieve (e.g. see the learning curves in [1]).\n\n3) Section 4  introduces some quite intuitive considerations in the form of convoluted Theorems which I did not find very clear (e.g., in Theorem 1 the authors introduce what they refer to as the \"step random estimation bias \u03c8\", but do not use  \u03c8 in any of the main statements of the Theorem. They then re-introduce the same quantity by copying the exact same description in Theorem 2). I would suggest rewriting the Section to convey the paper's consideration more concisely. When using Theorems, I would make sure there is a clear separation between the assumptions and \n the exact theoretical considerations that are being shown (which I found unclear in Theorem 3) \n\n3) The main novelty of the paper is the TDR procedure which foregoes dual TD-learning and chooses the target Q-function based on the lower TD error. However, I believe the paper does not convey a solid intuition as to why this heuristic is superior than standard minimisation of the target Qs. Hence, I am worried the proposed methodology might simply trade-off slightly improved sample-efficiency for stability and convergence (in harder environments/longer training regimes) due to 1) employing n-step returns 2) lowering the pessimism to counteract the actor's Q-maximization. In connection to Point 1 above I would really like to see experimental evidence showing how the proposed algorithms fare in harder environments and for longer training horizons, to empirically validate their efficacy.\n\n4) The authors claim several times that the dual TD-learning (target Q-function minimization from TD3) \"promotes a new problem of underestimation, which usually occurs during the early stage of learning, or when subjected to corrupted reward feedback or inaccurate states\" (page 4). Yet this claim is not supported by any empirical or theoretical evidence in the text.\n\nMinor:\n\nThe authors incorporate  what they refer to as 'Long N-step surrogate state' (LNSS), which appears to be a simple modification to n-step returns that takes the discounted average rather than the discounted sum of the rewards. While they remark several times this is a core component of their methodology, they never explain this simple procedure in the main text and relegate its description to a Section in the Appendix I found unnecessarily convoluted. Am I missing something about this method?\n\nSome typos are still present, I suggest making use of a spell-checker to parse the text (e.g., measureed. page 2)\n\n[1] https://github.com/denisyarats/pytorch_sac"
                },
                "questions": {
                    "value": "I would appreciate if the authors could address the questions and criticism I have included in connection to the weaknesses Section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5958/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698482282113,
            "cdate": 1698482282113,
            "tmdate": 1699636636022,
            "mdate": 1699636636022,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3Zv37yKrxf",
                "forum": "Ydlfehfvge",
                "replyto": "yvMKR2j1GO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5958/Authors"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Authors",
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5958/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">Benchmark Environment Selection\n\nPlease refer to our Common Point 4\n\n>The employed baselines use implementations that have not been optimized for the DeepMind Control suite\n\nThe question is fair, BUT we have taken this issue into consideration in our implementations when reporting results. Please read our implementation details of the paper (Appendix C) and Common points 3 more carefully. \nSpecifically, \n\n1. We use default environment setting (1000 steps horizon), which is well accepted as shown in previous papers [1,2,3]. \n\n2. The length of training at 1e6 maximum steps is also well accepted in the community [2,3]. \n\n3. we use the same common hyperparameters for all algorithms. This choice of hyperparameters is well accepted as shown in previous papers [1,2]. \n\nAs for the reference provided by the reviewer, it uses CUSTOM hyperparameters, while our selected set of hyperparameters are i) consistently used throughout our evaluations, and ii) also used in seminal works such as original TD3 and SAC.\n\n >Theorem unclear. the authors introduce what they refer to as the \"step random estimation bias $\\psi$\", but do not use $\\psi$ in any of the main statements of the Theorem. They then re-introduce the same quantity by copying the exact same description in Theorem 2\n\nIn Lemma 1, we show how TD error can be used to measure estimation error by formulating the step estimation bias $\\psi$ and the function estimation bias $\\Psi$ respectively as \n\\begin{equation}\n        \\mathbb{E}[\\Psi^{k+1} ]  = \\sum_{t=k+1}^{\\infty} \\gamma^{t-k-1} \\mathbb{E}[\\psi^t_{\\theta'_\\zeta}].\n\\end{equation}\n\nThis equation highlights the significance of the step estimation bias $\\psi$ in deriving our theorem which can use step bias to  predict function estimation bias in an unbiased way. For a comprehensive understanding of how this bias contributes to our results, we suggest the reviewer read the discussion/proof presented in Common Point 2 and the full proof of Theorems in the Appendix, which includes a further elaboration during rebuttal. This part of our work underscores the integral role of Lemma 1 in supporting the theorems.\n\nRegarding Theorem 3, we build upon the foundation laid in Theorem 2 to make a statement about the impact of suboptimal actor updates on the critic. Specifically, we address how these updates influence the Q value (represented in the critic) in two distinct scenarios: overestimation and underestimation.\n\\begin{equation}\n    \\mathbb{E}[Q_{\\theta_1}(s_k,\\pi_{DPG}(s_k)] \\geq \\mathbb{E}[Q_{\\theta_1}(s_k,\\pi_{TDR}(s_k))] \\geq \\mathbb{E}[Q^\\pi(s_k,\\pi_{True}(s_k))],\n\\end{equation}\nand in the underestimation case,\n\\begin{equation}\n    \\mathbb{E}[Q_{\\theta_1}(s_k,\\pi_{DPG}(s_k)] \\leq \\mathbb{E}[Q_{\\theta_1}(s_k,\\pi_{TDR}(s_k))] \\leq \\mathbb{E}[Q^\\pi(s_k,\\pi_{True}(s_k))].\n\\end{equation}\nThese relationships illustrate how the quality of actor updates, whether suboptimal or not, can significantly affect the critic's performance in terms of Q value estimation. We believe this concept is clearly presented and does not lead to any confusion.\n\n>Novolty and LNSS issue\n\nPlease refer to our Common Point 2 for novelty of TDR.\n\nLNSS is not among our claims for novelty in this study. Just like n-step to D4PG, LNSS is a useful add-on.  If the reviewer is interested in LNSS, please refer to their paper [4]. \n\nAs for the strategy of \"lowering the pessimism to counteract the actor's Q-maximization,\" this approach is central to the functionality of TD3. In TD3, the method involves directly minimizing the Q value, which serves to mitigate the pessimism that arises as a byproduct of the actor's maximization of the Q value. However, as we've identified, this direct minimization approach can lead to other issues, as highlighted by the reviewer. Please refer to our Common Point 2, Theorem 1 (specifically case 1 and case 2) where by using TDR, we effectively addressed this issue by selecting a critic value that is associated with a lesser TD error.\n\n>\"promotes a new problem of underestimation, which usually occurs during the early stage of learning, or when subjected to corrupted reward feedback or inaccurate states\" (page 4). Yet this claim is not supported by any empirical or theoretical evidence in the text.\n\nPlease refer to our Common Point 3. Consider at very early stage of training with $Q = 0$ as a common initialization. If the reward noise is negative, by inspecting the bellman equation, the estimation error will most likely result in an underestimation error. Similarly for inaccurate state and action due to  $s_{noise}$ and $a_{noise}$, it is possible to have $\\mathbb{E} [Q(s_{noise},a_{noise})] < \\mathbb{E} [Q(s_{true},a_{true}]$ which will result in underestimation. This is why we need to study estimation error under noisy reward, state and action rather than a noise free environment. This also shows the significance and novelty of our work."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695465362,
                "cdate": 1700695465362,
                "tmdate": 1700699851111,
                "mdate": 1700699851111,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "US8Pon3hge",
                "forum": "Ydlfehfvge",
                "replyto": "yvMKR2j1GO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5958/Authors"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Authors",
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5958/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "[1] Pardo, F. (2020). Tonic: A deep reinforcement learning library for fast prototyping and benchmarking. arXiv preprint arXiv:2011.07537.\n\n[2] Fujimoto, Scott, Herke Hoof, and David Meger. \"Addressing function approximation error in actor-critic methods.\" International conference on machine learning. PMLR, 2018.\n\n[3] Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint arXiv:1812.05905 (2018)\n\n[4] Zhong, Junmin, Ruofan Wu, and Jennie Si. \"A Long N-step Surrogate Stage Reward for Deep Reinforcement Learning.\" Thirty-seventh Conference on Neural Information Processing Systems. 2023."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695479043,
                "cdate": 1700695479043,
                "tmdate": 1700699866978,
                "mdate": 1700699866978,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rhhsUiCbtO",
            "forum": "Ydlfehfvge",
            "replyto": "Ydlfehfvge",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5958/Reviewer_KXvd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5958/Reviewer_KXvd"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new mechanism; instead of directly selecting the minimum value from twin Q values, it selects the target value with smaller TD errors. The authors claim that it could mitigate both overestimation and underestimation issues. The authors implement this mechanism in some popular algorithms and evaluate it in six tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I think it is a new try, and this paper gives some theoretical explanations."
                },
                "weaknesses": {
                    "value": "Major Weaknesses lie in the experiments listed in the Questions. Also, the presentation should be improved."
                },
                "questions": {
                    "value": "1. **[Problematic claim in the introduction]** The authors claimed that \"The clipped double Q-trick is designed to solve the overestimation caused by the max operator.\"  Actually, the max operator is not the culprit for the overestimation; we would encounter overestimation even using the bellman evaluation operator. The culprit is function approximation errors. \n\n2. **[Unreliable Baseline Performance]** I think this author's report of SAC and TD3 performances is not consistent with what is reported in other papers.\n   For example: \n\n   * AcrobotSwingup task, the author reported SAC performance is ~4@1M steps. However, refer to the TD-MPC[1] paper (Figure 3), ~100@500k steps. **This performance is much higher than the authors' own dTDR algorithm.**\n\n   * Quadruped Walk task, the authors report SAC performance of ~196@1M steps. However, refer to the BAC[2] paper, ~600@400k steps (Figure 32).\n\n   \n\n3. **[Need More Experimental Results]**  The evaluation of just 6 medium and easy DMControl tasks is not enough to support the effectiveness of this proposed practical mechanism. Add more experimental results on MuJoCo and DMControl tasks.\n\n   \n\n4. **[Arbitrary Claim on baseline D4PG]** The claim that D4PG is SOTA is an overly arbitrary one in abstract. There are different SOTA methods for different tasks, and I don't agree with the author's claim in the abstract. At least in the BAC [2] I pointed out above, it outperforms D4PG on some tasks. Also, RND[3], and REDQ achieve SOTA on some tasks. \n\n   \n\n5. **[Add related Works]** BAC is also a paper dedicated to underestimation and overestimation, and I see that it reports good performance and does many experiments on a wide range of benchmark tasks, so I suggest the authors add it to the related work discussion. And possibly do some experimental comparisons if the BAC authors are willing to provide the code or the experimental data.\n\n\n\n6. **[Move Table 1 to Appendix ]** Figure 2 and Table 1 are the results of the same set of experiments, and there is no need to occupy half a page in the text.\n\n\n\n7. **[Better not to use the limited results to answer Q1-Q6 questions repeatedly]** Your Experiment Section begins by proposing to answer six distinct questions. However, for several of these questions, you only have 1-2 supporting evidence repeatedly from your main experiments. Furthermore, the reader is required to make indirect associations to find these supporting evidence. To enhance the clarity and focus of your work, it may be advisable to pare down the number of questions you aim to address to a more manageable 2-3, then put the unimportant ones in the Appendix and conduct some more experiments to support, not repeatedly use the limited results to answer a lot of questions. \n\n[1] Hansen N, Wang X, Su H. Temporal difference learning for model predictive control[J]. arXiv preprint arXiv:2203.04955, 2022.\n\n[2] Ji T, Luo Y, Sun F, et al. Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic[J]. arXiv preprint arXiv:2306.02865, 2023.\n\n[3] Burda Y, Edwards H, Storkey A, et al. Exploration by random network distillation[J]. arXiv preprint arXiv:1810.12894, 2018\n\n[4] Chen X, Wang C, Zhou Z, et al. Randomized ensembled double q-learning: Learning fast without a model[J]. arXiv preprint arXiv:2101.05982, 2021."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5958/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5958/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5958/Reviewer_KXvd"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5958/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698592326794,
            "cdate": 1698592326794,
            "tmdate": 1699636635928,
            "mdate": 1699636635928,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5WhXpVRKnk",
                "forum": "Ydlfehfvge",
                "replyto": "rhhsUiCbtO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5958/Authors"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Authors",
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5958/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">The authors claimed that \"The clipped double Q-trick is designed to solve the overestimation caused by the max operator.\" Actually, the max operator is not the culprit for the overestimation; we would encounter overestimation even using the bellman evaluation operator. The culprit is function approximation errors\n\nWe would like to make the following two points.\n\n1) The notion of overestimation error has long been established and frequently used in  previous seminal works such as those we cited. For example, in Lan 2020 [2], published at ICLR, the first sentence of the second paragraph under section 3 says \"The overestimation bias occurs since the target $max_{a' \\in A} Q(s_{t+1},a')$ is used in the Q-learning update.\" TD3 [4] states  in the introduction that \"Overestimation bias is a property of Q-learning in which the maximization of a noisy value estimate induces a consistent overestimation\". In Thrun \\& schwartz 1993 [3], under section 2, it says that \"The max operator (in Q learning), however, always picks the largest value, making it particularly sensitive to overestimations.\"  \n\n2) About the reviewer's comment that \"we would encounter overestimation even using the bellman evaluation operator. The culprit is function approximation errors\", please refer to our Common Point 3 where we discussed causes/sources of estimation error, which is NOT ONLY just the function approximation error. It can also come from noisy reward, state, and action. This is the very reason of our experiment design, and is considered one of our major contributions to studying estimation error at the root cause. \n\n>Unreliable Baseline Performance\n\nIncorrect assessment. Please refer to our Common Point 3.\n\n>Need More Experimental Results. Add more experimental results on MuJoCo and DMControl tasks.\n\nThe reviewer may have missed reading SOTA developments and results on DRL. Please refer to our Common Points 3 and 4. Additionally, please note that Mujoco [1] is indeed a physical simulation engine.  But the DeepMind Control Suite (DMC) has established its position in DRL as providing benchmark environments, which are built upon the Mujoco simulator.\n\n>SOTA Baseline and some related work\n\nFirst, on using SOTA baseline, please carefully review our Common Point 4.\n\nAdditionally, our selection of off-policy baselines, as substantiated by extensive results based on the Deepmind Control Suite [5,6,7], represents the state-of-the-art (SOTA). Specifically, D4PG is widely recognized for its superior performance in continuous control problems, a fact underscored by its impressive scores, frequently surpassing 900 on a scale of 1000. This level of achievement has led to D4PG's extensive use in numerous significant and recent publications as a baseline method for comparison [8,9,10,11].\n\nIn regards to a comparison with BAC, please read our Common Point 3. Next, we noted that BAC is currently under review for ICLR 2024 and is yet to be officially published. Our criteria for choosing baselines prioritizes algorithms that are not only published but have also gained widespread acceptance in the research community. This approach ensures that our comparisons are made against benchmarks that are established and validated in the field, and the results can be repeated. \nOne more important detail that we'd like to point out - BAC only considers adding a single noise to the action. This does not allow a sufficient study of the estimation error issue. \n\n\nIn response to the recommendation to include RND in our comparison, it's important to note that RND primarily focuses on environments such as Montezuma\u2019s Revenge, which is not a continuous control problem. As for REDQ, its evaluation predominantly relies on the OpenAI Gym benchmarks. According to  benchmark studies such as Pardo 2020 [6], it has reached a consensus that  methods such as SAC and PPO, which may perform well in Gym environments, show less impressive results in DMC benchmarks. As such, when consider using the more established DMC benchmarks.  As previously mentioned, D4PG stands out as a well-recognized SOTA method within the community."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695254673,
                "cdate": 1700695254673,
                "tmdate": 1700699819955,
                "mdate": 1700699819955,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2v9Z4Kb0ox",
                "forum": "Ydlfehfvge",
                "replyto": "rhhsUiCbtO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5958/Authors"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Authors",
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5958/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">Move Table 1 to the Appendix\n\nWe thank the reviewer for the suggestion. But this table is highly informative, important, and comprehensive as it compliments the learning curves and other plots by providing quantitative performances upon learning convergence.  \nThis approach is standard and has been a common practice in the community. It can be seen in almost all seminal papers such as  TD3 and SAC.\n\n>Better not to use the limited results to answer Q1-Q6 questions repeatedly\n\nWe thank the reviewer for the feedback but we respectfully disagree. \nIn quantitative analysis of experimental results employing both horizontal and vertical comparison analyses is necessary and essential. This is a standard technique in scientific data analysis on different performance measures. They answer different questions. Additionally, our presentation of using both horizontal and vertical comparisons actually helps guide readers to read the results and verify our claims. \n\n[1] Mujoco website https://mujoco.org/\n\n[2] Lan, Qingfeng, et al. \"Maxmin q-learning: Controlling the estimation bias of q-learning.\" arXiv preprint arXiv:2002.06487 (2020).\n\n[3] Thrun, S. and Schwartz, A. Issues in using function approximation for reinforcement learning. In Proceedings of the 1993 Connectionist Models Summer School Hillsdale,NJ. Lawrence Erlbaum,1993.\n\n[4] Fujimoto, Scott, Herke Hoof, and David Meger. \"Addressing function approximation error in actor-critic methods.\" International conference on machine learning. PMLR, 2018.\n\n[5] Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. D. L., ... \\& Riedmiller, M. (2018). Deepmind control suite. arXiv preprint arXiv:1801.00690.\n\n[6] Pardo, F. (2020). Tonic: A deep reinforcement learning library for fast prototyping and benchmarking. arXiv preprint arXiv:2011.07537.\n\n[7] Barth-Maron, G., Hoffman, M. W., Budden, D., Dabney, W., Horgan, D., Tb, D., \\& Lillicrap, T. (2018). Distributed distributional deterministic policy gradients. arXiv preprint arXiv:1804.08617.\n\n[8] Huang, S., Abdolmaleki, A., Vezzani, G., Brakel, P., Mankowitz, D. J., Neunert, M., \\& Hadsell, R. (2022, January). A constrained multi-objective reinforcement learning framework. In Conference on Robot Learning (pp. 883-893). PMLR.\n\n[9] Gulcehre, C., Wang, Z., Novikov, A., Paine, T., G\u00f3mez, S., Zolna, K., \\& de Freitas, N. (2020). Rl unplugged: A suite of benchmarks for offline reinforcement learning. Advances in Neural Information Processing Systems, 33, 7248-7259.\n\n[10] Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., \\& Bellemare, M. (2022). Reincarnating reinforcement learning: Reusing prior computation to accelerate progress. Advances in Neural Information Processing Systems, 35, 28955-28971.\n\n[11] Chen, X., Mu, Y. M., Luo, P., Li, S., \\& Chen, J. (2022, June). Flow-based recurrent belief state learning for pomdps. In International Conference on Machine Learning (pp. 3444-3468). PMLR."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695307303,
                "cdate": 1700695307303,
                "tmdate": 1700699832041,
                "mdate": 1700699832041,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JEphuupFO8",
                "forum": "Ydlfehfvge",
                "replyto": "rhhsUiCbtO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5958/Reviewer_KXvd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5958/Reviewer_KXvd"
                ],
                "content": {
                    "title": {
                        "value": "Thank the authors for their rebuttal. Here are some kind suggestions."
                    },
                    "comment": {
                        "value": "Thank you for addressing my comments earlier. I would like to suggest considering the constructive advice offered by the reviewers, as it could enhance the quality of your work. Your responses did not seem to address my recommendations for additional experiments, which I believe are crucial for a more comprehensive evaluation of your approach.\n\nIn your paper, you identify a particular method as the state-of-the-art (SOTA) in Reinforcement Learning (RL). However, this claim seems to overlook significant developments in the field, suggesting a potential gap in familiarity with current RL literature. I recommend reviewing the additional references I cited, as they underscore the importance of broad and varied experimentation in algorithmic research.\n\nWhile I agree that comparing your method with the BAC introduced 5 months ago might not be necessary (it is just a mild suggestion, as shown in my initial tone), I am concerned about the reluctance to benchmark against well-established methods like REDQ or RND. These methods are noted for their superior performance and have been available for over a year, making them relevant comparators.\n\nFurthermore, limiting the scope of experimentation to a few medium and easy tasks does not fully demonstrate the robustness or effectiveness of a proposed method in algorithmic research.\n\nTaking these points into consideration, I have decided to maintain my initial score. I hope these observations will be helpful in guiding the further development of your work."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730978728,
                "cdate": 1700730978728,
                "tmdate": 1700730978728,
                "mdate": 1700730978728,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4NEB55BPEE",
            "forum": "Ydlfehfvge",
            "replyto": "Ydlfehfvge",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5958/Reviewer_LkhT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5958/Reviewer_LkhT"
            ],
            "content": {
                "summary": {
                    "value": "The paper first proposes a TD-Regularized Double Q Networks to effectively control the error of Q-value estimation, and then regularizes the actor based on TD-error to further control the error of Q-value. The paper demonstrates the effectiveness of the method in error control through a large amount of theoretical evidences. The experiments demonstrate that the combination of the method with distributional RL or LNSS also has significant improvement."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) This paper provides a simple yet effective method for mitigating estimation errors. The proposed method is easy to implement.\n2) Sufficient theoretical derivations are provided."
                },
                "weaknesses": {
                    "value": "1) The comparative experimental results are not sufficiently reliable. None of the baselines are designed for addressing the estimation error problem.\n2) The paper emphasizes that the Twin TD-Regularized method can effectively control errors, but the effectiveness of bias control has not been demonstrated in the experiment section.\n3) The ablation experiment is unreasonable, that is, the paper proposes to combine TDR with LNSS method effectively, but LNSS is not the work of this paper and is not suitable for ablation experiments.\n\n[Supplementary review] Thanks for the author's responses to my comments. Based on the comments of all reviewers and the author's reply, I decided to revise my score (Marginally below the acceptance threshold -> Reject, not good enough)."
                },
                "questions": {
                    "value": "1) The first Q-network is traditionally chosen to compute the actor TD regularization term. Will it have a negative impact on the selection of Q-network target values in the proposed method?\n2) In subsection 3.4, the paper only analyzes the advantages of TDR in theory compared to the previous TD-Regularized actor network method. It should be demonstrated through some experiments.\n3) In page 7, the meaning of \"TDR has helped successfully address the random initialization challenge caused by random seeds\" is confusing and difficult to understand, and it needs further explanation.\n4) It is very sensitive to parameters for DRL algorithms, and using different parameters can have different effects. Will the parameters of the comparison algorithms in the experiment be consistent with the original paper (the best situation), and will their performance be worse than the original paper?\n5) The paper only demonstrates that TDR can improve the performance of baselines. You should compare some SOTA  bias control algorithms in recent years.\n6) The ablation studies are not quite thorough. You should provide more experimental results about approximate estimation error for clearly suggesting the benefits of different components of the algorithm."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5958/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5958/Reviewer_LkhT",
                        "ICLR.cc/2024/Conference/Submission5958/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5958/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698658649613,
            "cdate": 1698658649613,
            "tmdate": 1700721113928,
            "mdate": 1700721113928,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kJyWZhnCJM",
                "forum": "Ydlfehfvge",
                "replyto": "4NEB55BPEE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5958/Authors"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Authors",
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5958/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">The comparative experimental results are not sufficiently reliable. None of the baselines are designed for addressing the estimation error problem.\n\nWe thank the reviewer for reading our paper and for providing comments.\nHowever, we respectfully disagree with the reviewer on this point. \nActually, our experiments were designed for the purpose of addressing estimation error. \nPlease refer to our Common Points 2 and 3.\n\nFurthermore, please note that the issue of estimation error is of great importance regardless if the baseline algorithms are designed for this purpose or not. Refer to Common Point 3, estimation error poses a great challenge for almost all DRL algorithms. As demonstrated in our results (Figures 4, 5 and Table 6 in the updated paper pdf during rebuttal), if we deliberately introduce those factors that may cause  estimation error (such as adding noises to $s, a, r$, all baseline algorithms show a significant performance drop from that without noise using the same benchmark environments. However, by using TDR, the lost performance were all  compensated for to a large degree, and the baseline methods with TDR still produce stable results. This clearly shows our contribution and the novelty of TDR.\n\n>Will the parameters of the comparison algorithms in the experiment be consistent with the original paper (the best situation), and will their performance be worse than the original paper\n\nPlease refer to our Common Point 3. If we evaluate under the Noise-Free conditions of the original papers, that defeats the purpose of this paper which aims at adequately addressing estimation errors that may come from multiple sources ($s, a, r$, and $\\theta$). In real-world applications, these factors are prevalent and ubiquitous. \n\nNonetheless, in the rebuttal, we have provided new data to further strengthen the novelty and significance of TDR. Please refer to Common Point 1 for our evaluations of baselines without Noise. \n\n>The paper emphasizes that the Twin TD-Regularized method can effectively control errors, but the effectiveness of bias control has not been demonstrated in the experiment section.\n\nWe adhere to the same performance measures used in prior works such as TD3 and SAC, both of which aim to address estimation error. Please refer to our Figures 2,4,5, under both noisy and noise free case, TDR boosts average reward of baseline methods.\n\nWe also remind the reviewer to read our Common Points 3 where we clearly explained why by adding multiple sources of noise, our baseline results no long replicate those published results (because they did not use noise).\n\n>The first Q-network is traditionally chosen to compute the actor TD regularization term. Will it have a negative impact on the selection of Q-network target values in the proposed method?\n\nPlease refer to Common Point 2 where as we discussed that, following the TD3 convention, we use the first critic to compute actor.  This is because theoretically $Q_{\\theta_1}$ and $Q_{\\theta_2}$  converge to the same result, making it redundant if we go between the two.\n\nFor impact on the selection of Q-network value please refer to common point 2. \n\n>In subsection 3.4, the paper only analyzes the advantages of TDR in theory compared to the previous TD-Regularized actor network method. It should be demonstrated through some experiments.\n\nTo highlight our points, We are now also providing additional results in the rebuttal Figure 7 of our updated paper. Our TDR actor leads to reduced learning variance, in agreement with the predictions made in Theorem 2. This improvement in our TDR actor's performance may primarily due to the discussion outlined in Common Point 2 that use both online critic can better measure the estimation error of critic updates. \n\n>In page 7, the meaning of \"TDR has helped successfully address the random initialization challenge caused by random seeds\" is confusing and difficult to understand, and it needs further explanation.\n\nPlease refer to one of our cited references by Henderson et al. [1]. In their Figure 5, it is clear that initializing the environment, numpy, and other packages with varying random seeds can lead to results with significant variance in learning performance. Therefore, the ability of a DRL method to effectively overcome this challenge becomes a critical performance evaluation metric."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695008880,
                "cdate": 1700695008880,
                "tmdate": 1700699789827,
                "mdate": 1700699789827,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QFKY9UIiVK",
                "forum": "Ydlfehfvge",
                "replyto": "4NEB55BPEE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5958/Authors"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Authors",
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5958/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">The paper only demonstrates that TDR can improve the performance of baselines. You should compare some SOTA bias control algorithms in recent years.\n\nIt's important to note that TD3 and SAC, the two baselines we've compared TDR against, are not only well-accepted in the field but also have demonstrated efficacy in directly addressing overestimation error, a key aspect of bias control. Additionally these two methods have been used as SOTA baselines in most recent top conference papers [4-7]. \n\nOur evaluations focus on showing the effectiveness of TDR within the context of general off-policy methods. By applying TDR to these widely recognized and established baseline methods (TD3, SAC, D4PG), we aim to demonstrate its capability to enhance performance in more realistic (aka noisy) environments.\n\n>The ablation studies are not quite thorough. You should provide more experimental results about approximate estimation error for clearly suggesting the benefits of different components of the algorithm. The paper proposes to combine TDR with LNSS method effectively, but LNSS is not the work of this paper and is not suitable for ablation experiments.\n\nWe respectfully disagree with the reviewer and here is why.\n\nIn our evaluations, we follow the same established protocol in widely accepted and sound research works. We recommend the reviewer  read TD3 and D4PG more carefully to find out. TD3's core innovation lies in its Clipped Double $Q$ Network, accompanied by additional features such as Delayed Policy Update and Target Policy Smoothing Regularization. In their ablation study, the authors of TD3 assessed the impact of each individual component by removing it from the TD3 framework.\n\nLikewise, D4PG's primary advancement is the integration of distributional learning into DDPG, enhanced by additional elements like Prioritized Experience Replay (PER) and n-step updates. The effectiveness of each component in D4PG was evaluated by incrementally adding them to DDPG.\n\nIn a similar vein, our approach with TDR involves components like TD regularized critic (TD critic), TD regularized actor (TD actor), and an additional enhancement, namely LNSS. We analyze the contribution of each of these elements by incorporating them into baseline algorithms, thereby assessing their individual impact on performance. This approach allows us to understand the distinct value added by each component in the TDR framework. \n\nIn terms of ...\"You should provide more experimental results about approximate estimation error...\", in addition to our evaluations provided in the paper, please also review our newly added data (Figures 4,5 and Table 6) that summarize TDR evaluations without noise. That actually is an evalution (and performance comparison) for approximation errors.\n\n[1] Henderson, Peter, et al. \"Deep reinforcement learning that matters.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 32. No. 1. 2018.\n\n[2] Fujimoto, Scott, Herke Hoof, and David Meger. \"Addressing function approximation error in actor-critic methods.\" International conference on machine learning. PMLR, 2018.\n\n[3] Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint arXiv:1812.05905 (2018)\n\n[4] Agarwal, Rishabh, et al. \"Reincarnating reinforcement learning: Reusing prior computation to accelerate progress.\" Advances in Neural Information Processing Systems 35 (2022): 28955-28971.\n\n[5]Eysenbach, Benjamin, et al. \"Contrastive learning as goal-conditioned reinforcement learning.\" Advances in Neural Information Processing Systems 35 (2022): 35603-35620.\n\n[6]Li, Chengshu, et al. \"Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation.\" Conference on Robot Learning. PMLR, 2023.\n\n[7]Liu, Zuxin, et al. \"Constrained variational policy optimization for safe reinforcement learning.\" International Conference on Machine Learning. PMLR, 2022."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695038044,
                "cdate": 1700695038044,
                "tmdate": 1700699804933,
                "mdate": 1700699804933,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "giOOrADmAW",
                "forum": "Ydlfehfvge",
                "replyto": "QFKY9UIiVK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5958/Reviewer_LkhT"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Authors",
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5958/Reviewer_LkhT"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's responses to my comments. Based on the comments of all reviewers and the author's reply, I decided to revise my score (Marginally below the acceptance threshold -> Reject, not good enough)."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721292576,
                "cdate": 1700721292576,
                "tmdate": 1700721292576,
                "mdate": 1700721292576,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vgjUvirqf7",
            "forum": "Ydlfehfvge",
            "replyto": "Ydlfehfvge",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5958/Reviewer_2Vrh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5958/Reviewer_2Vrh"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on addressing the problem of estimation bias in DRL. The authors developed the TD-regularized actor-critic (TDR) method, which aims to minimize both overestimation and underestimation errors. The paper also incorporates TDR with other effective DRL techniques, such as distributional learning and the long N-step surrogate stage reward (LNSS) method. The authors evaluate their method with different baselines in the DMC suite."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper studies how to improve value estimation in DRL, which is a core topic in the RL community. The method is clearly explained."
                },
                "weaknesses": {
                    "value": "The main weaknesses of the paper are the novelty of the approach and the significance of experimental results. It seems a A+B+C work, which modifies previous TD-regularized AC method (Parisi et al., 2019) marginally, and then combines distributional learning and N-step learning into the proposed approach. The experimental part is also a bit weird, where the replicated baselines seem worse than original papers."
                },
                "questions": {
                    "value": "> 3.1 DOUBLE Q IN ACTOR-CRITIC METHOD\n\nThis subsection is under the method section (Sec 3). However, the contents in Section 3.1 is actually a summary and description of the literature instead of the proposed method, which can confuse the readers.\n\n> Note from Equation (7) that TDR always uses a target value associated with a smaller target TD value (regardless of the error sign) between the two. ... TDR is naturally positioned to address both overesdiation and underestimation errors.\n\nIn fact, the estimation error is measured by comparing the estimated value with an expectation of the true value. How can this sample-based TD error serve as a sound measure for reducing the estimation error? Doesn't this lead to a large variance? In addition, there is a typo in here, where overesdiation should be overestimation.\n\n> Our TD-regularized actor network directly penalizes the actor\u2019s learning objective whenever there is a critic estimation error.\n\nThis statement is weird. It would be better to state this as \"whenever the critic estimation error is non-zero\". In addition, it is unavoidable to have estimation errors in practice when learning these objectives."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5958/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698744451747,
            "cdate": 1698744451747,
            "tmdate": 1699636635690,
            "mdate": 1699636635690,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3VlPqTWIEe",
                "forum": "Ydlfehfvge",
                "replyto": "vgjUvirqf7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5958/Authors"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Authors",
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5958/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">The main weaknesses of the paper are the novelty of the approach\n\nWe thank the reviewer for reading our paper and for providing commons. \nHowever, we respectfully disagree with the reviewer. The TDR method is NOT an A+B+C work. Please refer to our Common Point 2, especially the one on Novelty of TDR. \n\nAs a side note, LNSS is just a useful add-on, playing a role similar to PER in D4PG [3]. We never claimed any credit for LNSS as a (key) contribution of this work.\n\n>Experiment Results Reliable that the replicated baselines seem worse than original papers.\n\nWe thank the reviewer for noticing this. Please refer to Common Point 3. \n\n>Section 3.1 is a review and may confuse reader\n\n Yes, and No. Section 3.1 is not only a  background but also an introduction of necessary notations and problem formulation. This is very common, and necessary, in RL papers. Please refer to TD3, D4PG, SAC [1,2,3].\n\n>How this sample based TD error serve as a sound measure for reducing the estimation error? Doesn't this lead to a large variance?\n\nThis sample based approximation is a common approach in DRL. DPG [6] uses sampled  TD error to approximate true gradient. DQN, DDPG, SAC, D4PG [2,3,4,5] use collected samples to approximate $Q$ values. TD3 [1] uses the Q value samples from two target $Q$ networks to address overestimation bias. In a similar vein, our TDR method uses TD error samples from two target networks to approximate both overestimation and underestimation errors.\n\nRegarding the variance, \"the variance can grow rapidly with each update if the (estimation) error from each update is not tamed.\" as state by TD3 authors (refer to TD3 [1] section 5). Based on Theorem 1 and discussions of Common Point 2, we show that by using TDR, RL agents will result in better target values with less estimation error compared to previous clipped double $Q$ network in TD3. Therefore, following the same analysis as TD3, TDR will have the same or even more variance reduction.\n\n>Our TD-regularized actor network directly penalizes the actor\u2019s learning objective whenever there is a critic estimation error.This statement is weird. It would be better to state this as \"whenever the critic estimation error is non-zero\". In addition, it is unavoidable to have estimation errors in practice when learning these objectives.\n\nWe thank the reviewer for rephrasing the sentence for us. Actually there is no difference between how we and the reviewer said it. Nonetheless, we can update the phrase if the paper gets accepted. \n\n[1] Fujimoto, Scott, Herke Hoof, and David Meger. \"Addressing function approximation error in actor-critic methods.\" International conference on machine learning. PMLR, 2018.\n\n[2] Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint arXiv:1812.05905 (2018)\n\n[3] Barth-Maron, Gabriel, et al. \"Distributed distributional deterministic policy gradients.\" arXiv preprint arXiv:1804.08617 (2018).\n\n[4] Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\" arXiv preprint arXiv:1312.5602 (2013).\n\n[5] Lillicrap, Timothy P., et al. \"Continuous control with deep reinforcement learning.\" arXiv preprint arXiv:1509.02971 (2015).\n\n[6] Silver, David, et al. \"Deterministic policy gradient algorithms.\" International conference on machine learning. Pmlr, 2014."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694861540,
                "cdate": 1700694861540,
                "tmdate": 1700699773289,
                "mdate": 1700699773289,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]