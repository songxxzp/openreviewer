[
    {
        "title": "Fixed Non-negative Orthogonal Classifier: Inducing Zero-mean Neural Collapse with Feature Dimension Separation"
    },
    {
        "review": {
            "id": "BhEdsBvSC4",
            "forum": "F4bmOrmUwc",
            "replyto": "F4bmOrmUwc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4777/Reviewer_iab3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4777/Reviewer_iab3"
            ],
            "content": {
                "summary": {
                    "value": "The article introduces the concept of a \"Fixed Non-negative Orthogonal Classifier\" and its relationship with the phenomenon of \"Zero-mean Neural Collapse.\" Fixed classifiers in neural networks have shown cost efficiency and even surpassed learnable classifiers in certain benchmarks when incorporating orthogonality. However, the dynamics of fixed orthogonal classifiers concerning neural collapse, where last-layer features converge to a specific form called simplex ETF during training, have not been deeply explored. This paper addresses this gap by introducing the concept of zero-mean neural collapse in non-negative Euclidean space. The authors propose a fixed non-negative orthogonal classifier that optimally induces this collapse, maximizing the margin of an orthogonal layer-peeled model. This classifier also offers advantages in continual learning and imbalanced learning by separating the last-layer feature dimensions. The paper provides comprehensive experiments to validate its claims, demonstrating significant performance improvements."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ 1. The article is well-structured, logically sound, and skillfully written.\n+ 2. This paper conduct extensive experiments to justify *zero-mean neural collapse*, which combines the orthogonality and neural collapse."
                },
                "weaknesses": {
                    "value": "The problem that I'm concerned about most is **unclear motivation**. Authors mentioned in the introduction: \n*However, neural collapse differently occurs in the fixed orthogonal classifier due to their limitations from geometrical feature: orthogonality.* So, I have two questions, authors should provide more discussions to demonstrate the meaning in the main text: \n  + Why do we have to fix classifier as an orthogonal matrix ?\n  + Why studying neural collapse with fixed orthogonal classifier is necessary ?"
                },
                "questions": {
                    "value": "Does Remark.1 claim that zero-mean neural collapse can achieve max-margin? Consider the binary class classication, the max-margin feature should be Digon, which has the larger angle (180 degrees) than orthogonality (90 degrees).\n\nBy the way, the case that D > K is interesting. Authors can refer to [1] and [2].\n\n[1] https://en.wikipedia.org/wiki/Thomson_problem\n\n[2] https://arxiv.org/abs/2310.05351"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4777/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676696290,
            "cdate": 1698676696290,
            "tmdate": 1699636460071,
            "mdate": 1699636460071,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xVdDVsprAM",
                "forum": "F4bmOrmUwc",
                "replyto": "BhEdsBvSC4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4777/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4777/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses for weaknesses"
                    },
                    "comment": {
                        "value": "**Weakness 1.** Why do we have to fix classifier as an orthogonal matrix?\n> The reason why fixed classifiers have non-negativity and orthogonality constraints is that if those constraints do not exist in the LPM with a fixed classifier (not a simplex), it is not guaranteed that the collapse phenomenon between class means and their respective class weight vectors occurs when the LPM has the optimal solution.\nHowever, when the fixed classifier becomes a non-negative and orthogonal matrix, the LPM obtains the optimal solution even in inducing max margin in decision while the collapse occurs.\n\n---\n\n**Weakness 2.** Why studying neural collapse with fixed orthogonal classifier is necessary?\n> After introducing the necessary and theoretical benefits (optimal solution and max-margin in decision) of non-negativity and orthogonality in the LPM with a fixed classifier, we have proven the utility of those constraints, and in order to explain the collapse phenomenon that occurs when the LPM with a fixed non-negative orthogonal classifier achieves the optimal solution, it becomes necessary to define special neural collapse properties: zero-mean neural collapse.\n\n---\n\n**Question.** Does Remark.1 claim that zero-mean neural collapse can achieve max-margin? Consider the binary class classication, the max-margin feature should be Digon, which has the larger angle (180 degrees) than orthogonality (90 degrees).\n> According to the constraints of Eq. 5 in the revised paper, last-layer features are located on the non-negative Euclidean space because they followed the ReLU activation function and were centered at the origin.\nIn this situation, the max-margin in decision becomes orthogonal.\n\n---\n\n**Additional References.** By the way, the case that D > K is interesting. Authors can refer to [1] and [2].\n> Thanks for your thoughtful references about the future work.\nWe reflexted them to the future work paragraph in Section 8 (Conclusion and Future Work) on page 9 of the revised paper.\nWe don't doubt that these materials will be helpful to resolve the limitation."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700144527363,
                "cdate": 1700144527363,
                "tmdate": 1700144527363,
                "mdate": 1700144527363,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4rb0yGlplG",
            "forum": "F4bmOrmUwc",
            "replyto": "F4bmOrmUwc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4777/Reviewer_jWfm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4777/Reviewer_jWfm"
            ],
            "content": {
                "summary": {
                    "value": "The study delves into the intricacies of \"Fixed Non-negative Orthogonal Classifiers\" in the realm of neural networks, emphasizing their potential in inducing \"Zero-mean Neural Collapse.\" While fixed classifiers have historically demonstrated cost-effectiveness and even outperformed learnable ones with orthogonality, their behavior in the context of neural collapse\u2014a phenomenon where last-layer features align to a specific form, the simplex ETF\u2014remains underexplored. Addressing this, the paper pioneers the idea of zero-mean neural collapse within a non-negative Euclidean space and presents a novel classifier that optimally triggers this collapse. This innovation not only maximizes the margin of an orthogonal layer-peeled model but also enhances performance in continual and imbalanced learning scenarios. Through rigorous experimentation, the authors substantiate their findings, showcasing marked performance enhancements."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Effectiveness: The proposed methods improve the performance in long-tailed learning.\n2. Clarity: Overall, the paper is well-written and easy to follow. Besides, the main theoretical result (Theorem 1) is clear and correct."
                },
                "weaknesses": {
                    "value": "1. My main concern is the necessity of the new theory. The main result (Theorem 1) shares a similar formulation with Lemma 4.1 in [1], showing the zero-mean is unnecessary to achieve the neural collapse. Please provide more evidence of the advantages.\n2. Although the proposed methods are effective, the connections with the theoretical analysis seem unclear.\n3. The orthogonality is accessible when $d \\leq K$, could you please discuss the condition $d > K$?\n4. It will be more convincing if more competitors on ImageNet-LT and Places-LT are provided.\n\n\nRef:\n\n[1] Gao, P., Xu, Q., Wen, P., Yang, Z., Shao, H. and Huang, Q. Feature Directions Matter: Long-Tailed Learning via Rotated Balanced Representation. ICML, 2023."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4777/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4777/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4777/Reviewer_jWfm"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4777/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698849492768,
            "cdate": 1698849492768,
            "tmdate": 1699636459990,
            "mdate": 1699636459990,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yzX99zNMpa",
                "forum": "F4bmOrmUwc",
                "replyto": "4rb0yGlplG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4777/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4777/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses for weaknesses"
                    },
                    "comment": {
                        "value": "**Weakness 1.** My main concern is the necessity of the new theory. The main result (Theorem 1) shares a similar formulation with Lemma 4.1 in [1], showing the zero-mean is unnecessary to achieve the neural collapse. Please provide more evidence of the advantages.\n\n[1] Gao, P., Xu, Q., Wen, P., Yang, Z., Shao, H. and Huang, Q. Feature Directions Matter: Long-Tailed Learning via Rotated Balanced Representation. ICML, 2023.\n\n>**(Detailed explanation of our motivation)** Before describing the answer to [1], we want to emphasize our motivation.\nThe reason why fixed classifiers have non-negativity and orthogonality constraints is that if those constraints do not exist in the LPM with a fixed classifier (not a simplex), it is not guaranteed that the collapse phenomenon between class means and their respective class weight vectors occurs when the LPM has the optimal solution.\nHowever, when the fixed classifier becomes a non-negative and orthogonal matrix, LPM obtains the optimal solution even in inducing max margin in decision while the collapse occurs.\nThen, we have proven the utility of those constraints, and in order to explain the collapse phenomenon that occurs when LPM with a fixed non-negative orthogonal classifier achieves the optimal solution, it becomes necessary to define special neural collapse properties: zero-mean neural collapse.\n\n>**(Main answer)** Lemma 4.1 in [1] indicates the summation of vectors centered to their global mean.\nTherefore, it is a natural result that the average of centered vectors becomes zero as illustrated in the proof of Lemma 4.1 (Appendix B in [1]; p13).\nIn contrast, we centered the class means at the origin, which is the same effect that the global mean of class means is considered as zero-mean.\nWe defined the collapse properties in this case as zero-mean neural collapse.  \nApart from this, [1] also proposed an orthogonal-constrained classifier that converges to a $K$-equiangular ETF.\nHowever, this classifier was not analyzed with LPM and did not take into account non-negativity, which is an important factor that causes feature dimension separation in the weight matrix of the fixed orthogonal classifier.\n\n*We added this content to our reference list.\nPlease refer to the Section 2 (Related Work) on page 2 of the revised paper.*\n\n---\n\n**Weakness 2.** Although the proposed methods are effective, the connections with the theoretical analysis seem unclear.\n> To improve the connections, we changed the order of paragraphs in Section 5 on page 5 of the revised paper according to the modified logical flow of detailed explanation of our motivation in W1.\nAdditionally, we divided our previous Figure 1 by splitting it into three separate figures (Figure 1, 2, and 3).\nFigure 1 now illustrates the main motivation of our work, which is *how the collapse between class means and class weight vectors occurs in fixed classifier?*.\nFigure 2 describes the our method, FNO classifier.\nFigure 3 provides the utilization of feature dimension separation in continual learning and imbalanced learning.\nWe highlight the impact of the FNO classifier, which enhances masked softmax for continual learning and adjusts the original mixup on the hypersphere by using *arc-mixup* for imbalanced learning.\nPlease refer to Figure 1, 2, and 3 on pages 1, 5, and 6 of the revised paper for a clear visualization, respectively.\n\n---\n\n**Weakness 3.** The orthogonality is accessible when $d \\leq K$, could you please discuss the condition $d > K$?\n> We guess that this comment is the limitation that $D \\geq K$ in Section 8 (Future Work).\nTo reduce the confusion, we clarified it as: *$D$ should be greater than or equal to $K$*.\nWhen $D < K$, the weight matrix $W \\in R^{D \\times K}$ is impossible to be orthogonal because at least one weight vector $W_i \\in R^{D}$ is not linearly independent, *i.e.,* $\\exists_j W_j^{T} W_i \\neq 0$.\n\n---\n\n**Weakness 4.** It will be more convincing if more competitors on ImageNet-LT and Places-LT are provided.\n> We borrowed the performance of ETF in ImageNet-LT from [A1].\nAdditionally, to ensure a fair comparison, we reproduced the ETF classifier with the same hyperparameter settings to ours for CIFAR10/100-LT, ImageNet-LT and Places-LT.\nPlease refer to Tables 3 and 4 on page 9 and Tables 9, 10, and 11 on page 28 of the revised paper.\n\n[A1] Yibo Yang, Liang Xie, Shixiang Chen, Xiangtai Li, Zhouchen Lin, and Dacheng Tao. Do we really\nneed a learnable classifier at the end of deep neural network? arXiv preprint arXiv:2203.09081,\n2022b."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700144468983,
                "cdate": 1700144468983,
                "tmdate": 1700144468983,
                "mdate": 1700144468983,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0eJGqv8roy",
            "forum": "F4bmOrmUwc",
            "replyto": "F4bmOrmUwc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4777/Reviewer_EhX5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4777/Reviewer_EhX5"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the Fixed Non-negative Orthogonal Classifier (FNO), which is a novel approach to address the issue of neural collapse in training classification models. The authors propose the concept of zero-mean neural collapse, where the class means are centered at the origin instead of their global mean. The paper empirically validates the effectiveness of these methods in tasks such as continual learning and imbalanced learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper introduces a novel Fixed Non-negative Orthogonal Classifier (FNO classifier) and proposes the concept of zero-mean neural collapse. This combination of ideas is interesting.\n\n- The paper provides theoretical analysis of the FNO classifier and proves its benefits in terms of inducing zero-mean neural collapse. The experimental results demonstrate the effectiveness of the FNO classifier in both continual learning and imbalanced learning scenarios.\n\n- The paper is well-structured and clearly explains the motivation, methodology, and results"
                },
                "weaknesses": {
                    "value": "- The experiments in the paper are limited to continual and imbalanced learning scenarios for the FNO classifier. It would be beneficial to see how the FNO classifier performs compared to the ETF classifier in standard classification tasks. Additionally, in Table 4, which details the imbalanced learning experiments, the ETF classifier is absent from the comparison. Including it could provide a more comprehensive evaluation of the FNO classifier's performance.\n\n- A related work is missing for discussion. The orthogonality of the classifier in NC is explored funder MSE loss:\n\nZhou, Jinxin, et al. \"On the optimization landscape of neural collapse under mse loss: Global optimality with unconstrained features.\" International Conference on Machine Learning. PMLR, 2022.\n\nFor a full comparison, we may also want to consider the incorporation of MSE loss within the ETF classifier, which is guaranteed to be orthogonal classifier (assuming no bias)"
                },
                "questions": {
                    "value": "See the weaknesses part above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4777/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698913081654,
            "cdate": 1698913081654,
            "tmdate": 1699636459920,
            "mdate": 1699636459920,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "peKxxuWryA",
                "forum": "F4bmOrmUwc",
                "replyto": "0eJGqv8roy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4777/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4777/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses for weaknesses"
                    },
                    "comment": {
                        "value": "**Weakness 1-1.** The experiments in the paper are limited to continual and imbalanced learning scenarios for the FNO classifier. It would be beneficial to see how the FNO classifier performs compared to the ETF classifier in standard classification tasks.\n\n|Architecture|Method|MNIST|Fashion MNIST|CIFAR10|CIFAR100|\n|----|----|----|----|----|----|\n|ResNet|FC|99.64|93.56|89.45|65.11|\n|ResNet|ETF|99.66|93.82|88.05|64.23|\n|ResNet|FNO|99.67|94.28|89.59|66.20|\n> We conducted experiments of ETF in standard classification tasks.\nFor a fair comparison, we used the same environmental settings to ours when utilizing the analyses of ZNC in Appendix B.1 (Zero-Mean Neural Collapse in Image Classification Benchmarks) on page 20 of the revised paper.\nAs shown the above table, our FNO outperforms ETF in standard classification tasks.\n\n**Weakness 1-2.** Additionally, in Table 4, which details the imbalanced learning experiments, the ETF classifier is absent from the comparison. Including it could provide a more comprehensive evaluation of the FNO classifier's performance.\n> We borrowed the performance of ETF in ImageNet-LT from [A1] and also reproduced them in the same environmental setup with ours for fair comparison.\nIncluding the additional results, our FNO classifier with arc-mixup has consistently outperformed.\nPlease refer to Table 4 on page 9 of the revised paper.\n\n---\n\n**Weakness 2-1.** A related work is missing for discussion. The orthogonality of the classifier in NC is explored funder MSE loss\n> The findings of neural collapse under MSE loss [A2] yield similar results to ours.\nSpecifically, the class means and class weight vectors collapse in orthogonal shape.\nHowever, their classifier was not fixed and the cross entropy loss, which is the most widely used loss function in classification models, was not utilized.\nThese differences highlight the improved utility of our work.\nWe have included this content in our reference list.\nPlease refer to the end of Neural collapse and Orthogonality paragraph in the related work on page 3 of the revised paper.\n\n**Weakness 2-2.** For a full comparison, we may also want to consider the incorporation of MSE loss within the ETF classifier, which is guaranteed to be orthogonal classifier (assuming no bias)\n> When incorporating MSE loss within the fixed ETF classifier, the shape where trained class means converge will vary while aligning to their respective class weight vectors in a similar way to [A1], *i.e.*, it is not guaranteed that the class means collapse their respective class weight vectors at the orthogonal matrix when the fixed classifier is not orthogonal.\n\n---\n\n[A1] Yibo Yang, Liang Xie, Shixiang Chen, Xiangtai Li, Zhouchen Lin, and Dacheng Tao. Do we really\nneed a learnable classifier at the end of deep neural network? arXiv preprint arXiv:2203.09081,\n2022b.\n\n[A2] Zhou, Jinxin, et al. \"On the optimization landscape of neural collapse under mse loss: Global optimality with unconstrained features.\" International Conference on Machine Learning. PMLR, 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700144363294,
                "cdate": 1700144363294,
                "tmdate": 1700144363294,
                "mdate": 1700144363294,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yDnHvDoqsx",
            "forum": "F4bmOrmUwc",
            "replyto": "F4bmOrmUwc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4777/Reviewer_WNdB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4777/Reviewer_WNdB"
            ],
            "content": {
                "summary": {
                    "value": "The paper delves into the phenomenon of neural collapse, specifically in scenarios with fixed classifiers composed of orthogonal class prototypes. A central assertion of the paper is that neural collapse manifests differently when the classifier is fixed. To address this, the concept of 'zero-mean neural collapse' is introduced. This approach redefines neural collapse by centering class means to the origin in non-negative Euclidean space, rather than to their global mean. The occurrence of Zero-mean Neural Collapse (ZNC) is observed when the orthogonal Layer Peeled Model (LPM) achieves global optimality, simultaneously inducing a max-margin in decision-making. The paper further explores the implications of this phenomenon in the contexts of continual learning and imbalanced learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper poses an interesting problem and a good methodological choice for the substantiation of its main intentions. The work includes a comprehensive part of experiments across diverse contexts and with datasets the introduction and the related works are rich of interesting information and insights."
                },
                "weaknesses": {
                    "value": "The manuscript's writing and structure require refinement for better clarity and flow.\nThe concepts of masking in continual learning and mixup in imbalanced learning emerge unexpectedly within the text and would benefit from a better introduction with an improved link with neural collapse.\n\nThe introduction and related work sections could be condensed to allow for a more comprehensive introduction of Section 6.\n\nThe significance of Zero-mean Neural Collapse (ZNC) in non-negative Euclidean space (i.e., the positive hyper-octant) is not immediately apparent. The paper should clarify whether its importance is solely due to the optimality shown in the LPM model or if there are additional factors which are outside the proof. The rationale behind constraining the representation space to the positive hyper-octant warrants further explanation.\n\nThe nature of the problem posed by the LPM model is not shown. The manuscript should specify whether it is linear, non-linear, or solvable by known matrix factorization techniques. Moreover, the discussion on the complexity of providing values for W is insufficiently developed, leaving the reader questioning where the complexity of the problem truly lies.\n\nCould the authors provide insight into why LPM optimality does not manifest in the case of a regular fixed d-simplex, and conversely, why it appears to be present in the context of Zero-mean Neural Collapse (ZNC)?\n\nThe visual clarity and structural coherence of Figure 1 could be enhanced to better convey the intended information.\n\nThe tables detailing experimental results should more clearly differentiate the methodologies used, to avoid confusion. The complex nomenclature, such as FNODERMR++, could be simplified for better clarity.\n\nIn Remark 1 at the end of section 5, the statement regarding the inability of a fixed orthogonal classifier to address neural collapse needs further clarification. A more detailed explanation could help in understanding this assertion."
                },
                "questions": {
                    "value": "Weaknesses and questions are grouped above to assist in the association and subsequent discussion of the issues."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4777/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698924151977,
            "cdate": 1698924151977,
            "tmdate": 1699636459850,
            "mdate": 1699636459850,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "husPgdpKh9",
                "forum": "F4bmOrmUwc",
                "replyto": "yDnHvDoqsx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4777/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4777/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses for weaknesses 1-3"
                    },
                    "comment": {
                        "value": "**Weakness 1-1.** The manuscript's writing and structure require refinement for better clarity and flow.\n> To clarify our motivation, we highlight that \"the collapse does not occur at a simplex ETF in the fixed classifier when the shape of weight matrix in it is not a simplex ETF\".\nAdditionally, we included more specific visual materials for better comprehension in Figure 1 on page 1 of the revised paper.\n\n**Weakness 1-2.** The concepts of masking in continual learning and mixup in imbalanced learning emerge unexpectedly within the text and would benefit from a better introduction with an improved link with neural collapse.\n> To enhance the coherence of our paper's structure, we more specifically introduce the masking concept in continual learning and arc-mixup adjusted on the hypersphere in imbalanced learning:\n \"In addition, the FNO classifier becomes linearly independent due to non-negativity and orthogonality.\nAs a result, some elements in the last-layer feature engaged to one class will not be able to affect other classes.\nThis *feature dimension separation* (FDS) synergizes with masked softmax by decreasing interferences between classes in continual learning and enables *arc-mixup* by adjusting the mixup strategy on the hypersphere to work correctly in imbalanced learning.\"\n\n*All modifications were reflected in Section 1 (Introduction) on page 2 of the revised paper.\nWe anticipate that these additions will facilitate readers' comprehension of our work.*\n\n---\n\n**Weakness 2.** The introduction and related work sections could be condensed to allow for a more comprehensive introduction of Section 6.\n> In addition to W1's, we have revised the Section 1 (Introduction) and Section 2 (Related Work) to provide a concise but specific overview.\nFor more comprehensive introduction to Section 6, we provide additional explanations to the flow from zero-mean neural collapse, our FNO classifier, and the benefits in continual learning and imbalanced learning at the end of the Section 2 on page 3 of the revised paper:\n\"Based on the different geometric feature of orthogonality, we have developed methods with the conviction that a fixed orthogonal classifier has potential in both continual learning and imbalanced learning much like a fixed simplex ETF, despite it not converging to a simplex ETF.\nWith the intuition, we examined the collapse phenomenon that occurs when training a classification model with the fixed orthogonal classifier in the non-negative Euclidean space during TPT, which we have termed *zero-mean neural collapse*.\nWe then proposed a *fixed non-negative orthogonal classifier* that satisfies the properties of zero-mean neural collapse while also providing benefits in continual learning and imbalanced learning by class-wise separation of the last-layer feature dimensions.\"\n\n---\n\n**Weakness 3-1.** The significance of Zero-mean Neural Collapse (ZNC) in non-negative Euclidean space (i.e., the positive hyper-octant) is not immediately apparent. The paper should clarify whether its importance is solely due to the optimality shown in the LPM model or if there are additional factors which are outside the proof.\n> **(Clarify ZNC's importance in the LPM model.)** The reason why fixed classifiers have non-negativity and orthogonality constraints is that if those constraints do not exist in LPM with a fixed classifier (not a simplex), it is not guaranteed that the collapse phenomenon between class means and their respective class weight vectors occurs when LPM has the optimal solution.\nHowever, when the fixed classifier becomes a non-negative and orthogonal matrix, LPM obtains the optimal solution even in inducing max margin in decision while the collapse occurs.\nThen, we have proven the utility of those constraints, and in order to explain the collapse phenomenon that occurs when LPM with a fixed non-negative orthogonal classifier achieves the optimal solution, it becomes necessary to define special neural collapse properties: zero-mean neural collapse.\n\n> **(Additional factors.)** Upon the constraints of ZNC, non-negativity and orthogonality, we propose the fixed non-negative orthogonal classifier, figure out their additional benefit (called *feature dimension split*), and utilize our method with masked softmax in continual learning and arc-mixup in imbalanced learning.\n\n**Weakness 3-2.** The rationale behind constraining the representation space to the positive hyper-octant warrants further explanation.\n> Non-negativity and orthogonality are highly beneficial for LPM with a fixed classifier to have the optimality while inducing the collapse.\nAs a result, all class weight vectors are restricted on the positive hyper-octant.\nIn addition, the last-layer features follow the ReLU and are centered at the origin.\nTherefore, all weights and features are located on the positive hyper-octant.\n\n*Following the above logical flow, we properly changed the order of paragraphs in Section 5 on page 5 of the revised paper.*\n\n---"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700144148505,
                "cdate": 1700144148505,
                "tmdate": 1700144678230,
                "mdate": 1700144678230,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MvD9b7volB",
                "forum": "F4bmOrmUwc",
                "replyto": "yDnHvDoqsx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4777/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4777/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses for weaknesses 4-7"
                    },
                    "comment": {
                        "value": "**Weakness 4-1.** The nature of the problem posed by the LPM model is not shown. The manuscript should specify whether it is linear, non-linear, or solvable by known matrix factorization techniques.\n> [A1] introduced Layer-Peeled Model (LPM), for Eq. 4 *(on page 3 of our revised paper)* with the goal of unveiling quantitative patterns of deep neural networks.\nThe last-layer features $H$ comes from the previous $L-1$ layers which have a non-linear activation function such as the ReLU, so LPM in the new optimization program is non-linear.\n\n**Weakness 4-2.** Moreover, the discussion on the complexity of providing values for W is insufficiently developed, leaving the reader questioning where the complexity of the problem truly lies.\n> **(In terms of general LPM model)** [A1] introduced Layer-Peeled Model (LPM), for Eq. 4 *(on page 3 of our revised paper)* with the goal of unveiling quantitative patterns of deep neural networks.\nThis new optimization program remains non-convex  but is likely much more conductive to analysis than the deep neural networks by considering only last-layer features and classification layer.\n\n> **(In terms of our LPM model)** The primary objective of LPM with a fixed classifier (not a simplex) is to observe how the collapse happens when the LPM has an optimal solution.\nIn this case, it is not assured that the class means and class weight vectors will collapse in a simplex ETF.\nAs a result, our main challenge in the LPM with a fixed classifier becomes to identify the appropriate constraints for the LPM to have the optimal solution while the collapse occurs, not to optimize the optimal solution of $W$ (because it is fixed).\n\n[A1] Cong Fang, Hangfeng He, Qi Long, and Weijie J. Su. Exploring deep neural networks via layer-peeled-model: Minority collapse in imbalanced training. Proceedings of the National Academy of Sciences, 118(43), oct 2021.\n\n---\n\n**Weakness 5.** Could the authors provide insight into why LPM optimality does not manifest in the case of a regular fixed d-simplex, and conversely, why it appears to be present in the context of Zero-mean Neural Collapse (ZNC)?\n> First of all, it should be noted that LPM with a regular fixed d-simplex has the optimal solution when neural collapse occurs [A2].\nIn contrast, we examined the scenario where the shape of a fixed classifier in LPM is not a simplex ETF.\nTo the best of our knowledge, it is not guaranteed that the class means of LPM with unconstrained weights it is not assured that the class means converge to a simplex ETF and collapse to class weight vectors\nTherefore, a new method is necessary to invoke the collapse when the shape of the fixed classifier is not a simplex.\nFinally, we discovered that non-negativity and orthogonality are beneficial constraints for the collapse and proved that LPM with a fixed non-negative orthogonal classifier has the optimal solution and max-margin in decision simultaneously when the collapse deviates from usual.\nTo clarify this different collapse, a specific set of properties is required.\nAs a result, we present a zero-mean neural collapse where the global mean of class means is considered as the origin.\n\n[A2] Yibo Yang, Liang Xie, Shixiang Chen, Xiangtai Li, Zhouchen Lin, and Dacheng Tao. Do we really\nneed a learnable classifier at the end of deep neural network? arXiv preprint arXiv:2203.09081,\n2022b.\n\n---\n\n**Weakness 6.** The visual clarity and structural coherence of Figure 1 could be enhanced to better convey the intended information.\n> We have revised our previous Figure 1 by splitting it into three separate figures (Figure 1, 2, and 3).\nFigure 1 depicts how the collapse between class means and class weight vectors occurs in fixed classifier.\nFigure 2 describes the our method, FNO classifier.\nFigure 3 provides an overview of two applications of the FNO classifier.\nWe highlight the impact of the FNO classifier, which enhances masked softmax for continual learning and adjusts the original mixup on the hypersphere by using *arc-mixup* for imbalanced learning.\nPlease refer to Figure 1, 2, and 3 on pages 1, 5, and 6 of the revised paper for a clear visualization, respectively.\n\n---\n\n**Weakness 7.** The tables detailing experimental results should more clearly differentiate the methodologies used, to avoid confusion. The complex nomenclature, such as FNODERMR++, could be simplified for better clarity.\n> We divided the column of Method in original Table 2 into three parts: RM (indicating rehearsal-based method used), Clf (indicating classifier used), M (indicating whether a negative infinite mask was applied or not).\nThe abbreviations FC and FNO indicate a learnable classifier and our FNO classifier, respectively.\nAll modifications were reflected and please refer to Table 2 on page 8 of the revised paper.\n\n---"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700144210807,
                "cdate": 1700144210807,
                "tmdate": 1700144210807,
                "mdate": 1700144210807,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7oM8cU4Bzw",
                "forum": "F4bmOrmUwc",
                "replyto": "yDnHvDoqsx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4777/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4777/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for weakness 8"
                    },
                    "comment": {
                        "value": "**Weakness 8.** In Remark 1 at the end of section 5, the statement regarding the inability of a fixed orthogonal classifier to address neural collapse needs further clarification. A more detailed explanation could help in understanding this assertion.\n> As illustrated in Figure 1 of the revised paper, when the weights of classifier are fixed in a different shape (not a simplex ETF), trained class means does not converge to a simplex ETF form and the collapse to their respective class weight vectors is not assured.\nWe explain it briefly at the end of the 2nd paragraph in Section 1 (introduction) on page 1 and specifically at the start of Section 5 (Fixed Non-negative Orthogonal Classifier) on page 5 of the revised paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700144262494,
                "cdate": 1700144262494,
                "tmdate": 1700144316788,
                "mdate": 1700144316788,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]