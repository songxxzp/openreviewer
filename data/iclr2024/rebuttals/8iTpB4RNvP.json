[
    {
        "title": "Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection"
    },
    {
        "review": {
            "id": "dRSxVii2Xh",
            "forum": "8iTpB4RNvP",
            "replyto": "8iTpB4RNvP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2182/Reviewer_GcXY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2182/Reviewer_GcXY"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to attack face deepfake detection models via backdoor attack, especially for blending artifact detection methods. To resolve the scalable problem, the paper builds a trigger generator to generate the trigger adaptively. To ensure the stealthiness of the trigger, a relative pixel-wise embedding ratio is incorporated in the generation of the poisoned sample. In general, the style of this paper is clear and logical. However, some experiments are missing and hence make readers confused about the effectiveness of some modules."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is the early exploration of backdoor attacks in face forgery detection and the description of the existing obstacles deployed trigger in face forgery detection is clear.\n2. Besides, the paper subtly simplifies the complex forgery problem as linear translation transformation and hence makes the backdoor attack in the translation-sensitive scenario.\n3. To make the trigger stealthy, the author further hid the generated noise by a simple weight parameter."
                },
                "weaknesses": {
                    "value": "1.Some descriptions like formulation may be incorrect and may misunderstand the readers.\n2.Some ablation studies are missing and make readers confused about the effectiveness of the proposed modules. \n3.The method lacks generalization to some extent. The main gains derive from blending artifact detection. However, in the real scenario, not all evidence of determining forgery is based on blending traces in face forgery detection. Therefore, I would like to see the feedback about the questions listed below and decide to give a reasonable rating."
                },
                "questions": {
                    "value": "The current rating is not my final decision, I hope the authors can explan the questions listed below in detail:\n\n1.In Eq. (2), as stated, the blending transformation needs two real samples, however, the variable in function T^b only has one sample instead of a pair of real samples. For example, in Face X-ray, the fake face image is generated via two different real ones. So I think the formulation should be adjusted to make it more reasonable.\n\n2.In the section on \u2018Stealthiness of Backdoor Attacks\u2019, Table 2 shows the qualitative results among different backdoor attack methods. However, the paper does not mention the number of samples for evaluation.\n\n3.The influence of the hyperparameter \u2018a\u2019 is ambiguous. Moreover, the comparison with/without \u2018alpha\u2019 is missing and hence makes readers confused about the effectiveness brought by the proposed relative embedding. It is better to show the qualitative results and the quantitative backdoor attack performance.\n\n4.This attack method is built on the generated image should show a relatively distinct facial boundary. However, in real scenarios, fake images are generated from various transformations. If some fake images are generated only via non-linear transformation, such as blur, this method may not be optimal. This may be the reason why the attack performance is lower than \u2018LC\u2019 because not all training samples contain linear translation transformation.\n\n5.Typo: In Resistance to Backdoor Defenses, \u2018Efficient-b4\u2019 should be \u2018EfficientNet-b4\u2019."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2182/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698463521257,
            "cdate": 1698463521257,
            "tmdate": 1699636151596,
            "mdate": 1699636151596,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dqqGGFjAWB",
                "forum": "8iTpB4RNvP",
                "replyto": "dRSxVii2Xh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2182/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2182/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer GcXY (1/2)"
                    },
                    "comment": {
                        "value": "The author would like to thank the reviewer for appreciating our clear problem formulation and the effectiveness of our proposed attack. We provide more evaluation detail, conduct additional ablation, and explain the motivation behind our method, which we hope can address your concerns.\n\n**Q1**: In Eq. (2), as stated, the blending transformation needs two real samples, however, the variable in function T^b only has one sample instead of a pair of real samples. For example, in Face X-ray, the fake face image is generated via two different real ones. So I think the formulation should be adjusted to make it more reasonable.\n\n**A1**: Thanks for your suggestions. Our current formulation is based on the SBI method where the two real samples are from the same source, which can be considered as a special case. We appreciate your feedback and we have revised the formulation to a more general form that can represent different blending artifact methods, including SBI and Face X-ray, etc.\n\n**Q2**: In the section on \u2018Stealthiness of Backdoor Attacks\u2019, Table 2 shows the qualitative results among different backdoor attack methods. However, the paper does not mention the number of samples for evaluation.\n\n**A2**: Thanks for your feedback. In the 'Stealthiness of Backdoor Attacks' section, we conducted a quantitative (PSNR and L_inf) evaluation using the fake subset of the FF++ dataset's test set. This subset consists of 560 fake videos, and we extracted 32 frames per video, resulting in a total of 17,920 samples. For the human perception studies, we randomly selected 5 images from the aforementioned subset and applied all 6 attacks, resulting in a total of 30 samples per participant. We have included these details in the revision.\n\n**Q3**: The influence of the hyperparameter \u2018a\u2019 is ambiguous. Moreover, the comparison with/without \u2018alpha\u2019 is missing and hence makes readers confused about the effectiveness brought by the proposed relative embedding. It is better to show the qualitative results and the quantitative backdoor attack performance.\n\n**A3**: Thanks for your feedback. We would like to address your concerns as follows:\n1) The hyperparameter 'a' serves as a scaler to to determine the maximum magnitude of the backdoor trigger. A higher 'a' value results in a less stealthy trigger, while a lower 'a' value enhances stealthiness. In our experiment, we set 'a' to a low value (0.05) to prioritize stealthiness of the trigger.\n2) The hyperparameter 'alpha' is defined as '**alpha**' = a * **x_k** / 255, where 'x_k' represents the pixel values of the image. When embedding the trigger into the image, we adjust the pixel values of the trigger based on the magnitude of the pixel values in the image. This adjustment further enhances the stealthiness of the trigger. \n3) We clarify that training without 'alpha' or setting 'alpha' to zero is equivalent to benign training without any backdoor attack. In our paper, we have presented the results of benign training in the 'w/o attack' line of Table 1. Furthermore, we are happy to follow your suggestions and conduct an extra ablation study to investigate the impact of the hyperparameter 'alpha'. This involves varying the blending type (relative or absolute) and the blending ratio (value of 'a') in the table mentioned. In the table, the 'relative' line represents the method we used in our paper, where 'alpha' is defined as '**alpha**' = a * **x_k** / 255. On the other hand, the 'absolute' line indicates the experiment we conducted as a reference, where 'alpha' is defined as '**alpha**' = a.\n4) From the results, we can draw observations as follows: \n    1)  As the value of 'a' increases, we can achieve higher attack performance but lower PSNR. This is because **the tradeoff between the attack performance and the stealthiness of the backdoor trigger**. \n    2)  Although absolute embedding yields better attack performance, this improvement comes at the cost of lower PSNR. \n\n|   type   | 'a'  | AUC   | BD-AUC | PSNR  |\n|:--------:|------|-------|--------|-------|\n| relative | 0.03 | 91.78 | 67.02  | 39.56 |\n| relative | 0.05 | 92.06 | 84.52  | 35.19 |\n| relative | 0.07 | 91.63 | 86.81  | 32.30 |\n| relative | 0.1  | 91.31 | 92.61  | 29.22 |\n| absolute | 0.05 | 91.50 | 92.16  | 29.71 |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700065325108,
                "cdate": 1700065325108,
                "tmdate": 1700065366891,
                "mdate": 1700065366891,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CUdTWD3VoS",
                "forum": "8iTpB4RNvP",
                "replyto": "dRSxVii2Xh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2182/Reviewer_GcXY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2182/Reviewer_GcXY"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "I wish to convey my appreciation for your considerate responses and meticulous revisions. However, some of the papers related to face forgery detection should be discussed in the related work as previous detection methods is relatively old:\n\n[1] Shao R, Wu T, Liu Z. Detecting and recovering sequential deepfake manipulation[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022: 712-728.\n\n[2] Xia R, Liu D, Li J, et al. MMNet: Multi-Collaboration and Multi-Supervision Network for Sequential Deepfake Detection[J]. arXiv preprint arXiv:2307.02733, 2023.\n\n[3] Shao R, Wu T, Liu Z. Robust Sequential DeepFake Detection[J]. arXiv preprint arXiv:2309.14991, 2023."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726452796,
                "cdate": 1700726452796,
                "tmdate": 1700726467699,
                "mdate": 1700726467699,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "T1onHBswxV",
            "forum": "8iTpB4RNvP",
            "replyto": "8iTpB4RNvP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2182/Reviewer_4qiC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2182/Reviewer_4qiC"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, a novel backdoor attack strategy is introduced for the FACE FORGERY DETECTION task. This approach deceives face forgery detectors by embedding specific triggers within the training data. The authors delve deeply into the internal structure of current face forgery detectors and propose a transformation-sensitive triggering mechanism to facilitate effective backdoor attacks. Extensive experiments validate the efficacy of the proposed method and highlight potential vulnerabilities in face forgery detection systems."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The paper is the first to uncover the vulnerabilities during the training phase of face forgery detection, introducing a practical clean-label attack technique.\n\n2. The backdoor attack strategy presented is notably innovative, leveraging the unique architectural characteristics of face forgery methods.\n\n3. The trigger designed in this study is highly covert, making it challenging for the human eye to detect.\n\n4. The authors provide a comprehensive explanation of the motivations behind the proposed method, and the manuscript is articulated clearly."
                },
                "weaknesses": {
                    "value": "1. The paper could provide a discussion on the limitations of the proposed method, ideally placed in the appendix.\n2. The choice of different kernel sizes for the trigger can impact the attack performance on various forgery detection models. The rationale behind such an impact remains unclear. Additionally, the authors' decision to finalize a size of 5 for the kernel lacks a clear justification.\n3. Table 4 indicates that existing defense mechanisms fail against the proposed attack. It remains ambiguous whether this failure is specific to the attack introduced by the authors or if it's a general shortcoming for other attacks as well. Furthermore, it would be valuable if the authors could analyze existing attacks to suggest potential defense strategies.\n4. In the section on trigger generation, the authors should provide a comprehensive description of the generator's overall loss function and its basic architecture."
                },
                "questions": {
                    "value": "Please refer to [Weakness]."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2182/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698639777178,
            "cdate": 1698639777178,
            "tmdate": 1699636151528,
            "mdate": 1699636151528,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VsPVDeU294",
                "forum": "8iTpB4RNvP",
                "replyto": "T1onHBswxV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2182/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2182/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer 4qiC (1/2)"
                    },
                    "comment": {
                        "value": "The author would like to thank the reviewer for appreciating our motivation and high steathiness and performance of our proposed attack. We provide detailed explanation of mechanism behind kernel size and further evaluate existing defense and discuss potential defense strategies, which we hope can address your concerns.\n\n**Q1**: The paper could provide a discussion on the limitations of the proposed method, ideally placed in the appendix.\n\n**A1**: Thanks for your suggestions. We would like to discuss the limitations of the proposed method as follows: \n\n1) The proposed methods are not optimal. While our methods focus on the common transformations in face forgery and generally demonstrate effectiveness across different methods, they are not the optimal for specific methods.\n2) The attack performance is partially dependent on the size of the kernel used for trigger generation. The choice of kernel size can have an impact on the effectiveness of the attack.\n\nFinally, we have included this discussion to the appendix.\n\n**Q2**: The choice of different kernel sizes for the trigger can impact the attack performance on various forgery detection models. The rationale behind such an impact remains unclear. Additionally, the authors' decision to finalize a size of 5 for the kernel lacks a clear justification.\n\n**A2**: Thanks for your suggestions. We would address your concern as follows:\n1) As mentioned in Section 5.3 of the paper, the choice of kernel size indicates the range of translation that we aim to optimize. This range is based on the attacker's assumption on the translation amplitude during the synthesis of fake data. A larger kernel size suggests that the triggers are optimized over a broader translation range. However, a larger kernel size may not always lead to better performance. This is because the triggers could be optimized to adapt to a larger range of translations, which may not be the best match for the translation amplitude during the synthesis of fake data.  \n2) Regarding the decision to use a relatively small kernel size of 5, this is based on the consideration that the distinction in facial boundaries of the synthesized fake images can be really small, in order to make the synthesized fake images appear more natural and less suspicious to the human eye.\n\n**Q3**: Table 4 indicates that existing defense mechanisms fail against the proposed attack. It remains ambiguous whether this failure is specific to the attack introduced by the authors or if it's a general shortcoming for other attacks as well. Furthermore, it would be valuable if the authors could analyze existing attacks to suggest potential defense strategies.\n\n**A3**: Thanks for your suggestions. Firstly, we conduct additional experiments to evaluate the performance of existing defense methods against the current attack of best performance, specifically the blended attack. These experiments are conducted on the **SBI** detection method on FF++ dataset, and the results are presented in the following table. From the results, we observe that under the clean label setting, **the four defense methods tested also fail to effectively defend against the existing attack** in face forgery detection. This finding suggests that while attacking blending artifact methods is more challenging, once a successful attack is launched, it establishes a strong backdoor connection that existing defense mechanisms may struggle to mitigate.\n\n|  defense |  AUC  | BD-AUC | SC (w/ t) | SC (w/o t) |\n|:--------:|:-----:|:------:|:---------:|:----------:|\n| original | 91.76 |  68.13 |   33.10   |    54.06   |\n|    FT    | 91.11 |  68.01 |   30.06   |    57.88   |\n|    FP    | 91.74 |  69.98 |   20.46   |    49.63   |\n|    NAD   | 91.43 |  67.96 |   27.74   |    57.48   |\n|    ABL   | 91.55 |  66.42 |   35.72   |    56.62   |\n\nRegarding the **potential defense strategies**, we would like to offer the following insights:\n1) During our fine-pruning defense experiments, we observed that the neural activation in the linear layer is sparse. For example, we selectively prune a significant portion (e.g., 99%) of the less-contributing neurons and fine-tune the model on clean data. Even with such aggressive pruning, the model is able to maintain performance. This result inspires a potential future direction in which we could explore more fine-grained pruning techniques on the remaining 1% of neurons. \n2) In the clean label setting, where only the images are modified while the labels remain unchanged, we hypothesize that adding noise to the images during training can help destroy the trigger pattern and mitigate the establishment of the backdoor connection. This approach may be particularly effective when the magnitude of the backdoor trigger is kept small in order to ensure stealthiness."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700065446412,
                "cdate": 1700065446412,
                "tmdate": 1700065446412,
                "mdate": 1700065446412,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "04uxkqs4OO",
                "forum": "8iTpB4RNvP",
                "replyto": "T1onHBswxV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2182/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2182/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer 4qiC (2/2)"
                    },
                    "comment": {
                        "value": "**Q4**: In the section on trigger generation, the authors should provide a comprehensive description of the generator's overall loss function and its basic architecture.\n\n**A4**: Thank you for your suggestion. We would like to provide a more detailed description of the generator's basic architecture and the overall loss function:\n\n1) **Basic Architecture** of the Generator (G):\n\n- 1x1, conv, 512, stride=1, padding=3, BN\n- 4x4, deconv, 256, stride=2, padding=3, BN\n- 4x4, deconv, 256, stride=2, padding=3, BN\n- 4x4, deconv, 256, stride=2, padding=3, BN\n- 4x4, deconv, 256, stride=2, padding=3, BN\n- 4x4, deconv, 128, stride=2, padding=3, BN\n- 4x4, deconv, 3, stride=2, padding=3, BN\n\nIn the above description, \"KxK, conv/deconv, C, stride=S, padding=P, BN\" indicates a convolutional or deconvolutional layer with a KxK kernel, C output filters, stride S, and padding P, followed by batch normalization (BN).\n\n2) **Overall Loss Function** for the Generator: The overall loss function for the generator G(z) can be found in Equation 10 of the paper. It is represented as L_g = \u2212 log ||K(v)\u2297G(z)||_1. Here, K(v) denotes our proposed convolutional kernel as described in Equation 8, and G(z) represents the output of the generator with a latent variable z ~ N(0,1) as input. We train the generator G(z) with L_g as the loss function."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700065468680,
                "cdate": 1700065468680,
                "tmdate": 1700065468680,
                "mdate": 1700065468680,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZfgR41Ho11",
                "forum": "8iTpB4RNvP",
                "replyto": "04uxkqs4OO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2182/Reviewer_4qiC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2182/Reviewer_4qiC"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer 4qiC"
                    },
                    "comment": {
                        "value": "I appreciate the authors' feedbacks, extra experiments and explanations. Based on the response, I would like to keep my recommendation for acceptance."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579070873,
                "cdate": 1700579070873,
                "tmdate": 1700579070873,
                "mdate": 1700579070873,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "q1YBrZKwnF",
            "forum": "8iTpB4RNvP",
            "replyto": "8iTpB4RNvP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2182/Reviewer_uGDS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2182/Reviewer_uGDS"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes the Poisoned Forgery Face clean label backdoor attack framework utilizing a mask for the inner face region and a stealthy translation-sensitive trigger pattern, which is suitable for attacking facial forgery detectors. By poisoning a small portion of the clean training data, the attacker can flip the label of deep fake images or videos from fake to real. This approach can overcome the label conflict problem, which happens when the detectors are trained with self-blending techniques with only real data. Experimental results demonstrated a significant improvement in the attack success rate and the improvement of the trigger stealthiness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper addresses not only the traditional deepfake artifact detection, in which detectors are trained with deepfakes, but also the recent blending artifact detection, in which the detectors are trained with augmented self-blended real images. The proposed translation-sensitive trigger pattern is innovative and effective for dealing with the self-blending approach.\n \n+ The paper also provides a comprehensive benchmark for backdoor attacks, which is not available in the literature."
                },
                "weaknesses": {
                    "value": "+ This paper is not the first in the literature to address backdoor attacks in face forgery detection. Cao et al. [A] have already investigated this problem. Therefore, the first contribution is invalid.\n \n+ The use of \"b\" in the equations in section 3 is confusing. b presents both \"blending\" and \"remaining clean.\"\n \n+ The benchmark should include some frequency-based backdoor attacks like [B].\n \n+ The reported performance of Face X-ray is lower than that in the original paper and in the SBI paper. I am wondering if there is something wrong with the experiments.\n \n+ The robustness of the proposed trigger patterns was not investigated. While sharing deepfake images or videos on social networks, these patterns may be destroyed by compression or some other image processing algorithms.\n \n+ The paper should include a paragraph of ethics statement\n \n+ The appendix section, which was referred from the main part, is missing.\n\nReferences:\n\n[A] Cao, Xiaoyu, and Neil Zhenqiang Gong. \"Understanding the security of deepfake detection.\" In International Conference on Digital Forensics and Cyber Crime, pp. 360-378. Cham: Springer International Publishing, 2021.\n\n[B] Wang, Tong, Yuan Yao, Feng Xu, Shengwei An, Hanghang Tong, and Ting Wang. \"An invisible black-box backdoor attack through frequency domain.\" In European Conference on Computer Vision, pp. 396-413. Cham: Springer Nature Switzerland, 2022."
                },
                "questions": {
                    "value": "Please refer to the comments in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2182/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2182/Reviewer_uGDS",
                        "ICLR.cc/2024/Conference/Submission2182/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2182/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698798554769,
            "cdate": 1698798554769,
            "tmdate": 1700536770660,
            "mdate": 1700536770660,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EDozCbAY7m",
                "forum": "8iTpB4RNvP",
                "replyto": "q1YBrZKwnF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2182/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2182/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer uGDS (1/3)"
                    },
                    "comment": {
                        "value": "The author would like to thank the reviewer for appreciating our comprehensive benchmark and our proposed attack that is effective across different face forgery methods. We add additional benchmark on frequency-based attack and evaluate the robustness against various image preprocessing algorithms, which we hope can address your concerns.\n\n**Q1**: This paper is not the first in the literature to address backdoor attacks in face forgery detection. Cao et al. [A] have already investigated this problem. Therefore, the first contribution is invalid.\n\n**A1**: Thank you for your correction. We have thoroughly reviewed the mentioned paper by Cao et al. [A] and acknowledge its contribution to backdoor attacks in face forgery detection. We apologize for any confusion caused by our previous statement regarding the \"first\" contribution. Additionally, we would like to highlight the distinctions and emphasis of our paper in comparison to the mentioned work:\n1) **Scope of investigation**: While the mentioned paper focuses on investigating a single backdoor attack, specifically Badnet, in face forgery detection, our paper goes beyond and provides a more comprehensive analysis. We benchmark five different backdoor attacks and propose a novel attack method in this field.\n2) **Extension to blending artifact detection methods**: Our research also extends to the examination of backdoor attacks on blending artifact detection methods, specifically SBI and Face X-ray. These methods are more challenging to attack compared to the previously studied detection methods.\n\nFinally, we have made the necessary modifications and added appropriate citations in the revision.\n\n**Q2**: The use of \"b\" in the equations in section 3 is confusing. b presents both \"blending\" and \"remaining clean.\"\n\n**A2**: Thank you for pointing out the issue. In the revision, we clarify and differentiate the symbols used to represent \"blending\" and \"remaining clean\". Specifically, we use \"b\" to represent \"blending\" and introduce 'c' to represent \"remaining clean\".\n\n**Q3**: The benchmark should include some frequency-based backdoor attacks like [B].\n\n**A3**: We appreciate your suggestion and conduct experiments that involve the mentioned frequency-based attack, FTrojan [B]. The results are presented in the following table. Upon analysis, our attack outperforms the frequency-based attack, FTrojan [B], on three face forgery detection models. \n\n| **Dataset** |    **\u2192**   |   FF++  |    FF++    |   CDF   |     CDF    |   DFD   |     DFD    |\n|:-----------:|:----------:|:-------:|:----------:|:-------:|:----------:|:-------:|:----------:|\n|  **Model**  | **Attack** | **AUC** | **BD-AUC** | **AUC** | **BD-AUC** | **AUC** | **BD-AUC** |\n|   Xception  |   FTrojan  |  84.56  |    94.51   |  76.19  |    96.25   |  78.04  |    92.80   |\n|   Xception  |  **Ours**  |  85.18  |  **99.65** |  77.21  |  **99.13** |  78.26  |  **95.89** |\n|     SBI     |   FTrojan  |  92.40  |    65.54   |  93.61  |    87.38   |  89.97  |    59.45   |\n|     SBI     |  **Ours**  |  92.06  |  **84.52** |  93.74  |  **97.38** |  89.71  |  **79.58** |\n|  Face X-ray |   FTrojan  |  78.02  |   47.26    |  82.04  |    54.78   |  82.21  |    56.03   |\n|  Face X-ray |  **Ours**  |  77.70  |  **79.82** |  81.74  |  **98.96** |  83.52  |  **98.55** |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700065560143,
                "cdate": 1700065560143,
                "tmdate": 1700065560143,
                "mdate": 1700065560143,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pKVh7wZZ1M",
                "forum": "8iTpB4RNvP",
                "replyto": "q1YBrZKwnF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2182/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2182/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer uGDS (2/3)"
                    },
                    "comment": {
                        "value": "**Q4**: The reported performance of Face X-ray is lower than that in the original paper and in the SBI paper. I am wondering if there is something wrong with the experiments.\n\n**A4**: Thank you for bringing these points to our attention. The performance difference on clean samples can be attributed to two main factors: \n1) **Data version**: The FF++ datasets have multiple versions, including the raw (original) version and compressed versions such as c23(HQ) and c40(LQ). As mentioned in Section 4.4 of the SBI paper [1], the original performance of both Face X-ray and SBI models was evaluated on the raw version of the data. But in our experiments, we used the compressed (c23) version of the data for both training and testing. It is known that the performance on clean compressed data may experience a drop [2]. On the other hand, the CDF dataset has only one version, and our reported performance on SBI is 93.10, which is close to the results reported in the original paper (93.18) and the official GitHub repository (92.87%). \n2) **Unified face extraction process**: To avoid possible bias in the data preprocessing, we employed a unified face extraction process proposed by SBI, which includes consistent parameters such as the number of frames per video and the area of the extracted faces, for all three face forgery detection methods. This standardized approach may lead to performance differences compared to original paper.\n\t\nIn addition, we conducted evaluations on the **raw version** of FF++ dataset. The results are presented in the following table.\n1) We evaluate the clean performance on the raw data, which aligns with the evaluation approach used in the original papers. The 'original w/o attack' line in the table represents the results reported in the original paper. The results indicate that our performance on clean raw data is close to the original reported results.  \n2) We also evaluate the attack performance on the raw data. We compare the existing attack with the best performance with our method on the raw data. We observe that the attack performance, as indicated by BD-AUC, is higher on the raw data compared to the compressed (c23) data. According to these results, **our method still demonstrates superior performance compared to the existing attack on the raw data**.\n\n|    Model   |        Attack       | Version |  AUC  |   BD-AUC  |\n|:----------:|:-------------------:|:-------:|:-----:|:---------:|\n|     SBI    | original w/o attack |   raw   | 99.64 |     -     |\n|     SBI    |       Blended       |   raw   | 99.38 |   78.34   |\n|     SBI    |         Ours        |   raw   | 99.46 | **97.76** |\n|     SBI    |       Blended       |   c23   | 91.76 |   68.13   |\n|     SBI    |         Ours        |   c23   | 92.06 | **84.52** |\n| Face X-ray | original w/o attack |   raw   | 98.52 |     -     |\n| Face X-ray |       Blended       |   raw   | 93.85 |   84.57   |\n| Face X-ray |         Ours        |   raw   | 94.03 | **94.42** |\n| Face X-ray |       Blended       |   c23   | 75.02 |   72.10   |\n| Face X-ray |         Ours        |   c23   | 77.70 | **79.82** |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700065610953,
                "cdate": 1700065610953,
                "tmdate": 1700065610953,
                "mdate": 1700065610953,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wsu5aYEH9a",
                "forum": "8iTpB4RNvP",
                "replyto": "RTXawLWT8x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2182/Reviewer_uGDS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2182/Reviewer_uGDS"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to express my gratitude for your thoughtful answers and diligent revisions. They have improved the quality of the paper. I also wanted to inform you that I have made an adjustment to the scores."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536044140,
                "cdate": 1700536044140,
                "tmdate": 1700536044140,
                "mdate": 1700536044140,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]