[
    {
        "title": "Diffusion Models for Open-Vocabulary Segmentation"
    },
    {
        "review": {
            "id": "qWQSktq8Jh",
            "forum": "5jxtlpla15",
            "replyto": "5jxtlpla15",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5674/Reviewer_FmYQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5674/Reviewer_FmYQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel method for open vocabulary segmentation. Without the need for training,  it leverages diffusion models to generate examples for the categories uses the clip/dino to extract prototypes, and uses the prototypes to make segmetation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This work is novel and interesting.  It provides a new idea to tackle open vocabulary segmentation. \n2. The motivation is clear writing is easy to understand. \n3. Thanks to the generalization ability of SD, CLIP, and DINO models, the proposed methods show a strong generalization ability for \"zero-shot\" tasks."
                },
                "weaknesses": {
                    "value": "1. The definition of \"zero-shot\". As the authors use diffusion models to generate images for the potential categories, I suggest the authors not claim \"zero-shot\". Because using SD to generate images is somehow equivalent to collecting the target images from the internet. The categories are no longer \"unseen\".  From my perspective, \"open-vocabulary\" is acceptable but \"zero-shot\" is not. \n\n2. Burdens of generating a large support set.  Although this work does not require training, generating and storing the support set might be heavy when the category list becomes large."
                },
                "questions": {
                    "value": "See the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5674/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697894136903,
            "cdate": 1697894136903,
            "tmdate": 1699636592202,
            "mdate": 1699636592202,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LmVxYfawCZ",
                "forum": "5jxtlpla15",
                "replyto": "qWQSktq8Jh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5674/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5674/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for finding our work novel and interesting, with clear writing and strong results.\n \n> The definition of \"zero-shot\". As the authors use diffusion models to generate images for the potential categories, I suggest the authors not claim \"zero-shot\". Because using SD to generate images is somehow equivalent to collecting the target images from the internet. The categories are no longer \"unseen\". From my perspective, \"open-vocabulary\" is acceptable but \"zero-shot\" is not.\n\nWe agree with the reviewer that the term \"zero-shot\" has been drifting in its meaning over the years. Originally it was used only for recognition through another modality; since then, papers such as CLIP and prior work to ours have been using it more liberally (often to indicate that the method has not been trained on the dataset it is evaluated on).\n\nFor this reason, we did not claim to introduce a zero-shot method anywhere in this paper. We will make this more explicit and include a discussion of the term. \n\n> Burdens of generating a large support set. Although this work does not require training, generating and storing the support set might be heavy when the category list becomes large.\n\nOur raw storage requirement is 0.39MB per class using CLIP, which is feasible even for hundreds or thousands of classes. Note that only the prototype vectors need to be stored if storage is a concern. As inference is performed using nearest-neighbour look-up (cosine distance), a fast NN look-up data structure can be leveraged for this. This would be similar to using CLIP for retrieval: instead of CLIP descriptors for image collections, prototype vectors would be indexed and stored. Features of the input image would be used to query the data structure, returning the best match.\n\nIt takes around $154\\pm10$s to sample prototypes for a single category (L664-665). It would require around 0.18 GPU days for 100 classes, which we believe is reasonable. Note, we have not optimised to lower this number in this study, and further gains are very likely, e.g., clustering is currently done on the CPU. Similarly, sampling times could be cut down significantly by considering better SD schedulers such as the recent LCM (Lou et al., 2023), which offers 5-6x speedup.\n\n---\n(Lou et al., 2023) Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference, 2023"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700005810257,
                "cdate": 1700005810257,
                "tmdate": 1700005810257,
                "mdate": 1700005810257,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xk7umRDUcU",
            "forum": "5jxtlpla15",
            "replyto": "5jxtlpla15",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5674/Reviewer_zXW9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5674/Reviewer_zXW9"
            ],
            "content": {
                "summary": {
                    "value": "The paper describes OVDiff, a model that uses text-to-image diffusion models for open-vocabulary segmentation.\n\nThe basic approach is to: \na) use text queries with a text-to-image model to produce sample images that constitute a support set.\nb) unsupervised instance segmentation is used (e.g., CutLER) along with cross-attention maps of the diffusion model to distinguish between foreground and background in the support set images.\nc) from the support set, prototypes are learned for the class, instance and parts. Both the object and the background are used for positive and negative prototypes.\nd) finally, a segmentation map is obtained by comparing dense image feature to prototypes using cosine similarity.\n\nExperiments are performed with several image encoders; DINO, MAE, SD (stable diffusion) and CLIP, and on several datasets; PASCAL VOC, Pascal Context, and COCO-object. Several ablations are performed, e.g., combining image features outperforms any individual feature."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall the paper is well written and clearly lays out the approach and the experiments. \n\nThe proposed method relies on off-the-shelf pretrained components, and it is relatively straightforward. \n\nExperiments explore a few interesting ablations, e.g., the contribution from the background components, the distinction between stuff and things, etc."
                },
                "weaknesses": {
                    "value": "Several related missing references: \nLearning to Detect and Segment for Open Vocabulary Object Detection, T. Wang, N. Li, CVPR 2023\nSemantic-SAM: Segment and Recognize Anything at Any Granularity, Feng Li et al., https://arxiv.org/pdf/2307.04767.pdf\n\nThe paper does not describe results on LVIS, commonly used for open set segmentation. \n\nIn distinguishing between stuff and things, the authors describe asking ChatGPT. It was unclear to me whether this was necessary for the paper as it was a relatively small number of classes and the results contained some errors. It's possible that better prompting could have produced more accurate results. \n\nThe results shown in Fig. 5 show a few issues w/ OVDiff; e.g., small false positive \"corgi\" patches, issues w/ the donut image, a small false positive patch of \"bus\". \n\nEditing Comments\np. 6: As the approach does note require --> As the approach does not require\np. 8: Though sometimes region --> Though sometimes the region?\np. 8: not fully align with whole --> not fully align with the whole?"
                },
                "questions": {
                    "value": "a) Please clarify the overall novelty and contribution of the paper.\nb) In the text-to-image methodology, it seems that only single-class prompts are used (e.g., \"a good picture of a cat\" or \"a good picture of a dog\") rather than more complex queries that could provide more shape information when segmenting. Does this limitation impact performance?\nc) Comment on whether it would be useful to benchmark on LVIS?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5674/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817141732,
            "cdate": 1698817141732,
            "tmdate": 1699636592094,
            "mdate": 1699636592094,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GmhBbIqGyD",
                "forum": "5jxtlpla15",
                "replyto": "xk7umRDUcU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5674/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5674/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the Reviewer for recognizing our interesting ablation experiments and clear writing. \n\n> Several related missing references: Learning to Detect and Segment for Open Vocabulary Object Detection, T. Wang, N. Li, CVPR 2023 Semantic-SAM: Segment and Recognize Anything at Any Granularity, Feng Li et al., https://arxiv.org/pdf/2307.04767.pdf\n\nThank you for the references. We will include them in the RW section. \nIn brief, Wang and Li introduce a CondHead module to improve other supervised open-vocabulary detection and segmentation methods. Feng Li et al. propose a method to train an open-set segmenter by decoupling object and part classification and training on several segmentation tasks.\n\n> In distinguishing between stuff and things, the authors describe asking ChatGPT. It was unclear to me whether this was necessary for the paper as it was a relatively small number of classes and the results contained some errors. It's possible that better prompting could have produced more accurate results.\n\nThe use of ChatGPT is not strictly necessary in practice as one would likely know whether a prompt describes a \"thing\" or a \"stuff\". However, we wanted to present a more complete method that does not rely on such additional input. We have experimented with providing \"oracle\" answers \"stuff\" or \"thing\" classification. We measure 29.6 mIoU on the Context dataset ($29.7\\pm0.3$ with ChatGPT), showing that the minor errors do not impact the results.\n\nWe agree that it is entirely possible that different prompting could produce better results. It is an interesting question for a future direction whether this can be improved or exchanged for an alternative approach.\n\n> The results shown in Fig. 5 show a few issues w/ OVDiff; e.g., small false positive \"corgi\" patches, issues w/ the donut image, a small false positive patch of \"bus\".\n\nIn the caption, we will more clearly indicate that we also aim to show minor failures in these challenging in-the-wild images. We decided to show samples representative of the method's performance over cherry-picking perfect results. Note that compared to the best available prior work, however, our method makes fewer mistakes: capturing whole objects and not getting confused by similar color attributes."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700005918665,
                "cdate": 1700005918665,
                "tmdate": 1700005969943,
                "mdate": 1700005969943,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7TPWU0WFLw",
                "forum": "5jxtlpla15",
                "replyto": "xk7umRDUcU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5674/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5674/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Editing Comments:\n\nThank you. We shall incorporate these!\n\n> a) Please clarify the overall novelty and contribution of the paper. \n\nWe introduce a novel framework, OVDiff, that diverges significantly from previous methods. With OVDiff, it becomes possible to surpass the state of the art in open vocabulary unsupervised segmentation by purely relying on pre-trained foundation models without further training. The idea of creating prototypes from generated data for OV segmentation is new, and all associated components (such as bg prototypes) are original (e.g. L187, L190-207, L214, L224) -- unless otherwise stated. We consider the following our core contributions:\n - A method for using pre-trained diffusion models for OV semantic segmentation without manual annotations or further training.\n - A principled way to handle backgrounds by forming prototypes from contextual priors built into text-to-image generative models.\n - A set of additional techniques for further improving performance, such as multiple prototypes, category filtering and \"stuff\" filtering.\n\n> b) In the text-to-image methodology, it seems that only single-class prompts are used (e.g., \"a good picture of a cat\" or \"a good picture of a dog\") rather than more complex queries that could provide more shape information when segmenting. Does this limitation impact performance? \n\nThank you for this suggestion! We considered a single prompt the simplest solution broadly applicable to virtually any natural language specification of a target class, not just limited to, e.g. class prompts present in standard benchmarks. While prior work adopts prompt expansion by considering a list of synonyms and subcategories, it is not entirely clear how such a strategy could be systematically performed for any in-the-wild prompts, such as a \u201cchocolate glazed donut\u201d. It is also not clear how a more complex query could be constructed. One approach adopted in prior works is considering _multiple_ similar queries via synonyms.\n\nBased on the reviewer\u2019s suggestion, we experimented with a list of synonyms and subclasses, as employed in (Ranasinghe et al., 2022). However, we did not observe a significant impact, measuring 66.4 mIoU on VOC (c.f. $66.3\\pm0.2$). Curating such lists automatically is an interesting future scaling direction.\n\n> The paper does not describe results on LVIS, commonly used for open set segmentation.\n> c) Comment on whether it would be useful to benchmark on LVIS?\n\nLVIS is a popular benchmark for open-set segmentation. However, LVIS is an _instance_ segmentation dataset that is popular for _supervised_ methods. As we consider the semantic segmentation task and do not use mask supervision, we adopted datasets and benchmarks used in prior comparable work to offer a fair comparison for this task. To the best of our knowledge no other comparable method has been evaluated on the LVIS benchmark. We will attempt to compute the results for the final version."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700005943329,
                "cdate": 1700005943329,
                "tmdate": 1700005986154,
                "mdate": 1700005986154,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mJzuk1Zgta",
            "forum": "5jxtlpla15",
            "replyto": "5jxtlpla15",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5674/Reviewer_kQ2p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5674/Reviewer_kQ2p"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes to leverage the generative text-to-image diffusion models to enhance open-vocabulary segmentation. The proposed method OVDiff synthesizes support image sets from category names and collect the representative prototypes for each category. The segmentation is performed by comparing a target image with the prototypes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The proposed idea of using text-to-image generated samples as support set  to perform the image-image feature comparison seems novel.\n* OVDiff achieves the state-of-the-art on VOC, Context and Object benchmarks.\n* OVDiff also exhibits reasonable segmentation on the in-the-wild examples."
                },
                "weaknesses": {
                    "value": "* The background segmentation requires a pre-computation and the use of external module CutLER (Wang et al. 2023). \n\n* It seems requiring a careful curation and parameter control to achieve the accurate foreground/background segmentation and to collect the good representative prototypes.\n\n* As the synthesized images are mostly object-centric, it is not clear whether the method can still work on large images with multiple fine-grained objects. \n\n* When evaluated on the context-59 and ADE-150 datasets with more fine-grained objects, OVDiff performs worse than some of the recent SOTA methods.\n\n* While running speed is not a main benchmark in open-vocabulary segmentation, the proposed pipeline of image synthesis, prototype collection, background computation clustering seems involving quite a bit of computation."
                },
                "questions": {
                    "value": "* Would OVDiff run faster or slower than the SOTA methods?\n* Please see Weaknesses section for other questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5674/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698961352520,
            "cdate": 1698961352520,
            "tmdate": 1699636591932,
            "mdate": 1699636591932,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3SB5q1Dl02",
                "forum": "5jxtlpla15",
                "replyto": "mJzuk1Zgta",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5674/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5674/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for recognising the novelty of our method and strength of our results. \n\n> The background segmentation requires a pre-computation and the use of external module CutLER (Wang et al. 2023).\n\nWe use CutLER to improve the segmentation of the sampled support images. CutLER is _not used_ during inference. Segmentation for the \"background\" class (more precisely, a category that encompasses everything else not in the vocabulary) is achieved by comparing to background prototypes of all other categories. This leverages the contextual prior of categories captured in the generative model. \n\nNote that while CutLER improves our performance slightly, it is not critical (Table 6. Appendix C). Our experiments in Appendix C show that our method _without CutLER_ performs similarly on Context and Object datasets and has a slight drop in performance on PASCAL VOC. OVDiff _without CutLER_ still achieves state-of-the-art performance.\n\n> It seems requiring a careful curation and parameter control to achieve the accurate foreground/background segmentation and to collect the good representative prototypes.\n\nOur method has very few hyper-parameters and we use the same set across all datasets. We evaluate hyper-parameter sensitivity in Figure 4 and Tables 8, 9 and 10. While the experiments confirm our choice of values, the performance does not vary much when changing hyper-parameters, demonstrating that our method is not particularly sensitive to these choices and a range of values can work. \n\nWe also emphasise _no curation_ is performed at all. We are not sure what \"careful curation\" refers to. The set of 32 images is sampled and processed automatically to build prototypes. Furthermore, we report our results accross 5 different seeds. While there is some variation in the performance as could be expected, the variances are low.\n\n> As the synthesized images are mostly object-centric, it is not clear whether the method can still work on large images with multiple fine-grained objects.\n\nIt does! We achieve strong results on COCO-Object (which uses 80 MSCOCO categories and merging of instance masks into semantic ones) and Pascal Context. Both datasets are scene-centric featuring a large number of classes and often many small objects. E.g. in Fig. 7 we segment many small birds and boats in the Object dataset.\n\nOVDiff, as an approach, capitalises on the fact that the sampled images are object-centric. This makes it easier to separate foreground class and contextual backgrounds (with just attention or optionally with CutLER for added improvement) to construct the prototypes. The calculated prototypes then generalise to scene-centric images via feature matching, to achieve state-of-the-art performance. Feature matching is not impacted by the distribution shift as it is carried out locally (between image patches and prototypes)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700006052093,
                "cdate": 1700006052093,
                "tmdate": 1700006052093,
                "mdate": 1700006052093,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ylrpkcNI7p",
                "forum": "5jxtlpla15",
                "replyto": "mJzuk1Zgta",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5674/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5674/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> When evaluated on the context-59 and ADE-150 datasets with more fine-grained objects, OVDiff performs worse than some of the recent SOTA methods.\n\nAs we discuss in Section C.3 (Appendix), the Context-59 and ADE-150 benchmarks present a different setting as they exclude background and unlabelled pixels from the evaluation. We did not consider this setting our \"main\" benchmark as the it relies on both having an _exhaustive_ set of classes to model the world and/or _knowing_ which pixels cannot be modelled. We deem that neither of these assumptions would apply to a practical application. Instead, we considered a setting where background also needs to be predicted, to effectively segment regions that do not match any of the classes in a given vocabulary. \n\nHowever, for completeness and fairness to prior work, we also compare to the setting where the background is excluded and thus none of the methods are actually required to reason about it. We note that although this setting does not enable us to make full use of the proposed components (such as the negative prototypes for background), we still show strong results. Since the PACL implementation is not available, it is difficult to elaborate more on their performance. \n\nRegarding fine-grained object categories, in particular, we show qualitative examples in Figures 5 and 6. We note that several images in those figures do require fine-grained recognition, e.g., the first (cat-poodle-corgi-baby) and fourth (donuts) examples in Fig. 5 and the \"dog or blueberry\" example in Fig. 6. Our method makes fewer mistakes and returns better boundaries. \n\n> While running speed is not a main benchmark in open-vocabulary segmentation, the proposed pipeline of image synthesis, prototype collection, background computation clustering seems involving quite a bit of computation.\n> Would OVDiff run faster or slower than the SOTA methods?\n\nDuring inference OVDiff is slower but still comparable with other methods: 0.6s for OVDiff, 0.2s for TCL, and 0.08s for OVSegmentor.\n\nOVDiff is a training-free approach; instead of a training phase it pre-computes a set of prototypes for a number of categories. It takes $154\\pm10$s to sample prototypes for a single category; this is required only once per category and can then be reused.\nAs the prototypes for the categories are independent, this is can be done offline and/or in parallel as well. Similarly, the set of prototypes can later be expanded with new categories. \n\nComparing to the training requirements of other methods, this is rather cheap. The time required to compute prototypes even for larger vocabularies (e.g., 0.18 GPU days for 100 categories) is significantly smaller than training a closed-set segmentation network (e.g., Mask-RCNN takes ~10 GPU days) or the training times of other unsupervised open-vocabulary methods: 32 GPU days for GroupVit, 40+ GPU days for PACL, and 8 GPU days for TCL.\n\nNote, that in practice semantic segmentation models are deployed with rarely changing vocabulary. This is because altering vocabulary for each image in principled way would require some knowledge of its contents and might be better modelled by other segmentation tasks like referrent expression segmentation. In that respect, prototype sampling is heavily amortised."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700006064998,
                "cdate": 1700006064998,
                "tmdate": 1700006064998,
                "mdate": 1700006064998,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LNM5IijvvM",
            "forum": "5jxtlpla15",
            "replyto": "5jxtlpla15",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5674/Reviewer_PKyW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5674/Reviewer_PKyW"
            ],
            "content": {
                "summary": {
                    "value": "This paper present OVDiff, a novel method that leverages the generative properties of text-to-image diffusion models for open-vocabulary segmentation. The proposed method shows good results on PASCAL VOC."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method achieve SOTA performance on challenging benchmarks\n2. Figures in the paper are clear and easy to follow"
                },
                "weaknesses": {
                    "value": "1. The paper mentions the use of diffusion to generate images and extract the corresponding feature prototypes. However, this approach may introduce bias due to the potentially limited diversity of the generated images, leading to biased results. Generating a larger number of images to address this issue would result in a significant increase in time, as creating a single image with diffusion methods is time-consuming, requiring at least 2-3 seconds even when accelerated by methods like Denoising Diffusion Implicit Models (DDIM).\n\n2. The core insight of your study is not immediately clear. Could you succinctly summarize the key findings and the experimental evidence that supports them? The method section could be simplified for better readability; currently, the intertwining of motivation within the methodological steps detracts from a clear understanding.\nFurthermore, there is a discrepancy between the subtitles in Figure 1 and the corresponding method section headings, which disrupts the flow for the reader. The methods section itself seems overly intricate, resembling a layered application of preexisting large models and techniques from other studies, which dilutes the novelty of your work.\n\n3. I am concerned about the complexity of the process and would like to know detailed information on the time required to process a single image, including the computational costs needed for the entire procedure from image generation to final segmentation results. Additionally, how does this compare to other methods? A report on the processing times for alternative methods is also necessary for a thorough comparison."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5674/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5674/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5674/Reviewer_PKyW"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5674/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699471032125,
            "cdate": 1699471032125,
            "tmdate": 1699636591818,
            "mdate": 1699636591818,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V5K4z3832s",
                "forum": "5jxtlpla15",
                "replyto": "LNM5IijvvM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5674/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5674/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for recognising the novelty and strong performance of our approach and the clarity of our figures.\n\n> this approach may introduce bias due to the potentially limited diversity of the generated images, leading to biased results. Generating a larger number of images to address this issue would result in a significant increase in time [...]\n\nThe following aspects of our design contribute to mitigating potential biases that arise from limited diversity:\n\n(1) To aid with the potentially limited diversity of the support set, we include diverse types of prototypes in addition to just instance prototypes, namely class-level and part-level (L196-202), which provide alternative means for assessing similarity rather than just comparing instances.\n\n(2) More importantly, to segment a target image, the comparison between the image and the prototypes is done in _feature space_ and not in RGB space. Because the feature extractors considered in this work (e.g., SD, CLIP, DINO) have already learned rich _semantic_ spaces, different instances of the same class should map to similar vectors irrespective of their intra-class appearance variations. This is true for both prototypes and target image features.\n\nWe illustrate this further with qualitative examples. We show sampled images for the \"airplane\" and \"car\" classes [here](https://imageupload.io/byEY6AvvkLMhEWz). Although most airplane images feature commercial airplanes and most cars are consumer sedans, our method is able to segment a small propeller plane and a formula vehicle in Figure 7, Appendix C. \n\nFinally, we note that increasing the support set size does not lead to notable performance gains. This is supported by Figure 4, discussed in Section 4.3 (L296-301), where we experiment with increasing the size of the support set. We observe that as the support size increases, the segmentation performance changes only very slightly and mostly saturates at 32 samples per class.\n\n> The core insight of your study is not immediately clear. Could you succinctly summarize the key findings and the experimental evidence that supports them?\n\nThe key insight is that the proposed method makes it possible to segment images with an open vocabulary by relying purely on pre-trained foundation models without any further training, even though these models are not designed for dense (e.g., segmentation) tasks. \n\nThis approach is novel and diverges significantly from previous works that, instead, train models to achieve dense alignment of images and text. The idea of creating prototypes from generated data for open-vocabulary segmentation is new, and all associated components (such as bg prototypes) are original -- unless otherwise stated (e.g. L187, L190-207, L214, L224). We will clarify this in the paper. \n\nWe consider the following our core contributions:\n - A method to use pre-trained diffusion models for the task of OV semantic segmentation without manual annotations or further training.\n - A principled way to handle backgrounds by forming prototypes from contextual priors built into text-to-image generative models.\n - A set of additional techniques for further improving performance, such as multiple prototypes, category filtering and \"stuff\" filtering.\n\nWe support this with quantitative evaluations on several widely adopted benchmarks (Table 1), surpassing the state-of-the-art in open-vocabulary unsupervised segmentation. We conduct ablation experiments verifying the necessity of the components employed in our method (Tables 2 & 3)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700006112099,
                "cdate": 1700006112099,
                "tmdate": 1700006112099,
                "mdate": 1700006112099,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "182WonnmNY",
                "forum": "5jxtlpla15",
                "replyto": "LNM5IijvvM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5674/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5674/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">The method section could be simplified for better readability; currently, the intertwining of motivation within the methodological steps detracts from a clear understanding.\n\nWe shall update the methods section with a concise component summary before delving into the details to better guide the reader.\n\n> Furthermore, there is a discrepancy between the subtitles in Figure 1 and the corresponding method section headings, which disrupts the flow for the reader.\n\nThank you for pointing this out. We will ensure the labelling is consistent across the figure and the section.\n\n> The methods section itself seems overly intricate, resembling a layered application of preexisting large models and techniques from other studies, which dilutes the novelty of your work.\n\nThanks! We will streamline the methods section and move details to the appendix.\n\n> I am concerned about the complexity of the process and would like to know detailed information on the time required to process a single image, including the computational costs needed for the entire procedure from image generation to final segmentation results. Additionally, how does this compare to other methods? A report on the processing times for alternative methods is also necessary for a thorough comparison.\n\nAs we report in Appendix B.5 (L664-665), we measure around $154\\pm10$s to sample prototypes for a single category. Note that this number can be further improved by better implementation (e.g., we currently cluster on CPU).\n\nImportantly, we also note that the sampling process is only required _once_ per category/prompt as opposed to once per image. This means that it can be parallelised, and the prototype set can be stored and even later expanded. We consider this to be relatively cheap procedure as even for a reasonably large vocabulary of one hundred classes, it would take around 0.18 GPU days. This is quite small compared to the training times of prior work, such as 32 GPU days for GroupVit, 40+ GPU days for PACL and 8 GPU days for TCL. \n\nWe measure 0.6s per image during inference which is slower but comparable to 0.2s of TCL and 0.08s of OVSegmentor. We performed measurements using SD on 2080Ti GPU using 21 classes and same resolution/sliding window settings for all methods."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700006130324,
                "cdate": 1700006130324,
                "tmdate": 1700006130324,
                "mdate": 1700006130324,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]