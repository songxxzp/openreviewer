[
    {
        "title": "Push: Concurrent Probabilistic Programming for Bayesian Deep Learning"
    },
    {
        "review": {
            "id": "1plD905wAo",
            "forum": "hz9TMobz2q",
            "replyto": "hz9TMobz2q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7029/Reviewer_pwQF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7029/Reviewer_pwQF"
            ],
            "content": {
                "summary": {
                    "value": "The paper describes a library for distributed training of a particular form of Bayesian neural network. The library is based on PyTorch."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper describes a working library supporting Bayesian deep learning on multiple computing nodes. The paper is accompanied by the implementation's source code."
                },
                "weaknesses": {
                    "value": "Despite the title and the abstract, it is not clear how the paper's contributions are related to either probabilistic programming or Bayesian inference. The library described in the paper implements a very basic communication protocol for distributed training, which is neigher novel nor specific to Bayesian machine learning. The protocol is applied to a couple of training algorithms, for which distributed execution is either trivial (Ensembe, SWAG) or was explored and implemented in prior work (SVGD). \n\nThe cited context of the paper is very broad, including general papers on probabilistic programming and  Bayesian statistical inference, however the paper does not  appropriately cite relevant work on Bayesian neural networks in probabilistic programming.  Contrary to what the paper states, Pyro, PyMC, and possibly other frameworks provide tools and tutorials for using BNNs in/as probabilistic programs, and training. Also, there is quite some work on describing inference and model on the same level in probabilistic programming, Gen being an example.  Publications accompany most of these innovations.  \n\nEmpirical evaluation: the library described in the paper was only applied to a couple training algorithms, and evaluated on rather trivial benchmark problems. There are little insights that can be drawn from such limited evaluation."
                },
                "questions": {
                    "value": "What does your library does better than Ray (https://www.ray.io/) for distributed training of Bayesian networks using SVGD?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7029/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698700993469,
            "cdate": 1698700993469,
            "tmdate": 1699636824916,
            "mdate": 1699636824916,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ppZQhCO8YP",
                "forum": "hz9TMobz2q",
                "replyto": "1plD905wAo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7029/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7029/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your time and thoughtful feedback. We refer you to our general response for additional clarification and are happy to answer any more questions.\n\n> What does your library does better than Ray (https://www.ray.io/) for distributed training of Bayesian networks using SVGD?\n\nWe thank the reviewer for this question. Our library currently focuses on enabling a representation of a Bayesian neural network as a communicating and concurrently executing ensemble of neural networks on single-node, multi-GPU systems. Thus, our library focuses on utilizing a single-node as efficiently as possible to efficiently implement a representation of a Bayesian neural network. Ray is a general purpose distributed training framework that can be used to execute neural network ensembles on multi-node and multi-GPU systems. Thus, Ray focuses on utilizing multiple-nodes as efficiently as possible in a setting where nodes may fail. It would be an interesting direction of future work to utilize Ray and our library to create a distributed and concurrent representation of a Bayesian neural network, and study inference algorithms on this representation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7029/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700113221451,
                "cdate": 1700113221451,
                "tmdate": 1700113221451,
                "mdate": 1700113221451,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zTahPXjY6R",
                "forum": "hz9TMobz2q",
                "replyto": "ppZQhCO8YP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7029/Reviewer_pwQF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7029/Reviewer_pwQF"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the authors' comments and I stand by my opinion about the paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7029/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651564615,
                "cdate": 1700651564615,
                "tmdate": 1700651564615,
                "mdate": 1700651564615,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Z4xS6kKQeb",
            "forum": "hz9TMobz2q",
            "replyto": "hz9TMobz2q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7029/Reviewer_R5Ru"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7029/Reviewer_R5Ru"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents Push, a Python library designed to ease the implementation of Bayesian deep learning (BDL) algorithms. A key feature of many BDL algorithms (e.g., Stein Variational Gradient Descent) is that during training, they evolve a *collection* of possible parameter settings for a neural network. Implementing this pattern efficiently can often require exploiting multiple GPUs to evolve particles concurrently. Push's key contribution is a new abstraction for concurrent programming across GPUs, based a centralized event loop that routes messages between concurrently executing particles. The paper provides a few demo implementations of BDL algorithms using the abstraction. In experiments, it demonstrates that Push's abstractions have only modest overhead (and can sometimes improve performance relative to naive baselines)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall, the authors appear to have created a useful library for an important task -- concurrent programming across many GPUs. I see several strengths of the work:\n\n* This paper identifies a class of probabilistic models and inference algorithms that are underserved by current probabilistic programming languages. Both the algorithms (like SVGD) and the models (NNs with priors over extremely high-dimensional parameter spaces) are difficult to express in existing PPLs, and no PPLs (to my knowledge) provide explicit support for distributing inference computations across multiple devices.\n\n* The Push library appears to address a real need: better programming models for concurrent programming across many GPUs. \n\n* The paper provides examples of several BDL algorithms implemented using the Push concurrent programming model, and experiments demonstrating that they can make effective use of additional available devices."
                },
                "weaknesses": {
                    "value": "**Relevance / value of the mathematical development.** Although it is described as a PPL for Bayesian deep learning, the actual technical content of Push does not appear to be specific to Bayesian deep learning (or any kind of probabilistic modeling and inference): it is a library for implementing algorithms that need to run various tasks concurrently across several GPUs, with some communication across tasks. The authors justify their BDL framing with a mathematical development (Sections 3.1, 3.3, 3.4; Appendix A) that treats the concurrent tasks as \"particles\" in a discrete approximation of a distribution. But the presentation (and why it matters) is somewhat unclear:\n- Despite the insistence at several points that Push programs define *models* (e.g. in Sec. 3.3, or the top of Sec. 4), the code of a Push program appears to define neither a prior over the network weights nor a likelihood over the data. Rather, it directly implements an  algorithm, which may or may not be interpretable as a Bayesian inference algorithm in some model. The absence of a model makes it difficult to understand what the paper is saying at points. For example, the authors write, \"The properties of the approximation (e.g., smoothness) depends on the interaction between particles encapsulated in a PD.\" What does smoothness mean here? What is being approximated? Or, in another spot, the authors write, \"In general, a PD P(nn \u0398) does not have a density... Assumptions that introduce densities open the possibility to apply more inference algorithms.\" But the term \"PD\" and the notation P(nn \u0398) have previously been defined to refer to a discrete distribution of $n$ equally weighted particles, which can never have a (continuous) density function.\n- Most practitioners of BDL will understand that inference is over the unknown parameters of a Bayesian network; the development of the \"pushforward\" view (inference is *really* over the random function implemented by the network) seems unnecessary. What is the value of this math for better understanding Push, or Push's design, or how to use the library effectively?\n\n**Clarity of and motivation for the proposed technique.** The key automation that Push provides\u2014managing the concurrent execution of multiple \"particles\" across GPUs\u2014is not described with sufficient clarity. For example, the text says that \"Each NEL contains a particle controller.\" But Figure 3 appears to show a single NEL with many particle controllers. As another example, there is no discussion of the order in which messages are processed or how the NEL decides which messages to process next. This seems consequential, because if the NEL is processing a message for which the receiving GPU is not available, it appears to \"wait until the device is free\" (instead of moving on to process more messages?). It is not clear whether the user can control (or whether the system attempts to optimize) placement of workers on different GPUs. More generally, I would like to see better motivation for the approach. What key challenges is Push addressing, and how is it addressing them? What space of designs was considered for the implementation, and what is good about the strategy you chose?"
                },
                "questions": {
                    "value": "1. Is there an intuition you can provide for what problem (if any) the program illustrated in Figs 1 and 2 is solving? (E.g., why has a user decided to write `_gather`?)\n\n2. It seems somewhat unnatural to me that in your implementation of SVGD, the \"leader\" process needs to itself be a particle, rather than just a process that coordinates the other particles. At several points in the code you have to special-case the handling of the leader's parameters vs. everyone else's parameters. Why does Push require each \"worker\" in the concurrent algorithm to itself be a particle that is storing one set of weights for a neural network? Might it be more useful to present Push as a general-purpose library for concurrent programming on GPUs, with an application to BDL?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7029/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7029/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7029/Reviewer_R5Ru"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7029/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784353647,
            "cdate": 1698784353647,
            "tmdate": 1699636824774,
            "mdate": 1699636824774,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7BiRkm8rCQ",
                "forum": "hz9TMobz2q",
                "replyto": "Z4xS6kKQeb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7029/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7029/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your time and thoughtful feedback. We refer you to our general response for additional clarification and are happy to answer any more questions.\n\n> Is there an intuition you can provide for what problem (if any) the program illustrated in Figs 1 and 2 is solving? (E.g., why has a user decided to write _gather?)\n\nWe thank the reviewer for this question. The program illustrated in Figures 1 and 2 would be needed to implement the all-to-all communication pattern between neural networks required by SVGD. We use SVGD as an example of the densest communication pattern between neural networks that could be exhibited by a BDL algorithm. Deep ensembles are an example of a lack of communication between neural networks in a BDL context. We hope to investigate BDL algorithms and their characteristics  with sparser and dynamic communication patterns, i.e., communication dependent on the state of the neural networks, in future work. To more easily and systematically explore this, we were motivated to build a library that could easily express different communication patterns and heterogeneous computations across an ensemble of neural networks. \n\n> It seems somewhat unnatural to me that in your implementation of SVGD, the \"leader\" process needs to itself be a particle, rather than just a process that coordinates the other particles. At several points in the code you have to special-case the handling of the leader's parameters vs. everyone else's parameters. Why does Push require each \"worker\" in the concurrent algorithm to itself be a particle that is storing one set of weights for a neural network? Might it be more useful to present Push as a general-purpose library for concurrent programming on GPUs, with an application to BDL?\n\nWe thank the reviewer for this question. It is possible to design push as a process + particles system so that we do not need to special case the \"leader\" process. The tradeoff in this case is that the process would need to introduce primitives itself for communicating with particles and particles would need special case handling communication from the process. We choose the design where everything is a particle so that communication can be handled uniformly. It might be possible to present Push as a general-purpose library for concurrent programming with applications to BDL, although the multi-threaded execution model operates at the level of particles/neural networks and not tensors. As we mention in the paper, it is possible that Push is useful for expressing other algorithms such as meta-heuristic algorithms, although we leave these investigations for future work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7029/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700113083289,
                "cdate": 1700113083289,
                "tmdate": 1700113249275,
                "mdate": 1700113249275,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OLDkk5BNrs",
                "forum": "hz9TMobz2q",
                "replyto": "7BiRkm8rCQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7029/Reviewer_R5Ru"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7029/Reviewer_R5Ru"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for clarifying these two questions! My assessment of the paper as it stands is unchanged, although I think it could possibly be developed into a valuable future submission about concurrent GPU programming, with BDL as a key use case."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7029/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692341454,
                "cdate": 1700692341454,
                "tmdate": 1700692341454,
                "mdate": 1700692341454,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OQ71VM1GgH",
            "forum": "hz9TMobz2q",
            "replyto": "hz9TMobz2q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7029/Reviewer_G8dD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7029/Reviewer_G8dD"
            ],
            "content": {
                "summary": {
                    "value": "This paper describes a Python library, \"push\", which can be used to orchestrate \"particles\" on a multi-GPU, single-host system, for experimentation in Bayesian deep learning. The paper provides timing results demonstrating that collective primitives such as all-reduce can provide speed benefits to SVGD relative to a baseline which does not replicate all-to-all. Extending to a multi-host distributed regime is left to future work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "*Originality*\n\nThe idea of a framework for orchestrating an ensemble of BNN samples is original. The particular realization here, with a Node Execution Loop and future-based async activity is perhaps novel (but I have questions)--although looked at through another lens, Java has had ThreadPoolExecutors and Futures, which facilitate async execution for quite a long time. Still, this might be new to the ML community, and the application to multi-accelerator, in Python, is much more relevant here than Java.\n\n*Quality*\n\nThe experiments demonstrate reasonably well that push provides a low overhead approach to orchestrating various approaches to optimizing ensembles of NNs. User code is fairly readable (see appendix), though the async nature can make it a bit challenging to reason about what happens when.\n\n*Clarity*\n\nExposition is clear, the work is well presented, the experiments are well explained. The basics of how the system is implemented are clearly explained, in particular noting \"actor based\" and \"async-await\" concurrency.\n\n\n*Significance*\n\nHaving such a library available could be of interest to those in the Bayesian deep learning / BNN community. Arguably, a library that orchestrates asynchronous collective work across multiple accelerators is of interest to the community at large."
                },
                "weaknesses": {
                    "value": "*Originality*\n\nSee questions below, e.g. why not straight torch, why not JAX?\n\nI'm not sure that treating a single realization of a torch.Module weights sample as a \"particle\" and calling that module functor a pushforward is particularly novel. NNs already claim to be \"function approximators\" -- the very fact that torch.Module implements `__call__` suggests this was self-evident to the implementors.\n\n*Quality*\n\nThe work is more a presentation of a new async parallel collectives orchestration system embedded in Python, built around torch.Module, than a presentation of novel results achieved using the system. In the vein of \"experiment quality\", it would be more compelling to me to also see some experiments along the lines of \"here's something we could do now, that we couldn't have done before\". Could we get BNN samples of ViT which are competitive with or superior to those in uncertainty_baselines?\n\n*Clarity*\n\nI thought the work was presented clearly, don't have substantial concerns here.\n\n*Significance*\n\nI have some hesitation about venue. The work feels a bit more like something I'd read from MLSys or JStatSoft than from ICLR. Put another way, I am unsure how many ICLR attendees would be impacted by learning about such a system. On the one hand, I want this system to be of much more general interest (along the lines of \"pytorch async orchestrator\"); on the other hand, I believe systems already exist that provide such solutions, so perhaps specialization to the space of BDL/BNNs is helpful. But then, reading the user code required to implement such BDL solutions atop Push, it's not clear how specialized to Bayes the system really is."
                },
                "questions": {
                    "value": "The most immediate question to my mind is, why is straight torch insufficient to the task? With primitives such as model.to(torch.device(\"cuda:0\")), model.to(torch.device(\"cuda:1\")), etc., we can control the GPU residence of torch models, and data transfers, without an intervening library.\n\nThe next question that comes to mind is, why not jax.pmap? https://github.com/google/jax#spmd-programming-with-pmap\nAll of the BDL proposals tested in this work can be implemented as pure functions of weights, and JAX provides collectives such as psum and all-gather https://jax.readthedocs.io/en/latest/jax.lax.html#parallel-operators which seem to answer most of the useful parallel comms needs of push. Add to those a JIT compiler to manage memory and hide latency effectively.\n\nEven if both of the above suggestions are for some reason deficient, it would be helpful to readers of the paper to elucidate reasons why."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7029/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807237615,
            "cdate": 1698807237615,
            "tmdate": 1699636824647,
            "mdate": 1699636824647,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NSsUOKbukL",
                "forum": "hz9TMobz2q",
                "replyto": "OQ71VM1GgH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7029/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7029/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your time and thoughtful feedback. We refer you to our general response for additional clarification and are happy to answer any more questions.\n\n> The most immediate question to my mind is, why is straight torch insufficient to the task? With primitives such as model.to(torch.device(\"cuda:0\")), model.to(torch.device(\"cuda:1\")), etc., we can control the GPU residence of torch models, and data transfers, without an intervening library.\n\nWe thank the reviewer for this question. The advantage of using the library is primarily speed and convenience. The library automates the underlying threading mechanism that coordinates concurrent particle communication and execution on different GPUs for the user. This is why in Figure 4, the library implementation of Stein Variational Gradient Descent (which has all-to-all communication) is faster than the PyTorch only implementation even on 1 GPU across a range of particles. The library additionally creates views of parameters sets to help the user with memory management in cases like Stein Variational Gradient Descent where multiple neural network parameter sets are considered for the parameter update. The underlying multi-threaded implementation of the library also enables the library to utilize all GPU devices concurrently, whereas a straight PyTorch implementation does not. This can lead to speedups since the library implementation can access the GPU devices in any order and concurrently, whereas a PyTorch only implementation would access the GPU devices in the order specified in the original program and sequentially. While it is certainly possible to utilize multiple GPUs using PyTorch alone, one might need to write additional multi-threaded code or other modules to get similar performance.\n\n> The next question that comes to mind is, why not jax.pmap? https://github.com/google/jax#spmd-programming-with-pmap All of the BDL proposals tested in this work can be implemented as pure functions of weights, and JAX provides collectives such as psum and all-gather https://jax.readthedocs.io/en/latest/jax.lax.html#parallel-operators which seem to answer most of the useful parallel comms needs of push. Add to those a JIT compiler to manage memory and hide latency effectively.\n\nWe thank the reviewer for this question. While this may change in the future, our understanding is that \"Jax has limited support for Python concurrency\" as stated in the documentation https://jax.readthedocs.io/en/latest/concurrency.html. Thus, it would be difficult to implement a concurrent library on top of Jax. It would be an interesting direction of future work to try to implement the library in Jax given its many useful features (such as JIT compilation) and nice engineering. Pmap and all-gather are useful constructs, but they operate at a lower level of abstraction compared to the level that particles are represented at. The concurrent execution semantics of particles in the library can provide the illusion of parallelism to scale across additional GPUs, but it also makes it easier to implement BDL algorithms where the communication patterns involved are not uniform and the computations that each particle perform may depend on the state of the particle. For example, an all-to-all communication pattern as in Stein Variational Gradient Descent is not scalable, and so we may want to explore BDL algorithms that have sparser and dynamic communication patterns instead that would not be easily expressed with pmap or all-gather."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7029/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700112997564,
                "cdate": 1700112997564,
                "tmdate": 1700113274922,
                "mdate": 1700113274922,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cFlp8GEV6c",
                "forum": "hz9TMobz2q",
                "replyto": "NSsUOKbukL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7029/Reviewer_G8dD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7029/Reviewer_G8dD"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledged"
                    },
                    "comment": {
                        "value": "Thanks for the discussion.\nI'm a bit surprised if PyTorch doesn't support async dispatch for multiple GPUs. I agree that would imply programming at a lower level, and thus Push can add value in terms of software design and implementation readability.\n\nI still think the package is positioned in an uncomfortable space between the more general purpose \"async distributed orchestrator for PyTorch\" and the more focused \"parallel BDL toolkit\": I think Push may already fulfill the general purpose tool in all but name/branding/positioning, and it seems to lack affordances that make parallel BDL very much easier or different from implementing such algorithms using a general toolkit.\n\nMy point w.r.t JAX is that GPU primitives are by default dispatched asynchronously, such that the cross thread concurrency may no longer be needed: a single thread can schedule the work of all devices on a single host, because JAX Arrays serve as a Promise/Future. (But note, there may be some complexities w.r.t. cross-device copies, which would need resolving.)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7029/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669192434,
                "cdate": 1700669192434,
                "tmdate": 1700669192434,
                "mdate": 1700669192434,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "z3LDqkF31F",
                "forum": "hz9TMobz2q",
                "replyto": "OQ71VM1GgH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7029/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7029/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your additional comments and feedback.\n\n> I'm a bit surprised if PyTorch doesn't support async dispatch for multiple GPUs.\n\nThis is a subtle point that we were also surprised by. PyTorch does support async dispatch to GPUs in the sense that accessing GPU:0 and then accessing GPU:1 can continue executing on the CPU before GPU:0 and GPU:1 finish computing. However, if you do this from the same process, then PyTorch ensures that the request to access GPU:0 completes before the request to access GPU:1 which requires synchronizing the \"multi-GPU\" accesses. This is a performance cost which our system solves, since we remove the constraint that accessing GPU:0 must happen before GPU:1.\n\n> I still think the package is positioned in an uncomfortable space between the more general purpose \"async distributed orchestrator for PyTorch\" and the more focused \"parallel BDL toolkit\": I think Push may already fulfill the general purpose tool in all but name/branding/positioning, and it seems to lack affordances that make parallel BDL very much easier or different from implementing such algorithms using a general toolkit.\n\nWe thank you for this feedback. We agree that there are more general use cases for the tool and mentioned some in the paper. We plan to explore these in future work, but have not tested it due to the constraints of a single paper.\n\n> My point w.r.t JAX is that GPU primitives are by default dispatched asynchronously, such that the cross thread concurrency may no longer be needed: a single thread can schedule the work of all devices on a single host, because JAX Arrays serve as a Promise/Future. (But note, there may be some complexities w.r.t. cross-device copies, which would need resolving.)\n\nThank you for this explanation. This would be interesting to take a look into. However, as we explained above in PyTorch, if that single thread that JAX uses to schedule multi-GPU accesses guarantees that the Promise/Future obtained from GPU:0 and GPU:1 on a single thread must be produced in the order they are accessed, then there again would need to be synchronization across multi-GPU accesses."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7029/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672929650,
                "cdate": 1700672929650,
                "tmdate": 1700672949337,
                "mdate": 1700672949337,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "suuVZD9uSJ",
            "forum": "hz9TMobz2q",
            "replyto": "hz9TMobz2q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7029/Reviewer_omDv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7029/Reviewer_omDv"
            ],
            "content": {
                "summary": {
                    "value": "This paper describes a new probabilistic programming library called Push, for composing together\nBayesian neural networks. Through an actor-inspired concurrency model where neural networks\naccept and send a collection of particles. This allows the system to readily encode scatter-gather\npatterns that are very amendable to working on GPUs as well as scaling linearly with multiple\nGPUs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This is a very unique and original approach for representing Bayesian neural networks. The design\nis well-thought out and the experiments are equally thoughtful. The paper is clearly written and\nI can easily see others using and extending this work."
                },
                "weaknesses": {
                    "value": "The paper says this architecture can support many BDL algorithms but only SWAG and SVGD are presented.\nWhile I think it's too much to ask experiments to be done on more algorithms, it would be nice if\nat a high-level it can be shown how many of the most popular BDL algorithms would be supported by\nthe Push library."
                },
                "questions": {
                    "value": "What BDL algorithms could be represented in this library?\nWhat BDL algorithms would be challenging to use with this library?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7029/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699515566833,
            "cdate": 1699515566833,
            "tmdate": 1699636824517,
            "mdate": 1699636824517,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mKkaFi14OY",
                "forum": "hz9TMobz2q",
                "replyto": "suuVZD9uSJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7029/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7029/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your time and thoughtful feedback. We refer you to our general response for additional clarification and are happy to answer any more questions.\n\n> What BDL algorithms could be represented in this library? What BDL algorithms would be challenging to use with this library?\n\nWe thank the reviewer for this question. It is possible to implement Monte-Carlo drop-out in this library in the standard way. It is possible to implement a Markov-Chain Monte Carlo algorithm in this library by using a particle to represent a proposal and keeping the particle if the proposal is accepted or reusing the particle and performing new proposals if the previous proposal is rejected. These proposals can use gradients. It is also possible to implement a variety of variational approximations such as Stein Variational Gradient Descent. It is also possible to implement message-passing style algorithms in the library by defining the appropriate send functions to update other particles parameters with knowledge of the calling particle's parameters/state. We hope to explore all of these avenues in future work. Algorithms such as backprop-by-bayes are not applicable in our representation since we do not directly define a distribution on parameters. We also hope that the representation of a communicating ensemble of particles may inspire the study of new BDL algorithms that have more complex communication patterns and use more particles."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7029/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700112922119,
                "cdate": 1700112922119,
                "tmdate": 1700113333230,
                "mdate": 1700113333230,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "60Hqgo8MfI",
                "forum": "hz9TMobz2q",
                "replyto": "mKkaFi14OY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7029/Reviewer_omDv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7029/Reviewer_omDv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks! Would it be possible if some of this answer made it into the paper?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7029/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681053626,
                "cdate": 1700681053626,
                "tmdate": 1700681053626,
                "mdate": 1700681053626,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]