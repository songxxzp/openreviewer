[
    {
        "title": "NaturalSigner: Diffusion Models are Natural Sign Language Generator"
    },
    {
        "review": {
            "id": "ChyDvXZb6P",
            "forum": "4JjSJyT15z",
            "replyto": "4JjSJyT15z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5020/Reviewer_s9QL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5020/Reviewer_s9QL"
            ],
            "content": {
                "summary": {
                    "value": "The focus of this paper is on development for improved models for signal language generation. They propose a classifier-free diffusion model that goes from gloss or text to animation and produces parameters derived from SMPL-X. Results on Phoenix-2014 and Phoenix-2014T are significantly better than competing methods according to translations metrics (Rough, BLEU-4). Qualitative results show that animations from this model are preferred over one competing model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The motivation behind using diffusion models is sound (e.g., overcoming the one-to-many problem)\n* The use of SMPL-X seems like a nice improvement over more common key point-based approaches. \n* The experiment ablations are nice and I appreciate the authors for breaking apart the prompting mechanism, semantic encoder, and deef forward denier."
                },
                "weaknesses": {
                    "value": "* Related work is missing many references in the NLP, HCI, and Accessibility communities. There has been a lot of interest in SL modeling over the past 1-2 years but many of the references refer primarily to a line of work by Saunders et al. ending in 2021.\n* The descriptions of the model/system formulation could benefit from more depth. Specifically, the introduction of the duration model isn't entirely clear. My hypothesis is that this is being used in the same way as duration models in TTS speech systems, but I'm not entirely sure. As an aside, given connections to TTS systems, it may be worth digging into that more in the related work section. \n* I didn't quite understand the motivation behind the prompt encoder. Maybe I missed it, but it might be worth clarifying why this is used. Are the results in Table5 and 6 statistically significant? The results with prompt encoder and without are very similar and in one case (Table 5 test) better without this encoder. \n* Is FID a good metric for this problem? Is there a demonstrable correlation between improved FID and improved signing quality?\n* Subjective evaluation shows that this is better than Saunders, but doesn't give an overall sense of quality. Without videos it's unclear how well the approaches work (subjectively)."
                },
                "questions": {
                    "value": "I am on the fence about this paper. Can the authors provide video references for the animations? And answer some of the questions in the 'weakness' section?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5020/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698680633205,
            "cdate": 1698680633205,
            "tmdate": 1699636490644,
            "mdate": 1699636490644,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2Ma5nOqWvf",
                "forum": "4JjSJyT15z",
                "replyto": "ChyDvXZb6P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5020/Reviewer_s9QL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5020/Reviewer_s9QL"
                ],
                "content": {
                    "title": {
                        "value": "Post rebuttal"
                    },
                    "comment": {
                        "value": "I have glanced through other reviews and I have looked at the rebuttal and a handful of the animation videos. I remain borderline. I appreciate how much extra effort the authors have put into this work and think it would be a reasonable paper to accept. I do not feel strongly enough to champion it against dissent from other reviews.\n\nSome aspect, such as the lack of facial animation, continue to be a concern. I also appreciate that the author ran a user study but think the ultimate results are limited given the relative scores between two models (as opposed to overall global assessment of whether or not the models were useful)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706795681,
                "cdate": 1700706795681,
                "tmdate": 1700706795681,
                "mdate": 1700706795681,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W7QX1fVVtH",
            "forum": "4JjSJyT15z",
            "replyto": "4JjSJyT15z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5020/Reviewer_9cnq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5020/Reviewer_9cnq"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a diffusion model based SLG approach. The presented model uses SMPL-X body parameters as sign representations, and is able to generate realistic signer poses when being prompted with sign gloss and spoken language text. The authors conduct extensive experiments on the Phoenix datasets and report significantly better back-translation and FID scores compared to the state-of-the-art."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Although diffusion models have been used for SLG before (https://arxiv.org/pdf/2308.16082v1.pdf) for photo realistic avatar generation, this paper is the first application of diffusion models to generate sequence of signer poses given sign glosses and spoken language text, to the best of my knowledge.\n- The authors are sharing their source code. \n- The proposed approach achieves significantly better back-translation and FID scores on the Phoenix2014 datasets compared to the state-of-the-art.\n- The authors conduct a user study with 10 participants to qualitatively assess their approach. I should stress the value of this, as the CV community does not commonly conduct user studies.\n- Ablation study was informative."
                },
                "weaknesses": {
                    "value": "- Not including facial expressions was disappointing. As the authors would appreciate, to fully convey and understand meaning of sign language utterances one must consider facial expressions, mouthings, gaze and mouth gestures. \n- Given the authors use a parametric body model as their sign representation, it was quite surprising to see that they've chosen skeletons as their visualization instead of a canonical body being driven by the generated pose configurations. \n- Although popular, Phoenix datasets are quite limited in terms of domain and signing variance. I'd have greatly strengthened the paper if the authors conducted studies in other datasets, like OpenASL or CSL, to set baselines for future research."
                },
                "questions": {
                    "value": "Q: User Study: Were the participants deaf and proficient in DGS? I am asking since the authors mention that the participants evaluate the approaches based on the generated sequence being \"easier to understand and closer to the ground truth\u201d. Please clarify this in the manuscript. \n\nSuggestions and Minor Fixes: \n- Please use the term \"Deaf and Hard of Hearing\" instead of \"Hearing-impaired\"\n- Use \u201c \u201c instead `` \u2018\u2019 in the latex to fix the quotation mark issues. \n- Page 2 naturalSigner -> NaturalSigner"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5020/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698707826488,
            "cdate": 1698707826488,
            "tmdate": 1699636490556,
            "mdate": 1699636490556,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "HtXkRSUQC0",
            "forum": "4JjSJyT15z",
            "replyto": "4JjSJyT15z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5020/Reviewer_UZBd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5020/Reviewer_UZBd"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to use a diffusion model to generate sign language keypoints. More specifically, the authors first design a sign language prompting mechanism considering the signer identity inforation. Then a mixed semantics encoder considering both text, gloss, and prompt, and a duration predictor are proposed as a prior model. Finally, a diffusion process is achieved as usual. The overall method achieves new SOTA performance on a series of metrics."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Use SMPLX parameters to represent motion is reasonable.\n2. Sign languge prompting considers signer identity information.\n3. SOTA performance on Phoenix-2014T on multiple metrics.\n4. A user study is conducted."
                },
                "weaknesses": {
                    "value": "1. The completeness of the paper is low.\n\na) I don't think generated keypoints are understandable to the deaf. Facial expression and mouth movement are important to sign language understanding but they are not included as shown in the demo. The generated keypoints may be better than baselines, but they are still far from being understood by the deaf people.\n\nb) A classic prior work, FS-Net [1], has already achieved **video** generation using the keypoints to drive a signer image. But the proposed method doesn't support video generation.\n\nc) Similarly, a recent open-sourced work, SignDiff [2], also applies the diffusion model on sign language generation, while it also provides video results.\n\n2. The novelty is limited. The paper is more like an application of diffusion model on sign language generation, while the applicability has already been verified in SignDiff [2]. Although some adaptations are proposed, some of them, e.g., duration predictor, already appear in existing sign language papers [3,4].\n\n3. It is good to involve signer identity information in the sign langauge prompting, but there are not corresponding experiments to verify its effectiveness. I don't think current generated results can reflect signer identity information.\n\n4. Need more details for the user study. Since the task is for the deaf people, how are they fluent with sign language? Are they Germany sign language users? How many videos are given to the users? Are the text and gloss annotations given to them?\n \n5. An important benchmark, CSL-Daily, is missing. Phoenix-2014 and Phoenix-2014T are quite similar, and thus the conclusions on these two benchmarks are always consistent. Other benchmarks from a different language, e.g., CSL-Daily, is necessary.\n\n[1] Signing at Scale: Learning to Co-Articulate Signs for Large-Scale Photo-Realistic Sign Language Production, CVPR 2022\n\n[2] SignDiff: Learning Diffusion Models for American Sign Language Production, arXiv 2023\n\n[3] SimulSLT: End-to-End Simultaneous Sign Language Translation, MM 2021\n\n[4] Towards Fast and High-Quality Sign Language Production, MM 2022"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5020/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769037926,
            "cdate": 1698769037926,
            "tmdate": 1699636490475,
            "mdate": 1699636490475,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "7SsEz93Pu5",
            "forum": "4JjSJyT15z",
            "replyto": "4JjSJyT15z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5020/Reviewer_BJCa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5020/Reviewer_BJCa"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the topic of sign language generation. It proposes a NaturalSigner framework to leverage the strong modeling capability of diffusion model. The experiments on two datasets demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper studies the topic of sign language generation. It proposes a NaturalSigner framework to leverage the strong modeling capability of the diffusion model. \n\nThe experiments on two datasets demonstrate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "One of the main concerns is that the presented qualitative results are not satisfying. For the keyframe in Figure 4/video on the demo webpage, I do not see a clear improvement over the previous method.\n\nThe title is confusing. What does the word natural mean? \n\nThe novelty is somewhat limited. The authors should clearly state why diffusion better works for SLG.\n\nI do not think section 3.2 should be called in-context learning. It is just an embedding technique.\n\nWhat is the specific setting of zero-shot SLG?"
                },
                "questions": {
                    "value": "The questions are listed above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5020/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699207101875,
            "cdate": 1699207101875,
            "tmdate": 1699636490377,
            "mdate": 1699636490377,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]