[
    {
        "title": "EntProp: High Entropy Propagation via Auxiliary Batch Normalization Layers"
    },
    {
        "review": {
            "id": "ku9RbH2Sup",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2712/Reviewer_2wsR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2712/Reviewer_2wsR"
            ],
            "forum": "GzG4BqztV8",
            "replyto": "GzG4BqztV8",
            "content": {
                "summary": {
                    "value": "The paper introduces a new approach EntProp for enabling deep neural networks to generalize to out-of-distribution domains. The key contribution of this work is the proposal of selecting clean, high-entropy samples to the auxiliary batch normalization layers (ABNs) to improve the standard accuracy and robust accuracy. This work presents two techniques, data augmentation, and free adversarial training, that increase entropy and bring the sample further away from the in-distribution domain. EntProp effectively disentangles clean and transformed sample domains, leading to higher standard accuracy and robustness with a lower training cost than existing methods. The paper provides detailed experimental results on CIFAR-100 and CUB-200-2011 datasets, showcasing the method's efficiency and ability to prevent overfitting compared to existing techniques."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) The paper introduces a unique approach, EntProp, challenging the conventional separation of clean and transformed samples as distinct domains. It offers an innovative perspective on domain separation and out-of-distribution robustness.\n\n(2) The paper is well-structured, with clear explanations of the proposed method and the underlying hypotheses, especially in explaining the pseudo-code and details.\n\n(3)The experiments are comprehensive for evaluating the proposed EntProp approach, including ablation studies, sample selection bias analysis, and comparison to existing methods.\n\n(4) This work achieves both high standard accuracy and robustness against out-of-distribution data, which is a critical problem in practical machine learning applications. EntProp's ability to enhance both while avoiding overfitting to adversarial attacks holds practical value."
                },
                "weaknesses": {
                    "value": "(1) The paper lacks a theoretical foundation to support its claims. Providing theoretical insights into why increasing entropy improves disentangled learning and robustness would greatly enhance the paper's clarity.\n\n(2) This work lacks scalability analysis for very large datasets or complex models. Investigating the method's performance on more extensive tasks would demonstrate its practicality in real-world scenarios.\n\n(3) While the paper compares its method with other baselines, it could benefit from a more extensive comparison with state-of-the-art methods[1, 2, 3] in disentangled learning, out-of-distribution robustness, and adversarial training to demonstrate its superiority convincingly.\n\n(4) A more comprehensive analysis of robustness encompassing diverse distribution shifts and attack strategies would strengthen the paper's contributions. Furthermore, the evaluation based on just two datasets (CIFAR-100 and CUB-200-2011) may not be as persuasive compared to other research in the same domain. Expanding the evaluation to include a more diverse set of datasets would enhance the paper's credibility.\n\n(5) The paper lacks an in-depth discussion of scenarios where the proposed method may fail or have limitations. Acknowledging these limitations and providing insights into potential remedies would enhance the paper's credibility.\n\n[1]  Domain Generalization with Mixstyle, ICLR 2021\n\n[2] Out-of-Distribution Detection with Deep Nearest Neighbors, ICML 2022\n\n[3] Adversarial Distributional Training for Robust Deep Learning, NeurIPS 2020"
                },
                "questions": {
                    "value": "(1) The paper's evaluation is limited to two datasets, CIFAR-100 and CUB-200-2011. It would be valuable to provide a justification for the selection of these datasets. How well do they represent the diversity of real-world scenarios, and can the findings be extended to other datasets and domains?\n\n(2) While the paper highlights improvements in adversarial robustness, it would be beneficial to include a more extensive analysis covering various types of adversarial attacks to provide a comprehensive assessment of the method's effectiveness.\n\n(3) It would be insightful if the authors could compare their proposed method with other state-of-the-art techniques for adversarial training and robustness enhancement. Such comparisons would offer a more comprehensive understanding of the method's relative performance.\n\n(4) The paper primarily focuses on image classification tasks. Is there potential for the proposed approach to generalize to a broader spectrum of machine learning tasks, such as object detection or natural language processing? Exploring its applicability in diverse domains would be valuable."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2712/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697344554414,
            "cdate": 1697344554414,
            "tmdate": 1699636213319,
            "mdate": 1699636213319,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "NFkrYGpomg",
            "forum": "GzG4BqztV8",
            "replyto": "GzG4BqztV8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2712/Reviewer_hjam"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2712/Reviewer_hjam"
            ],
            "content": {
                "summary": {
                    "value": "This paper falls in the field of robustness. More specifically, it argues that some samples after applying some transformation will still be \"in-distribution'' based on the sample's entropy. The authors introduce two techniques to increase the entropy and push the sample away from the \"in-distribution\" domain. Experiments are conducted to show that the proposed techniques can help."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. This paper has a good motivation to improve the robustness of trained models. \n2. The paper is generally well-written."
                },
                "weaknesses": {
                    "value": "1. Correct me if I am wrong, but in my opinion the researching question seems to be constantly true. It is almost certainly that some transformations result in in-distribution samples, for example translation and rotation. It would make more sense if there are more constraints on types of transformation. \n2. I believe the contribution bullet no. 3 shall be merged with bullet no. 2 since they are all describing similar outcomes, i.e., EntProp improves the standard and robustness accuracy. In such a case I think the contribution of this manuscript is not sufficient. \n3. I would suggest a better structure of Related Works. For example, it would be better to separate architectures (like multiple BNs) with adversarial training methods (augmentation, adv attacks etc.). \n4. I have concern about the novelty of this manuscript: it seems that all the techniques are just taken from existing works. MixUp is well-known and the Fast Adversarial Training is from Shafahi et al. (2019)."
                },
                "questions": {
                    "value": "1. Looking at Table 3, it seems that the performance generally goes up when $k$ increases. What is the performance when $k$ equals $0.8$ or $1$? Can they achieve better performance? If yes, then the subsampling process could be not effective at all. \n2. In Table 2, what is the performance without sample selection (i.e., the simpest baseline? )\n3. In Table 4, why not conduct experiments with k=0.6, which has the overall highest performance in Table 3? Is there any specific reason for this choice? \n4. How to pick $k$ in principle?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2712/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698704779216,
            "cdate": 1698704779216,
            "tmdate": 1699636213250,
            "mdate": 1699636213250,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "LhI3AzW2vK",
            "forum": "GzG4BqztV8",
            "replyto": "GzG4BqztV8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2712/Reviewer_CJYg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2712/Reviewer_CJYg"
            ],
            "content": {
                "summary": {
                    "value": "This article presents a learning framework that aims at improving out-of-distribution performance. Based on AdvProp and MixProp, it proposes to use Main or Auxiliary batch normalization layers (MBNs/ABNs) that learn on separated distributions of data. Based on data augmented on MixUp, the method considers high entropy samples that are also augmented with adversarial examples to be out-of-distribution and trains them on the ABNs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The article is well-written and easy to follow.\n\nThe experiments and ablation studies allow a good understanding of the effects of the method and to compare it to the others (with some caveats). In particular, Table 3 showcases well that for a given compute budget, EntProp improves the robustness of the network in most cases.\n\nEntropy seems like a natural metric to distinguish the samples and is well correlated with both augmentations."
                },
                "weaknesses": {
                    "value": "**Results** Table 7 shows that the standard accuracy of the network decreases significantly with EntProp, thus making less obvious the choice of EntProp over the other techniques, in particular MixProp. There is only two different values of $k,n$ for Table 4 compared to Table 3, making it difficult to see their effect. In Table 3, it seems that increasing $k$ increases the accuracy. However, the Table stops at $k=0.6$, while it seems important to increase the value up to $1$ to see where the improvement stops.\n\n**Mixup** Mixup is defined as \"the most typical data augmentation\" which is surprising. Seeing as all experiments on done on images, there are way more common data augmentation techniques, such as random crop and random flip. Note that these augmentations are done in MixProp. Are those more common data augmentations used in the experiments here? \n\nThe citation for Mixup is also very surprising, seeing as it was introduced by Zhang et al. 2018 and not Pang et al. 2019. Furthermore, in neither Zhang et al. 2018 or Pang et al. 2019 is the mixup loss defined like in this article. The labels in mixup are also mixed and a single loss is applied, which is different from the definition used in this paper where two losses on both clean labels are applied and then mixed. This seems to however be also the case in MixProp, which also does not explain where this new MixUp loss comes from (which seems to appear in several other articles that are not cited in either MixProp or EntProp).\n\n**Novelty** This article presents very limited novelty. This is a combination of AdvProp and MixProp, but using the mixed-up examples also in the MBN, and only a percentage of examples in the ABN.\n\n**Metric** The choice of the Harmonic Mean between SA and RA seems a bit arbitrary since it does not seem to be used in other articles.\n\n**Adversarial robustness** MixUp decreases strongly adversarial robustness, however, this is noted by the authors as being different from their wanted metric of out-of-distribution robustness.\n\n**Entropy** Entropy is never defined precisely in the article, how is it measured in practice?\n\n**Other metrics** Why not choose the Confidence metric rather than Entropy? It gives consistently better results than Entropy (except for ResNeXt-50). I am also surprised that the adversarial robustness for the Random metric is given in Table 5, but not the accuracy, which seems significant to see the improvement due to the metrics. \n\n\n**Hyperparameter** How are k and n chosen in practice? By cross-validation? This is never very clear in the article."
                },
                "questions": {
                    "value": "* Why do feeding random samples (Fig 3) to the ABN increase the accuracy of the network? Since the samples are random, it would seem to me that the ABN would learn the same statistics from the MBN, resulting in similar accuracy. Or I may be mistaken and adversarial examples are being used in what I thought was an ablation experiment because $n$ is not specified.\n\n* The variance of the metrics seems to increase a lot with Mixup in Table 2. This is not elaborated upon as variance is not given in further Tables.\n\n* As $k$ increases, the number of examples in the loss increases, artificially increasing the batch size per iteration. Why is there no normalization taking into account this increase of batch size? (Or other changes)\n\n* Why do the authors think that \"AdvProp consistently shows significantly lower results than vanilla training\" on the CUB-200-2011 dataset, but consistently the best or second best results on CIFAR-100 ? This seems like a surprising decrease.\n\n* Figure 1: In this case, are transformed samples equal to mixup samples for MixUp and adversarial examples for AdvProp? This is not clear in the text.\n\n* Entropy seems to decrease in training. Why do the authors think this happens?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2712/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757761301,
            "cdate": 1698757761301,
            "tmdate": 1699636213181,
            "mdate": 1699636213181,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "8zXAvEp89y",
            "forum": "GzG4BqztV8",
            "replyto": "GzG4BqztV8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2712/Reviewer_A6wK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2712/Reviewer_A6wK"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a novel approach called High Entropy Propagation (EntProp) that builds on disentangled learning with mixture distribution via Auxiliary Batch Normalization layers (ABNs). The key insight is that some transformed samples are not completely different from clean samples. EntProp identifies clean high-entropy samples and applies transformations to push them further into the out-of-distribution domain."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. identifying and transforming high-entropy samples to generate out-of-distribution samples is an interesting approach, and also an intuitively efficient method. \n\n2. The idea that EntProp can prevent overfitting on small datasets is quite interesting, this is often an under-studied topic. \n\n3. The discussion on computational efficiency is helpful for one to better appreciate the method."
                },
                "weaknesses": {
                    "value": "1. the emprical scope of this paper is unfortunately too limited. At this moment, it is probably too far for being considered at this venue. For example, the authors' writing suggests that the authors are contributing to the line of research in robustness (maybe particular with data augmentation), then there are tons of methods that are along the robustness research, and many of them are using data augmentations. There are many other OOD benchmarks, like ImageNet variants, many corruptions datasets, like -C datasets, and many other adversarial attacks. The empirical coverage of this paper is too limited that it unfortunately does not touch any of the above directions sufficiently. \n\n2. The main idea of this paper is quite similar to [1], in the sense of first identifying the samples with high entropy (high loss) and then perform worst-case data augmentation to those samples (adversarial attack or dropout-style augmentation). It could be interesting if the authors offer more in-depth discussion of the relationship. \n\n[1]. The two dimensions of worst-case training and their integrated effect for out-of-domain generalization"
                },
                "questions": {
                    "value": "1. It is unclear to me why the authors choose to work on these two datasets instead of more standard ones. \n\n2. When the authors make claims like this \"In this paper we focus on improving both standard accuracy and robustness against out-ofdistribution domains.\" one will typically expect much stronger empirical results. It seems the authors might want to significantly re-write the scope of this paper. Maybe it's only about methods that are explicitly dealing with augmentation, OOD, and BN."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2712/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802693066,
            "cdate": 1698802693066,
            "tmdate": 1699636213102,
            "mdate": 1699636213102,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]