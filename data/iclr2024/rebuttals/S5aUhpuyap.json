[
    {
        "title": "Complex priors and flexible inference in recurrent circuits with dendritic nonlinearities"
    },
    {
        "review": {
            "id": "xOq0C8UA4E",
            "forum": "S5aUhpuyap",
            "replyto": "S5aUhpuyap",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2900/Reviewer_A1Uf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2900/Reviewer_A1Uf"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel recurrent circuit model based on diffusion models that incorporates several biologically plausible properties. This method implicitly encodes priors over latent variables and can combine this information with other sources, such as sensory input, to encode task-related posteriors. The approach is mapped to a recurrent network with multi-compartment neuron models, and its effectiveness is demonstrated through experiments on toy datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper successfully maps diffusion model-inspired dynamics to a recurrent circuit with multi-compartment neuron models,\n\n* The modifications to diffusion model dynamics to align with biologically plausible properties are commendable, especially as they do not harm the network quality,\n\n* The provided code is clear and enhances the paper's reproducibility."
                },
                "weaknesses": {
                    "value": "I think the paper has several weaknesses. Please see the following list and the questions sections.\n\n* Regarding the following sentence in page 5: \"First, we tested ... autocorrelation ... remains steadily around zero proving that ... samples are essentially independent.\" I might agree that the samples are independent. However, isn't it mathematically misleading to use the word \"prove\" based on correlation?\n\n* Regarding the sentence in page 5: \"Overall, these results indicate that the constraints imposed by biology may have a minimal effect on the quality ... compared to DMs\". Isn't it too early to state such conclusion. The experiment is on a quite toy-dataset.\n\n* The experimental details for the stochastic neural network in Appendix B.2 are lacking, and providing more comprehensive information in this section would strengthen the paper.\n\n* Overall, the experiments are on quite toy datasets.\n\n\n**Minor Comments**\n\n*  In the second line of Section 2, \"nosy\" should be corrected to \"noisy.\"\n\n* Figure 1 lacks a caption for (F).\n\n*  In Figure 4 caption (for (B)), it is written \"(left) ... (right)\" to point out to the corresponding plots, but I guess the caption should use \"(top)\" and \"(bottom)\" to indicate the corresponding plots.\n\n* The first line of Section B.3: \"Fig. ??\" which figure is that?"
                },
                "questions": {
                    "value": "* In Figure 1D, the caption mentions \"multi-compartment neuron.\" Shouldn't it be \"multi-compartment neurons\"? I thought each gray triangle is a multi-compartment neuron.\n\n* The paper repeatedly mentions \"optimized ReLU nonlinearities.\" I did not really understand the meaning of \"optimized\" in them?\n\n* Is $\\sigma_\\lambda$ in Equation 1 the same with $\\beta_\\lambda$ in Figure 1?\n\n* Regarding the following sentence in page 6: \" In principle, this might lead to catastrophic accumulation of errors and large sampling biases; however, the attractor dynamics prevent this from happening.\" Can authors elaborate on this a bit more? It is not clear to me why?\n\n* In page 6, it is written \"$\\mathbf{g}\\_t (\\mathbf{x}\\_{t-1}, s) = \\frac{1}{\\sigma_s^2} \\mathbf{M}_s \\mathbf{M}\\_s^T (\\mathbf{x} - \\mathbf{x}\\_c)$\". Is it $\\mathbf{g}\\_t (\\mathbf{x}\\_{t-1}, s) = \\frac{1}{\\sigma\\_s^2} \\mathbf{M}\\_s \\mathbf{M}\\_s^T (\\mathbf{x}\\_{t-1} - \\mathbf{x}\\_c)$?\n\n* Does the last sentence of Figure 3 caption correspond to Figure 3D? There is no caption for (D)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2900/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2900/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2900/Reviewer_A1Uf"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2900/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698515728849,
            "cdate": 1698515728849,
            "tmdate": 1699636233600,
            "mdate": 1699636233600,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IE3ZTJGWjH",
                "forum": "S5aUhpuyap",
                "replyto": "xOq0C8UA4E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2900/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2900/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply reviewer A1Uf"
                    },
                    "comment": {
                        "value": "Thank you for the positive overall evaluation of the work. \nWe agree that numerically validating that the autocorrelation of the RNN-generated samples decaying to zero at small lags does not mathematically \u2018prove\u2019 that this will be the case in all conditions; the same goes when demonstrating that the additional biological constraints do not harm sampling performance. We can still say that \u2013empirically\u2013  it is the case across the distributions that we have tried (including new results on MNIST), both for the prior and the posterior samples.\n\n\nRegarding the role of attractor dynamics in correcting for the approximations, one way to think about the approximations introduced in the service of biological plausibility is as inducing small noisy deviations from what would have been the ideal trajectory, defined by the solution without the approximations. Since in our model the representation of distributions in the embedding manifolds is driven by the attractor flow fields, adding moment by moment stochasticity on top of such dynamics can be partially corrected for by the push towards the manifold. The dynamics will not end up exactly at the same location on the manifold compared to the exact version, but they will still sample from the right distribution.\nAbout the use of toy datasets: see common reply. Briefly, we now numerically demonstrate the scalability of our approach using MNIST.\n\n\nWe have corrected the minor typos and so on (see updated pdf). \nQuestions: \n- Multi-compartment neuron should be plural (typo). \n- We used the phrase \u201coptimized ReLU nonlinearities\u201d to mean that the dendritic nonlinearities have been optimized on the denoising objective defined by Eq. (4). These nonlinearities are functionally instantiated by linear weight matrices and ReLU activations. We have modified the main text for clarity.\n- It is indeed the case that $\\sigma_\\lambda$ in Eq 1 is the same as $\\beta_\\lambda$ in Fig. 1. To keep our notation consistent, we changed Fig 1 to use $\\sigma_\\lambda$ instead.\n- This sentence relates to our approximation of the likelihood, which consists of evaluating $g_t$ at $x_{t-1}$ rather than at $\\mu_\\theta(x_{t-1})$. There is a substantial penalty associated with this approximation, since $x_{t-1}$ is the noisy sample at time $t-1$, while $\\mu_\\theta(x_{t-1})$ is the network\u2019s (denoised) estimate of the clean sample at time $t$, which lies closer to the manifold of clean images. Since this approximation error is incurred at every timestep, it is possible for these errors to accumulate over time and lead to large sampling biases in the resulting posterior distribution. Instead, we find in our numerical simulations that this is not the case; rather, the discrepancy (measured using the KL divergence) tends to decrease over the course of the inference process.\n- Yes, this is correct. We have fixed the typo to include the time step in $\\mathbf{g}$. \n- The last sentence of the caption of Figure 3 does indeed correspond to Figure 3D. We have fixed the caption."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2900/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713417005,
                "cdate": 1700713417005,
                "tmdate": 1700713541101,
                "mdate": 1700713541101,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BbYHa4lp9U",
            "forum": "S5aUhpuyap",
            "replyto": "S5aUhpuyap",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2900/Reviewer_7hMe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2900/Reviewer_7hMe"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the state-of-the-art diffusive model developed in deep learning could be a running algorithm in recurrent neural circuit. Specifically, it claims the nonlinear dendrites of neurons with globally controlled dendritic noises can be used to implement the reverse phase of diffusive models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The concept of the present study is original and can significantly advance our understanding of the stochastic recurrent neural circuit once some of my major concerns are solved (see weaknesses). The structure of the manuscript is organized well, and the introduction and discussion are clearly written with a thorough review of the research history as well as possible experimental verification of the theory."
                },
                "weaknesses": {
                    "value": "I have major concerns about the justification and derivation of two central claims (dendritic nonlinear and global oscillating signal) about neural circuit implementation of diffusive models. A possibility is that the text doesn't explain the key math steps sufficiently well. I look forward to seeing some justification in the rebuttal.\n\n### Dendritic nonlinearity\nThe paper directly states the nonlinear dendritic operation $f(x_t, \\beta_t)$ after Eq. 5 without explaining how it is derived. I am confused about how this nonlinearity comes out. Is it directly from $\\mu_\\theta (x_{\\lambda + t}, \\lambda)$ in Eq. 2? In this case, does it imply the dendritic nonlinearity needs to be readjusted if the transition operator in the reverse process is changing? Or it is just used as a way to capture the nonlinearity in biological neurons? \n\n### Global oscillating signal\nI don't understand how the global oscillating signal is derived. Although the author explained the $\\beta_t$ is analogous to the sequence of noise variance in the diffusive model, it seems that the diffusive model doesn't have such an oscillating signal if I understood correctly. I have no idea how Eq. 5 was derived, what assumptions it relies on, and why the $\\beta_t$ becomes a sinusoid function there.\n\n### Training an ANN-based model for dendrites\nThe author says the dendrite is modeled as an ANN whose parameters were trained via gradient descent. Does this imply some mechanism to adjust the dendritic parameters in real neurons? If so, what are the possible biological mechanisms? I don't see related discussions in the paper.\n\nOverall, I suggest the author explain the key motivations, and assumptions in deriving the nonlinear dendrite, global oscillating signals in modulating dendritic noises around Eq. 5. They seem quite critical in this work. In addition, it is better to write a concrete, commonly used recurrent neural dynamics in a single line, and compare it directly with the reverse process in diffusive models. If my major concerns are solved, I'd like to increase my rating."
                },
                "questions": {
                    "value": "- Fig. 1E caption: why does neural dynamics push off the network off manifold? Does the author mean the neural dynamics of sensory transmission correspond to injecting noise in a similar fashion with the forward process in a diffusive model? At least this neural dynamics is not the same recurrent neural dynamics which the author claims to implement a reverse process to sample from posteriors.\n\n- Fig. 4B caption: it should be (top) and (bottom) because no (left) and (right) here.\n\n- It seems that the iterative steps in diffusive model are indexed by $\\lambda$ while that in the neural dynamics was indexed by time $t$. What's the relation between $\\lambda$ and $t$? My understanding is that $\\lambda$ is a non-negative number while $t$ can go to infinity. Does it imply the equilibrium neural dynamics repeatedly sample distribution of $x_0$ over time?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2900/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698692023458,
            "cdate": 1698692023458,
            "tmdate": 1699636233488,
            "mdate": 1699636233488,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gbYl5mk54l",
                "forum": "S5aUhpuyap",
                "replyto": "BbYHa4lp9U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2900/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2900/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply reviewer 7hMe"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing the originality of our work and we apologize for the lack of clarity in the description of the model.\n\nDendritic nonlinearities: The collection of single neuron nonlinearities (which have a shared parametric form, but with neuron-specific parameters), are the direct result of the denoising optimization procedure, and their operations map one to one onto  the reverse transition operator $\\mu_\\theta(x_{\\lambda+1}, \\lambda)$ in  Eq. 2.  This indeed implies that the dendritic nonlinearity parameters would change when modeling a different prior distribution (biologically achievable through a combination of synaptic and branch plasticity, although we do not model the mechanics of learning here). \n\nGlobal oscillations: there is a one-to-one map between the oscillation phase in our model  and the noise schedule in the traditional diffusion models at sampling time. In more detail: in diffusion models, the degree of noise at step $\\lambda$ is given by a noise schedule $\\sigma_\\lambda$ that decreases monotonically over the course of inference as a noisy image $x_\\lambda$ is iteratively denoised. In other words, images with the largest degree of noise are found at $\\lambda=\\Lambda$ and clean images are produced at $\\lambda=0$. While the exact form of the schedule can matter in the details, there is a range of possible choices available. In the interest of biology we used a sinusoidal interpolation between  $\\lambda=\\Lambda$  and  $\\lambda=0$. This allows us to establish a direct map between  $\\Lambda$  and the ascending phase of a circuit oscillation (see Eq.5), such that images with the largest and smallest degrees of noise correspond to the trough and peak of the oscillation, respectively (this is a somewhat arbitrary choice, motivated by some of the past literature). There is no direct correspondence to the descending phase of the $\\beta_t$ oscillation in diffusion models, which is a unique element of our solution. The traditional forward process would provably achieve the correct outcome computationally (since its entire purpose is to map the actual input distribution into a gaussian), something which is not formally guaranteed for the oscillation. However, that solution would not match the biological constraint that the neural dynamics driving the evolution of network activity cannot dramatically change every few ms or tens of ms, whereas we can use the same RNN dynamics with increasing levels of noise and keep things consistent.\n\nTraining an ANN-based model for dendrites: the biological implementation of this process is beyond the scope of this paper, but there are some technical results that suggest that one can define a local denoising objective that does not require knowledge of the clean image, but only of the the general noise statistics used to corrupt it (Raphan and Simoncelli, 2014), so it is possible in principle to construct a denoising optimization procedure that involves relatively local computations. To which extent those operations map to experimental observations on synaptic and branch plasticity remains to be determined.\n\nQuestion answers:\n-in Fig.1E The only difference between the ascending and descending phases is whether the degree of noise that is present in the neural transition operator is decreasing or increasing. The increasing level of noise in the dynamics effectively drives the activity pushing it increasingly off manifold. The statistics of exactly where one ends up are not provably identical to those imposed by the traditional feedforward process, but when interacting with the attractor dynamics in the ascending oscillation phase it seems to be close enough to effectively decorrelate samples across cycles. \n- sorry for the misdirection, we have now corrected the caption to top/bottom\n- we made the choice of indexing the network by time but the nature of the operations really depends by the phase of the oscillation, which maps one to one to the time step within a cycle in regular diffusion. We were hoping that it makes the correspondence between what happens algorithmically and what might happen in the brain clearer, but it does add extra notation, perhaps unnecessarily."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2900/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713165124,
                "cdate": 1700713165124,
                "tmdate": 1700715474179,
                "mdate": 1700715474179,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qyc8cSYI5z",
            "forum": "S5aUhpuyap",
            "replyto": "S5aUhpuyap",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2900/Reviewer_XKTM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2900/Reviewer_XKTM"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors proposed a diffusion-based recurrent circuit for sampling-based probabilistic computation. They used recurrent circuits to represent complex priors implicitly, and the sampling-based inference was accomplished using noise modulated by an oscillatory global signal. The recurrent circuits implement the diffusion models with dendritic nonlinearities and stochastic somatic integration. They showed that the dynamics can be gated by bottom-up or top-down signals to generate samples from the corresponding posteriors in low-dimensional nonlinear manifolds and multimodal posteriors to achieve flexible inference."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "There have been earlier works exploring ideas of recurrent connections to encode priors and the neural dynamics to perform Langevin sampling. The proposed plausible neural circuit implementation of across-task inference in which a common prior is encoded in the recurrent connections with dendritic nonlinearities optimized for denoising is novel.  The connection to diffusion models,  the use of global oscillatory signals as sampling control, and the use of bottom-up and top-down signals for gating the samplings across multiple tasks are also new and interesting and represent a conceptual advance. It does provide a new plausible framework to allow flexible sampling of complex distributions."
                },
                "weaknesses": {
                    "value": "While the connection to DM is inspiring, there is no direct evidence to support the key assumptions and innovation of the model -- the dendritic nonlinearities and DM-inspired oscillatory sampling schedule. They remain a fragment of imagination.  As it is mostly a theoretical neuroscience model that works only on a toy example for demonstration, it would be worthwhile to articulate the predictions and the assumptions of the model that can be tested and evaluated by neurophysiological experiments."
                },
                "questions": {
                    "value": "How can the models be tested? What evidence would prove they are correct or falsify them?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2900/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771581180,
            "cdate": 1698771581180,
            "tmdate": 1699636233395,
            "mdate": 1699636233395,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aByQbGWjJv",
                "forum": "S5aUhpuyap",
                "replyto": "qyc8cSYI5z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2900/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2900/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Individual reply Reviewer XKTM"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive comments. \n\nIt may be worth noting that our sampling dynamics are qualitatively quite different from Langevin, even though it is indeed the case that our work builds upon a body of work on MCMC approaches for neural sampling. \n\nProviding direct evidence for individual elements of our solution is indeed difficult, but not outside the scope of future experiments. There is ample indirect evidence that similar kinds of computations are possible in the brain: certain interneurons types have both oscillatory responses and target principal cells\u2019 dendritic arbors, which means that they can modulate the effective neuron nonlinearities in a cyclic fashion. Similarly, the phase of the local oscillation can affect somatic subthreshold variability and change the noisiness of the generated neuron outputs. Finally, there is ample evidence (at least from within the hippocampus and for hippocampal-cortical interactions) that across area communication happens preferentially at certain phases of a local oscillation, and Communication through coherence theories have made similar arguments for the cortex. So, we would argue that all of the individual elements of the model have at least loose experimental support (as in something of that flavor is known to happen, even if it cannot be matched in the details).  That said, we do identify one very concrete analysis that would prove or disprove the model in the paper, in the form of activity pattern similarities across different phases of an oscillatory cycle. There is experimental proof of feasibility of such an analysis (used for a slightly different purpose) in the work of Berkes et al.\n\nAbout the use of toy example models: see common reply. Briefly, the low dimensionality of the distributions considered is a convenient choice rather than a limitation of the model; we now include a MNIST experiment representing actual images to illustrate that point."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2900/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713091211,
                "cdate": 1700713091211,
                "tmdate": 1700715147987,
                "mdate": 1700715147987,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JAhq2gHQni",
            "forum": "S5aUhpuyap",
            "replyto": "S5aUhpuyap",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2900/Reviewer_8Gad"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2900/Reviewer_8Gad"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a neural circuit model for Bayesian inference, where the prior is proposed to be encoded via a DDPM-like mechanism in the dendrites of neurons. Some additional modifications were made to be more biorealistic, such as the dendrites instantiated as tree-structured MLPs, and iterative prior sampling driven by a global oscillatory noise schedule (as opposed to the standard, \u201cdiscontinuous\u201d reverse diffusion sampling). The authors show demonstrate that, on a swiss-roll toy task, the neural sampler performs similarly to the standard DDPM. Furthermore, the authors demonstrate how the prior can be flexibly combined with multiple additional sources of information (likelihoods), such as sensory observation and contextual signals, for posterior sampling. Finally, the authors make experimental predictions on how different phases of the global oscillation would be informative if such a representation is implemented in real neural circuits."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I found the paper to be clearly and concisely written, with very informative figures to illustrate the main idea and key results. The proposed circuit implementation of a diffusion model in the dendrites is interesting and novel, while the oscillatory noise schedule is an elegant device, with an intuitive (though potentially misleading) mapping to neural oscillations. As demonstrated, the proposed model can incorporate multiple likelihood sources separately, and at least for the toy tasks, achieve good performance. I did not examine the math thoroughly, but overall I believe the paper is of high technical quality, and presents an interesting hypothesis (and testable model) for how neural circuits may encode priors."
                },
                "weaknesses": {
                    "value": "For me, the paper suffers from two major weaknesses:\n\nFirst, while the model is elegant and combines a SOTA class of generative model in ML (DDPMs) with neural circuitry, the end product feels too artificial in construction and biologically implausible, with its many strong restrictions / assumptions. For one, the diffusion model in this implementation can be trained as per usual, but how would real dendrites in a neuron go through this learning? The authors state the the learning aspect is for future work, but that\u2019s a huge \u201cif\u201d\u2014is it really realistic to suppose that such a complex mechanism can emerge in vivo, without any hint for the readers of how it could? Additionally, sampling critically depends on a stationary and permanent oscillation, which is rarely found in the brain. Even hippocampal theta, which the authors draw inspiration from, are irregular in time and frequency, nevermind oscillations in other cortical areas. \n\nSecond, the performance is only demonstrated on a relatively simple task of sampling from a very smooth and low-dimensional manifold. I understand that the quantitative results are mostly a demonstration of proof of principle. However, there\u2019s no indication that this would work with mildly more complex high-d distributions, as the authors had originally motivated (and pointed out as a weakness in previous literatures, i.e., mostly Gaussian prior/posteriors). It would be nice to have some indication of how this can be scaled to perform a mildly more complicated task, like conditionally sampling MNIST, or would it be completely infeasible given the architectural constraints? \n\nThese are the two major categories of concern, and I have a number of other concrete issues in the limitation / questions sections below. Taken together, I am skeptical of how much of the claims regarding \u201cneural circuit implementation\u201d is substantiated. And if not, how impactful would the contribution be, which is essentially connectivity-constrained DDPM with an oscillatory noise schedule. Therefore, I recommend borderline reject, noting that it is a well written paper (with some technically dense sections) and solid work but perhaps for a more niche readership, and that I am open to be convinced of its potential biological relevance."
                },
                "questions": {
                    "value": "- is there recurrent interaction between neurons? It\u2019s also a bit unclear how many neurons there are, or are all the results from a single dendritic tree? Also, what exactly are the inputs/outputs of the dendritic networks, and what exactly is the somatic \u201ccompartment\u201d doing, or is that functionally just the last layer of the dendrites? Apologies if I had missed this obvious info.\n- In the case of posterior inference (Figure 2/3), do the samples still show zero autocorrelation?\n- I may have fundamentally misunderstood something, but the authors motivate their proposed architecture as flexible, since it can be reused for various inference scenarios, just swapping out or combining likelihoods. This is demonstrated well for the current model, but wouldn\u2019t this be true for a model where a different population encodes the prior as well? Why is it necessary that it\u2019s in the dendrites? Or is that just a \u201csemantic\u201d difference, since the branching networks can equally be implemented as different neurons?\n- Isn\u2019t it typically the case that bottom-up sensory info and contextual info are thought to be likelihood and prior, respectively? Whereas here, they are represented as two different steams of likelihoods. Can the authors comment on this discrepancy?\n- there is a recent body of experimental evidence implicating oscillations of different frequencies coordinating bottom up (gamma, ~40Hz) vs. top down (beta, ~15Hz) signaling (see A. Bastos, EK Miller, etc.), while here it\u2019s crucial that there is a global oscillation of a single frequency, otherwise the prior and likelihood sampling are temporally misaligned.\n- The proposed implementation draws one prior / posterior sample per oscillation cycle, which, given the fastest cortical rhythm (40Hz gamma), results in 40 samples per second, and more likely to be less, e.g., 8Hz theta in the hippocampus. Is this sampling speed too slow? Does it match behavioral data of evidence accumulation?\n- As I mentioned above, most of the time, most areas of the cortex are not experiencing oscillations. Furthermore, oscillations tend to disappear during task engagement (such as 10Hz alpha in visual areas and 20Hz beta in motor areas), which would be a time that is critical for sampling. How reliable would the proposed implementation be in such scenarios?\n- caption for Figure 1F is missing"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2900/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2900/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2900/Reviewer_8Gad"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2900/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698882833087,
            "cdate": 1698882833087,
            "tmdate": 1699636233283,
            "mdate": 1699636233283,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xbmbj538Sz",
                "forum": "S5aUhpuyap",
                "replyto": "JAhq2gHQni",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2900/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2900/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Individual reply Reviewer 8GAd"
                    },
                    "comment": {
                        "value": "Thank you for the feedback.\nAbout the general biological plausibility of our scheme (see also reply to reviewer XKTM):  \n\nOscillation: It is fundamentally in the nature of modeling to abstract away some of the inessential details. Our oscillatory signal is such an abstraction. In new results we now show numerically that the core mechanism remains largely unaffected when using an imperfect oscillation, which varies in amplitude and period from cycle to cycle by a significant margin (Suppl. Section B5). \n\nLearning: Learning probabilistic representations is a huge open question in the field. Virtually none of the published circuit level models provide a learning procedure that can construct them. This is true about models of priors (Ganguli and Simoncelli, 2014, although something similar can be learned with a REINFORCE type of rule, Bredenberg et al, 2020) and -to our knowledge- all neural sampling literature. That said, denoising as a learning objective is more local and in some ways mathematically simpler, which makes us hopeful about our ability to map it into plasticity-based biological learning. Indeed, there are some technical results that suggest that one can learn a Bayesian estimator without requiring knowledge of the distribution of clean data, but only of the the general noise statistics used to corrupt it (Raphan and Simoncelli, 2007), which suggests a mechanism by which biological systems may learn to denoise without having ever seen clean data.\n\n\nAbout task complexity: see common reply. Briefly, the distributions we consider are actually quite complex in light of the previous neural sampling literature (complexity here is defined as nonlinear low-d manifolds embedded in high-d ambient spaces and strong multimodality). None of the previously published neural sampling models would be able to represent or sample from our \u2018simple\u2019 toy examples, so this is already a qualitative improvement over the comp. neuro. SOTA. Further, we now include new experiments with MNIST to show that the model can be scaled up without problems (Suppl.Section B6).  \n\nA more detailed note on the limits of distributional complexity that can be in principle represented: there is a natural question about \u201chow complex can it possibly get?\u201d Based on the machine learning literature on diffusion models the answer is at least pixel level natural images, although the number of parameters and complexity of dendritic nonlinearities involved in a naive implementation of that is unrealistically high. In subsequent work we are exploring how hierarchical versions of the same idea would scale up prior complexity while keeping the neural resources required for its representation relatively small. \n\nDetailed question answers:\n\nRecurrence: indeed this is a network of all-to-all connected units, each of which involves a neuron specific nonlinearity, with parameters trained by denoising.  The number of neurons is the same as the ambient space (10). The somatic compartment has a somewhat privileged role in that it introduces stochasticity in the neuron responses (although we have tried a version where noise was injected along the dendritic tree, with no particular ill effect). These details are now clearly explained in the updated manuscript.\n\nAutocorrelation function of posterior: Yes, the samples still show zero autocorrelation for posterior inference. The results are shown in supplement, in the section titled B.3 Posterior Autocorrelation.\n\nFlexible inference in other schemes: In principle, a linear PPC circuit that separately encodes the log prior can combine its representation additively with incoming sensory/contextual log likelihoods to perform inference; flexible inference would require a different kind of gating between subpopulations, but it can in principle be done, although to our knowledge it hasn\u2019t been demonstrated explicitly. Nonetheless, this only works for one-dimensional or fully factorized joint distributions. The type of priors the brain needs to represent are arguably more complex which has motivated probabilistic circuits based on sampling. We now provide a way to achieve flexible inference for such complex distributions.\n\nShould the top-down contribution be called \u2018prior\u2019 or \u2018likelihood\u2019: We agree that our choice of wording is (although formally correct) a little inconsistent with some of the neural literature. We are thinking about these qualifiers from the perspective of an intermediate stage of a hierarchical inference process, and so the definition of the role of top-down information is with regards to the locally represented latents."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2900/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713011283,
                "cdate": 1700713011283,
                "tmdate": 1700714972929,
                "mdate": 1700714972929,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]