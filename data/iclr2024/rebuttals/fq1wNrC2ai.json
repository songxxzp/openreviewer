[
    {
        "title": "Sample-efficient Learning of Infinite-horizon Average-reward MDPs with General Function Approximation"
    },
    {
        "review": {
            "id": "DOTUMXrkKw",
            "forum": "fq1wNrC2ai",
            "replyto": "fq1wNrC2ai",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4471/Reviewer_jdKK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4471/Reviewer_jdKK"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies sample efficiency in infinite-horizon averaged MDP setting with general function approximation.\nThe authors propose average-reward generalized eluder coefficient to characterize the exploration difficulty in learning, and contribute an algorithm called Fixed-Point Local Optimization and establish sublinear regret with polynomial dependence on AGEC coefficient and span of optimal state bias, etc."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper writing is clear. The notations, definitions are clearly stated. Comparison with previous works are also clearly summarized in Table 1."
                },
                "weaknesses": {
                    "value": "1. It seems to me the main contribution of this paper is to transfer and generalize some existing techniques and results (especially in finite horizon setting) to infinite horizon averaged return setting. The definition of AGEC Complexity Measure, FLOP algorithm and the techniques for analyzing lazy update rules seem share much similarity with previous literature like [1], [2]. I didn't find much novelty in algorithm design or technique analysis.\n\n2. There is no conclusion section. I would suggest the authors at least have a few works to summarize the paper and discuss the future works. \n\n\n[1] Zhong et. al., Gec: A unified framework for interactive decision making in mdp, pomdp, and beyond\n\n[2] Xiong et. al., A general framework for sequential decision- making under adaptivity constraints\n\n\n## Post Rebuttal \n\nThanks for the detailed reply. I think my main concerns are addressed, and I'm willing to increase my score."
                },
                "questions": {
                    "value": "* What are the novelties in technical or algorithmic level in this paper? What are the new challenges for exploration in infinite horizon averaged reward setting?\n\n* Is the \"lazy policy update\" really necessary? Although the authors explain the motivation for low policy switching is because of the additional cost in regret analysis. I'm curious whether it can be avoidable or it reveals some fundamental difficulty.\n\n* In Theorem 3, the definition of $\\beta$, is $sp(v^*)$ inside or outside of the log?\n\n* Why the algorithm is called \"Fixed-Point ...\"? I'm not very understand why Eq. 4.1 is a fixed-point optimization problem."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4471/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4471/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4471/Reviewer_jdKK"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4471/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697558529425,
            "cdate": 1697558529425,
            "tmdate": 1700919870940,
            "mdate": 1700919870940,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l7rk4rE0uK",
                "forum": "fq1wNrC2ai",
                "replyto": "DOTUMXrkKw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4471/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4471/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jdKK (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your comments and assessments!\n\n> What are the novelties in the technical or algorithmic level in this paper? What are the new challenges for exploration in infinite horizon averaged reward setting?\n\n- Please see our general response for details.\n\n> It seems to me the main contribution of this paper is to transfer and generalize some existing techniques and results (especially in finite horizon setting) to infinite horizon averaged return setting. The definition of AGEC Complexity Measure, FLOP algorithm and the techniques for analyzing lazy update rules seem share much similarity with previous literature like [1], [2]. I didn't find much novelty in algorithm design or technique analysis.\n\n- While we admit that the structural assumption over the Bellman dominance shares a similar form as the standard GEC (Zhong et al., 2022), the transferability assumption is new. Moreover, our algorithm design is different from the posterior-sampling-based approach presented in Zhong et al. (2022). We opt for an optimism-based algorithm for establishing a low policy switch updating, which is crucial in the achievement of sublinear regret for AMDPs.  \n- Compared with Xiong et al. (2023), we adopt a weaker assumption over the Bellman dominance and provide a more rigorous definition of the eluder condition with nuanced adpation to the average-reward setting. Moreover, the average-reward RL adopts a completely different notion of regret, and the Q- or V- function does not naturally exist due to the infinite horizon without discount. we consider a broad subclass where a modified version of the Bellman optimality equation holds and present a novel regret decomposition for theoretical analysis of the explorability of AMDPs, resulting in significantly different regret analysis from the aforementioned work.\n\n> Is the \"lazy policy update\" really necessary? Although the authors explain the motivation for low policy switching is because of the additional cost in regret analysis. I'm curious whether it can be avoidable or it reveals some fundamental difficulty.\n\n- That's a really interesting question. There are two primary approaches to address online learning of AMDPs: the reduction to episodic MDPs, and the utilization of a low policy switching algorithm. However, achieving $\\mathcal{\\tilde{O}}(\\sqrt{T})$ has proven to be challenging for reduction-based algorithms in existing works under the online learning framework (Wei et al., 2019, Wei et al., 2021), unless fortified with strong assumptions, such as ergodic AMDPs, where every policy is guaranteed to reach every state after a sufficient number of steps. In our pursuit of a near-optimal algorithm without relying on such stringent assumptions, we introduce an additional yet mild structural assumption to ensure the validity of a low-switching algorithm for general function classes. \n\n- At out best knowledge, all existing algorithms for online learning of AMDPs depend on such lazy update strategy to achieve a near optimal regret beyond strong assumptions, from the classical UCRL2 algorithm for tabular AMDPs (Jaksch et al., 2010), to FOPO for linear AMDPs (Wei et al., 2021) and (UCRL2-VTR for linear mixture MDPs (Wu et al., 2022). Furthermore, we speculate that the underlying mechanism behind this phenomenon lies in the challenge of accurately evaluating $\\pi_t$ when imposing a step-by-step policy update, i.e., $\\pi_{t}\\neq\\pi_{t+1}$ for all $t\\in[T]$. This difficulty arises due to the intricate dependency structure in the historical data $\\{(s_i,a_i)\\}_{i\\leq[t]}$.\n\n- Specifically, in the episodic setting, the learner executes the policy $\\pi_t$ throughout the epside, and the value function can be computed by bellman equation starting from a zero function. Thus, the error can be controlled and is bounded by the cummulative error across episode following decoposition ${\\rm Reg}(T)\\leq\\sum_{t=1}^T\\sum_{h=1}^H\\mathbb{E}[\\mathcal{E}(f_t)(s_h^t,a_h^t)\\vert\\pi_t]$. This means that the estimation error of the current policy can be controlled by the sum of uncertainty. But for average reward case, if you update policy in each iteration, we cannot bound the suboptimality of the current policy by a sum of estimation errors as ${\\rm Reg}(T)\\leq\\sum_{t=1}^T\\mathcal{E}(f_t)(s_t,a_t)+\\sum_{t=1}^T\\left(\\mathbb{E}[V_t(s_{t+1})]-V_t(s_t)\\right)$.\n\n- The low-switching policy update in our algorithm ensures that the learner scrutinizes the policy based on data collected from new epochs. This corresponds to the update condition in our algorithm, where we only update when a substantial increase in cumulative discrepancy surpassing $3\\beta$ has occurred since the last update."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4471/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205381535,
                "cdate": 1700205381535,
                "tmdate": 1700205381535,
                "mdate": 1700205381535,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lQNpYbZEZb",
            "forum": "fq1wNrC2ai",
            "replyto": "fq1wNrC2ai",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4471/Reviewer_q11B"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4471/Reviewer_q11B"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers reinforcement learning for infinite-horizon average-reward MDPs under function approximation, and (i) generalizes the concept of eluder dimension to average-reward MDPs as a complexity measure, and (ii) proposes a new algorithm named FLOP to solve average-reward MDPs with low complexity in the sense of the generalized eluder dimension defined in the paper."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problem addresses the challenging problem of infinite-horizon, average-reward MDPs in the function approximation setting. The idea of extending eluder dimension to this class of MDPs is a good idea. Also, the proposed algorithm that achieves sublinear regret seems to be a promising extension of the fitted Q-iteration algorithm, which takes lazy policy change into account."
                },
                "weaknesses": {
                    "value": "The definition of AGEC (Definition 3), which is the central object and contribution in this paper, lacks clarity:\n- Big-O notation is used in the definition, where the defined quantities $d_G$ and $\\kappa_G$ (which also appear in $\\mathcal{O}$) are smallest numbers that satisfy the inequalities that involve $\\mathcal{O}$. This does not make much sense as a mathematical definition, with $\\mathcal{O}$ being asymptotic.\n- The set of discrepancy functions $\\{l_f\\}_f$ abruptly appears in Definition 3 without any proper definition. In later sections, we observe that it is an important quantity.\nI would suggest a clear, mathematical definition of the complexity measure that constitutes one of the major contributions of this paper."
                },
                "questions": {
                    "value": "In addition to the clarification of the definition of AGEC, I have the following questions:\n- The function approximation error can be critical in RL, as its multiplying factor usually depends on the exploration performance in various forms. If we remove the realizability assumption, how does the additional term depend on the complexity measure defined in this paper?\n- In Equation 2.1, what is $V^*(s,a)$? Should it be $Q^*$?\n- In the abstract and in multiple places in the paper, $sp(v^*)$ appears with $v^*$. Should it be $sp(V^*)$? In the paper, it is assumed that $sp(V^*)$ is known. Is this knowledge necessary?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4471/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4471/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4471/Reviewer_q11B"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4471/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826693311,
            "cdate": 1698826693311,
            "tmdate": 1700733543532,
            "mdate": 1700733543532,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jgThjfiXgx",
                "forum": "fq1wNrC2ai",
                "replyto": "lQNpYbZEZb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4471/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4471/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer q11B"
                    },
                    "comment": {
                        "value": "Thank you for your comments and assessments!\n\n> The definition of AGEC (Definition 3), which is the central object and contribution in this paper, lacks clarity.\n\n- Thank you for pointing out the lack of rigor in our definitions and your comments have been well-taken. In the original version, we used $\\mathcal{O}$ notation to hide the contants in the definition, which may not adhere to mathematical rigor. In the revised version, we substitute the $\\mathcal{O}$ notation with explicit definitions of absolute constants (highlighted in blue), which does not hurt the correctness of the result.\n- Regarding the dicrepancy function, we would like to highlight that the definition of the discrepancy function is inherently **problem-specific**, making it challenging to offer a unified definition applicable across all cases. To ensure clarity in our presentation, we provide a detailed definition of the problem-specific discrepancy function in each corresponding section such as (3.1), (3.3), (C.2).\n\n> The function approximation error can be critical in RL, as its multiplying factor usually depends on the exploration performance in various forms. If we remove the realizability assumption, how does the additional term depend on the complexity measure defined in this paper?\n\n- We would like to clarify that our complexity measure characterizes the difficulty in exploring the best-in-class model within the designated hypothesis class. On the other hand, the approximation error stems from the disparity between the best-in-class approximator and the ground-truth optimal function, which is not theoretically provable. Consequently, the removal of the realizability assumption will sure introduce additional error terms; however,  **it does not depend on the proposed complexity measure**.\n- A generalization of realizability for value-based problems is that there exists $f\\in\\mathcal{F}$ and $\\epsilon_{\\rm real}>0$ such that $\\Vert Q^*-Q_f\\Vert_\\infty\\leq\\epsilon_{\\rm real}$. The condition requires that the optimal bias state-action value function lies in the function class approximately with $\\epsilon_{\\rm real}$ error, enabling a more nuanced analysis on approximation. In this case, we conjecture that our algorithm, designed with a larger $\\beta$ to accommodate misspecification, is capable of finding an $(\\epsilon + \\mathrm{poly}(d, \\mathrm{sp}(V^*)) \\cdot \\epsilon_{\\rm real})$-optimal policy using polynomial (in $\\epsilon$) samples. It is indeed a very interesting future direction to design a unified sample-efficient algorithm considering the approximation error under general function approximation. We leave more meticulous analysis as future work.\n\n---\n\n> In Equation 2.1, what is $V^*(s,a)$. Should it be $Q^*$\u00a0? \n\n- Thank you for pointing that out. We have rectified the typo (Page 3), which should be \n$$J^*+Q^*(s,a)=r(s,a)+\\mathbb{E}_{s'\\sim\\mathbb{P}(\\cdot|s,a)}[V^*(s')].$$ \n\n---\n\n> In the abstract and in multiple places in the paper, ${\\rm sp}(v^*)$ appears with $v^*$. Should it be ${\\rm sp}(V^*)$ ? In the paper, it is assumed that ${\\rm sp}(V^*)$ is known. Is this knowledge necessary?\n\n- Thank you for pointing out the confusion in the notations. We have now replaced ${\\rm sp}(v^*)$ by ${\\rm sp}(V^*)$ for a clearer presentation in the revised version. Regarding the span ${\\rm sp}(V^*)$, we assert that the knowledge of the span of the optimal state bias function is crucial for **establishing a more rigorous regret guarantee** due to theoretical necessity. It is noteworthy that, unlike the episodic setting, there is no inherent upper bound for the optimal function $V^*$ in the average-reward setting, leading to a hypothesis class with uncontrollable complexity. The assumption of a known span implies a bounded span for the optimal bias function, with ${\\rm sp}(V^)$ being available to the learner. This knowledge allows for the construction of a more stringent optimistic parameter. Alternatively, in practical scenarios, the learner can choose a **conservative estimation** of the span, such as a sufficiently large constant $B_{sp}>0$, ensuring $B_{sp}\\geq{\\rm sp}(V^*)$. The substitution of ${\\rm sp}(V^*)$ with $B_{sp}>0$ in the construction of $\\beta$ ensures the persistence of the optimistic planning in Algorithm 1, and results in an inflation of regret in squared root."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4471/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700204881578,
                "cdate": 1700204881578,
                "tmdate": 1700204881578,
                "mdate": 1700204881578,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PgIO0puqIH",
                "forum": "fq1wNrC2ai",
                "replyto": "jgThjfiXgx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4471/Reviewer_q11B"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4471/Reviewer_q11B"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their detailed responses, and the changes regarding the manuscript, which improved the presentation. I have no further questions."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4471/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733486290,
                "cdate": 1700733486290,
                "tmdate": 1700733486290,
                "mdate": 1700733486290,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kW7CquvnfM",
            "forum": "fq1wNrC2ai",
            "replyto": "fq1wNrC2ai",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4471/Reviewer_SbWP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4471/Reviewer_SbWP"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies infinite-horizon average-reward MDPs (AMDPs) with general function approximation. It extends the generalized eluder coefficient to average-reward generalized eluder coefficient (AGEC) under infinite-horizon MDPs. After showing that the low AGEC captures most existing structured MDPs, the paper develops an algorithm called FLOP to solve AMDPs with sublinear regret $O(\\sqrt{T})$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper provides a more general complexity measure AGEC that captures a large class of MDPs.\n2. The design of confidence set is new, and the lazy update of policy is a good feature of algorithm design, which might be helpful for real implementations."
                },
                "weaknesses": {
                    "value": "1. While the paper states that method covers most existing works, the detailed comparisons in terms of the regret performance are missed."
                },
                "questions": {
                    "value": "Can authors provide a brief comparison between this work and existing works in terms of the regret results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4471/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838336859,
            "cdate": 1698838336859,
            "tmdate": 1699636422727,
            "mdate": 1699636422727,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q87HySzMJL",
                "forum": "fq1wNrC2ai",
                "replyto": "kW7CquvnfM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4471/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4471/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SbWP"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for the positive assessment of this work! Here's our response to the question.\n\n> Can authors provide a brief comparison between this work and existing works in terms of the regret results?\n\n- Please see our general response for details."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4471/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700204465431,
                "cdate": 1700204465431,
                "tmdate": 1700204465431,
                "mdate": 1700204465431,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uKAA4Vfdm5",
            "forum": "fq1wNrC2ai",
            "replyto": "fq1wNrC2ai",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4471/Reviewer_pAvA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4471/Reviewer_pAvA"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the infinite-horizon average-reward Markov Decision Process (MDP) setting and introduces a comprehensive function approximation framework, accompanied by a corresponding complexity measure (AGEC) and an algorithm (FLOP). When compared with other work addressing AMDPs, the proposed framework covers the widest range of settings including both model-based and value-based, in addition, the theoretical analysis is based on the Bellman optimality assumption and the sample complexity is dependent on finite span, which is weaker compared with previous work (compared with communicating AMDP assumption and finite diameter dependence)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper proposes a general framework for AMDP, encompassing both model-based and value-based. \n\n- The authors propose a new complexity measure that is larger than the Eluder dimension.\n\n- The authors propose a new algorithm that matches the state-of-the art sample complexity results."
                },
                "weaknesses": {
                    "value": "- The sample complexity is based on GEC and is not entirely novel. Specifically, Definition 3 (Bellman Dominance) in this paper is of the same form with Definition 3.4 of Zhong et al. (2022b), where $d$ is defined such that the sum of the Bellman error being less than the in-sample training error plus the burn-in cost.\n\n- The authors introduce the discrepancy function in the definition of AGEC, and shows a simple example of discrepancy function being the Bellman error for the value-based case, and $(r_g+P_g V_{f'})(s_t, a_t)-r(s_t, a_t)+V_{f'}(s_{t+1})$ for the model-based case.  However, there seems a lack of discussion regarding alternative choices for the discrepancy function, such as the Hellinger distance-based discrepancy in Zhong et al. (2022b).\n\n- The algorithm is based on upper confidence bound, which the confidence region chosen based on the discrepancy function. This approach is closely related to Algorithm 1 in Chen et al. (2022b) and Algorithm 1 in Jin et al. (2021).\n\n- The proposed algorithm is usually impractical to implement, since it involves solving a global constrained optimization."
                },
                "questions": {
                    "value": "- In this paper, the authors directly assume the transferability of the discrepancy function, a concept closely related to Lemma 41 in Jin et al. (2021). Could the authors elaborate on the primary technical challenges they encountered while deriving their theoretical results, when adapt the GEC in Zhong et al. (2022b) to the infinite-horizon average-reward setting, with a constrained algorithm, which seems to be a generalization of the framework established by Jin et al. (2021)?\n\n- Can the authors elaborate the optimality of the regret in Theorem 3, when restricted to each specific instances? i.e. linear mixture AMDP, linear AMDP, etc."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4471/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699608388626,
            "cdate": 1699608388626,
            "tmdate": 1699636422573,
            "mdate": 1699636422573,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "etFeu12Jp5",
                "forum": "fq1wNrC2ai",
                "replyto": "uKAA4Vfdm5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4471/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4471/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pAvA"
                    },
                    "comment": {
                        "value": "Thank you for your comments and assessments!\n\n> The sample complexity is based on GEC and is not entirely novel. Specifically, Definition 3 (Bellman Dominance) in this paper is of the same form with Definition 3.4 of Zhong et al. (2022b), where is defined such that the sum of the Bellman error being less than the in-sample training error plus the burn-in cost.\n-  While we admit that the structural assumption over the Bellman dominance shares a similar form as the standard GEC (Zhong et al., 2022b) with nuanced adaption in average-reward terms, the transferability assumption is a novel addition. This additional structural constraint presents the unique challenges of AMDPs, and plays a pivotal role in facilitating the design of a low-switching algorithm.  This is particularly essential in addressing the inherent difficulty in exploring AMDPs. \n-  Moreover, our algorithm design diverges from the posterior-sampling-based approach presented in Zhong et al. (2022b). We embrace an optimism-based algorithm to establish a low policy switch updating mechanism, a crucial element in achieving sublinear regret for AMDPs. As a consequence, the technical details of our regret analysis differ significantly from the aforementioned work. Please refer to our general response more desciptions of technical novelties. \n\n>  However, there seems a lack of discussion regarding alternative choices for the discrepancy function, such as the Hellinger distance-based discrepancy in Zhong et al. (2022b).\n\n- We appreciate the reviewer's suggestion regarding the importance of discussing alternative choices for the discrepancy function, including the Hellinger distance-based discrepancy. Discussions about MLE-AGEC concerning an alternative discrepancy functions are presented in **Appendix C** (highlighted in blue) with a different transferability assumption over the structure. Here, we choose the total variation (TV)-based discrepancy function instead, which results from the slight difference between the choice of discrepancy: $l_{f'}(f,g,\\zeta_t)=\\frac{1}{2}|P_f(s_{t+1}|s_t,a_t)/P_{f^*}(s_{t+1}|s_t,a_t)-1|$ (ours) and $l_{f'}(f,\\zeta_t)=\\frac{1}{2}(P_f(s_{t+1}|s_t,a_t)/P_{f^*}(s_{t+1}|s_t,a_t)-1)^2$ (Zhong et al., 2022b). We remark that both choices of discrepancy is sufficient to cover MLE-based algorithms.\n\n> The algorithm is based on upper confidence bound, which the confidence region chosen based on the discrepancy function. This approach is closely related to Algorithm 1 in Chen et al. (2022b) and Algorithm 1 in Jin et al. (2021).\n> The proposed algorithm is usually impractical to implement, since it involves solving a global constrained optimization.\n\n- While acknowledging the resemblance of our algorithm design to previous lines of work on optimism-based algorithms for general function approximation, we notably diverge by employing a low-switch updating strategy coupled with a discrepancy-based updating condition.\n- Moreover, we would like to comment that our algorithm is in some sense information theoretical -- our work identifies a statistically tractable subclass of AMDPs, and the algorithm **serves as a certificate for the regret upper bound**. The main focus of our paper is to address the problem of minimal structural assumptions that empower sample-efficient learning in AMDPs, which facilitates algorithm design beyond specific problems such as linear AMDPs and linear mixture AMDPs.\n\n> Could the authors elaborate on the primary technical challenges they encountered while deriving their theoretical results, when adapt the GEC in Zhong et al. (2022b) to the infinite-horizon average-reward setting, with a constrained algorithm, which seems to be a generalization of the framework established by Jin et al. (2021)?\n\n- Please see our general response for details.\n\n> Can the authors elaborate the optimality of the regret in Theorem 3, when restricted to each specific instances? i.e. linear mixture AMDP, linear AMDP, etc.\n\n- Please see our general response for details.\n\n---\n[A1] Han Zhong, Wei Xiong, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang, and Tong Zhang. Gec: A unified framework for interactive decision making in mdp, pomdp, and beyond. arXiv preprint arXiv:2211.01962, 2022a."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4471/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700204351924,
                "cdate": 1700204351924,
                "tmdate": 1700204351924,
                "mdate": 1700204351924,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]