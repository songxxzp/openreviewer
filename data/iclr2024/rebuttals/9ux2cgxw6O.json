[
    {
        "title": "LOVECon: Text-driven Training-free Long Video Editing with ControlNet"
    },
    {
        "review": {
            "id": "SCluH7MCAn",
            "forum": "9ux2cgxw6O",
            "replyto": "9ux2cgxw6O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3522/Reviewer_WhZ8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3522/Reviewer_WhZ8"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on long video editing with training-free diffusion models.  The authors propose a useful cross-window attention mechanism to ensure the consistency and length of the video. They also leverage DDIM for accurate control and a video frame interpolation model to mitigate the frame-level flickering issue. The authors presented rich and excellent experimental results, and provided their code and products in supplementary materials."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Long video editing, consistency maintenance, and structural fidelity are fundamental issues in text guided long video editing. The authors propose a simple and systematic solution for training free long video editing. The authors present a good number of experiments validating the effectiveness of their approach. The paper is overall well written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The authors have pieced together too many other people's methods to achieve the goals, so their model capabilities are limited, such as being unable to perform shape transformations, object additions or deletions. So their products are limited to color or style changes.\n\n2. The innovation of the methods proposed by the authors is limited, as they have not effectively established the temporal information of the video. The so-called cross-window is naive (a minor change of ControlVideo **[1]**) and may be helpful for local video smoothing, but it still cannot truly establish the temporal dependence of long videos. This is reflected in the experimental results that although the generated video actions are locally coherent, the overall appearance is somewhat strange.\n\n**[1]** Zhang, Yabo, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo and Qi Tian. \u201cControlVideo: Training-free Controllable Text-to-Video Generation.\u201d ArXiv abs/2305.13077 (2023): n. pag."
                },
                "questions": {
                    "value": "Cross-window attention is proposed to improve inter-window consistency, then how do the authors ensure consistency within the window? \nAs shown in Figure 5 (a), fully crossframe attention-based ControlVideo not only maintains the loyalty of the background, but also maintains the temporal consistency of the target subject. On the contrary, the author's method blurs the background and changes the target subject in the temporal sequence (the car window turns red)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3522/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3522/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3522/Reviewer_WhZ8"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3522/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698525115946,
            "cdate": 1698525115946,
            "tmdate": 1699636306041,
            "mdate": 1699636306041,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YmkwiOxa0E",
                "forum": "9ux2cgxw6O",
                "replyto": "SCluH7MCAn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3522/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3522/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable feedback!"
                    },
                    "comment": {
                        "value": "We would like to express our gratitude to Reviewer **WhZ8** for reviewing our paper and raising valuable questions. Below we respond to the questions. We would highly appreciate it if the reviewer agree with our response and consider raising the score. Thank you so much!\n\n**Q1. The model capabilities are limited, such as being unable to perform shape transformations, object additions, or deletions.**\n\n**Please first refer to the general reply for the clarification of our contributions.** We also clarify that the inability to perform shape transformations, object additions, or deletions does not stem from the fact that our model capacities are not enough. The related works [1, 2, 3] can neither achieve that because we all hinge on pre-trained diffusion models and controlnets for **training-free video editing**. Besides, as argued, one of our main goals is to keep the fidelity of the generations to the source videos, instead of creating new content. We sincerely hope the reviewer can re-evaluate the contribution of this work based on these clarifications.\n\n**Q2. The innovation is limited, and cross-window cannot establish the temporal dependence of long videos\uff1f**\n\nThanks for sharing your concerns. We would like to clarify again that we extend the sparse attention mechanism to cross-window attention for long video editing, while vanilla sparse attention or cross-frame attention leads to noticeable transitions between windows. Fig 5 a) shows the result of the separate edit of each window based on naive cross-frame attention. From the experiments, **we can conclude that the cross-window can establish the temporal dependence of long videos to some extent and alleviate the different styles between windows.** \n\n**Q3. Consistency within the window, and worse performance than fully cross-frame attention-based ControlVideo?**\n\nThanks for digging into the details. In cross-window attention, we **incorporate the features of the last frame in the current window in the KV matrix**, which can ensure consistency within the window. In fact, the hierarchical sampler in ControlVideo serves the same purpose of establishing the temporal dependence of long videos. Our cross-frame attention is a replacement for it in an auto-regressive pipeline. We also clarify that **it is natural to get better results when incorporating more features (as done by fully cross-frame attention-based ControlVideo), but we argue that we can get comparable results with less computation overhead and quite better results than vanilla sparse attention, and visualization results are not cherry-picked.** Besides, with our complete pipeline, we perform better than baselines, which shows that each part makes a difference to the final effects, especially the latent fusion module can provide more accurate controls and further improve consistency (see car_comparision.mp4 in the updated supplementary for the comparison results of the \"a red car\" case).\n\n[1] Zhao et.al. ControlVideo: Adding Conditional Control for One Shot Text-to-Video Editing\n\n[2] Khachatryan et.al. Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators\n\n[3] Zhang et.al. ControlVideo: Training-free Controllable Text-to-Video Generation"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3522/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700117675951,
                "cdate": 1700117675951,
                "tmdate": 1700122143212,
                "mdate": 1700122143212,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fnsRHtY7yn",
            "forum": "9ux2cgxw6O",
            "replyto": "9ux2cgxw6O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3522/Reviewer_59Nz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3522/Reviewer_59Nz"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to address the problems of limited video length and temporal inconsistency in text-driven video editing task.\nIt introduces a training-free pipeline based on ControlNet (i.e., LOVECon) for efficient long video editing, including three key designs.\nIn specific, LOVECon develops a novel cross-window attention mechanism to ensure the consistency of global style.\nSecondly, it fuses the latents of source video to obtain more accurate control.\nFinally, it incorporate a video frame interpolation model to deflicker the generated videos."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "See summary."
                },
                "weaknesses": {
                    "value": "1. The main contribution of long-video editing is limited in paper. The cross-window attention mechanism is common in long video editing/generation [1].\n2. Question about visualization results of ControlVideo-II [2] in Fig.3. ControlVideo-II manages to achieve fine-grained control in original paper[2] (e.g., hair color), but fails to do so in Fig.3. Could the authors explain the reason?\n\n[1] Fu-Yun Wang, Wenshuo Chen, et al. Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising.\n[2] Min Zhao, Rongzhen Wang, et al. Controlvideo: Adding conditional control for one shot text-to-video editing."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3522/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698571818935,
            "cdate": 1698571818935,
            "tmdate": 1699636305975,
            "mdate": 1699636305975,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1WzH99X5ub",
                "forum": "9ux2cgxw6O",
                "replyto": "fnsRHtY7yn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3522/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3522/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable feedback!"
                    },
                    "comment": {
                        "value": "We would like to extend our appreciation to Reviewer **59Nz** for taking the time to review our paper and posing questions. Below we respond to the questions. We would highly appreciate it if the reviewer agree with our response and consider raising the score. Thank you so much!\n\n**Q1. The cross-window attention mechanism is common in long video editing/generation in Gen-L-Video.**\n\nThanks for pointing out the connection between our method and Gen-L-Video. From our limited understanding, Gen-L-Video proposes bidirectional cross-frame attention, which is used with a short video clip instead of the whole video, and suggests learning clip identifier $e^i$ for each short video clip to guide the model in denoising the corresponding clip.  But **our proposed cross-window attention is for the whole video**, which is the main difference.\n\nIn this field, cross-frame attention is widely used and different papers set different keyframes but we think they serve the same purpose. However, for the whole video consistency when dealing with long videos, we design the cross-window attention mechanism. **Refer to the general reply for more clarifications of our contributions.**\n\n**Q2. ControlVideo-II manages to achieve fine-grained control in the original paper (e.g., hair color), but fails to do so in Fig.3.**\n\nThanks for noticing the minor differences. In the original codebase of ControlVideo-II, it can only generate 8 frames at a time. For a fair comparison, when generating longer videos, we concatenated the first frame as references (i.e., including the cross-window attention to some extent), and we didn't change other parts. **The visualization results are from the codes with modifications mentioned above.**  If you would like to check the authenticity, you may check the ControlVideo-II codes and generate the video for comparison. The source video is in the supplementary."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3522/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700116990094,
                "cdate": 1700116990094,
                "tmdate": 1700122129785,
                "mdate": 1700122129785,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GOg2Nw0NBN",
            "forum": "9ux2cgxw6O",
            "replyto": "9ux2cgxw6O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3522/Reviewer_EDK1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3522/Reviewer_EDK1"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new long video editing method based on video controlnet and text-to-image latent diffusion models. The main contributions of the proposed method are 1) a cross-clip sparse attention to ensure the consistency of the long video while saving memory, 2) a latent fusion mechanism based on attention maps of the editing target, and 3) a frame smoothing mechanism based on near frame interpolation. The experimental results on 30 videos and the CLIP-based metrics demonstrate that the proposed method has better temporal consistency and video quality than the compared baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- [writing] This paper is easy to follow, and the overall writing is good to me.\n- [method] This work tackles the challenging task that editing long videos.\n- [experiment] The visual results show that the proposed method allows the editing on the target area while maintaining the other image regions intact. Quantitative results also verify that the proposed method has better video quality and temporal consistency."
                },
                "weaknesses": {
                    "value": "- [method] The first drawback of the proposed method is that it is very similar to the VideoControlNet, where the main difference is the modification to tackle long video editing cases. 1) The cross-window attention is a special sparse attention mechanism, which has been widely used by other vision models. 2) The latent fusion module has also been used in other works. 3) The frame interpolation mechanism is a long-history video frame smoothing approach. Although combining existing approaches to solve a challenging problem is meaningful, I am hesitant to give a high score on this work since the \"added\" modules are all very common and bring limited insight into the community.\n- [experiment] In the visual comparison, I find the main visual benefit of the proposed method is that it only edits the target region while keeping the other part intact, which, according to my understanding, is contributed by the latent fusion module, making it hard to evaluate the effectiveness of the cross-window attention. I think more visual (like fig. 5a) and quantitative comparisons without the latent fusion would help.\n- [experiment] The experiments are only based on 30 videos, which is far from enough to show the robustness of the proposed method. Moreover, I feel CLIP-based metrics may not be reliable enough to evaluate the overall quality of the generated videos. IS and FVD (if having a larger evaluation set as the reference) should be used for the quality comparison."
                },
                "questions": {
                    "value": "No"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3522/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805984095,
            "cdate": 1698805984095,
            "tmdate": 1699636305876,
            "mdate": 1699636305876,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zNyMRiE4iM",
                "forum": "9ux2cgxw6O",
                "replyto": "GOg2Nw0NBN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3522/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3522/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable feedback!"
                    },
                    "comment": {
                        "value": "We thank Reviewer **EDK1** for offering suggestions for improvements. Below we respond to the questions. We would highly appreciate it if the reviewer agree with our response and consider raising the score. Thank you so much!\n\n**Q1. The method is similar to the VideoControlNet, and modules are very common?**\n\nThanks for pointing out such a connection. Here we clarify that in VideoControlNet, the editing process for a video involves multiple stages. They acquire inpainting masks based on optical flows, generate the keyframes, and interpolate the inner frames irrelevant to the diffusion pipeline. This resembles the hierarchical sampler in [1]. Instead, we edit the whole video with our diffusion pipeline auto-regressively. We think **the two methods are fundamentally different. Please refer to the general response for more clarifications of our contributions.**\n\n**Q2. Influenced by the latent fusion module, it is hard to evaluate the effectiveness of the cross-window attention?**\n\nThanks for the suggestion. We **add a new baseline** in Fig 5 a), the separate edit of each window based on naive cross-frame attention. **Note that the experiments do not include a latent fusion module.** Comparing our cross-window attention without the latent fusion module to this baseline, we can **conclude that cross-window attention can effectively improve consistency in long video editing.**\n\n**Q3. Experiments don't show the robustness and CLIP-based metrics are not reliable?**\n\nThanks for pointing it out. We clarify that the number 30 was carefully chosen after thoughtful consideration. In the quantitative comparison in [2], only 25 videos were selected for comparisons with baselines. In [3], 40 videos from the Davis dataset and other videos from the wild were used for comparisons. \n\nRegarding metrics, unfortunately, there is currently no standardized validation set or evaluation metric available. Therefore, we have followed the practice of utilizing metrics from the CLIP series and conducting user experiments, which align with previous works like [4] in this field. We also add Con-L2 as a metric to evaluate consistency. We welcome more specific suggestions on the metric.\n\n[1] Yang et.al. Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation\n\n[2] Khachatryan et.al. Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators\n\n[3] Zhao et.al. ControlVideo: Adding Conditional Control for One Shot Text-to-Video Editing\n\n[4] Qi et.al. FateZero: Fusing Attentions for Zero-shot Text-based Video Editing"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3522/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111067128,
                "cdate": 1700111067128,
                "tmdate": 1700122157925,
                "mdate": 1700122157925,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]