[
    {
        "title": "Contrastive Difference Predictive Coding"
    },
    {
        "review": {
            "id": "lJwlZm0trs",
            "forum": "0akLDTFR9x",
            "replyto": "0akLDTFR9x",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7601/Reviewer_Jmo8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7601/Reviewer_Jmo8"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the representation learning for goal-conditioned reinforcement learning problems. Built upon InfoNCE objective, this paper proposes a temporal difference estimator of InfoNCE objective and applied it to goal-conditioned RL algorithm.\n\nIn the experiment section, in the online RL setting, the proposed method is compared with prior goal-conditioned RL algorithms, including quasimetric RL, contrastive RL (Monte Carlo estimator of NCE objective), and hindsight experience relabelling. The proposed method achieved a higher average return in the comparison. Also, in the offline RL setting, The proposed method is compared with quasimetric RL, contrastive RL, and SOTA offline RL algorithms. The proposed algorithm generally outperforms baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is well-written and easy to read. The proposed method is explained and presented clearly.\n\nThis paper provides clear derivation and a solid theoretical foundation for the proposed method in Section 3. I \n\nThe proposed method is supported by extensive experiments comparing with many baseline approaches.\n\nThe analysis in Section 4.3 and 4.4 validate the advantages of the proposed method in comparison with other representation learning approaches. It is impressive to see that the proposed method can stitch together pieces of data."
                },
                "weaknesses": {
                    "value": "Some statements, especially some in the introduction part, seem not fully supported by evidence provided in the paper.\n\nFor example, it is claimed that the proposed method \"enables us to perform counterfactual reasoning\". However, this point is not clear in the following section. Could you please explain it in detail?"
                },
                "questions": {
                    "value": "How is the proposed method sensitive to hyper-parameters? Do we need careful hyper-parameter tuning to make it work? Is there any intuitive guidance about how to adjust the hyper-parameters?\n\nIn Algorithm 1, many notations are introduced for the first time without any definition. Could you please clarify them?\n\nOne important baseline, contrastive RL, is the Monte Carlo estimator of the NCE loss. Could you please also compare with the algorithm using Monte Carlo estimator of InfoNCE loss, since it is already introduced in the prior work Eysenbach et al., 2022"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7601/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698294059203,
            "cdate": 1698294059203,
            "tmdate": 1699636921829,
            "mdate": 1699636921829,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6pUV0o2KzK",
                "forum": "0akLDTFR9x",
                "replyto": "lJwlZm0trs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the responses and suggestions for improving the paper. The reviewer brought up two main questions about hyperparameter selection and comparisons to Monte Carlo InfoNCE loss (contrastive RL (CPC)) in the experiments. We have attempted to address these questions by ablating different hyperparameters (new Fig. 10) and adding comparisons to contrastive RL (CPC) in online experiments (Fig. 2 and Appendix Fig. 6). **Together with the discussion below, does this fully address the reviewer\u2019s concerns?**\n\n> How is the proposed method sensitive to hyper-parameters? Do we need careful hyper-parameter tuning to make it work? Is there any intuitive guidance about how to adjust the hyper-parameters?\n\nWe ran additional ablation experiments to study the effect of different hyperparameters for TD InfoNCE listed in table 2 and the discount factor $\\gamma$. For each hyperparameter, we selected a set of different values and conducted experiments on $\\texttt{push (state)}$ and $\\texttt{slide (state)}$, one easy task and one challenging task, for five random seeds. We report mean and standard deviation of success rates in Appendix E.1. These results suggest that while some values of the hyperparameter have similar effects, e.g. actor learning rate = $5 \\times 10^{-5}$ vs $1 \\times 10^{-4}$, our choice of combination is optimal for TD InfoNCE. For fair comparison, we fixed the batch size of TD InfoNCE to 256, the same number as other baselines (Appendix E of [1] and Appendix C.3 of [2]).\n\n> Could you please also compare with the algorithm using Monte Carlo estimator of InfoNCE loss\n\nAs suggested by the reviewer, we ran new experiments comparing TD InfoNCE to contrastive RL (CPC) (MC InfoNCE) using the official contrastive RL repository [1]. We report results on the Fetch robotics benchmark in Fig. 2 and Appendix Fig. 6, showing mean and standard deviations over five random seeds. Note that we use contrastive RL to denote MC InfoNCE (contrastive RL (CPC)) and contrastive RL (NCE) to denote the original contrastive RL algorithm (See [1] for details). These results suggest that using InfoNCE loss for contrastive RL achieves a slightly better performance than using NCE loss for it on 4 / 8 tasks. Nonetheless, TD InfoNCE still outperforms this InfoNCE variant of contrastive RL (or Monte Carlo InfoNCE) and our conclusions remain the same. We have revised Sec. 4.1 to add this baseline.\n\n> For example, it is claimed that the proposed method \"enables us to perform counterfactual reasoning\". However, this point is not clear in the following section. Could you please explain it in detail?\n\nWe have revised this sentence to clarify what we mean by counterfactual reasoning (off-policy reasoning): (a) successfully stitching together pieces of different trajectories in an offline dataset to find a path between unseen state and goal pairs; (b) Finding a path that is shorter than the average paths demonstrated in the dataset. These two capabilities are demonstrated by our experiments in Sec. 4.4 and Appendix D.4.\n\n> In Algorithm 1, many notations are introduced for the first time without any definition. Could you please clarify them?\n\nWe have added notations in Algorithm 1 and defer details of the full goal-conditioned TD InfoNCE algorithm to Appendix D.1 due to limited pages."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700370320172,
                "cdate": 1700370320172,
                "tmdate": 1700370320172,
                "mdate": 1700370320172,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Rj5iTfeRRv",
                "forum": "0akLDTFR9x",
                "replyto": "lJwlZm0trs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7601/Reviewer_Jmo8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7601/Reviewer_Jmo8"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your detailed response"
                    },
                    "comment": {
                        "value": "Thank authors for the detailed response. I especially appreciate the additional experiment results, which help mitigate my concerns. I'd keep leaning toward acceptance."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514866114,
                "cdate": 1700514866114,
                "tmdate": 1700514866114,
                "mdate": 1700514866114,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Tn58GxBzxM",
            "forum": "0akLDTFR9x",
            "replyto": "0akLDTFR9x",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7601/Reviewer_a2XQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7601/Reviewer_a2XQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends previous works on contrastive RL by using a temporal difference estimator. Similar to contrastive RL, the paper proposes to use contrastive learning, InfoNCE in particular, for estimating the discounted state occupancy measure. Unlike the previous contrastive RL approach, which averages over the goal distribution, the proposed method uses a temporal difference estimator and results in a Bellman-like update rule (TDInfoNCE). Comparing the Bellman update for value function, TDInfoNCE requires taking expectation over the future states from a potentially different goal. It turns out that this can be estimated using importance weight. Then, the paper shows how the estimated state occupancy measure can be used in conjuction with goal-conditioned RL to form a full-fledged RL agent. Experimental results are shown on both online and offline settings and compared to a range of existing methods. In both settings, TDInfoNCE outperforms in most of the environments compared to previous approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is mostly well written, apart from some details (see questions section). \n\nThe derivations are sound. \n\nExperimental results show strong performance comparing to previous methods. The paper also presents some analysis and insights to explain the performance."
                },
                "weaknesses": {
                    "value": "The novelty is slightly limited. The idea of using InfoNCE to estimate the state occupancy measure has been presented in contrastive RL; the Bellman-like update and the use of importance weight has been presented in C-Learning."
                },
                "questions": {
                    "value": "1. Questions regarding the algorithm:\n - It's not clear to me what's the goal distribution is used? Is it a random goal? Does the different goal distribution affect data efficiency?\n- How is $s_{t+}$ sampled? Is it the same as previous approaches - sample t from a geometric distribution?\n\n2. The notation in Figure 1 is not very clear to me. Is it suppose to visualize Eq. 4?\n\n3. In Eq. 7 and the one above, shoud it be $p^{\\pi}(s_{t+}^{(1)}|s^{'}, a^{'}, g)$ instead of  $p^{\\pi}(s_{t+}^{(1)}|s^{'}, a^{'})$?\n\n4. In Table 1, result for contrastive RL on large-diverse-v2 should not be bold; result for TDInfoNCE on unmaze-v2 should not be bold."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7601/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7601/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7601/Reviewer_a2XQ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7601/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766442062,
            "cdate": 1698766442062,
            "tmdate": 1699636921709,
            "mdate": 1699636921709,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oHWa3JwYI0",
                "forum": "0akLDTFR9x",
                "replyto": "Tn58GxBzxM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed response and helpful suggestions for improving the paper. It seems that the reviewer\u2019s main question is about the novelty of the paper. Our work does build upon prior work of contrastive RL and C-Learning. Contrastive RL is an on-policy Monte Carlo method, while our proposed method is a off-policy temporal difference version of it, achieving higher sample efficiency. C-learning can be interpreted as a TD version of a NCE (2-class) contrastive loss, while our proposed method is a TD version of the infoNCE (many-class) contrastive loss. We have clarified this difference in the final paragraph of Sec. 3.1. Empirically, TD InfoNCE is $1500 \\times$ more sample efficient than its Monte Carlo counterpart ($6.5 \u00d7 10^3$ vs $10^7$ transitions) and achieves a comparable loss with $130\\times$ less data than C-Learning ($7.7 \u00d7 10^4$ vs $10^7$ transitions) (See Fig. 3 (Right)).  **Together with the discussion below, does this address the reviewer\u2019s concerns?**\n\n> It's not clear to me what's the goal distribution is used? Is it a random goal? Does the different goal distribution affect data efficiency?\n\nWe assume that the goal space is the same as the state space and do not constraint the distribution of goal. In practice, we usually define goals using human preference. For example, in task $\\text{pick \\\\& place}$, we sample the target position of the robot arm to be either in the air or on the table with the block in the gripper.\n\n> How is s_{t+} sampled? Is it the same as previous approaches - sample t from a geometric distribution?\n\nOur TD InfoNCE is an off-policy algorithm, indicating that it does not require sample future states from an on-policy geometric distribution. Instead, we sample $s_{t+}$ randomly from the replay buffer or the offline dataset. See Appendix D.1 and Algorithm 1 for details.\n\n> The notation in Figure 1 is not very clear to me. Is it suppose to visualize Eq. 4?\n\nThanks for the suggestion! Figure 1 is supposed to visualize a sample-based version of Eq. 4 and our theoretical analysis in Appendix A also indicates that TD InfoNCE builds upon a Bellman equation similar to the one in this figure. The color of the edges should correspond to different terms in Eq. 4. We have updated Eq. 4 and Fig. 1 to make the notations consistent.\n\n> In Eq. 7 and the one above, should it be $p^{\\pi}(s_{t+}^{(1)} \\mid s\u2019, a\u2019, g)$ instead of $p^{\\pi}(s_{t+}^{(1)} | s\u2019, a\u2019)$?\n\nThanks for finding the typo. We have updated the notation in Eq. 8.\n\n> In Table 1, the result for contrastive RL on large-diverse-v2 should not be bold; result for TDInfoNCE on unmaze-v2 should not be bold.\n\nThanks for the suggestion. In Table 1, we bold the success rates within one standard deviation of the best methods. In this case, the result for contrastive RL on $\\texttt{large-diverse-v2}$ should be bold and the result for TD InfoNCE on $\\texttt{unmaze-v2}$ should not be bold. We have revised the table."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700370153297,
                "cdate": 1700370153297,
                "tmdate": 1700370153297,
                "mdate": 1700370153297,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wCjeB1iXF7",
                "forum": "0akLDTFR9x",
                "replyto": "oHWa3JwYI0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7601/Reviewer_a2XQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7601/Reviewer_a2XQ"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the reply"
                    },
                    "comment": {
                        "value": "I would like to thank the author for the response. The updates did make the paper clearer. I would maintaining my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733879786,
                "cdate": 1700733879786,
                "tmdate": 1700733879786,
                "mdate": 1700733879786,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cIhT3oTCV1",
            "forum": "0akLDTFR9x",
            "replyto": "0akLDTFR9x",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7601/Reviewer_2Mg8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7601/Reviewer_2Mg8"
            ],
            "content": {
                "summary": {
                    "value": "This paper derives a TD variant of the InfoNCE objective function, relating this to some of the work in reinforcement learning using future distributions of state as an objective (i.e. successor representations/features). This algorithm is then applied to goal-conditioned reinforcement learning, showing competitive performance among several baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The derived method fits nicely within the literature and seems to fill a nice gap between contrastive objectives from self-supervised objectives and more online focused temporal-difference updates."
                },
                "weaknesses": {
                    "value": "After the rebuttal, I'm raising my score. While I believe there are issues with empirical section still, these are issues the rest of the literature are also facing. I don't think rejecting this paper is a way to a solution. I also appreciate the fix to some of the inaccurate statements that were overlooked! \n\nGreat job authors!\n\n--------before edit-------\n\nThis paper struggles with clarity and accuracy in some of the ancillary statements made about the literature surrounding the paper and in the main content of the paper itself. There are also several issues with the experimental section that should be addressed.\n\n## Accuracy and Clarity:\n1. On page 1, in the paragraph starting with \u201cthe key aim\u2026\u201d, the language on motivating why a TD version of InfoNCE is oddly phrased. I think a fix should be easy here by removing phrases such as \u201cmay allow\u201d, \u201cmay enable\u201d and be more actionable in your language. This could also be replaced by actual hypotheses of what you expect to see from your objective and stated more formally.\n2. In the same paragraph, and further throughout, you make a statement which suggests TD can do counter factual reasoning (or in the parlance of RL make use of off-policy updates) while Monte Carlo estimates cannot.  This is not true as Monte Carlo estimates can be made with off-policy corrections (using Importance Sampling like in TD). While this induces a more variant estimate as compared to TD (there is a classic bias-variance trade-off between MC and TD updates) and a TD update enables the use of incomplete trajectories (because you are using an estimate to inform your update rather than a full trajectory), I came out of the paper with the feeling the paper was suggesting Monte Carlo estimates couldn\u2019t be off-policy. \n    - This comes up very strongly on page 5 (the first paragraph) \u201cThis is, we cannot share\u2026\u201d. We should be able to derive an off-policy version of the monte-carlo update for InfoNCE. This doesn\u2019t mean it would be an estimator we would want to use in this setting, but it should be definable. If this is not the case, the paper should show that this can\u2019t be done using importance sampling or cite a reference which shows monte carlo estimates can\u2019t be off-policy.\n\n3. **Notation clarity issues:** \n   - In your expectations above equation 7, I\u2019m not sure what s\u2019, a\u2019 are here. Are you using these instead of s_{t+1} as used in equation 4? This notation should be unified. \n   - Equation (1) and following uses, I\u2019m not sure you explain what the superscript is signifying. I think it is time, but it is not clear from the writing.\n   - Shouldn\u2019t the expectation in the middle of page 4 on the RHS (i.e. after applying the importance weight) be selecting actions a\u2019 from the behavior policy? Or is the importance weight only correcting for the state distribution? Shouldn\u2019t we also correct for the action distribution as well?\n\n\n\n## Empirical Results:\n\nThere are two major issues with the empirical results as presented. \n\n### Major issues:\n\n4. 3 seeds is too few to get any statistical confidence, especially without doing independent hyperparameter sweeps for each baseline. While in the past this has been standard, as a field we continually have shown that the statistical power of our experiments are laughably poor, even if the bounds of our results show statistical significance. This continually misleads the community, and needs to be addressed. This doesn\u2019t include the issue with not running hyperparameter studies on the methods independently. See https://sites.ualberta.ca/~amw8/cookbook.pdf for a reference. While this paper is a draft it does a good job going through these issues and providing context from the literature.\n\n5. In the appendix it is mentioned \u201cWe increase the capacity of the goal-conditioned state-action encoder\u2026\u201d. This suggests the model you are using may have more parameters than your counterparts. Is this true? Also did you use a larger batch size for all baselines or just your algorithm? If this was done for your method, this makes the results difficult to trust. If not, it likely means the hyperparameters of your baselines are now invalid. If both methods have the same hyperparameters then I\u2019m usually ok with re-using old hyperparameter studies, unfortunately when any of the hypers change OR the new method has additional hypers this begins to weaken the validity of re-using the same hypers.\n\n### Minor Issues:\n- You should include all hyperparameters you used, even for baselines. \n- You include only success rate as a metric to compare. While this is reasonable, I think there is a lot that could be learned from more traditional metrics (i.e. return or something similar). This is especially the case when the success metric doesn\u2019t clearly separate the methods (for instance reach, Push, slide (state) in firgure 2a). \n\n\n\n# Edits/Suggestions \n- I don\u2019t like the notation $s_{t+}$. I think it could be replaced with something that is more understandable on first read (and looks less like a mistake). This is a preference though.\n- Page 4: \u201cOur proposed method (Sec 3)\u2026\u201d Should say Sec 3.2.\n- Page 4: \u201cwe conjecture that our\u2026\u201d -> You should state that you test this in the empirical section (I think you do at least).\n- Page 7: \u201cWe will to evaluate\u201d -> \u201cwe will evaluate\u201d\n- If you reference a result in the main paper you should include this in the main paper. \u201cTD InfoNCE achieves a 2x median improvement etc\u2026\u201d references figure 6 (I believe)."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7601/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7601/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7601/Reviewer_2Mg8"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7601/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789364159,
            "cdate": 1698789364159,
            "tmdate": 1700586181515,
            "mdate": 1700586181515,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SWkbxHY6b0",
                "forum": "0akLDTFR9x",
                "replyto": "cIhT3oTCV1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors - Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed response and for the suggestions for improving the paper. To address the concerns about the experiments, we have run additional random seeds and clarified that the baselines mostly use the same number of parameters and batch size as our method. We have also revised the paper to address concerns about clarity. **Together with the discussion for other questions below, does this fully address the reviewer\u2019s concerns about the paper?**\n\n> This doesn\u2019t include the issue with not running hyperparameter studies on the methods independently.\n\nAs suggested by the reviewer, we ran additional experiments to study the effect of different hyperparameters for TD InfoNCE in table 2. For each hyperparameter, we selected a set of different values and conducted experiments on $\\texttt{push (state)}$ and $\\texttt{slide (state)}$, one easy task and one challenging task, for five random seeds. We report mean and standard deviation of success rates in Appendix E.1. These results suggest that while some values of the hyperparameter have similar effect, e.g. actor learning rate = $5 \\times 10^{-5}$ vs $1 \\times 10^{-4}$, our choice of combination is optimal for TD InfoNCE.\n\n> This suggests the model you are using may have more parameters than your counterparts. Is this true?\n\nFor fair comparisons, we also increased the size of neural networks in baselines to the same number whenever possible in the original submission. We have added a sentence in Appendix D.5 to clarify. We list the number of parameters for each method evaluated on online GCRL benchmarks in the table below.\n\n| | TD InfoNCE | QRL | Contrastive RL | Contrastive RL (NCE) | GCBC | DDPG + HER|\n|:---------------------------:|:---------------------:|:-----------------:|:---------------------:|:---------------------:|:---------------------:|:---------------------:|\n|   number of parameters    |    $1,629,184$   | $2,147,328$ | $1,629,184$ | $1,629,184$ | $1607680$ | $1,629,022$ |\n\nAs shown in the table, TD InfoNCE mostly uses the number of parameters less than or equal to baselines. Note that QRL contains a feedforward encoder $f$, a residual latent transition $T$, and a projector $d_{\\theta}$ in its critic (Appendix C.3 of [2]). Even if we set the size of $f$ and $T$ to be the same as the networks in TD InfoNCE, the number of parameters in QRL is still larger than that of TD InfoNCE. \n\n> Also did you use a larger batch size for all baselines or just your algorithm?\n\nAll baselines used the same batch size as our method: 256. We have added this detail to Appendix D.5.\n\n> 3 seeds is too few to get any statistical confidence \u2026\n\nAs suggested, we have repeated experiments in Fig. 2 (Appendix Fig. 6) and Table 1 with two more random seeds (five in total) to increase the statistical confidence of our experiments. We report results on the Fetch robotics benchmark and D4RL benchmark in the revised paper. These results suggest that TD InfoNCE is stable across different random seeds and our conclusions in Sec. 4.1 and Sec. 4.2 remain the same.\n\n> This is not true as Monte Carlo estimates can be made with off-policy corrections (using Importance Sampling like in TD). \n\nThanks for catching this. We agree that Monte Carlo estimates can be used in an off-policy manner by applying importance weights to correct actions. We have revised the text in the final paragraph of Sec. 3.1 to note that MC methods with importance weighting could in theory perform off-policy evaluation (specifically, off-policy contrastive RL in our problem), while citing prior work that shows that off-policy evaluation based on importance weights tends to be unstable [5, 6].\n\n> \u2026 the language on motivating why a TD version of InfoNCE is oddly phrased.\n\nWe have revised this sentence in the introduction. \n\n> In your expectations above equation 7, I\u2019m not sure what s\u2019, a\u2019 are here. Are you using these instead of s_{t+1} as used in equation 4? This notation should be unified.\n\n$s\u2019$ is the next state sampled from the environment while $a\u2019$ is the corresponding action sampled from the goal-conditioned policy given $s\u2019$. We have revised equation 4 to make the notation consistent.\n\n> Equation (1) and following uses, I\u2019m not sure you explain what the superscript is signifying. I think it is time, but it is not clear from the writing.\n\nIn equation 1, we sample one \"positive\" $y$ (denoted $y^{(1)}$) and $N - 1$ \"negative\" $y$s (denoted $y^{(2:N)} = \\\\{y^{(2)}, \\cdots, y^{(N)} \\\\}$). We follow the notational convention of prior work [1, 3, 4]. We have clarified this detail in the sentence before equation 1."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700369440149,
                "cdate": 1700369440149,
                "tmdate": 1700369440149,
                "mdate": 1700369440149,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZecfYeOpfn",
                "forum": "0akLDTFR9x",
                "replyto": "cIhT3oTCV1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7601/Reviewer_2Mg8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7601/Reviewer_2Mg8"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response! I think you have done a great job addressing my concerns! I think you have clarified my questions and concerns. While the empirical section is still not up to the standards I would hope, this is more inline with the literature now.\n\nGreat job!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586029387,
                "cdate": 1700586029387,
                "tmdate": 1700586194723,
                "mdate": 1700586194723,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7ITt49PRjA",
            "forum": "0akLDTFR9x",
            "replyto": "0akLDTFR9x",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7601/Reviewer_yDjK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7601/Reviewer_yDjK"
            ],
            "content": {
                "summary": {
                    "value": "Predicting future states is crucial for many time-series tasks, including goal-conditioned reinforcement learning (RL). While contrastive predictive coding (CPC) has been used to model time series data, learning representations that capture long-term dependencies often requires large datasets. This paper introduces a temporal difference (TD) version of CPC that combines segments of different time series, reducing the data needed to predict future events. This representation learning method is applied to derive an off-policy algorithm for goal-conditioned RL. Experiments show that the proposed method achieves higher success rates with less data and better handles stochastic environments compared to previous RL methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper proposes a new temporal difference (TD) estimator for the InfoNCE loss, which is shown to be more efficient than the standard (Monte Carlo) estimator.\n\n- The proposed goal-conditioned reinforcement learning (RL) algorithm outperforms prior methods in both online and offline settings.\n\n- The proposed algorithm is capable of handling stochasticity in the environment dynamics.\n\n- In stochastic tasks, there is an excellent improvement in performance versus the baseline of Quasimetric RL, with some healthy gains on non stochastic tasks versus other baselines, although this is not the primary target of the paper.\n\n- The paper provides a clear and concise explanation of the proposed algorithm.\n\n- The paper is well-written and easy to understand.\n\n- The paper is well-supported by experiments.\n\n- The proposed algorithm is evaluated on a variety of tasks, including the Fetch robotics benchmark.\n\n- TD InfoNCE learns on image-based pick & place and slide, while baselines fail to make any progress.\n\n- TD InfoNCE maintains high success rates on more challenging tasks with observation corruption, while the performance of QRL decreases significantly."
                },
                "weaknesses": {
                    "value": "- The paper focuses on fairly trivial environments, it would be nice to see these methods working on more challenging higher dimensional goal conditioned RL tasks, as its not a given that these gains will carry over to tasks that matter a lot more.\n\n- The proposed TD estimator is more complex than the standard (Monte Carlo) estimator and its implementation requires more hyperparameters.\n\n- The performance of the proposed goal-conditioned RL algorithm on the most challenging tasks was less than 50%.\n\n- QRL assumes deterministic dynamic of the environment, while TD InfoNCE learns without such assumption.\n\nLoss Function Composition: The loss function L(\u03b8) is composed of two cross-entropy (CE) loss terms, one for predicting the next state and one for predicting the future distribution of states. The \u03b3 hyperparameter is used to weight these two terms, but the choice of \u03b3 and its impact on the algorithm's performance are not discussed in detail."
                },
                "questions": {
                    "value": "Can you explain how you selected the hyperparameters for the proposed algorithm?\n\nCan you provide more details about the observation that TD InfoNCE learns on image-based pick & place and slide, while baselines fail to make any progress?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7601/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699530774902,
            "cdate": 1699530774902,
            "tmdate": 1699636921449,
            "mdate": 1699636921449,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sYCQEd1Zlv",
                "forum": "0akLDTFR9x",
                "replyto": "7ITt49PRjA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the responses and suggestions for improving the paper. The reviewer brought up two questions regarding the experiments: selection of hyperparameters and explanation of an observation on challenging image-based tasks. We have attempted to address the question about hyperparameter selection by running ablation experiments, and we answer the question about the performance of TD InfoNCE below. **Together with the discussion below, does this fully address the reviewer\u2019s questions?**\n\n> Can you explain how you selected the hyperparameters for the proposed algorithm?\n\nWe ran ablation experiments to find the best hyperparameters for TD InfoNCE (new Appendix E.1). Since our algorithm shares the same set of hyperparameters with contrastive RL, we started with the default hyperparameters from this prior work. Among those hyperparameters we chose to sweep over actor learning rate, critic learning rate, representation normalization, MLP hidden layer sizes, and representation dimensions in a range of different values (see new Appendix Fig. 10). These results suggest that while some values of the hyperparameter have similar effect, e.g. actor learning rate = $5 \\times 10^{-5}$ vs $1 \\times 10^{-4}$, our choice of combination is optimal for TD InfoNCE.\n\n> The \u03b3 hyperparameter is used to weight these two terms, but the choice of \u03b3 and its impact on the algorithm's performance are not discussed in detail.\n\nThe $\\gamma \\in (0, 1]$ is the discount factor of the underlying Markov decision process. We ran ablation experiments with $\\gamma \\in \\\\{0.90, 0.95, 0.99\\\\}$. Results in Appendix Fig. 10 suggests that a larger discount factor achieves a higher success rate, which can be explained by focusing more on (higher weight) correctly predicting which states are likely future states (Eq. 10).\n\n> Can you provide more details about the observation that TD InfoNCE learns on image-based pick & place and slide, while baselines fail to make any progress?\n\nThe image-based $\\texttt{pick \\\\& place}$ and $\\texttt{slide}$ tasks are the most challenging tasks we consider \u2013 not only do they involve manipulating other objects, but also require learning such behavior from high-dimensional sensory input (i.e., images). We believe that TD InfoNCE outperforms prior methods on this task because it is able to make better use of limited data (e.g., through the temporal difference updates).\n\n> The paper focuses on fairly trivial environments, it would be nice to see these methods working on more challenging higher dimensional goal conditioned RL tasks \u2026\n\nOur current set of experiments on 14 tasks (8 on the Fetch robotics benchmark, 6 on the D4RL benchmark) includes tasks with 12288-dimensional (64 x 64 x 3) image observations. Prior methods struggle on at least some of these tasks (\\texttt{pick & place (image)} and \\texttt{slide (image)}), suggesting that these tasks are at least challenging for prior methods. While this breadth of tasks at least matches that of prior work [1, 2], we would be happy to investigate additional challenging tasks if suggested by the reviewer.\n\n> QRL assumes deterministic dynamics of the environment, while TD InfoNCE learns without such assumption.\n\nWe believe this is a strength of TD InfoNCE, rather than a weakness. In general, making assumptions about the dynamic of the environment narrows the domain where an RL algorithm might be applicable. Fig. 2b shows that TD InfoNCE can solve some tasks with stochastic dynamics while QRL suffers from a significant drop in performance compared to its result on the deterministic version. We suspect that TD InfoNCE can be applied to a wider class of goal-conditioned RL problems.\n\n[1] Eysenbach, B., Zhang, T., Levine, S. and Salakhutdinov, R.R., 2022. Contrastive learning as goal-conditioned reinforcement learning. Advances in Neural Information Processing Systems, 35, pp.35603-35620.\n\n[2] Wang, T., Torralba, A., Isola, P. and Zhang, A., 2023. Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning. arXiv preprint arXiv:2304.01203."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700369111488,
                "cdate": 1700369111488,
                "tmdate": 1700369111488,
                "mdate": 1700369111488,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]