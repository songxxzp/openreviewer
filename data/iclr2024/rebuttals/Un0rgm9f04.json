[
    {
        "title": "VDT: General-purpose Video Diffusion Transformers via Mask Modeling"
    },
    {
        "review": {
            "id": "Q8jIBJ59lY",
            "forum": "Un0rgm9f04",
            "replyto": "Un0rgm9f04",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission710/Reviewer_mnca"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission710/Reviewer_mnca"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a video diffusion model based on the Transformer architecture, a departure from the commonly used U-Net structure in previous Video Diffusion Models (VDM). The utilization of the Transformer model for this task is a significant contribution, especially given its novelty in the field. Additionally, the incorporation of a mask-based modeling mechanism extends the applicability of the model to a wider range of tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Pioneering exploration of the Transformer architecture for video diffusion models, marking a valuable starting point.\n- Validation of the effectiveness of the proposed architecture and methods across multiple task datasets.\n- Achieving state-of-the-art results on several mainstream benchmarks."
                },
                "weaknesses": {
                    "value": "- While the authors conducted validations on various tasks, the ablation experiments in Table 2 are insufficient. Verification in unconditional video generation tasks would enhance the persuasiveness of the study.\n- The method is limited to lower resolutions, diminishing its practical applicability.\n- I believe it would be valuable to compare training and inference times. Understanding the computational costs associated with both training and real-time usage is crucial for assessing the feasibility of the approach in real-world scenarios. It would enhance the completeness of the evaluation and provide readers with a comprehensive understanding of the method's efficiency. Therefore, I recommend including a comparison of training and inference times in the manuscript to strengthen the overall analysis."
                },
                "questions": {
                    "value": "- I previously reviewed an earlier version of this manuscript, and there have been notable improvements, particularly in clarifying the significance of the Transformer in the introduction, which I find valuable. \n- I am still curious about why the patch-based method is limited to such low resolutions. Even at 256 resolution, the token length is not excessively long, and it could potentially be experimented with. Is this limitation due to computational constraints?\n- Additionally, it would be beneficial to compare the proposed method with more recent publications. A comparison with the results summarized in [a] could be enlightening.\n- In conclusion, I find the innovation in this paper substantial, and it presents a compelling argument. For now, I am inclined to recommend acceptance.\n\n[a] Xing, Zhen, et al. \"A Survey on Video Diffusion Models.\" arXiv preprint arXiv:2310.10647 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission710/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698406897252,
            "cdate": 1698406897252,
            "tmdate": 1699635998199,
            "mdate": 1699635998199,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7rMXgqxLXW",
                "forum": "Un0rgm9f04",
                "replyto": "Q8jIBJ59lY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mnca (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for the positive comments and insightful suggestions. Your insightful questions and valuable suggestions have been immensely helpful in enhancing the paper's quality. \n\n**Q1: While the authors conducted validations on various tasks, the ablation experiments in Table 2 are insufficient. Verification in unconditional video generation tasks would enhance the persuasiveness of the study.**\n\n**A:** Thank you for the suggestion! We have conducted a detailed ablation study on UCF101 using DiT-S. The results show that reducing the Patchsize, increasing the number of Layers, and increasing the Hidden Size all further improve the model's performance. The position of Temporal and Spatial attention and the number of attention heads do not significantly affect the model's results. Further comprehensive analysis indicates that generally, **an increase in GFlops leads to better results, demonstrating the scalability of VDT**. When maintaining the same GFlops, some trade-offs in design are necessary, but overall, the model's performance does not differ significantly. And we have included these experiments in the Appendix A of our paper.\n\n| Patch Size, Iteration=40k| GFlops | UCF101 FVD |\n|------------|------------|------------|\n| 4    |   1.9   | 643.9      |\n| 2    |   7.7   | 554.8      |\n| 1    |   30.9   | **466.2**      |\n\n\n| Head, Iteration=40k| GFlops | UCF101 FVD |\n|------------|------------|------------|\n| 3      |   7.7      | 559.8      |\n| 6      |   7.7      | **554.8**      |\n| 12     |   7.7       | 598.7      |\n\n| Depth, Iteration=40k| GFlops | UCF101 FVD |\n|------------|------------|------------|\n| 6       |   3.9     | 580.7      |\n| 12      |   7.7      | 554.8      |\n| 18      |   11.6     | **500.6**      |\n\n\n| Hidden Size, Iteration=40k| GFlops | UCF101 FVD |\n|------------|------------|------------|\n| 192     |   1.9         | 704.2    |\n| 384     |   7.7         | 554.8      |\n| 768     |   30.9         | **464.2**      |\n\n\n| Architecture, Iteration=40k| GFlops | UCF101 FVD |\n|------------|------------|------------|\n| Spatial First   |   7.7         | 550.2      |\n| Temporal First   |   7.7         | 554.8      |\n\n\n**Q2: The method is limited to lower resolutions, diminishing its practical applicability. I am still curious about why the patch-based method is limited to such low resolutions. Even at 256 resolution, the token length is not excessively long, and it could potentially be experimented with. Is this limitation due to computational constraints?**\n\n\n**A:** Thank you for your question regarding the resolution constraints of our patch-based method. We understand your curiosity, and the limitations are indeed primarily due to data availability and computational resources:\n\n1. **Data Availability:** The development of high-resolution models (e.g., Align Your Latents) often require access to large and proprietary datasets, which are not publicly available now. Unfortunately, there is a notable scarcity of high-quality, high-resolution video datasets in the public domain. This lack of available data poses a significant challenge in training and testing our method at higher resolutions.\n\n2. **Computational Constraints:** Addressing the computational aspect, for a video with a resolution of 16x256x256, post VAE encoding, the token length reaches approximately 16,384 (16x32x32). If we were to scale up to a resolution of 16x512x512, the token length would increase to around 65,536 (16x64x64). While our computational resources might be capable of handling this increased scale, it would considerably impact our Batchsize. This factor is crucial for the efficiency and practicality of diffusion models. It's noteworthy that even recent high-resolution works, such as 'Imagen Video' and 'Align Your Latents', still rely heavily on super-resolution (SR) models to manage the computational demands associated with higher resolutions.\n\nThese factors combine to create a practical ceiling on the resolution we can effectively work with, given current public data availability and our computational resources. While we acknowledge this limitation, we believe our approach still offers valuable insights and applications within its operational resolution range."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228951774,
                "cdate": 1700228951774,
                "tmdate": 1700233965899,
                "mdate": 1700233965899,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QWRciDvUIM",
                "forum": "Un0rgm9f04",
                "replyto": "Q8jIBJ59lY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mnca (Part 2)"
                    },
                    "comment": {
                        "value": "**Q3: I believe it would be valuable to compare training and inference times. Understanding the computational costs associated with both training and real-time usage is crucial for assessing the feasibility of the approach in real-world scenarios. It would enhance the completeness of the evaluation and provide readers with a comprehensive understanding of the method's efficiency. Therefore, I recommend including a comparison of training and inference times in the manuscript to strengthen the overall analysis.**\n\n**A:** Good suggestion! We have therefore included a comparison of training and inference times (per second), all experiments are conducted on NVIDIA A100 GPU. And we have included these experiments in the Appendix B of our paper.\n\n|   | Resolution | VAE (per sample, bs=1) | Training (per sample, bs=1) | Inference (per sample, t=256, bs=1) | \n|---|------------|------------------------------|-----------------------------|-----------------------------------|\n| VDT-S | 16x64x64      | 0.0042                | 0.022                       | 1.10                                      |\n| VDT-L | 16x64x64      | 0.0042                | 0.051                       | 2.57                              |        \n| VDT-S | 16x128x128        | 0.0051                | 0.024                       | 1.21                              |        \n| VDT-L | 16x128x128        | 0.0051                | 0.057                       | 6.3                               |        \n| VDT-S | 16x256x256        | 0.0058                | 0.026                       | 2.63                              |        \n| VDT-L | 16x256x256        | 0.0058                | 0.111                       | 25.31                             |        \n\n**Q4: Additionally, it would be beneficial to compare the proposed method with more recent publications. A comparison with the results summarized in [a] could be enlightening.**\n\n**A:** Thank you for the recommendation! we have updated our manuscript to include these comparisons. Should there be any specific recent publications that we may have overlooked or that you believe should be included in our manuscript or any other questions, please feel free to let us know.\n\n\nThanks again for your time and effort!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229011329,
                "cdate": 1700229011329,
                "tmdate": 1700274190689,
                "mdate": 1700274190689,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gfziYWU6uG",
                "forum": "Un0rgm9f04",
                "replyto": "Q8jIBJ59lY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your post-rebuttal feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer mnca,\n\nThanks again for your constructive suggestions, which have helped us improve the quality and clarity of the paper!\n\nIn our previous response, we have carefully studied your comments and made detailed responses. As the discussion period will end soon in 1 days, we are happy to provide any additional clarifications that you may need. Please do not hesitate to contact us if there are other clarifications we can offer. We appreciate your suggestions.\n\nThanks for your time and efforts!\n\nWarm regards, \\\nAuthors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621317084,
                "cdate": 1700621317084,
                "tmdate": 1700621317084,
                "mdate": 1700621317084,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J4XZpP5qcd",
                "forum": "Un0rgm9f04",
                "replyto": "gfziYWU6uG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission710/Reviewer_mnca"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission710/Reviewer_mnca"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply, most of my concerns solved, I would like to keep the positive score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720332222,
                "cdate": 1700720332222,
                "tmdate": 1700720332222,
                "mdate": 1700720332222,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ko3cniob3w",
                "forum": "Un0rgm9f04",
                "replyto": "Q8jIBJ59lY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer mnca,\n\nThanks again for spending a huge amount of time on our paper, which has helped us improve the quality and clarity of the paper! We are glad to see that our response has addressed most of your concerns.\n\nThanks for your time and efforts again!\n\nBest, \\\nAuthors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727994054,
                "cdate": 1700727994054,
                "tmdate": 1700728013452,
                "mdate": 1700728013452,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Avh9COtk2R",
            "forum": "Un0rgm9f04",
            "replyto": "Un0rgm9f04",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission710/Reviewer_cvvZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission710/Reviewer_cvvZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Video Diffusion Transformer (VDT), a video generation model with pure transformer architecture.\nThe core idea is to replace the unet structure of the existing video generation model with a pure transformer structure.\nThe authors claimed that it's the first successful model in transformer-based video diffusion.\nThey propose a unified spatial-temporal mask modeling mechanism for VDT, enabling it to unify a diverse array of general-purpose tasks.\nThey show the effectiveness of VDT on several video-related tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Compared with the current U-net-based network with demonic attention structures, VDT is a pure transformer network. If it is indeed the first Transformer-only video diffusion network, it would be a good baseline for this area.\n2. This paper further proposes the spatiotemporal mask mechanism, which can make VDT adapt to various video tasks, including video generation, video prediction, image-to-video generation, video completion, etc.\n3. This paper verifies the effectiveness of VDT on multiple video generation tasks.\n4. The code in this article is open source and will help others replicate this work."
                },
                "weaknesses": {
                    "value": "1. The core idea of this paper is simple and reasonable. Replacing the original CNN with transformer is an effective method that has been widely verified in the whole field. In addition, this paper adopts the basic attention mechanism, and the mask strategy proposed is also commonly used in video tasks. So, from this perspective, the contribution is slightly less obvious.\n2. I feel that the work in this paper can be a good benchmark in this field. However, this paper only verifies that it can be effective on multiple video tasks. As a good benchmark, this paper should give more detailed experimental analysis of ablation. For example, why does space-time pay attention to time before space? In this paper, the influence of each module on the video generation effect, the comparison experiment of hyperparameter and so on.\n3. Tab.5, 142.3 (blacken) is not the best one."
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission710/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698588711926,
            "cdate": 1698588711926,
            "tmdate": 1699635998096,
            "mdate": 1699635998096,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Y0RNjx127b",
                "forum": "Un0rgm9f04",
                "replyto": "Avh9COtk2R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cvvZ"
                    },
                    "comment": {
                        "value": "Thank you for the positive comments and insightful suggestions. Your insightful questions and valuable suggestions have been immensely helpful in enhancing the paper's quality. \n\n**Q1: The core idea of this paper is simple and reasonable. Replacing the original CNN with transformer is an effective method that has been widely verified in the whole field. In addition, this paper adopts the basic attention mechanism, and the mask strategy proposed is also commonly used in video tasks. So, from this perspective, the contribution is slightly less obvious.**\n\n**A:** Thank you for your comments on the core idea of our paper. We appreciate your perspective regarding the simplicity and effectiveness of our VDT design. \n\n*  We concur that our Video Diffusion Transformer (VDT) design, while simple, is influenced by established methodologies in the field. Nevertheless, we firmly believe that the essence of innovation often lies in simplicity and effectiveness. Our aim with VDT was not to overcomplicate the design merely for the appearance of novelty, but rather to focus on functional efficiency and adaptability. VDT marks a pioneering effort in applying transformer structures specifically for video diffusion tasks, which, despite its conceptual simplicity, represents a significant and non-trivial leap in this domain.\n\n*  Meanwhile, one of the key strengths of VDT is its ability to unify a broad spectrum of video generation tasks within a single, scalable model, without the need for specialized or task-specific designs. This aspect of VDT, which goes beyond the complexity of its structure, is one import contribution - offering a versatile and robust framework for general purpose video generation tasks.\n\n*  Furthermore, VDT's token-concat approach for conditioning can be seen as  coarse-grained training strategy for autoregressive models. Given the proven success of autoregressive approaches in other domains, VDT's methodology could potentially spearhead significant advancements in large-scale computer vision models.\n\nWe agree that the structural design of VDT might not appear novel in isolation. However, the application of this concept in the realm of video diffusion is innovative. The value of our work lies not only in its design but also in its application, scalability, and the potential it offers for video generation using transformer architectures.\n\n**Q2: I feel that the work in this paper can be a good benchmark in this field. However, this paper only verifies that it can be effective on multiple video tasks. As a good benchmark, this paper should give more detailed experimental analysis of ablation. For example, why does space-time pay attention to time before space? In this paper, the influence of each module on the video generation effect, the comparison experiment of hyperparameter and so on.**\n\n**A:** Thank you for the suggestion! We have conducted a detailed ablation study on UCF101 using DiT-S. The results show that reducing the Patchsize, increasing the number of Layers, and increasing the Hidden Size all further improve the model's performance. The position of Temporal and Spatial attention and the number of attention heads do not significantly affect the model's results. Further comprehensive analysis indicates that generally, **an increase in GFlops leads to better results, demonstrating the scalability of VDT**. When maintaining the same GFlops, some trade-offs in design are necessary, but overall, the model's performance does not differ significantly. And we have included these experiments in the Appendix A of our paper.\n\n| Patch Size, Iteration=40k| GFlops | UCF101 FVD |\n|------------|------------|------------|\n| 4    |   1.9   | 643.9      |\n| 2    |   7.7   | 554.8      |\n| 1    |   30.9   | **466.2**      |\n\n\n| Head, Iteration=40k| GFlops | UCF101 FVD |\n|------------|------------|------------|\n| 3      |   7.7      | 559.8      |\n| 6      |   7.7      | **554.8**      |\n| 12     |   7.7       | 598.7      |\n\n| Layer, Iteration=40k| GFlops | UCF101 FVD |\n|------------|------------|------------|\n| 6       |   3.9     | 580.7      |\n| 12      |   7.7      | 554.8      |\n| 18      |   11.6     | **500.6**      |\n\n\n| Hidden Size, Iteration=40k| GFlops | UCF101 FVD |\n|------------|------------|------------|\n| 192     |   1.9         | 704.2    |\n| 384     |   7.7         | 554.8      |\n| 768     |   30.9         | **464.2**      |\n\n\n| Architecture, Iteration=40k| GFlops | UCF101 FVD |\n|------------|------------|------------|\n| Spatial First   |   7.7         | 550.2      |\n| Temporal First   |   7.7         | 554.8      |\n\n\n**Q3: Tab.5, 142.3 (blacken) is not the best one.**\n\n**A:** Thank you for pointing it out! We have revised it.\n\n\nThanks again for your time and effort! For any other questions, please feel free to let us know during the rebuttal window."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228903059,
                "cdate": 1700228903059,
                "tmdate": 1700274224505,
                "mdate": 1700274224505,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kG7ilzxaaD",
                "forum": "Un0rgm9f04",
                "replyto": "Avh9COtk2R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your post-rebuttal feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer cvvZ,\n\nThanks again for your constructive suggestions, which have helped us improve the quality and clarity of the paper!\n\nIn our previous response, we have carefully studied your comments and made detailed responses. As the discussion period will end soon in 1 days, we are happy to provide any additional clarifications that you may need. Please do not hesitate to contact us if there are other clarifications we can offer. We appreciate your suggestions.\n\nThanks for your time and efforts!\n\nWarm regards, \\\nAuthors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621274520,
                "cdate": 1700621274520,
                "tmdate": 1700621274520,
                "mdate": 1700621274520,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TksbZRiW3K",
                "forum": "Un0rgm9f04",
                "replyto": "Y0RNjx127b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission710/Reviewer_cvvZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission710/Reviewer_cvvZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed response. I would like to give a score that is above borderline considering the contribution."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657484629,
                "cdate": 1700657484629,
                "tmdate": 1700657484629,
                "mdate": 1700657484629,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pfyZZpTxg3",
            "forum": "Un0rgm9f04",
            "replyto": "Un0rgm9f04",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission710/Reviewer_HPRy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission710/Reviewer_HPRy"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a new method called VDT for generating videos. In this approach, transformer architecture replaces U-net as the backbone for the diffusion model. The method employs a unified spatial-temporal mask for diverse tasks and yields state-of-the-art results. The authors also provide an explanation of the mechanism behind VDT for better understanding."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The method is pretty novel and compelling. \n(2) Different tasks are designed to clarify the superiorities.\n(3) The analysis of the training strategy is technically sound."
                },
                "weaknesses": {
                    "value": "(1) According to Table 4, the FVD values of the diffusion model pre-trained with U-Net are significantly better than VDT. However, the authors mention GPU resource limitations as a potential issue. I suggest that the authors compare the results of relevant tasks to provide further clarification.\n(2) It is unclear why MCVD-concat performs better than VDT in FVD.\n(3) Due to the resolution, it is difficult to distinguish the differences between Videofusion and VDT in the TaiChi-HD section."
                },
                "questions": {
                    "value": "--The presentation was found to be easy to follow by the reviewer. However, there were some typos in the paper that caused misunderstandings. For example, in one instance, the paper mentioned that \"the input can either be pure noise latent features or a concatenation of conditional and noise latent features.\" The word \"conditional\" should have been replaced with \"conditions\". Also, the paper stated that \"the results of convergence speed and sample quality are presented in Figure 3 and Table 2, respectively.\" However, the actual result was shown in Figure 7.\n\n--Although the current quantitative analysis indicates that VDT performs better, the experimental results may not support this fact. Can you provide further analysis and results with improved quality?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission710/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837757079,
            "cdate": 1698837757079,
            "tmdate": 1699635998010,
            "mdate": 1699635998010,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9WFLBfuKpA",
                "forum": "Un0rgm9f04",
                "replyto": "pfyZZpTxg3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HPRy (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for the positive comments and insightful suggestions. Your insightful questions and valuable suggestions have been immensely helpful in enhancing the paper's quality. \n\n**Q1: According to Table 4, the FVD values of the diffusion model pre-trained with U-Net are significantly better than VDT. However, the authors mention GPU resource limitations as a potential issue. I suggest that the authors compare the results of relevant tasks to provide further clarification.**\n\n\n**A:** Thank you for your positive comments and for suggesting a more detailed comparison regarding the FVD values of our diffusion model in relation to U-Net pre-trained models.\n\nWe fully recognize the value of such a comparison in providing a clearer understanding of our model's performance. However, as you rightly pointed out, we face significant limitations due to computational resource constraints. This restricts our ability to perform direct comparisons with highly resource-intensive models like Make-A-Video, which benefits from pre-training on extensive datasets (for instance, 2.3 billion image-text pairs and 20 million image-text pairs).\n\nTo circumvent this limitation and still offer a meaningful comparison, we have expanded the analysis in Figure 4 (also presented below) to include additional video diffusion models. This broader comparison set includes models such as Make-A-Video, PyoCo, VideoGen, and VDM, which employ similar structures. Our findings indicate that our VDT model competes favorably with non-pretrained structures like VDM and PyoCo, achieving FVD scores of 225.7 compared to their scores of 295.0 and 310.0, respectively.\n\n\n\n| Diff. based on U-Net with Pre: |            |       |\n|--------------------------------|------------|-------|\n| VideoGen                       | 16\u00d7256\u00d7256 | 345.0 |\n| Make-A-Video                   | 16\u00d7256\u00d7256 | 81.3  |\n| **Diff. based on U-Net:**          |            |       |\n| PYoCo                          | 16\u00d764\u00d764   | 310.0 |\n| VDM                            | 16\u00d764\u00d764   | 295.0 |\n| **Diff. based on Transformer:**    |            |       |\n| VDT                            | 16\u00d764\u00d764   | 225.7 |\n\nThis comparative analysis underscores the efficiency and potential of the VDT network structure, even in the absence of extensive pre-training. \n\n**Q2: It is unclear why MCVD-concat performs better than VDT in FVD**\n\n**A:** Thank you for your excellent question regarding the performance comparison between MCVD-concat and VDT, specifically in the context of the FVD metric.\n* On the Cityscaps dataset, we observed that while VDT outperforms MCVD in terms of SSIM, it shows a slightly weaker performance in FVD compared to MCVD. The FVD metric primarily assesses how closely the sample space of generated videos resembles the original video space. MCVD, which is specifically tailored for video prediction, creates shorter video clips (3-5 frames) based on previous 2 conditional frames and then concatenates them. This approach allows MCVD to maintain the features of the conditional frame effectively in the spatial domain, thereby giving it an edge in the FVD metric. On the other hand, our VDT model generates the entire video sequence in one go, based on the conditional frame, which slightly impacts its performance in FVD. However, it's important to note that VDT still achieves results very close to MCVD in FVD.\n\n* The SSIM metric, contrasting with FVD, measures the similarity between predicted and target frames for each video, necessitating higher accuracy in video prediction. As MCVD generates only a few frames at a time, it can suffer from issues like color jitter and speed discrepancies across the entire video sequence. Our VDT model, which models the entire prediction process holistically, manages to maintain overall video coherence and accuracy. Consequently, VDT significantly outperforms MCVD in the SSIM metric, demonstrating its strength in ensuring consistency and precision in video prediction.\n\nWe hope this explanation clarifies the distinct advantages and limitations of both MCVD-concat and VDT in different evaluation metrics and their respective implications for video prediction performance."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228562239,
                "cdate": 1700228562239,
                "tmdate": 1700274509380,
                "mdate": 1700274509380,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NBoW3p0Bhg",
                "forum": "Un0rgm9f04",
                "replyto": "pfyZZpTxg3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HPRy (Part 2)"
                    },
                    "comment": {
                        "value": "**Q3: Due to the resolution, it is difficult to distinguish the differences between Videofusion and VDT in the TaiChi-HD section. Although the current quantitative analysis indicates that VDT performs better, the experimental results may not support this fact. Can you provide further analysis and results with improved quality?**\n\n**A:** Sorry for the challenge in differentiating between Videofusion and VDT in the TaiChi-HD section due to the resolution issue.\n\nTo address this, we have uploaded high-resolution comparison images in our appendix **(Figure 8)**, which showcases the differences more clearyly. Both VideoFusion and our VDT can generally produce videos. However, VideoFusion's generated results often exhibit less coherent movements (e.g., shaky hands, making it difficult to discern the shape of the hands), while VDT demonstrates more fluid and dynamic movement. Also, in terms of video details, VideoFusion generates clothing folds that are less natural, whereas the folds in VDT adjust more naturally with the movement of the characters. These demonstrate VDT's superior performance in maintaining details and dynamic motion.\n\n\n\n**Q4: There were some typos in the paper that caused misunderstandings.**\n\n**A:** Thank you for pointing them out! We have revised the paper accordingly.\n\nThanks again for your time and effort! For any other questions, please feel free to let us know during the rebuttal window."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228609851,
                "cdate": 1700228609851,
                "tmdate": 1700274241112,
                "mdate": 1700274241112,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ThzeXVRq34",
                "forum": "Un0rgm9f04",
                "replyto": "pfyZZpTxg3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your post-rebuttal feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer HPRy,\n\nThanks again for your constructive suggestions, which have helped us improve the quality and clarity of the paper!\n\nIn our previous response, we have carefully studied your comments and made detailed responses. As the discussion period will end soon in 1 days, we are happy to provide any additional clarifications that you may need. Please do not hesitate to contact us if there are other clarifications we can offer. We appreciate your suggestions.\n\nThanks for your time and efforts!\n\nWarm regards, \\\nAuthors"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621221393,
                "cdate": 1700621221393,
                "tmdate": 1700621221393,
                "mdate": 1700621221393,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eeXDXko35B",
                "forum": "Un0rgm9f04",
                "replyto": "pfyZZpTxg3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission710/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, thanks again for your detailed review.\n\nSince we are nearing the end of the discussion period today (Nov 22, 2023), we wanted to politely reach out to see if our response has addressed your questions satisfactorily (and we would greatly appreciate it if you could revisit your scores accordingly). We are also happy to answer any other/follow up questions before the deadline.\n\nThanks for your time and efforts!\n\nBest, \\\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727869652,
                "cdate": 1700727869652,
                "tmdate": 1700727869652,
                "mdate": 1700727869652,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]