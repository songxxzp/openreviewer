[
    {
        "title": "EMO: EARTH MOVER DISTANCE OPTIMIZATION FOR AUTO-REGRESSIVE LANGUAGE MODELING"
    },
    {
        "review": {
            "id": "8wUoPIKgar",
            "forum": "4bLXfRd0CX",
            "replyto": "4bLXfRd0CX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1681/Reviewer_XE26"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1681/Reviewer_XE26"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a training objective called Earth Mover Distance Optimization (EMO) for autoregressive language models. In contrast to maximum likelihood estimation, which solely enhances the likelihood of the ground-truth token, EMO takes into account the embedding similarity between predicted tokens and the ground-truth token, promoting predictions of tokens with high embedding similarity. Experimental results demonstrate that EMO generates superior outputs compared to MLE when employing unbiased sampling."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is easy to follow. The presentation is mostly clear."
                },
                "weaknesses": {
                    "value": "1. **There are significant concerns regarding the evaluation methodology in this paper. Comparing MLE and EMO based on outputs generated by unbiased sampling is unfair, as the two objectives result in considerably different model distributions.** The expected MLE loss is minimized when the model distribution Q matches the data distribution P. As mentioned in the paper, the expected EMO loss is $E_{v_i ~ Q}[\\sum_{j=1}^{|V|}P(v_j)C(v_i, v_j)]$, which is minimized when the model distribution Q is a one-hot distribution that only outputs the token i that maximizes $\\sum_{j=1}^{|V|}P(v_j)C(v_i, v_j)$. Consequently, models trained using EMO will predict a much sharper distribution, leading to higher-quality but lower-diversity outputs when sampling from EMO. The evaluation methodology in this paper, i.e., comparing the sampling results of EMO and MLE, is therefore like comparing the output quality of greedy decoding and unbiased sampling, which is inherently unfair. The fair way is to compare their decoding results of greedy/beam search, necessitating further experiments to establish the effectiveness of EMO.\n\n2. Based on the above analysis, the authors' motivation appears to be flawed. Under ideal circumstances, MLE trains the model to conform to the data distribution. In contrast, EMO results in a one-hot distribution, which is recall-prioritization and negative diversity ignorance. \n\n3. The proposed EMO loss is very similar to the word embedding-based loss [1]. The only difference lies the choices of distance mertics (Cosine distance in EMO, Euclidean distance in [1]).\n\n[1] https://arxiv.org/pdf/1807.11219.pdf"
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1681/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1681/Reviewer_XE26",
                        "ICLR.cc/2024/Conference/Submission1681/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1681/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697725746625,
            "cdate": 1697725746625,
            "tmdate": 1700218576441,
            "mdate": 1700218576441,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hKE3zW6KYD",
                "forum": "4bLXfRd0CX",
                "replyto": "8wUoPIKgar",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, we sincerely appreciate your time reading the paper. Here we do our best to address your concerns.\n- **MLE/EMO-induced distribution**: For per-token loss, both MLE and EMO are minimized **theoretically** when the model distribution $Q_{\\theta}$ is one-hot. However, in practical training which involves iterative stochastic gradient descent on human text data, both MLE and EMO optimize model distribution to become more human-like. Moreover, as stated in Section 4.1.1 Training Details, the final loss of EMO is the additive combination of MLE and DEMD. The difference between the final distributions induced by MLE and EMO emerge as the aggregated effect of per-token gradient update throughout training, where EMO explicit penalizes low-quality tokens and takes more account into plausible alternatives. We demonstrate this quantitatively and qualitatively\n- **Quantitative difference**: We show the differences between MLE and EMO induced distributions by measuring the forward cross-entropy $\\text{CE}^{\\text{forward}}=-\\sum{P\\log{Q_{\\theta}}}$, reverse cross-entropy $\\text{CE}^{\\text{reverse}}=-\\sum{Q_{\\theta}\\log{P}}$, and their harmonic mean against a reference distribution $P$ provided by a larger language model GPT-Neo. The GPT-2 results of $Q_{\\theta}^{\\text{MLE}}$, $Q_{\\theta}^{\\text{EMO}}$, and a smoothed one-hot distribution are shown in the table below. As can be seen, both $Q_{\\theta}^{\\text{EMO}}$ and $Q_{\\theta}^{\\text{MLE}}$ deviate far from one-hot and $Q_{\\theta}^{\\text{EMO}}$ is **more similar to reference distribution** by better harmonizing recall and precision.\n|  |  | WikiText-2 |  |  | WritingPrompt |  |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n|  | $\\text{CE}^{\\text{reverse}}$\u2193 | $\\text{CE}^{\\text{forward}}$\u2193 | Harmonic Mean\u2193 | $\\text{CE}^{\\text{reverse}}$\u2193 | $\\text{CE}^{\\text{forward}}$\u2193 | Harmonic Mean\u2193 |\n| $Q^{\\text{one-hot}}$ | 3.22 | 16.83 | 5.15 | 3.32 | 17.74 | 5.33 |\n| $Q_{\\theta}^{\\text{MLE}}$ | 4.50 | 4.33 | 4.22 | 4.25 | 3.92 | 3.99 |\n| $Q_{\\theta}^{\\text{EMO}}$ | 3.94 | 4.45 | 3.97 | 3.54 | 4.01 | 3.66 |\n        \n    We also report the Distinct N-gram of the sampled outputs from $Q_{\\theta}^{\\text{MLE}}$, $Q_{\\theta}^{\\text{EMO}}$, and reference text. As can be seen,  $Q_{\\theta}^{\\text{EMO}}$ is also more similar to human text distribution in various aspects of the sampled outputs.\n|  |  |  | WikiText2 |  |  |  |  |  | WritingPrompt |  |  |\n|---|:---:|:---:|:---:|:---:|:---:|---|:---:|:---:|:---:|---|:---:|\n|  |  |  | Dist$_{n}$ |  | MAUVE |  \\|  |  |  | Dist$_{n}$ | | MAUVE |\n|  | n=1 | n=2 | n=3 | n=4 | - | \\| | n=1 | n=2 | n=3 | n=4 | - |\n| $Q_{\\theta}^{\\text{MLE}}$ | 0.748 | 0.971 | 0.994 | 0.998 | 77.5 | \\| | 0.806 | 0.983 | 0.995 | 0.997 | 83.6 |\n| $Q_{\\theta}^{\\text{EMO}}$ | 0.698 | 0.955 | 0.990 | 0.997 | 87.5 | \\| | 0.762 | 0.972 | 0.993 | 0.996 | 87.4 |\n| Human | 0.687 | 0.958 | 0.988 | 0.996 | 100 | \\| | 0.764 | 0.974 | 0.993 | 0.997 | 100 | \n- **Qualitative difference**: Here we show some predicted next token probability from $Q_{\\theta}^{\\text{MLE}}$ and $Q_{\\theta}^{\\text{EMO}}$. We show the top 5 predicted tokens and their probabilities.\n    - Prefix: Donald Trump has multiple occupations, such as\n    - $Q_{\\theta}^{\\text{MLE}}$: the(0.393), a(0.325), construction(0.100), executive(0.093), president(0.088),...\n    - $Q_{\\theta}^{\\text{EMO}}$: a(0.366), the(0.353), president(0.100), executive(0.091), construction(0.091),...\n    - Prefix: Elephant is the largest living\n    - $Q_{\\theta}^{\\text{MLE}}$: animal(0.275),creature(0.231),elephant(0.215),mammal(0.197),ant(0.079),...\n    - $Q_{\\theta}^{\\text{EMO}}$: animal(0.268),creature(0.257),mammal(0.216),elephant(0.196),rept(0.062),...\n\n- **Greedy/Beam search**: The results of greedy/beam search of $Q_{\\theta}^{\\text{MLE}}$ and $Q_{\\theta}^{\\text{EMO}}$ on WikiText-2 test set are shown in the table below. **More greedy decoding results of LLaMa2 are presented in our General Response**.\n|  | Method | MAUVE | BLEU | ROUGE-1 |\n|---|:---:|:---:|:---:|:---:|\n| Greedy Search | $Q_{\\theta}^{\\text{MLE}}$ | 7.10 | 21.64 | 38.8 |\n|  | $Q_{\\theta}^{\\text{EMO}}$ | 11.0 | 22.27 | 38.0 |\n| Beam Search | $Q_{\\theta}^{\\text{MLE}}$ | 9.93 | 21.67 | 37.0 |\n|  | $Q_{\\theta}^{\\text{EMO}}$ | 13.14 | 22.37 | 37.2 |\n\n- **Difference between EMO and word embedding-based loss**: We would like to modestly point out that the word embedding-based loss described in the paper mentioned by the reviewer is indeed a weighted variant of MLE. In their method, the gradient of the **ground-truth token's probability** is weighted by the sum of its Euclidean distance to all tokens in the target vocabulary, .i.e, $\\nabla Q_{\\theta}(y_i)\\sum_k d(E(v_k),E(v_i))$. In contrast, EMO involves the sum of gradients of **all tokens' probability**, weighted by their respective expected transport cost. We will cite this paper in the final version.\n\nReference: \n\n[1] Meister et. al: On the Efficacy of Sampling Adapters. ACL 2023"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700109026120,
                "cdate": 1700109026120,
                "tmdate": 1700109026120,
                "mdate": 1700109026120,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lw60qWQvRW",
                "forum": "4bLXfRd0CX",
                "replyto": "8wUoPIKgar",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1681/Reviewer_XE26"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1681/Reviewer_XE26"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply.\n\n1. I appreciate the clarification. I didn't notice that the final loss also includes MLE, so it turns out that the distribution of EMO is not as sharp as I expected. However, I disagree with the author's claim that \"both MLE and EMO optimize model distribution to become more human-like.\" Based on my analysis, the expected EMO loss is minimized when the model distribution Q is a one-hot distribution that only outputs the token i that maximizes $\\sum_{j=1}^{|V|}P(v_j)C(v_i, v_j)$. Consequently, the perplexity of EMO is higher than that of MLE, indicating a less human-like distribution. If DEMD were used solely as the loss function, I believe the resulting model distribution would be sharper and less human-like, leading to a significantly higher perplexity. A similar phenomenon can be observed in language GANs [1].\n\n2. I maintain my point that the paper's motivation is flawed, and I suggest the author consider presenting the story of EMO from a different perspective.\n\n3. I acknowledge my previous misunderstanding of the word embedding-based loss and appreciate the clarification provided.\n\nI have a few questions:\n\n1. Could you explain the concept of Q^{one-hot}?\n\n2. In the WikiText-2 test set results for greedy/beam search, why are the MAUVE scores substantially lower than those obtained through unbiased sampling, as shown in Table 1?\n\n[1] https://arxiv.org/pdf/1811.02549.pdf"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218500883,
                "cdate": 1700218500883,
                "tmdate": 1700218923275,
                "mdate": 1700218923275,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kSczYGKKRA",
                "forum": "4bLXfRd0CX",
                "replyto": "8wUoPIKgar",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for reading our responses. Regarding your questions, our answers are as follows:\n- The concept of $Q^{\\text{one-hot}}$ refers to the one-hot distribution constructed based on the ground-truth next token. \n The various divergence measures of $Q^{\\text{one-hot}}$ we've reported in our response serve as a reference to indicate how far are $Q_{\\theta}^{\\text{EMO}}$) from such one-hot distribution. Compared to $Q_{\\theta}^{\\text{MLE}}$, $Q_{\\theta}^{\\text{EMO}}$ is **closer** to the reference distribution $P$ as measured by the harmonic mean of forward/reverse cross-entropy.\n- This is because MAUVE score measures how similar the model distribution $Q_{\\theta}$ is to the data distribution $P$. Concretely, MAUVE computes the area under an interpolated KL divergence curve. **It rewards model distribution that balances recall($Q_{\\theta}$ has sufficient cover of $P$'s mode) and precision($Q_{\\theta}$ does not overestimates regions where $P$ is low)**. Two identical distributions will have a MAUVE score of 100. **Both greedy and beam search are maximization-based decoding strategies that discard all possible outputs except the most possible one**. This is equivalent to a high-precision but significantly low-recall output distribution. Therefore, the MAUVE scores of greedy and beam search are usually much smaller than sampling-based methods, e.g., unbiased/top-p/top-k/typical sampling. The same phenomenon was also observed in the original MAUVE paper as well as a prior work[1]'s Table 1.\n\nIn the following we would like to explicate more about your comments:\n- We would like to kindly clarify that, in the context of auto-regressive language modeling(where we have a collection of text corpus for training), the mathematically lowest loss at each time step is achieved when the ground-truth next token's probability under $Q_{\\theta}$ is 1, for both MLE and DEMD: (1) for MLE, -log1=0, and (2) for DEMD, 1-1*1*cos(0)=0. Both MLE and EMO optimize model towards such solution, but through different trajectories, i.e., gradient signal. The gradient signal of EMO informs more about \"bad\" tokens and alternative \"good\" tokens. \n- \"higher perplexity\": MLE has lower perplexity is expected because MLE's objective is to minimize perplexity($e^{-\\log{Q_{\\theta}(x)}}$), which is the so-called recall-prioritization[2]. EMO has higher perplexity because it cares more about diversity in the range of other \"good\" tokens.\n- \"less human-like distribution\": According to an exhaustive array of quantitative metrics we've reported, i.e., MAUVE, Harmonic mean of forward/reverse cross-entropy, Distinct N-gram, and the alignment experiments upon LLaMa2, the EMO-induced distribution is more human-like. We would much appreciate if the reviewer could elaborate further why EMO is less human-like.\n\nPlease let us know you have further questions and we will try our best to solve your issues.\n\nReferences:\n\n[1] Su et.al: A Contrastive Framework for Neural Text Generation. NeurIPS 2022.\n\n[2] Meister et. al: On the Efficacy of Sampling Adapters. ACL 2023"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244197873,
                "cdate": 1700244197873,
                "tmdate": 1700300485018,
                "mdate": 1700300485018,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1LuevD1kud",
                "forum": "4bLXfRd0CX",
                "replyto": "kSczYGKKRA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1681/Reviewer_XE26"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1681/Reviewer_XE26"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your clarification. When analyzing the model distribution induced by loss, we need to analyze the expected loss with respect to the data distribution, rather than the loss of individual samples. The expected MLE loss, $-\\sum P \\log Q$, is minimized when $P=Q$, which is a well known property of MLE. On the other hand, the expected EMO loss, $E_{v_i ~ Q}[\\sum_{j=1}^{|V|}P(v_j)C(v_i, v_j)]$, is minimized when the model distribution Q is one-hot that only outputs the token i that maximizes $\\sum_{j=1}^{|V|}P(v_j)C(v_i, v_j)$. Consequently, I believe that using EMO as the sole loss function would result in a sharper, less human-like model distribution, leading to a significantly higher perplexity (the perplexity is also minimized when $P=Q$).\n\nThe current implementation, which combines MLE and EMO to train the model, seems to mitigate this issue to some extent, allowing for an improvement in quality without severely compromising diversity. As long as the authors enhance the presentation of the results by simultaneously displaying the model's quality and diversity, as well as their trade-off (e.g., through temperature sampling), the experimentation will be satisfactory.\n\nHowever, I still maintain a negative score because the motivation is flawed. The authors argue that MLE is recall-prioritization and negative diversity ignorance, a claim with which I disagree. Based on the analysis above, using EMO as the loss function would theoretically induce a one-hot distribution, which is recall-prioritization and negative diversity ignorance. Therefore, I am against the publication of this paper, as it will be misleading to the community."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406755473,
                "cdate": 1700406755473,
                "tmdate": 1700406755473,
                "mdate": 1700406755473,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mvBcmdzAKB",
                "forum": "4bLXfRd0CX",
                "replyto": "OHJFNzvB3C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1681/Reviewer_XE26"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1681/Reviewer_XE26"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply.\n\nI don't agree that we can justify focusing solely on one-hot data distribution due to the limited size of training set. One of the most important characteristics of Deep Neural Networks is their exceptional generalization ability, allowing them to perform well on unseen data after being trained on a limited set of examples. Despite the limited amount of training data and their statistical distribution being primarily one-hot, we still should analyze the theoretical properties of model based on the real data distribution, as the model possesses the ability to generalize to real distribution.\n\nA simple counterexample is that if we only consider one-hot data distribution, the negative log-likelihood loss $-\\log Q(x_t|x_{<t})$ and negative probability loss $-Q(x_t|x_{<t})$ are equivalent, both making the model converge to the data distribution. However, on the wikitext-103 dataset, fine-tuning a MLE-trained model with negative probability loss for just one epoch increases the perplexity from 27 to 2000, indicating a significant deviation of the model distribution from the data distribution. Additionally, the average prediction entropy of the model reduces from 3.14 to 0.46, suggesting a much sharper model distribution. This phenomenon can be well-explained if we consider the expected loss under the real data distribution. The model distribution that minimizes the negative probability loss is one-hot, which outputs $argmax_{x_t}P(x_t|x_{<t})$, so training with this kind of loss would severely damage the model's diversity. \n\nRegarding recall-prioritization and negative diversity ignorance, I checked the provided references and find that some of them mention these concepts [1,2] but most of these references are irrelevant. Thus, I don't think that these two properties of MLE are widely recognized. \n\n[1] Meister et. al: On the Efficacy of Sampling Adapters. ACL 2023.\n[2] Li et. al: Data-Dependent Gaussian Prior Objective for Language Generation. ICLR 2020."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729184366,
                "cdate": 1700729184366,
                "tmdate": 1700729184366,
                "mdate": 1700729184366,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ymzpfZWBLq",
            "forum": "4bLXfRd0CX",
            "replyto": "4bLXfRd0CX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1681/Reviewer_JZqx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1681/Reviewer_JZqx"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel loss function, EMO. The main idea behind emo is to soften the loss function (vanilla cross entropy) based on the cosine similarity of pretrained lm head embeddings. EMO demonstrates consistently better than MLE across models, tasks, scales, and metrics."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-organized and well-written, so it is generally easy to follow.\n2. The intuition of the idea makes sense, and it is good that the complex earth mover distance loss can be reformulated into the cosine similarity dot probability. \n3. The author(s) conducted extensive and detailed experiments, encompassing different model sizes, data scales, and evaluation metrics (which is very important). The thorough and detailed experiments show the advantages of EMO, adding to the soundness of the paper."
                },
                "weaknesses": {
                    "value": "I have two concerns here.\n\n1) Originality of the method.\nIn my view, the final loss function is very similar to the d2gpo loss, the authors did cite the d2gpo paper, but they ignored the methodology comparison and they should add d2gpo as a baseline.\n\n2) Why Cosine Similarity?\nUsing cosine similarity is a choice, but may not be the best choice. The cosine similarity relies on the pre-trained llm head embedding, which makes it not unbiased. And through the 3.3 section, the gradient of the proposed EMO is very similar to the REINFORCE with cosine similarity as the reward, so maybe RLHF/RLAIF will be a better reward model?"
                },
                "questions": {
                    "value": "pls see weakness and the following questions:\n\n3) why EMO can make ppl better? ppl is directly related to MLE, in other words, there does not exist a training-test mismatch problem. Is that because the evaluation is conducted over the sampled sentences instead of the test set. If so, please also report the ppl in another table.\n\n4) please also compare with label smoothing, which is also a very useful loss function in the era of pre-llm.\n\n5) what does the accuracy mean in sec 4.2.3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1681/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1681/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1681/Reviewer_JZqx"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1681/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764474065,
            "cdate": 1698764474065,
            "tmdate": 1699636096300,
            "mdate": 1699636096300,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A7jI4XmIvG",
                "forum": "4bLXfRd0CX",
                "replyto": "ymzpfZWBLq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, we sincerely appreciate your time reading the paper. Here we do our best endeavor to address your concerns:\n- **Choice of Baselines**: Our primary rationale for selecting baselines is to compare methods that optimize different **probability divergence measures** between model distribution and data distribution. Among our baselines, MLE optimizes the **forward cross-entropy**, TaiLr focuses on the **total variation distance** and MixCE mixes **forward and reverse cross-entropy**. D2GPo did not propose to optimize new divergence measures but instead focused on a **new target distribution**. Moreover, we didn't find the publicly available official implementation of D2GPo. Therefore, we didn't include D2GPo as our baseline in our submitted draft. Our reimplementation of D2GPo shows similar performance to MLE.\n- **Choice of Cosine Distance**: The reasons for using cosine distance between llm head embedding are three-fold:\n  - The general quadratic form of DEMD using a $|V|\\times|V|$ transport cost matrix $C$ is $O(|V|^2)$. By utilizing the cosine distance, we can decrease the computation complexity from $O(|V|^2)$ to $O(|V|H)$(Eq. 13->Eq. 14).\n  - The output logits of LM are computed based on the dot-product between hidden states and LLM head embedding. The computation of cosine distance is also based on dot-product between normalized llm head embedding, exploiting the information stored in llm head. In fact, the transport cost matrix can also be computed based on the lm head of the model undergoing fine-tuning, which gets updated to incorporate the distribution shift from the pre-training corpus to task-specific training data. We term this variant of EMO as $\\text{EMO}_{\\text{adaptive}}$and conduct experiments to verify this and the results of language modeling are shown in the table below:\n|  | WikiText-2 | WritingPrompts |\n|:---:|:---:|:---:|\n| MLE | 77.5 | 83.6 |\n| EMO | 87.5 | 87.4 |\n| EMO$_{\\text{adaptive}}$ | 88.1 | 88.3 |\n  - The main purpose of policy gradient methods such as REINFORCE and PPO is to enable differentiable learning towards potentially non-differentiable metrics. Moreover, the reward is usually a sequence-level delayed one. In contrast, the research objective of EMO focuses on exploring more effective probability divergence measures at the token level through the lens of Earth Mover Distance. In the LLM area, the former usually happens in the alignment stage while EMO is more suitable to be applied to (1) domain-specific fine-tuning; (2) continual pre-training and (3) instruction-tuning. We have included additional results on instruction tuning upon LLaMa2 in the general response. Please refer to it for more details.\n- **Perplexity**: You are right, the evaluation is conducted over the sampled sentences instead of the test set. The $\\text{PPL}^{\\text{oracle}}$ reported in Table 2 is computed using the Oracle data generator on the model-generated samples. A lower $\\text{PPL}^{\\text{oracle}}$ indicates higher precision. Here we additionally include the $\\text{PPL}^{\\text{test}}$ on the test set in the table below:\n|  | $\\text{PPL}^{\\text{test}}$\u2193 | $\\text{PPL}^{\\text{oracle}}$\u2193 | MAUVE\u2191 |\n|:---:|:---:|:---:|:---:|\n| MLE | 70.1 | 114.5 | 77.5 |\n| EMO | 74.9 | 55.9 | 83.4 |\n- **Label Smoothing**: The MAUVE scores of label smoothing(smoothing factor set to 0.1) on language modeling tasks are shown below. Label smoothing performs even worse than MLE. The reason is that the number of plausible alternatives to ground-truth tokens is typically much smaller than the number of improbable tokens. Thus,  label smoothing blindly puts more probability mass on low-quality tokens, leading to worse precision.\n|  | WikiText-2 | WritingPrompt | PTB | AG News |\n|---|:---:|:---:|:---:|:---:|\n| MLE | 77.5 | 83.6 | 76.1 | 75.0 |\n| Label Smoothing | 75.8 | 81.6 | 71.9 | 73.8 |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700109015506,
                "cdate": 1700109015506,
                "tmdate": 1700109015506,
                "mdate": 1700109015506,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bLykPJIQJI",
                "forum": "4bLXfRd0CX",
                "replyto": "A7jI4XmIvG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1681/Reviewer_JZqx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1681/Reviewer_JZqx"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply. \n\n1. The ppl test of EMO is much higher than MLE, in my view it seems EMO trades diversity for quality.\n2. You said Moreover, the reward is usually a sequence-level delayed one. I think that is not the main difference between PPO/RLHF and EMO. I can also train a word-level one, and obviously the seq-level one is better due to the reward should be signed after the whole sentecne generated instead of internal state."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700115327681,
                "cdate": 1700115327681,
                "tmdate": 1700115327681,
                "mdate": 1700115327681,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YAKjEzddJf",
                "forum": "4bLXfRd0CX",
                "replyto": "ymzpfZWBLq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for reading our responses. Regarding your questions, our further responses are as follows:\n- On one hand, the **diversity** of MLE often come at the cost of generting adverse outputs due to its low precision. For language generation systems, high precision is arguably a higher priority since a single bad output can leave a lasting poor impression on the user. On the other hand, EMO's diversity manifests in the range of **good tokens**, which is more **aligned/similar to human text distribution**, as indicated by the Distinct N-gram, MAUVE score, and harmonic mean of forward/reverse cross-entropy.\n- You are right, the reward can also be a word-level one, depending on what the environment can offer. To summarize, the main differences between EMO and RL-based training include: (1) motivation: exploring alternative **probability distance measures** for language modeling v.s. steer language model towards certain aspects (2) formula: weighted combination of gradient of each token in the vocabulary v.s. reward-augmented maximum likelihood of tokens in the output, and (3) application scenario: domain-specific fine-tuning/adaptation, instruction-tuning, and continual pre-training v.s. alignment."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700144913849,
                "cdate": 1700144913849,
                "tmdate": 1700145177737,
                "mdate": 1700145177737,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DQeY1ZVRuR",
                "forum": "4bLXfRd0CX",
                "replyto": "YAKjEzddJf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1681/Reviewer_JZqx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1681/Reviewer_JZqx"
                ],
                "content": {
                    "comment": {
                        "value": "About the second point, I still can not agree with you. So can you add a section (probably at appendix) to discuss the difference between RLHF and EMO. \nPlease remember I am not mean to criticize EMO, even RLHF is similar to EMO, EMO still has it own advantages (e.g., no need to train an extra rewrad model)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378372464,
                "cdate": 1700378372464,
                "tmdate": 1700378372464,
                "mdate": 1700378372464,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Eqbaew6cgZ",
                "forum": "4bLXfRd0CX",
                "replyto": "8UUmqjcUAl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1681/Reviewer_JZqx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1681/Reviewer_JZqx"
                ],
                "content": {
                    "comment": {
                        "value": "I think you are wrong about REINFORCE, because the probability of the token are computed after softmax function, so every token will have gradient."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700392914695,
                "cdate": 1700392914695,
                "tmdate": 1700392914695,
                "mdate": 1700392914695,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KTCGbptnN5",
            "forum": "4bLXfRd0CX",
            "replyto": "4bLXfRd0CX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1681/Reviewer_TjsD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1681/Reviewer_TjsD"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose to train the language model using an upper bound of Earth Mover Distance. The cost function of EMO is established by the similarity of embeddings from pretrained language model. The authors provide theoretical analysis and argue that EMO is better at handling synonyms compared with MLE. Experiments on various tasks demonstrate EMO's superiority."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors propose to apply the Earth Mover Distance to train the language model and establish an upper bound for practical backward propagation training.\n2. The experiments demonstrate promising performance of EMO on various tasks."
                },
                "weaknesses": {
                    "value": "1. The authors argue that MLE exhibits a recall-prioritization behavior, which serves as the primary motivation for introducing their proposed approach, EMO. The claim appears to be confusing, as MLE is equivalent to minimizing the forward KL-divergence, i.e., $KL(p||q_{\\theta})$.\nIf the model $q_{\\theta}$ has sufficient capacity, the optimal $q_{\\theta}$ converges to $p$. Otherwise, $q_{\\theta}$ tends to exhibit a mean-seeking behavior. Therefore I have doubts about whether \"recall-prioritization\" is proper.\n\n2. As pointed out in Section 3.3, EMO exhibits a property of harmonizing recall and precision. A straightforward inference is that EMO is better at handling synonyms compared to MLE, potentially granting EMO-trained models the capability to generate more diverse texts than models trained with MLE. The authors did not conduct such experiments.\n\n3. I guess the distribution of model trained by EMO is very different from that by MLE. However, there's no experiments and analyses regarding that."
                },
                "questions": {
                    "value": "What are the results of EMO-trained models and MLE-trained models when employing beam search instead of sampling?\nI am curious as sampling reflects the entire distribution, whereas beam search captures the distribution's mode."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1681/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1681/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1681/Reviewer_TjsD"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1681/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837750531,
            "cdate": 1698837750531,
            "tmdate": 1699636096216,
            "mdate": 1699636096216,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9lojp2Wxj4",
                "forum": "4bLXfRd0CX",
                "replyto": "KTCGbptnN5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, we genuinely thank you for dedicating your time to reviewing the paper. Here, we strive to effectively respond to the issues you raised.\n- **recall-prioritization**: Conceptually, as described in Section 2.2.1, a high recall means that high likelihood tokens under $P$ shall also have high likelihood under $Q_{\\theta}$. In contrast, precision focuses on measuring whether low-quality tokens (unlikely under $P$) have low probabilities under $Q_{\\theta}$. Ideally, with unlimited training data and model capacity, as well as a perfect optimizer, MLE-learned distribution will conform to the real data distribution. However, in practice, due to the lack of explicit training signals to penalize low-quality tokens, MLE-learned distribution tends to put non-trivial probability mass over low-quality samples. \n      \n\n    The **operationalization of recall and precision** in language modeling has also been investigated by Meister et. al[1]. They quantitiatively measure the recall by forward cross-entropy $\\text{CE}^{\\text{forward}}=-\\sum{P\\log{Q_{\\theta}}}$, and precision by reverse cross-entropy $\\text{CE}^{\\text{reverse}}=-\\sum{Q_{\\theta}\\log{P}}$. **The recall-prioritization(low-precision) behavior of MLE can be observed by comparing the original distribution against the distributions modified by various inference-time truncation methods, e.g., top-p, top-k.**\n      \n\n    Here we demonstrate this by reporting $\\text{CE}^{\\text{forward}}$, $\\text{CE}^{\\text{reverse}}$, and their harmonic mean against a reference distribution $P^{\\text{ref}}$ provided by a larger GPT-Neo model. Results on the test set of WikiText-2 and WritingPromptare are shown in the following table. As can be seen, EMO emerges as a training objective that strikes a better balance between recall and precision.\n|  |  | WikiText-2 |  |  | WritingPrompt |  |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n|  | $\\text{CE}^{\\text{reverse}}$\u2193 | $\\text{CE}^{\\text{forward}}$\u2193 | Harmonic Mean\u2193 | $\\text{CE}^{\\text{reverse}}$\u2193 | $\\text{CE}^{\\text{forward}}$\u2193 | Harmonic Mean\u2193 |\n| $Q^{\\text{one-hot}}$ | 3.229 | 16.83 | 5.154 | 3.316 | 17.742 | 5.326 |\n| $Q_{\\theta}^{\\text{MLE}}$ | 4.503 | 4.333 | 4.219 | 4.250 | 3.920 | 3.994 |\n| $Q_{\\theta}^{\\text{EMO}}$ | 3.940 | 4.455 | 3.973 | 3.538 | 4.008 | 3.664 |\n- **diversity**: Though EMO is better at harmonizing recall and precision, it does not directly translate to higher diversity in the model generations. In fact, the MLE-tuned model may generate more diverse outputs because it allocated probability mass more than it should to low-quality outputs. We present the Distinct N-gram($\\frac{|\\text{unique n-grams}|}{|\\text{total n-grams}|}$) of GPT-2-generated continuations on the WikiText-2/WritingPrompt test set in the table below. Compared to MLE, the EMO-tuned model shows n-gram repetitiveness and MAUVE score **closer** to that of human texts by notable margins.\n|  |  |  | WikiText2 |  |  |  |  |  | WritingPrompt |  |  |\n|---|:---:|:---:|:---:|:---:|:---:|---|:---:|:---:|:---:|---|:---:|\n|  |  |  | Dist$_{n}$ |  | MAUVE |  \\|  |  |  | Dist$_{n}$ | | MAUVE |\n|  | n=1 | n=2 | n=3 | n=4 | - | \\| | n=1 | n=2 | n=3 | n=4 | - |\n| $Q_{\\theta}^{\\text{MLE}}$ | 0.748 | 0.971 | 0.994 | 0.998 | 77.5 | \\| | 0.806 | 0.983 | 0.995 | 0.997 | 83.6 |\n| $Q_{\\theta}^{\\text{EMO}}$ | 0.698 | 0.955 | 0.990 | 0.997 | 87.5 | \\| | 0.762 | 0.972 | 0.993 | 0.996 | 87.4 |\n| Human | 0.687 | 0.958 | 0.988 | 0.996 | 100 | \\| | 0.764 | 0.974 | 0.993 | 0.997 | 100 |\n- **Qualitative difference**: In addition to the quantitative difference between MLE and EMO, here we show some predicted next token probabilities from $Q_{\\theta}^{\\text{MLE}}$ and $Q_{\\theta}^{\\text{EMO}}$. We show the top-5 predicted tokens along with their probability due to space constraints. EMO tends to a\n    - Prefix: Donald Trump has multiple occupations, such as\n    - $Q_{\\theta}^{\\text{MLE}}$: the(0.393), a(0.325), construction(0.100), executive(0.093), president(0.088),...\n    - $Q_{\\theta}^{\\text{EMO}}$: a(0.366), the(0.353), president(0.100), executive(0.091), construction(0.091),...\n    - Prefix: Elephant is the largest living\n    - $Q_{\\theta}^{\\text{MLE}}$: animal(0.275),creature(0.231),elephant(0.215),mammal(0.197),ant(0.079),...\n    - $Q_{\\theta}^{\\text{EMO}}$: animal(0.268),creature(0.257),mammal(0.216),elephant(0.196),rept(0.062),...\n- **Beam Search**: We present the MAUVE, BLEU, and ROUGE-1/2/L scores of model outputs generated by beam search(beam width set to 4) on the test set of WikiText-2. With beam search, EMO still displays higher similarity with human texts. For more results, please refer to our general responses where we compare the quality of greedy decoding generations from fine-tuned LLaMa2 models.\n|  | MAUVE | BLEU | ROUGE-1 | ROUGE-2 | ROUGE-L |\n|:---:|:---:|:---:|:---:|:---:|:---:|\n| $Q_{\\theta}^{\\text{MLE}}$ | 9.93 | 21.31 | 37.0 | 24.7 | 33.6 |\n| $Q_{\\theta}^{\\text{EMO}}$ | 13.14 | 22.34 | 37.2 | 22.9 | 33.9 |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700108944070,
                "cdate": 1700108944070,
                "tmdate": 1700108944070,
                "mdate": 1700108944070,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CjxSeW95PG",
                "forum": "4bLXfRd0CX",
                "replyto": "9lojp2Wxj4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1681/Reviewer_TjsD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1681/Reviewer_TjsD"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply.\n\nI found the analysis of diversity above appears to be contradictory to the motivation of EMO.\n\nIn the motivation (Sec 2.2.2), the authors contend that **\"the ideal training objective should assign high probabilities to those synonyms tokens rather than penalize them as did in MLE\"**. To this end, the authors propose EMO, which considers specific tokens as synonyms by assigning them a low transportation cost, leading to a minor penalty for these outputs. Based on its motivation, EMO is anticipated to acquire a more varied distribution (i.e., considering synonyms).\n\nHowever, the authors find EMO leads to a worse diversity and attribute the reason to that **\"MLE-tuned model allocated probability mass more than it should to low-quality outputs\"**. This is not convincing."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700119635840,
                "cdate": 1700119635840,
                "tmdate": 1700119635840,
                "mdate": 1700119635840,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u8BiaGqnws",
                "forum": "4bLXfRd0CX",
                "replyto": "KTCGbptnN5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for spending time reading our reponse. Regarding your concern, our responses are as follows:\n\n- The number of synonyms/semantically-similar tokens is typically much smaller than that of improbable tokens. Therefore, by explicitly signifying training signal of **low-quality tokens** and the **relative goodness of high-quality tokens**, $Q_{\\theta}^{\\text{EMO}}$ is less diverse in terms of **full output distribution** because a large portion of probability mass are removed from those low-quality tokens and reassigned to a smaller set of **plausible tokens**. In other words, EMO is more **diverse** in the range of good tokens. Another evidence to show this that, even after applying advanced inference-time sampling strategies(e.g., temperature(T=0.7), top-p(p=0.9), and top-k(k=20)), MLE still underperform EMO because the distribution rescaling behavior of these sampling strategies is token-agnostic. On WikiText-2, the MAUVE score of MLE with top-p and top-k are 81.5, 80.3 respectively.\n- Pursuing diversity in the full-distribution does not entail a human-like distribution. As indicated by the results we reported above, EMO leads to more **human-like** distribution in terms of (1) output N-gram diversity (2) MAUVE score and (3) Harmonic mean of forward/reverse cross-entropy."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122091405,
                "cdate": 1700122091405,
                "tmdate": 1700125534306,
                "mdate": 1700125534306,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0oVBFqVxiQ",
                "forum": "4bLXfRd0CX",
                "replyto": "u8BiaGqnws",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1681/Reviewer_TjsD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1681/Reviewer_TjsD"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply.\n\nYou said **EMO is more diverse in the range of good tokens**. How to prove that? A natural inference is that EMO are better in terms of both quality and diversity. However, it is not the case. Moreover, I don't see the connection between the provided results of MLE and your claim on EMO's diversity. \n\nI think this problem is major, as it is directly related to the motivation of this paper."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700397629977,
                "cdate": 1700397629977,
                "tmdate": 1700397629977,
                "mdate": 1700397629977,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IUohXjvzQZ",
            "forum": "4bLXfRd0CX",
            "replyto": "4bLXfRd0CX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1681/Reviewer_2RWM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1681/Reviewer_2RWM"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to train \"decoder-only\" language model with a different loss function EMO  derived from the EMD (Earth Moving Distance).  The motivation is threefold: Tradeoff between Recall and Precision, Negative Diversity, Train-Test Consistency. These three points are clearly defined and compared for the standard loss (maximum loglikelihood, MLE) and EMO. The authors propose to define the inner cost of this distance with a semantic similarity (cosine between word vectors obtained from a pretrained LM). They also use a more tractable upperbound on the EMD.  The experimental setup proposes different kind of evaluation to assess the impact of this new training strategy."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is overall well written and describes an interesting idea. The experimental setup is well described and the results look reproducible."
                },
                "weaknesses": {
                    "value": "A \"related work\" section is missing, and it could be nice to better discuss the introduction of EMD (and optimal transport in general) in NLP and this kind of task. \n\nThe experimental setup focuses on the improvement of MAUVE. This is a quite recent metric that makes a tradeoff between precision and recall. While it is interesting to use that metric, it could be nice also to provide also perplexity. I know MLE optimizes the perplexity so it is not fair for EMO, but it can provides a meaningful comparison point (I mean in table 1)."
                },
                "questions": {
                    "value": "The decoding process is unique and since your purpose is to improve diversity, it could be nice to have a discussion on decoding, ie generation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1681/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699431252947,
            "cdate": 1699431252947,
            "tmdate": 1699636096123,
            "mdate": 1699636096123,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lRKA4fd5B8",
                "forum": "4bLXfRd0CX",
                "replyto": "IUohXjvzQZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks very much for your valuable suggestions and comments. We sincerely appreciate your time in reading the paper, and our point-to-point responses to your comments are given below.\n- **related work section**: Thanks for your advice, we will include a related work section discussing how optimal transport was previously explored and exploited in NLP.\n- **perplexity**: We present additional results of fine-tuned GPT-2's perplexity on the test set of WikiText-2 and WritingPrompts in the following table. In addition, we also report the token-level averaged forward cross-entropy $\\text{CE}^{\\text{forward}}=-\\sum{P\\log{Q_{\\theta}}}$, reverse cross-entropy $\\text{CE}^{\\text{reverse}}=-\\sum{Q_{\\theta}\\log{P}}$, and their harmonic mean, computed using the learned model distribution $Q_{\\theta}$and the reference distribution $P_{\\text{ref}}$ provided by a language model with stronger capacity, i.e., GPT-Neo. These metrics comprehensively delineate the behavior differences between MLE and EMO. Based on the results, $ Q_{\\theta}^{\\text{EMO}} $ exhibits slightly higher ppl compared to $Q_{\\theta}^{\\text{MLE}}$. However, EMO excels at striking the balance between recall and precision, resulting in a more human-like distribution. \n|  | Perplexity | MUAVE | $\\text{CE}^{\\text{forward}}$ | $\\text{CE}^{\\text{reverse}}$ | Harmonic Mean |\n|---|---|---|---|---|---|\n| $ Q_{\\theta}^{\\text{MLE}} $ | 29.8 | 77.5 | 4.33 | 4.503 | 4.219 |\n| $ Q_{\\theta}^{\\text{EMO}} $ | 30.9 | 87.5 | 4.45 | 3.940 | 3.973 |  \n\n- **Decoding**: Would you please kindly explain \"discussion on decoding, ie generation\"? If you are referring to the decoding method, we employ unbiased sampling as our primary decoding method as it allows us to explore the learned distribution in an unbiased way[1,2]. With EMO, we expect the output from the learned model distribution to be more similar to human distribution than MLE. Besides MAUVE, we also include the Distinct N-gram($\\frac{|\\text{unique n-grams}|}{|\\text{total n-grams}|}$) of model-generated continuations on the test set of WikiText-2 and WritingPrompt. The closer to that of human text, the better. As shown in the table, EMO also displays higher similarity in terms of n-gram characteristics.  \n|  |  | WikiText-2 |  |   |  | WritingPrompt |  |  |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n|  | n=1 | n=2 | n=3 | n=4 | \\| |n=1 | n=2 | n=3 | n=4 |\n| MLE | 0.738 | 0.971 | 0.994 | 0.998 | \\| | 0.806 | 0.983 | 0.995 | 0.997 |\n| EMO | 0.698 | 0.955 | 0.990 | 0.997 | \\| | 0.762 | 0.972 | 0.993 | 0.996 |\n| Human | 0.687 | 0.948 | 0.988 | 0.996 | \\| | 0.764 | 0.974 | 0.993 | 0.997 |\n\nReference: \n\n[1] Zhang et al.: MIXCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies. ACL 2023 \n\n[2] Eikema et al.: Is MAP decoding all you need? the inadequacy of the mode in neural machine translation. ICCL 2020"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700108907154,
                "cdate": 1700108907154,
                "tmdate": 1700108907154,
                "mdate": 1700108907154,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]