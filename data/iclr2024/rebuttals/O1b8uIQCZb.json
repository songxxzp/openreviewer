[
    {
        "title": "VEGA: Visual Expression Guidance for Referring Expression Segmentation"
    },
    {
        "review": {
            "id": "bTOClSqrCX",
            "forum": "O1b8uIQCZb",
            "replyto": "O1b8uIQCZb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission903/Reviewer_qp2v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission903/Reviewer_qp2v"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a visual expression guidance framework for referring expression segmentation, called VEGA. VEGA enables the network to refer to the visual expression that complements the linguistic expression information by providing relevant visual information to the target regions. A visual information selection module is introduced to select the semantic visual information related to the target regions, enhancing adaptability to various language expressions and image contexts. Experiments show the proposed method obtain good performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The proposed method achieves good performance on three referring expression segmentation datasets.\n\n* The paper writing is good and easy to follow."
                },
                "weaknesses": {
                    "value": "* The innovation of the Visual Information Selection module is limited. K-Net [1] has already proposed a method for selecting top-k enhanced visual features in the universal segmentation field. SADLR applies the idea of K-Net to referring expression segmentation tasks, and PPMN [2] also applies top-k selection to enhance phrase features in a similar panoptic narrative grounding task. Therefore, the reviewers believe that this module has minimal differentiation from previous methods.\n\n* The reviewers have some doubts about the implementation of the Visual Information Selection module. In Equation (4), S_norm already sets the similarity of pixel tokens not belonging to the top-k to 0, so why is it necessary to multiply it with M? Additionally, there are two multiplication operations in the middle part of Equation (4) that use the same symbol. If they are both element-wise multiplication or matrix multiplication, it seems incorrect in terms of dimensions. Furthermore, E is already an image feature obtained by weighted summation of K pixels, so what is the purpose of performing cross-attention again?\n\n* From the ablation experiments, the performance improvement from selecting top-k and visual expression seems marginal. The state-of-the-art performance of this paper may be achieved based on a strong baseline, which does not provide strong support for the effectiveness of the proposed innovations.\n\n[1] Zhang et al. K-net: Towards unified image segmentation. NeurIPS 2021\n\n[2] Ding et al. PPMN: Pixel-Phrase Matching Network for One-Stage Panoptic Narrative Grounding. ACM MM 2022."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission903/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission903/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission903/Reviewer_qp2v"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission903/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698638149245,
            "cdate": 1698638149245,
            "tmdate": 1699636017178,
            "mdate": 1699636017178,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "0rTKXzxaV8",
            "forum": "O1b8uIQCZb",
            "replyto": "O1b8uIQCZb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission903/Reviewer_LTim"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission903/Reviewer_LTim"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a Visual Expression GuidAnce framework for referring expression segmentation, which enables the network to refer to the visual expression that complements the linguistic expression information to improve the guidance capability. The proposed semantic visual information selection leverages the similarity between word tokens and pixel tokens to select top-$k$ pixel tokens for each word token, which are used to collect the semantic information relevant to the target regions by cross-attention mechanism. Extensive experimental results on three benchmark datasets show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method is well-motivated and technically sound. The paper is well-organized and shows state-of-the-art performance. The qualitative results are also adequate the visually show its effectiveness."
                },
                "weaknesses": {
                    "value": "1. The proposed method is with limited novelty. The visual information selection module, which includes the top-k selection and visual expression extraction, is simple and straightforward.\n2. The performance gain over existing methods is marginal. Besides, according to Table 2, the proposed visual expression is of limited effect."
                },
                "questions": {
                    "value": "What is the limitation of this method? Please show some failure cases of this method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission903/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675479960,
            "cdate": 1698675479960,
            "tmdate": 1699636017104,
            "mdate": 1699636017104,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "2b7ZrDxjk2",
            "forum": "O1b8uIQCZb",
            "replyto": "O1b8uIQCZb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission903/Reviewer_PVNu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission903/Reviewer_PVNu"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new framework to tackle referring image segmentation. In contrast to other works that only use text tokens to segment the target object, the proposed framework makes use of both visual tokens and text tokens to guide the segmentation. To do so, they develop a selection module that first gets top-k image features based on their similarity with text tokens, then goes through a set of transformer layers to obtain the visual tokens. Experiments on several datasets show that the proposed method is robust and effective."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-written and easy to follow.\n2. Experimental results show improvements on 3 datasets."
                },
                "weaknesses": {
                    "value": "1. The motivation is confusing. Why do we need to use visual knowledge to complete text? If the text query is enough to localize the target object, it is not necessary to complete it. If not, how can we know the target object and complete the text?\n2. What will happen if noisy complements are generated?\n3. Many previous works also incorporate vision knowledge into text, such as [A-C]. What is the difference between the proposed method with them?\n\n[A] Key-word-aware network for referring expression image segmentation. In ECCV, 2018.\n[B] Cross-modal self-attention network for referring image segmentation, In CVPR 2019.\n[C] See-through-text grouping for referring image segmentation. In ICCV, 2019."
                },
                "questions": {
                    "value": "Please see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission903/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699079048047,
            "cdate": 1699079048047,
            "tmdate": 1699636017026,
            "mdate": 1699636017026,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]