[
    {
        "title": "Exploring Adversarial Robustness of Graph Neural Networks in Directed Graphs"
    },
    {
        "review": {
            "id": "3bZRuLFw5Q",
            "forum": "oKGDfMrD4A",
            "replyto": "oKGDfMrD4A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6767/Reviewer_pTW4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6767/Reviewer_pTW4"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into the domain of adversarial machine learning, focusing on the vulnerabilities of deep neural networks when faced with adversarial attacks. The authors present a broad study of various adversarial attack methods and their defences, evaluating their effectiveness on multiple datasets. By introducing a novel evaluation metric, the paper seeks to shed light on the nuances of designing robust models and offers a foundation for further research in the critical area of AI security."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper explores an interesting topic of trustworthy GNNs, i.e., the robustness of the GNNs for directed graphs. The authors conduct preliminary evaluations on the vulnerability of current GNN models, followed by discussing the observations. Next, they proposed a new operation to promote the robustness of GNNs."
                },
                "weaknesses": {
                    "value": "1. The thread model should be presented in the section 2.2.\n\n2. It is worth noting that the current attacking algorithm focuses on the gradient-based method. Other attack method (e.g., RL-based methods) should also be reviewed.\n\n3. Current attacks range in budgets from 25% to 100%. However, adversarial samples should be generated by considering a limited budget (e.g., k edges, k<=5). Authors are suggested to report the evaluation results on a limited budget, as the proposed 100% perturbations are extremely noticeable.\n\n4. Unclear statement. Authors are suggested to explain the details of the so-called adapative attacks.\n\n4. Limited practicality. As mentioned by the authors, out-links are generated by proactive behaviours from the source nodes, and their awareness of these out-links makes the hardness of manipulate malicious perturbations (i.e., adding out links for target nodes). This consideration makes the 1-hop perturbations practical and 2-hop perturbations unpractical. \n\n5. Logical flaws. When considering the graph structure in GNNs, the authors proposed that the A is generated from out-links. In this case, authors are suggested to explain why the 1-hop perturbations are effective for the target nodes, as these perturbations should not be involved in the message aggregation of GNNs. On the other hand, the out-link should be avoided because of their noticeability. Given these considerations, the authors are suggested to explain how to use in-links to attack target nodes in this paper. Otherwise, section 3 should conduct evaluations on GNNs devised for directed graphs.\n\n6. Confusing contributions. If this paper is designed to propose a new method to defend GNNs on directed graphs, the proposed method should be integrated into the above GNNs. If this paper aims to devise a new GNN architecture for directed graphs, comparisons should not focus only on robustness. Performance evaluations on other datasets should be presented in this paper.\n\n7. Missed evaluations. As discussed above, the proposed method only be evaluated on two datasets. The evaluations on other datasets (e.g., large-scale datasets) should be included to verify the effectiveness of the proposed method."
                },
                "questions": {
                    "value": "Refer to the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6767/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698638273086,
            "cdate": 1698638273086,
            "tmdate": 1699636780537,
            "mdate": 1699636780537,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9GlmXNzk1I",
                "forum": "oKGDfMrD4A",
                "replyto": "3bZRuLFw5Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6767/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6767/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer, thanks for your valuable comments and the recognition of our research topic, com-\nprehensive analysis and effective method. However, we believe there exist some misunderstandings\nabout our work, and we are happy to fully address all of your concerns as follows.\n\n**Q1:** The thread model should be presented in the section 2.2.\n\n**A1:** Thanks for your comments. We already include the attack principles in Section 2.2\nand the experimental attack setting in Section 4.1. Because of the space limit, we put the\nthe details of attack algorithm in appendix A.1. Please kindly let us know what specific\ndetails about the threat model you suggest us to present in Section 2.2, and we are happy to\nrevise our paper according to your suggestions.\n\n**Q2:** It is worth noting that the current attacking algorithm focuses on the gradient-based method.\nOther attack method (e.g., RL-based methods) should also be reviewed.\n\n**A2:**  Thank you for your suggestions. RL-based methods (e.g., RL-S2V [3] and ReWatt\n[2]) is designed for black-box attack, which is much weaker than the white-box gradient-\nbased attack used in our paper. As we already shown in our comprehensive evaluation, even\nthe gray-box transfer attack (which is stronger than the black-box RL attack), the evaluation\ncan suffer from a very misleading and strong false sense of robustness. This is why we\nchoose to evaluate under the strongest gradient-based attack we have found so far. While we\nbelieve our method will surely work under RL attack evaluation, there could be significant\nover-estimation of rosbutness. Therefore, evaluation under RL-based method does not align\nwith our goal of avoiding a false sense of security.\n\nTo obtain a theoretically guaranteed robustness evaluation, we also provide an additional\nevaluation of the certified robustness. Specifically, we leverage the randomized smoothing\non GNNs in Wang et al (2021) to evaluate the certified accuracy of the smoothed GCN.\nThe experimental results in the following table show that our BBRW-GCN can effectively\noutperform the GCN in terms of certified robustness. This result demonstrates that our\nadvantage is theoretically guaranteed regardless of the specific attack algorithms. We believe\nour comprehensive evaluations already provide sufficient evidence to support the significant\nimprovements of our method.\n\n- [1] Mujkanovic, Felix, et al. \u201dAre Defenses for Graph Neural Networks Robust?.\u201d Advances\nin Neural Information Processing Systems 35 (2022): 8954-8968.\n- [2] Ma, Y., Wang, S., Derr, T., Wu, L., & Tang, J. (2019). Attacking graph convolutional\nnetworks via rewiring. arXiv preprint arXiv:1906.03750.\n- [3] Dai, H., Li, H., Tian, T., Huang, X., Wang, L., Zhu, J., & Song, L. (2018, July).\nAdversarial attack on graph structured data. In International conference on machine learning\n(pp. 1115-1124). PMLR.\n\n\n### Table: Certified accuracy (%) (Cora-ML)\n\n| Perturbation Size | 0      | 1      | 2      | 3  | 4  |\n|-------------------|--------|--------|--------|----|----|\n| GCN               | 61.39% | 59.18% | 38.78% | 0% | 0% |\n| BBRW-GCN          | 83.67% | 81.63% | 73.47% | 59.39% | 0% |"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700474965221,
                "cdate": 1700474965221,
                "tmdate": 1700474965221,
                "mdate": 1700474965221,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1LRPfUJtY9",
            "forum": "oKGDfMrD4A",
            "replyto": "oKGDfMrD4A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6767/Reviewer_54vV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6767/Reviewer_54vV"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the adversarial attack robustness on directed graphs. This paper introduces a directed attack setting, differentiating between out-link and in-link attacks. The authors propose a message-passing layer, Biased Bidirectional Random Walk (BBRW). The experiments demonstrate the robustness of BBRW."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper focuses on an important problem, the adversarial robustness of GNN.\n2. This paper proposes Biased Bidirectional Random Walk (BBRW) with theoretical analysis.\n3. Experiments show the robustness of BBRW."
                },
                "weaknesses": {
                    "value": "1. This paper lacks empirical evaluations on larger datasets, such as ogb datasets[1] or reddit[2], which makes us concerned about usefulness on large networks. \n2. This work lacks some necessary baselines, making the experiments unreliable. It is recommended to add the adversarial training method FLAG[3], as well as graph purification methods GARNET[4], ProGNN[5], and STABLE[6]. \n3. The experimental settings are unclear.\n    a) Section 4.1 states, \"We randomly select 20 target nodes per split for robustness evaluation.\" If my understanding is correct, does this mean 20 nodes are selected per split for training, validation, and testing, totaling 60 nodes? If so, there are 20 nodes from the training set, and is it reasonable to evaluate accuracy on the training set?\n    b) Regarding \"multiple link budgets \u2206 \u2208 { 0%, 25%, 50%, 100% } of the target node\u2019s total degree,\" when \u2206=100%, are the attack still imperceptible? Is it necessary to preserve the original label prediction?\n    c) It would be useful to report the model's accuracy when the attack perturbation is at 5%, 10%, 15%, and 20%, as done in other studies[5,6].\n4. The high accuracy of the GCN under 25% adaptive attack does not align with the findings of the original paper[7]. Please provide performance under the same adaptive attack settings as in the original paper[7].\n\n[1] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. NeurIPS \u201920.\n[2] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. NeurIPS \u201917.\n[3] Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor, and Tom Goldstein. Robust optimization as data augmentation for large-scale graphs, CVPR\u201922.\n[4] Chenhui Deng, Xiuyu Li, Zhuo Feng, and Zhiru Zhang. GARNET: reduced-rank topology learning for robust and scalable graph neural networks. In LoG \u201822\n[5] Wei Jin, Yao Ma, Xiaorui Liu, Xian-Feng Tang, Suhang Wang, and Jiliang Tang. Graph structure learning for robust graph neural networks. KDD \u201920\n[6] Kuan Li, Yang Liu, Xiang Ao, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He. Reliable representations make a stronger defender: Unsupervised structure re\ufb01nement for robust gnn. KDD \u201922.\n[7] Felix Mujkanovic, Simon Geisler, Stephan G\u00fcnnemann, and Aleksandar Bojchevski. Are defenses for graph neural networks robust? NeurIPS \u201922."
                },
                "questions": {
                    "value": "1. The performance of BBRW is incredible, as mentioned in Section 4.3. What contributes to its effectiveness?\n2. How does BBRW handle node injection attacks[1,2]? In these attacks, only nodes or edges are injected, with the injected edges being directed. This scenario is practical and worth exploring.\n\n[1] Xu Zou, Qinkai Zheng, Yuxiao Dong, Xinyu Guan, Evgeny Kharlamov, Jialiang Lu, and Jie Tang. Tdgia: Effective injection attacks on graph neural networks. KDD \u201921.\n[2] Shuchang Tao, Qi Cao, Huawei Shen, Junjie Huang, Yunfan Wu, and Xueqi Cheng. Single node injection attack against graph neural networks. CIKM \u201921."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6767/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6767/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6767/Reviewer_54vV"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6767/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767124697,
            "cdate": 1698767124697,
            "tmdate": 1699636780420,
            "mdate": 1699636780420,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2hGLDeZAjq",
                "forum": "oKGDfMrD4A",
                "replyto": "1LRPfUJtY9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6767/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6767/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer, thanks for your valuable comments and the recognition of the significance of the\nproblem we addressed and the effectiveness of our method. However, we believe there exist some\nmisunderstandings about our work, and we are happy to fully address all of your concerns as follows.\n\n**Q1:** This paper lacks empirical evaluations on larger datasets, such as ogb datasets[1] or\nreddit[2], which makes us concerned about usefulness on large networks.\n\n**A1:** Thank you for your suggestion. Our BBRW shares the same scalability as the\nbackbone GNN models. To further validate the effectiveness of our BBRW, we include\nanother three datasets: ognb-arviv, pubmed and wikics. The budget is set as the 0%-50%\nof the target node\u2019s degree. We perform local attacks on 100 target nodes one by one and\nreport the average accuracy in the table. The experimental results show that our proposed\nmethod can be very effective in large-scale datasets and multiple relational networks, which\nis consistent with our observation of other datasets shown in our submission.\n\n### Table: Classification accuracy (%) under different perturbation rates of graph attack.\n\n| Dataset   | Model     | 0%   | 10%  | 20%  | 30%  | 40%  | 50%  |\n|-----------|-----------|------|------|------|------|------|------|\n| Ogbn-Arxiv| GCN       | 64.8%| 58.3%| 25.0%| 20.8%| 16.7%| 4.2% |\n|   Ogbn-Arxiv        | BBRW-GCN  | 64.7%| 58.3%| 41.7%| 37.5%| 33.3%| 25.0%|\n| PubMed    | GCN       | 78.1%| 66.7%| 54.2%| 37.5%| 35.9%| 33.33%|\n|  PubMed          | BBRW-GCN  | 77.4%| 76.2%| 70.8%| 66.7%| 62.5%| 60.0%|\n| WikiCS    | GCN       | 70.0%| 55.1%| 43.0%| 39.0%| 31.0%| 23.0%|\n|   WikiCS        | BBRW-GCN  | 70.0%| 68.2%| 64.0%| 57.3%| 56.0%| 49.0%|\n\n\n\n**Q2:** This work lacks some necessary baselines, making the experiments unreliable. It is recommended to add the adversarial training method FLAG[3], as well as graph purification\nmethods GARNET, ProGNN, and STABLE.\n\n**A2:** We would like to point our that one great advantage of our defense is that its contribution\nis orthogonal to existing defenses. For instance, our BBRW combined with SoftMedian\nexhibits state-of-the-art robustness in our paper. Therefore, adversarial training can be\napplied to our method for further improvement as well.\n\nMoreover, graph purification methods such as ProGNN have been claimed to be much\nweaker than SoftMedian under adaptive attacks in [1]. Therefore, we choose to compare our\nmethod with this stronger defense (SoftMedian) in our submission.\n\nWe also follow your suggestions to include the adversarial training method (GCN-AT) and\ntwo graph purification methods (GARNET and ProGNN) to further validate our statement.\nThe result shows the adversarial trained GCN (GCN-AT) still underperforms our BBRW\nmethod. In addition, our BBRW method also significantly outperforms GARNET, ProGNN,\nand SoftMedian, which is consistent with the paper [1]. Note that the experiment results in [2] show that\nthe robustness of STABLE is worse than RGCN (included in our experiments) across several\nattacks (e.g., Nettack, TDGIA, G-NIA, etc.).\n- [1] Mujkanovic, Felix, et al. \u201dAre Defenses for Graph Neural Networks Robust?.\u201d Advances\nin Neural Information Processing Systems 35 (2022): 8954-8968.\n- [2] Tao, S., Cao, Q., Shen, H., Wu, Y., Xu, B., & Cheng, X. (2023). IDEA: Invariant Causal\nDefense for Graph Adversarial Robustness. arXiv preprint arXiv:2305.15792.\n\n### Table: Classification accuracy (%)  (Cora-ML)\n\n| Method         | 0% Clean | 25% Transfer | 25% Adaptive | 50% Transfer | 50% Adaptive | 100% Transfer | 100% Adaptive |\n|----------------|----------|--------------|--------------|--------------|--------------|---------------|---------------|\n| RGCN           | 88.0\u00b16.0 | 72.5\u00b18.4     | 66.0\u00b17.7 | 44.0\u00b18.9     | 36.0\u00b15.4     | 17.5\u00b18.7      | 7.0\u00b14.6       |\n| ProGNN         | 89.0\u00b13.7 | 82.0\u00b19.8     | 76.0\u00b14.9     | 54.0\u00b113.6    | 26.0\u00b110.2    | 26.0\u00b11.2      | 10.0\u00b16.3      |\n| GCN-AT         | 86.0\u00b18.0 | 84.0\u00b110.2    | 74.0\u00b18.9     | 82.0\u00b17.5     | 52.0\u00b17.5     | 80.0\u00b111.0     | 28.0\u00b17.5      |\n| GARNET         | 89.0\u00b13.7 | 66.0\u00b13.8     | 66.0\u00b15.8     | 49.0\u00b16.6     | 38.0\u00b19.8     | 18.0\u00b17.5      | 9.0\u00b14.9       |\n| STABLE        | 89.0\u00b13.7 | 79.0\u00b18.6 | 66.0\u00b18.0 | 47.0\u00b18.7 | 35.0\u00b110.5 | 23.0\u00b16.8|  21.0\u00b13.7|\n| GCN            | 89.5\u00b16.1 | 66.0\u00b19.7     | 66.0\u00b19.7     | 40.5\u00b18.5     | 40.5\u00b18.5     | 12.0\u00b16.4      | 12.0\u00b16.4      |\n| BBRW-GCN       | 90.0\u00b15.5| 89.5\u00b16.1   | 89.0\u00b16.2     | 86.0\u00b15.4 | 85.0\u00b16.3 | 85.0\u00b17.1  | 75.0\u00b110.2 |\n| APPNP          | 90.5\u00b14.7 | 81.5\u00b19.5   | 80.5\u00b110.4   | 66.5\u00b18.7     | 66.0\u00b17.9     | 44.0\u00b19.2      | 43.5\u00b16.4      |\n| BBRW-APPNP     | 91.0\u00b14.9| 89.0\u00b15.4  | 87.5\u00b15.6     | 85.0\u00b17.1     | 83.0\u00b16.4     | 83.5\u00b16.3      | 69.0\u00b19.7      |\n| SoftMedian     | 91.5\u00b15.5 | 86.0\u00b17.0     | 83.0\u00b17.1     | 75.0 \u00b1 8.4 |73.0\u00b17.1     | 48.5\u00b111.4    | 47.5\u00b19.3      |\n| BBRW-SoftMedian| 92.0\u00b14.6| 91.5\u00b15.0  | 92.0\u00b14.6   | 89.5\u00b16.9   | 88.0\u00b15.1   | 87.0\u00b18.4    | 84.5\u00b18.8    |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700473611096,
                "cdate": 1700473611096,
                "tmdate": 1700532169726,
                "mdate": 1700532169726,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "d3R7f3SFni",
            "forum": "oKGDfMrD4A",
            "replyto": "oKGDfMrD4A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6767/Reviewer_twAK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6767/Reviewer_twAK"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the adversarial attack and robustness on directed graphs. The authors first propose a simple and more practical setting for attacks on directed graphs which restricted the perturbations on out-links. They conduct experiments with undirected graph neural networks to show there might be a false sense of robustness on directed graphs. To overcome this issue and to enhance the robustness of directed graph, the authors propose a biased bidirectional random walk, which balance the trustworthiness of out-links and in-links with a hyper-parameter $\\beta$. They also provide a comprehensive theoretical analysis on the optimal selection of this hyper-parameter. When coupled with the proposed plug-in defense strategy, this framework achieves outstanding clean accuracy and state-of-the-art robust performance against both transfer and adaptive attacks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed message-passing framework is a simple and effective approach by leveraging the directional information to enhancing the robustness of GNNs in directed graphs.\n\n2. The paper introduces a new and more realistic directed graph attack setting to overcome the limitations of existing attacks.\n\n3. The paper provides a comprehensive evaluation of the proposed framework and compares it with existing defense strategies on undirected graphs. The experiment results and findings demonstrate that the proposed framework achieves outstanding clean accuracy and state-of-the-art robust performance against both transfer and adaptive attacks.\n\n4. This work presents an innovative approach to GNN attacks, focusing on improving the robustness and trustworthiness of directed graphs."
                },
                "weaknesses": {
                    "value": "1. The description of the attack setting in this paper requires further clarification and precision. If I am understanding correctly, in terms of commonly used terms for adversarial attacks on graphs, this paper focuses on a target attack, while the transfer attack indicates the gray-box attack and adaptive attack is the white box setting. A more explicit definition and distinction between these terms would enhance the reader\u2019s comprehension and align the terminology with established norms in the field.\n\n2. The section discussing catastrophic failures due to indirect attacks seems somewhat disjointed from the existing body of work on directed graphs. The majority of experiments in Table 1 are centered around undirected graph neural networks, leading to a claim of a severe false sense of robustness against transfer attacks in these networks. Given the paper\u2019s earlier assertion that out-links and in-links should be treated distinctly in directed graphs, this leap in logic is perplexing and necessitates a more thorough explanation. A more robust motivation could potentially be achieved by exploring existing attacks on directed graph neural networks, such as DiGNN and MagNet.\n\n3. The proposed attack budget settings, encompassing 25%, 50%, and 100%, appear impractical and neglect the crucial aspect of attack unnoticeability. A more realistic and subtle approach to defining attack budgets would likely yield more applicable and insightful results.\n\n4. The paper seems to lack a discussion on related works specifically addressing attacks or defenses in directed graphs. Clarification is needed as to whether this work stands alone in its focus on directed graph robustness or if there are other relevant studies in this domain.\n\n5. The paper\u2019s approach to conducting adaptive attacks solely on undirected graph neural networks raises questions of fairness and relevance, given the unique characteristics of directed graphs. If the issue stems from challenges related to gradient backpropagation, it would be beneficial to consider relevant baselines, such as Rossi, Emanuele, et al. \"Edge Directionality Improves Learning on Heterophilic Graphs.\" arXiv preprint arXiv:2305.10498 (2023).\n\nMinor Issues:\n1. The paper would benefit from the inclusion of explanations for specific notations used, such as $\\mathcal{N}$ in section 2.2 on adversarial capacity, and $A_{sym}$ in section 3.1, to aid reader comprehension and provide a more seamless reading experience."
                },
                "questions": {
                    "value": "Please refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6767/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6767/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6767/Reviewer_twAK"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6767/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783922229,
            "cdate": 1698783922229,
            "tmdate": 1699636780237,
            "mdate": 1699636780237,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YBOc22bo2v",
                "forum": "oKGDfMrD4A",
                "replyto": "d3R7f3SFni",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6767/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6767/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer, thanks for your valuable comments and the recognition of the novelty, simplicity,\nand effectiveness of our method. However, we believe there exist some misunderstandings about our\nwork, and we are happy to fully address all of your concerns as follows.\n\n**Q1:** The description of the attack setting in this paper requires further clarification and precision.\nIf I am understanding correctly, in terms of commonly used terms for adversarial attacks on\ngraphs, this paper focuses on a target attack, while the transfer attack indicates the gray-box\nattack and adaptive attack is the white box setting. A more explicit definition and distinction\nbetween these terms would enhance the reader\u2019s comprehension and align the terminology\nwith established norms in the field.\n\n**A1:** We include the details of the attack setting in Section 4.1, and your understanding of the\nattack setting is correct. Let us further clarify them here: (1) This paper mainly focuses on\ntarget attacks. (2) Transfer attack is gray-box attack, in which the adversarial perturbations\nare transferred from the surrogate model GCN to the specific victim model. Therefore, it\ncould be much weaker in many cases. (3) Adaptive attack is white-box attack, and in \nour paper we employ PGD attack to evaluate our model, which is claimed as the strongest\nadaptive attack in [1] and verified by our experiments in Appendix A.2.\nWe will improve the clarity in our revision following your suggestions.\n\n- [1] Mujkanovic, Felix, et al. \u201dAre Defenses for Graph Neural Networks Robust?.\u201d Advances\nin Neural Information Processing Systems 35 (2022): 8954-8968."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472283151,
                "cdate": 1700472283151,
                "tmdate": 1700472283151,
                "mdate": 1700472283151,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YCIyBGy1rS",
                "forum": "oKGDfMrD4A",
                "replyto": "d3R7f3SFni",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6767/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6767/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q3:** The proposed attack budget settings, encompassing 25%, 50%, and 100%, appear impractical and neglect the crucial aspect of attack unnoticeability. A more realistic and subtle\napproach to defining attack budgets would likely yield more applicable and insightful results.\n\n**A3:** We would like to point out that evaluating target attack with budgets 25%, 50%,\nand 100% is very common in the literature, such as [1][2]. And the modified edges under\nthese budgets are still unnoticeable. Specifically, the budget is set according to the target\nnode\u2019s degree. For example, if the target node has 6 neighbors and the budget \u2206 = 50%,\nthen the attacker can only change up to 6 \u00b7 0.5 = 3 edges. Therefore, we only modify a\nunnoticeable number of edges in the whole graph. We are also happy to provide more\ncomparison under small budgets following your suggestion (from 0% to 20%). The results\nprovide similar phenomenon as the case of 25% budget and also validate the effectiveness\nof our model.\n- [1] Mujkanovic, Felix, et al. \u201dAre Defenses for Graph Neural Networks Robust?.\u201d Advances\nin Neural Information Processing Systems 35 (2022): 8954-8968.\n- [2] Z  \u0308ugner, D., Akbarnejad, A., & G  \u0308unnemann, S. (2018, July). Adversarial attacks on\nneural networks for graph data. In Proceedings of the 24th ACM SIGKDD international\nconference on knowledge discovery & data mining (pp. 2847-2856).\n\n\n### Table: Classification accuracy (%) (Cora-ML)\n\n| Method    | 0% Clean | 5% Transfer | 5% Adaptive | 10% Transfer | 10% Adaptive | 15% Transfer | 15% Adaptive | 20% Transfer | 20% Adaptive |\n|-----------|----------|-------------|-------------|--------------|--------------|--------------|--------------|--------------|--------------|\n| MLP       | 73.5\u00b17.4 | 73.5\u00b17.4    | 73.5\u00b17.4    | 73.5\u00b17.4     | 73.5\u00b17.4     | 73.5\u00b17.4     | 73.5\u00b17.4     | 73.5\u00b17.4     | 73.5\u00b17.4     |\n| MagNet    | 88.5\u00b13.2 | 88.0\u00b13.3    | \\           | 86.5\u00b13.9     | \\            | 85.0\u00b10.05477 | \\            | 78.5\u00b18.1     | \\            |\n| RGCN      | 88.0\u00b16.0 | 87.5\u00b16.7    | 87.0\u00b16.8    | 87.0\u00b16.4 | 87.0\u00b16.8   | 85.0\u00b16.7     | 84.5\u00b16.9     | 81.0\u00b18.6     | 77.5\u00b110.3            |\n| GCN       | 89.5\u00b16.1 | 88.0\u00b16.8    | 88.0\u00b16.8    | 87.5\u00b16.8   |  87.5\u00b16.8 | 83.0\u00b17.1     | 83.0\u00b17.1     | 76.5\u00b19.8     | 76.5\u00b19.8               |\n| BBRW-GCN  | 90.0\u00b15.5 | 90.0\u00b15.5    | 90.0\u00b15.5    | 90.0\u00b15.5  | 90.0\u00b15.5     | 89.5\u00b15.2     | 89.5\u00b15.6     | 89.5\u00b15.7     | 89.0\u00b17.6     | \\            |\n\n\n\n**Q4:** The paper seems to lack a discussion on related works specifically addressing attacks or\ndefenses in directed graphs. Clarification is needed as to whether this work stands alone in\nits focus on directed graph robustness or if there are other relevant studies in this domain.\n\n**A4:** We would like to emphasize that the attack and defense for directed attacks are largely\nunexplored in the literature to the best of our knowledge. Therefore, there is no attack\nspecifically designed for directed graphs, not to mention the defenses. This work provides\nthe first of such exploration by proposing new settings/algorithms from both attack and\ndefense perspectives. Our major discovery is that we can obtain significantly improved\nrobustness orthogonal to existing defense techniques by properly exploiting the valuable\ndirectional information in directed graphs. This points out an new direction and unprece-\ndented opportunities for future research, highlighting the significant contribution of this\nwork. We will make further clarifications on this point in our revision."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472594736,
                "cdate": 1700472594736,
                "tmdate": 1700474650943,
                "mdate": 1700474650943,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "c1yQfPSt6n",
            "forum": "oKGDfMrD4A",
            "replyto": "oKGDfMrD4A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6767/Reviewer_kogv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6767/Reviewer_kogv"
            ],
            "content": {
                "summary": {
                    "value": "The authors study adversarial robustness w.r.t. perturbations of the graph structure for directed graphs. They identify a gap in the litreature, namely that most previous robustness studies forcus on undirected graphs. They argue that there is an asymmetry between out-links and in-links and that in some applications it is much easier for an adversary to perturb the in-links compared to the out-links of a target node. They propose RDGA -- a modification of existing attacks with additional restrictions on the out-links of the target node. They also propose a new heuristic defense where we a tunable parameter can place different weights on the in-links and out-links. This change is ortogonal to other defense measures and can be composed with both vanilla GNNs and existing defenses."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The simplicity of the proposed defense is in my opinion its biggest strengths. The experiments suggest that there is a range of $\\beta$ values for which the clean accuracy is well mantained while showing a boost in adversarial accuracy -- sugestting that we can improve robustness without a significant trade-off.\n\nThe experimental analysis is comprehensive. The addition of adaptive attacks is especially appreciated since they are crucial for properly evaluating heuristic defenses.\n\nThe theorethical analysis is interesting even though it relies on many simplifying assumptions."
                },
                "weaknesses": {
                    "value": "A big weakness of the paper is that the evaluation is focused on citation networks only -- where for the classification task the out-links are already quite informative. It's not clear how much benefit there could be for other types of networks, where e.g. the patterns of in vs. out-links are different. \n\nThe proposed attack can be seen as a relaxation of the even more stringent indirect attack (aslo called the influencer attack by Zugner et al. (2018)) where neither the in-links nor the out-links of the target node can be modified. Therefore, it is not surprusing that the performance is somewhere between the unrestricted and the fully restricted attack. I think elaborating on this connection in the related work is warranted.\n\nStudying the directed setting is imporant, however, whether the attacker is more likely to be able to change the in-links or the out-links depends highly on the application. For example, in social networks it might be true that changing out-links is more difficult but this is not necessarily always the case. Moreover, often there is no \"attacker\" and we are conducting adversarial robustness studies to quantify e.g. the robustness to worst-case noise -- i.e. treating nature as an adversary. That is to say, I don't think that all future studies should adopt a restricted attack such as the one proposed, but rather include this as another viewpoint.\n\nI think the paper can benefit from further studying the impact of the proposed defense on other aspects of robustness:\n- Is the robustness to attribute/feature perturbation positively/negatively affected?\n- Does the proposed defense also improved certified robustness (which can be easily tested with one of the black-box randomized smoothing certificates)?\n- How is the robustness to global (untargeted) attacks affected?\n\nEssentially the question is what are the trade-offs from adopting the $\\beta$ weighted adjacency matrix."
                },
                "questions": {
                    "value": "1. In the ablation study currently the authors break down the links in terms of 1-hop, 2-hop and others. It would be intresting to see the breakdown in terms of in vs. out-links as well, i.e. show: 1-hop (always in-links), 2-hop in-link, 2-hop out-link, etc.\n2. How does this approach compare to the more stringent indirect (adaptive) attack?\n3. In Table 4 the masking rate starts at 50\\%, it would be insightful to also show lower masking rates, and in particular 0\\% which would correspond to unrestricted attacks. This will help in understanding whether the proposed defenses is \"universally\" helpful for different threat models.\n4. It should be straightforward to set different $\\beta$ values for each node, rather than a single global value. Tuning can be avoided by setting these values based on the theory."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6767/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834173046,
            "cdate": 1698834173046,
            "tmdate": 1699636780121,
            "mdate": 1699636780121,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "puMPVv5wDw",
                "forum": "oKGDfMrD4A",
                "replyto": "c1yQfPSt6n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6767/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6767/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, thank you so much for your valuable comments and your recognition of the simplicity and effectiveness of our method, our comprehensive experiments, and the theoretical analysis. We are happy to fully address all of your concerns as follows.\n\n**Q1:**  A big weakness of the paper is that the evaluation is focused on citation networks only \u2013\nwhere for the classification task the out-links are already quite informative. It\u2019s not clear\nhow much benefit there could be for other types of networks, where e.g. the patterns of in vs.\nout-links are different.\n\n**A1:** Thank you for your suggestion! Generally speaking, both the out-links and in-links can be informative when considering the robustness of GNNs. Our proposed BBRW leverages both the out-links and in-links information and has the flexibility to adjust our trust in them through hyperparameter \u03b2. Therefore, our method is generally effective for other networks.\n\nBesides citation networks, we also include the wikipedia-based relational network called WikiCS. We use the dataset in [1] and follow the dataset preprocessing setting in [2]. We perform the experiments on WikiCS across various budgets and report the result in the following table. The following results show the significant robustness of BBRW and validate that our BBRW can be generalized to different types of networks beyond citation networks.\n\nReferences:\n- [1] P\u00e9ter Mernyei and Catalina Cangea. Wiki-cs: A wikipedia-based benchmark for graph neural networks. arXiv preprint arXiv:2007.02901, 2020.\n- [2] Zhang, X., He, Y., Brugnone, N., Perlmutter, M., & Hirn, M. (2021). Magnet: A neural network for directed graphs. Advances in neural information processing systems, 34, 27003-27015.\n\n### Table : Classification accuracy (%) under different perturbation rates of graph attack. (WikiCS)\n\n| Method   | Clean (0%)    | Transfer (25%)  | Adaptive (25%) | Transfer (50%) | Adaptive (50%) | Transfer (100%)  | Adaptive (100%) |\n|----------|-----------|-----------|----------|-----------|----------|-----------|----------|\n| MLP      | 44.0\u00b19.2  | 44.0\u00b19.2  | 44.0\u00b19.2 | 44.0\u00b19.2  | 44.0\u00b19.2 | 44.0\u00b19.2  | 44.0\u00b19.2 |\n| MagNet   | 64.0\u00b15.8  | 40.0\u00b13.2  | \\        | 29.0\u00b17.3  | \\        | 25.0\u00b17.7  | \\        |\n| RGCN     | 70.0\u00b14.5  | 45.0\u00b111.8 | 39.0\u00b19.7 | 36.0\u00b112.4 | 23.0\u00b112.9| 22.0\u00b16.0  | 17.0\u00b19.8 |\n| GCN      | 70.0\u00b18.4  | 39.0\u00b112.4 | 39.0\u00b112.4| 23.0\u00b18.7  | 23.0\u00b18.7 | 16.0\u00b18.6  | 16.0\u00b18.6 |\n| BBRW-GCN | 70.0\u00b18.3  | 62.0\u00b19.3  | 61.0\u00b110.7| 55.0\u00b110.5 | 49.0\u00b112.4| 51.0\u00b113.2 | 39.0\u00b113.2|\n\n**Q2:** The proposed attack can be seen as a relaxation of the even more stringent indirect attack\n(also called the influencer attack by Zugner et al. (2018)) where neither the in-links nor\nthe out-links of the target node can be modified. Therefore, it is not surprising that the\nperformance is somewhere between the unrestricted and the fully restricted attack. I think\nelaborating on this connection in the related work is warranted. How does this approach\ncompare to the more stringent indirect (adaptive) attack?\n\n**A2:** Thank you for your thoughtful comments. Exactly, our proposed attack RDGA can\nbe seen as a bridge between direct attack and indirect attack. As shown in Figure 3, the attacker\ntries to make a decision to maximize the attack strength between in-link direct attack and\nindirect attack. Therefore, RDGA is stronger than the indirect (adaptive) attack. To verify\nthis conclusion, we also provide the comparison of the strengths of RDGA and Indirect\nadaptive attack as follows. The comparison indicates that the proposed RDGA attack is\nindeed stronger than the Indirect attack setting by Zugner et al (2018) as you suggest. We\nwill discuss this connection in our revision.\n\n### Table: Classification accuracy (%) of BBRW-GCN (Citeseer)\n\n|  Budget        | 0%        | 25%       | 50%       | 100%      |\n|----------|-----------|-----------|-----------|-----------|\n| RDGA     | 61.5\u00b17.4  | 43.0\u00b110.3 | 27.0\u00b114.4 | 20.5\u00b19.6  |\n| Indirect | 61.5\u00b17.4  | 60.0\u00b18.4  | 46.0\u00b110.7 | 35.0\u00b17.1  |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471465073,
                "cdate": 1700471465073,
                "tmdate": 1700471465073,
                "mdate": 1700471465073,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k4kNc3RhwY",
                "forum": "oKGDfMrD4A",
                "replyto": "GhOLaf9u60",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6767/Reviewer_kogv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6767/Reviewer_kogv"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response. I keep my score in favor of acceptance."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729047429,
                "cdate": 1700729047429,
                "tmdate": 1700729047429,
                "mdate": 1700729047429,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]