[
    {
        "title": "The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry"
    },
    {
        "review": {
            "id": "a9VIdkeDR1",
            "forum": "4g02l2N2Nx",
            "replyto": "4g02l2N2Nx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8562/Reviewer_PsQx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8562/Reviewer_PsQx"
            ],
            "content": {
                "summary": {
                    "value": "The authors found that prior linear attentions miss two key properties of softmax attention that are crucial for good performance: having low-entropy (spiky) weights and maintaining dot-product monotonicity. A new learnable linear attention called Hedgehog was introduced. Hedgehog retains these crucial properties while keeping linear complexity. It uses trainable MLPs to produce attention weights similar to softmax attention. The results indicate that Hedgehog can achieve over 99% of the standard Transformer's performance, outdoing previous linear attentions in various settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Technically sound well-written paper dealing with an important research task."
                },
                "weaknesses": {
                    "value": "Table 3 is confusing since there are many more higher performing solutions for LRA: https://paperswithcode.com/sota/long-range-modeling-on-lra\nTaylor expansion in transformers is well-known approach: eg. https://arxiv.org/pdf/2206.08898.pdf\nWhile Hedgehog aims to retain the properties of softmax attention in a linear attention framework, this could introduce its own complexities in terms of implementation and optimization."
                },
                "questions": {
                    "value": "Since the function mimicking softmax is learned how scalable it is to other modalities?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8562/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834672634,
            "cdate": 1698834672634,
            "tmdate": 1699637070701,
            "mdate": 1699637070701,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "usgrztGzjr",
                "forum": "4g02l2N2Nx",
                "replyto": "a9VIdkeDR1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8562/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8562/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PsQx"
                    },
                    "comment": {
                        "value": "Thank you for your review! We appreciate that you find our paper well-written and dealing with an important research task. \n\nWe also appreciate your comments on the LRA comparisons and scalability to other modalities. We have updated the paper to address these, where we:\n1. Clarify the scope of the LRA comparison (where we aim to compare against Transformers and attention approximations)\n2. Clarify the framing of the Taylor approximation here vs in past work\n2. Add additional results showing Hedgehog can apply to additional modalities (e.g., images via Vision Transformers). \n3. Provide additional code blocks and algorithms (App A.3) to show how Hedgehog training can be implemented in simple training loops, with minimal extra complexity or optimization overhead.\n\n---\n\n**LRA comparison**  \nWe clarify that although recently many non-Transformer models have shown impressive results outperforming standard (softmax) Transformers on LRA (e.g., those based on deep state-space models like S4 [1]), because our work focuses on how to improve linear attentions and fits in the Transformer framework, we focus the comparison against other attention-based methods. \n\nWe have updated the framing in Sec 5.2. to clarify this. \n\n[1] Efficiently Modeling Long Sequences with Structured State Spaces (Gu et al., 2021).\n\n---\n\n**Taylor approximation**  \nThanks as well for the comments on Taylor series approximations being well-known. We updated Sec. 4.1 to reference the original citations. We would also like to clarify that although prior works explore Taylor series approximations for the exponential, they do so under different motivations, (e.g., as linked, as a replacement for the softmax used in the final layer of a classification network).   \n* Here we use the Taylor Series approximation to empirically study its properties for a good linear attention mechanism. We further point out the practical inefficiencies with the Taylor approximation with respect to prior linear attentions, and use these limitations to motivate the development of our Hedgehog method. \n---\n\n**Scaling to other modalities**  \nThanks for this question. We ran additional experiments for Hedgehog applied to vision, and found Hedgehog can recover 99% of pretrained Vision Transformer top-1 accuracy when doing finetuned conversion for ImageNet-1K. \n\nWe use the readily available ViT-base model (vit-base-patch16-224), and find that while the original softmax attention version gets 80.3% top-1 accuracy, Hedgehog obtains 79.5% top-1 accuracy. We added additional experimental details and comparisons in the updated draft (Section 5.4, Appendix B.4).\n\n---\n\n**Hedgehog implementation complexity**  \nWe updated the appendix (App. A.3) to show how Hedgehog training can be performed in a simple training loop, where we can jointly optimize all Hedgehog MLPs with the same optimizer in an end-to-end fashion. We make use of popular pretrained Transformer APIs such as HuggingFace transformers to output the attentions at each layer.  \n\nThis enables training without much more complexity than a two-step process where we (1) freeze all original model parameters, insert the Hedgehog feature maps, and train the Hedgehogs end-to-end with the attention distillation loss, before (2) unfreezing the original model parameters and training the entire model end-to-end on the task-loss."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8562/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613971327,
                "cdate": 1700613971327,
                "tmdate": 1700613971327,
                "mdate": 1700613971327,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4M84HaQsnE",
            "forum": "4g02l2N2Nx",
            "replyto": "4g02l2N2Nx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8562/Reviewer_y6Q1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8562/Reviewer_y6Q1"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes *Hedgehog*, a new linear attention mechanism for Transformers. \n\nThe authors start by noting that previous linear attention mechanisms still lag considerable behind full quadratic attention. The authors then investigate why this is the case, by studing the properties of the attention maps produced by full attention and linear approximations. They find that two properties in full attention that previous linear attention methods seem to lack:\n\n- low-entropy \"spiky\" attention weights\n- monotonicity over query-key dot products\n\nThe authors then attempt to measure the importance of these two properties by inducing these properties in some test tasks, finding that these properties seem to correlate with performance of the trained model.\n\nFinally, the authors propose Hedgehog: the core idea being using linear map from the query/key space into a lower-dimensional feature space, but learning this linear map to *explicitly* approximate the final attention maps produced by dense attention. \n\nExperimental results are quite convincing: Hedgehog outperforms most other linear attention methods on multiple tasks (including LM perplexity and popular downstream tasks), both when training models from scratch and when \u201clinearizing\u201d models with dense attention."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper provides a quite consistent story: they identified a problem (current linear-attention LLMs/transformers don\u2019t work that well), studied potential reasons for their failure mode and verified their importance for the performance of models (the two properties of attention) and proposed and experimentally verify a fix to these problems that leads to linear-attention models that get closer to dense attention models)\n- The paper is well-written, and the problem studied very relevant: the quadratic complexity is one of the main bottlenecks preventing the scaling of transformers to longer contexts.\n- The identified properties of dense attention (and the subsequent analysis of their relation to downstream performance) are super interesting (and could be a contribution almost independently of the proposed method)\n- The proposed Hedgehog method seems to be effective for the model-size studied in this work, with Hedgehog leading to much less performance degradation than other linear-attention approaches (when compared to dense-attention model)"
                },
                "weaknesses": {
                    "value": "- My main criticism of the study is that they only studied relatively small transformer models:  they only studied 125M decoder only models. Previous work [1] has shown that linear attention approach are less amenable to scaling than dense attention (meaning as models get larger, the gap between dense and linear attention increases) and so it\u2019s unclear if this approach would still lead to such small gaps with dense attentions for 1B+ models (which IMO, is the main use case for this technique). While I already really liked the paper, I think this paper could be much stronger impact on the community if results were validated on larger LLMs.\n\n[1] https://arxiv.org/pdf/2207.10551.pdf"
                },
                "questions": {
                    "value": "- In the train-from-scratch setting, how is the Hedgehog attention initialize?. From my understanding, Hedgehog requires an already-trained dense attention model to finetune the feature map. Does this imply that in this scenario we still need to train a dense attention model anyway?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8562/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698933811058,
            "cdate": 1698933811058,
            "tmdate": 1699637070552,
            "mdate": 1699637070552,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KSwd1IVo6D",
                "forum": "4g02l2N2Nx",
                "replyto": "4M84HaQsnE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8562/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8562/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer y6Q1"
                    },
                    "comment": {
                        "value": "Thank you very much for your review! We are glad that you appreciated our work, and are likewise excited about scaling Hedgehog up to larger models. We address your questions on this scaling and clarification of Hedgehog initialization when training-from-scratch below.\n\n**Hedgehog scaling to Llama-2 7B**  \nInspired by your comments, we added initial experiments showing that Hedgehog enables conversion of Llama-2 7B models into linear attention versions, where subsequent parameter-efficient finetuning results can significantly improve performance over the base model on a downstream summarization task. Furthermore, we found prior attempts to swap the attentions for linear attentions did not seem to work. \n\nWe describe these findings below, but please find additional results (Sec. 5.4, Table 11; Appendix C.3)  and experimental details (Appendix B.5) in our updated draft.\n\n***Llama-2 linearization setup***    \nTo do the Llama-2 linearization, we first train Hedgehog MLPs to match the attentions layer-by-layer and head-by-head with frozen Llama-2 weights on downstream task data (e.g., the SAMSum summarization corpus [1]). Doing so amounts to training 0.5% of the original model parameters. Following this attention distillation, we replace the original Llama attentions with the Hedgehog attentions (keeping the original Llama weights for query, key, value, and output projections, but computing the QKV interactions via linear attention after applying the Hedgehog feature maps). \n\nWe then apply low-rank adaptation (LoRA) [2] to the query, key, value, and output projections, and finetune the entire model on the downstream task (summarization). \n\n***Llama-2 linearization results***   \nTable 1 shows our results, where we report ROUGE metrics (unigram, bigram, longest common subsequence) for test set generations. As baselines, we compare with zero-shot Llama-2 model and Llama-2 with LoRA. We also compare with linearizing Llama-2 using the prior linear attention conversion method, Transformer-to-RNN (T2R), followed by LoRA finetuning. \n\n|                     |  ROUGE-1  |  ROUGE-2  |  ROUGE-L  |\n|---------------------|:----:|:----:|:----:|\n| Softmax (Zero-shot) | 19.3 |  6.8 | 14.9 |\n| Softmax (LoRA)      | 51.1 | 27.6 | 43.5 |\n| T2R (LoRA)          |  2.8 |  0.0 |  2.6 |\n| Hedgehog (LoRA)     | 47.4 | 23.4 | 39.1 |\n\nWhile there exist a smaller quality gap with the full softmax-attention Llama-2 with finetuning, we find Hedgehog enables linear attention finetuning that significantly improves the base zero-shot performance (e.g., 14.9 to 39.1 ROUGE-L). Meanwhile, we struggled to get prior linearization techniques to work (2.6 ROUGE-L with T2R). \n\nFor more granular evaluation of summarization quality, we encourage viewing the sample generations among the LoRA-finetuned models in Appendix C.3. Here we find that the Hedgehog-Llama-2 models are able to produce coherent summaries similar to standard Llama-2, while the T2R-Llama-2 models struggle to generate coherent text. \n\n\n[1] SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization (Gliwa et al., 2019)\n\n[2] LoRA: Low-Rank Adaptation of Large Language Models (Hu et al., 2021)\n\n---\n\n**Clarification on Hedgehog attention initialization when training-from-scratch**  \nWe clarify that for training-from-scratch, we actually do not do the first step of attention-weight distillation. We instead initialize the Hedgehog MLPs as identity matrices (keeping the exponential feature map form discussed in Eq. 3), and train the entire model end-to-end just with the task-specific loss. Thus in this case we do not need to train a dense attention model first. \n\nWe updated the draft to clarify this in Section 5.2, and provide additional implementation details with pseudocode in Appendix A.2 and A.3."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8562/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612586542,
                "cdate": 1700612586542,
                "tmdate": 1700612586542,
                "mdate": 1700612586542,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Txr5iZ6dGK",
            "forum": "4g02l2N2Nx",
            "replyto": "4g02l2N2Nx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8562/Reviewer_ZtxG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8562/Reviewer_ZtxG"
            ],
            "content": {
                "summary": {
                    "value": "This paper address the quadratic scaling of attention mechanism in transformers. There are 2 main contributions:\n\n1. Defining characteristics that make an attention mechanism work well: (a) low-entropy \"spikyness\" and (b) dot-product monotonicity\n\n2. Two new attention mechanisms to evaluate their claims. Taylor and Hedgehog\n\n They evaluate attention mechanisms in several scenarios (1) Training-from-scratch, (2) Finetuned-conversion, and (3) Pretrained-conversion."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The exploration into \"spikyness\" and monotonicity is interesting and novel to me.\n\nThe regime for evaluating their attention mechanism is thorough and covers a wide variety of situations.\n\nThe ablations are well done and the experimental results on benchmarks and efficiency are strong."
                },
                "weaknesses": {
                    "value": "I found the presentation a little confusing. In some figures and tables only Hedgehog is shown. In others only Taylor is shown. Should both be evaluated?\n\nIs the GPT-2 column in Table 6 unfair? It seems they should finetune with the full attention mechanism?"
                },
                "questions": {
                    "value": "There doesn't seem to be much downside to this method except for training from scratch? Should everyone be finetuning and converting their pretrained models to Hedgehog?\n\nWas there any investigation into the learned weights of Hedgehog to gain intuition on how it's approximating softmax?\n\nHow does Hedgehog's softmax approximation do with length generalization (sequence length longer than training data) or data that is out of distribution?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8562/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698963624366,
            "cdate": 1698963624366,
            "tmdate": 1699637070439,
            "mdate": 1699637070439,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CQCDoY4zXv",
                "forum": "4g02l2N2Nx",
                "replyto": "Txr5iZ6dGK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8562/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8562/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZtxG (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your review! We appreciate finding our work interesting and novel, with thorough and strong benchmarks. \n\nWe also appreciate the feedback on presentation on comparisons between the Taylor approximation for exponentials and Hedgehog, and comparison to GPT-2. We first address these below, and have updated the draft to reflect these changes. \n\nIn the next reply thread, please find our answers to your questions on Hedgehog generalization and understanding of the learned weights. We appreciate the insightful questions, and have updated both the main paper and appendix to include additional analysis inspired by your comments. \n\n**Comparison between Hedgehog and Taylor Series exponential approximation**  \nWe clarify that we study the Taylor Series approximation as *motivation* for our Hedgehog method. We primarily aim to recover the desirable expressive traits of the Taylor approximation, while overcoming its efficiency limitations (despite being computable as a linear attention, the Taylor approx. introduces extra inefficiencies via larger feature dimension (c.f. \u201cCaveats\u201d in Section 4.1). \n\nAs such, for our main results we only evaluate Hedgehog. \n\nFrom your feedback, we updated our motivating analysis in Section 3 to show that Hedgehog performs comparably on all the Taylor series results. \n* In particular, we update Fig. 2 and 3 to show Hedgehog indeed recovers the spiky and monotonic properties, and update Table 3 to show that Hedgehog also solves the associative recall task. \n\n**Comparison to GPT-2 column in Table 6**  \nThanks for pointing this out. We updated Table 6 (now Table 10) to also include a finetuned version of GPT-2 with the full attention mechanism (reproduced below). \n\nWhile the fully finetuned GPT-2 gets lowest PPL at 15.8, this incurs relatively more expensive quadratic attention during finetuning. Meanwhile, Hedgehog still significantly closes the gap among subquadratic models, via finetuning with only linear attention.  \n\nOriginally, we intended the evaluation in Table 6 to test if Hedgehog enables significant downstream transfer, i.e., we can successfully finetune the linearized GPT-2 on new tasks it is not already good at. Hence we included the zero-shot GPT-2 as a baseline reference, where the large improvement in perplexity (from 28.0 to 16.7) suggests Hedgehog not only enables recovering the base capabilities (As explored in Sec 5.3 with the BERT models), but also further supports finetuning on new tasks. \n\nWe included comparisons to other subquadratic architectures (H3, Hyena) to show the effectiveness of this pretrained conversion; i.e., instead of training a new subquadratic model from scratch, we can start with existing powerful but expensive-to-compute quadratic models, and get a \u201cbest-of-both-worlds\u201d situation with Hedgehog by leveraging the pretrained weights but via more efficient linear attention."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8562/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612232667,
                "cdate": 1700612232667,
                "tmdate": 1700612232667,
                "mdate": 1700612232667,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hxMaQtFyRF",
                "forum": "4g02l2N2Nx",
                "replyto": "Txr5iZ6dGK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8562/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8562/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZtxG (2/2)"
                    },
                    "comment": {
                        "value": "**Investigation into the learned weights of Hedgehog for intuition**    \nWe explored this under two angles and added our findings in the updated draft. \n\nFirst, we tried to better understand if the learned weights are actually important, or if just the exponential feature map suffices. We expanded Sec. 4.3 (now Sec. 5.1) to visualize the attention weights of Hedgehog without training (Fig. 8) in comparison to with training (Fig. 7) and found that learning the MLP weights (vs just replacing the MLPs with the identity matrix) lead to significantly better matching of softmax attentions. Please see additional visualizations in Appendix C.3 and C.4 for additional results. \n\nWe also tried visualizing the actual MLP weights of trained Hedgehog layers, but were not able to find any discernible patterns or insights as of yet. We definitely think further analysis into how these weights \u201cshape\u201d the softmax over time could be interesting to explore for additional understanding.\n\n**How does Hedgehog's softmax approximation do with length and data generalization?**  \nThanks for these questions! We explored these further in our updated section on benchmarking Hedgehog (now Sec. 5.1) and added additional studies in the appendix (C.2.1 - C.2.3) to dig deeper. We describe some findings below, but please find our full investigations in the updated draft. \n\n***New data generalization***  \nFirst, on new data, we find Hedgehog feature maps learned on one dataset can continue to approximate softmax attentions for another. We trained Hedgehog MLPs for BERT models on the CoLA GLUE task, and evaluated how well the resulting attention weights would match in comparison to softmax attention weights on different GLUE tasks (using KL divergence to measure). We also compare with the attentions using alternative linear attention feature maps from prior work (1 + ELU, Performer, CosFormer). \n\nTable 2 below shows that despite being trained to match softmax attention weights over CoLA data, Hedgehog feature maps continue to better match the softmax attentions than alternative linear attentions (reporting KL divergence, lower is better; please see Fig. 9, 10, 11 for attention weight visualizations, Table 14 for results in updated draft, Table 4 for abridged in main paper). We do note a drop in fidelity (higher KL divergence) and think investigating and overcoming the source of this would be exciting future work. \n\n| Method          |  CoLA |  MNLI |  MRPC |  QNLI |  QQP  |  RTE  | SST-2 | STS-B |\n|-----------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n| Hedgehog (CoLA) | 0.173 | 0.340 | 0.652 | 0.673 | 0.374 | 0.932 | 0.211 | 0.275 |\n| 1 + ELU         | 1.231 | 1.500 | 2.150 | 1.947 | 1.509 | 2.491 | 1.505 | 1.285 |\n| Performer       | 1.293 | 1.592 | 2.239 | 2.059 | 1.588 | 2.594 | 1.558 | 1.352 |\n| CosFormer       | 1.191 | 1.341 | 1.987 | 1.840 | 1.330 | 2.398 | 1.443 | 1.142 |\n\n***Longer length generalization***  \nSecond, on new lengths, we find that the lower KL divergence achieved via Hedgehog also remains consistently low when transferring to longer samples. For this, we first combine individual CoLA samples into longer sequences up to 4096 tokens long. We then evaluate how Hedgehog MLPs trained on individual CoLA samples generalize to matching the softmax attentions when computed over these longer sequences in Table 3 below (Table 5 in updated draft).\n\n| Sequence Length |  256  |  1024 | 2048 |  4096 |\n|-----------------|:-----:|:-----:|:----:|:-----:|\n| KL Divergence   | 0.182 | 0.187 | 0.19 | 0.181 |\n\nWe think these questions on generalization are particularly exciting, perhaps motivating an interesting line of work for \u201cpretrained\u201d linear attention feature maps. Ideally, we could train feature maps once, e.g., distilling the attention weights from some base Transformer model over some pretraining corpus (perhaps similar to pretraining large LLMs today), and seeing if the learned feature maps could then be a \u201cuniversal\u201d drop-in replacement for softmax attentions (applicable to new data, but also perhaps new models)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8562/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612365992,
                "cdate": 1700612365992,
                "tmdate": 1700615420490,
                "mdate": 1700615420490,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]