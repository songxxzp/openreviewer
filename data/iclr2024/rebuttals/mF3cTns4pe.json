[
    {
        "title": "Sum-Product-Set Networks: Deep Tractable Models for Tree-Structured Graphs"
    },
    {
        "review": {
            "id": "pQAdgIWS09",
            "forum": "mF3cTns4pe",
            "replyto": "mF3cTns4pe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7005/Reviewer_9xzE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7005/Reviewer_9xzE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes sum-product-set networks (SPSN), a tractable probabilistic model for tree-structured data. Interestingly, the structure of the tree and dimensionality of the data is assumed to be random rather than fixed. This is achieved through the use of random feature sets (RFS), which allows for the specification of distributions over sets of varying length taking value in some domain. RFS are integrated with Sum-Product Networks (SPN) through the use of a schema/template for the data, which hierarchically specifies the fixed parts of the tree structure (heterogenous nodes) and the variable parts (homogenous nodes). Under some (strong) assumptions on the set unit distributions and the query, inference is shown to be tractable in SPSNs. Empirical results show that the classification performance is slightly worse than, but competitive with, approaches based on neural networks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper presents an (as far as I am aware) novel problem of developing a tractable probabilistic model for tree-structured data, where the graph of the tree is itself random. Such data structures naturally arise in many areas, such as XML/JSON, scientific domains, natural language (e.g. syntax trees), and relational data. The proposed solution, SPSNs, are a well-designed variant of sum-product networks that utilizes random features sets (set units) to allow tree nodes to have a random number of children while maintaining tractability.\n\n- Novel and adept application of RFS theory to deep tractable model architectures (SPNs). This enables the specification of distributions over hierarchical random trees.\n- The work could have significant impact in pushing the application of TPMs towards new domains, such as natural language processing. \n- The clarity and technical quality of the paper is excellent. In particular, Figure 2 was very useful for understanding the role of sum, product and set units in relation to the tree schema."
                },
                "weaknesses": {
                    "value": "- The requirement of full independence in the distribution of a set unit seems quite stringent and potentially unrealistic. For example, for the mutagenesis example in Figure 1, this would correspond to atoms in a molecule being independent (conditional on the molecule size). \n- On the empirical side, to justify the importance of tractability it would be useful to test some example queries on the learned SPSNs, and their domain-specific interpretation."
                },
                "questions": {
                    "value": "- Is it possible to relax the assumption of full independence in a set unit, or is this a fundamental limitation? E.g. through partial exchangability for the set unit distributions?\n- Details on how SPSNs are learned seem to be missing. It seems the structure of the SPSN is fixed (Pg 4.), but how are the parameters learned? Is the cardinality distribution learned, and if so, how is it parameterized?\n- Is there any technical reason why the SPSN architecture cannot be extended to DAG, rather than tree structured data? \n- The related work section is thorough, but, especially considering that the tested datasets all come from a relational schema, it would be useful to better understand the relationship with relational SPNs. For example, how does one translate a relational schema to a tree schema as in Figures 4-11 (in what way is relational data \"a particular form of graph-structured data\")?\n- It is not clear what the result is in Proposition 2 (the statement seems more like a definition)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7005/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7005/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7005/Reviewer_9xzE"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7005/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831219436,
            "cdate": 1698831219436,
            "tmdate": 1699636821062,
            "mdate": 1699636821062,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G6LKsULLVp",
                "forum": "mF3cTns4pe",
                "replyto": "pQAdgIWS09",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7005/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7005/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*The requirement of full independence (...)*\n\nWe reply on this in one of your questions below.\n\n*On the empirical side (...)*\n\nCurrently, the only queries we consider are the marginal ones used for dealing with the missing values in Section 5. We will provide more queries in future work. Thank you for this constructive suggestion.\n\n*Is it possible to relax the assumption of full independence in a set unit (...)*\n\nYes, the independence assumption can be relaxed. To satisfy our key modeling assumption, i.e., the homogeneous nodes are RFSs, the feature density would have to be represented by a sub-SPSN that is fully exchangeable under the $m!$ re-orderings of the elements of the set (and, indeed, as you suggest, the partial exchangeability can apply within the elements). This can be accomplished in two possible ways. The first one lies in modeling the interactions between the elements of the set only through the sum units, whereas the product unis will be applied consequently to model interactions withing the elements. The second one is to make the feature density fully exchangeable by marginalizing over all $m!$ permutations of the elements. This option imposes no restrictions on modeling the feature density (apart from Definition 6 to preserve the tractability), but the computational cost would be very high as discussed in Section B (Exchangeability of RFSs).\n\nNonetheless, we do not see the independence assumption as a limitation in the context of the JSON files. The homogeneous nodes have the same schema, i.e., their structure is the same for all elements of the set; therefore, we see the elements as instances of the same random variable.\n\n*Details on how SPSNs are learned seem to be missing. It seems the structure of the SPSN is fixed (Pg 4.), but how are the parameters learned?*\n\nThe details about the learning are in Section E. We perform the grid-search over the hyper-parameters that determine the structure of the SPSN network. Then, we select the best network based on the validation data.\n\n*Is the cardinality distribution learned, and if so, how is it parameterized?*\n\nYes, the parameters of the cardinality distribution are learned through the gradient-based optimization, along with all other parameters of the network. We experimented with many variants of cardinality distributions (including mixtures), but the Poisson distribution turn out to deliver the best performance. The rate parameter of the Poisson distribution is expressed in the log-scale to allow for unconstrained optimization.\n\n*Is there any technical reason why the SPSN architecture cannot be extended to DAG (...)*\n\nWe do not see any technical reason that would prevent the extension of SPSN networks to generic acyclic graphs, including directed acyclic graphs as a special case. However, such an extension will require certain modifications. In the current paper, we are motivated by the abundant use of the JSON files in practical applications (e.g., cyber-security). For this reason, we set out to focus on the tree-structured graphs. We plan to provide the extension of SPSNs to generic acyclic graphs in future work.\n\n*The related work section is thorough, but, especially considering that the tested datasets all come from a relational schema, it would be useful to better understand the relationship with relational SPNs. For example, how does one translate a relational schema to a tree schema as in Figures 4-11 (in what way is relational data ``a particular form of graph-structured data\")?*\n\nThis is an excellent question. We forgot to mention this in the paper. We describe this in Section G of the updated version of the manuscript.\n\n*It is not clear what the result is in Proposition 2 (...)*\n\nThank you for noticing this, we corrected Proposition 2. Please, see the updated version of the manuscript."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7005/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700138120719,
                "cdate": 1700138120719,
                "tmdate": 1700138120719,
                "mdate": 1700138120719,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eQIHC4U5O8",
                "forum": "mF3cTns4pe",
                "replyto": "G6LKsULLVp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7005/Reviewer_9xzE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7005/Reviewer_9xzE"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I thank the authors for their detailed response and the updated manuscript. I maintain my positive assessment of the paper.\n\nI appreciate the additional details about learning added to the manuscript and the clarification regarding the missing data experiments. I am still not fully convinced regarding the independence assumption not being a limitation, as it seems that exchangability rather than independence is really what you want with homogenous units - as in the mutagenesis example with molecules. But I understand that the computational cost would be high in general, and the sum units provide some expressivity in this respect."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7005/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716964987,
                "cdate": 1700716964987,
                "tmdate": 1700716964987,
                "mdate": 1700716964987,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9mIfNLJFiP",
            "forum": "mF3cTns4pe",
            "replyto": "mF3cTns4pe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7005/Reviewer_snrr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7005/Reviewer_snrr"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose sum-product-set networks, an extension of probabilistic circuits from unstructured tensor data to tree-structured graph data. Key to their approach is the use of random finite sets to reflect a variable number of nodes and edges in the graph and allow for exact and efficient inference. The empirically demonstrate that their approach is on par with other intractable neural models."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is decently written, although, in my opinion, some important aspects seem to be missing.\n\n- Despite the potentially toyish nature of the experiments considered, the proposed models seem to be on par with other intractable neural models, as well as significantly more robust to missing data."
                },
                "weaknesses": {
                    "value": "- The writing of the paper would be greatly improved by adding informal intuitions here and there, as well as a (toy) complete example of an SPSN.\n\n- The authors do not make it clear that one could not obtain a distribution over tree-structured graphs using e.g. knowledge compilation to compile the distribution over tree-structured graphs into a logical circuit whose parameters could then be learnt, inducing a distribution over the desired structured objects. If so, is one theoretical conclusion that there is an expressivity gap between PCs with and without set units?\n\n- The authors only remark in passing that the infinite sum required to evaluate the set unit reduces to a finite one in practice; an argument upon which their tractability results hold. This should be made more formal.\n\n- The writing doesn't really give us an idea of the scale of the experiments performed, but they seem toyish."
                },
                "questions": {
                    "value": "- The first question that comes to mind is: can we not use knowledge compilation [1] to compile the distribution over tree-structured graphs into a logical circuit? One can use logical circuits to induce distributions over many different structured objects such as paths in a grid, hierarchies of classes, preferences, as well as subsets of size $k$ [2, 3, 4]. One can then learn the parameters of such a distribution from the data. It might very well be the case that the distribution over tree-structured graphs does not admit a tractable circuit, but such an assertion seems to be absent from the paper.\n\n- Could you please explain what a schema is? I understand how one could obtain a schema from a tree-structured graph, but aside from the definition, I was hoping for an intuitive explanation. ( I am familiar with the term in the context of databases, which does not seem to translate? )\n\n- Am I correct in my understanding that, according section, set units only apply to homogeneous nodes?\n\n- I really would've like to see a (toy) complete example of an SPSN. Could you please provide such an example?\n\n- Assumption 1 (Requirements on the set unit) \"states that the cardinality distribution vanishes for a sufficiently large $m$\", What exactly do you mean by that? As a follow up, am I correct in understanding that the elements of the set are independent given the cardinality? i.e. we do not consider the statistical correlations between the elements of a set? ~To me this consequently puts into questions the tractability of SPSNs laid out in proposition 1.~\n\n- Could you please say more regarding Definition 5? What structural constraint is being imposed here exactly? To me, \"follow only a single child of each sum unit\" reads as determinism?\n\nReferences:\n\n[1] On probabilistic inference by weighted model counting. Mark Chavira, Adnan Darwiche. Journal on Artificial Intelligence 2008.\n\n[2] Neuro-Symbolic Entropy Regularization. Kareem Ahmed, Eric Wang, Kai-Wei Chang, Guy Van den Broeck. UAI 2022.\n\n[2] Semantic Probabilistic Layers for Neuro-Symbolic Learning. Kareem Ahmed, Stefano Teso, Kai-Wei Chang, Guy Van den Broeck, Antonio Vergari. NeurIPS 2022.\n\n[3] SIMPLE: A Gradient Estimator for k-Subset Sampling. Kareem Ahmed, Zhe Zeng, Mathias Niepert, Guy Van den Broeck. ICLR 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7005/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7005/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7005/Reviewer_snrr"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7005/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698960657774,
            "cdate": 1698960657774,
            "tmdate": 1700712303575,
            "mdate": 1700712303575,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zFjVLDNoIn",
                "forum": "mF3cTns4pe",
                "replyto": "9mIfNLJFiP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7005/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7005/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*The writing of the paper would be greatly improved by (...)*\n\nCould you please point to specific parts of the text where these intuitions are needed? We will try to modify the manuscript in light of your suggestions. We added an example of an SPSN (Figure 4 in the updated manuscript).\n\n*The authors do not make it clear that one could not obtain a distribution over tree-structured graphs using e.g. knowledge compilation*\n\nWe reply on this question below.\n\n*The authors only remark in passing that (...)*\n\nWe formally specified this in Assumption 1(d) by considering $p(m)=0$ for a sufficiently large $m$. Assumption 1 is connected to our tractability result in Proposition 3. We also devoted to this a separate paragraph at the end of Section 3.1 and Remark 1 in Section B.\n\n*The writing doesn't really give us an idea of the scale of the experiments (...)*\n\nWe extended Table 2 with the size of each dataset. We also comment on the computation of the experiments in the last paragraph of Section F. Moreover, we improved Figures 5-12 by including the number of instances at each level of the tree. Even though the citeseer dataset (Figure 8) has only 3312 instances (n_inst), there is approximately 300k instances (strings) in the leaf nodes.\n\n*The first question that comes to mind is (...)*\n\nWe have not been aware of the existence of the approach you refer to. We find it very interesting and we would like to thank you for pointing it to our attention. We cannot see that this approach is possible in our setting, as all the references you suggest assume that the input of a PC is a fixed-size vector. Please, recall from our paper that we consider variable-size graphs. Nonetheless, it will be possible to apply such an approach if we restrict ourselves to tree-structured graphs of fixed size. We added a similar comment into the related work section.\n\nIs there an expressivity gap between PCs with and without set units? We cannot answer this question as the two models assume different inputs (the fixed size vectors versus variable-size graphs). We do not see an experiment that would allow us to compare expressivity in these two different settings.\n\n*Could you please explain what a schema is (...)*\n\nThe key motivation of our paper is to create a tractable model for processing of three-structured data in the JSON format. Therefore, the definition of schema in our paper is motivated by the schema of the JSON format. It was not our intention to imply any relation to the schema used in databases. We added a footnote on this matter in Definition 2.\n\n*Am I correct in my understanding that, according section, set units only apply to homogeneous nodes?*\n\nYes.\n\n*I really would've like to see a (toy) complete example (...)*\n\nPlease, see Figure 4 in the supplementary material.\n\n*Assumption 1 (Requirements on the set unit) ``states that the cardinality distribution vanishes for a sufficiently large $m$\", What exactly do you mean by that?*\n\nThis means that we require $p(m)\\rightarrow 0$ for $m\\rightarrow\\infty$. In practical cases, the homogeneous nodes contain a finite number of elements. Consequently, after learning the parameters of the cardinality distribution (the Poisson distribution in our case), we have $p(m)=0$ for $m<<\\infty$, which implies that we sum over a finite number of elements in Proposition 1.\n\nFor example, consider a single homogeneous node in the tree-structured graph. This node is at the same position in all instances of the training dataset (as follows from the schema). The specific value of $m$, for which $p(m)=0$, lies right after the maximum cardinality from all these instances of the homogeneous node. Alternatively, it can be restricted to a user-defined value, say, according to an allowable computational budged.\n\n*As a follow up, am I correct in understanding that the elements of the set are independent given the cardinality? (...) To me this consequently puts into questions the tractability of SPSNs (...)*\n\nYes, as stated in Assumption 1, the elements of the set are considered independent. Could you please be more specific about why it puts the tractability of SPSNs in question?\n\n*Could you please say more regarding Definition 5? (...)*\n\nWe do not assume determinism. If you convert an SPN to its mixture representation using the induced trees as in Definition 3 of [1], you can notice that each component is a product of input units with a unique scope. The union of these scopes is the scope of the root unit. The structural constrain is that $f$ must be the product of simpler functions, $\\lbrace f_u\\rbrace_{u\\in\\mathsf{I}}$, which are defined over these unique scopes. This allows the integration of $f$ to be propagated down to the input units, as the $f_u$-parts of $f$ can be integrated w.r.t. to their corresponding input units. We provide the analogous result for SPNs in Proposition 5.\n\n[1] Trapp, M., et al. 2019. Bayesian learning of sum-product networks. Advances in neural information processing systems, 32."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7005/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700137750452,
                "cdate": 1700137750452,
                "tmdate": 1700137750452,
                "mdate": 1700137750452,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TgI1qurxG1",
                "forum": "mF3cTns4pe",
                "replyto": "zFjVLDNoIn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7005/Reviewer_snrr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7005/Reviewer_snrr"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks you for your response. I am satisfied with most of your answers to my concerns and am updating my score accordingly."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7005/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712265550,
                "cdate": 1700712265550,
                "tmdate": 1700712265550,
                "mdate": 1700712265550,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mmEfGGCDjH",
            "forum": "mF3cTns4pe",
            "replyto": "mF3cTns4pe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7005/Reviewer_k1pi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7005/Reviewer_k1pi"
            ],
            "content": {
                "summary": {
                    "value": "The paper suggests a new type of probabilistic circuit (PC) that can be trained and perform inference in tree-structured graph data. Standard sum-product networks (SPNs), a PC, represent a probability density over unstructured data, which forms the input random variables. To model a density over tree-structure graphs, the manuscript introduces \"set units,\" nodes in the PC that allow for a variable number of nodes/edges in the data graph."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Tractability and Exchangeability: The manuscript presents a theoretical foundation for the tractability of the method, based on PC results in Section 3.1 and SPSNs' exchangeability based on its node types in Section 3.2.\n* Using the theory of finite random sets in SPSNs yields a simple and elegant way of representing tree-structured graphs."
                },
                "weaknesses": {
                    "value": "* Implementation: the manuscript does not provide a transparent discussion about the implementation of SPSNs. The non-formal description provided in \"Building SPNs\" raises relevant questions related to the convergence and size of the model. Moreover, from the setup, it is unclear how sensitive parameter initialization and/or hyper-parameter tuning the model is.\n* Experiments are encouraging but not convincing. Section 5 is unclear on how the tractability and exchangeability properties of SPSNs are exploited in the experiments. While the missing values results in Figure 3 are beneficial, they do not highlight \"efficient inference over\nspecific parts of the data graph,\" as stated in the Conclusion. Moreover, it might be helpful to the manuscript to compare the results with recent works, such as the ones discussed in Section 4.\n* Paper presentation\n    - The writing is unclear between PCs and SPNs, as the title evokes SPNs while the text uses PCs. The authors should clarify the difference between the two or assume interchangeable usage under assumptions.\n    - It could be beneficial to discuss the differences between SPNs and SPSn sooner in the paper, as it is a key contribution of the work. The sentence \"This differs from the conventional sum-product network...\" is helpful but only appears in Section 3.\n    - The manuscript could better articulate its motivation by connecting the problems presented in the introduction with some of the results. For instance, it is not clear in the paper how SPSNs take advantage of \"the parent-child ancestry inherent in tree-structured graphs\" in a different way than competitive generative models."
                },
                "questions": {
                    "value": "* Were there any empirical boundaries or assumptions when implementing the recursive algorithm described in \"Building SPSNs\"? How do you deal with large data graphs with multiple heterogeneous nodes and size constraints?\n* Could you please expand on the \"(...) building a set of trees based on a user-specified neighborhood\" in Section 4 regarding the similar graph-based approach from (Errica & Niepert, 2023)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7005/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7005/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7005/Reviewer_k1pi"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7005/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699074951605,
            "cdate": 1699074951605,
            "tmdate": 1700855307857,
            "mdate": 1700855307857,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6VOCURjbrC",
                "forum": "mF3cTns4pe",
                "replyto": "mmEfGGCDjH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7005/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7005/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*Implementation: the manuscript does not provide a transparent discussion about the implementation (...)*\n\nWe added Section E into the updated manuscript. It contains a more detailed description of constructing SPSNs. We also included comments on the size of each SPSN block in the network.\n\n*Section 5 is unclear on how the tractability and exchangeability properties of SPSNs are exploited in the experiments.*\n\nWe added comments into the last paragraph of Section 5 that will better connect our claims about tractability with the experiments.\n\n*While the missing values results in Figure 3 are beneficial, they do not highlight ``efficient inference over specific parts of the data graph,\" as stated in the Conclusion.*\n\nWe decided to demonstrate the tractable inference of SPSNs on the task of marginalizing the missing values in the leaf nodes. This corresponds to marginalizing out the whole leaf nodes, which we see as a part of the data graph. Our motivation to show the tractability on the marginalization task lies in that it is at the core of many more advanced queries, implying that they can also be evaluated tractably.\n\n*Moreover, it might be helpful to the manuscript to compare the results with recent works.*\n\nThe related work on the specific type of tree-structured graphs considered in our case is rather sparse to the best of our knowledge. We assume the tree-structured graphs where the inner nodes have no feature vectors and the leaf nodes have feature vectors with varying dimensionality and data type. We found the most related approaches to this setting in the NLP domain. They are mentioned in Section 4 and compared to our approach in Section 5. The remaining methods discussed in Section 4 assume generic (undirected and cyclic) graphs with all nodes containing feature vectors of the same dimension. Though they are principally applicable to tree-structured graphs, there would be the need to make non-trivial adaptations to deal with the empty features and varying dimensions.\n\n*The writing is unclear between PCs and SPNs (...)*\n\nThank you for pointing this to our attention. We revised the usage of PCs throughout the text. We consider SPNs as a sub-class of the large family of PCs. We included a comment on this matter into third paragraph of Section 1. Please, note that sometimes we use the term PCs deliberately since we need to refer to certain properties of SPSNs that are inherited from PCs.\n\n*It could be beneficial to discuss the differences between SPNs and SPSn sooner in the paper (...)*\n\nWe added a comment on this matter into the third paragraph of Section 1.\n\n*The manuscript could better articulate its motivation by connecting the problems presented in the introduction with some of the results. For instance, it is not clear in the paper how SPSNs take advantage (...)*\n\nWe elaborated on this in the second and fourth paragraph of Section 1.\n\n*Were there any empirical boundaries or assumptions when implementing the recursive algorithm described in ``Building SPSNs\"? How do you deal with large data graphs with multiple heterogeneous nodes and size constraints?*\n\nLine 6 in the procedure scope_layers of Algorithm 2 repeatedly checks if the set of scopes $\\psi$ contains a singleton set or if we reached a prescribed number of layers of the SPSN block. If any of these two conditions holds, then the procedure stops to create more layers in the SPSN block. This means that if $n_l$ and the heterogeneous nodes are large, the SPSN block can grow exponentially in size (similarly to the conventional SPN architectures). We provide concrete formulae to compute the number of parameterized computational units of the SPSN block in the last paragraph of Section E. In our experiments, we set the boundary on the computational budged by fixing $n_l$ to a given value.\n\nIf we set a reasonably small $n_l$ to limit the computational budget and the heterogeneous nodes are still large after performing several splits in the product layers, we apply the full independence assumption to model the elements of the heterogeneous nodes in the input units (which we do not present in Algorithms 1 for simplicity). This is a common practice when applying conventional SPNs to high-dimensional vector data, especially if we use the binary split in the product units [1].\n\n[1] Peharz, R., et al. 2020. Random sum-product networks: A simple and effective approach to probabilistic deep learning. In Uncertainty in Artificial Intelligence (pp. 334-344). PMLR.\n\n*Could you please expand on (...) Errica \\& Niepert, 2023?*\n\nThe authors decompose generic cyclic graphs into a collection of trees. Then, they use each tree as a template to design a hierarchy of SPNs. In this hierarchy, there is a single SPN per each node in the tree in order to model the feature vector of that node. The tree has a height $L$, which is a user-defined hyper-parameter defining the neighborhood of a selected vertex $v$ in the original graph."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7005/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700134665718,
                "cdate": 1700134665718,
                "tmdate": 1700134665718,
                "mdate": 1700134665718,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7WB1rN60Zp",
                "forum": "mF3cTns4pe",
                "replyto": "mmEfGGCDjH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7005/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7005/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer k1pi,\n\nYour valuable recommendations have allowed us to make several enhancements to the paper. Once again, thank you for investing your time and experience in helping us improve our work. As the discussion period ends soon, we kindly ask if you have considered increasing the rating based on our responses.\n\nKind regards,\n\nAuthors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7005/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732055147,
                "cdate": 1700732055147,
                "tmdate": 1700732055147,
                "mdate": 1700732055147,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4NFcCOtB73",
            "forum": "mF3cTns4pe",
            "replyto": "mF3cTns4pe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7005/Reviewer_gTbZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7005/Reviewer_gTbZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper develops a sum-product-set networks (SPSNs) to study the tree-structured graphs. It develops new variant of probabilistic circuts to obtain tractable inference for SPSNs. In the experiments, it shows that SPSNs obtains comparable performance to Neural Networks in the graph classification task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper has the following strengths:\n\n**1** the problem this paper working on is important to the community. The way of extending the applicability of probabilistic circut to the tree-structured graph data would be interesting and promising to the community. \n\n**2** the presentation and writing are very well. Although I am new to this topic, I can easily understand the main points and the main mechanism of this SPSNs method. \n\n**3** I like the investigation of the exchangeability of SPSNs. The study seems complete."
                },
                "weaknesses": {
                    "value": "Due to my limited expertise, I did not have identified meaningful weaknesses."
                },
                "questions": {
                    "value": "Sorry I am not an expert in this topic, I did not have particular technical questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7005/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699486957016,
            "cdate": 1699486957016,
            "tmdate": 1699636820756,
            "mdate": 1699636820756,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NGDb4wdLDp",
                "forum": "mF3cTns4pe",
                "replyto": "4NFcCOtB73",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7005/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7005/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank you for seeing our paper as easily understandable even for newcomers to the field of probabilistic circuits."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7005/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700132889763,
                "cdate": 1700132889763,
                "tmdate": 1700132889763,
                "mdate": 1700132889763,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]