[
    {
        "title": "Bayesian Exploration Networks"
    },
    {
        "review": {
            "id": "xultIfGEzY",
            "forum": "NiefAhgJqH",
            "replyto": "NiefAhgJqH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7595/Reviewer_Z7MF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7595/Reviewer_Z7MF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel model-free method for learning Bayes-optimal policies. It is found through theoretical analysis that existing model-free methods fails to address the epistemic uncertainty in the future or optimizes over a set of contextual policies. The proposed Bayesian exploration network addresses these gaps by simultaneously modeling both epistemic and aleatoric uncertainties through the use of normalizing flows. The method has potential to learn the Bayes-optimal in the limit of complete optimization."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces a novel method that models epistemic and aleatoric uncertainties using normalizing flows. This could be a valuable contribution to the field of reinforcement learning."
                },
                "weaknesses": {
                    "value": "1.  **Mathematical Rigor:** Although the paper provides extensive mathematical analysis, there's a discernible lack of mathematical rigor.\n    \n*   _Theorem 1:_ The theorem hinges on Lemma 1. However, Lemma 1's proof is questionable as its final equality doesn't hold true. The MDP parameter, $\\phi$, doesn't impact future decisions made by the contextual policy, which depend solely on history.\n*   _Theorem 2:_ The theorem states that the posterior of a mis-specified deterministic model concentrate on the maximum likelihood estimation (MLE). However, it is obvious that given few data, there could be multiple models fit the data perfectly. All of them are MLE but not all MLE has non-zero posterior probability. Besides, if no model can fit the data perfectly, then the likelihood of data under any model are always $0$, implying that marginal likelihood of the data is also $0$. In this case, the posterior probability is the probability of the model conditioned on an impossible event, which is undefined.\n*   _Theorem 3:_ The third theorem is based on the presumption that $b_t$ is a sufficient statistic. However, this is not the case if no further assumptions are made. We can easily construct examples where the pair $(s_{t+1},r_t)$ cannot be differentiated by just looking at $b_t=r_t+\\gamma \\sup_{a\u2019}Q^*(h_{h+1},a\u2019)$.\n\n2.  **Exploration-Exploitation Dilemma:** The goal of this paper is to learn a Bayes-optimal policy that strikes a balance between exploration and exploitation. But the process of learning this policy through environmental interactions has its own exploration-exploitation quandary, which isn't adequately tackled.\n\n3.  **Structure and Presentation:** The paper's layout is unwieldy. While the bulk of main text reiterates well-established and well-recognized results, critical details about the new method are relegated to the appendix. Consequently, understanding the new approach demands a thorough read of the appendix.\n\n4.  **Related Work:** The paper overlooks model-free meta-reinforcement learning methods, such as those in [1], which align closely with its theme. Additionally, RL methods for POMDPs, e.g., [2], are not mentioned. It is not clear how the proposed method performs compared to methods that directly solve the history-MDP.\n\n\n**Minor Points:**\n\n*   Notations are introduced without prior definitions, such as $P_Q$ in Definition 1 and $P_{al}$ in page 19.\n*   Definition 1: Replace \"... over and a model \u2026\" with \"... over a model \u2026\"\n*   Definition 1 & Last paragraph of Sec 3: The term $P_Q$ is ambiguous. Typically, a Q-function wouldn't yield a distribution.\n*   First paragraph in Sec 4.1: Change $\\pi(\\cdot,\\theta)$ to $\\pi(\\cdot,\\phi)$\n*   Theorem 3: The phrase \"be a measurable mapping $\\mathcal{S}\\times\\mathbb{R}\\to\\mathbb{R}$\" is initially perplexing and warrants elaboration.\n\n\n[1] Beck, J., Vuorio, R., Liu, E.Z., Xiong, Z., Zintgraf, L., Finn, C. and Whiteson, S., 2023. A survey of meta-reinforcement learning. arXiv preprint arXiv:2301.08028.\n[2] Hausknecht, M. and Stone, P., 2015, September. Deep recurrent q-learning for partially observable mdps. In 2015 aaai fall symposium series."
                },
                "questions": {
                    "value": "How come the variational posterior isn't history-dependent?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7595/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7595/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7595/Reviewer_Z7MF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7595/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698596743276,
            "cdate": 1698596743276,
            "tmdate": 1699636920369,
            "mdate": 1699636920369,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Sn4Ocz51bh",
                "forum": "NiefAhgJqH",
                "replyto": "xultIfGEzY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7595/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7595/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Z7MF - Lemma 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and detailed review and thank them for pointing out typos and points of clarification. We respond to detailed feedback regarding the validity of our mathematical results over two separate comments:\n\nThe reviewer questions the validity of the final line and states: `The MDP parameter $\\phi$ doesn't impact future decisions made by the contextual policy, which depend solely on history.'\n\nWe're not sure what the reviewer means by this. If they are claiming our formulation is myopic, this can be proved as not the case by constructing the equivalent formulation using the myopic transition distributions. In the myopic formulation, the current posterior at timestep $t$ is used to marginalise across all future timesteps $t'>t$: \n$$\np(r_{t'}\\vert s_{t'},a_{t'},h_t)=\\int p(r_{t'}\\vert s_{t'},a_{t'} ,\\phi) dP_{\\Phi}(\\phi\\vert h_t),$$\n\n$$p(s_{t'+1}\\vert s_{t'},a_{t'},h_t)=\\int p(s_{t'+1}\\vert s_{t'},a_{t'} ,\\phi) dP_{\\Phi}(\\phi\\vert h_t).$$ \nSubstituting gives the density over trajectories: \n\\begin{align}\n\tp(\\tau_{t:\\infty}\\vert h_{t},a_{t})=\\prod_{i=t}^\\infty p(s_{i+1}\\vert s_{i},a_i,h_t)p(r_i\\vert s_{i},a_i,h_t) \\pi(a_{i+1}\\vert s_{i+1},h_t),\n\\end{align}\nwhich clearly cannot be factored to give the result in Lemma 1. \n\nMaybe it would be easier to explain our result by taking the Bayesian RL objective as a starting point. It is well established that the Bayesian RL objective can be written as an outer prior marginalisation over an inner expectation:\n\\begin{align}\n\tJ^\\pi_\\textrm{Bayes}=\\int \\int \\sum_{i=0}^\\infty \\gamma^i r_i dP_{\\infty}^\\pi(\\tau_\\infty\\vert \\phi) dP_\\Phi(\\phi).\n\\end{align}\nwhere:\n\\begin{align}\np_{\\infty}^\\pi(\\tau_\\infty\\vert \\phi)=p_0(s_0)\\prod_{i=0}^\\infty\\pi(a_i\\vert  h_i) p(s_{i+1}\\vert s_i,a_i,\\phi) p(r_i\\vert s_i,a_i,\\phi).\n\\end{align}\nIn this form, the BAMDP can be solved by sampling MDPs from the prior, and training a history dependent policy / Q function on them (this is precisely what naively applying a POMDP solver like RL$^2$ [Duan et al., 17] does as we mention in the paper). Just because the objective can be written in this form does not mean that $\\phi$ does not impact future decisions. \n\nIf we now limit the space of policies to contextual policies, i.e. policies that can be factored as $\\pi(a_t\\vert h_t) =\\int \\pi(a_t\\vert s_t,\\phi) dP_\\Phi(\\phi\\vert h_t )$, elementary laws of probability show that under the expectation,\n\\begin{align}\n\tJ^\\pi_\\textrm{Contextual}=\\int \\int \\sum_{i=0}^\\infty \\gamma^i r_i dP_{\\infty}^\\pi(\\tau_\\infty\\vert \\phi) dP_\\Phi(\\phi),\n\\end{align}\nwhere \n\\begin{align}\n\tp_{\\infty}^\\pi(\\tau_\\infty\\vert \\phi)=p_0(s_0)\\prod_{i=0}^\\infty\\pi(a_i\\vert  s_i,\\phi) p(s_{i+1}\\vert s_i,a_i,\\phi) p(r_i\\vert s_i,a_i,\\phi).\n\\end{align}\nIf a proof of this is needed, consider:\n\\begin{align}\n\t\\int \\int f(\\tau_2) p^\\pi_2(\\tau_2\\vert \\phi)  p(\\phi) d\\phi d\\tau_2=\\int f(\\tau_2) \\int  p^\\pi_2(\\tau_2\\vert \\phi)  p(\\phi) d\\phi d\\tau_2.\n\\end{align}\nNow: \n$$\n\t\\int  p^\\pi_2(\\tau_2\\vert \\phi)  p(\\phi) d\\phi =\\int p_0(s_0)\\pi(a_0\\vert  s_0)\\pi(a_1\\vert h_1)   p(s_2,r_1\\vert s_1,a_1,\\phi) p(s_1,r_0\\vert s_0,a_0,\\phi) p(\\phi) d\\phi,$$\n$$\n\t=\\pi(a_0\\vert  s_0)\\pi(a_1\\vert h_1)   \\int p(s_2,r_1, s_1,r_0,s_0\\vert a_1,a_0,\\phi) p(\\phi) d\\phi,\n$$\n$$\n\t\t=\\pi(a_1\\vert h_1)    \\pi(a_0\\vert  s_0) p(s_2,r_1, s_1,r_0,s_0\\vert a_1,a_0),\n$$\n$$\n\t=\\int  \\pi(a_1\\vert s_1,\n\\phi) p(\\phi\\vert h_1) d\\phi  \\pi(a_0\\vert  s_0)p(s_2,r_1,s_1,r_0,s_0\\vert a_0),\n$$\n$$\n\t=\\int  \\pi(a_1\\vert s_1,\n\\phi) \\frac{p(h_1\\vert \\phi)  p(\\phi)}{p(h_1)}d\\phi p(h_1),\n$$\n$$\n\t=\\int  \\pi(a_1\\vert s_1,\n\\phi) p(h_1\\vert \\phi)  p(\\phi)d\\phi ,\n$$\n$$\n\t=\\int p_0(s_0)\\pi(a_0\\vert  s_0,\\phi) p(s_1,r_0\\vert s_0,a_0,\\phi) \\pi(a_1\\vert  s_1,\\phi)p(s_2,r_1\\vert s_1,a_1,\\phi)  p(\\phi)d\\phi.\n$$\n Applying the same proof but conditioning on $h_t,a_t$ recovers our result from Lemma 1. We are more than happy to include a full step by step derivation if the reviewer still feels it is necessary."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7595/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700246745511,
                "cdate": 1700246745511,
                "tmdate": 1700246745511,
                "mdate": 1700246745511,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E4nD1yvqnm",
                "forum": "NiefAhgJqH",
                "replyto": "xultIfGEzY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7595/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7595/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Z7MF - Further Points"
                    },
                    "comment": {
                        "value": "MLE Estimator:\n \n We would like to emphasise that Theorem 2 is an example demonstrating the need to model aleatoric uncertainty. The issues that the reviewer takes can be resolved by assuming that the (empirical) projection exists and is unique: \n \\begin{align}\n \t\\phi^\\star_\\textrm{MLE}(h_t)\\in\\arg\\inf_{\\phi\\in\\Phi}\\left(\\sum_{i=0}^{t-1} (b_i-B(h_i,a_i,\\phi)^2\\right).\n \\end{align}\n This is an assumption that is commonly (and most typically implicitly [4,5,6]) made in works that analyse fitted/projection operator methods [1,2,3]. Again, we emphasise that the purpose of the theorem is to show why modelling aleatoric uncertainty is essential, as even under somewhat contrived assumptions, the best we can hope for is a policy that has no exploratory properties. We thank the reviewer for bringing this to our attention. \n \nSufficiency of $b_t$:\n\nWe would like to clarify what we mean here: our goal is to find an optimal policy and we propose that either $b_t$ or $r_{t}$ and $s_{t+1}$ is a sufficient statistic for solving the underlying RL problem (that is finding $\\pi^\\star$), not that $b_t$ is a sufficient statistic for  $r_{t}$ and $s_{t+1}$. This must hold or else model-free methods in general, which estimate $b_t$, would not be sufficient for learning optimal policies. We recognise that this is confusingly worded and thank the reviewer for bringing this to our attention. We will amend this and emphasise this key difference in an updated version. \n \n \nRelated Work:\n\nWe are aware of the review of meta RL cited by the reviewer, however we are not sure which methods they are referring to. We take care to emphasise that naively applying ANY POMDP solver to solve a BAMDP when taking a model-based approach, regardless whether the solver itself uses a model-free or a model-based approach, is not feasible for reasons of tractability. The only tractable methods that attempt this are based on VariBAD, which, as we explain, forfeit being Bayesian over part of their model. We took the time to carefully explain that model-free Bayesian approaches model uncertainty in the Bellman operator (see Definition 1).  None of the methods presented in the review do this. We emphasise that just because a POMDP solver uses model-free methods to solve the BAMDP does not mean that it is a model-free approach to Bayesian RL. Model-free Bayesian RL approaches characterise uncertainty in the variable $b_t$. We are happy to add the review to our list of citations. \n\n[1] Antos et al.Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path. Machine Learning, 07.\n\n[2] Antos et al. Fitted Q-iteration in continuous action-space MDPs. NeurIPS 07\n\n[3] A. Antos et al. Value-iteration based fitted policy iteration: learning with a single trajectory. IEEE 07\n\n[4] Riedmiller, Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method,  ECML 05\n\n[5] Sutton et al. Fast Gradient-Descent Methods for Temporal-Difference Learning with Linear Function Approximation. ICML 09\n\n[6] Maei et al. Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation. NeurIPS 09"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7595/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700246841325,
                "cdate": 1700246841325,
                "tmdate": 1700246841325,
                "mdate": 1700246841325,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mrGcptQf1B",
                "forum": "NiefAhgJqH",
                "replyto": "E4nD1yvqnm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7595/Reviewer_Z7MF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7595/Reviewer_Z7MF"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for your response.\n\nOn Lemma 1: It is said in the response that elementary laws of probability show that under the expectation,\n$$P_\\infty^\\pi(\\tau_\\infty|\\phi)=p_0(s_0)\\prod_{i=0}^\\infty\\pi(a_i|h_i)p(s_{i+1}|s_i,a_i,\\phi)p(r_i|s_i,a_i,\\phi)$$\n$$=p_0(s_0)\\prod_{i=0}^\\infty p(s_{i+1}|s_i,a_i,\\phi)p(r_i|s_i,a_i,\\phi)\\int \\pi(a_i|s_i,\\phi)dP_\\phi(\\phi|h_i)$$\n$$=p_0(s_0)\\prod_{i=0}^\\infty p(s_{i+1}|s_i,a_i,\\phi)p(r_i|s_i,a_i,\\phi)\\pi(a_i|s_i,\\phi),$$\nwhich is, however, apparently incorrect. The issue arises because $\\phi$ inside the integral is a variable over which you are integrating, while $\\phi$ outside the integral is a fixed parameter. This mismatch means that $\\phi$ inside and outside the integral are not the same entity and should not be treated as such.\n\nIn the provided proof for the $t=2$ case, I found multiple errors. Here is my derivation for the last four equalities, \n$$\\pi(a_1|h_1)\\pi(a_0|s_0)p(s_2,r_1,s_1,r_0,s_0|a_1,a_0)$$\n$$=\\int \\pi(a_1|s_1,\\phi)p(\\phi|h_1)d\\phi \\pi(a_0|s_0)p(s_2,r_1,s_1,r_0,s_0|a_1,a_0)$$\n$$=\\int\\pi(a_1|s_1,\\phi)\\frac{p(\\phi)p(s_1,r_0,s_0,a_0|\\phi)}{ p(s_1,r_0,s_0,a_0)}d\\phi p(s_2,r_1,s_1,r_0,s_0, a_0|a_1)$$\n$$=\\int\\pi(a_1|s_1,\\phi)p(\\phi)p(s_1,r_0,s_0,a_0|\\phi) d\\phi \\frac{p(s_2,r_1,s_1,r_0,s_0, a_0|a_1)}{p(s_1,r_0,s_0,a_0)}.$$\n\nOn Theorem 2: If the mean squared error is to be taken as the negative likelihood, then the posterior would not be degenerated as intended. If you insist in using Dirac delta as the likelihood function, then the posterior should be what I stated in the review. By the way, I believe this theorem is of no avail even if it may be fixed in some way. The assumption looks contrived, and the conclusion feels apparent. As I have suggested in the review, I believe contents like this should make way for critical details of the proposed method. \n\nOn Theorem 3: I am not convinced that $b_t$ is sufficient in inferring $\\phi$, which is exploited in Equation (10). Can you provide a formal proof?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7595/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700321805138,
                "cdate": 1700321805138,
                "tmdate": 1700321805138,
                "mdate": 1700321805138,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WvVnRfguXA",
                "forum": "NiefAhgJqH",
                "replyto": "xultIfGEzY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7595/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7595/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Lemma 1"
                    },
                    "comment": {
                        "value": "We think the confusion may be coming from the fact Lemma 1 holds only because we are restricting ourselves to contextual policies. For simplicity, consider the space of contextual optimal policies, as this is what Theorem 1 analyses. As we write in our paper, a contextual optimal policy is defined as: \n\\begin{align}\n\t\\pi^\\star(\\cdot,\\phi)\\in \\arg \\max_{\\pi\\in\\Pi_\\Phi} J^\\pi(\\phi).  \n\\end{align}\nGiven an MDP indexed by $\\phi$, we can always find a contextual optimal policy by taking the optimal action $a\\in\\arg\\max_{a'} Q^\\star(s,a,\n\\phi)$, where:\n\\begin{align}\n\tQ^\\star(s,a,\\phi)=E_{\\tau_\\infty \\sim P_\\infty^\\star (s,a,\\phi)} \\left[\\sum_{i=0}^\\infty \\gamma^i r_i \\right].\n\\end{align}\nand:\n\\begin{align}\n\tp_\\infty^\\star (\\tau_\\infty\\vert s,a,\\phi)=\\prod_{i=0}^\\infty p(r_i\\vert s_i,a_i,\\phi)p(s_{i+1}\\vert s_i,a_i,\\phi) \\pi^\\star(a_{i+1}\\vert s_{i+1},\\phi).\n\\end{align} \nFinding $Q^\\star(s,a,\\phi)$ can be achieved by solving the contextual optimal Bellman equation \t$B[Q^\\star](s,a,\n\\phi)=Q^\\star(s,a,\n\\phi)$ where:\n\\begin{align}\n\tB[Q^\\star](s,a,\n\t\\phi)=E_{s',r\\sim P_{R,S}(s,a,\\phi)}[r+\\gamma\\sup_{a'} Q^\\star(s',a',\\phi)].\n\\end{align} \n We can thus (in principle) learn a set of contextual optimal policies by learning $Q^\\star(s,a,\n\\phi)$ and $\\pi^\\star(\\cdot,\\phi)$ for all $\\phi\\in \\Phi$ by solving the afformentioned optimal contextual Bellman equation. Existing model-free BRL methods then characterise the uncertainty in $\tB[Q^\\star](s,a,\n\\phi)$ by first sampling $s',r\\sim P_{R,S}(s,a,\\phi^\\star)$ from the environment and then applying the transformation $b=r+\\gamma\\sup_{a'} Q^\\star(s',a',\\phi)$. A posterior over $\\phi$ can be inferred $p(\\phi\\vert h_t)$, which is used to marginalise across all optimal Bellman operators $B[Q^\\star](h_t,\na_t)=E_{\\phi \\sim {P_\\Phi}( h_t) } [ B [ Q^\\star ] ( s_t,a_t,\n\\phi) ] $\n\nIn practice, existing methods partially solve the corresponding contextual Bellman equation online $B[Q^\\star(h_t,a_t)=Q^\\star(h_t,a_t)$ after inferring each $p(\\phi\\vert h_t)$ using variational inference and employing posterior sampling to yield a tractable algorithm. Regardless, the goal of these approaches is still to find: \n\\begin{align}\n\tB[Q^\\star](h_t,\na_t)=\tE_{\\phi\\sim P_\\Phi( h_t) }[B[Q^\\star](s_t,a_t,\n\\phi)]=E_{\\phi\\sim P_\\Phi( h_t)}[Q^\\star(s_t,a_t,\\phi)].\n\\end{align}\nNow:\n\\begin{align}\n\tE_{\\phi\\sim P_\\Phi( h_t) }[Q^\\star(s_t,a_t,\\phi)]=\tE_{\\phi\\sim P_\\Phi( h_t) }\\bigg[E_{\\tau_{t:\\infty} \\sim P_{t:\\infty}^\\star ( s_t,a_t,\\phi)} \\bigg[\\sum_{i=t}^\\infty \\gamma^i r_i \\bigg]\\bigg]\n\t=\tE_{\\tau_\\infty \\sim P_{t:\\infty}^\\star (h_t,a_t)} \\bigg[\\sum_{i=0}^\\infty \\gamma^i r_i \\bigg],\n\\end{align}\nwhere:\n \\begin{align}\n\tp_{t:\\infty}^\\star (\\tau_{t:\\infty}\\vert h_t,a_t)= \\int \\prod_{i=0}^\\infty p(r_i\\vert s_i,a_i,\\phi)p(s_{i+1}\\vert s_i,a_i,\\phi) \\pi^\\star(a_{i+1}\\vert s_{i+1},\\phi) dP_\\Phi(h_t).\n\\end{align}\nand so the proof of Lemma 1 holds, but only because we are limiting ourselves to the space of contextual optimal policies."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7595/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489544880,
                "cdate": 1700489544880,
                "tmdate": 1700494599489,
                "mdate": 1700494599489,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Px02CZwRdr",
                "forum": "NiefAhgJqH",
                "replyto": "WvVnRfguXA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7595/Reviewer_Z7MF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7595/Reviewer_Z7MF"
                ],
                "content": {
                    "comment": {
                        "value": "This new definition of contextual policies clearly diverges from the definition in the paper, which writes that $\\Pi^*_{Contextual}\\coloneq \\arg\\sup_{\\pi\\in\\Pi_{Contextual}}J_{Bayes}^\\pi$.\nUnder the new definition, contextual policies are basically QMDP policies introduced in [1], which makes Theorem 1 a tautology.\n\nBy the way, I notice that, in Section 4.2, the presented myopic policies are referred to as QMDP, which is incorrect. The so-called myopic policies are learned assuming future epistemic uncertainty remains unchanged, while QMDP assumes the epistemic uncertainty disappears after one step.\n\n[1] Littman, Michael L., Anthony R. Cassandra, and Leslie Pack Kaelbling. 1995. \u201cLearning Policies for Partially Observable Environments: Scaling Up.\u201d In Machine Learning Proceedings 1995, 362\u201370."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7595/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495046166,
                "cdate": 1700495046166,
                "tmdate": 1700495046166,
                "mdate": 1700495046166,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8P0lvpvY3o",
            "forum": "NiefAhgJqH",
            "replyto": "NiefAhgJqH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7595/Reviewer_Hvyz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7595/Reviewer_Hvyz"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a model-free Bayesian RL method.\n\nAs opposed to maintaining and using a posterior over the MDP (transition/reward) model, this approach attempts to directly learn the (history-based) Q-function that is optimized with a Bayesian Bellman update.\nWhile this update is defined as an expectation of the posterior over the MDP, the method avoids computing this posterior by instead learning the posterior over the Bellman update as a sufficient statistic.\n\nThis posterior is rather complex and, instead, they learn an approximation through variational inference with normalizing flow networks.\nNow given this approximate posterior over the Bellman update, a typical RNN is used to learn the Bayesian Q-function."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Bayesian RL is an important line of work that provides a solution to the exploration but suffers from computational complexity.\nAny progress on this front, as a result, should be relevant to a significant portion of ICLR's community.\n\nAdditionally, as described in the paper, while model-based methods have proliferated, model-free approaches have seen less attention.\nHence, this work is an important contribution.\n\nAn important aspect of this is the rigorous theory to support the rather novel perspective taken in the paper which is well supported (in the appendix).\nLastly, it is interesting to see that despite it being a model-free method, model-based prior knowledge is still an important aspect (which is positive because generally, those priors are much more practical to define)."
                },
                "weaknesses": {
                    "value": "Two key rooms for improvement are the clarity (presentation) and experimental section.\n\nFirst, while the theoretical support in the appendix is certainly substantial, I found it rather difficult to follow key parts of the method description.\nI believe this is, first, because the (general/theoretical) learning objective and its concrete (practical, normalizing flow network approximation) implementation are presented simultaneously.\nPotential more important is the fact that the method description did not start until page 7 with background plus related work taking up much of the space which is a nice addition, but in this case in my opinion wrong priorities.\n\nIn terms of evaluation, the experiments were relatively bare (in terms of baselines an domains), limited in scope, and assumed a lot of prior knowledge.\nFor instance, the complete prior and posterior that a model-based approach would have used in the tiger problem is a distribution over two elements (the tiger is behind door 1 or door 2, with a uniform prior).\nFurthermore, all the experiments are one-shot tasks. \nMaking it hard to estimate how good the method would do with less prior knowledge and in a more typical reinforcement learning setting.\n\nAs a result, in general, I found the paper more difficult to comprehend (than I believe necessary), and overall the empirical evaluation was lacking.\nI do not know whether this is because some scaling difficulties stop the proposed solution from being tested on problems with less prior knowledge."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7595/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7595/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7595/Reviewer_Hvyz"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7595/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698997691561,
            "cdate": 1698997691561,
            "tmdate": 1699636920245,
            "mdate": 1699636920245,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "y14V9rCzjC",
                "forum": "NiefAhgJqH",
                "replyto": "8P0lvpvY3o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7595/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7595/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Hvyz"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback, especially regarding the structure of the paper. Due to the novelty of our approach, we have had often contractionary feedback about how to best present our work. We do really like the suggestion to shorten the background material and splitting up the theoretical and implementation details and will re-structure the paper according to this proposal. With regards to experiments, not all of our implementations are one-shot tasks. As discussed in Section 6 we evaluate BEN in the episodic setting for the search and rescue task too. We focused on the one-shot setting for our ablations as this more closely mimics what a rescue robot would have to achieve in real life (including being given training knowledge in the form of simulated related situations) and it highlights the important failures of existing model-free approaches to BRL."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7595/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700246145746,
                "cdate": 1700246145746,
                "tmdate": 1700246145746,
                "mdate": 1700246145746,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7d0FHSmaVw",
                "forum": "NiefAhgJqH",
                "replyto": "y14V9rCzjC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7595/Reviewer_Hvyz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7595/Reviewer_Hvyz"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response and looking forward to the continuation of the discussion with the reviewer below!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7595/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479644491,
                "cdate": 1700479644491,
                "tmdate": 1700479644491,
                "mdate": 1700479644491,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5KOsl17ZbS",
            "forum": "NiefAhgJqH",
            "replyto": "NiefAhgJqH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7595/Reviewer_MKkw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7595/Reviewer_MKkw"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new model-free framework, Bayesian exploration networks, for performing Bayesian reinforcement learning. The first contribution is to show the shortcomings of existing model-free BRL methods. They show that these existing approaches don't learn Bayes-optimal policies because they don't properly propagate uncertainty through the MDP and only solve an approximation to the true Bayesian objective. Their second contribution is to propose a solution for these issue in the form of Bayesian exploration networks. The BEN framework simplifies the objective by using a Q-function approximator to reduce the dimensionality of the input, which is then passed to a Bayesian network. This results in a fewer number of parameters over which inference must be performed. Beyond theoretical results, they demonstrate the practical performance of BENs in a search and rescue problem."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Clear communication. The authors present prior work and their own work in a clear and concise manner. The logical flow of the paper is very nice.\n- The authors are very clear about the shortcomings of prior model-free BRL methods and how exactly their proposed approach addresses these shortcomings.\n- The structure of BENs is not overly complicated. They use well-known building blocks, such as Q-function approximating functions and normalizing flows, to address the need to model uncertainty in certain parts of the method."
                },
                "weaknesses": {
                    "value": "- While the authors do a nice job of reviewing prior literature, the magnitude of the contribution presented here is not clear. I am inclined to say that the importance of the authors' contributions is relatively low, although they are novel. The theoretical results showing the shortcomings of other model-free BRL approaches is arguably their most important contribution, but it's not clear that that's a sufficient contribution in isolation. I view their formulation of BENs as less impactful.\n- The solution to circumventing costly nested optimization in Algorithm 1 is questionable. I would want to see more results that this is not harmful.\n- Lack of comparison to methods. I would like to see further empirical evaluation of their approach, both in terms of environments tested and methods compared."
                },
                "questions": {
                    "value": "In which practical situations would you genuinely seek to avoid existing model-free BRL methods? Do the theoretical shortcomings of existing approaches translate into material empirical shortcomings in common situations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7595/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7595/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7595/Reviewer_MKkw"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7595/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699267697418,
            "cdate": 1699267697418,
            "tmdate": 1699636920119,
            "mdate": 1699636920119,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S9L8akPqq6",
                "forum": "NiefAhgJqH",
                "replyto": "5KOsl17ZbS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7595/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7595/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MKkw"
                    },
                    "comment": {
                        "value": "We really thank the reviewer for their feedback. We are glad that they enjoyed reading the paper and appreciated their thoughtful criticisms. In response to their question about when we would genuinely seek to avoid existing model-free BRL methods, we believe that BEN offers a Pareto improvement on these methods and many can be recovered by using a linear flow. We believe that there might be slight benefits from a practitioner's perspective in using existing approaches: implementing BEN  requires expertise in normalising flows whereas existing methods are slightly simpler. For toy domains where exploration is difficult but the environment's dynamics are simple enough to avoid the pathological examples outlined in our work, for example in domains like Mountain Car, existing approaches will suffice."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7595/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700246139698,
                "cdate": 1700246139698,
                "tmdate": 1700246139698,
                "mdate": 1700246139698,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]