[
    {
        "title": "Multimodal Molecular Pretraining via Modality Blending"
    },
    {
        "review": {
            "id": "nMnOEKHWcY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4139/Reviewer_3A6t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4139/Reviewer_3A6t"
            ],
            "forum": "oM7Jbxdk6Z",
            "replyto": "oM7Jbxdk6Z",
            "content": {
                "summary": {
                    "value": "This paper seeks to understand the inherent connection between 2D and 3D representations, capturing the essential structural attributes of molecules through atomic-relation level multimodal pretraining techniques. \nIn this process, the authors initially combine atom relations from various modalities into a single cohesive matrix for combined encoding, and subsequently retrieve specific information for both 2D and 3D structures separately."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Learning the qualified representation of molecules is important for various downstream tasks.\n2. To the best of my knowledge, this is the first work that aligns atom-level representation for 2D and 3D representations of molecules."
                },
                "weaknesses": {
                    "value": "1. As highlighted by the authors, earlier studies typically aligned different modalities at a broader molecule level, potentially hindering the capture of detailed molecular structures. Initially, when considering atom-level depictions of a molecule, our focus might be on the atom-level rather than the atomic-relation level. For instance, we might design a model that determines 3D atom coordinates based on a 2D molecular graph. What's the rationale behind emphasizing atomic-relation level multi-modal pretraining? It would be beneficial to provide a thorough reasoning in the methodology section and draw empirical comparisons in the experiments.\n\n2. The experimental findings appear to be underwhelming. MoleBlend's performance, as shown in Tables 1 and 2, doesn't seem to fare well against prior studies."
                },
                "questions": {
                    "value": "Provided above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4139/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4139/Reviewer_3A6t",
                        "ICLR.cc/2024/Conference/Submission4139/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697212364630,
            "cdate": 1697212364630,
            "tmdate": 1700753332136,
            "mdate": 1700753332136,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gILAH39je7",
                "forum": "oM7Jbxdk6Z",
                "replyto": "nMnOEKHWcY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3A6t"
                    },
                    "comment": {
                        "value": "We express our gratitude to the reviewer for providing valuable feedback. Below we address the comments in detail.\n\n**Q1: Why atomic-relation level rather than atomic-relation level.**\n\nWe thank the reviewer for the question.\n\nIn our analysis and intuitive insights, different modalities share the identical atoms (C, N, O, etc) and **diverse atomic relations**. Upon closer examination, the modalities mainly differ in how they depict **atom relations**, while the atoms are commonly shared. To this end, to align different modalities, we propose that the key to aligning different modalities lies in aligning the distinct atom relations that encapsulate richer information from various perspectives, rather than focusing solely on the identical atoms.\n\n**Predicting atom coordinates implicitly learns atom relations.** Regarding the coordinates mentioned, we posit that the individual atom coordinates themselves may not hold primary significance. Instead, the **relation/interplay between atoms (i.e., atom distance in 3D space, the structure)** imbues the molecule with diverse properties [1]. While predicting all coordinates collectively offers a degree of insight into structures and implicitly learns the atom relations, we contend that focusing on atom relations, specifically the 3D directed distances, **more directly and accurately convey the structural information**.\n\nAt present, we recognize that the effectiveness of predicting atom coordinates lies in its implicit learning of structure and atom relations. The rationale behind the superiority of atom-level predictions over atom-relation level remains elusive. To this end, experiments exploring atom-level alignment falls beyond the scope of our current paper and can be left as future work.\n\nIf any aspects require further clarification, please feel free to communicate them to us. We are eager to engage in additional discussions and appreciate your consideration. Thank you.\n\n[1] Structure-based drug design with geometric deep learning.\n\n**Q2: The experimental findings appear to be underwhelming.**\n\nOur method achieves state-of-the-art results across 16 out of 23 mainstream tasks evaluated. This is a substantial rather than marginal improvement. Notably, in Table 1, we achieve an average improvement of 1.09 point across all 8 MolNet tasks over the prominent SOTA MoleculeSDE [1] from ICML 2023. This improvement is obvious and far from being considered marginal.\n\nBesides, different from current 2D+3D baselines [1][2][3] that all rely on strong established methods like contrastive learning to enhance their performance, our method is a de novo endeavor -- the first attempt to align modalities at atom-relation level. We think these results are particularly encouraging, considering our distinctive perspective that offers a fresh outlook on the problem.\n\n[1] A group symmetric stochastic differential equation model for molecule multi-modal pretraining. ICML 2023\n\n[2] Pretraining molecular graph representation with 3d geometry. ICLR 2022\n\n[3] 3d infomax improves gnns for molecular property prediction. ICML 2022"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484917836,
                "cdate": 1700484917836,
                "tmdate": 1700484917836,
                "mdate": 1700484917836,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZzGc6ZZ3ad",
                "forum": "oM7Jbxdk6Z",
                "replyto": "gILAH39je7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4139/Reviewer_3A6t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4139/Reviewer_3A6t"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the authors for their detailed comments.\n\nRegarding Q1, I wonder if the authors can share their analysis and intuitions that the major difference exists in the depiction of the relationship. I disagree with the authors in the sense that the main difference exists in the relationship between atoms, while atoms are commonly shared. Can you clarify this part more specifically?\n\nIn my opinion, I believe in a 2D graph, the atom is commonly represented by its atomic number and corresponding characteristics, while in a 3D structure, atom coordinates play an important role. I believe the atom itself is different.\n\nMoreover, I additionally found a relevant baseline that predicts atomic level representation with multiple modalities, 2D and 3D [1]. I think this work should also be included in related works and experiments to demonstrate the importance of learning the **relationship** between atoms.\n\n[1] Unified 2D and 3D Pre-Training of Molecular Representations, 2023 KDD."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611435194,
                "cdate": 1700611435194,
                "tmdate": 1700611435194,
                "mdate": 1700611435194,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CRfkRYYvNi",
                "forum": "oM7Jbxdk6Z",
                "replyto": "nMnOEKHWcY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3A6t"
                    },
                    "comment": {
                        "value": "We thank Reviewer 3A6t for the further comment and address the further concerns below.\n\n**Q1: Why the key of modality alignment exists in the atom relationships.**\n\nThe reviewer's major concern lies in that atom coordinates play a key role. We argue that **when considering coordinates as attributes, what genuinely matters is the relative relation between the coordinates, i.e., the atom relation.**\n\n **The coordinate themselves does not matter**, as supported by the fact that when we rotate a molecule, its properties remain constant and invariant while all coordinates changed. The true determinant of a molecule's property is **the invariant atom relations** during rotation.\n\nTo this end, the employment of coordinates is typically coupled with equivariant constraints [1][2]. We posit that in essential, coordinate + equivariant constraints captialize on **atom relations**. Both the equivariant operation for processing coorinates (Figure 1 of [1], [2]) and the equivariant head used for predicting coordinates (Eq 6 of [3]) leverages **atom relations to guarantee equivariance** [1][2][3]. It is crucial to reiterate that coordinates must be coupled with equivariance for modeling molecules. A theoretical study [4] deduces that equivariance in essential can be expressed by relative scalars (e.g. relative distance), underscoring the significance of relative atom relations (or the equivalance of atom relations to equivariance and invariance).\n\nIn summary, equivariance and invariance are indispensable for molecule properties, and atom relations are the key for ensuring equivariance and invariance. Notably, atom coordinates themselves lack inherent invariance and equivariance. \n\nWe sincerely thank the reviewer for the question, and the relevant reference and rationale will be organized and included in our updated paper. We seek clarification on whether the reviewer agrees with this point.\n\n[1] SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks. NIPS 2020\n\n[2] E(n) Equivariant Graph Neural Networks. ICML 2021\n\n[3] One Transformer can understand both 2d & 3d molecular data. ICLR 2023\n\n[4] Scalars are universal: Equivariant machine learning, structured like classical physics. NIPS 2021\n\n**Q2: Baseline [1]**\n\nIn our experiments, we did not compare with [1], as we find problems existing in their evaluation procedure, and their MoleculeNet finetuning practice in the official repository differs from mainstream methods [2][3]. For example, they finetune and evaluate ClinTox on only one sub-task (https://github.com/teslacool/UnifiedMolPretrain/blob/7f299f65433f9f1b7ceef5a003315640ae0cec50/pretrain3d/data/DCGraphPropPredDataset/deepchem_dataloader.py\\#L16), whereas mainstream evaluation trains on two sub-tasks simultaneously with a single head, which is much more difficult. We also struggled to evaluate it under the commonly adopted evaluation protocol as it did not provide a pretrained checkpoint. To this end, we opt not to include its numbers as our baseline. However, we have discussed their method in related work in our initial submission.\n\nThe state-of-the-art work MoleculeSDE [2] from ICML 2023 also does not include its results as a baseline. \n\n[1] Unified 2D and 3D Pre-Training of Molecular Representations. KDD 2022\n\n[2] A group symmetric stochastic differential equation model for molecule multi-modal pretraining. ICML 2023\n\n[3] Pretraining molecular graph representation with 3d geometry. ICLR 2022"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700628082342,
                "cdate": 1700628082342,
                "tmdate": 1700628145890,
                "mdate": 1700628145890,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZfHyk36MaL",
                "forum": "oM7Jbxdk6Z",
                "replyto": "CRfkRYYvNi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4139/Reviewer_3A6t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4139/Reviewer_3A6t"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the author for further clarifying the paper.\n\nThanks to the author's clarification, it becomes more clear that the relationship (spd for 2D and distance for 3D) between the atoms is different between modalities.\n\nHowever, I still believe the baseline [1] model should be included in experiments to demonstrate the importance of learning from the relationship between the atoms.\n\nTherefore, I will keep my score.\n\n[1] Unified 2D and 3D Pre-Training of Molecular Representations. KDD 2022"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629582890,
                "cdate": 1700629582890,
                "tmdate": 1700629582890,
                "mdate": 1700629582890,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UO0x783NyX",
                "forum": "oM7Jbxdk6Z",
                "replyto": "nMnOEKHWcY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4139/Reviewer_3A6t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4139/Reviewer_3A6t"
                ],
                "content": {
                    "comment": {
                        "value": "Hello reviewer TmmD!\n\nThank you for pointing out that the paper does not satisfy the SE(3)-equivariance property.\nHowever, I believe the inclusion of this baseline (or even a training approach as an ablation study) can definitely show the importance of learning from the relationship between atoms is importance, and improve the quality of the work.\n\nI'm happy to further listen to your opinion on this."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631677001,
                "cdate": 1700631677001,
                "tmdate": 1700631767603,
                "mdate": 1700631767603,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o71qxQitNM",
                "forum": "oM7Jbxdk6Z",
                "replyto": "nMnOEKHWcY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4139/Reviewer_3A6t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4139/Reviewer_3A6t"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer TmmD\n\nWhat I mean by relationship here is the relationship between atoms defined in this paper, i.e., distance, spd, edge type. \n\nAs my understanding, GraphMVP and 3D Infomax are training the model by learning the representation of molecule by **molecule-level** alignment, while this paper aims to learn the representation by aligning **atom-level** representation.\n\nTo the best of my knowledge, the KDD paper is the only paper with **atom-level** alignment, and this paper learns the representation by directly generating in data space without regarding **relationship**(i.e., distance, spd, edge type).\nTherefore, I believe the key difference between this paper and the KDD paper is regarding the relationship between the atoms, and the inclusion of this would improve the paper.\n\nI'm happy to be corrected if there is something wrong.\nFeel free to share your thoughts!\n\n**Also, for authors, if there is something wrong in my knowledge, please don't hesitate to correct it.**"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633522079,
                "cdate": 1700633522079,
                "tmdate": 1700635091682,
                "mdate": 1700635091682,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zQLC9wPoCD",
                "forum": "oM7Jbxdk6Z",
                "replyto": "nMnOEKHWcY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the prompt response."
                    },
                    "comment": {
                        "value": "We sincerely appreciate the prompt response from Reviewer 3A6t. We have incorporated the results from [1] on partial MolNet tasks in Table 1. However, it is important to note that these results are 1) incomplete MolNet results and 2) have been evaluated under a distinct protocol. Therefore, the comparability of these results to other baselines and our work may be limited. \n\nConcretely, one most obvious deviation is that the common protocal [2][3][4] of finetuning on MolNet is multitask training (as shown in Table 8 of GraphMVP [2] and Table 6 of Mole-BERT [4], for example, Tox21 and ClinTox are trained on 12 and 2 sub-tasks jointly). However, as per [1]'s official github repo, they focus only on one sub-task: ClinTox (https://github.com/teslacool/UnifiedMolPretrain/blob/7f299f65433f9f1b7ceef5a003315640ae0cec50/pretrain3d/data/DCGraphPropPredDataset/deepchem_dataloader.py#L16), Tox21 (https://github.com/teslacool/UnifiedMolPretrain/blob/7f299f65433f9f1b7ceef5a003315640ae0cec50/pretrain3d/data/DCGraphPropPredDataset/deepchem_dataloader.py#L34).\n\nWe follow the [2][3][4]'s protocal for evaluation, which is also the mainstream protocal adopted by the community. To this end, our results are not comparable to [1]. However, we have discussed the methodology of this work in the related work section. \n\nWe welcome the feedback on whether these adjustments adequately address the reviewer's concerns.\n\nThank you.\n\n[1] Unified 2D and 3D Pre-Training of Molecular Representations. KDD 2022\n\n[2] Pretraining molecular graph representation with 3d geometry. ICLR 2022\n\n[3] A group symmetric stochastic differential equation model for molecule multi-modal pretraining. ICML 2023\n\n[4] Mole-BERT: Rethinking Pre-Training Graph Neural Networks for Molecules. ICLR 2023"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633932072,
                "cdate": 1700633932072,
                "tmdate": 1700633984896,
                "mdate": 1700633984896,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W58r0R4aUJ",
                "forum": "oM7Jbxdk6Z",
                "replyto": "nMnOEKHWcY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the discussion from Reviewer 3A6t and TmmD"
                    },
                    "comment": {
                        "value": "We extend our sincere gratitude to the reviewers for your invaluable efforts and dedicated time invested in reviewing our paper.\n\nLet us organize the discussion thread to establish a clear logical progression.\n\nOur primary motivation is that aligning fine-grained atom-relations is crucial for aligning different modalities. This is well established in our introduction and the response to Reviewer TmmD and 3A6t. We thank Reviewers TmmD and 3A6t for the assistance in elucidating this point. Further, we have conducted theoretical analysis and experiments to validate the effectiveness of this fine-grained alignment, thereby supporting our motivation and claim.\n\n[1] explores the effectiveness of atom-level alignment, which is another form of fine-grained alignment of modalities. Reviewer 3A6t recommends that the comparison to [1] can enhance the quality of our paper. However, a notable challenge arises due to their adoption of a distinct evaluation protocol from mainstream studies and the absence of a provided pretrained checkpoint, making the evaluation of their method challenging. This lack of a standardized basis hinders a fair and quick comparison.\n\nIt is imperative to underscore, however, that whether atom-level alignment is crucial extends beyond the current scope of our research and is less directly connected to our motivation. While we acknowledge the significance of this aspect, we believe that its thorough exploration can be left for future investigations.\n\n[1] Unified 2D and 3D Pre-Training of Molecular Representations. KDD 2022"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636138015,
                "cdate": 1700636138015,
                "tmdate": 1700700862969,
                "mdate": 1700700862969,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xzf6XJZSNi",
            "forum": "oM7Jbxdk6Z",
            "replyto": "oM7Jbxdk6Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4139/Reviewer_w2xw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4139/Reviewer_w2xw"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed to align molecule 2D and 3D modalities at the atomic-relation level and introduce MOLEBLEND. This multimodal molecular pretraining method explicitly utilizes the intrinsic correlations between 2D and 3D representations in pertaining. Extensive evaluation demonstrates that MOLEBLEND achieves state-of-the-art performance over diverse 2D and 3D tasks, verifying the effectiveness of relation-level alignment."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to understand.\n2. The authors conducted extensive experiments on both 2D and 3D molecule tasks and showed their good performance."
                },
                "weaknesses": {
                    "value": "1. The authors did not discuss the training time cost of different pretraining methods.\n2. A series of pertaining baselines are missed in related works and comparisons. For example:\n\n[1] Xu M, Wang H, Ni B, et al. Self-supervised graph-level representation learning with local and global structure. ICML 21.  \n[2] Zhang Z, Liu Q, Wang H, et al. Motif-based graph self-supervised learning for molecular property prediction. NeurIPS 21.  \n[3] Zaidi S, Schaarschmidt M, Martens J, et al. Pre-training via denoising for molecular property prediction. ICLR 23.  \n\n3. The theoretical analysis is good. However, could the authors provide more insights into why MOLBLEND overperforms existing methods theoretically?\n\n4. How does the different choice of encodings for 2D/3D modalities influence the pretaining?"
                },
                "questions": {
                    "value": "Please see the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4139/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4139/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4139/Reviewer_w2xw"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698418565283,
            "cdate": 1698418565283,
            "tmdate": 1699636379400,
            "mdate": 1699636379400,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WoAfsfxdVp",
                "forum": "oM7Jbxdk6Z",
                "replyto": "xzf6XJZSNi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer w2xw"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful comments. We have uploaded a revision of our paper. Below we address the detailed questions.\n\n**Q1: Pretraining time cost**\n\nWe thank the reviewer for the comment. The computation required are listed below. We can observe that GNN based pretrainig methods have relatively small compute cost. Transformer based pretraining methods cost a lot more computation.\n\n| Method      | Pretraining Time Cost     |\n| ----------- | ------------------------- |\n| AttrMask    | 9.2h on one V100 GPU      |\n| ContextPred | 23.3h on one V100 GPU     |\n| MolCLR      | 16.7h on one V100 GPU     |\n| InfoGraph   | 10h on one V100 GPU       |\n| GROVER      | 2.5 days on 250 V100 GPUs |\n| 3D Infomax  | Not reported              |\n| GraphMVP    | Not reported              |\n| MoleculeSDE | 50h on one V100 GPU       |\n| Ours        | 24 hours on 8 A100 GPUs   |\n\n**Q2: Missed baselines.**\n\nWe thank the reviewer for the reference. [1][2] have been added into our paper for comparison. \n\nWe do not compare with the number in [3] as they use a much stronger model backbone that excels at 3D property prediction, which even outperforms all Transformer-based pretraining method without pretraining. \n\nHowever, we implement the same denoising idea [3] on our backbone for comparison. In Table 3, where the first row is denoising (Noisy Node [4] is identical to [3] when applied to pretraining, as discussed in Section 2 and 3.2.2 of [3]), the second and the third row are our methods. The results show that our method achieve better results across a broad range of tasks.\n\n[1] Self-supervised graph-level representation learning with local and global structure. ICML 21.\n\n[2] Motif-based graph self-supervised learning for molecular property prediction. NeurIPS 21.\n\n[3] Pre-training via denoising for molecular property prediction. ICLR 23.\n\n[4] Simple GNN regularisation for 3d molecular property prediction and beyond. ICML 2022\n\n**Q3: More insights into why MOLBLEND overperforms existing methods theoretically.**\n\nTheoretically, the superiority of MoleBLEND lies in the more comprehensive and fine-grained optimization target for molecular modeling when compared with existing methods.\n\n- More comprehensive: as evidenced by Proposition 3.2, MoleBLEND can be viewed as a comprehensive target that encompasses single-modal masking, cross-modal contrastive and predictive methods, with their conditional versions. Therefore, MoleBLEND is able to learn cross-modal correspondences as well as single-modal context dependencies, which are both essential for multimodal molecular modeling. However, many existing methods focus solely on one aspect. For example, 3D InfoMax does not explicitly model the single-modal context dependencies.\n- More fine-grained: In the perspective of mutual information maximization, cross-modal contrastive and predictive methods aim at $\\max I(A,B)$, where $A$ and $B$ are two modalities of the input. In contrast, MoleBLEND performs fine-grained modality alignment in Eq.15, i.e. $\\max I(A_1,B_1)$, where $A_1$ and $B_1$ denote two modalities of the randomly partitioned input. This also makes the former objective a special case of ours. Fine-grained alignment encapsulates the cross-modal correspondence of molecular substructures, a critical characteristic of molecules that is rarely captured by previous methods.\n\n**Q4: How does the different choice of encodings for 2D/3D modalities influence the pretaining?**\n\nTo the best of our knowledge, 2D shortest path, edge type, 3D euclidean distance are the most commonly used encodings for molecules. The only other encoding we know is molecule shape [1], but this encoding cannot be effectively represented in terms of atoms and their relationships, thus it is not compatible for our method. We acknowledge the potential existence of other encodings and are open to exploring their impact on pretraining. Should you have relevant references, we are willing to delve into further learning and are prepared to conduct swift experiments to assess their influence.\n\n[1] Learning Harmonic Molecular Representations on Riemannian Manifold. ICLR 2023"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484104401,
                "cdate": 1700484104401,
                "tmdate": 1700484971787,
                "mdate": 1700484971787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RI6SHQUluF",
                "forum": "oM7Jbxdk6Z",
                "replyto": "xzf6XJZSNi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Anticipating Your Participation as Reviewer-Author Discussion Deadline Approaching"
                    },
                    "comment": {
                        "value": "We express our gratitude to the reviewer for dedicating time to review our paper. **We have provided comprehensive clarifications to all the questions including 1) the pretraining time cost, 2) missed baselines, 3) theoretical insights of why MoleBLEND performs better, and 4) influences of different 2D/3D encodings.** As the discussion deadline looms within a day, we would like to inquire if our responses have adequately addressed your questions. We anticipate your participation in the Reviewer-Author discussion phase, as your insights are invaluable for refining and enhancing the quality of our paper. Should you have any additional queries or require further clarification, please do not hesitate to inform us. We are more than willing to address any concerns and ensure a comprehensive resolution. Thank you for your time and consideration."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657688792,
                "cdate": 1700657688792,
                "tmdate": 1700657688792,
                "mdate": 1700657688792,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hB3Pb5HJCI",
            "forum": "oM7Jbxdk6Z",
            "replyto": "oM7Jbxdk6Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4139/Reviewer_TmmD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4139/Reviewer_TmmD"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes MolBlend, a method that explores the intrinsic alignment between 2D and 3D for molecule pretraining. MolBlend aims to conduct the 2D-3D pretraining based on the atom relations, which is finer-grained than previous works."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The key idea is clear and straightforward: to use the attention module to help augment the 2D-3D atom-relation for molecule pretraining.\n- The theoretical proof is interesting."
                },
                "weaknesses": {
                    "value": "- The motivations are not clearly claimed or supported.\n    - For instance, on Page 2, the authors say that they \u201cobserve that although appearing visually distinct \u2026 are intrinsically equivalent as they are essentially different manifestations of the same atoms and their relationships\u201d. What does \u201cequivalent\u201d mean here? A lot of 2D-3D pretraining methods start by saying such two modalities are complementary to each other.\n    - Additionally, on Page 2, what is the motivation to feed both modalities as one unified data structure to one single model in MoleBlend?\n\n- Notations are misleading in Sec 3.1.\n    - Why is $R_{spd}$ required? Because they can be derived from $R_{edge}$?\n    - For Eq 1, the notation should be $R_{2D3D,S}$ (with S in the subscript).\n    - Is $R_{2D3D}$ the masked version of $R_{spd}, R_{edge}, and R_{distance}$?\n\n- Other minor comments:\n    - The title can be further improved, especially that what modalities are considered is not explicit.\n    - The distance modeling is invariant. A more advanced equivariant model is preferred here.\n    - It would be better to explicitly add a column in the result tables on what backbone models are pretrained/comparing.\n    - The citation of 3D InfoGraph is wrong. Please fix it.\n\nI will consider raising the score once the authors answer/fix these comments."
                },
                "questions": {
                    "value": "- I am confused about the Fig 1.b. Does this mean the input can be either 2D or 3D, and the output can be both 2D and 3D?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4139/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4139/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4139/Reviewer_TmmD"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698716126874,
            "cdate": 1698716126874,
            "tmdate": 1700486035509,
            "mdate": 1700486035509,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SIpl3gUzQx",
                "forum": "oM7Jbxdk6Z",
                "replyto": "hB3Pb5HJCI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TmmD"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback. Below we address your comments in detail.\n\n**Q1: What does 2D and 3D 'equivalent' mean? Motivation to feed both modalities as one unified data structure to one single model.**\n\nThey are indeed complementary as existing 2D-3D pretraining methods claim. Furthermore, through our analysis and intuitive insights, we claim that they are essentially equivalent and just appear different. \n\nDifferent modalities share the same underlying structures, i.e., atoms and their relations. Different modalities share the identical atoms (C, N, O, etc). The 2D and 3D structures (edge type and 3D distance) are essentially different depictions of the **same** atom relations. To this end, we treat them as essentially equivalent. For example, a C-N bond in a 2D molecular graph and the corresponding 3D distance between C and N atoms essentially represent **the same thing**.\n\nGiven they are essentially equivalant and complementary, these modalities should be amalgamated to form a more comprehensive information set. Consequently, we advocate for feeding these modalities as one unified data into one single model, to learn a comprehensive representation of molecules.\n\nIf there are any aspects that need additional clarification, please let us know. We would be delighted to engage in further discussions. Thank you.\n\n**Q2: Notations Clarification**\n\n> Why is R_spd required? Because they can be derived from R_edge?\n\nWe would like to clarify that R_spd and R_edge are two different types of information. R_spd is the shortest path between two atoms on a molecular graph, reflecting the topology information of the molecule. R_edge is the edge type (e.g. single bond), reflecting the chemistry property. Providing information from different perspectives can enhance model understanding of molecules.\n\n> For Eq 1, the notation should be (with S in the subscript).\n\nWe thank the reviewer for the advice. We have updated this notation in our paper.\n\n> Is R_2d3d the masked version of R_spd, R_edge, R_distance?\n\nYes, it is the blended and masked version of R_spd, R_edge, and R_distance.\n\n**Q3: Equivariant Head**\n\nWe thank the reviewer for pointing this out and the advice. We conduct some preliminary attempts and transform our 3D prediction head to an equivariant head.\n\nOriginally, to predict the 3D distance between atoms, we perform outer product operations between atoms representation, followed by flattening and projection operations to map them into the target space to predict the $N^2$ atom relations:\n\n$$o_{ij} = \\text{G}(W_{l} X_i^{L+1}) \\otimes \\text{G}(W_{r} X_j^{L+1})^\\top \\in \\mathbb{R}^{m\\times m}$$\n\n$$z_{ij} = W_{\\text{head}}\\cdot \\text{Flatten}(o_{ij}) \\in \\mathbb{R}^{c}$$\n\nFollowing [1], in order to achieve equivariant 3D relative position prediction, we decompose the flattened relational representation into three directions by multiplying the normalized relative position offset $\\Delta_{i,j}=\\frac{r_{i,j}}{\\|\\|r_{i,j}\\|\\|_2}$ and apply linear projection head to each component of the 3D relational representation in their respective direction:\n\n$$o_{ij} = \\text{G}(W_{l} X_i^{L+1}) \\otimes \\text{G}(W_{r} X_j^{L+1})^\\top \\in \\mathbb{R}^{m\\times m}$$\n\n$$z_{ij}^{k} = W_{N}\\cdot \\Delta_{i,j}^{k} \\text{Flatten}(o_{ij}), \\quad  k=0,1,2$$\n\nwhere $\\text{Flatten}(o_{ij})$ is the flattened relational representation, $\\Delta_{i,j}^k$ is the k-th element of the directional vector $\\frac{r_{i,j}}{\\|\\|r_{i,j}\\|\\|_2}$ between atom $i$ and atom $j$ and $W_N\\in \\mathbb{R}^{1\\times d}$ is learnable weight matrices.\n\n\nWe pretrain for 200K steps and compare it apple-to-apple with blending. The experiment results are shown below. Current equivariant head brings performance boost on 2 out of 4 tasks evaluated.\n\n| Method | BBBP | BACE | Tox21 | ToxCast |\n| --- | --- | --- | --- | --- |\n| Blending  | 71.68 | 83.41 | 76.58 | 65.46 |\n| Blending w/ Equivariant head | 72.60 | 81.55 | 76.41 | 66.03 |\n\nPlease note that this is just a preliminary attempt, and further investigation can be left as future work.\n\n[1] One transformer can understand both 2d & 3d molecular data. ICLR 2023\n\n**Q4: Explanation of Fig 1 b**\n\nThe output is still either 2D or 3D. The work of Fig 1 (b) is [1]. Their multimodal pretraining task involves taking a 2D input and predicting its corresponding 3D information, or taking a 3D input and predicting the associated 2D information, which corresponds to Fig 1 (b).\n\n[1] Unified 2d and 3d pre-training of molecular representations.\n\n**Q5: Other improvements**\n\n> It would be better to explicitly add a column in the result tables on what backbone models are pretrained/comparing.\n\n> The citation of 3D InfoGraph is wrong. Please fix it.\n\nWe thank the reviewer for these suggestions. We have uploaded an updated version of our paper that addresses these questions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483336200,
                "cdate": 1700483336200,
                "tmdate": 1700484968353,
                "mdate": 1700484968353,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G6ssauE81g",
                "forum": "oM7Jbxdk6Z",
                "replyto": "hB3Pb5HJCI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4139/Reviewer_TmmD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4139/Reviewer_TmmD"
                ],
                "content": {
                    "title": {
                        "value": "Respone"
                    },
                    "comment": {
                        "value": "Thank you. My questions have been addressed."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485948680,
                "cdate": 1700485948680,
                "tmdate": 1700485948680,
                "mdate": 1700485948680,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ziWfxesiLs",
                "forum": "oM7Jbxdk6Z",
                "replyto": "ZfHyk36MaL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4139/Reviewer_TmmD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4139/Reviewer_TmmD"
                ],
                "content": {
                    "title": {
                        "value": "A Quick Comment"
                    },
                    "comment": {
                        "value": "Hi reviewer 3A6t,\n\nThank you for pointing out this KDD paper. I read this, and I know that you are definitely not related.\n\nI just want to kindly point out that the 2D to 3D generative pretraining in this work (which is called DMCG) does not satisfy the SE(3)-equivariance property. This should be pointed out by the reviewers of KDD, but it seems they just missed this part.\n\nI am happy to discuss this in more detail.\n\nRegards"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630397777,
                "cdate": 1700630397777,
                "tmdate": 1700630397777,
                "mdate": 1700630397777,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9dkDuYXgMM",
                "forum": "oM7Jbxdk6Z",
                "replyto": "UO0x783NyX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4139/Reviewer_TmmD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4139/Reviewer_TmmD"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "Hi reviewer 3A6t,\n\nSure, I am happy to share more thoughts on this!\n\nSo the way I viewed this is that starting from GraphMVP and 3D InfoMax, people have been working on 2D-3D pretraining, which somehow learns/gives the relationship between atom representations. Just to double-check, is the relationship you are referring to?\n\nIf not, feel free to correct me :)\n\nIf so, then the following works (like the KDD paper you shared and MoleculeSDE) are doing generative pretraining in the data space (2D & 3D graphs) instead of representation space. This gives a more direct way of showing the relationships between atoms. BTW. The MoleculeSDE papers are compared in this work.\n\nFeel free to share your thoughts!\n\nRegards"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632877889,
                "cdate": 1700632877889,
                "tmdate": 1700632877889,
                "mdate": 1700632877889,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bnKGSvn6Ch",
            "forum": "oM7Jbxdk6Z",
            "replyto": "oM7Jbxdk6Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4139/Reviewer_XaRC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4139/Reviewer_XaRC"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a molecular representation learning method that fusing the information from both 2D and 3D molecule structures. A unified relation matrix is constructed to describe the relationships between each pair of atoms, so that both 2D and 3D information can be injected into the matrix for fusion. For the 2D structure, based the bonds between atoms, shortest path and edge type information can be calculated for each entry of the relation matrix, and for the 3D structure, the entry can records the 3D Euclidean distance between atoms. The 2D and 3D information are blended in one relation matrix and a Transformer backbone is trained to recover the full information."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of using a relation matrix to unify the 2D and 3D information for molecular representation learning is novel.\n2. Theoretical analysis provide more insights to the proposed method.\n3. The paper is well writen and structured and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The information gathered in the relation matrix is quite limited and much information in the original structure is lost, especially those in the 3D structure. The matrix construction is quite similar to the work of  \"One transformer can understand both 2d & 3d molecular data\" published in ICLR 2023.\n2. Ablation studies on blending two masks should be provided.\n3. Some details of the experimental setup is missing."
                },
                "questions": {
                    "value": "1. Does the author run all baseline methods on the experimented splits or cite some results from other papers? For the results in Table 4, does the  single-modality mask-then-predict strategies use the same network as the proposed blending strategy?\n\n2. In Table 4, I think the author compares blending three mask to using only one mask. What's the effect of blending two masks?\n\n3. The conclusion in section D.1 is not well supported, since different 3D networks may perform differently and may have different speed of convergence. It's not enough to draw the conclusion by comparing the proposed method to only one 3D model.\n\n4. Why run ablation studies on different datasets and different tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740569143,
            "cdate": 1698740569143,
            "tmdate": 1699636379244,
            "mdate": 1699636379244,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ix0fULWIYE",
                "forum": "oM7Jbxdk6Z",
                "replyto": "bnKGSvn6Ch",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XaRC (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful comments. Below we address each question in detail.\n\n**Q1: The information gathered in the relation matrix is limited and lost a lot.** \n\nWe thank the reviewer for the question. We would like to clarify that no information is lost in our method in both the pretraining and finetuning stage.\n\nPlease note that in modality-blending, we are performing self-/un-supervised pretraining. The learning task is to leverage the blended relation matrix to **predict non-blended elements**. Although some information in the original structure is not blended and lost, these elements will be **used as targets for prediction**. This supervision guarantees no information is lost in the process.\n\nDuring supervised finetuning and inference stage, we **utilize all available modalities without random blending, where no information in the original structure is lost**. The choice of modalities depends on the specific scenario at hand:\n\n- When a large amount of 2D molecular graphs are available while 3D conformations are too expensive to obtain, the model can be finetuned and perform inference with only 2D inputs.\n- When it comes to scenarios where 3D information is available, we propose to incorporate both 2D and 3D information as model input, as generating 2D molecular graphs from 3D conformations is free and can bring in useful information from 2D perspective.\n\nDetailed descriptions about how to incorporate each modality under different scenarios are presented in Section 3.3 of the paper. Our modality-blending pretraining technique enables the model to be versatile and adaptable to various downstream scenarios.\n\nIf there are any aspects that need additional clarification, please let us know. We would be delighted to engage in further discussions. Thank you.\n\n**Q2: The matrix construction is similar to Transformer-M.**\n\nWe thank the reviewer for the comment. It is imperative for us to elucidate that our primary emphasis lies in the development of an innovative \"pretraining algorithm\", and follow the model backbone of Transformer-M [1] and Graphformer [2]. In Section 3.2, we explicitly cite that we follow Transformer-M and Graphformer to choose 2D Shortest Path distance, 2D Edge Type, and 3D Euclidean distance as atom-relation encodings.\n\nIt is crucial to underscore that our focus lies in improving the **incorporation** of the given 2D and 3D information, while the specific choice of 2D and 3D features is not our focus. Our method is generic and can be applied to any given 2D and 3D matrix, orthogonal to the specific choice.\n\nTo draw an analogy, our work can be likened to the relationship of BERT to Transformer. In the realm of pretraining, our concentration is the formulation of an effective training algorithm. The design of the model backbone and data encoding falls outside the scope.\n\n[1] One transformer can understand both 2d & 3d molecular data. ICLR 2023\n[2] Do transformers really perform badly for graph representation? NIPS 2021\n\n**Q3: Ablations on blending two masks.**\n\nWe thank the reviewer for the suggestion. The ablation results are presented in the table below, showing that blending 3 kinds of atom relations achieves best performance.\n\n| Method | BBBP | Bace | Tox21 | ToxCast |\n| --- | --- | --- | --- | --- |\n| SPD mask | 68.95  | 80.64  | 75.59  | 62.82 |\n| Edge mask  | 69.02  | 81.97  | 76.01  | 63.81 |\n| 3D mask   | 67.60  | 80.35  | 75.65  | 63.28 |\n| SPD_EDGE | 70.56 | 82.37 | 75.07 | 65.12 |\n| EDGE_3D | 71.12 | 81.43 | 76.07 | 65.10 |\n| SPD_3D | 71.23 | 79.1 | 75.90 | 65.04 |\n| Blending   | **71.68** | **83.41** | **76.58** | **65.46** |\n\n**Q4: details of the experimental setup.**\n\nWe have made revisions to our paper (Section A.1). Below we address the detailed questions respectively.\n\n> Baseline results:\n\n- GraphMVP, MoleSDE, GraphCL, GraphMAE are from their own paper.\n- AttrMask, ContextPred, InfoGraph, MolCLR from MoleculeSDE\n- MoleBERT, 3D Infomax are from MoleBERT\n- The results of GROVER are from Uni-Mol\n\nAll compared methods use the same dataset split.\n\n> Table 4, does the single-modality mask-then-predict strategies use the same network as the proposed blending strategy?\n\nYes, the compared experiments use the same network. All settings are the same, isolating training objective as the varying factor for ablation."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482910101,
                "cdate": 1700482910101,
                "tmdate": 1700485006358,
                "mdate": 1700485006358,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Tnslh4zVS0",
                "forum": "oM7Jbxdk6Z",
                "replyto": "bnKGSvn6Ch",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XaRC (2/2)"
                    },
                    "comment": {
                        "value": "**Q5: The conclusion in section D.1 is not well supported.**\n\nWe thank the reviewer for the comment. We sincerely agree that more experiments are needed to support such conclusions. Due to this is not our primary focus and the convergence comparison with more 3D models needs further detailed investigation, we delete this experiment from our paper and leave it as future work.\n\n**Q6: Why run ablation studies on different datasets and different tasks?**\n\nWe thank the reviewer for the question. For our final model, we evaluate it on a broad range of tasks (8 MolNet classification tasks, 3 MolNet regression tasks, and 12 QM9 tasks).\n\nFor ablations, we assess partial tasks for two reasons: 1) the conclusion typically shares across different tasks, and can already be apparently established on conducted experiments; 2) conducting ablations on all datasets and tasks would be prohibitively costly.\n\nFor ablation of Table 3, we evaluate full 8 molnet classification tasks plus two QM9 tasks. As for the ablations in Table 4 and 5, where the conclusions are clear and easily established due to a substantial performance margin, we opt to evaluate only on a subset of tasks from MolNet and QM9. These selected tasks align with our internal development process and serve as a validation of our methodology. In these instances, we believe it is unnecessary to conduct evaluations on all tasks, given the evident and easily comprehensible conclusions arising from the observed substantial margins."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482950527,
                "cdate": 1700482950527,
                "tmdate": 1700484994580,
                "mdate": 1700484994580,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5wR5AR3dN5",
                "forum": "oM7Jbxdk6Z",
                "replyto": "bnKGSvn6Ch",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Anticipating Your Participation as Reviewer-Author Discussion Deadline Approaching"
                    },
                    "comment": {
                        "value": "We express our gratitude to the reviewer for dedicating time to review our paper. **We have provided comprehensive clarifications to all the questions, e.g. 1) the information gathered in the relation matrix is limited, 2) the similarity to Transformer-M.** As the discussion deadline looms within a day, we would like to inquire if our responses have adequately addressed your questions. We anticipate your participation in the Reviewer-Author discussion phase, as your insights are invaluable for refining and enhancing the quality of our paper. Should you have any additional queries or require further clarification, please do not hesitate to inform us. We are more than willing to address any concerns and ensure a comprehensive resolution. Thank you for your time and consideration."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657550802,
                "cdate": 1700657550802,
                "tmdate": 1700657550802,
                "mdate": 1700657550802,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]