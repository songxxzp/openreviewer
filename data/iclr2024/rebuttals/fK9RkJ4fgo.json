[
    {
        "title": "Stochastic interpolants with data-dependent couplings"
    },
    {
        "review": {
            "id": "oWdPt8cXjX",
            "forum": "fK9RkJ4fgo",
            "replyto": "fK9RkJ4fgo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9016/Reviewer_1uKk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9016/Reviewer_1uKk"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces couplings between the prior and the target distribution in order to do (conditional) generative modelling. Here, plenty of former generative modelling frameworks are generalized. First the authors introduce a stochastic interpolant with coupling, then it is shown that the density satisfies the transport equation and a loss is derived. Furthermore, in the style of diffusion forward and reverse SDEs are introduced. Then this approach is applied to (class) conditional sampling for instance image superresolution and inpainting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea is very neat and the theory seems well-executed. Qualitatively the experiments look really nice. Furthermore, the approach presents a nice unifying framework for many papers attempting to handcraft the couplings."
                },
                "weaknesses": {
                    "value": "1) The biggest glaring weakness is the lack of quantitative experiments. While I am a huge fan of the idea and the developed theory, I think quantitative experimental evaluation is necessary for acceptance. An appropriate baseline could be the OT coupling flow from Tong et al.  \n\n2) In the proof of Theorem 1, I do not fully understand one step. In equation (23) for the second equality apparently the definition of conditional expectation is used. Please clarify this via some additional justification and the definition of conditional expectation. You kinda have to use that the expected value only depends on (the time derivative) of the stochastic coupling. \n\n3) Is there any intuitive interpretation for the losses like in the diffusion case? \n\n4) It would be nice to see failure cases of  joint learning of time coefficients and score. Since one simultaneously learns the time coefficients $\\alpha$, $\\beta$ .. and the $g$ I am expecting some not so nice local minima when one is not careful with initializing. Did you encounter any of those? Why didnt you use it in the inpainting/superres experiments and decided to fix $\\alpha$ and $\\beta$?\n\n5) In the paper it is discussed, that one needs $\\sigma>0$ in the superresolution experiment. If not one would try to establish a normalizing flow between a lower and a higher dimensional manifold. However when one thinks about the inpainting example filling all the boxes with random (Gaussian) noise could lead to an overestimation of the dimension of the target data, therefore prohibiting \"a true transport equation\" to hold as this defines a normalizing flow. \n\n6) Is it possible to derive the losses from the forward/reverse SDEs so one does not necessarily enforce invertibility?\n\n7) A small nitpick: I think the formulation \"reverse\" SDE is more appropriate since backward has a different meaning in the probability theory context."
                },
                "questions": {
                    "value": "See weaknesses. Overall I appreciate the idea, but imo the following three things would greatly strengthen the paper: some quantitative evaluation, some discussion on the \"invertibility\" constraints and showing that learning of the schedules $\\alpha,...$ also works in these image examples."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9016/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9016/Reviewer_1uKk",
                        "ICLR.cc/2024/Conference/Submission9016/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9016/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698067154578,
            "cdate": 1698067154578,
            "tmdate": 1700392149457,
            "mdate": 1700392149457,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bwe81c7eB4",
                "forum": "fK9RkJ4fgo",
                "replyto": "oWdPt8cXjX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9016/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Really to reviewer 1uKk"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their input and address their main concerns one-by-one below. We also refer them to our general reply for some more information.\n\n\n***Lack of quantitative experiments:*** We thank the reviewer for raising this point. We are currently running additional quantitative tests by comparing FID scores for our approach to existing baselines in the literature on super-resolution and in-painting. These results are shown in the table in our summary reply, where we achieve improved scores relative to earlier methods. We are continuing to train our models and expect the metrics to further improve for when we post our revision.\n\n***Proof of Theorem 1:*** The penultimate step in Eq. (23) uses the the tower property of the conditional expectation. That is, \n$$\\mathbb{E}[(\\dot\\alpha x_0+ \\dot\\beta x_1 + \\dot\\gamma z)\\cdot \\nabla \\phi(x_t)] = \\mathbb{E}[\\mathbb{E}[(\\dot\\alpha x_0+ \\dot\\beta x_1 + \\dot\\gamma z)\\cdot \\nabla \\phi(x_t)|x_t]].$$ The last step uses the fact that the conditioning of $\\phi(x_t)$ on $x_t$ is trivial, i.e.\n$$ \\mathbb{E}[\\mathbb{E}[(\\dot\\alpha x_0+ \\dot\\beta x_1 + \\dot\\gamma z)\\cdot \\nabla \\phi(x_t)|x_t]]\n= \\mathbb{E}[\\mathbb{E}[(\\dot\\alpha x_0+ \\dot\\beta x_1 + \\dot\\gamma z)|x_t]\\cdot \\nabla \\phi(x_t)]$$\n\n***Role of $\\alpha(t)$, $\\beta(t)$, and $\\gamma(t)$:*** In the present paper, these functions of time are prescribed and are not optimized upon. The optimization is only performed over the parameters in the velocity field.\n\n***Invertibility:*** The flow from base to target that we construct does not need to be invertible. In particular, it can map samples from a higher-dimensional manifold to samples on a lower-dimensonal one (though this comes at the expense of requiring a velocity that is singular at $t=1$). In contrast, traversing from a lower-dimensional manifold to a higher-dimensional one is more problematic, since it requires the velocity to be singular at $t=0$, and therefore there is ambiguity on how to start the flow. To avoid this latter scenario, we add a bit of noise to the sample from the base density that we use. \n\n\n***Interpretation of the loss:*** One loss gives the conditional velocity appearing in the transport equation. The second loss gives the denoiser $\\mathbb{E}[z|x_t=x]$, which allows us to estimate the score since $\\nabla \\log \\rho(t,x) = - \\gamma^{-1}(t) \\mathbb{E}[z|x_t=x]$. That is, the loss for the denoiser $\\mathbb{E}[z|x_t=x]$ can be viewed as a reweighted version of the loss for the score $\\nabla \\log \\rho(t,x)$. This trick is exploited in diffusion models when learning a \"noise model\".\n\n\n**Reversed-time vs backward SDE:** We agree that the first denomination is better and we have adopted it."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240172081,
                "cdate": 1700240172081,
                "tmdate": 1700240172081,
                "mdate": 1700240172081,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wKHtJKrum9",
                "forum": "fK9RkJ4fgo",
                "replyto": "bwe81c7eB4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9016/Reviewer_1uKk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9016/Reviewer_1uKk"
                ],
                "content": {
                    "title": {
                        "value": "Good update"
                    },
                    "comment": {
                        "value": "Thanks for the response.\n\nRegarding the invertibility, this of course makes sense. As far as I understand your theory you basically need to assume Lebesgue densities for theorem 1 and corollary 2, right? In standard diffusion this is usually not needed, and there might be a way with your forward/reverse formulation to even derive a loss in this case. \n\nregarding the other points: the empirical evaluation seems now great, thanks! \n\nI am raising my score to 6. Please update the paper correspondingly, so I can check the updates."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700392122944,
                "cdate": 1700392122944,
                "tmdate": 1700392122944,
                "mdate": 1700392122944,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "X91Fh1Iw2d",
            "forum": "fK9RkJ4fgo",
            "replyto": "fK9RkJ4fgo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9016/Reviewer_RKsJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9016/Reviewer_RKsJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends the framework of stochastic interpolants to conditional generation. In particular, one conditions the interpolating density between $x_0$ and $x_1$ with a conditional $\\xi$. The $\\xi$ can be incorporated in a data dependent and independent way, which allows for applications in conditional generation as well as upsampling/infilling."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The paper is theoretically sound, as the derivations follow directly from the continuity equation.\n* Additionally, some experiments show the method's viability for common image generation tasks."
                },
                "weaknesses": {
                    "value": "* I'm not sure the method is that original in practice. In particular, the paper notes that much of the construction can be connected with existing SDE and ODE formulations, all of which depend on the score function ([1] for the straight path ODE that is described in the paper, otherwise the standard OU process). In that case, the conditional methodology would follow from the score function argument as well, implying there would be little difference on the empirical side with existing methodologies. However, the proposed framework does generalize beyond this to other base distributions (for example), so I would expect (or rather, like to see) more empirical emphasis to be placed on this setting.\n* In a similar vein, for the inpainting experiments, there is a big issue in that existing score based methods (e.g. ScoreSDE) can inpaint (up to some approximation error + some necessary hacks) without having to retrain, while the current results come about through retraining.\n* The experiments don't give me that much confidence. In particular, the results are entirely qualitative (meaning they can be easily cherrypicked). For the upsampling experiments, I want to see some numerical comparisons against the standard cascaded diffusion models setup (eg generate 64x64 and upscale to 256x256 to compare FIDs).\n\n[1] https://arxiv.org/abs/2303.00848"
                },
                "questions": {
                    "value": "Nothing beyond addressing the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9016/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9016/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9016/Reviewer_RKsJ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9016/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784541459,
            "cdate": 1698784541459,
            "tmdate": 1700340078668,
            "mdate": 1700340078668,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zGXwVZl4EV",
                "forum": "fK9RkJ4fgo",
                "replyto": "X91Fh1Iw2d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9016/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer RKsJ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their input and we address their main concerns one-by-one below. We also refer them to our general reply for further information.\n\n***Data-dependent coupling vs. class-conditioning:*** We stress that there is a distinct difference between putting conditional information in the velocity/score field and learning a map between coupled densities. For a visual explanation, see the figure posted [**here**](https://drive.google.com/file/d/1J04tNwIAkgaHXFpGSgse96o1WXGs2igc/view?usp=drive_link). The point of our paper is that data-dependent couplings can offer advantages that are orthogonal to (but can be combined with) conditional information placed in the velocity field. To emphasize this point, we have removed the conditional variable $\\xi$ from the main text, and have clarified how it can be used with our coupling framework in the appendix.\n\n***Originality:*** Our method with data-dependent coupling is general, as the base density $\\rho(x_0|x_1)$ can be designed in many different ways. This offers a degree of design flexibility that is not available to existing approches.\n\n***Need to retrain in the in-painting experiments:*** We would kindly like to request some clarification on the precise meaning of \"retraining\". In the response below, we have assumed that this refers to \"classifier\" or \"guidance\"-based methods for diffusion models, which can leverage a pre-trained model for conditioning. If this is incorrect, please let us know and we will provide an additional reply.\n\nTo this end, we would first like to emphasize that we focus here on data-dependent couplings, as emphasized in our summary reply above. Nevertheless, to improve performance in super-resolution and in-painting, we *also* leverage conditional information. We would like to point out that conditioning can be used to improve performance in our method, but is not strictly necessary. For example, for in-painting, our coupling pairs a masked image with its unmasked counterpart. By contrast, a guidance-free diffusion model would likely perform much worse because it would have no information about the mask.\n\nWe would also like to stress that our conditioning is over *high-dimensional images*, rather than the more standard scalar class label leveraged in guidance-based approaches. This means that the corresponding guidance model used in score-based methods is a **second** high-dimensional generative model, and its training is **comparably expensive** to the problem we solve here. In this sense, we would like to emphasize that our method does not require \"retraining\", as guidance-based methods would require an equivalent training effort.\n\nAs stated above, please let us know if this is not what was meant by \"retraining\", and we will provide further explanation.\n\n***Confidence in the experiments:*** We have added the requested FID benchmarks and see that they outperform the existing approaches."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240121429,
                "cdate": 1700240121429,
                "tmdate": 1700240121429,
                "mdate": 1700240121429,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0spNbhYHfl",
                "forum": "fK9RkJ4fgo",
                "replyto": "zGXwVZl4EV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9016/Reviewer_RKsJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9016/Reviewer_RKsJ"
                ],
                "content": {
                    "title": {
                        "value": "Found New Experiments Compelling,"
                    },
                    "comment": {
                        "value": "Thanks for including the new experiments, which show that the method is able to achieve much better superresolution results than previous methods. Perhaps a comparison with ADM (Dhariwal and Nichol 2021) is applicable, since they also do 64x64->256x256 superresolution. \n\nFor the inpainting, can there be a comparison with the method from Song et al 2021 (ScoreSDE)? I am not sure if there is a public checkpoint for diffusion models on ImageNet256x256, but perhaps we can compare on a smaller ImageNet scale (like 64x64) using a public checkpoint for an ImageNet UNet.\n\nIn light of the experimental results for super-resolution, I am tentatively raising my score to a 6."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700340068774,
                "cdate": 1700340068774,
                "tmdate": 1700340068774,
                "mdate": 1700340068774,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VR50zOmxiZ",
                "forum": "fK9RkJ4fgo",
                "replyto": "X91Fh1Iw2d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9016/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comment on the  suggested experiments"
                    },
                    "comment": {
                        "value": "Thank you for the feedback! \n\nFor ***super-resolution***, [Ho et al 2021] (Cascading Diffusion Models, CDM) actually compares against the [Nichol and Dhariwal, 2021] (Improved DDPM) and [Dhariwal and Nichol, 2021] (ADM/guided-diffusion) models mentioned. For 64x64->256x256, [Ho et a 2021] reports FID scores of 12.26 for improved DDPM; 7.49 for ADM; and 4.88 for CDM.  ****The method we propose gives an FID score of 2.13****.  While the numbers here are reported from the tables in [Ho et al 2021], we will be glad to additionally run versions of them in our codebase over the coming weeks and report both the original and reproduced numbers.\n\nRegarding ***inpainting benchmarks***: after reading through the works you suggested, we found that [Song et al 2021] do not train any Imagenet models and that [Song, Durkan, et al 2021] only train on Imagenet 32x32. We have not trained any models at the lower resolutions of 32x32 and 64x64, so we will need time to do this.\n\nWe will work on these additional benchmarks. In the meantime, we hope that our internal benchmark, in which we compare the results of our approach with data-coupling to those obtained using the interpolant framework with conditioning only and no data-coupling (similar to what score-based diffusion models would use), is informative: using only conditioning  performs well visually, but with worse FID score (1.35) than what we get with data-conditioned (**1.13**): see the table in the main reply above). \n\n[Ho et a 2021] Cascaded Diffusion Models for High Fidelity Image Generation\n\n[Nichol and Dhariwal, 2021] Improved Denoising Diffusion Probabilistic Models\n\n[Dhariwal and Nichol, 2021] Diffusion Models Beat GANs on Image Synthesis\n\n[Song et al 2021] Score-based Generative Modeling through Stochastic Differential Equations\n\n[Song, Durkan, et al 2021]  Maximum Likelihood Training of Score-Based Diffusion Models"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497687892,
                "cdate": 1700497687892,
                "tmdate": 1700508568066,
                "mdate": 1700508568066,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bEaJBg0UB6",
            "forum": "fK9RkJ4fgo",
            "replyto": "fK9RkJ4fgo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9016/Reviewer_v6gN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9016/Reviewer_v6gN"
            ],
            "content": {
                "summary": {
                    "value": "The paper formalizes conditional and data-dependent generative modeling within the stochastic interpolates framework. The authors derive the relevant transport equation for the deterministic scenario (ODE) and the forward and backward SDE for the stochastic scenario. They demonstrate that these equations can be acquired by minimizing straightforward regression losses. Lastly, data-dependent coupling is introduced, providing a recipe for constructing base densities that depend on the target distribution."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper formalizes two important notions in generative modeling, conditional and data dependent coupling, in the stochastic interpolates framework. \n- The authors show how to construct both conditional and data-dependent coupling."
                },
                "weaknesses": {
                    "value": "1. **Limited contribution** - the work does not introduce a new concept and is a formulation of existing concepts into an existing framework. \n\n   1. The derivation of the transport equations in section 2.1, which takes a great portion of the paper, was already done in section 4 of [3] for the unconditional case, where the addition of the conditioning repeats the same derivation with marginalization over the condition. Furthermore, conditioning for super-resolution has been shown in [5] as well as beening widely used in diffusion models (e.g., [4]), and since they can be thought of as particular cases of stochastic interpolants, the addition of conditioning is straightforward. \n   2. Data dependent coupling was already introduced in the context of Flow-Matching [1,2], which is an essentially equivalent framework to stochastic interpolants. \n\nWhile the work provides a coherent, complete formulation of conditional and data-dependent generative modeling in the stochastic interplant framework, I believe the paper needs to be reframed and further emphasize the analogies to existing works and highlight the benefits of formulating these concepts in the stochastic interpolants framework as opposed to for example Flow-matching which already provides the same degrees of flexibility in the design of generative models, or another example, the inpainting application considered in section 3.1 which is equivalent to the setting used in [4] only with a different noise scheduling. \n\n   \n2. **Empirical evaluation** - the empirical evaluation is solely qualitative, which makes it impossible to assess whether there is a benefit in using conditional and data dependent couplings in the stochastic interpolant framework. \n\n\n[1] Pooladian et. al., Multisample Flow Matching: Straightening Flows with Minibatch Couplings (2023)\n\n[2] Tong et. al., Improving and generalizing flow-based generative models with minibatch optimal transport (2023)\n\n[3] Albergo et. al., Stochastic Interpolants: A Unifying Framework for Flows and Diffusions (2023)\n\n[4] Saharia et. al., Palette: Image-to-Image Diffusion Models (2022)\n\n[5]. Lipman et. al., Flow Matching for Generative Modeling (2022)"
                },
                "questions": {
                    "value": "1. Can the authors emphasize the analogies to existing works and highlight the benefits of formulating conditioning and data-dependent coupling concepts in the stochastic interpolants framework?\n2. Does this formulation add flexibility compared to [1,2] or [6] which uses both conditioning and data dependent coupling (sec 5.3). (I'm aware [6] is not to be considered as previous work, but I want to understand the superiority of this work over applications already present in other frameworks).\n\n[6] Song et. al., [Equivariant Flow Matching with Hybrid Probability Transport for 3D Molecule Generation](https://openreview.net/pdf?id=hHUZ5V9XFu)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9016/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838222593,
            "cdate": 1698838222593,
            "tmdate": 1699637135490,
            "mdate": 1699637135490,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZBMBX2WnXY",
                "forum": "fK9RkJ4fgo",
                "replyto": "bEaJBg0UB6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9016/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer v6gN"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their input and we address their main concerns one-by-one below. We also refer them to our general reply for further information.\n\n***Derivation:***\n- While class-conditioning is indeed easily incorporated into the interpolant framework, coupling the base density to the data is a more significant change compared to the work in [3]. The resulting derivation of the transport equation follows similar steps, but the way to construct the coupled density $\\rho_0(x_0,x_1) = \\rho_0(x_0|x_1) \\rho_1(x_1)$ via proper definition of $\\rho_0(x_0|x_1)$ is non-trivial, as emphasizd in the next point.\n\n***Data-dependent coupling vs mini-batch OT:*** \n- The data-dependent coupling introduced here is general and can be used with any suitable $\\rho(x_0,x_1) = \\rho_0(x_0|x_1) \\rho_1(x_1)$, which can be tailored to the application domain. In the experiments, we focus on examples in which the base $\\rho(x_0 | x_1)$ is Gaussian conditional on the data $x_1$, but we stress that both the coupled density $\\rho(x_0, x_1)$ and the marginal density $\\rho_0(x_0) = \\int \\rho(x_0,x_1) dx_1$ are *not* Gaussian even in this case.\n- While related work by [1] and [2] showed how to construct minibatch couplings that try to approximate the *optimal* coupling, their perspective does not give tools to perform the more general types of data-dependent modeling we consider here. Moreover, the couplings we introduce are exact, and do not require (but also do not preclude) the use of additional algorithms such as Sinkhorn.\n\nWe believe that these features make the approach we propose more flexible and performant than existing methods. In the revision, we will rework our presentation to clarify these points. We will also notify the reviewer as soon as the revised version becomes available.\n\n***Empirical evaluation:*** \n- As mentioned in our general reply, we have added FID benchmarks to demonstrate the utility of our data-dependent coupling.\n\n**Questions:**\n> *Can the authors emphasize the analogies to existing works and highlight the benefits of formulating conditioning and data-dependent coupling concepts in the stochastic interpolants framework?*\n- To re-emphasize our previous points, the main novelty of our work is to propose a generic way to perform data-dependent coupling. To do so, we exploit the flexibility in the base density afforded by the stochastic interpolant framework. This is different from class-conditioning (though the two approaches can be used in conjunction) and it offers a degree of design flexibility that is not available to existing methods. \n\n> *Does this formulation add flexibility compared to [1,2] or [6] which uses both conditioning and data dependent coupling (sec 5.3). (I'm aware [6] is not to be considered as previous work, but I want to understand the superiority of this work over applications already present in other frameworks).*\n- For our reply about references [1] and [2], please see the second bullet under ***Data-dependent coupling vs mini-batch OT***.\n- In the recent reference [6], to the best of our knowledge, the coupling proposed is another OT coupling akin to that of [1] and [2]. While the iterative algorithm presented there is interesting, and while we will surely cite this work, it remains a somewhat orthogonal to the general method we propose here.\n\n[1] [Pooladian et. al., Multisample Flow Matching: Straightening Flows with Minibatch Couplings (2023)](https://arxiv.org/abs/2304.14772) \n\n[2][Tong et. al., Improving and generalizing flow-based generative models with minibatch optimal transport (2023)](https://arxiv.org/abs/2302.00482)\n\n[3] [Albergo et. al., Stochastic Interpolants: A Unifying Framework for Flows and Diffusions (2023)](https://arxiv.org/abs/2303.08797)\n\n[4] [Saharia et. al., Palette: Image-to-Image Diffusion Models (2022)](https://arxiv.org/abs/2111.05826)\n\n[5] [Lipman et. al., Flow Matching for Generative Modeling (2022)](https://arxiv.org/abs/2210.02747)\n\n[6] [Song et. al., Equivariant Flow Matching with Hybrid Probability Transport for 3D Molecule Generation](https://openreview.net/pdf?id=hHUZ5V9XFu)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240063389,
                "cdate": 1700240063389,
                "tmdate": 1700240063389,
                "mdate": 1700240063389,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fTNC7D8DXm",
                "forum": "fK9RkJ4fgo",
                "replyto": "bEaJBg0UB6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9016/Reviewer_v6gN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9016/Reviewer_v6gN"
                ],
                "content": {
                    "title": {
                        "value": "Follow up question"
                    },
                    "comment": {
                        "value": "I thank the authors for their response. \n\n**Data-dependent coupling vs mini-batch OT:** I better understand the subtleties now. [1,2] focus on constructing the coupling satisfying both marginal constraints, i.e., define $\\rho(x_0,x_1)$ such that $\\rho_0(x) = \\int \\rho(x_0,x_1) dx_1$, $\\rho_1(x) = \\int \\rho(x_0,x_1) dx_0$ given $\\rho_0$ and $\\rho_1$. In this work, the coupling is assumed to be a given  $\\rho(x_0,x_1)$ and the paper shows how to construct $\\rho_0$ implicitly by defining $\\rho(x_0|x_1)$ given the coupling. \n\nThe above perspective drives me to the realms of image to image translation applications (as also shown in the paper) but I believe there is not enough discussion on the relations to that. A very closely related paper with a great overlap in the basic ideas is [7]. Although framed under the Schr\u00f6dinger Bridge formulation, to my understanding, the concepts are very similar except for this work smoothing with a gaussian the base $\\rho(x_0|x_1)$. Furthermore, [7] has already shown the benefits of changing the base distribution over conditioning (like you showed in the additional experiments now).\n**Can the authors comment on the relation to this work?**\n\nIn that case, I stand by my statement that I believe the contribution of this paper is limited. \n\nI do appreciate the formulation in another framework and I think the stochastic interpolants framework is simple and elegant, but this paper either needs to be reframed as a \"formulation paper\" and not as one introducing new concepts and adding a thorougher discussion on existing works or show strong experimental evaluation and ablations for the benefits of the subtle differences proposed in this work. \n\n\n\n\n[7] [Liu et. al., $I^2SB$: Image-to-Image Schr\u00f6dinger Bridge (ICML 2023)](https://arxiv.org/abs/2302.05872)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556038351,
                "cdate": 1700556038351,
                "tmdate": 1700556090234,
                "mdate": 1700556090234,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]