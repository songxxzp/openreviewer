[
    {
        "title": "Conservative Reinforcement Learning by Q-function Disagreement"
    },
    {
        "review": {
            "id": "sNUrC4G4jJ",
            "forum": "Cfi68cGzIt",
            "replyto": "Cfi68cGzIt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3825/Reviewer_bv75"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3825/Reviewer_bv75"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new Q-learning algorithm by slightly modifying the Double Q-learning's objective function. More precisely, the authors propose to subtract the Double-Q objective by a \"std\" term, computed as the standard deviation of the two Q-value networks. Experiments are provided to compare the new modification with existing Q-learning algorithms, using tasks from the Mujoco simulation.\n\n## After rebuttal\n\n\nI cannot access the responses to my three questions.\n\nThe other responses did not solve my primary concern that the paper's contribution is too incremental and lacks in-depth analysis. I, therefore, maintain my score."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is clear, solid, and easy to read. I actually like this style of writing. \nThe idea of adding the \"std\" term to the Q-learning objective makes sense to me -- since it aims to reduce the gap between two Q values, this term would make the convergence faster. The new term is also easy to implement."
                },
                "weaknesses": {
                    "value": "The contributions are too incremental. It is just a minor adaptation of the Double Q-learning algorithm. The majority of the paper is devoted to describing well-known methods. The main idea is presented in less than 2 pages with little insight.\n\nIn fact, the term \"std\" is just to reduce the absolute gap between the two value networks, and I do not see it should be considered as a significant contribution. Moreover, computing standard deviation using only two samples is nonsense to me. The regularizer term is just to reduce the gap between two Q-values, so something like |Q_1 \u2013 Q_2| would be enough. The authors should not name this term as \"standard deviation.\"\n\nThe experiments are not very convincing. For instance, Table 4 shows that the average improvement rate is negative (so the new approach is worse than its counterpart)."
                },
                "questions": {
                    "value": "1. It is worth showing the gap between the two Q-values during the training, before and after the inclusion of the \"std\" term.\n2. Would simpler terms such as norm(Q_1-Q_2) work?\n3. When talking about \"std,\" I would expect to see more samples. Maybe the authors could consider adding more Q-value networks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3825/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3825/Reviewer_bv75",
                        "ICLR.cc/2024/Conference/Submission3825/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3825/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698642471832,
            "cdate": 1698642471832,
            "tmdate": 1700681832948,
            "mdate": 1700681832948,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lLdZ4HMist",
                "forum": "Cfi68cGzIt",
                "replyto": "sNUrC4G4jJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3825/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3825/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comment"
                    },
                    "comment": {
                        "value": "You touched on the main motivation for the article \u2013 a short and clear formula for a sharp improvement of an RL algorithm complex.\n\nThe formula is essentially different from that of Double Q-learning.\n\nThe structure of the article is clear and essentially similar to the nature of most articles in the literature.\n\nThe greatness of the article is a short and clear formula that gives a sharp improvement to a line of RL algorithms with a minimum of code \u2013 half a line.\n\nYou touched on an important point \u2013 the article presents a very simple general method for implementing the formula in the form of an RL algorithm.\n\nIt will obviously work on a larger ensemble.\n\nUnderstanding the phenomenon on a larger ensemble is beyond the scope of the article.\n\nThe method you suggested will work at most \u2013 on an ensemble of size 2.\n\nThe name of the formula distills its essence \u2013 standard deviation between Q-networks.\n\nThe article focuses on TD7 as state-of-the-art and shows an improvement of 5% overall.\n\nThe purpose of the rest of the experiments is to show that the formula works practically on other algorithms \u2013 a general formula.\n\nQ1: It is worth showing the gap between the two Q-values during the training, before and after the inclusion of the \"std\" term.\nQ2: Would simpler terms such as norm(Q_1-Q_2) work?\nQ3: When talking about \"std,\" I would expect to see more samples. Maybe the authors could consider adding more Q-value networks?\n\nA1: Attached\nA2: Attached, works only for two Q-networks.\nA3: Attached.\n\nA1-3: https://docs.google.com/document/d/1IbTyH4kAjA6TmAlHDmFlwCldld-zb04FFGwkN2akmNY/edit?usp=sharing"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3825/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409099436,
                "cdate": 1700409099436,
                "tmdate": 1700409099436,
                "mdate": 1700409099436,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "n45NUKv9k5",
            "forum": "Cfi68cGzIt",
            "replyto": "Cfi68cGzIt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3825/Reviewer_n5m3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3825/Reviewer_n5m3"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a variant of value-based policy optimization methods that can be applied to several deep RL algorithms.\nIn particular, for any algorithm with one or more copies of the Q-function approximator, the authors propose to measure the \"disagreement\" among the two (ore more) models on a batch of state-action pairs as the average (over the batch) of the empirical standard deviation (across the Q functions), and subtract to the Q-learning target a penalty that is proportional to this measure of disagreement. The intuitive idea is to be conservative (that is, underestimate the Q value) whenever there is a lot of uncertainty, testified by the disagreement. This technique can be used in several deep RL algorithms: double Q-learning, DDPG, TD3, and more. The benefits of this technique are tested on Mujoco and D4RL tasks, comparing with the performance of the original algorithms (TD3, TD7, MaxMin Q-Learning, REDQ)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is generally clear and well written. The experiments are well designed, the empirical results are communicated with an appropriate level of detail."
                },
                "weaknesses": {
                    "value": "The work is fundamentally incremental. A small variant is applied on top of state-of-the art deep RL algorithms to show some improvement in the learning curves.\nFirst of all, the improvement is not particularly consistent over the different algorithms and tasks that are considered. In many of them, the improvement is not statistically significant, and there is even a slight degradation in some cases.\nMore importantly, there is no strong motivation underlying the proposed technique, besides the brief intuitive motivation that is provided. Previous approaches to mitigate the overestimation bias of Q-learning are mentioned, but not discussed critically. Nor did the author discuss the similarities between the proposed approach and other methods that try to capture the uncertainty of value estimates, such as distributional RL or ensembles. The latter were only mentioned as another family of algorithms that may benefit from Q-function disagreement.\nSimilarly, there is no in depth discussion on why the proposed technique works in some cases, and not in others. There is no theory or ablation studies to understand the effect of the proposed algorithmic addition in better detail.\nThe writing, although generally good, seems rushed in some ways: for example, many parentheses are missing from the citations."
                },
                "questions": {
                    "value": "How did you select the hyperparameters of the different algorithms, especially alpha?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3825/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786775955,
            "cdate": 1698786775955,
            "tmdate": 1699636339926,
            "mdate": 1699636339926,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kKqaGRetBX",
                "forum": "Cfi68cGzIt",
                "replyto": "n45NUKv9k5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3825/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3825/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comment"
                    },
                    "comment": {
                        "value": "You touched on the main motivation for the work \u2013 a short and refined formula for a sharp improvement in the results of RL algorithms.\n\nThe work focuses on TD7 as state-of-the-art and shows a +5% overall improvement.\n\nThere isn't a single RL algorithm that beats all RL algorithms in all problems \u2013 if that's the case \u2013 what's the need for the rest of RL algorithms?\n\nThe purpose of the rest of the experiments is to practically demonstrate that the formula is general \u2013 it can work on any RL algorithm.\n\nThe motivation is clear.\n\nThe exact formula was distilled by a series of practical experiments.\n\nThe paper focuses on one core contribution \u2013 SQT.\n\nThe article distills the relevant material from all the works \u2013 it does not pretend to present the work of other researchers who understand it better than I do.\n\nDistributional RL methods model a rewards distribution instead of the expected rewards \u2013 this is a separate algorithmic class.\n\nThe article focuses on TD7 as state-of-the-art and shows a 5% improvement. Additional experiments on REDQ and MaxMin aim to demonstrate that the formula works on a wide variety of RL algorithms.\n\nThere is no RL researcher who fully understands the results of his algorithm.\n\nThis is a practical paper \u2013 no theory.\n\nThe citations are clear \u2013 this is our style.\n\nQ: How did you select the hyperparameters of the different algorithms, especially alpha?\n\nA: Empirically per problem."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3825/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408749356,
                "cdate": 1700408749356,
                "tmdate": 1700408749356,
                "mdate": 1700408749356,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]