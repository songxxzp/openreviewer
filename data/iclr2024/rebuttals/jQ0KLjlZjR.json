[
    {
        "title": "CookingCLIP\uff1aLearning a Contextualized Multimodal Embedding from Instructional Cooking Videos for Zero-shot Recipe Generation"
    },
    {
        "review": {
            "id": "SmkCPCU2kr",
            "forum": "jQ0KLjlZjR",
            "replyto": "jQ0KLjlZjR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3855/Reviewer_f8ZN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3855/Reviewer_f8ZN"
            ],
            "content": {
                "summary": {
                    "value": "The paper generates recipes with timestamps for each step from a video. Their architecture composes a pair of original CLIP encoders for each modality, each of which being extended with a pair of auto-regressive decoders for localization and caption generation. Experiments on YouCook and CrossTask demonstrate the effectiveness of the proposed embedding."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Their paper both generates the natural language steps and  temporal localizations for each step.\n2. Their method outperforms the baselines in terms of Bert-Score and CLIP-Score."
                },
                "weaknesses": {
                    "value": "1. There notations are not well-defined in the paper. Different variables use the same annotation, e.g. W can be timestamps (Experiment section),  embeddings (eq (1)) and tokens (Wm=EOS), which is very confusing. wk is not defined or used later either, only the recurrence function is given.\n2. It seems temporal localization is a significant bottleneck of the method. Both from the qualitative examples and Table 3, where the mIoU of the proposed model is significantly lower than the baseline. Action detection/localization models could also be included as baselines. The annotated step timesteps are included in the dataset, ASR can include dense timestamps and there are datasets with dense timestamp annotation for each step/sentence (EPIC-KITCHENS), which can be utilized.\n3. Bert-Score and CLIP-Score needs to be justified. E.g. In table 1 step 6, the difference of CLIP score between the methods is much smaller than Bert-Score. Traditional scores like BLEU-4, METEOR, CIDEr, and ROUGE also needs to be reported. Metrics focusing on actions and entities can also be reported."
                },
                "questions": {
                    "value": "1. Figure 4, why m seems to decrease as \\lamda increases? Which is counterintuitive according to eq (1)\n2. In Section A1.1.1, where is the title used in the method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3855/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698141746858,
            "cdate": 1698141746858,
            "tmdate": 1699636344178,
            "mdate": 1699636344178,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "zjLAmG64uo",
            "forum": "jQ0KLjlZjR",
            "replyto": "jQ0KLjlZjR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3855/Reviewer_Qj9E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3855/Reviewer_Qj9E"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to pre-train vision-language models from instructional video. The method aims to extend a clip-like model with temporal context by explicitly dividing time-series features into `when` and `which.`\nThe pre-trained model is evaluated on YouCook2 and CrossTask datasets with a dense video captioning task, but only with the author\u2019s proposed metrics."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The motivation is clear.\n- The result looks slightly better than the baselines in the manual initialization setting."
                },
                "weaknesses": {
                    "value": "1. The evaluation protocol is not well justified.\n\nThe authors proposed a new metric, but no traditional metrics for text generation, but only those for latent feature extractions. Since these tasks are traditionally evaluated on metrics for text generation, the authors are responsible for justifying the new metrics. A deeper experimental analysis is necessary to switch from a traditional evaluation protocol. \n\n2. Unclear presentation\n\nFirst of all, the authors never define mathematical notations in the text, but only in Figure 2. Since the definition heavily relies on the user\u2019s understanding of visual information, they are not rigorous and hard to understand correctly. For example, this reviewer does not understand the difference between S_m^*. S\u2019_m, and S_m^{which}. This unclarity was critical for this reviewer because it is impossible to guess the difference between $L^{when}$ and $L^{which}$; these are the core ideas of this method. The other figures also have too many texts (Fig. 1, 3, Table 1), which is not an ordinal paper format. Fig. 3 has too many unreadably tiny fonts, and sometimes they overlap. These texts may violate the ICLR format."
                },
                "questions": {
                    "value": "Please point out any factual errors in this review if the authors find them."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed.",
                        "Yes, Discrimination / bias / fairness concerns",
                        "Yes, Privacy, security and safety",
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)",
                        "Yes, Potentially harmful insights, methodologies and applications",
                        "Yes, Responsible research practice (e.g., human subjects, data release)",
                        "Yes, Research integrity issues (e.g., plagiarism, dual submission)",
                        "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)",
                        "Yes, Other reasons (please specify below)"
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3855/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698383241220,
            "cdate": 1698383241220,
            "tmdate": 1699636344076,
            "mdate": 1699636344076,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "sOE4hwZQch",
            "forum": "jQ0KLjlZjR",
            "replyto": "jQ0KLjlZjR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3855/Reviewer_1XSR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3855/Reviewer_1XSR"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the strong cross-modal and cross-context dependencies characteristic of cooking videos by making two significant adaptations to the original CLIP embedding: firstly, by extending static multi-modal CLIP embedding with a temporal dimension for context-aware semantic understanding, and secondly, by introducing zero-shot embedding to sequence-to-sequence dense prediction domains. This innovative approach enables CookingCLIP to not only recognize \"Which\" (cross-modal recognition) but also determine \"When\" (cross-context localization). The efficacy of CookingCLIP is demonstrated through experiments on challenging cooking caption generation benchmarks, YouCook and CrossTask, showcasing its effectiveness in the domain."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The proposed idea is straightforward and intuitive.\n2. The targeted problem is interesting."
                },
                "weaknesses": {
                    "value": "1. The contribution is limited.\n\na. The technical innovation is to add a location prediction branch in the decoding process based in automatically generated weak supervision. This idea itself of joint localization and understanding is not really new to this domain. The content decoding branch is also built upon existing work DeCap.\n\nb. The provided insight is also somewhat limited. The authors leave the truly interesting part: why does the learning process of location helpful for improving the caption generation as future research. However, the true insight behind these behavior is still missing. It is not clear whether the location is really the key here or the key is actually to **perform domain adaptation of the CLIP embedding on the cooking videos**. To really understand this, the authors should have provided more thorough comparison with other baselines like simply finetuning/or training adapters based on CLIP on the cooking videos without formulating the when loss but use other loss formulation like masked token modeling, regular contrastive learning and etc.\n\nc. The contribution is also over-claimed. It is not appropriate to claim that this work is \"zero-shot, open-vocabulary, and fully free from manual supervision\". The current limited domain itself has already set the limitation of the generalization. Moreover, the zero-shot in this paper is exactly defined based on a close vocabulary of cooking recipes, which is defined in each of the datasets.\n\n2. The presentation really needs further improvement.\n\na. Definition of terminology is confusing. In the reviewer's view, this is nothing about recipe generation. A recipe should be a general sequence of steps to making a meal but this model is really a video content translator and localizer.\n\nb. Almost all the figures and the tables contain way too much details. Some of them are difficult to read even when zoomed in on a 27'' display. The authors should really reduce details and make the key message clearer on all of the figures and tables by reducing details.\n\n3. Further baselines like SOTA models VidSeq for caption generation with Google Cloud automatic segmentation or VideoCLIP as the localizer should be provided to help the audience understand better about the performance."
                },
                "questions": {
                    "value": "Please check weakness for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3855/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812933479,
            "cdate": 1698812933479,
            "tmdate": 1699636343979,
            "mdate": 1699636343979,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "KfVQrPGjCc",
            "forum": "jQ0KLjlZjR",
            "replyto": "jQ0KLjlZjR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3855/Reviewer_5T4L"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3855/Reviewer_5T4L"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces CookingCLIP, which adapts the CLIP (Contrastive Language-Image Pre-training) embedding concept from the general domain to the specific domain of cooking understanding. The authors enhance the original image-based vision-language contrastive learning by incorporating a temporal dimension, presenting this as the technical novelty. Furthermore, they propose the zero-shot recipe generation task, evolving from the traditional dense video captioning benchmark."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper endeavors to introduce a CLIP-like multimodal embedding tailored for zero-shot cooking understanding, presenting an overarching concept that is fundamentally sound at a high level."
                },
                "weaknesses": {
                    "value": "The paper appears to be poorly articulated, leading me to suspect that it may be an incomplete submission."
                },
                "questions": {
                    "value": "1. What is $T_R$ in the beginning of Sec. 3? How about $W^V_k$, $W^S_k$, $V^{which}_k$, and $S^{which}_k$? These notions are not defined.\n\n2. Which part of Sec. 3 details the technical novelty in temporal modeling?\n\n3. What is \u201csemantic density\u201d described in Sec. 4.1?  \n\n4. Could you explain the process for constructing positive and negative pairs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3855/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817310589,
            "cdate": 1698817310589,
            "tmdate": 1699636343907,
            "mdate": 1699636343907,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]