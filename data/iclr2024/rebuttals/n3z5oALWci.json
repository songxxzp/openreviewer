[
    {
        "title": "ReX: A Framework for Incorporating Temporal Information in Model-Agnostic Local Explanation Techniques"
    },
    {
        "review": {
            "id": "O5BK1lyk1D",
            "forum": "n3z5oALWci",
            "replyto": "n3z5oALWci",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2333/Reviewer_9eyb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2333/Reviewer_9eyb"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a general framework for adapting various explanation techniques to models that process variable-length inputs, expanding explanation coverage to data points of different lengths to address limitation of existing model-agnostic general explanation techniques do not consider the variable lengths of input data points, which limits their effectiveness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1)\tThis paper proposed to incorporate temporal information in local explanations to machine learning models that can capture temporal information in the inputs, which makes these explanations more faithful and easier to understand.\n2)\tThis paper proposed a general framework REX to automatically incorporate the above information in popular local explanation techniques."
                },
                "weaknesses": {
                    "value": "1)\tThe experimental part is not very sufficient, and the comparison models used are not many and not the most advanced.\n2)\tAfter adding the REX framework, the efficiency of the model has decreased significantly, and I feel that it is not very practical."
                },
                "questions": {
                    "value": "1)\tAfter adding the REX framework, the efficiency of the model has decreased significantly. Does this have practical application value?\n2)\tThe reference mentioned in the related work seems to be several years ago. Is there any related work in recent years?\n3)\tWhich models correspond to the methods in Table 4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2333/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2333/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2333/Reviewer_9eyb"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698755677280,
            "cdate": 1698755677280,
            "tmdate": 1699636165767,
            "mdate": 1699636165767,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GbtgmBjWGW",
                "forum": "n3z5oALWci",
                "replyto": "O5BK1lyk1D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thanks a lot for your review. We address your concerns/questions below:\n\n**[W1]** \n\nThe experimental part is not very sufficient, and the comparison models used are not many and not the most advanced.\n\n**[AW1]**\n\nWe are conducting an experiment on *AcME\u2014Accelerated model-agnostic explanations: Fast whitening of the machine-learning black box* and will report the results later.\n\n**[Q1]**\n\nAfter adding the REX framework, the efficiency of the model has decreased significantly. Does this have practical application value?\n\n**[AQ1]**\n\nAs we discussed in section 4.2, For LIME and KSHAP, ReX is not the main time-consuming part and does not need any more sampling to get explanations with more fidelity. \n\nFor Anchors, the running time heavily depends if there are suitable predicates in the vocabulary. If ReX adds suitable predicates, the Anchors will run faster and otherwise slower.\n\nAnd as we reported in Table 3, we didn't encounter notable issues with efficiency in experiments.\n\n**[Q2]** \n\nThe reference mentioned in the related work seems to be several years ago. Is there any related work in recent years?\n\n**[AQ2]** \n\nTo our knowledge, LIME, SHAP, and Anchors are three basic local model-agnostic explanation techniques, and many recent methods are mainly derived from these three methods, such as *TimeSHAP: Explaining Recurrent Models through Sequence Perturbations*, *LIMESegment: LIMESegment: Meaningful, Realistic Time Series Explanations*, and so on. We included the related ones like *LIMESegment* in the related work section.\n\nAnd we just found a recently released method AcME, we are willing to add it to the related work section.\n\n**[Q3]**\n\nWhich models correspond to the methods in Table 4?\n\n**[AQ3]**\n\nAs we mentioned in Section 4.3, Table 4 is related to explaining the sentiment analysis LSTM with Anchors and Anchors. We are happy to also add such information in the caption of Table 4."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292184539,
                "cdate": 1700292184539,
                "tmdate": 1700292232657,
                "mdate": 1700292232657,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "L4RPe2thEu",
            "forum": "n3z5oALWci",
            "replyto": "n3z5oALWci",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2333/Reviewer_s4Qe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2333/Reviewer_s4Qe"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a framework, REX, designed to offer model-agnostic explanation techniques for variable-length input data. The framework utilizes both 1-D and 2-D predicates to address the relative positions of individual features within the input data. REX can be plugged into other explanations directly. Specifically, REX is shown to extend the capabilities of Anchors, LIME, and Kernel-SHAP. The efficacy of the proposed method is evaluated on two datasets using metrics including coverage/precision, accuracy, and AUROC. Notably, coverage/precision is also employed as a measure of human interpretability. The results affirm that REX not only enhances the fidelity of explanations but also improves human understanding without imposing additional computational overhead."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper addresses an important research question concerning the impact of temporal relationships between features in the input on the quality of explanations for model decisions. The proposed explanation method is versatile enough to handle varying input lengths, an advantage over traditional model-agnostic explanation techniques. The examples provided offer compelling motivation for the algorithm's design.\n\n2. The experimental evaluation of REX is thorough, covering two distinct tasks: sentiment analysis using multiple language models, and anomaly detection using RNNs. The results are analyzed through a variety of metrics, including both automatic measures of fidelity and human-centric metrics for understanding. The performance improvements achieved by REX are promising."
                },
                "weaknesses": {
                    "value": "1. Algorithm for \"Extending Vocabularies\": The paper would benefit from a more detailed explanation of how 1-D and 2-D predicates are extracted from the model inputs, as this is a core component of REX. Additional analyses on the algorithm for extending vocabularies would be beneficial. Specifically, do all 1-D and 2-D predicates positively impact explanations? Including more examples beyond those discussed in Section 3.1 could strengthen the paper's argument for the advantages of REX. Additionally, incorporating another textual dataset could bolster claims regarding REX's generalizability.\n\n2. Details on Human Evaluations: The section on human evaluations lacks some details. For instance, is the experimental design a within-groups setup? If so, biases could arise from the order in which explanations from different methods are presented. What is the precise procedure for the user study? Does each set consist of ten new sentences used specifically for that test block? Furthermore, exploring challenging tasks, such as instances where the model makes incorrect predictions, would offer additional insights into human understanding of model reasoning. If humans can also make \u201cwrong predictions\u201d made by the model, it is very convincing that users understand the model's reasoning.\n\n3. Broader Impact of REX: Could REX be generalized to image inputs, particularly simple image sequences with important temporal information for classification? A discussion on the generalizability of REX\u2019s two core modules\u2014Extending Vocabularies and Perturbation Models\u2014could enhance the paper."
                },
                "questions": {
                    "value": "1. Analysis of Table 4: The interpretation of Table 4 in Section 4.3 lacks precision. Specifically, the claimed improvements in precision and coverage, cited as \"80.9% and 56.7%\" in the text, cannot be observed from the results in Table 4.\n\n2. Question on Input Length: What is the average length of the textual input data? If the text input data consists of long paragraphs, would that impose computational burdens on REX?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2333/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2333/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2333/Reviewer_s4Qe"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796999093,
            "cdate": 1698796999093,
            "tmdate": 1699636165661,
            "mdate": 1699636165661,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S3xLfhgUz5",
                "forum": "n3z5oALWci",
                "replyto": "L4RPe2thEu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thanks a lot for your review. We address your concerns/questions below:\n\n**[W1]**\n\nAlgorithm for \"Extending Vocabularies\": Additional analyses on the algorithm for extending vocabularies would be beneficial. Specifically, do all 1-D and 2-D predicates positively impact explanations? Including more examples beyond those discussed in Section 3.1 could strengthen the paper's argument for the advantages of REX. Additionally, incorporating another textual dataset could bolster claims regarding REX's generalizability.\n\n**[AW1]**\n\n1. Do all 1-D and 2-D predicates positively impact explanations?\n\nWe agree that there could be some redundant 1-D and 2-D predicates. \n\nBut the essential point is that there exists some effective 1-D and 2-D predicates helping to construct explanations with higher fidelity. \nFor rule-based methods like Anchors, if a predicate is not contributing to the explanation, it will simply be excluded. Moreover, for an instance that does not need 1-D and 2-D predicates, ReX generates explanation with the original feature predicates.\nFor attribution-based like LIME and KSHAP, \n\nThis ensures that **ReX does not make an explanation worse**.\n\nWe appreciate your suggestion and will expand on this discussion.\n\n2. \n\nWe are conducting experiments on the Llama-2 model and the financial phrasebank datasets. We will report the results later.\n\n**[W2]**\n\nDetails on Human Evaluations: The section on human evaluations lacks some details. For instance, is the experimental design a within-groups setup? If so, biases could arise from the order in which explanations from different methods are presented. What is the precise procedure for the user study? Does each set consist of ten new sentences used specifically for that test block? Furthermore, exploring challenging tasks, such as instances where the model makes incorrect predictions, would offer additional insights into human understanding of model reasoning. If humans can also make \u201cwrong predictions\u201d made by the model, it is very convincing that users understand the model's reasoning.\n\n**[AW2]** \n\nWe are happy to provide more details.\n\n**The questionnaires are similar for all users with minor variations in the order of presentation.**. We presented the five questions, Q1, Q2,..., and Q5 in a random order. For each question, we presented the explanations generated without/with ReX in a random order. Also, we presented the ten sentences to be predicted in a random order.\n\nAmong all the 50 sentences to be predicted, on 19 sentences,  the prediction by LSTM model differs from groundtruth(the sentiment classified by humans). On these sentences ReX can still help people to predict better. Users make the same prediction as the LSTM model on 48.9% of these instances with vallina explanation methods, while ReX improves this to 66.7%.\nIn other words, ReX indeed helps humans make \u201cwrong predictions\u201d made by the model."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291321018,
                "cdate": 1700291321018,
                "tmdate": 1700732612108,
                "mdate": 1700732612108,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tUSgpaxvWY",
                "forum": "n3z5oALWci",
                "replyto": "NvXTR4XYzp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2333/Reviewer_s4Qe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2333/Reviewer_s4Qe"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' response to my comments. However, there remain a few issues that need further attention. Primarily, my concern regarding the \"Extending Vocabularies\" algorithm is not addressed and still stands, and additional examples are missing as well (W1). Furthermore, the current response lacks experimental results from an additional dataset, which would significantly enhance the robustness of the study.\n\nIt appears that the authors could utilize more time to refine their work and make it technically sound and solid. Therefore, I would maintain my initial evaluation score"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690270293,
                "cdate": 1700690270293,
                "tmdate": 1700690270293,
                "mdate": 1700690270293,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mI8Oo963o2",
                "forum": "n3z5oALWci",
                "replyto": "L4RPe2thEu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Experimental results"
                    },
                    "comment": {
                        "value": "Thanks for your response. \n\nWe have finished the experiments with the following setup:\n1) explain the Llama-2-7b model with LIME and ReX+LIME on the SST dataset, and\n| Methods            | LIME | ReX+LIME|\n| --------         | -------- | -------- |\n| Accuracy(%)        | 63.5     |  71.8    |\n| AUROC             | 51.7      | 69.0    |\n| Time(s)      | 823.2      |  828.3    |\n\n2) explain the Bert model with LIME and ReX+LIME on the financial phrasebank dataset.\n| Methods            | LIME | ReX+LIME|\n| --------         | -------- | -------- |\n| Accuracy(%)        | 48.8     |  62.4    |\n| AUROC             | 0.500      | 0.633    |\n| Time(s)      | 486.1 |  480.3    |\n\nOn both setups, ReX improves the fidelity of the based methods."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734646101,
                "cdate": 1700734646101,
                "tmdate": 1700735008975,
                "mdate": 1700735008975,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9NfJROIztb",
            "forum": "n3z5oALWci",
            "replyto": "n3z5oALWci",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2333/Reviewer_zbfs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2333/Reviewer_zbfs"
            ],
            "content": {
                "summary": {
                    "value": "This work extends existing explanation methods (Anchors, lime, Kernel Shap) to augment temporal information in the explanations. Temporal information is the explanations is defined with upto 2 features by highlighting the position/distance between features (e.g. position of feature K - position of feature L >= 3). The explanations are evaluated on text and time series data (which is the target domain of such explanations) which show improvements in metrics as well as a user study involving 19 individuals. In the context of explainability, temporal information has earlier been used in timeseries data related to shaplets, albeit in a different manner."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, the idea of incorporating temporal information using position/distance between features is simple and novel. Temporal information is useful in general and it does help in minimizing the ambiguity in existing explanations as shown by authors for text and time series examples. The fact that existing explanation methods can be extended simply by using an alternate perturbation approach as claimed by authors, is useful."
                },
                "weaknesses": {
                    "value": "- Empirical evaluation can include more models and datasets. \n- The user study could be extended in size and diversity of users. Although positional/distance between features is useful, in case of timeseries the shape of a curve (e.g. shaplets) provides richer information and more useful information to SMEs. It might be good to present use cases where SMEs value the distance information in specific domains."
                },
                "questions": {
                    "value": "- In case of LIME-Text explainer (equivalently for SHAP), words of a sentence are randomly deleted to obtain different binary vectors (1= word present, 0=absent) in order to compute distance between original & perturbed samples and fit a linear model. If the words are shifted, then the binary vectors remain unchanged. It was unclear from text how temporal information is recovered in this setting using the existing ridge regression model used by LIME?\n\n- Compared to the base case of no temporal information, how do (a) number of perturbed samples and (b) number of parameters in the linear model fit by lime/shap, scale when temporal information is requested from the explainer?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830167411,
            "cdate": 1698830167411,
            "tmdate": 1699636165572,
            "mdate": 1699636165572,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i3NLgA09FA",
                "forum": "n3z5oALWci",
                "replyto": "9NfJROIztb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thanks a lot for your review. We address your concerns/questions below:\n\n**[W1]**\n\nEmpirical evaluation can include more models and datasets.results later.\n\n**[AW1]**\n\nWe are conducting experiments on the Llama-2 model and the financial phrasebank datasets. We will report the results later.\n\n**[Q1]**\n\nIn case of LIME-Text explainer (equivalently for SHAP), words of a sentence are randomly deleted to obtain different binary vectors (1= word present, 0=absent) in order to compute distance between original & perturbed samples and fit a linear model. If the words are shifted, then the binary vectors remain unchanged. It was unclear from text how temporal information is recovered in this setting using the existing ridge regression model used by LIME?\n\n**[AQ1]**\n\nThe temporal information is recovered using the temporal predicates. \nAfter LIME is augmented by ReX, the 1-D and 2-D predicates represent the effect of absolute positions and relative positions respectively. And for the corresponding binary vectors, 1 means the predicates evaluate to true, while 0 means false. If a word is shifted, its absolute position and relative position to other words are also changed. Therefore, **the binary vectors corresponding to the predicates about the word's position are changed**, so the temporal information can be recovered using the existing regression model.\n\n**[Q2]**\n\nCompared to the base case of no temporal information, how do (a) number of perturbed samples and (b) number of parameters in the linear model fit by lime/shap, scale when temporal information is requested from the explainer?\n\n**[AQ2]**\n\n(a) We achieve higher fidelity by using **the same** number of perturbed samples.\n\n(b) The number of parameters equals the number of predicates in the vocabulary. For sentiment analysis, we simply use all the feature, 1-D, and 2-D predicates. For anomaly detection, we limit the temporal predicates in a window of 20 steps."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291226442,
                "cdate": 1700291226442,
                "tmdate": 1700291226442,
                "mdate": 1700291226442,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bZ86PBf9fr",
                "forum": "n3z5oALWci",
                "replyto": "i3NLgA09FA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2333/Reviewer_zbfs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2333/Reviewer_zbfs"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for addressing my questions, I will retain my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669065536,
                "cdate": 1700669065536,
                "tmdate": 1700669065536,
                "mdate": 1700669065536,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3ULi8Z7jO0",
                "forum": "n3z5oALWci",
                "replyto": "9NfJROIztb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Expeirment results"
                    },
                    "comment": {
                        "value": "Thanks for your response. \n\nWe have finished the experiments with the following setup:\n1) explain the Llama-2-7b model with LIME and ReX+LIME on the SST dataset, and\n| Methods            | LIME | ReX+LIME|\n| --------         | -------- | -------- |\n| Accuracy(%)        | 63.5     |  71.8    |\n| AUROC             | 51.7      | 69.0    |\n| Time(s)      | 823.2      |  828.3    |\n\n2) explain the Bert model with LIME and ReX+LIME on the financial phrasebank dataset.\n| Methods            | LIME | ReX+LIME|\n| --------         | -------- | -------- |\n| Accuracy(%)        | 48.8     |  62.4    |\n| AUROC             | 0.500      | 0.633    |\n| Time(s)      | 486.1 |  480.3    |\n\nOn both setups, ReX improves the fidelity of the based methods."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734437552,
                "cdate": 1700734437552,
                "tmdate": 1700734995735,
                "mdate": 1700734995735,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Hdd817lqyR",
            "forum": "n3z5oALWci",
            "replyto": "n3z5oALWci",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2333/Reviewer_MXvE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2333/Reviewer_MXvE"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose REX, a framework that incorporates \"temporal information\" in local post-hoc explanations of DNNs that can take varying-length sequence data as input. \n\nSpecifically, REX provides explanations over a vocabulary of \"feature predicates\" that specify temporal relationships between features, e.g. \"the token at index i is 'never', the token at index j is 'fails', and 'never' occurs immediately before 'fails' (j - i = 1)\".  The authors illustrate via examples to calculate existing post-hoc explanation techniques (such as LIME and Anchors) over this new vocabulary of predicates.  The authors demonstrate the value of their new approach by arguing that it results in improvements in fidelity measures and users' performance on a forward simulation task, compared to \"naive\" application of explanation techniques like LIME and Anchors."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors' predicate definitions (Def. 3.1 and 3.2) are intuitive to interpret (e.g., the 2D predicates can be used to specify the number of tokens between two particular tokens in a sentence).  The authors illustrate the potential utility of attributing importance to such predicates rather than individual features in Figure 1.\n2. The authors' experimental results (Table 2 and Figure 4) demonstrate the potential value of REX. The explanations provided by REX have higher coverage, precision, and are more accurate surrogate models compared to the naive explanations provided over the original feature set."
                },
                "weaknesses": {
                    "value": "* **Weakness #1: Clarity.** My primary critique of this work is that I found the present draft difficult to understand.  The notation used was not sufficiently explained, and I found the authors' descriptions of their methodology and experiments to be severely underspecified.  Unless these details are clarified, I do not believe this draft is ready to be published in its present state. Specifically:\n  * Section 2 (Notation). The description that you provided in the second paragraph is confusing, and does not clarify exactly how to interpret the notation.  Specifically, what is $d$: is it the order in which the token appears in the sentence? What is the minus sign notation (what is $Pos_g$, and how does it differ from $Pos_f$)? What do the numbers mean (e.g., why is there a 2 in the statement $Pos_{fails} - Pos_{never} >= 2$?)\n  * Section 3. Your notation is under-specified. It may be helpful in this section to clarify what the 'features' of the example inputs you presented are (e.g., what are the 'features' of the sentence input \"Bob is not a bad boy\")? Is $f_j$ in this case the token that appears at index $j$ in the sentence, or something else? Similarly, do your feature predicates $p_j$ compare the value of token at index $j$ to some threshold (e.g. \"the token at timestep $t$ = 'fails'\", or something else? (EDIT: After reading Section 3.1, it seems like $f_j$ is the value of the token at index $j$?  Can you clarify?)\n  * Section 3.2. \"Extending vocabularies: I don't understand what is described in this paragraph. What do you mean by 'serve as a feature of an input', and a 'method to evaluate the predicate on a given input'? Do you mean that you construct a new set of indicators for each original datapoint where you evaluate whether the predicate holds for that datapoint, and then learn each \"surrogate model\" using the set of original features plus the predicates?\n  * Section 3.3. I am confused from the explanation provided about how the perturbation model that you've described is used for each of the individual explanation methods.  It is unclear to me how this perturbation method, evaluates the possible importance of all of the possible predicates (from the many possibilities that exist).  Take LIME as an example. I am confused about how being able to generate new datapoints where features are \"switched\" (for example, say we switch $f_j = c_j$ and $f_i = c_i$, allows us to assign an \"importance score\" to, for example, the predicate $f_j = c$ AND $f_i = c_i$ AND $i - j >= 1$.  Can you clarify exactly how you calculate the importance scores for each of the 1D and the 2D temporal predicates using such perturbation methods?\n  * Section 4.3. Can you clarify how you assigned participant to explanation conditions in the user study? Did a single participant only see explanations from 1 of {Anchor, Anchor*}?  Did you present a single \"test\", and then ask them to simulate the model on 10 sentences, several different times (so elicited 50 total predictions), or show all 5 \"tests\" right away and then collect 10 predictions per user?\n  * Section 4.3. Can you provide screenshots of your study interface (e.g., how you presented the Anchors explanations to users)? I wonder if simply highlighting the most important tokens identified by Anchors within the existing sentence, like in Figure 4 of [1] (rather than just showing them the rule) may result in similar performance increases as showing them the Anchors* explanation.\n* **Weakness #2: No comparison to popular existing post-hoc methods for sequential data.** In your Related Work and in the paper's Introduction, you compare post-hoc feature attribution methods like LIME to DFA methods that explain RNNs.  However, there have been many other post-hoc explanation method techniques that have been proposed to explain sequence models like transformers (most notably, attention [2]). Can you provide additional motivation for the benefits of your proposed method over other existing popular methods?\n\n[1] https://arxiv.org/pdf/2302.08450.pdf\n\n[2] https://arxiv.org/abs/1908.04626"
                },
                "questions": {
                    "value": "See the listed Weaknesses for my high-priority questions.\n\nSome additional comments/suggestions I had that are less relevant to my score are:\n* Introduction: \"the explanations can still be complex as RNNs often fail to internalize 'the perfect regular expressions' and contain noise\". Why does failure to learn the true regular expression imply that the network is difficult to explain?\n* Figure 2: I am confused about this example.  My understanding is that the anomaly data point is 428, and both Anchors and Anchors* identify only datapoints before this anomaly point.  Wouldn't a more suitable explanation include both datapoints that come before, and after point 428?  Is the problem set-up here that you must predict if timestep $x_t$ is an anomaly, given only the observations that came before it?\n* Section 2, nit: isn't it more appropriate for the local explanation to be a function of $f$, i.e., $g(x, f)$?\n* Section 3. \"Anchors is a conjunction which must evaluate to true on $x$\".  Can you define what a \"conjunction\" is inline?  What do you mean by \"evaluate to true on $x$\"?\n* Section 3.1. Can you provide intuition for the explanation with the \"I hate that man\u2026\" example?  Why does it make sense for \"j >= 3\" to be the explanation? This seems less important than \"but\" coming before \"love\", or after \"hate\"?\n* Section 4.1. Can you provide more detail about the anomaly detection ECG dataset? Is there only a single \"feature\" being measured at different timesteps? What is the prediction task here (is it given the measurements $x_1, \u2026, x_{(T - 1)}$, to predict if $x_T$ is anomalous)?\n* Section 4.2. Your paper says that \"REX improves the coverage by 98.2%\".  Do you mean that $1.982 x = y$, where $x$ is the average coverage across all of the models and explanation methods before REX, and $y$ is the average coverage across all of the models and explanation methods after REX?\n* Section 4.2. I don't understand the intuition for REX's runtime. You state, \"REX does not require a larger number of sampled instances\".  Why is this the case?\n* For your \"it's not a bad journey at all\" example in 4.2, I think your explanation is incorrect (the token \"good\" doesn't appear in the original sentence).\n* Section 4.3. You state that users \"would ignore input length constraints\" when \"misusing [the original] Anchors' explanations\".  But unless I am mistaken, the original Anchors did not include predicates over the input's length? Can you clarify what you mean by this?'"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2333/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2333/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2333/Reviewer_MXvE"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699076872747,
            "cdate": 1699076872747,
            "tmdate": 1699636165501,
            "mdate": 1699636165501,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZQUGf4bZMF",
                "forum": "n3z5oALWci",
                "replyto": "Hdd817lqyR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors (I)"
                    },
                    "comment": {
                        "value": "Thanks a lot for your review. We address your concerns/questions below:\n\n### [Weakness 1& Questions] \n\n### a. Section 2 (Notation):  The description that you provided in the second paragraph is confusing, and does not clarify exactly how to interpret the notation. Specifically, what is $d$: is it the order in which the token appears in the sentence? What is the minus sign notation (what is $Pos_g$, and how does it differ from $Pos_f$)? What do the numbers mean (e.g., why is there a 2 in the statement $Pos_{fails}-Pos_{never}>=2$?)\n\n**Answer:**\n\nSorry for the confusing notation. We would appreciate it if you could point out the specific notations in **the second paragraph of Section 2**. Note the points you raised (e.g., ones about $Pos$) appear in Section 1.\n\nRegarding the notation $Pos_{fails}-Pos_{never} \\ge 2$, we have explained this in **the third to the last paragraph of Page 2**. *$Pos_f$ is an integer and means the position of feature $f$*. \n\nSpecifically, $Pos_f$ and $Pos_g$ are both integers, representing the position of feature $f$ and $g$ respectively. The minus sign between $Pos_f$ and $Pos_g$ denotes integer subtraction between these two integers. $d$ is an integer. Therefore, $Pos_f-Pos_g \\ge d$ implies that the difference between the two numbers is greater than or equal to $d$.\n\nTo provide a concrete example, let's consider the sentiment analysis example presented in Table 1. In this case, the features are words from the input. $Pos_{fails}$ represents the position of the word \"fails\" in the input, while $Pos_{never}$ represents the position of the word \"never\". Both $Pos_{fails}$ and $Pos_{never}$ are integers. Thus, $Pos_{fails}-Pos_{never}\\ge 2$ means that the result of subtracting $Pos_{never}$ from $Pos_{fails}$ is greater than 2.\n\n### b.Section 3. Your notation is under-specified. It may be helpful in this section to clarify what the 'features' of the example inputs you presented are (e.g., what are the 'features' of the sentence input \"Bob is not a bad boy\")? Is $f_j$ in this case the token that appears at index $j$ in the sentence, or something else? Similarly, do your feature predicates $p_j$ compare the value of token at index $j$ to some threshold (e.g. \"the token at timestep $t$ = 'fails'\", or something else? (EDIT: After reading Section 3.1, it seems like $f_j$ is the value of the token at index $j$? Can you clarify?)\n\n**Answer:**\n\n1. what the 'features' of the example inputs you presented are?\n\nThere is a general definition of features that defines them as **attributes of input data.** For example, in image data, features can be pixels, and in sentence data, features can be words. Since this definition of features is widely used and given the limited space, we did not provide a detailed explanation of what features are.\n\nFor the example in section 3.1, the \"features\" refer to the tokens (words) of the example inputs.\n\n\n2. Is $f_j$ in this case the token that appears at index $j$ in the sentence?\n\nYes. We have defined $f_j$ in **the last paragraph of page 3**, where we stated that *$f_j$ represents the $j$th feature of input $x$*. In the example, $f_j$ represents the j-th token of the example inputs.\n\n\n3. do your feature predicates $p_j$ compare the value of token at index $j$ to some threshold.\n\nYes. We have also defined feature predicates $p_j$ in **the last paragraph** of page 3, where we stated that *$p_j := f_j\\ op\\ c$*. Here, $p_j$ compares the value of the token at index $j$ to a threshold value denoted as $c$.\n\n\nWe are happy to incorporate additional explanations to provide further clarity in the paper.\n\n\n### c.Section 3.2. \"Extending vocabularies: I don't understand what is described in this paragraph. What do you mean by 'serve as a feature of an input', and a 'method to evaluate the predicate on a given input'? Do you mean that you construct a new set of indicators for each original datapoint where you evaluate whether the predicate holds for that datapoint, and then learn each \"surrogate model\" using the set of original features plus the predicates?\n\n**Answer:**\n\nWe define the *vocabulary* in the first paragraph of page 4. The vocabularies are a set of predicates used to construct the explanations.\n\nThe original vocabularies only contain **feature predicates**. ReX **extends** the vocabularies by incorporating additional predicates such as the 1-D and 2-D temporal predicates mentioned in **Section 3.1**, so that can incorporate temporal information in the explanation(**Definition 3.3**). \n\nSince the vallina explanation methods use only the original features as predicates, we said we should make new predicates also \"serve as a feature\".\n\nWhen utilizing the predicates in the explanation techniques, existing explanation methods need to know if the predicates are true or false, so we need to \"evaluate the predicate on a given input\"."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639592208,
                "cdate": 1700639592208,
                "tmdate": 1700639592208,
                "mdate": 1700639592208,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sX3M2AuoAJ",
                "forum": "n3z5oALWci",
                "replyto": "Hdd817lqyR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors (II)"
                    },
                    "comment": {
                        "value": "### d.Section 3.3. I am confused from the explanation provided about how the perturbation model that you've described is used for each of the individual explanation methods. It is unclear to me how this perturbation method, evaluates the possible importance of all of the possible predicates (from the many possibilities that exist). Take LIME as an example. I am confused about how being able to generate new datapoints where features are \"switched\" (for example, say we switch $f_j = c_j$ and $f_i = c_i$, allows us to assign an \"importance score\" to, for example, the predicate $f_j = c$ AND $f_i = c_i$ AND $i - j >= 1$. Can you clarify exactly how you calculate the importance scores for each of the 1D and the 2D temporal predicates using such perturbation methods?\n\n**Answer:**\n\nLet's take ReX+LIME(LIME*) and sentiment analysis as an example to clarify how the perturbation methods calculate the importance scores for each predicates.\n\nFor an input sequence $x = w_1w_2w_3$, where $w_i$ represents the $i$-th word in the sequence, we can perform a perturbation by switching the 2nd and 3rd words, resulting in $x' = w_1w_3w_2$.\n\nThe feature predicates $f_2 = w_2$ and $f_3=w_3$ will evaluate to **false** on $x'$ since the 2nd feature of $x'$  is $w_3$ while the 3rd one is $w_2$. \n\nFor 2-D predicates like \n\\begin{array}{r@{}l}\n    &\\exists i, j \\in \\mathbb{Z}^{+}\\ such\\ that\\ f_i\\ =w_2  \\wedge f_j\\ =\\ w_3 \\wedge j - i\\ \\ge \\ 1 \\\\\n\\end{array}\nwill evaluate to false. since there exists $f_3 = w_2$ and $f_2 = w_3$ but $2-3<1$.\n\nLIME samples multiple datapoints using the perturbation model. For each sampled datapoint, LIME* calculates the results of all predicates in the explanation vocabularies, and stores the results as a binary vector, along with the corresponding model output for that datapoint.\n\nThis process generates a matrix, as shown below:\n\n\n|     | Predicate 1 | Predicate 2 | ... | Predicate m | Model Output |\n| --- | ----------- | ----------- | --- | ----------- | ------------ |\n| data1 | 1 | 1 | ... | 0 | 1 |\n| data2 | 1 | 1 | ... | 1 | 0 |\n| ... | ... | ... | ... | ... | ... |\n| datak | 1 | 0 | ... | 1 | 1 |\n\nLIME uses a regression algorithm to assign importance scores to each predicate based on this matrix.\n\n### e. Section 4.3. Can you clarify how you assigned participant to explanation conditions in the user study? Did a single participant only see explanations from 1 of {Anchor, Anchor*}? Did you present a single \"test\", and then ask them to simulate the model on 10 sentences, several different times (so elicited 50 total predictions), or show all 5 \"tests\" right away and then collect 10 predictions per user?\n\n**Answer**\n\nWe are happy to provide more details about the user study.\n\n1. Can you clarify how you assigned participant to explanation conditions in the user study?\n\nWe showed all users the five questions and 2 explanations. The questionnaires are similar for all users with minor variations in the order of presentation. We presented the five questions, Q1, Q2,..., and Q5 in a random order. For each question, we presented the explanations generated without/with ReX in a random order. Also, we presented the ten sentences to be predicted in a random order.\n\n2. Did a single participant only see explanations from 1 of {Anchor, Anchor*}?\n\nNo, each participant saw both explanations.\n\n\n3. Did you present a single \"test\", and then ask them to simulate the model on 10 sentences, several different times (so elicited 50 total predictions), or show all 5 \"tests\" right away and then collect 10 predictions per user?\n\nWe presented one explanation at a time and asked participants to simulate the model on 10 sentences for each explanation. Since there were 10 explanations in total, we repeated this process 10 times."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639680881,
                "cdate": 1700639680881,
                "tmdate": 1700639680881,
                "mdate": 1700639680881,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wO8q0zbc2s",
                "forum": "n3z5oALWci",
                "replyto": "Hdd817lqyR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors (III)"
                    },
                    "comment": {
                        "value": "### f. Section 4.3. Can you provide screenshots of your study interface (e.g., how you presented the Anchors explanations to users)? I wonder if simply highlighting the most important tokens identified by Anchors within the existing sentence, like in Figure 4 of [1] (rather than just showing them the rule) may result in similar performance increases as showing them the Anchors* explanation.\n\n**Answer**\nSorry that it is not possible to directly upload images on OpenReview.  However, I can provide a textual representation of how we present the Anchors explanations to the users. The format is as follows:\n\n* **Original sentence:**  pretentious editing ruins a potentially terrific flick. \n* **RNN output:** negative. \n* **Explanation 1:** the word \"ruins\" appears in the sentence at the specific position. \n\n\nPlease predict the RNN output on each sentence below according to each explanation. You can answer 0. negative, 1. positive, or 2. I don't know.\n\n\n\n| Sentence | Prediction 1 | \n| -------- | -------- | \n| ruins beneath terrific design.     |      |  \n|  pretentious editing ruins a potentially terrific methodology.     |          |\n| ...|\n| cult ruins a potentially lucrative planet.     |      |  \n\n* **Explanation 2**: both \"ruins\" and \"terrific\"  appear in the sentence, \"terrific\" is behind \"ruins\", and there are at least \"0\" words between them.\n\n| Sentence |  Prediction 2|\n| -------- | -------- | \n| ruins beneath terrific design.     |      |\n|  pretentious editing ruins a potentially terrific methodology.     |  \n| ...|\n| cult ruins a potentially lucrative planet.     |      |   \n\nRegarding Figure 4 of [1], if altering the presentation of the explanation leads to performance improvements, it is likely to benefit both Anchors and  ReX+Anchors.\n\n### g.Introduction: \"the explanations can still be complex as RNNs often fail to internalize 'the perfect regular expressions' and contain noise\". Why does failure to learn the true regular expression imply that the network is difficult to explain?\n\n**Answer**\n\nIn this part of the paper, we discuss using DFAs as global surrogates to explain RNNs. \n\nWhen using RNNs to learn simple regular expressions, they are often unable to represent regular expressions perfectly and may contain noise, so the decision procedure of these RNNs will be complex. These RNNs are hard to explain by DFAs. As the referred paper *Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples* concluded, for networks with complicated behavior, extraction becomes extremely slow and returns large DFAs. Large DFAs are also difficult to understand by end users.\n\n### h.Figure 2: I am confused about this example. My understanding is that the anomaly data point is 428, and both Anchors and Anchors* identify only datapoints before this anomaly point. Wouldn't a more suitable explanation include both datapoints that come before, and after point 428? Is the problem set-up here that you must predict if timestep $x_t$ is an anomaly, given only the observations that came before it?\n\n**Answer**\n\nDue to the workflow of RNNs(https://en.wikipedia.org/wiki/Recurrent_neural_network#/media/File:Recurrent_neural_network_unfold.svg), in this task, we give the anomaly detection RNN inputs data in the time order. **After giving the value of $x_1,x_2,...,x_t$, the RNN will predict if $x_t$ is an anomaly immediately**.\n\n### i.Section 2, nit: isn't it more appropriate for the local explanation to be a function of $f$, i.e., $g(x,f)$?\n\n**Answer**\n\nThanks for your suggestion, we will make local explanation a function in our revised version.\n\n### j.Section 3: Can you define what a \"conjunction\" is inline? What do you mean by \"evaluate to true on $x$\"?\n\n**Answer**\n\nIn Section 3, we use the common definition of conjunction as provided in the link to Wikipedia (https://en.wikipedia.org/wiki/Logical_conjunction). This definition is widely used and accepted.\n\nRegarding the phrase \"evaluate to true on $x$,\" it simply means that $x$ satisfies the condition described by the predicate $p_j$. For a concrete example, please refer to the example provided in the **Answer of d.**.\n\n### k.Section 3.1: Can you provide intuition for the explanation with the \"I hate that man\u2026\" example? Why does it make sense for \"j >= 3\" to be the explanation? This seems less important than \"but\" coming before \"love\", or after \"hate\"?\n\n**Answer**\n\nJust like your example, the two predicates\n\n1. \"but\" is before \"love\", and\n2. \"but\" is after \"hate\"\n\ncan also construct an explanation.\n\n\nHowever, Anchors tends to find the explanation that covers the most data points with precision >= given threshold. In this example, both this explanation and the one in Section 3.1 can reach the precision threshold, but the one in Section 3.1 covers more data points, so Anchors chooses it as an explanation."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639790890,
                "cdate": 1700639790890,
                "tmdate": 1700639790890,
                "mdate": 1700639790890,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BPkpPvzcti",
                "forum": "n3z5oALWci",
                "replyto": "Hdd817lqyR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors (IV)"
                    },
                    "comment": {
                        "value": "### l.Section 4.1: Can you provide more detail about the anomaly detection ECG dataset? Is there only a single \"feature\" being measured at different timesteps? What is the prediction task here (is it given the measurements $x_1, \u2026, x_{(T - 1)}$, to predict if $x_T$ is anomalous)?\n\n**Answer**\n\nFor the anomaly detection ECG dataset, it consists of measurements of a single feature (ECG signal) taken at different time steps. \n\nYou can refer to the **Answer of h** and the GitHub repo we referred to in the paper(https://github.com/chickenbestlover/RNN-Time-series-Anomaly-Detection).\n\n### m.Section 4.2: Your paper says that \"REX improves the coverage by 98.2%\". Do you mean that $1.982x = y$, where $x$ is the average coverage across all of the models and explanation methods before REX, and $y$ is the average coverage across all of the models and explanation methods after REX?\n\n**Answer**\n\nYes.\n\n### n.Section 4.2:  For your \"it's not a bad journey at all\" example in 4.2, I think your explanation is incorrect (the token \"good\" doesn't appear in the original sentence).\n\n**Answer**\n\nThanks for your notification. \n\nThat's a typo. It should be $\\{not, bad\\}\\wedge pos_{bad}-pos_{not}\\ge 1$.\n\n### o.Section 4.3\uff1a  You state that users \"would ignore input length constraints\" when \"misusing [the original] Anchors' explanations\". But unless I am mistaken, the original Anchors did not include predicates over the input's length? Can you clarify what you mean by this?'\n\n**Answer**\n\nAs a black-box method, Anchors generates explanations based on the model's predictions on the data points sampled by the perturbation model.\n \n\nIn Section 2, we explained that existing local techniques implement the perturbation models by changing feature values, represented as $per(x) \\subseteq \\mathbb{R}^{|x|}$. Anchors generates explanations by considering only the perturbation space. Consequently, the explanations can only explain the behavior of the model within this space. Specifically, the explanations focus on data points of the same length and variations in the values of some features.\n\nWhen we mentioned that users \"would ignore input length constraints\", we meant that they might mistakenly apply the explanations beyond the intended scope. We also addressed these input length constraints in the explanations we provided to our users.\n\n### [W2] No comparison to popular existing post-hoc methods for sequential data. In your Related Work and in the paper's Introduction, you compare post-hoc feature attribution methods like LIME to DFA methods that explain RNNs. However, there have been many other post-hoc explanation method techniques that have been proposed to explain sequence models like transformers (most notably, attention [2]). Can you provide additional motivation for the benefits of your proposed method over other existing popular methods?\n\n**Answer**\n\nThere is a minor misconception that attention-based methods are actually ante-hoc.\n\nThe benefits of ReX with existing local model-agnostic methods are that ReX can explain any machine learning model without knowing its internal information, while white-box methods like attention-based methods can only explain a specific kind of models.\n\nBut we are happy to compare ReX with these methods. We are conducting an experiment over an attention-based method *Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience*.\n\nThanks for your reading."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640304078,
                "cdate": 1700640304078,
                "tmdate": 1700640304078,
                "mdate": 1700640304078,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0rkv50zGC0",
                "forum": "n3z5oALWci",
                "replyto": "Hdd817lqyR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2333/Reviewer_MXvE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2333/Reviewer_MXvE"
                ],
                "content": {
                    "title": {
                        "value": "Response acknowledged"
                    },
                    "comment": {
                        "value": "Thanks to the authors for responding to all of my listed points.  Their responses clarified several questions I had on details that I found to be unclear.  I especially found the authors' written descriptions of their $Pos_f$ notation, and example of how \"ReX+LIME(LIME*)\" is calculated to be particularly clarifying.  I also found the descriptions of the user study (and example text that was shown to users) to be illustrative. I'd encourage the authors to add in some of these explanations in a future draft of the paper. I also would have appreciated if the reviewers had updated the draft (rather than retrospectively writing clarifications) before the deadline to do so had passed.\n\nAfter reviewing the larger discussion (and with my new understanding of how the perturbation model is used to assign importance scores to the predicates), I still have a few remaining questions.  One question which is relevant to reviewer s4Qe's comment requesting clarity on whether 2D predicates are \"redundant\" with 1D predicates, and how the vocabulary is extended in practice. I also am curious about the extent to which different predicates are highly correlated with each other, which violates LIME's assumption that all of the input features are (somewhat) independent. Does this pose problems when assigning importance scores to highly-correlated predicates? For example, the predicate \"token i is in front of token j\" is always true when the predicate \"token i is at least 2 positions in front of token j\" is true.\n\nFor the above reasons, I do not recommend the present draft for acceptance and retain my original score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703669980,
                "cdate": 1700703669980,
                "tmdate": 1700703705360,
                "mdate": 1700703705360,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]