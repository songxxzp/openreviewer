[
    {
        "title": "Scalable Monotonic Neural Networks"
    },
    {
        "review": {
            "id": "aW8GfAmW8L",
            "forum": "DjIsNDEOYX",
            "replyto": "DjIsNDEOYX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7723/Reviewer_y34F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7723/Reviewer_y34F"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose a new method, Scalable Monotonic Neural Networks (SMNN), to learn monotonic neural networks. Monotonic features are propagated forward using exponentiated units, which guarantees monotonicity. Non-monotonic features are propagated forward through confluence units and ReLU units, where confluence units capture interactions between monotonic features and non-monotonic features, and ReLU units capture interactions among non-monotonic features. The authors provide a theoretical result to verify the monotonicity of SMNN and experimental results to demonstrate the performance of SMNN."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The investigated problem, learning monotonic neural networks, is important in many fields, where some features are believed to have a positive or negative impact on the concerned output.\n2. To the best of my knowledge, the proposed SMNN is novel.\n3. The proposed SMNN is succinct. It is easy to understand as it only consists of commonly used activation functions, exponentiated weights, and a two-group architecture. Thus, SMNN can be trained by traditional backpropagation algorithms. It is also intuitive that SMNN can guarantee monotonicity."
                },
                "weaknesses": {
                    "value": "1. The authors review previous methods in detail in the introduction part but the introduction might be too long and Table 1 does not fully distinguish SMNN and other methods. In Table 1, LMN, constrained MNN and SMNN have the same characteristics. It would be better to summarize the difference between SMNN and other methods in Table 1, which helps readers grasp the advantage of SMNN without reading through the long introduction.\n2. The authors provide many experiments but it seems that the proposed SMNN has a similar performance to LMN. There are 5 datasets. SMNN wins on 2 of them, loses on 2 of them, and is comparable on the last one.\n----\nThe authors' answers are convincing and I have updated my ratings."
                },
                "questions": {
                    "value": "1. On page 2, the authors claim that the assurance of monotonicity is not always guaranteed as a regularization method. This is true but I think this should not be treated as the drawbacks of regularization methods. The monotonicity usually comes from experience and may not be the truth. When we prefer to believe the experience, we can use methods in the first group to guarantee monotonicity. But when the experience contradicts collected data, then regularization provides a chance to balance between past experience and new data. Thus, I think it is more suitable to say these two groups of methods have their applications in the real world rather than treating not guaranteed monotonicity as a drawback.\n2. Authors provide two definitions for partial monotonicity, one using function values and one using partial derivatives. However, it seems that the definition using function values is not correct. I think the correct one should be \"The function $f(\\boldsymbol{x})$ is partially monotonically non-decreasing on $\\boldsymbol{x}_m$ iff $\\forall i, x_i \\leqslant x_i' , \\forall j \\neq i , x_j = x_j' \\Rightarrow f(\\boldsymbol{x}) \\leqslant f(\\boldsymbol{x}')$. Take a 2-dimensional case as an example. Let the 1st coordinate be the monotonic feature, and the 2nd is not. Let $f(x_1 , x_2) = x_1 + x_2^2$. Then $f$ is partially monotonic according to the definition using partial derivatives but is not partially monotonic according to the definition using function values.\n3. On page 4, the authors claim that \"ReLU is commonly used as an activation function due to its advantages such as avoiding gradient vanishing\". ReLU helps address the problem of gradient vanishing (compared with sigmoid) but it cannot \"avoid\" gradient vanishing. When the weight parameters are sufficiently small and the number of layers is large, ReLU also faces the problem of gradient vanishing, and other techniques are needed to help address the problem of gradient vanishing. Thus, it would be better to use the verb \"alleviate\" rather than \"avoid\".\n4. In Eq. (5), $h_{con}$ should be $h_{conf}$.\n5. In the current SMNN structure, both the exp unit and conf unit use the ReLU-n activation function for the purpose of universal approximation and aligning output magnitudes. But in the ReLU unit, ReLU is used rather than ReLU-n. I am wondering what will happen if all these three units use the ReLU-n activation function. I think a unified activation function makes the structure more succinct.\n6. In theorem 1, $x$ belongs to $\\boldsymbol{x}_m$. To my understanding, $\\boldsymbol{x}_m$ is a vector rather than a set, which indicates that the belonging relation is not rigorously defined. I think $x$ is a coordinate of the vector $\\boldsymbol{x}_m$. A suitable way is to define the subscript set of $\\boldsymbol{x}_m$ as $M$, and then use $x_i$ with $i \\in M$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7723/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7723/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7723/Reviewer_y34F"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7723/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698154567918,
            "cdate": 1698154567918,
            "tmdate": 1700551651322,
            "mdate": 1700551651322,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Nq9v5Z5uNE",
                "forum": "DjIsNDEOYX",
                "replyto": "aW8GfAmW8L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7723/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7723/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "W1: Certainly, we appreciate your constructive feedback. Table 1 has been updated in response to your suggestion, incorporating an additional column labeled `trainable using traditional gradient descent.\u2019 Notably, our method allows for training with the conventional gradient descent algorithm, while both LMN and Constrained MNN require intentional modifications to weights during their training, making traditional gradient descent infeasible. Furthermore, we have introduced a classification of benchmark techniques into two major groups for clarity.\n\nW2: Certainly, we acknowledge that our method demonstrated performance on par with existing methods. While achieving lower performance could be considered a weakness, we would like to emphasize that comparable performance should not be perceived as a weakness, in our perspective. While striving for better performance is always desirable, please understand that attaining the best performance in the field of monotonic neural networks is not the sole objective of this research. Our method, as highlighted in Table 1, presents advantages over existing techniques, maintaining competitive performance levels. Given that our approach supports gradient descent learning without the need for additional modifications (such as weight normalization or weight absolutization) or the imposition of mathematical constraints such as the Lipschitz constraint, we see our method as a valuable addition to the field of monotonic neural networks because it will be suitable for expanding research into different complex network architectures for future works, such as neural additive models or convolution structure.\n\nQ1: We deeply appreciate your valuable feedback. As we totally agree with your comment, we have made the following modification.\nBefore: However, this method has some drawbacks. It requires additional processes to enforce and verify monotonicity, and the assurance of monotonicity is not always guaranteed as a regularization method.\nAfter: However, it requires additional processes to enforce and verify monotonicity. As a regularization method, this method does not always ensure monotonicity. Therefore, in scenarios where the guarantee of monotonicity is essential, opting for the approaches with customized architecture may be preferable.\n\nQ2: Again, we deeply appreciate your valuable feedback. As we totally agree with your comment, we have made the modification on the definition using function values. Please refer to the revised manuscript.\n\nQ3: This feedback is very considerate. Thank you so much for very carefully reviewing our paper. The word 'avoiding\u2019 has been replaced with the word 'alleviating.\u2019\n\nQ4: We have modified the equation by following your comment. Thank you.\n\nQ5: Thank you for raising an interesting issue. Please refer to the newly created Appendix C1 on this matter. Additionally, we have inserted the following sentence into the middle of the 'Network structure\u2019 paragraph.\n`` To maintain a consistent activation function throughout the network, it is acceptable to use the ReLU-n function for the ReLU unit (See Appendix C1.).\u2019\u2019\n\nQ6: We believe that your comments have significantly improved our paper. We have made modifications to Theorem 1 based on this feedback."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700164694976,
                "cdate": 1700164694976,
                "tmdate": 1700164694976,
                "mdate": 1700164694976,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "npqt6MERrO",
                "forum": "DjIsNDEOYX",
                "replyto": "Nq9v5Z5uNE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7723/Reviewer_y34F"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7723/Reviewer_y34F"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed answering. The revised version and answers solve all my concerns. Especially, the revised Table 1 clearly demonstrates the difference between the proposed method and traditional ones, which also implies the potential advantage of the proposed method. After reading other reviewers' comments and authors' corresponding feedback, I find no obvious weakness. Thus, I have updated my ratings and suggest acceptance for this paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551730146,
                "cdate": 1700551730146,
                "tmdate": 1700551730146,
                "mdate": 1700551730146,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nNSuwROTfd",
                "forum": "DjIsNDEOYX",
                "replyto": "eGGfdC8jAb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7723/Reviewer_y34F"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7723/Reviewer_y34F"
                ],
                "content": {
                    "comment": {
                        "value": "I think it is a great and potential idea to use only exp units but duplicated non-monotonic inputs. At least from the aspect of formulation, it provides a unified and simple way to guarantee monotonicity."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552465054,
                "cdate": 1700552465054,
                "tmdate": 1700552465054,
                "mdate": 1700552465054,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YdhslEfSgU",
                "forum": "DjIsNDEOYX",
                "replyto": "MtMvyhx3Ka",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7723/Reviewer_y34F"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7723/Reviewer_y34F"
                ],
                "content": {
                    "comment": {
                        "value": "I have some further comments to share with and be confirmed by authors. It would be great if authros can provide their feedback on these ideas.\n1. ReLU-n is used in exp unit since ReLU cannot achieve universal approximation while ReLU-n can. I think another motivation is the gradient problem. If we use ReLU in exp unit and $exp(w)^\\top x$ is large, then a small change of $exp(w)^\\top x$ will cause a large change of the output, which may suffer from gradient exploding problem. Thus, it seems that the truncation is necessary to alleviate gradient exploding when using exp unit.\n2. While exp unit guarantees monotonicity, it is hard for exp unit to handle useless feature. If a feature $x$ has no impact on the output, then in traditional neuron, its weight usually converges to 0. But in exp unit, the weight needs to converge to $- \\infty$, which may cause instability. Based on this, it is important to carefully choose monotonic features. This also provides some insights on the method mentioned by Reviewer sRQA: learning all features with exp unit may be difficult when there exist useless features."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726967056,
                "cdate": 1700726967056,
                "tmdate": 1700726967056,
                "mdate": 1700726967056,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hnFVXScPEP",
                "forum": "DjIsNDEOYX",
                "replyto": "uDdm9hOONV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7723/Reviewer_y34F"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7723/Reviewer_y34F"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. I have no further questions. : )"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738006536,
                "cdate": 1700738006536,
                "tmdate": 1700738006536,
                "mdate": 1700738006536,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vHxl9lg4Dm",
            "forum": "DjIsNDEOYX",
            "replyto": "DjIsNDEOYX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7723/Reviewer_iEFi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7723/Reviewer_iEFi"
            ],
            "content": {
                "summary": {
                    "value": "The paper tackles the inductive bias of monotonicity in Neural Networks. The authors propose a module based on nonnegative weights with a ReLU-n activation to enforce monotonicity. The proposed architecture comprises three types of hidden layers to handle monotonic and non-monotonic inputs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is very clear and easy to follow.\n- The method itself is simple and easy to implement.\n- Variety of experiments across several domain-specific datasets and toy problems."
                },
                "weaknesses": {
                    "value": "- It could be argued that the novelty of the approach is limited since Mikulincer & Reichman (2022) already describe a very similar architecture and prove the universality of 4-layer threshold networks with nonnegative weights in approximating monotonic datasets. ReLU-n is a continuous relaxation of the threshold function, so it is likely that universality holds here as well for the same reasons and might possibly offer better convergence properties, but this is not explored.\n- There seems to be a big focus on scalability, but it\u2019s unclear how this approach is better than current state-of-the-art models in this respect. The experiments do not compare against other models, and for good reason, I imagine; there is little difference in the scalability of the proposed method and existing works in the literature. So, this begs the question of why scalability is a central thesis in the paper.\n\nOverall, this paper feels like yet another approach to the inductive bias of monotonicity in neural networks. It has no big flaws, but the advantages do not appear to be substantial compared to current methods. The approach is simple enough and is worth considering, but the paper is slightly below the acceptance threshold."
                },
                "questions": {
                    "value": "- Could you clarify what this sentence means: \u201cNevertheless, it should be noted that an optimally trained network is not always guaranteed with respect to the Lipschitz constant \u03bb, particularly for larger constants\u201d?\n- I am unsure why scalability is considered such a central point in the paper. Some of the other methods are clearly just as scalable, including LMNs and constrained monotonic networks. Am I missing something?\n\n**********Nits:**********\n\n- Missing slash for $\\exp$ instead of $exp$ in the proof of theorem 1 on page 5."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7723/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814040173,
            "cdate": 1698814040173,
            "tmdate": 1699636941680,
            "mdate": 1699636941680,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WeJV4IRVWW",
                "forum": "DjIsNDEOYX",
                "replyto": "vHxl9lg4Dm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7723/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7723/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "W1: We sincerely acknowledge your point. Indeed, as you mentioned, the research by Mikulincer & Reichman (2022) demonstrated the universal approximation property for 4-layer monotonic neural networks. Yes, this research served as a motivation for our approach. Due to the reasons outlined below, we believe there are distinct differences between the two studies and that our proposed method has its own contribution.\nThe conclusion of Mikulincer & Reichman (2022) states the following. ``One aspect we did not consider here is learning neural networks with positive parameters using gradient descent. It would be interesting to examine the efficacy of gradient methods both empirically and theoretically. Such study could lead to further insight regarding methods that ensure that a neural network approximating a monotone function is indeed monotone. Finally, we did not deal with generalization properties of monotone networks: Devising tight generalization bounds for monotone networks is left for future study.\u2019\u2019 As evident from their conclusion, they did not provide empirical verification through actual implementation, nor did they validate learning using gradient descent methods. Remarkably, our SMNN method directly translates their theoretical findings into practical implementation. As such, we firmly believe that our work stands at the forefront of the latest research advancements. While they did not propose a specific network structure and did not consider partially monotonic cases, we addressed both aspects in our study.\n\nW2: We have performed the experiments for scalability comparison. Please refer to the next paragraph. We genuinely appreciate your insightful feedback. While it is a fact that our method exhibits scalability when compared to benchmark methods, the primary reason for our use of the term 'scalability\u2019 is the absence of prior claims in this particular aspect within existing research. We believed that introducing this term could contribute to differentiating our work. Furthermore, as demonstrated, our method remains scalable with respect to both increasing numbers of monotonic features and expanding network sizes. We are encouraged by the agreement on this scalability aspect expressed by some reviewers. We wish your perspective on extending the term 'scalability\u2019 in a broader context. Our method's generic nature, as highlighted by other reviewers, positions it well for future research endeavors exploring diverse and complex network architectures, such as neural additive models or convolution structures. Your thoughtful consideration of these aspects will be highly appreciated.\nIndeed, creating a fair experimental environment for scalability comparison, considering different learning ways, structures, and feasibility based on the number of monotonic inputs, poses challenges in ensuring identical network sizes for training/test times. Nevertheless, we made efforts to assess the speed of our method in comparison to benchmarking methods, and the results are presented in the table below. While this is not a comprehensive comparison, it is evident that our method was faster than others based on the available comparisons (Notice that verifying time is not test time but additionally required time only for Certified MNN. We did not perform the comparison of test times as it is unnecessary because all methods except for COMET require a similar small inference time.). We hope this outcome addresses your concern. As we have already demonstrated the scalability of our method empirically in Section 4.1, we have chosen not to include the table below in the revised manuscript.\n|Dataset|Method|# Parameter|# Monotonic Features|Training Time (s)|Verifying Time (s)|\n|:---:|:---:|:---:|:---:|:---:|:---:|\n|Auto-MPG|Certified MNN|$11006$|$3$|$99.88\\pm13.37$|$28.83\\pm12.89$|\n||COMET|$421$|$3$|$--$|$--$|\n||HLL|$14301$|$3$|$113.64\\pm1.89$|$--$|\n||LMN|$14025$|$3$|$22.20\\pm1.31$|$--$|\n||Constrained MNN|$14025$|$3$|$26.04\\pm1.20$|$--$|\n||SMNN (Ours)|$14193$|$3$|$\\mathbf{17.00\\pm0.91}$|$--$|\n|Heart Disease|Certified MNN|$7318$|$2$|$27.16\\pm6.71$|$11.31\\pm6.71$|\n||COMET|$785$|$2$|$--$|$--$|\n||HLL|$7788$|$2$|$16.56\\pm0.92$|$--$|\n||LMN|$7617$|$2$|$10.04\\pm1.17$|$--$|\n||Constrained MNN|$7617$|$2$|$12.04\\pm0.17$|$--$|\n||SMNN (Ours)|$7905$|$2$|$\\mathbf{9.40\\pm0.91}$|$--$|"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700164612789,
                "cdate": 1700164612789,
                "tmdate": 1700166687269,
                "mdate": 1700166687269,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hWG2QVERDB",
                "forum": "DjIsNDEOYX",
                "replyto": "vHxl9lg4Dm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7723/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7723/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q1: Intended meaning of the sentence is as follows. Due to the constraint imposed by $\\lambda$ in LMN, when the Lipschitz constant $\\lambda$ is smaller than the intrinsic data monotonicity, it is impossible to sufficiently learn the given data using LMN because the inherent monotonicity is greater than $\\lambda$ (LMN cannot approximate any function with a gradient exceeding the given Lipschitz constant). As you mentioned, it seems that the original sentence does not accurately convey the intended meaning. Therefore, in order to clearly convey the meaning, we have revised the sentence as follows.\n``Nevertheless, it is important to note that if the Lipschitz constant \\lambda remains smaller than the inherent monotonicity scale within data, LMN may struggle to effectively learn from the data. This limitation arises because LMN cannot approximate any function that holds a gradient exceeding the given Lipschitz constant.\u2019\u2019\n\nQ2: Please refer to our answer to W2.\n\nN1: Thank you for the comment. We have made modifications to the exponential function by following your feedback."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700164629322,
                "cdate": 1700164629322,
                "tmdate": 1700166774470,
                "mdate": 1700166774470,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VdPtKJxW2F",
                "forum": "DjIsNDEOYX",
                "replyto": "hWG2QVERDB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7723/Reviewer_iEFi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7723/Reviewer_iEFi"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thoughtful response. I appreciate the effort that went into the reply, yet my original conclusion still holds: while this paper has no immediate weaknesses, the contributions remain limited. The approach has little technical or theoretical novelty and does not represent a significant improvement over prior works in any particular area. The scalability argument alone is insufficient to set this approach apart, especially because some baselines are just as scalable. The table above seems rather inconclusive (e.g., LMN and SMNN are within error bars in dataset 2), though I want to highlight that I don't think scalability should be a point of contention in the first place. There is no reason LMNs and SMNNs should have different scaling. Additionally, the original experiments in the paper show virtually no performance gap between the two approaches.\n\nIn short, this paper is a new implementation of monotonic networks and does what it's supposed to do. However, I think my original score remains appropriate because SMNN does not excel in any specific aspect."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620453275,
                "cdate": 1700620453275,
                "tmdate": 1700620453275,
                "mdate": 1700620453275,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AiwYKaSyrM",
                "forum": "DjIsNDEOYX",
                "replyto": "feQfKpblnG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7723/Reviewer_iEFi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7723/Reviewer_iEFi"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. You mentioned using a neural additive model for interpretability. It would have been great to see a full study in this direction, and I think it would strengthen the submission substantially. However, I see no reason why one couldn't use LMN modules in a neural additive model. Could you elaborate on that?"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657783761,
                "cdate": 1700657783761,
                "tmdate": 1700657783761,
                "mdate": 1700657783761,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nGjgy21vbp",
                "forum": "DjIsNDEOYX",
                "replyto": "vHxl9lg4Dm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7723/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7723/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Firstly, we sincerely appreciate your positive feedback on the idea of a monotonic neural additive model (NAM) and your engagement in a further discussion. We would like to clarify that our thoughts shared here are preliminary and based on our current understanding at an idea level.\n\nRegarding our comment about it being \u2018challenging for LMN,\u2019 we want to clarify that we did not imply it is impossible. The challenge we mentioned to is associated with the Lipschitz constant ($\\lambda$). In a basic NAM structure, each subnetwork necessitates its own $\\lambda$ for its specific input feature, requiring $k$ hyperparameters for $k$ input features. While we believe there is an optimal combination of $k$ $\\lambda$'s, discovering it poses a substantial challenge. Moreover, in a higher-order NAM structure or a multitask NAM structure, determining the optimal $\\lambda$ values becomes even more difficult, as the number of subnetworks remarkably increases. This is the basis for our thoughts of it being challenging for LMN."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667295820,
                "cdate": 1700667295820,
                "tmdate": 1700667434967,
                "mdate": 1700667434967,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eGGfdC8jAb",
            "forum": "DjIsNDEOYX",
            "replyto": "DjIsNDEOYX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7723/Reviewer_sRQA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7723/Reviewer_sRQA"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an architecture for a neural net that guarantees the monotonicity of the output wrt a subset of predefined inputs, and still allows interactions between features derived from all inputs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality\n--------------\nThe method is related to several existing pieces of work, but the design of the specific architecture, enabling the integration of information from the non-monotonous inputs in a principled way, at every level, is original to my knowledge.\nThe architecture is generic enough that it should be applicable in a wide variety of settings.\n\nQuality\n----------\nExperiments are well designed and match well with the (implicit) research questions, demonstrate well the behaviours of the algorithm, in particular:\n1. when scaling up the number of parameters\n2. when scaling up the number of monotonic features\n3. when increasing the noise (compared to less-constrained or regular MLPs)\n\nExperiments on real datasets show that this method is at least competitive with state-of-the-art methods.\n\nClarity\n---------\nThe paper is clearly written, does not pose any notable challenge for comprehension. The method is defined clearly enough to be reimplemented from the paper, even without the provided source code.\n\nSignificance\n-----------------\nA scalable, end-to-end learning method guaranteeing the monotonicity wrt some inputs would have an impact for ML application where interpretability, trust, or predictability are necessary."
                },
                "weaknesses": {
                    "value": "Quality\n----------\n1. Regarding the theoretical aspect, and the proof of Theorem 1, an issue is that ReLU-n is not a differentiable function, contrary to the definitions in section 2. Its derivative is not defined at 0 or n. I think the conclusion is still correct due to it being continuous, and that both the left-derivative and the right-derivative at every point exist and are >=0, but the proof and definition should be updated.\n2. The scaling of compute time wrt number of monotonic features cannot really be extrapolated from varying the dimensions from 1 to 20. It's likely that at this scale, the execution time is dominated by constant factors, and it's unclear how it would actually scale to \"high-dimensional monotonic features\", compared to Certified MNN or COMET for instance. Due to the architecture, it is likely that the scaling time _per training step_ would scale up as well as a regular MLP, however optimization issues could happen and slow down convergence, for instance. It would be more effective to demonstrate the scalability of this method by applying it on a problem too big for Certified MNN or COMET.\n3. In Tables 2 and 3, statistical ties should be bolded, and be assessed by a statistical test taking into account the variance of _both_ distributions. Assuming the \u00b1 numbers indicate 95% CIs, there should be many more ties, and the conclusions about SMNN outperforming all other methods on regression tasks to not really hold. For instance, on AutoMPG, 7.44\u00b11.20 and 7.58\u00b11.20 should clearly overlap.\n4. The COMPAS dataset is used despite ethical concerns, since these features were used by the original COMPAS system to produce a score that was unfair and biased. This work uses the dataset despite not explicitly aiming at producing unbiased decisions, or examining critically the trained system.\n\nClarity\n---------\n1. In Table 1, maybe indicate which methods are categorized as \"regularization\", and which ones are \"customized architectures\". Or, if \"customized architecture\" is synonymous with \"end-to-end learning\", maybe indicate it in the caption.\n2. The circled \"+\" signs in Figure 1 is confusing if the operation is concatenation, not addition. Maybe it would be clearer to have both arrows point directly at the next \"Exp unit\"? (Idem for the fc layer)\n3. Why are Table 2 and 3 separate? Both have a mix of regression and classification datasets, but they have a different set of methods, and those in Table 3 do not report parameter counts.\n4. Not clear at first what \"subjected to denoising\" (S. 4.1) means.\n5. Clarify what the \"\u00b1\" numbers in the table means: standard deviation, confidence interval?\n\nSignificance\n-----------------\nThe small scale of experiments is a limitation. Given that the ambition of the algorithm is to be \"scalable\", I think it should try to demonstrate scaling where some other algorithms are limited.\n\nMinor points\n-----------------\n1. Datasets should have citations or footnotes when they are introduced in the main text, not only in the appendix. Maybe also refer to the appendix explicitly for which features were considered monotonic."
                },
                "questions": {
                    "value": "1. To disentangle the effects of the connectivity pattern from the ones of monotonous parametrization, I'd be curious to see the performance of a network composed only of Exponential units (all fully connected), but where the non-monotonous inputs would be duplicated as [x_\u00acm, -x_\u00acm]. Is that something you tried?\n\nUpdate after discussion\n--------------------------------\nMany points have been addressed, so I'm moving my score from 6 to 8."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns",
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The COMPAS dataset is used despite ethical concerns, since these features were used by the original COMPAS system to produce a score that was unfair and biased. This work uses the dataset despite not explicitly aiming at producing unbiased decisions, or examining critically the trained system."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7723/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7723/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7723/Reviewer_sRQA"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7723/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698897304415,
            "cdate": 1698897304415,
            "tmdate": 1700688476148,
            "mdate": 1700688476148,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "J0h9HsVpFZ",
                "forum": "DjIsNDEOYX",
                "replyto": "eGGfdC8jAb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7723/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7723/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "W1 (Quality): Thank you very much for pointing this out. According to your comment, we have modified the definition and the proof in Section 3 by using the left and right-hand limits.\n\nW2 (Quality): We acknowledge your concern. In response, we conducted an additional experiment involving the expansion of the model in equation (11). Specifically, we created a 220-dimensional dataset, excluding the output feature, consisting of 20 uniform distribution-based dummy inputs and 200 monotonic inputs achieved by expanding each group of features from 5 to 50 features. Notably, in this experiment, the dummy features exert much less influence during training compared to our original experiment. The training time and MSE results are shown in the table below. We observed the consistent trend of training times remaining constant while MSE decreases as the number of monotonic inputs increases. Please consider that the intention behind the scalability test using equation (11) was to demonstrate not only the constancy of training time but also the improvement in MSE performance with the gradual increase in monotonic inputs. We believe that this additional experiment adequately addresses your concern. However, since the original experiment effectively conveyed our intended message and presenting the equation for the 220-dimensional dataset might be challenging in the paper, we have chosen not to replace the original experiment with this new one.\n|# Monotonic Features|# Parameter|MSE|Time (s)|\n|:---:|:---:|:---:|:---:|\n|$m=10$|$29414$|$0.204\\pm0.12$|$78.76\\pm2.22$|\n|$m=20$|$29977$|$0.216\\pm0.12$|$78.40\\pm2.57$|\n|$m=50$|$31657$|$0.175\\pm0.09$|$78.24\\pm1.51$|\n|$m=100$|$34457$|$0.117\\pm0.09$|$79.08\\pm2.06$|\n|$m=200$|$40057$|$0.013\\pm0.01$|$78.84\\pm2.01$|\n\nW3 (Quality): \nWe appreciate the constructive feedback. To enhance the statistical clarity of our results, we have made revisions to Tables 2 and 3, accounting for statistical ties. The values following the `\u00b1\u2019 symbol in these tables indicate the standard deviation based on 25 repetitions of our experiments. As the references do not explicitly provide details on experimental configurations such as the number of repetitions, we lack the necessary information to conduct statistical tests for benchmarking methods. Consequently, we have defined a statistical tie when, for both methods, one mean falls within the range (mean\u00b1std) of the other. Instances of statistical ties by this definition are marked with a $\\dagger$ symbol.\n\nW4 (Quality): We acknowledge the ethical concerns surrounding the use of the COMPAS dataset (we already mentioned this in Appendix A2.), particularly given its history of being utilized in ways that produced biased and unfair outcomes. While our work does not explicitly focus on producing biased/unbiased decisions or critically examining the trained system from a fairness perspective, we used the COMPAS dataset as a means to test and demonstrate the technical capabilities of our model in a widely recognized context. Please also notice that this dataset has been used in several published papers for the same purpose with ours. Nevertheless, if it is still unable to avoid ethical concerns, we will definitely stop using the data and remove its results.\n\nW1 (Clarity): Thank you for the constructive comment. By following the comment, we have revised Table 1. In addition, we have incorporated an additional column (trainable using traditional gradient descent) to illustrate the distinction between our approach and the LMN and Constrained MNN methods. Unlike our method, both LMN and Constrained MNN necessitate deliberate adjustments to weights during their training processes, rendering the utilization of the traditional gradient descent algorithm infeasible.\n\nW2 (Clarity): Thank you for your comment. The `+\u2019 in a circle symbol generally represents concatenation in the related literature. To enhance understanding, Fig. 1 has been modified. The meaning of the symbol has been noted in the bottom right corner of the figure. We would be grateful if you could confirm whether this modification has clearly resolved the issue.\n\nW3 (Clarity): As mentioned in the main text, Tables 2 and 3 include experimental results from the existing benchmark studies. Therefore, the tables have been separated for entry (not dividing the tables based on classification/regression). In Table 3, the original benchmark literature 'COMET\u2019 [Sivaraman et al., 2020] was published without specifying the number of parameters. And the studies 'LMN\u2019 [Nolte et al., 2022] and 'Constrained MNN\u2019 [Runje & Shankaranarayana., 2023] also missed the number of parameters. Therefore, unfortunately, we could only include the performances of benchmark methods for comparison. Please refer to the benchmark literatures.\n\nW4 (Clarity): We have modified the sentence as follows. ``Fig.3(c) presents an interesting observation where the noise of the test dataset was eliminated.\u2019\u2019"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700164460882,
                "cdate": 1700164460882,
                "tmdate": 1700166025160,
                "mdate": 1700166025160,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pcbMSCqcxz",
                "forum": "DjIsNDEOYX",
                "replyto": "eGGfdC8jAb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7723/Reviewer_sRQA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7723/Reviewer_sRQA"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the comprehensive updates, they address most of the points raised.\n\nI think these are the 2 main remaining weaknesses:\n1. Scalability is presented as one of the main advantages of this method compared to SOTA, but the paper does not demonstrate cases where, because of scalability, this method actually outperforms others (either because other methods become infeasible or too expensive, or because their performance drops).\n2. The architecture introduces some complexity (confluence units, specific connectivity) and hyperparameters (relative size of the 3 ReLU / confluence / exp sub-layers at each layer), but the paper does not make a strong case against simpler variants.\n\nMinor points:\n1. The updated proof of monotonicity is not complete, as it does not show that the resulting function `g` is continuous (nor does it use the fact that ReLU-n is). For instance, a function like $x \\mapsto x - \\lfloor x \\rfloor$ would satisfy your proof, but would not actually be monotonic.\n2. It would be nice to have the additional tables you pasted here in the Appendix, even if you chose not to replace the ones in the main paper."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688375365,
                "cdate": 1700688375365,
                "tmdate": 1700688375365,
                "mdate": 1700688375365,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wKSnTNd7U9",
            "forum": "DjIsNDEOYX",
            "replyto": "DjIsNDEOYX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7723/Reviewer_tddM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7723/Reviewer_tddM"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes Scalable Monotonic Neural Network (SMNN) to learn neural networks that preserve monotonicity of a model w.r.t. a subset of inputs. Existing works require additional overhead in terms of inference cost, weight constraints during training, scalability issues w.r.t. network size and number of monotonic inputs, etc. This work bakes in the monotonic properties directly into the network architecture by designing hidden layers with three different units ( exponential unit, ReLU unit and confluence unit ). Exponential unit preserves the monotonic nature of its input and are explicitly tied to the monotonic inputs. Remaining inputs go through the confluence and the ReLU unit, while the output of the confluence unit also goes through addition with the input for the next exponential unit (see Figure 1). Thus, monotonic inputs pass through a unique path in the network that preserves their monotonic nature. Empirical evaluation demonstrates that this method achieves comparable performance to the existing state-of-the-art and helps eliminate many issues arising in those works."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Interaction of the monotonic inputs is segregated from the other inputs through a dedicated path in the neural network. This ensures the monotonic nature as these inputs pass through exponential units in the hidden layers \n- Empirical experiments show the viability of such a simple approach to enforcing monotonicity"
                },
                "weaknesses": {
                    "value": "- Only performs on-par as the existing methods for enforcing monotonicity in neural networks \n- Its unclear if there's any computational advantage (in terms of training/inference cost) compared to existing baselines\n- Limited discuss on extensions to other activations or expressivity of the network"
                },
                "questions": {
                    "value": "- Does restricting interaction between monotonic and non-monotonic inputs hurt expressivity of this network?\n- How does one incorporate other activations/non-linear operators in this architecture?\n- How do you select the parameter n in ReLU-n activation?\n- Do you have any comparison on train/test time for the baselines with the SMNN architecture? Does it take longer to converge compared to existing methods?\n- Why are parameters missing for XGBoost and Isotonic methods in Table 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7723/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698936805838,
            "cdate": 1698936805838,
            "tmdate": 1699636941463,
            "mdate": 1699636941463,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wMNFyUgvAw",
                "forum": "DjIsNDEOYX",
                "replyto": "wKSnTNd7U9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7723/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7723/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "W1: Certainly, we acknowledge that our method demonstrated performance on par with existing methods. While achieving lower performance could be considered a weakness, we would like to emphasize that comparable performance should not be perceived as a weakness, in our perspective. While striving for better performance is always desirable, please understand that attaining the best performance in the field of monotonic neural networks is not the sole objective of this research. Our method, as highlighted in Table 1, presents advantages over existing techniques, maintaining competitive performance levels. Given that our approach supports gradient descent learning without the need for additional modifications (such as weight normalization or weight absolutization) or the imposition of mathematical constraints such as the Lipschitz constraint, we see our method as a valuable addition to the field of monotonic neural networks because it will be suitable for expanding research into different complex network architectures for future works, such as neural additive models or convolution structure.\n\nW2: Please see our response to Q4.\n\nW3: Please see our responses to Q1, Q2, and Q3.\n\nQ1: Our answer to your question is yes. To investigate the effect of the confluence unit, specifically designed for handling interactions between monotonic and non-monotonic features, we conducted an ablation study and integrated its outcomes into Appendix C3 of the revised manuscript. The following sentence was accordingly modified in the `Network structure\u2019 section of the revised manuscript.\n`` the confluence unit combines the outputs of the exponentiated and ReLU units to capture interactions between both types of features (See Appendix C3 for the justification of the confluence\nunit.).\u2019\u2019\n\nQ2: We appreciate the constructive feedback regarding the extension for the activation function. In order to address this concern, we added the following sentences at the end of the paragraph of `Activation functions\u2019,\n`` Alternative activation functions offering the similar benefit, including both convexity and concavity, can be substituted for the ReLU-$n$ function (See our ablation study in Appendix C1.).\u2019\u2019\nand the ablation study in Appendix C1.\n\nQ3: Thank you for your valuable feedback. Indeed, the parameter $n$ in ReLU-$n$ is a hyperparameter to be predetermined as we mentioned in Appendix A1. We consistently set it to 1 throughout all our experiments, because the primary purpose with this activation function was to utilize the convex and concave properties, making detailed tuning of this value unnecessary. Nonetheless, to empirically investigate whether the performance of our method holds across different values of $n$, we conducted an ablation study and integrated its outcomes in Appendix C2 of the revised manuscript.\n\nQ4: Indeed, creating a fair experimental environment for comparison, considering different learning ways, structures, and feasibility based on the number of monotonic inputs, poses challenges in ensuring identical network sizes for training/test times. Nevertheless, we made efforts to assess the speed of our method in comparison to benchmarking methods, and the results are presented in the table below. While this is not a comprehensive comparison, it is evident that our method was faster than others based on the available comparisons (Notice that verifying time is not test time but additionally required time only for Certified MNN. We did not perform the comparison of test times as it is unnecessary because all methods except for COMET require a similar small inference time.). We hope this outcome addresses your concern. As we have already demonstrated the scalability of our method empirically in Section 4.1, we have chosen not to include the table below in the revised manuscript.\n|Dataset|Method|# Parameter|# Monotonic Features|Training Time (s)|Verifying Time (s)|\n|:---:|:---:|:---:|:---:|:---:|:---:|\n|Auto-MPG|Certified MNN|$11006$|$3$|$99.88\\pm13.37$|$28.83\\pm12.89$|\n||COMET|$421$|$3$|$--$|$--$|\n||HLL|$14301$|$3$|$113.64\\pm1.89$|$--$|\n||LMN|$14025$|$3$|$22.20\\pm1.31$|$--$|\n||Constrained MNN|$14025$|$3$|$26.04\\pm1.20$|$--$|\n||SMNN (Ours)|$14193$|$3$|$\\mathbf{17.00\\pm0.91}$|$--$|\n|Heart Disease|Certified MNN|$7318$|$2$|$27.16\\pm6.71$|$11.31\\pm6.71$|\n||COMET|$785$|$2$|$--$|$--$|\n||HLL|$7788$|$2$|$16.56\\pm0.92$|$--$|\n||LMN|$7617$|$2$|$10.04\\pm1.17$|$--$|\n||Constrained MNN|$7617$|$2$|$12.04\\pm0.17$|$--$|\n||SMNN (Ours)|$7905$|$2$|$\\mathbf{9.40\\pm0.91}$|$--$|\n\nQ5: As mentioned in the main text, the results in Table 2 came from the references (Liu et al., 2020 ; Nolte et al., 2022; Runje & Shankaranarayana, 2023). The paper where these experimental results for XGBoost and Isotonic were first presented is [Liu et al., 2020] and the number of parameters for XGBoost and Isotonic were missing in the original paper. This may be because they are not based on neural networks. We could not include the number of parameters for those methods."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700164267841,
                "cdate": 1700164267841,
                "tmdate": 1700165794136,
                "mdate": 1700165794136,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]