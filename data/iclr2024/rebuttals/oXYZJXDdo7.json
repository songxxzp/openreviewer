[
    {
        "title": "Retrieval is Accurate Generation"
    },
    {
        "review": {
            "id": "KVavUIO4Zk",
            "forum": "oXYZJXDdo7",
            "replyto": "oXYZJXDdo7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5008/Reviewer_6EhH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5008/Reviewer_6EhH"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores text generation, specifically by creating a larger vocabulary which additionally consists of phrases extracted from a large corpus via certain rules, and whose representations are formed by a transformer model encoding the wider context they appear in. \nWhen decoding the model is then able to either generate standard tokens, or longer phrases. \nResults are presented on open generation and question answer generation tasks, showing that the proposed model is able to perform more accurately than 3 other recent retrieval based baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Solid set of empirical results are presented showing the model performs well on the tested tasks. \n* Comparisons against recent baselines are given. \n* Additional phrase representations (produced by the same model used in training) are able to be added at inference time, with the decode model able to operate with these extra, new phrases. \n* The method removes the inference-time dependence on document retrieval that other retrieval augmented generation papers have. It does move this phrase creation process (along with the embedding of these) to training time."
                },
                "weaknesses": {
                    "value": "* The claims about inference speed are \n* Can examples or further clarification be given for the 3.1 sentence \"enhancing the accountability of the output\"? This isn't clear, at least to me. \n* There are a lot of heuristics in extracting the phrases. This may not be easy to repeat, or result in the same level of gains on other datasets or related problems. \n* The decoding method seems very custom. Forcing a limited use of phrases, and blending top-k and top-p sampling. What happens if you just arg-max decode from the resulting model? Does it emit phrases way too often?\n* Distributional sparsity  -- this section is not very clear. \n* Is the likelihood estimation of summing paths well motivated? I'm not sure this is principled, but open to this being further justified. \n* \"For efficiency issues\" in 4.1, does this mean for stability? Keeping the embeddings (of tokens and phrases) means the problem is stable I presume. I think this needs more explanation however. \n* The numbers in table 1 are not described."
                },
                "questions": {
                    "value": "* Was the vanilla LM trained with the phrase extended vocabulary? Or was this just using the gpt2 tokenisation alone? What happens if you  do this, rather than blending the contrastive phrase loss with the common CE loss?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5008/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698167089124,
            "cdate": 1698167089124,
            "tmdate": 1699636489086,
            "mdate": 1699636489086,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JwufOMPVVD",
                "forum": "oXYZJXDdo7",
                "replyto": "KVavUIO4Zk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5008/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5008/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful review. We are delighted to learn that you believe our empirical results are \u201csolid\u201d and recognize several merits of our proposed methods as strengths.\n\n**W1:** *The claims about inference speed are*\n\n**A1**: It seems the sentence is incomplete. Could you please clarify your question? We will answer your question accordingly.\n\n**W2**: *Can examples or further clarification be given for the 3.1 sentence \"enhancing the accountability of the output\"? This isn't clear, at least to me.*\n\n**A2**: Thanks for your insightful question. We are happy to clarify this statement. When we say *\"enhancing the accountability of the output\"*, we're referring to the traceability of the phrases that our model generates. In traditional language models, the generated tokens are produced based on the model's internal representations, which can be difficult to interpret or trace back to the training data. However, in our approach, each phrase that is retrieved during the generation process can be directly traced back to its original document. This means that for each output of our model, we can provide a clear lineage or 'account' of where that output came from in terms of the supporting documents. This traceability enhances the accountability of the model, as it allows us to better understand and explain the model's outputs.\n\n- Example:\nLet's say we have a language model trained on a large corpus of news articles. When generating a news headline, traditional language models might produce a phrase like *\"Breaking: New Study Shows Link Between Coffee and Cancer.\"* However, it can be challenging to determine where exactly this information came from within the training data. In our approach, when generating the same headline, our model retrieves phrases directly from the supporting documents. So, instead of just providing the headline, our model can trace back and indicate that the phrase *\"New Study Shows Link Between Coffee and Cancer\"* was retrieved from a specific news article published by a reputable scientific journal. This traceability enhances the accountability of the output, as it allows us to understand the source of the information and verify its credibility.\n\n**W3**: *There are a lot of heuristics in extracting the phrases. This may not be easy to repeat, or result in the same level of gains on other datasets or related problems.*\n\n**A3**: Thanks for your insightful question. We would like to clarify that the heuristics for phrase extraction are based on general linguistics principles like syntactic structure, semantic similarity, and distributional sparsity. In fact, our experiments are conducted on general corpora for language model pretraining (*i.e., MiniPile and Wikipedia*), without specific tailoring to any particular domain or problem. However, we acknowledge that some hyper-parameters may require tuning when scaling up the training data or altering the data mixture.\n\n**W4**: *The decoding method seems very custom. Forcing a limited use of phrases, and blending top-k and top-p sampling. What happens if you just arg-max decode from the resulting model? Does it emit phrases way too often?*\n\n**A4**: If we simply use argmax decoding, the model's output will contain a lot of repetitions, leading to text degeneration, a well-known issue for traditional language models as well. That's why we employ top-k and top-p sampling in our experiments. For your question on the emission rate of phrases, we conducted an experiment on the dev set of MiniPile. Specifically, we asked the model to predict the next token/phrase token-by-token. We found that **59%** of the time, the top-1 prediction is a phrase.\n\n**W5**: *Distributional sparsity -- this section is not very clear.*\n\n**A5**: Thanks for your constructive feedback. In our approach, we treat lexically identical phrases in different contexts as separate entries in the phrase pool. This means that each high-frequency phrase could potentially introduce tens of thousands, or even millions, of entries. During our analysis of Wikipedia, we observed that approximately 1% of the phrases have a frequency far exceeding that of the others. By removing just these **top 1%** high-frequency phrases, we can reduce the total number of entries by **50%**. Furthermore, we find that these top 1% phrases, such as *\"as well as*\", \"it is,\" *\"there are*\", and others, often lack specific meanings and are context-independent. Therefore, we decided not to introduce these high-frequency phrases into our phrase pool, as they do not contribute significantly to the model's understanding of specific contexts and would lead to imbalanced training that may potentially affect the overall performance of the model."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489697461,
                "cdate": 1700489697461,
                "tmdate": 1700489697461,
                "mdate": 1700489697461,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XGKBMwrdEL",
            "forum": "oXYZJXDdo7",
            "replyto": "oXYZJXDdo7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5008/Reviewer_uQM4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5008/Reviewer_uQM4"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel method for language modeling that instead of generating tokens, retrieves phrases from a phrase-based index. This differs a lot from standard language models, which generate text by selecting tokens from a fixed, finite, and standalone vocabulary. Furthermore, this new approach leverages a more balanced encoding architecture for both the input and target tokens, as opposed to a single token embedding layer on the target side employed in standard language models. Moreover, their paradigm is the first that performs text generation through direct phrase retrieval, steering away from common 2-staged pipeline approaches, and thus removing the dependence on document retrieval and achieving lower latencies.\nThe authors shed light on how to determine the training oracles that allow this kind of training, and they propose to initialize them using linguistic heuristics. Also, in order to allow the model to adjust its own generation paths based on the capabilities it has acquired, they also bootstrap the oracles through iterative self-reinforcement, which gradually refines the oracles with each iteration by transitioning from imitating the oracles to reinforcing its own preferences.\nIn this new paradigm, text generation is achieved by copying retrieved phrases corresponding to constituent units in a syntactic parse tree, but the model still has the ability to generate individual tokens.\nThe effectiveness of their models is validated on various downstream tasks, including open-domain and domain-specific question answering, as well as open-ended text generation, attaining substantial improvements over standard LMs and several retrieval-augmented baselines. Transitioning to phrase retrieval improves interpretability and factuality on text generation tasks, as the semantics of phrases are enhanced by their surrounding contexts, and each retrieved phrase can be traced back to its original document. Finally, enlarging the phrase index during inference, and the plug-and-play feature of the index are shown to be effective and efficient methods for boosting the model's performance and adapting to out-of-domain distributions respectively, without any further training."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- A novel approach for retrieval augmented generation \n- Holistic evaluation not only by measuring the fluency in open-ended text generation but also by carrying out comprehensive evaluation in a wide range of knowledge-intensive tasks, such as open-domain question answering.\n- Plug-and-play feature of the phrase index, as a way of adapting to out-of-domain distributions (such as the Medical domain) by simply changing/extending the phrase index with a domain-specific index without any further training.\n- Paper is generally well written and easy to follow\n- Good explanation of how standard LLMs can be viewed as dual-encoding matching networks connecting different prefixes and tokens, and shedding light into the architecture imbalances between the prefix and the target encoders."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n- More implementation details regarding the size of the phrase index, etc would be good to have in the paper.\n- The work might also benefit from some discussion regarding scalability of the phrase index\n\nMinor suggestions:\n\n- As Figure 1 is the main overview of the approach proposed in the paper, a more detailed footnote would be appreciated.\n- Section 6 \"Results\" wouldn't be better under subsection 5.2.2, as it reflects on results from the Open-Ended Text Generation experiments.\n- Typos: Section 2, line 2. \"The\" after \"Hence, \" should be in lower-case.\n\nMissing references:\n- Minjoon Seo's work on phrase index QA: https://arxiv.org/pdf/1804.07726.pdf, https://arxiv.org/pdf/1906.05807.pdf\n\n\nOverall I think this is an interesting method and the authors perform extensive experimentation to empirically justify their approach"
                },
                "questions": {
                    "value": "- Could you discuss any potential limitations or failure cases of the model, providing insights into scenarios where the proposed approach might not perform as effectively?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5008/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760538227,
            "cdate": 1698760538227,
            "tmdate": 1699636488980,
            "mdate": 1699636488980,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x10qkZWQD8",
                "forum": "oXYZJXDdo7",
                "replyto": "XGKBMwrdEL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5008/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5008/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your recognition of our work. We very much appreciate your detailed review describing our approach as \u201cnovel\u201d (\u201ddiffers a lot from standard language models\u201d), our evaluation as \u201cholistic\u201d and emphasizing the interpretability and \u201cplug-and-play\u201d features of our method. In the following, we would like to answer your concerns and questions one-by-one.\n\n**W1:** *More implementation details regarding the size of the phrase index, etc would be good to have in the paper.*\n\n**A1**: As mentioned in Section 5.1.1 (Domain Adaption), the size of our phrase index is **137,101,097**. We will make it clearer in the next version of this paper.\n\n**W2**: *The work might also benefit from some discussion regarding scalability of the phrase index*\n\n**A2**: Thank you very much for bringing up this insightful question! We consider the current results as a proof-of-concept for the proposed new paradigm. When scaling up to a larger phrase index, we certainly will face more computational challenges. To ensure scalability, we believe potential solutions are clustering, dimensionality reduction, quantization, and fast vector search algorithms with sub-linear time complexity, etc. We leave the implementation and exploration of these solutions as future work and will include this discussion in the next version of this paper.\n\n**Q1:** *Could you discuss any potential limitations or failure cases of the model, providing insights into scenarios where the proposed approach might not perform as effectively?*\n\n**A3**: One potential limitation is that our model relies heavily on the quality and coverage of the phrase index. If the phrases in the index are not diverse enough or do not cover certain fields, the model may struggle to generate accurate and coherent outputs for those topics. Therefore, in terms of failure cases, one scenario could be when the target text contains a lot of novel phrases or concepts that are not present in the phrase index. Another potential failure case could arise from the way we segment Wikipedia into documents of a maximum length of 128. This segmentation could result in some documents lacking sufficient context or information to effectively support the phrases within them. Our model relies on these supporting documents to obtain the phrase representations. If these documents are not clear or detailed enough, it could impact the model's performance. We are actively working on addressing these issues and improving our model's performance in these challenging scenarios.\n\n**Minor suggestions:**\n\n**M1**: *As Figure 1 is the main overview of the approach proposed in the paper, a more detailed footnote would be appreciated.*\n\n**A4**: Sure. we will use the following caption in the next version of this paper: \u201cComparison between our method and standard language models. Standard language models can be viewed as a dual-encoder matching network that connects different prefixes and tokens. In this architecture, the source encoder is implemented by a multi-layer neural network, while the target encoder is simply a token embedding layer. In contrast, our method adopts a balanced architecture, incorporating a phrase encoder to encode candidate phrases from supporting documents. As a result, we perform text generation through phrase retrieval, leveraging the contextual information captured by the phrase encoder.\u201d\n\n**M2**: *Section 6 \"Results\" wouldn't be better under subsection 5.2.2, as it reflects on results from the Open-Ended Text Generation experiments.*\n\n**A5**: Thanks for pointing it out. It should be Section 5.2.2.\n\n**M3**: *Typos: Section 2, line 2. \"The\" after \"Hence, \" should be in lower-case.*\n\n**A6**: Thanks for pointing out the typos. We will fix them in the next version of this paper.\n\n**M4**: *Missing references: Minjoon Seo's work on phrase index* QA:\u00a0https://arxiv.org/pdf/1804.07726.pdf,\u00a0https://arxiv.org/pdf/1906.05807.pdf\n\n**A7**: We will include them in the next version of this paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489385050,
                "cdate": 1700489385050,
                "tmdate": 1700489385050,
                "mdate": 1700489385050,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BsX3IuseMx",
            "forum": "oXYZJXDdo7",
            "replyto": "oXYZJXDdo7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5008/Reviewer_iZXf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5008/Reviewer_iZXf"
            ],
            "content": {
                "summary": {
                    "value": "This paper combines retrieval with test generation and introduces an approach at retrieves context-aware phrases from a database of documents for generation. They use a set of linguistics heuristics combined with a bootstrapping method to extract phrases. The authors have done studies to show the effectiveness of their method and perform ablation study on the effect of different elements. They also study the inference speed of their approach and compare it with other approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well written and easy to follow.\n- Their approach on text generation and selecting phrases is novel and introduces an interesting approach to text generation.\n- The authors study the effectiveness of their approach well and provide comparisons with other approaches.\n- Their zero-shot results on knowledge intensive tasks is convincing of the effectiveness of their approach."
                },
                "weaknesses": {
                    "value": "- Lack of any human evaluations: Although there are automatic metrics for text generation, there still a need to have humans judge the generation.\n- The paper does not provide deep insights into the observed results. For example, Section 6, Main Results, related to Table 4, it is not clear why the MAUVE score has such a huge jump for their method, or why finetuning the base model drops this score by a lot."
                },
                "questions": {
                    "value": "- What corpus is used to finetune the base model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5008/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5008/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5008/Reviewer_iZXf"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5008/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699212178486,
            "cdate": 1699212178486,
            "tmdate": 1699636488899,
            "mdate": 1699636488899,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8ZvcGzUDJO",
                "forum": "oXYZJXDdo7",
                "replyto": "BsX3IuseMx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5008/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5008/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your positive comments and describing our proposed approach as \u201cnovel\u201d and \u201cinteresting\u201d, and the experiment results on knowledge-intensive tasks \u201cconvincing\u201d.  We provide our response to address your questions as below.\n\n**W1:** *Lack of any human evaluations: Although there are automatic metrics for text generation, there still a need to have humans judge the generation.*\n\n**A1:**  Thank you for your question. To address your concern, we conducted a human evaluation study on a random sample of 100 cases. We evaluate the results of the base LM, the base LM without fine-tuning (w/o FT), and our model from four perspectives: fluency, coherence, informativeness, and grammar. Each aspect is scored on a Likert scale from 1 to 4 ( 1 represents \u201c*bad*\u201d, 2 stands for \u201c*Fair*\u201d, 3 is considered \u201c*good*\u201d, and 4 signifies \u201c*very good*\u201d), and then we calculate the average scores. The results are as follows:\n\n| Model | Fluency | Coherence | Informativeness | Grammar |\n| --- | --- | --- | --- | --- |\n| Base LM (w/o FT) | 2.91 | 2.33 | 2.35 | 3.00 |\n| Base LM | 2.81 | 2.37 | 2.40 | 2.79 |\n| Ours | 2.95 | 2.70 | 2.67 | 3.02 |\n\nAs we can see, our method outperforms the base LM in all four categories, especially in coherence and informativeness. Upon analysis, we find that the outputs from our method often have a tighter connection with the preceding text (coherence) and exhibit stronger knowledge characteristics (informativeness). For instance, they often include specialized terms, which are not observed in the outputs from the base LM.\n\nAs for the lower scores of the base LM compared to the base LM (w/o FT), the issue lies in the fact that the fine-tuned model frequently outputs \"References: xxx\" and \"External Links: xxx\". This is related to the characteristics of the Wikipedia dataset, where each article typically ends with references and external links. These elements do not contribute positively to the perceived fluency and grammar.\n\nWe will add the above discussion to the next version of this paper.\n\n**W2**: *The paper does not provide deep insights into the observed results. For example, Section 6, Main Results, related to Table 4, it is not clear why the MAUVE score has such a huge jump for their method, or why finetuning the base model drops this score by a lot.*\n\n**A2:** Following the above manual analysis (please refer to **A1**), we find that the main difference between the performance of the Base LM and the Base LM (w/o FT) is that the former often outputs \"References: xxx\" and \"External Links: xxx\".\n\nTo further investigate this, we retest the Base LM on the test set of MiniPile, excluding all outputs containing \"References\" and \"External Links\" (which accounts for **26.77%** of the cases). The resulting MAUVE score is **60.23**, slightly lower than the **69.68** of the base LM (w/o FT), but substantially higher than the previous score of **42.61**. This suggests that the main reason for the significant drop in the MAUVE score after fine-tuning the base model is due to these extraneous outputs.\n\nAs for the improvement in MAUVE for our method, this can be explained based on the notable improvements in terms of coherence and informativeness. This indicates that our model, based on phrase retrieval, is better at capturing the context and provides more informative content.\n\nWe will add the above discussion to the next version of this paper.\n\n**Q1:** *What corpus is used to finetune the base model?*\n\n**A3**: We use the MiniPile and Wikipedia datasets to fine-tune the base model. Note that all baseline methods are trained using the same datasets."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489200473,
                "cdate": 1700489200473,
                "tmdate": 1700489200473,
                "mdate": 1700489200473,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ODlNfBYq4Z",
            "forum": "oXYZJXDdo7",
            "replyto": "oXYZJXDdo7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5008/Reviewer_Uubi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5008/Reviewer_Uubi"
            ],
            "content": {
                "summary": {
                    "value": "Retrieval augmented generation models are very powerful  in making generation more attributable and trustworthy. The proposed approach in the paper belongs to this family. It is inspired from CoG (Lan et al.)  that retrieves phrases from similar contexts, however, unlike CoG, it doesn\u2019t employ a two-stage pipeline, specifically document retrieval followed by grounded phrase extraction. The proposed approach removes the dependence on document retrieval. \n\nInterestingly, the authors propose to use linguistics-motivated heuristics to initialize the training oracle phrases, followed by a bootstrapping mechanism through self-reinforcement to refine the oracle with each iteration. This linguistically inspired approach could be very useful in providing meaningful attributions to their sources. \n\nThe experiments on Open-book qa and open ended generation show consistent improvements over competitive baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed approach seems very interesting and will be useful to the generation community. As I mentioned earlier, the linguistically inspired approach could be very useful in providing meaningful attributions to their sources. \n\nStrong results on a variety of benchmarks from Open book qa and open ended generation tasks."
                },
                "weaknesses": {
                    "value": "The authors proposed a very interesting approach but I felt a lot of important details are missing. Please see my questions/comments below. It is unclear whether or not the code will be released from this work.\n\nAnother weakness of the work I believe is that this approach will not be robust to languages or domains where our syntactic parsing capabilities are limited."
                },
                "questions": {
                    "value": "\u201ceach phrase possesses a relatively complete and well-defined meaning\u201d -> Will this approach be not generalizable to languages and domains where the availability of syntactic parsers is limited? Also is it feasible to annotate the whole training set?\n\n\u201cIncorporating high-frequency phrases can significantly increase the total number of phrases, leading to an extremely large candidate pool\u201d -> Won\u2019t the low-frequency phrases significantly increase the size of the candidate pool? \n\nSecond paragraph under \u201cSemantic similarity\u201d: I felt lots of details were missing here to better understand the quality of phrases, and the feasibility of the proposed approach. The Appendix A do not provide all necessary details. Is this done on the pretraining corpus? What trivial constituents were dropped out and why (some examples would help)? \n\nSec 3.2.2: I found the explanation a bit confusing. Could you add an algorithm and/or an example demonstrating the algorithm? \n\n\u201cIf no such phrase is found, we retain the previous target.\u201d -> When would this occur? When the candidate pool is empty? \n\n\u201cwe also add the token vocabulary to our phrase table\u201d -> Are they subword units? What is the vocabulary? \n\n\u201cWe train our model on the training set of MiniPile2 (Kaddour, 2023)\u201d -> What is this dataset? Is it a pretraining set of finetuning set? How are they used during training? This dataset is discussed again in 5.2.\n\n\u201cNote that the sum of all possible paths can be computed efficiently using dynamic programming with time complexity O(n 2 ), where n represents the number of tokens in the text\u201d -> Will this be limiting for long form outputs? \n\nTable 1: Are the numbers for baselines taken from the respective papers or are they reproduced by the authors?\n\nSec 6: Results: This should be Sec 5.2.2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5008/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699314735764,
            "cdate": 1699314735764,
            "tmdate": 1699636488767,
            "mdate": 1699636488767,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n0r0gcoGO1",
                "forum": "oXYZJXDdo7",
                "replyto": "ODlNfBYq4Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5008/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5008/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your encouraging feedback and describing our work as \u201cvery interesting\u201d, \u201cvery useful\u201d, and \u201cstrong results\u201d.  Below, we would like to address your concerns point by point.\n\n**W1**: *implementation details and code release*\n\n**A1**:  We will add more details to the paper and release our code upon acceptance.\n\n**W2**: *robustness to languages or domains where our syntactic parsing capabilities are limited.*\n\n**A2**: Thanks for this insightful question. Indeed, the initialization of our approach relies on syntactic parsing. We anticipate performance degradation for languages and domains when the parser accuracy is relatively low. However, we would like to mention that syntactic parsing is a very well-studied task in NLP as well as its cross-domain and cross-language generalization. For example, the Universal Dependencies (**https://universaldependencies.org/**) project provides consistent grammatical annotation across over 100 languages. To our knowledge, the state-of-the-art parsing accuracies are pretty high for major languages such as English, Chinese, Italian, Japanese, Portuguese, etc. We will explore cross-language and cross-domain robustness in future work.\n\n**Q1**: *\u201ceach phrase possesses a relatively complete and well-defined meaning\u201d -> Will this approach be not generalizable to languages and domains where the availability of syntactic parsers is limited? Also is it feasible to annotate the whole training set?*\n\n**A3**: Please refer to our answer to W2. We would like to clarify that today\u2019s syntactic parsers cover most common languages. For situations where a syntactic parser is unavailable, alternative methods may be utilized such as unsupervised syntactic parsing and unsupervised tokenization methods (*e.g.,* BPE, sentencepiece).\n\nAs for the concern about annotating the whole training set. The Stanza parser we used only occupies about 100MB in disk (approximately 0.027B parameters). In fact, the syntactic parsing of the whole MiniPile dataset (5.7GB in disk) takes approximately **10 hours on 8 V100 GPUs**. Therefore, the cost of annotating the training set is relatively small compared to the cost of training the model.\n\n**Q2**: *\u201cIncorporating high-frequency phrases can significantly increase the total number of phrases, leading to an extremely large candidate pool\u201d -> Won\u2019t the low-frequency phrases significantly increase the size of the candidate pool?*\n\n**A4**: Thanks for your insightful question! Low-frequency phrases do contribute to the size of the candidate pool. However, we find that high-frequency phrases have a much larger impact by contrast. Note that lexically identical phrases in different contexts are treated as different entries in the phrase pool. This means that a high-frequency phrase could potentially introduce tens of thousands, or even millions, of entries. In processing Wikipedia, we found that by removing just the **top** **1%** high-frequency phrases, we can reduce the total number of entries by **50%**. In contrast, the top **69%** of phrases with the lowest frequency account for only **15%** of the total number of entries.\n\n**Q3**: *Second paragraph under \u201cSemantic similarity\u201d: I felt lots of details were missing here to better understand the quality of phrases, and the feasibility of the proposed approach. The Appendix A do not provide all necessary details. Is this done on the pretraining corpus? What trivial constituents were dropped out and why (some examples would help)?*\n\n**A5**:\n\n- For the feasibility of the proposed approach, we believe you're concerned about the time cost of the entire preprocessing process. We processed the entire pretraining corpus (i.e., MiniPile and Wikipedia). This process (including syntactic parsing, phrase selection, and semantic matching) took approximately **24 hours on 8 V100 GPUs**. The overhead is small compared to the cost of training the model.\n- As for trivial constituents, we refer to constituents that are not semantically rich, such as conjunctions (*e.g., \"and\", \"or\"*), predeterminers (*e.g., \"both\", \"all\"*), wh-determiners (*e.g., \"which\"*), and wh-pronouns (*e.g., \"who\", \"what\"*).\n- We will add more details as mentioned above to the next version of this paper. Thanks for your suggestions."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489795677,
                "cdate": 1700489795677,
                "tmdate": 1700489795677,
                "mdate": 1700489795677,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]