[
    {
        "title": "EGALA: Efficient Gradient Approximation for Large-scale Graph Adversarial Attack"
    },
    {
        "review": {
            "id": "a9wFiBUb4t",
            "forum": "Yd7idEYzNv",
            "replyto": "Yd7idEYzNv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3975/Reviewer_BFci"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3975/Reviewer_BFci"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes EGALA, a scalable graph adversarial attack based on gradient approximation. Specifically, authors exclusively focus on the SGC as the surrogate model for attacking. By formulating the gradient of loss with respect to adjacency matrix as matrix product, EGALA adopts a scalable nearest neighbor search algorithm to identify the edges with largest gradients. Experimental results indicate that EGALA is more effective and efficient than prior scalable attack methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Overall, the paper is well-written.\n- Addressing the scalability issue of graph adversarial attacks is important.\n- Authors have evaluated on large-scale datasets."
                },
                "weaknesses": {
                    "value": "- The gradient derivation is exclusively based upon the SGC model, which raises uncertainty about whether EGALA remains applicable to other, more advanced GNN models (e.g., GAT, GPRGNN, etc.). While authors have mentioned this limitation, I believe this is a critical issue and has to be addressed. Otherwise, I regret to say that the contribution of this work may not appear significant.\n- There are some approximation steps in EGALA, such as Equation 16 and the nearest neighbor search. Given that authors have not provided the theoretical analysis on those approximation errors, it is less convincing whether EGALA indeed accurately identifies those edges with largest gradients. One way to address this concern could be comparing the gradients approximated by EGALA with the actual gradients on some small datasets.\n- Authors only attack the scalable defense approach Soft Median. The results would be more compelling if authors could also attack other types of scalable defense methods (e.g., graph purification)."
                },
                "questions": {
                    "value": "- Have authors adopted mini-batch training on large graphs? How does the mini-batch training affect the gradient approximation in EGALA?\n- How do authors perform hyperparameter tuning on all GNN models in the experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3975/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3975/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3975/Reviewer_BFci"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3975/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698603692258,
            "cdate": 1698603692258,
            "tmdate": 1699636358939,
            "mdate": 1699636358939,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LZf8bZ2ewb",
                "forum": "Yd7idEYzNv",
                "replyto": "a9wFiBUb4t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3975/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3975/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the concern you have raised regarding the gradient derivation of our EGALA model and its applicability to more advanced GNN models such as GAT and GPRGNN. We fully recognize the importance of this issue. We have made some further studies.\n\n**[W1] Regarding the gradient derivation:**\nAlthough our proposed method only use SGC as the surrogate model, SGC is very useful for its simplicity, scalability and effectiveness.\nSGC is commonly used as surrogate model in graph adversarial attacks [1][2]. Also, the simplicity of SGC makes it even more applicable to large graphs. We have expanded our empirical evaluation, incorporating additional experiments to benchmark our EGALA model against adaptive PRBCD and GRBCD attacks. These new comparisons are detailed in the revised Appendix B.1. Using more advanced GNNs as surrogate models for attack does not necessarily improve the attack performance. Additionally, we examined the computational resources required conducting attacks on more advanced GNNs. Both time and memory costs rise significantly with the complexity of GNN models, constraining the scalability of adaptive attacks.\n\nIn the future, we will try to find ways to apply our model to white-box setting, comparing the approximated gradients with actual ones, and evaluating on other defense methods.\n\n**[Q1] Mini-batch training on large graphs:**\nWe adopt full-batch training on large graphs, which is the same as the baselines. Specifically, they chunk some operations (e.g. matrix multiplication) within the message passing step to successfully scale to larger graphs.\n\n**[Q2] Hyperparameter tuning:**\nTo further explore parameter sensitivity and complexity, we have conducted additional ablation studies, presented in Appendix C, to analyze the influence of clustering parameters on our model's performance, both the accuracy and computational cost. For GNN models, we follow the training settings of the original paper of PRBCD and GRBCD.\n\nWe appreciate the opportunity to address these critical aspects of our research and believe that the revisions have significantly strengthened the contribution of our work.\n\n[1] Li, Jintang, et al. \"Adversarial attack on large scale graph.\" IEEE Transactions on Knowledge and Data Engineering, 2021.\n\n[2] Z\u00fcgner, Daniel, et al. \"Adversarial attacks on neural networks for graph data.\" Proceedings of the 24th ACM SIGKDD, 2018."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3975/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609914162,
                "cdate": 1700609914162,
                "tmdate": 1700609914162,
                "mdate": 1700609914162,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y3LRwhxoaL",
                "forum": "Yd7idEYzNv",
                "replyto": "LZf8bZ2ewb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3975/Reviewer_BFci"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3975/Reviewer_BFci"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Thanks for the response. However, most of my major concerns are still valid (e.g., gradient derivation and limited experiments on scalable defense methods). Thus, I keep my score unchanged."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3975/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686537094,
                "cdate": 1700686537094,
                "tmdate": 1700686537094,
                "mdate": 1700686537094,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ecD5wDec4A",
            "forum": "Yd7idEYzNv",
            "replyto": "Yd7idEYzNv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3975/Reviewer_d4tk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3975/Reviewer_d4tk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a graph adversarial attack method, EGALA, which is efficient and can be applied to large-scale graphs. The core idea of EGALA is to reconfigure the computation of loss gradients across the entire adjacency matrix as the inner product of two N-by-d matrices. Then, EGALA utilizes clustering and Approximate Nearest Neighbor Search (ANNS) to efficiently identify the entries with the most significant gradients in the adjacency matrix without the need for exact gradient computation, thus significantly enhancing the model\u2019s scalability. The authors conduct comprehensive experiments across various datasets, demonstrating the effectiveness and transferability of EGALA."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed idea is technically sound and seems novel to me.\n\n2. The proposed method imposes minimal computational burden in terms of gradient calculations, making it highly efficient and memory-saving. It can be extended to larger graphs and avoids the instability associated with random block sampling.\n\n3. The proposed method is easy to implement."
                },
                "weaknesses": {
                    "value": "1. The proposed method uses SGC as the surrogate model and cannot be extended to other surrogate models. Additionally, in the experiments, the surrogate model used in the baseline is SGC, which may reduce the baseline's attack capabilities.\n\n2. The experiments in the paper are not comprehensive enough. Providing more ablation experiments would be beneficial\u2014for example, the impact of \u0394_t in the algorithm. I also want to know the performance comparison of the PDG topology attack [1] and EGALA on small datasets.\n\n3. The proposed method is applicable to attacks that only involve structural perturbations, limiting the method's applicability.\n\n[1] Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong, and Xue Lin. Topology attack and defense for graph neural networks: An optimization perspective. arXiv preprint"
                },
                "questions": {
                    "value": "Please see [Weaknesses] above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3975/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698717843615,
            "cdate": 1698717843615,
            "tmdate": 1699636358804,
            "mdate": 1699636358804,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fBvqaTQy9l",
                "forum": "Yd7idEYzNv",
                "replyto": "ecD5wDec4A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3975/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3975/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed and valuable feedback. Your input has led to substantive revisions and enhancements in our paper, and we are grateful for the opportunity to improve our work.\n\n**[W1] Regarding the extension to other surrogate models and the surrogate model of baselines:**\nFirstly, although our proposed method cannot be extended to other surrogate models, we demonstrate the importance of using SGC as our surrogate model for its simplicity, scalability and effectiveness.\nSGC is commonly used as surrogate model in graph adversarial attacks [1][2]. Also, the simplicity of SGC makes it even more applicable to large graphs.\nWe have expanded our empirical evaluation, incorporating additional experiments to benchmark our EGALA model against adaptive PRBCD and GRBCD attacks. These new comparisons are detailed in the revised Appendix B.1. Using more advanced GNNs as surrogate models for attack does not necessarily improve the attack performance.\nAdditionally, we examined the computational resources required conducting attacks on more advanced GNNs. Both time and memory costs rise significantly with the complexity of GNN models, constraining the scalability of adaptive attacks.\n\nSecondly, we carried out additional experiments to consider PRBCD and GRBCD utilizing their originally suggested surrogate model, Vanilla GCN. We present the results of this analysis in Appendix B.2.\n\n**[W2] Regarding the distribution of budgets:**\nIn our experiemnt, the $\\Delta_t$ we use is the same as that of GRBCD, where the budget is evenly distributed. This was done to ensure an equitable comparison of results. Furthermore, we acknowledge the potential for future work to explore more sophisticated budget distributions, as also suggested in the GRBCD original paper. Our current choice of $\\Delta_t$ serves as a starting point, and we are open to investigating more complex allocation strategies as part of our continued research efforts.\n\n**[W2] Regarding the comparison with PDG:**\nWe would like to clarify the rationale behind our baseline selection process. The PDG model has already been comprehensively compared with both PRBCD and GRBCD in their respective original publications. Consequently, we focused our comparative evaluation on the two more recent and advanced models: PRBCD and GRBCD. Nonetheless, we appreciate the importance of thorough baseline validation and are prepared to include PDG in our future evaluations to corroborate and extend the findings of the original papers.\n\n[1] Li, Jintang, et al. \"Adversarial attack on large scale graph.\" IEEE Transactions on Knowledge and Data Engineering, 2021.\n\n[2] Z\u00fcgner, Daniel, et al. \"Adversarial attacks on neural networks for graph data.\" Proceedings of the 24th ACM SIGKDD, 2018."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3975/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610056678,
                "cdate": 1700610056678,
                "tmdate": 1700610056678,
                "mdate": 1700610056678,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EBt9Y6ZV1z",
                "forum": "Yd7idEYzNv",
                "replyto": "fBvqaTQy9l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3975/Reviewer_d4tk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3975/Reviewer_d4tk"
                ],
                "content": {
                    "title": {
                        "value": "Official comments"
                    },
                    "comment": {
                        "value": "Thanks for your detailed response. The proposed method heavily depends on one model, weakening the contribution of this work. Thus, I am inclined to retain my score as it is."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3975/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731046022,
                "cdate": 1700731046022,
                "tmdate": 1700731046022,
                "mdate": 1700731046022,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ImFSvE7DRi",
            "forum": "Yd7idEYzNv",
            "replyto": "Yd7idEYzNv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3975/Reviewer_rxwz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3975/Reviewer_rxwz"
            ],
            "content": {
                "summary": {
                    "value": "The authors present EGALA, a method for constructing adversarial attacks on two-layer linear graph adversarial attacks at scale. EGALA approximates the gradient computation of the adjacency matrix as matrix product and efficiently identifies large entries in the gradients using Approximate Nearest Neighbor Search (ANNS), offering more scalable attacks with reduced memory and time consumption."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The derivation of the approximating gradient is convincing and elegant. The paper provides a solid theoretical analysis of how to derive the gradient of loss with respect to the adjacency matrix as a simple matrix product. \n\n2. The proposed EGALA improves the efficiency of naive attacks without sacrificing the attack capability in the evaluated transfer attack setting. The author approximates the gradient with a matrix product and leverages the acceleration techniques, ANNS algorithm, to further improve the efficiency. The motivation and design mechanism of this method is sound."
                },
                "weaknesses": {
                    "value": "1. EGALA is limited to attacking the surrogate model of two-layer linear GCN (essentially 2-layer SGC), and it can be only applied in the transfer attack setting when the victim model is not the same as the surrogate model. However, it has been shown in [1] that the transfer attack is much weaker than the adaptive attack, and the robustness evaluated under transfer attacks exhibits a strong false sense of security. This concern significantly weakens the contribution of this work.\n\n2. The major baseline PRBCD is a randomized block coordinate method. PRBCD is efficient by selecting a small block size. More importantly, it is a general attack algorithm that can be applied to potentially any GNN model, without being limited to two-layer linear GCNs. Overall, the advantages of EGALA over PRBCD and GRBCD are not convincing enough. First, the attack performances of EGALA, PRBCD, and GRBCD are comparable in the transfer attack, while it is expected that PRBCD and GRBCD will provide much stronger adaptive attacks, especially when the evaluated model is robust GNNs (although no such study is presented). Second, the time complexity depends on many hyperparameters such as block size and number of clusters. However, there is no discussion and ablation study on the hyperparameter setting of baselines such as PRBCD and GRBCD. Therefore, the reported time cost comparison is not convincing enough. \n\n3. There is a lack of time complexity comparison of EGALA and EGALA-N. It will be better to provide detailed analysis as well as corresponding ablation experimental results. In Table 3, the paper shows that EGALA and EGALA-N share the same time and memory cost,  which raises concerns about the advantage of the proposed clustering-based ANNS. Additionally, it is unclear why the cost of EGALA on PubMed is higher than the other baselines.\n\n4. Lack of comprehensive ablation studies on several components, e.g., clustering method, number of clusters and number of closest vector pairs, period of cluster update. These components or hyperparameters can influence the accuracy and computation cost. Ablation studies should be included to show the impact of each technical component.\n\n\n[1] Mujkanovic, Felix, et al. \"Are Defenses for Graph Neural Networks Robust?.\" Advances in Neural Information Processing Systems 35 (2022): 8954-8968."
                },
                "questions": {
                    "value": "Please refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3975/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779112419,
            "cdate": 1698779112419,
            "tmdate": 1699636358707,
            "mdate": 1699636358707,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rODTzIDShN",
                "forum": "Yd7idEYzNv",
                "replyto": "ImFSvE7DRi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3975/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3975/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough and insightful comments. Your constructive feedback has played a crucial role in the enhancement of our manuscript. We have undertaken a series of thoughtful modifications and additions in our work to address your points. Moreover, there are certain clarifications we would like to present regarding aspects of our research.\n\n**[W1 \\& W2] Regarding the adaptive attack:**\nWe expanded our empirical evaluation, incorporating additional experiments to benchmark our EGALA model against adaptive PRBCD and GRBCD attacks. These new comparisons are detailed in the revised Appendix B.1. Our results indicate that while adaptive attacks on the Vanilla GCN indeed demonstrate effectiveness, adaptive GRBCD does not consistently outperform its transfer attack, particularly within more complex GNN models like Vanilla GDC. The adaptive GRBCD fails to surpass its transfer counterpart in these contexts, suggesting that the advantage of adaptivity in greedy methods is non-universal and may not be decisive in all scenarios. Additionally, we examined the computational resources required for these adaptive attacks. We found that both time and memory costs rise significantly with the complexity of GNN models, constraining the scalability of adaptive attacks. This finding is important as it underscores the trade-offs between the intensity of attack methodologies and their practical deployability, especially with more elaborate GNN architectures.\n\n**[W2 \\& W3] Regarding the computational cost:**\nWe would like to apologize for any confusion caused by our initial presentation of the computational cost comparison. We have revised Table 3 in Section 4.2 to seperate the computational analysis of both EGALA and EGALA-N. The the increment of computational cost of EGALA on small datasets can be attributed to the ANNS methodology employed in EGALA. Exploring more efficient ANNS implementations holds the potential to notably diminish the overall computation cost. More importantly, for smaller datasets like Cora and Citeseer, the direct computation of matrix multiplication proves to be both rapid and incurs minimal memory cost. Conversely, for larger datasets like Arxiv and Products, the incorporation of the ANNS mechanism significantly mitigates computational costs, justifies its application.\n\nWhen comparing the computational cost of PRBCD and GRBCD, we used the recommended hyperparameter settings in the original papers to ensure fidelity in our comparative analysis. We also studied the influence of block size on the performance of the baseline. The results are shown in the following table.\n\n| Block size | $10^3$ | $10^4$ | $10^5$ | $10^6$ | $10^7$ |\n| --- | ---: | ---: | ---: | ---: | ---: |\n| Accuracy | 0.645 | 0.630 | 0.622 | 0.616 | 0.619 |\n| Time per step (s) | 0.014 | 0.014 | 0.015 | 0.051 | 0.195 |\n|Memory (MB) | 649 | 735 | 797 | 1101 | 2097 |\n\n\n**[W4] Ablation studies:**\nWe conducted ablation studies, presented in Appendix C, to analyze the influence of clustering parameters on our model's performance, comparing both the accuracy and computational cost simultaneously.\n\nYour feedback has been instrumental in propelling our research forward. We appreciate the opportunity to address these critical aspects of our research and believe that the revisions have significantly strengthened the contribution of our work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3975/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610652218,
                "cdate": 1700610652218,
                "tmdate": 1700726080178,
                "mdate": 1700726080178,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "H7EzXqTd7x",
                "forum": "Yd7idEYzNv",
                "replyto": "rODTzIDShN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3975/Reviewer_rxwz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3975/Reviewer_rxwz"
                ],
                "content": {
                    "title": {
                        "value": "Further concerns"
                    },
                    "comment": {
                        "value": "While I have not yet fully evaluated all of the points in your response, I would like to quickly express my unsolved concerns about the adaptive attack.\n\nI appreciate the additional experiments on the comparison of the transfer attack of your EGALA approach and the adaptive attack of PRBCD and GRBCD. However, these results are not fully convincing due to the following reasons.\n\n(1) The graph structure attack has been shown to transfer well when the surrogate model and victim model have similar architectures, such as similar graph convolutions. Therefore, it is not surprising that transfer attacks can still attack GCN and GDC in the provided experiments. \n\nHowever, the failure of transfer attacks and the false sense of security mainly happen when the victim model and surrogate model have intrinsic differences [1]. For instance, the surrogate model is vanilla GCN but the victim model is a robust GNN with new graph convolutions like SoftMedian [1] or ElasticGNN [2]. This is why I wonder how well the proposed transfer attack works for robust GNNs in my initial comment. In fact, there is no evaluation of how well the proposed algorithm attacks any robust GNNs.\n\n(2) It is unclear why the adaptive PRBCD and GRBCD can not consistently outperform the transfer attacks. For the chosen datasets like Cora and CiteSeer, it is feasible to select a large block size to cover the full-gradient case, which avoids any randomness or approximation error for gradient estimation. More studies and analyses on their differences need to be justified. \n\n[1] Are Defenses for Graph Neural Networks Robust? NeurIPS 2022\n\n[2] Elastic Graph Neural Networks, ICML 2021"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3975/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689065772,
                "cdate": 1700689065772,
                "tmdate": 1700689065772,
                "mdate": 1700689065772,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1tTaNrBeXR",
                "forum": "Yd7idEYzNv",
                "replyto": "eKrvoMBNMz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3975/Reviewer_rxwz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3975/Reviewer_rxwz"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for reminding me that some robust GNNs are included. Could you please provide some explanation or intuition as to why adaptive attacks can not outperform transfer attacks? It is quite unusual according to existing research and my experience. BTW, I will be happy to reevaluate my score after the reviewer discussions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3975/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695276639,
                "cdate": 1700695276639,
                "tmdate": 1700695276639,
                "mdate": 1700695276639,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sxzzfLgBcl",
            "forum": "Yd7idEYzNv",
            "replyto": "Yd7idEYzNv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3975/Reviewer_ca4y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3975/Reviewer_ca4y"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a novel strategy, termed EGALA, for performing an adversarial attack on graph neural networks w.r.t. the discrete graph structure. For this, the authors utilize an efficient approximate method for determining the elements with the largest gradient in the N x N adjacency matrix (where N is the number of nodes). The authors compare their method to the state-of-the-art attacks PRBCD and GRBCD."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. In contrast to the state-of-the-art PRBCD and GRBCD, the novel approach EGALA does not rely on randomly sampled candidate edges to achieve efficiency. Instead, EGALA relies on approximate nearest neighbor search (with randomization) to focus always on the important edges.\n1. EGALA is 3.5 times faster than PRBCD and 1.5 faster than GRBCD on the large products graph. The memory cost is about 30% smaller.\n1. In the presented experiments, EGALA quite consistently outperforms PRBCD and GRBCD in terms of attack strength, although, the differences are often small."
                },
                "weaknesses": {
                    "value": "1. The empirical evaluation is not exhaustive. E.g., the authors should evaluate also local attacks or visualize the approximation of the gradient (for small graphs). This would make the work more convincing in regard of general applicability as well as that the approximation is sensible. \n1. The authors only consider a grey box setting where the perturbations are transferred between models. This setting certainly has its merits. However, as pointed out previously, it is vital to assess neural networks with adaptive attacks [I, II] to get a proper estimate of the model's robustness. The authors should have prominently placed disclaimers and a comprehensive discussion on for what purpose the attack could be used.\n1. The attack is model specific and thus, it is not straightforward to make it \"adaptive\" for other GNNs than SGC.\n1. In connection to 2 & 3, the authors should craft experiments where they compare their transfer EGALA with an adaptive PRBCD and GRBCD. For example, the authors could attack defenses like Jaccard GCN, or SVG GCN (see [I]).\n1. The authors do neither test nor discuss local attacks on larger graphs like Papers100M (like PRBCD/GRPCB did).\n\nMinor:\n1. The authors could improve the references from Sec. 3.3. to eq. 11\n \n[I] - On Adaptive Attacks to Adversarial Example Defenses, Carlini et al., NeurIPS 2020\n[II] - Are Defenses for Graph Neural Networks Robust?, Mujkanovic et al., NeurIPS 2022"
                },
                "questions": {
                    "value": "1. What is the exact asymptotic complexity of the approach?\n1. How is the computational cost affected by the hyperparameters?\n1. Is it necessary to approximate the derivate d a_ij / d e_ij for scalability?\n\nI will raise the score if the questions and other points are addressed accordingly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3975/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3975/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3975/Reviewer_ca4y"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3975/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792763006,
            "cdate": 1698792763006,
            "tmdate": 1699636358625,
            "mdate": 1699636358625,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "r2yzw78sV9",
                "forum": "Yd7idEYzNv",
                "replyto": "sxzzfLgBcl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3975/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3975/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough and constructive feedback. Your insights have been invaluable in refining our paper. Based on your suggestions, we have implemented several modifications and enhancements to our work.\n\n**[W2, W3, W4] Regarding the adaptive attack:**\nWe have expanded our empirical evaluation, incorporating additional experiments to benchmark our EGALA model against adaptive PRBCD and GRBCD attacks. These new comparisons are detailed in the revised Appendix B.1. Our results indicate that while adaptive attacks on the Vanilla GCN indeed demonstrate effectiveness, adaptive GRBCD does not consistently outperform its transfer attack, particularly within more complex GNN models like Vanilla GDC. The adaptive GRBCD fails to surpass its transfer counterpart in these contexts, suggesting that the advantage of adaptivity in greedy methods is non-universal and may not be decisive in all scenarios. Additionally, we examined the computational resources required for these adaptive attacks. We found that both time and memory costs rise significantly with the complexity of GNN models, constraining the scalability of adaptive attacks. This finding is important as it underscores the trade-offs between the intensity of attack methodologies and their practical deployability, especially with more elaborate GNN architectures.\n\n**[Q2] Computational cost affected by hyperparameters:**\nTo further explore parameter sensitivity and complexity, we have conducted additional ablation studies, presented in Appendix C, to analyze the influence of clustering parameters on our model's performance, both the accuracy and computational cost. Also, it is noteworthy that the primary contributor to computational load in EGALA on small datasets is the Approximate Nearest Neighbor Search (ANNS). As such, pursuing more advanced ANNS methods is likely to reduce computational costs substantially.\n\n**[Q3] The approximation of $\\frac{\\partial a_{ij}}{\\partial e_{ij}}$:**\nThe approximation of the derivative $\\frac{\\partial a_{ij}}{\\partial e_{ij}}$ is necessary and indispensable. Empirically, the attack performance degrades if we ignore this part. Moreover, according to Equation (14) of our paper, $\\frac{\\partial a_{ij}}{\\partial e_{ij}}$ is inversely proportional to the squared root of node degrees. By attacking edges connected to lower-degree nodes, the model incurs a higher misclassification rate, which is consistent with recent findings in [1]. The attack performance without the approximation of $\\frac{\\partial a_{ij}}{\\partial e_{ij}}$ is shown in the following table.\n\n| Model | SGC | GCN | GDC | PPRGo | S-GDC | S-PPRGo\n|---|---:|---:|---:|---:|---:|---:|\n| EGALA | 0.599 | 0.615 | 0.655 | 0.695 | 0.725 | 0.766 |\n| No $\\frac{\\partial a_{ij}}{\\partial e_{ij}}$ | 0.780 | 0.788 | 0.824 | 0.798 | 0.824 | 0.797 |\n\nIn the future, we will conduct more experiments to evaluate on local attacks, visualizing the approximated gradients, finding ways to apply our model to white-box setting, and evaluating on other defense methods. Your comments have been instrumental in improving our study and the clarity of our findings. We appreciate the opportunity to address these critical aspects of our research and believe that the revisions have significantly strengthened the contribution of our work.\n\n[1] Li, Kuan, et al. \"Revisiting graph adversarial attack and defense from a data distribution perspective.\" The Eleventh International Conference on Learning Representations, 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3975/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610937794,
                "cdate": 1700610937794,
                "tmdate": 1700726038016,
                "mdate": 1700726038016,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wAMk9DvQh6",
                "forum": "Yd7idEYzNv",
                "replyto": "r2yzw78sV9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3975/Reviewer_ca4y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3975/Reviewer_ca4y"
                ],
                "content": {
                    "title": {
                        "value": "Follow up"
                    },
                    "comment": {
                        "value": "I thank the authors for addressing the posed weaknesses and questions of the presented work.\n\nIt is interesting that GRBCD performs worse in the adaptive setup. Perhaps a more extensive hyperparameter search is required for it to perform on par or better. Nevertheless, the adaptive PRBCD seems to be substantially stronger in the adaptive case. Having said this, it appears to be a misleading presentation to put the adaptive results in the appendix and they should be incorporated into the main part (e.g., Table 2).\n\nAgain, I absolutely agree that the grey box setting has its merits (e.g. computational complexity). Nevertheless, the authors should make clear that EGALA is not a substitute for adaptive/whitebox attacks. I stand to my opinion that \n> The authors should have prominently placed disclaimers and a comprehensive discussion on for what purpose the attack could be used.\n\nPerhaps also elaborating on the trade-offs mentioned in the author's rebuttal.\n\nRegarding d a_ij / d e_ij.  It would be helpful to place this \"assumption\" more prominently in section 3. And include the discussion of the rebuttal.\n\nIn summary, I think there are merits to this work and the availability of multiple adversarial attacks is vital for an adaptive evaluation of a method. Moreover, for this, it would be beneficial to highlight the differences in the found solutions / perturbed adjacency matrices in comparison to P/GRBCD.\n\nSince the requested discussion of grey box vs. adaptive attacks is not reflected in the updated paper and the results using adaptive attacks are deferred to the appendix, I am inclined to retain my score as it is."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3975/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729940112,
                "cdate": 1700729940112,
                "tmdate": 1700729940112,
                "mdate": 1700729940112,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]