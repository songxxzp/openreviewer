[
    {
        "title": "AN ENTROPY PERSPECTIVE IN KNOWLEDGE DISTILLATION"
    },
    {
        "review": {
            "id": "qDYi0qW0ug",
            "forum": "QAq5JTFJmp",
            "replyto": "QAq5JTFJmp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9314/Reviewer_kj6Z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9314/Reviewer_kj6Z"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzed reasons of performance degradation of distilling knowledge from an overly large teacher model to the student, and proposed to narrow entropy gap and free energy gap between the student model and the teacher model.  Through theoretical analysis, the paper proposed an alignment  module to narrow these gaps, thus enhancing the student performance.  Experiment results validated the effectiveness of the proposed alignment module."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper provided theoretical analysis. And, these mathematical proof is easy to understand\n2. The author's writing is very good, and the entire paper is relatively easy to understand"
                },
                "weaknesses": {
                    "value": "1. The contribution of this method is not very important. In section 4, the authors summarized that \"Different from Xu et al. [35] that normalizes the features in the penultimate layer of the network to perform distillation, our methods perform normalization on the logits layer.\".  Although the paper is different from Xu et al. [35], it is not difficult to derive this method from Xu et al. [35].\n2. The paper proposed two metrics (entropy gap and free energy gap) to evaluate the gap of the student and the teacher. They are very close as shown in Eq. 9 and Eq.13, so they cannot provide more information from different perspectives.\n3. Some important works are missing and not to compare, e.g. Zhao, Borui, Quan Cui, Renjie Song, Yiyu Qiu and Jiajun Liang. \u201cDecoupled Knowledge Distillation. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022): 11943-11952.   \n4. || * || is not exactly indicated, it is better to add explanations, e.g. L1 or L2, or others"
                },
                "questions": {
                    "value": "1. When proving equation 8, the Taylor expansion is a first-order expansion, and when proving equation 12, the Taylor expansion is a second-order expansion. Why use different formulas?\n2. There are no experiments showing the combination of SKD with other KD methods. SKD adds an alignment module, so it can be directly combined with other KD methods. Can it improve the performance of other KD methods?\n3. In section 3, there are duplicate descriptions \", e.g. Learning rate is dropped at 30, 60, 80 and 90 epoch.\" and \"learning rate dropped by 0.1 in 30, 60, 80, 90 epoch\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9314/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9314/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9314/Reviewer_kj6Z"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9314/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697766772924,
            "cdate": 1697766772924,
            "tmdate": 1699637172928,
            "mdate": 1699637172928,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "jfBNvb4EpK",
            "forum": "QAq5JTFJmp",
            "replyto": "QAq5JTFJmp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9314/Reviewer_YbW2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9314/Reviewer_YbW2"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates knowledge from the entropy perspective and proposes a knowledge distillation method by first normalizing the logits and then minimizing the KL-divergence. In this way, the entropy gap can be reduced. Experiments on CIFAR-100 and ImageNet are conducted and show promising results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is easy to follow. \n2. The proposed method is easy to implement.\n3. Comparison with different methods are reported."
                },
                "weaknesses": {
                    "value": "1. The improvement is minor. As shown in Table 4, there is almost no improvements over CRD (ICLR'2020).\n2. The proposed method of normalizing the logits is not a big contribution, since normalizing the logits is widely used in metric learning and few-shot learning literature."
                },
                "questions": {
                    "value": "1. First, this paper defines entropy gap, which I think is reasonable. Then, this paper simply assumes a small entropy gap leads to a better performance and proposes a method to minimize the entropy gap. However, there is no theory or comprehensive experiments supporting this assumption. When this assumption works, and for what kinds of teacher and student networks?\n2. CRD was proposed in 2020. Recent SOTA approaches are not compared."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9314/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698940458206,
            "cdate": 1698940458206,
            "tmdate": 1699637172795,
            "mdate": 1699637172795,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "yhCQRBnjzB",
            "forum": "QAq5JTFJmp",
            "replyto": "QAq5JTFJmp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9314/Reviewer_dqtM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9314/Reviewer_dqtM"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the performance degradation of student models when the teacher model is significantly large. The authors provide theoretical results on the discrepancy of student and teacher models, in terms of entropy rates & free energy. The authors also suggested spherical knowledge distillation, which is shown to perform well on multiple benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The authors tested their scheme for multiple settings. \n* The empirical results are promising for some cases."
                },
                "weaknesses": {
                    "value": "* The theoretical results are quite easy to derive. From the definition, entropy and free energy is bit related, so figuring out the relationship in the distillation setup does not seem to be a critical contribution.\n* It is unclear why simple normalization of logics (as in eq.15) is motivated by the theoretical results. The authors say \u201cIt can be seen that ||v||^2 - ||z||^2 plays a key role and can be removed by normalization techniques\u201d, but why are we removing the important term having key role? \n* I personally believe current theoretical explanation is not enough to justify why spherical KD is working much better than vanilla KD."
                },
                "questions": {
                    "value": "* It\u2019s a minor issue, but the title in the manuscript is different from the title in the console."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9314/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699017284910,
            "cdate": 1699017284910,
            "tmdate": 1699637172660,
            "mdate": 1699637172660,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]