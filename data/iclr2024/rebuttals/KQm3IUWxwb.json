[
    {
        "title": "Disentangled Heterogeneous Collaborative Filtering"
    },
    {
        "review": {
            "id": "cAxTPgRzSr",
            "forum": "KQm3IUWxwb",
            "replyto": "KQm3IUWxwb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8872/Reviewer_KKy5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8872/Reviewer_KKy5"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors focus on combining intent disentanglement and multi-behavior modeling for collaborative filtering. The proposed method -- DHCF utilizes parameterized heterogeneous hypergraph to encode intents embeddings, and introduces behavior-wise contrastive learning to improve model robustness. Offline experiments are conducted on public datasets to demonstrate the performance of DHCF on Top-k item recommendation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper provides insight on utilizing multi-behavior data in recommendation systems.\n2. The proposed method outperforms the baseline methods on HR and NDCG for top-10 recommendation. \n3. Ablation analysis is included."
                },
                "weaknesses": {
                    "value": "1. The model consists of multiple components, which are difficult to optimize and converge. It is hard to be applied in the real-world case.\n2. The robustness analysis is not convincing. To demonstrate the model's robustness, we expect it to achieve stable performance on difficult tasks, where baseline methods perform poorly compared to easy tasks. However, in Figure 3, the baseline methods achieve similar performance on different user groups. This comparison is not convincing evidence of the method's robustness."
                },
                "questions": {
                    "value": "1. It is strange that the basic methods like NCF achieve similar performance on different user groups (in Figure 3). Instead of evaluating on tailed items, why did the authors test the performance on different user groups for robustness analysis?\n2. Is the meta-learning process considered while calculating the complexity in section 3.7?\n3. Any evidence to support the convergence of the learnable hypergraphs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8872/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698446196722,
            "cdate": 1698446196722,
            "tmdate": 1699637116696,
            "mdate": 1699637116696,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "URZ5HCxIp7",
                "forum": "KQm3IUWxwb",
                "replyto": "cAxTPgRzSr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8872/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8872/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Responses to Reviewer KKy5\n\nThank you for your dedicated efforts and valuable feedback. We would like to address your concerns as follows:\n\n1. The practicality of the proposed DHCF model (W1)\n\n- Model Efficiency. In our efficiency study, as presented in Section 4.6 and Table 2, we demonstrate that the per-epoch training and testing time of DHCF is comparable to existing baseline models. This indicates that DHCF is applicable in real-world scenarios and is efficient in terms of computational resources.\n\n- Model Convergence. Regarding the convergence of the model, our DHCF follows the same maximum training epochs as other baseline methods, which is set to 100 epochs. This suggests that there are no disadvantages in terms of convergence speed for DHCF compared to other models.\n\n2. Further clarification of the NCF's results on different user groups (in Figure 3).  (W2 & Q1)\n    \n    Thank you for your insightful feedback. We appreciate your comment and would like to address it with the following points:\n - **Increment of supervision signals does not always improve performance**. The observation that basic methods like NCF achieve similar performance on different user groups, as shown in Figure 3, is consistent with findings from previous works on multi-behavior recommendation, such as [1-2]. This phenomenon highlights that simply increasing supervision signals does not necessarily lead to improved model performance. The lack of high-quality self-supervision signals can limit the effectiveness of increasing supervision, and this issue can also affect self-supervised learning methods based on random augmentations.\n - **Consistent performance gain of DHCF**. In contrast to existing supervision-enhancing methods like SGL, our DHCF achieves performance improvement across different data sparsity levels through intent disentanglement and modeling of heterogeneous relation dependencies. This design effectively enhances the overall model performance. The consistent performance superiority of DHCF over the baseline models further validates the effectiveness of these designs.\n\n    [1] \"Knowledge Enhancement for Contrastive Multi-Behavior Recommendation\"\n\n    [2] \"Graph Meta Network for Multi-Behavior Recommendation\"\n\n3. Performance evaluation on tailed items (Q1)\n\n- Thank you for your suggestion. We appreciate your interest in evaluating our model's performance on tailed items. In our experiment, we specifically considered a user set composed of long-tailed users to address the issue of sparsity. These users have a maximum of 35 interactions across different types, with an average of only 3.81 purchase interactions. This indicates that these users have a significantly low number of interactions. While we did not explicitly mention \"tailed items\" in our paper, our analysis does encompass the evaluation of our model's performance on this particular long-tailed user set.\n\n4. Is the meta-learning process considered while calculating the complexity in section 3.7? (Q2)\n- We appreciate your thoughtful question. Although we did not explicitly mention the complexity of the meta network in our paper for the sake of simplicity, it involves a straightforward linear transformation. This transformation incurs a cost of $O((I+J)\\times d^2)$ for each of the $K$ heterogeneous relations. Empirically, we have found that this complexity is close to $O(|\\mathcal{X}|\\times d)$, which is smaller than the complexity of the graph relation learning process. The graph relation learning process has a cost of $O(|\\mathcal{X}|\\times d\\times L)$, where $|\\mathcal{X}|$ represents the size of the dataset and $L$ denotes the number of graph convolutional layers.\n\n5. Discussion of model convergence with the learnable hypergraphs? (Q3)\n- Thank you for your question. During the training process of DHCF, we have noticed that the performance on the validation set remains stable in the last few epochs. This consistent behavior can be replicated using the code we have released, providing strong evidence for the convergence of the learnable hypergraphs."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494886356,
                "cdate": 1700494886356,
                "tmdate": 1700494886356,
                "mdate": 1700494886356,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l6Kh3kWx9H",
                "forum": "KQm3IUWxwb",
                "replyto": "URZ5HCxIp7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8872/Reviewer_KKy5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8872/Reviewer_KKy5"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the responses"
                    },
                    "comment": {
                        "value": "After carefully reading the responses and the comments from other reviewers, I would keep my score at this point."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692827761,
                "cdate": 1700692827761,
                "tmdate": 1700692827761,
                "mdate": 1700692827761,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lpBDSezj9b",
            "forum": "KQm3IUWxwb",
            "replyto": "KQm3IUWxwb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8872/Reviewer_7WGw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8872/Reviewer_7WGw"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of modern recommender systems that often utilize low-dimensional latent representations to embed users and items based on their observed interactions. However, many existing recommendation models are primarily designed for coarse-grained and homogeneous interactions, which limits their effectiveness in two key dimensions: i) They fail to exploit the relational dependencies across different types of user behaviors, such as page views, add-to-favorites, and purchases; and ii) They fail to disentangle the latent intent factors behind each behavior, which leads to suboptimal recommendations.\n\nThe authors argue that these limitations can be addressed by a novel recommendation model called Disentangled Heterogeneous Collaborative Filtering (DHCF). DHCF effectively disentangles users' multi-behavior interaction patterns and the latent intent factors behind each behavior. The authors propose a parameterized heterogeneous hypergraph architecture that captures the complex and diverse interactions among users, items, and behaviors. They also introduce a novel contrastive learning paradigm that improves the model's robustness against data sparsity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors' approach is based on a hypergraph structure that allows for the modeling of multiple types of interactions among users and items. The hypergraph structure is parameterized, which means that the model can learn the weights of the hyperedges that connect users and items based on their interactions.\n\n2. Contrastive learning is a technique that learns representations by contrasting positive and negative examples. In the context of DHCF, the authors use contrastive learning to learn representations of users and items that are optimized for predicting the interactions between them. By using contrastive learning, the authors are able to learn more robust representations that are less sensitive to data sparsity.\n\n3. The authors' experiments show that DHCF significantly outperforms various strong baselines on three public datasets, which further supports the effectiveness of their approach."
                },
                "weaknesses": {
                    "value": "1. The proposed Dynamic Hypergraph Collaborative Filtering (DHCF) approach presents a unique take on recommendation systems; however, its distinctiveness and advancements over existing methodologies in the literature are not sufficiently highlighted. To strengthen the paper, the authors should conduct a more comprehensive comparison of DHCF with prevailing models, pinpointing exact areas of improvement and innovation. Integrating and discussing the influence of more contemporary trends in recommendation systems, such as applications of deep learning or graph neural networks, would further enrich the paper's relevance and depth.\n\n2. The paper currently lacks clarity and detail regarding the algorithms and techniques underpinning the DHCF approach. To remedy this, a more explicit elucidation of the methodology is required. Additionally, incorporating visual aids or concrete examples could help in visualizing the hypergraph structure and elucidating the concept of behavior-wise contrastive learning, making the paper more accessible and informative.\n\n3. A more thorough examination of DHCF would contribute to a balanced and comprehensive understanding of the approach. Specific areas such as the scalability of DHCF to larger datasets and its sensitivity to hyperparameter choices warrant detailed discussion."
                },
                "questions": {
                    "value": "1. In the paper, you mention that many existing recommendation models fail to exploit the relational dependencies across different types of user behaviors. Could you provide more details on how DHCF addresses this limitation? How does the parameterized heterogeneous hypergraph architecture capture the complex and diverse interactions among users, items, and behaviors?\n\n2. In the paper, you also mention that many existing recommendation models fail to disentangle the latent intent factors behind each behavior. Could you provide more details on how DHCF disentangles the latent intent factors behind each behavior? How does the behavior-wise contrastive learning paradigm facilitate adaptive data augmentation at both the node and graph levels?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8872/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698713578822,
            "cdate": 1698713578822,
            "tmdate": 1699637116566,
            "mdate": 1699637116566,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ygHJL7W6Un",
                "forum": "KQm3IUWxwb",
                "replyto": "lpBDSezj9b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8872/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8872/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Responses to Reviewer 7WGw\n\nThank you for your kind feedback. We appreciate your concerns, and we would like to address them as follows:\n\n1. Further clarifications of difference between the new model over existing methods. (W1)\n\n- We appreciate your feedback. In our work, we address two important challenges that have been overlooked in the field of collaborative filtering: interaction heterogeneity and fine-grained modeling of user intents. To bridge these gaps, we propose DHCF, a method that integrates intent disentanglement and multi-behavior modeling using a parameterized heterogeneous hypergraph architecture. Furthermore, we introduce a novel approach to heterogeneous contrastive learning, leveraging the encoded disentangled heterogeneous interaction patterns.\n\n2. Further discussion of some details in the methodology part. (W2)\n\n- Thank you for your feedback. To better address your concern, we kindly request specific feedback on the parts that require further clarification. This will enable us to identify and improve those specific areas more effectively. Please let us know if there are any specific questions or sections where you would like us to provide more details.\n\n3. Clarifications about the already conducted scalability and hyperparameter studies. (W3)\n\n- We appreciate your thoughtful suggestion. In our work, we have thoroughly investigated the influence of hyperparameter settings in Section 4.5 and examined model scalability in Section 4.6. Additionally, we have conducted comprehensive experiments covering various aspects. Specifically, we have performed a rigorous performance comparison with 18 diverse baselines, explored the impact of hyperparameter settings, conducted module ablation studies, assessed model scalability, evaluated robustness against data sparsity, and presented a detailed case study on the learned hypergraph structures.\n\n4. How does DHCF exploit the relational dependencies across different types of user behaviors? (Q1)\n\n- Thank you for providing the feedback. In DHCF, we integrate cross-type contrastive learning tasks at both the node level and the graph level to leverage the relational dependencies across different types of user behaviors. At the node level, our contrastive learning approach aligns the learned node representations across diverse heterogeneous interaction types, facilitating the identification of shared characteristics across different behavior modes. Additionally, at the graph level, our contrastive learning aligns the representations for the entire graph, taking into account the cross-type commonalities in global graph structures.\n\n5. How does the parameterized heterogeneous hypergraph architecture capture the complex and diverse interactions among users, items, and behaviors? (Q1)\n\n- Our parameterized hypergraph networks capture user-wise and item-wise relations from a global perspective by utilizing hypergraph-based global relation learning for each behavior type. This approach enables us to effectively model the heterogeneous interactions and fuse multi-type representations, thereby facilitating the modeling of relational dependencies between different entities.\n\n6. How does DHCF disentangle the latent intent factors behind each behavior? (Q2)\n\n- DHCF leverages a parameterized hypergraph neural network to disentangle the latent intent factors that underlie each user behavior. In this architecture, the hyperedges in the hypergraph represent the latent user intents. Through iterative hypergraph structure learning, DHCF establishes connections between users/items and their associated hyperedges, effectively capturing users' frequent intents. This disentanglement process enables DHCF to model and differentiate the underlying intent factors that drive different user behaviors.\n\n7. How does the behavior-wise contrastive learning paradigm facilitate adaptive data augmentation at both the node and graph levels? (Q2)\n\n- In DHCF, the adaptive data augmentation primarily targets the node level and involves personalized transformations tailored to different users and behavior types. Specifically, DHCF incorporates user-specific embedding transformations for their type-specific embeddings, which enables accommodating user-specific variations in behavior types. This approach allows DHCF to adaptively augment the data at a personalized level, enhancing the modeling of user behaviors with consideration for individual differences."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494911484,
                "cdate": 1700494911484,
                "tmdate": 1700494911484,
                "mdate": 1700494911484,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EP2bsLgXqD",
            "forum": "KQm3IUWxwb",
            "replyto": "KQm3IUWxwb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8872/Reviewer_TWFu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8872/Reviewer_TWFu"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a Disentangled Heterogeneous Collaborative Filtering (DHCF) for a recommendation system. Specifically, the model integrates a parameterized heterogeneous hypergraph network with a hierarchical contrastive learning paradigm, to capture the latent intent factors and the multi-behavior dependencies in an adaptive and self-supervised manner."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The task of recommendation with heterogeneous interactions is interesting and valuable. \n2. The paper is written well and is easy to understand.\n3. Extensive experiments have been conducted to validate the proposed model."
                },
                "weaknesses": {
                    "value": "1. The methods proposed in the paper lack innovation significantly. Sections 3.1-3.5 follow very common design paradigms, and their method designs exhibit a certain degree of similarity to HCCF, ICL, and others. The paper should discuss the differences in the technical details between them.\n2.  In section 3.6, there are two loss functions proposed for relationship learning but in reality, they belong to the same paradigm. The paper lacks sufficient theoretical justification for their validity.\n3.  The font size in Figure 1 is too small."
                },
                "questions": {
                    "value": "My major concern lies in the technical details. Many of the described methods bear a resemblance to existing approaches. It is crucial to clearly explain the distinctions and improvements made by DHCF in comparison to these existing methods. Additionally, please provide a more detailed explanation of the motivations behind these improvements."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8872/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833310404,
            "cdate": 1698833310404,
            "tmdate": 1699637116447,
            "mdate": 1699637116447,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TY10KZ9CRv",
                "forum": "KQm3IUWxwb",
                "replyto": "EP2bsLgXqD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8872/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8872/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Responses to Reviewer TWFu\n\nThank you for your valuable feedback. We would like to address your comments as follows:\n\n1. Further discussion about the our encoder novlty. (W1 & Q)\n\nOur methodology comprises two primary components: the encoding part, which utilizes a heterogeneous hypergraph neural network, and the self-supervised learning part, employing heterogeneous contrastive learning. In order to address these concerns and highlight the technical novelty of our DHCF model, we provide the following justifications:\n\n- **Uniqueness of DHCF's Encoding Part**. While our DHCF model and existing works such as HCCF share some commonalities (e.g., the use of hypergraph neural networks), there are two significant technical differences in the design of our encoder: **heterogeneity** and **disentanglement**. Unlike existing hypergraph-based methods like HCCF, which primarily focus on homogeneous data, our DHCF model addresses the challenge of heterogeneity by considering interaction types and effectively preserving heterogeneity information in the hypergraph-based encoder. Furthermore, the adoption of a parameterized hypergraph neural network in our DHCF model facilitates efficient and learnable intent disentanglement. This disentanglement is vital not only during the encoding process but also in the subsequent self-supervised learning part, as discussed below. These technical distinctions highlight the unique contributions of our DHCF model, enabling it to handle heterogeneity and disentangle user intents more effectively compared to existing approaches.\n\n- **Novelty of Our Contrastive Learning and Its Relation to Encoder**. Our paper introduces a significant technical contribution in the form of the proposed disentangled heterogeneous contrastive learning method. This method incorporates several key components, including a parameter personalization module, a node-level heterogeneous self-discrimination task, and a graph-level contrastive learning task. These components leverage hypergraph-based global representation and shuffling-based negative sample construction techniques. By employing these designed techniques, our method greatly enhances the training of models for heterogeneous collaborative filtering. It achieves this by facilitating adaptive supervision enhancement while considering global intent disentanglement. Notably, our approach outperforms vanilla graph contrastive learning methods such as SGL, primarily due to the valuable heterogeneity and disentanglement information extracted by our designed encoding part.\n\n\n- **Theoretical Contribution**. Our paper makes a contribution by providing an in-depth theoretical analysis that highlights two important advantages of DHCF that distinguish it from existing works: i) hypergraph-based disentanglement enhances the adaptability of contrastive supervision signals, ii) graph-level contrastive learning injects global disentangled representations into node-level similarities. Thank you for your insightful feedback. We will further highlight the novelty and uniqueness of our encoding component in future versions.\n\n2. Further theoretical justification of SSL loss terms (W2)\n\n- The two SSL loss functions employed in our study follow the contrastive learning paradigm. However, they differ in their specific focuses and objectives. The first SSL loss function concentrates on micro-scale learning, targeting the individual nodes within the graph. It enables fine-grained learning at the node level, capturing local heterogenous collaborative relationships among users and items. The second SSL loss function places emphasis on macro-scale learning, aiming to maximize learning for the overall graph structure. This macro-scale approach ensures that the model captures the global context and structural information within the multi-behavior data. Furthermore, since natural negative samples are lacking at the graph level, we introduce a shuffling-based negative sampling generation method, which effectively enlarges the technical similarity between the two loss functions.\n\n3. Font size of Fig.1 is too small. (W3)\n- Thank you for your thoughtful suggestion. We will improve the clarity by updating the figure in later versions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494939762,
                "cdate": 1700494939762,
                "tmdate": 1700494939762,
                "mdate": 1700494939762,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NkEWpNcIpc",
                "forum": "KQm3IUWxwb",
                "replyto": "TY10KZ9CRv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8872/Reviewer_TWFu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8872/Reviewer_TWFu"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the responses"
                    },
                    "comment": {
                        "value": "Thank you for the responses, which have addressed my concerns to some extent. After carefully considering the comments from other reviewers, I have decided to withhold my score at this time."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574330282,
                "cdate": 1700574330282,
                "tmdate": 1700574330282,
                "mdate": 1700574330282,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]