[
    {
        "title": "CNNGEN: A GENERATOR AND BENCHMARK FOR SUSTAINABLE CONVOLUTIONAL NEURAL NETWORK SEARCH"
    },
    {
        "review": {
            "id": "OxL65SwO1B",
            "forum": "HgndgAbBcR",
            "replyto": "HgndgAbBcR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3267/Reviewer_pJLK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3267/Reviewer_pJLK"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a new benchmark consisting of various performed architectures automatically generated from domain-specific language (DSL) and the performance, training code, and energy consumption information of each architecture."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This work presents a new benchmark and an image-based performance predictor."
                },
                "weaknesses": {
                    "value": "1. Why the performance of networks should be diverse? Note the goal of NAS is to find architecture better than human crafted ones, while human designed architecture perform quite well in nearly all cases. The importance of NAS to distinguish models with similar high performance. For instance, NAS is designed to distinguish ConvNeXt and ResNeXt, rather than ConvNeXt and LeNet. The previous benchmark that has most architectures with very close and high accuracy actually make more sense than the proposed ones.\n\n2. DSL has limitation in the expressivity of the architecture. The proposed DSL seems only supports sequential architecture without branches ( Fig 3 and Fig. 8). \n\n3. Intuition of predictor. Why do you think a CNN can know the network's performance by looking at the image of its architecture? What is the intuition behind it? For me, it just overfits a small dataset."
                },
                "questions": {
                    "value": "1. Why do we need an accurate number of energy consumption? Why not just model parameters and FLOPs? Energy consumption is very sensitive to the setup of machines and can be easily outdated. A model that consumes many energy might be very energe-saving in the next year due to the new software support and hardware update."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3267/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788084988,
            "cdate": 1698788084988,
            "tmdate": 1699636275128,
            "mdate": 1699636275128,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FmMq0Sh4Ya",
                "forum": "HgndgAbBcR",
                "replyto": "OxL65SwO1B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3267/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pJLK"
                    },
                    "comment": {
                        "value": "Thank you for your review.  \n\nRegarding weakness 1, we refer to our comment to reviewer (vffX): we want to evaluate and design predictors in a general way: these predictors should also be able to rule out bad architectures, which is not the case for the NAS benchmarks which all the topologies have similar good performance.  We agree intending to design (in the long term) better architectures than human-crafted ones.  Our motivation is that it extends to the whole topology and not only to parts (cells) of a given topology (i.e., architecture like LeNet). Therefore, our approach allows us to explore the CNNs\u2019 search space in the large rather than in the small for a predefined topology.\n\n\nRegarding weakness 2, this is not correct. Our grammar can generate architectures with an infinity of branches in theory. In practice,  we constrain the topological space with at most 2 branches per node for a given network. It is true that we only showed linear ones in the original paper for the sake of simplicity, our mistake.  We have added in Appendix A a few more complex architectures generated in our benchmark. You can also see in the Zenodo in the folder output/architecure_img 1300 architectures that our generator synthesizes.\n\nRegarding weakness 3, the intuition behind using CNN to predict the network performance is that the image contains the topology and hyperparameters and this information is necessary to predict performance. \n\nWe trained each predictor 10 times (see Section 6.1) and reported an average of the performance (cf code in Zenodo). There is also a CSV file that contains the results of each run. \n\nRegarding question 1, it is common to report energy consumption as it characterizes the environmental impact of these models. We did not see these concerns widely covered in existing benchmarks, which motivated providing a new one (in addition to network diversity). We do agree that it varies depending on hardware and software optimization. We rely on the code carbon library, which reports operations as well and could help design novel predictors in the future."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240946120,
                "cdate": 1700240946120,
                "tmdate": 1700240946120,
                "mdate": 1700240946120,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "19mLPf2YbZ",
            "forum": "HgndgAbBcR",
            "replyto": "HgndgAbBcR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3267/Reviewer_vffX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3267/Reviewer_vffX"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new way to generate a NAS search space. The authors dubbed their method CNNGen. This new methods relies on domain-specific language (DSL) to capture neural architectures using a dedicated grammar. The authors claim this methods results in more diverse architectures to search. The authors also present a method to compare their work against other search spaces and claim their method outperforms state-of-the-art. The authors also propose adding a carbon footprint metric to the models."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I highlight the following strengths:\n- The idea of adding DSL to generate network candidates is interesting. Using grammar to represent networks is a good idea and it has the potential to represent more diversity of solutions as the authors point out in their results\n- Representing an a network candidate as an image is also interesting and it allows for metrics that are more aligned with computer vision tools. The authors demonstrate the use of these metrics in the paper"
                },
                "weaknesses": {
                    "value": "I see 2 critical flaws with this work:\n- The benchmark's are insufficient. The authors only compare the data with the one presented in [1]. This comparison is limited to CIFAR data only. Even in [1] the authors present a comparison using ImageNet data. Additionally, the authors present a benchmark but don't really benchmark any NAS methods. To show good power of the benchmark the authors need to present results showing that NAS methods can benefit from this benchmark and find architectures with better accuracy and even better carbon footprint (since the authors present this as a metric). As it stands I don't see sufficient evidence of novelty or impact for this work. The authors should run a real benchmark of methods and present more extensive results.\n- The authors completely ignore the advancements in differentiable NAS ([2,3,4,5]). In these works there is no need to use a search space and one can optimize an architecture using gradient decent. In fact, some works like [3] and [5] show how to do this and also add constraints for latency and power consumption. Given the existence of these methods, I don't see the value of creating a new search space benchmark. I ask the authors to clarify the lack of mention and comparison here and clarify any misunderstanding from my side.\n\nGiven these flaws, I don't yet see merit in the work presented, the authors need to provide a more extensive benchmark and compare against more efficient methods like [2,3,4,5].\n\n\n[1] Wen, Wei, et al. \"Neural predictor for neural architecture search.\" European Conference on computer vision. Cham: Springer International Publishing, 2020.\n\n[2] Liu, Hanxiao, Karen Simonyan, and Yiming Yang. \"DARTS: Differentiable Architecture Search.\" International Conference on Learning Representations. 2018.\n\n[3] Wu, Bichen, et al. \"Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.\n\n[4] Li, Guohao, et al. \"Sgas: Sequential greedy architecture search.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\n[5] Li, Guohao, et al. \"LC-NAS: Latency constrained neural architecture search for point cloud networks.\" 2022 International Conference on 3D Vision (3DV). IEEE, 2022."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3267/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699096275244,
            "cdate": 1699096275244,
            "tmdate": 1699636275035,
            "mdate": 1699636275035,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kZHNasbWJk",
                "forum": "HgndgAbBcR",
                "replyto": "19mLPf2YbZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3267/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vffX"
                    },
                    "comment": {
                        "value": "Thank you for your review and the references, we indeed used the neural predictor [1] as a reference to compare the performance predictors we designed.\n\nThe motivation to use our generator to create a benchmark is to allow to design more generalizable predictors (which should stem from the diversity of networks created) and also to allow for predictors that focus on or integrate CNNs\u2019 energy consumption. To the best of our knowledge, we did not see this information in existing NAS benchmarks."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240899989,
                "cdate": 1700240899989,
                "tmdate": 1700240899989,
                "mdate": 1700240899989,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s23fyNs5Vh",
            "forum": "HgndgAbBcR",
            "replyto": "HgndgAbBcR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3267/Reviewer_WMeX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3267/Reviewer_WMeX"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new approach to Neural Architecture Search (NAS) that uses a domain-specific language (DSL) to generate convolutional neural networks without predefined cells or base skeletons named CNNGen.\nThis approach offers a more diverse and sustainable solution to NAS, addressing the limitations of cell-based methods.\nThe paper outlines the comprehensive pipeline used by CNNGen to store network descriptions and fully trained models, and discusses the growing concern for sustainability in neural network design and implementation. Overall, CNNGen is an innovative and promising tool for generating and benchmarking sustainable convolutional neural networks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper introduces a new approach to use a domain-specific (natural) language (DSL) to generate convolutional neural networks, that allows the exploration of diverse and potentially unknown topologies.\n- The paper discusses the growing concern for sustainability in neural network design, also computes energy consumption and carbon impact for green machine learning endeavors."
                },
                "weaknesses": {
                    "value": "- The extracted five key concepts used to describe architectures (architecture, featureExtraction, featureDescription, classification) are still limited, not sure why this would lead to better diversity in neural architectures."
                },
                "questions": {
                    "value": "- It's unclear how the author choose the five key concepts used to describe architecture.\n- I feel the method is just trying to use natural language to replace the tradtional symbolic respresentation in NAS search spaces. I'm sceptical of why the proposed method could generate more diverse architectures."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3267/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699256176425,
            "cdate": 1699256176425,
            "tmdate": 1699636274911,
            "mdate": 1699636274911,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fLpjKD9hgw",
                "forum": "HgndgAbBcR",
                "replyto": "s23fyNs5Vh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3267/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WMeX"
                    },
                    "comment": {
                        "value": "Thank you for your review. \n\nRegarding your first question, the five concepts are only the top-level ones in our grammar that represent the different phases of processing in a CNN. They were found based on our analysis of the literature and the function of the different layers (or groups of layers) from usual DNNs. In addition, we want to highlight that, as described in Section 4.1, we decompose these concepts into other ones to describe the network\u2019s functions (convolution, pooling, batchNormalisation, etc.). We added the full Xtext Grammar in Appendix B. It contains 16 terminals and 17 non-terminals.  \n\nRegarding your second question, we do not use natural language but a formal context-free grammar defining the language of possible CNN architectures. Therefore, it is also a symbolic approach. We achieve diversity by assembling network functions sequentially while respecting grammar rules. Our generator achieves diversity as it does not start with a predefined topology but combines the network functions sequentially while respecting the grammar rules. Thus, it can form combinations of multiple architectures in one network or completely new ones.  This diversity results in the varied CNN accuracies as depicted in Figure 1b."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240844522,
                "cdate": 1700240844522,
                "tmdate": 1700240844522,
                "mdate": 1700240844522,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]