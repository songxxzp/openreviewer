[
    {
        "title": "Predicting masked tokens in stochastic locations improves masked image modeling"
    },
    {
        "review": {
            "id": "1rcEKxNR5l",
            "forum": "jLnygpRFYm",
            "replyto": "jLnygpRFYm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1616/Reviewer_gTwQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1616/Reviewer_gTwQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes stochastic positional embeddings (StoP) to improve masked image modeling (MIM), which incorporates location uncertainty by conditioning the model on stochastic masked token positions drawn from Gaussian distribution. Experimental results demonstrate that using StoP reduces overfitting to location features and guides the model toward learning features that are more robust to location uncertainty, which also leads to better performance on a variety of downstream tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea of stochastic positional embedding proposed here is novel to me\n- Experiments are sufficient to support the proposed method, showing that the proposed method can achieve significant improvements on various downstream tasks"
                },
                "weaknesses": {
                    "value": "Several parts of the proposed method are not properly introduced and may cause some confusions, details can be found in Questions part"
                },
                "questions": {
                    "value": "- I am a bit confused on step 11 in Algorithm 1. As in Figure 2, the context and masked representations are computed by adding their tokens and positional embeddings together. Then for step 11, I suppose $\\psi_{B_x}$ should refer to the positional embedding, and $A s_x+b$ should refer to context token? Why do we need an additional linear transformation on $s_x$? Some explanations may be needed for this part. \n- Based on the above concern, I am also confused by later explanations in section 3.2 and 4.3, The authors seem to let $s_{x_i}$ (resp. $n_j$) as context (resp. masked) tokens, and $b$ (resp. $\\tilde{m}$) corresponds to the bias for context (resp. masked) tokens. However, I suppose $n_j$ should simply be used to compute stochastic positional embedding as in (2), and $s_{x_i}$ is computed from encoder $f_\\theta$ to encode context information. How can they have the same role? \n- Moreover, with the above correspondence, we should have $A s_x+b$ (resp. $An+\\tilde{m}$) as context (resp.) tokens, then the positional embedding is simply $\\psi_{B_x}$ (resp. $\\psi_{B_y}$), and where is the stochasticity? I suppose there might be some misunderstanding. \n- I would also like to see more discussions on the connection between StoP and vanilla MIM. I suppose we can replace step 10 with $\\tilde{m} + \\psi_{B_y}$, and step 11 with $s_x+\\psi_{B_x}$ to reduce to vanilla MIM, is it correct? Such discussions may make it easier to understand the proposed method. \n- While the authors have mentioned the necessity of regularization on A, the regularization with context token is a bit confusing. I note that the authors have conducted additional experiments in section 4.3 that uses L1 regularization on A. Nevertheless, L1 regularization should aim to obtain a sparse matrix A, which seems to contradict with the original aim to avoid zero A. The authors may consider using some other regularization (and also remove A in computing context tokens) and see how such modification works compared to Algorithm 1. \n\nMinor: the authors may also need to pay more attention on notations and typos. An example is on the top of page 5 \u201cContext Encoding\u201d, \u201cWhere\u201d is wrongly capitalized (in fact the capitalization is used very arbitrarily and may require a careful proof-reading). Also, the notation through this paper is not consistent, especially for representations $c$ and $m$. Some revisions may be needed as well."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1616/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697783718150,
            "cdate": 1697783718150,
            "tmdate": 1699636090118,
            "mdate": 1699636090118,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TVwejL6MOM",
                "forum": "jLnygpRFYm",
                "replyto": "1rcEKxNR5l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1616/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1616/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gTwQ"
                    },
                    "comment": {
                        "value": "Thank you for the thoughtful consideration of the paper and constructive feedback. We\u2019ve incorporated your feedback and uploaded a new revision of the paper. \n\n**Q: \u201cAs in Figure 2, the context and masked representations are computed by adding their tokens and positional embeddings together\u201d**\n\nThank you for pointing this out, there is a mistake in Figure 2 and we apologize for the confusion. To clarify, the masked and context tokens are computed as follows (as in Algorithm 11):\n\n11: $n \\sim \\mathcal{N}(0, \\sigma I)$\n\n12: $m = An + \\psi_{B_y} + \\tilde{m}$\n\n13: $c = As_x + b + \\psi_{B_x}$\n\nWe uploaded a new paper revision and fixed Figure 2. \n\n**Q: \u201cFor step 11, I suppose $\\psi_{B_x}$  should refer to the positional embedding, and \n$As_x + b$ should refer to context token?**\n\nYou are right.\n\n**Q: Why do we need an additional linear transformation on $s_x$? Some explanations may be needed for this part.**\n\nThe mapping projects $s_x$ from the output dimension of the encoder to the input dimension of the predictor (this is standard for other approaches like MAE and I-JEPA).\n\n**Q: The authors seem to let $s_{x_i}$ (resp. $n_j$) as context (resp. masked) tokens, and \n$b$ (resp. $\\tilde{m}$) corresponds to the bias for context (resp. masked) tokens. However, I suppose $n_j$ should simply be used to compute stochastic positional embedding as in (2), and $s_{x_i}$ is computed from encoder $f_{\\theta}$ to encode context information. How can they have the same role?\u201d**\n\nWe assume that you ask why both the context $s_{x_i}$ and noise $n_j$ linearly projected by matrix $A$. In the original MIM formulation there is no stochastic positions and the masked tokens and context tokens are computed as follows:\n\n12: $m = \\psi_{B_y} + \\tilde{m}$\n\n13: $c = Bs_x + b + \\psi_{B_x}$\n\nWhen applying StoP with the reparameterization trick (Eq.3, revised manuscript), the noise is now linearly projected with a matrix $A$ (defined in Eq. 1) and summed with the positional embeddings. Therefore, both the sampled noise and the context tokens are now linearly projected:\n\n11: $n \\sim \\mathcal{N}(0, \\sigma I)$\n\n12: $m = An + \\psi_{B_y} + \\tilde{m}$\n\n13: $c = Bs_x + b + \\psi_{B_x}$\n\nNote that here the context tokens and noise use different projections $A$ and $B$. However, we find that the weights of A are quickly scaled down during training, setting $An=0$ which overcomes the noise during training, resorting to the original MIM introduced before, and the same empirical downstream accuracy. We discussed this in Section 3.1 and 3.2 (see \u201cAvoiding a degenerate determinism solution\u201d and \u201cMasked tokens in stochastic locations\u201d.)\n\nTo avoid this, we use the same matrix $A$ to also project the context tokens $s_x$ (line 13), instead of using a different projection matrix $B$:\n\n11: $n \\sim \\mathcal{N}(0, \\sigma I)$\n\n12: $m = An + \\psi_{B_y} + \\tilde{m}$\n\n13: $c = As_x + b + \\psi_{B_x}$\n\nThe motivation for using $A$ to project both the context features and noise can be understood by considering two extreme cases. When $A=0$, there is complete certainty about the positional embeddings but all context is lost ($As_{x}=0$). On the other hand, when $A$ is large the context information is preserved, but due to the large magnitude of $A$ the noise is amplified and camouflages the positional embedding features of the masked tokens: $An + \\psi_{B_y}$. This dual role of matrix A forces the model to balance between location certainty and the influence of context features in predictions. It optimizes the trade-off for each feature, balancing their presence in predictions against the need for precise spatial locations.\n\nWe discussed this in Section 3.2 in the original submission but following the comments we revised the manuscript to make this more clear. \n\n**Q: I would also like to see more discussions on the connection between StoP and vanilla MIM.**\n\nThe above answer should clarify this comment as well. We follow your advice and clarify this in Algorithm 1 caption and highlight the differences in the Algorithm (see revised manuscript)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699835540770,
                "cdate": 1699835540770,
                "tmdate": 1699900596777,
                "mdate": 1699900596777,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BEdP0kkSlQ",
                "forum": "jLnygpRFYm",
                "replyto": "xlZ1DBcapH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1616/Reviewer_gTwQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1616/Reviewer_gTwQ"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledging the responses"
                    },
                    "comment": {
                        "value": "I suppose some of my previous concerns are successfully resolved, mainly on the connection between StoP and vanilla MIM. Nevertheless, I still have some confusions regarding some details of StoP:\n- The use of using the same matrix $A$ for both projection and covariance still sounds strange to me. I understand that currently StoP effectively prevents $A=0$ as it will lead to $As_x=0$ (no context). Nevertheless, I suppose there might be some other implementations, a straight-forward idea is to use an additional matrix $B$ and computes $m = An+\\psi_{B_y}+\\tilde{m}, c = BAs_x+b+\\psi_{B_x}$. In such case, $A=0$ also leads to $BAs_x=0$ (no context). I wonder if the authors can provide some discussions on that. \n- I am now a bit confused on the experiments on regularization. I suppose you are trying to prove that the improvements of StoP do not solely come from regularizing $A$ (which is used as the projection matrix for context token)? However, I am not sure if the matrix $A$ in StoP is really regularized towards a sparse matrix. Given that you observed that the norm of $A$ decreases with increasing $\\sigma$, I suppose you should try $\\ell_2$ regularization (which regularizes the norm of matrix $A$) instead of $\\ell_1$, and see if that can lead to much improvement."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557102685,
                "cdate": 1700557102685,
                "tmdate": 1700557102685,
                "mdate": 1700557102685,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UIpVI2p7aS",
                "forum": "jLnygpRFYm",
                "replyto": "1rcEKxNR5l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1616/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1616/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, thank you for the reply and we are happy that some of your previous concerns are resolved.\n\n**Q: I suppose there might be some other implementations, a straight-forward idea is to use an additional matrix B**\n\nThank you for this suggestion. The idea to use the matrix B would cancel the noise and lead to a deterministic solution (i.e., removing our novel noise component), and thus this is undesirable. For example: \nIf  $A = \\epsilon I$, then  the noise is scaled down via $An$ and the positional embedding $\\psi_{B_y}$ is unaffected. B can then be set to be $B = A^{-1} = \\frac{1}{\\epsilon} I$, and this will preserve the context tokens information. \n\nThere might be other ways to regularize A that can be explored, for example, by incorporating additional (multiple) loss terms that ensure A has a large enough norm, and that it is full rank. However, our solution is simpler as it doesn't require additional losses and hyperparam tuning.\n\n**Q: I am now a bit confused on the experiments on regularization. I suppose you are trying to prove that the improvements of StoP do not solely come from regularizing $A$? (which is used as the projection matrix for context token)? However, I am not sure if the matrix $A$ in StoP is really regularized towards a sparse matrix. Given that you observed that the norm of $A$ decreases with increasing $\\sigma$, I suppose you should  try  \u21132 regularization (which regularizes the norm of matrix $A$) instead of \u21131, and see if that can lead to much improvement.**\n\nIndeed, we wanted to show that the improvements of StoP are not just due to reducing the norm of A. Clearly, there are several notions of norm, and these can be explored. We focused on $L_1$ because this is a standard approach to regularizing the rank of $A$ for the diagonal case. Furthermore, it is well known that optimization with SGD implicitly regularizes $L_2$ norm (e.g., see https://arxiv.org/abs/1906.05890), so we wanted to test a norm that is not implicitly regularized. \n\nWe note that our $L_1$ regularization experiments also resulted in low $L_2$ (the higher the $L_1$ regularization loss coefficient , the lower the $L_2$ norm, see table below).\n\n| $L_1$ loss Coeff      | $L_2$ norm |\n| ----------- | ----------- |\n| 1.0     | 0.00002       |\n| 0.1  | 0.00007        |\n| 0.01 | 0.00010        |\n| 0.001 | 0.00020        |"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580067486,
                "cdate": 1700580067486,
                "tmdate": 1700643698613,
                "mdate": 1700643698613,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hV71Paztkn",
                "forum": "jLnygpRFYm",
                "replyto": "1rcEKxNR5l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1616/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1616/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> The authors may consider using some other regularization (and also remove A in computing context tokens) \n\n> I suppose you should try \u21132 regularization (which regularizes the norm of matrix $A$) instead of \u21131, and see if that can lead to much improvement.\n\nDear reviewer, we follow up on your suggestion and include additional experiments applying $L_2$ regularization on $A$.  \n\nSpecifically, we trained ViT-B/16 baseline models using deterministic sine-cosine positional embeddings for 150 epochs while adding $L_2$ regularization loss weighted by $\\alpha \\in${$1.0, 0.1, 0.01, 0.001$}. We then applied the ImageNet linear probing protocol, then report the results below.\n\nThese results indicate that StoP cannot be merely replaced by $L_2$ regularization over $A$.\nPlease let us know if there are any other concerns, and we are open to hear more feedback or provide further clarification if needed. \n\n\n| Model      | Top-1 Acc |\n| ----------- | ----------- |\n| Baseline, $\\alpha=0.001$      | 61.7       |\n| Baseline, $\\alpha=0.01$      | 62.7       |\n| Baseline, $\\alpha=0.1$      |    61.9   |\n| Baseline, $\\alpha=1.0$      | 59.8       |\n| StoP      | 64.8 (+2.1)       |"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643314772,
                "cdate": 1700643314772,
                "tmdate": 1700643450033,
                "mdate": 1700643450033,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OSqcpz8DwK",
            "forum": "jLnygpRFYm",
            "replyto": "jLnygpRFYm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1616/Reviewer_92Mr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1616/Reviewer_92Mr"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes modeling a distribution over positional embeddings instead of learning/using deterministic ones which is compatible with any Masked Image Modeling (MIM) framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Authors propose smart modeling design choice to avoid collapsing model to just learn deterministic embeddings. Experimental evaluation shows consistent improvements compared to deterministic MIM (i.e. I-JEPA) for models of different sizes. Also, ablation study is great, authors ablate and deeply study different aspects of the model."
                },
                "weaknesses": {
                    "value": "Honestly, I don't see any obvious weaknesses of the work."
                },
                "questions": {
                    "value": "To strengthen the evaluation, it would be nice to see linear probes/finetuning results on the larger set of downstream datasets. Also, it could be nice to have a model pretrained on a larger dataset rather than Imagenet-1000 as it could lead to stronger model and will enable better transfer to downstream problems which is important to have such representations for the community."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1616/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698677098462,
            "cdate": 1698677098462,
            "tmdate": 1699636090042,
            "mdate": 1699636090042,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ABqGc1lxlT",
                "forum": "jLnygpRFYm",
                "replyto": "OSqcpz8DwK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1616/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1616/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 92Mr"
                    },
                    "comment": {
                        "value": "Thank you for the thoughtful consideration of the paper and very positive feedback. \n\n**Q: To strengthen the evaluation, it would be nice to see linear probes/finetuning results on the larger set of downstream datasets.**\n\nThank you for the comment. We\u2019ve evaluated StoP on 5 different datasets (ImageNet, iNat, Places, DAVIS 2017, CLEVR).  Following your comment, we will run additional evaluations on CUB-200, Flowers-102 and IN-100 and will include it in the final manuscript.\n\n**Q: it could be nice to have a model pretrained on a larger dataset rather than Imagenet-1000 as it could lead to stronger model and will enable better transfer to downstream problems which is important to have such representations for the community.**\n\nThank you for the suggestion. We think that running large scale experiments (e.g, on LAION 5B) with StoP is exciting. Since this might require non trivial engineering efforts and amounts of resources, we leave this for future work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699835579516,
                "cdate": 1699835579516,
                "tmdate": 1699899698781,
                "mdate": 1699899698781,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tNF7G888zy",
                "forum": "jLnygpRFYm",
                "replyto": "ABqGc1lxlT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1616/Reviewer_92Mr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1616/Reviewer_92Mr"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for the clarifications and will maintain my initial assessment of the paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700128351587,
                "cdate": 1700128351587,
                "tmdate": 1700128351587,
                "mdate": 1700128351587,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "puwvdym51j",
            "forum": "jLnygpRFYm",
            "replyto": "jLnygpRFYm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1616/Reviewer_Hczn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1616/Reviewer_Hczn"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes the Stochastic Positionalem beddings (StoP) to MIM in order to perturb the location information of images as a way of regularization. This avoids overfitting the model. The paper motivates and derives the empirical training loss of such perturbation that allows for end to end training by borrowing the well known reparametrization trick. Empirical evidence shows that the proposed method improves the existing SOTA method by evident margin."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper has several strengths including:\n\nS1. It introduces Stochastic Positional Embeddings (StoP) for the purpose of adding perturbations to the location information of images within the MIM framework, thus serving as a means of regularization. This measure intuitively can prevent the model from overfitting. \n\nS2. By employing a reparametrization trick, the paper trivially both justifies and develops the empirical training loss associated with this form of perturbation, enabling end-to-end training. \n\nS3. Empirical results highlight that this proposed technique significantly enhances the state-of-the-art method, demonstrating a noticeable improvement."
                },
                "weaknesses": {
                    "value": "However, there are also several concerning points that needs to be addressed:\n\nW1: It is unclear to me why it is necessary to learn optimal $\\Sigma$ via additional parameterization. What is the benefits of introducing additional degree of freedom here to learn Sigma? What if we fix Sigma without learning? Isn't it a simpler way to avoid degeneracy of matrix A?  Please explain the motivation. \n\nW2: I understand that adding stochastic perturbation to position of the images makes sense in regularizing the model. However, why the same spectral decomposition is applied to features s_x (by multiplying with A)? This step also lacks motivation and seems to be heuristic, please clarify on this point, \n\nW3: What exactly architecture did the paper use to parameterize the matrix $\\Sigma$ ? An architecture flow illustration will help better illustrate this mechanism. Currently, I am not sure how the back-propagation of $\\Sigma$ flows back to the network  (figure 1 does not have this part ) and how it affects the SSL learning with a positive gain. \n\nW4: I am not sure of the significance of proposition 1. I do not see why using this optimal predictor can help achieve better generalization ability of the SSL pretraining on downstream tasks."
                },
                "questions": {
                    "value": "Please see above for the in total 4 questions to be addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1616/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698778665050,
            "cdate": 1698778665050,
            "tmdate": 1699636089962,
            "mdate": 1699636089962,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KqHmFEPwhp",
                "forum": "jLnygpRFYm",
                "replyto": "puwvdym51j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1616/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1616/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Hczn"
                    },
                    "comment": {
                        "value": "Thank you for the thoughtful consideration of the paper and constructive feedback. We\u2019ve incorporated your feedback and uploaded a new revision of the paper.\n\n**W1: It is unclear to me why it is necessary to learn optimal \u03a3  via additional parameterization. What is the benefits of introducing additional degree of freedom here to learn Sigma? What if we fix Sigma without learning? Isn't it a simpler way to avoid degeneracy of matrix A? Please explain the motivation.**\n\nWe compare fixed $\\Sigma$ to learned $\\Sigma$ in Figure 3. Like you mentioned, using a fixed $\\Sigma$ indeed prevents degeneracy of $A$. However, learned $\\Sigma$ works better empirically ($+3.5%$ compared to $+1.9%$, see Figure 3). Implementation wise, both approaches are very simple. \n\nThe motivation to use a learned $\\Sigma$ is to avoid having to perform an extensive grid search to find the optimal $\\Sigma$ values. It is easier to let the model find the values itself.\n\n**W2: I understand that adding stochastic perturbation to position of the images makes sense in regularizing the model. However, why the same spectral decomposition is applied to features $s_x$ (by multiplying with $A$)? This step also lacks motivation and seems to be heuristic, please clarify on this point.**\n\nWithout posing any constraint over $A$, we find that the weights of $A$ are quickly scaled down during training, setting $An=0$ to overcome the noise:\n\n11: $n \\sim \\mathcal{N}(0, \\sigma I)$\n\n12: $m = An + \\psi_{B_y} + \\tilde{m}$\n\nTherefore, we resort to the basic MIM without stochasticity. To avoid this, we use $A$ to project both the noise tokens $n$ and the context tokens $s_x$:\n\n11: $n \\sim \\mathcal{N}(0, \\sigma I)$\n\n12: $m = An + \\psi_{B_y} + \\tilde{m}$\n\n13: $c = As_x + b + \\psi_{B_x}$\n\nThe motivation for using $A$ to project both the context features and noise can be understood by considering two extreme cases, when $A=0$, there is complete certainty about the positional embeddings of the masked tokens $\\psi_{B_y}$ but all context is lost ($As_{x}=0$), thus making the MIM prediction task impossible. On the other hand, when $A$ is large the context information $As_{x}$ is preserved, but due to the large magnitude of $A$ the noise is amplified and camouflages the positional embedding features of the masked tokens: $An + \\psi_{B_y}$, which makes the prediction task hard as well. This dual role of matrix $A$ forces the model to balance between location certainty and the influence of context features in predictions. It optimizes the trade-off for each feature, balancing their presence in predictions against the need for precise spatial locations.\n\nWe discuss this in Section 3.1 (see \u201cAvoiding a degenerate determinism solution\u201d) and in Section 3.2 in the original submission but following the comment we revised the manuscript to make this more clear.\n\n**W3: What exactly architecture did the paper use to parameterize the matrix \u03a3? An architecture flow illustration will help better illustrate this mechanism. (figure 1 does not have this part ) Currently, I am not sure how the back-propagation of \u03a3  flows back to the network and how it affects the SSL learning with a positive gain.**\n\nWe defined $\\Sigma= \\sigma AA^t$ (See revised manuscript Eq. 2) where $\\sigma$ is a scalar hyperparameter and $A$ is a learned matrix.  However, instead of sampling from Eq.1 (where we cannot backprop through $A$), we use the reparametrization trick to sample noise $n \\sim \\mathcal{N}(0, \\sigma I)$, and multiplying by $A$ to get the stochastic positional embeddings: $An + \\psi_{B_y}$ (see revised manuscript Eq 3). This is differentiable w.r.t $A$ because the sampling distribution does not depend on $A$. Note that $Cov(An + \\psi_{B_y}) = \\Sigma$.\n\nWe followed your suggestion and revised the architecture figure (Figure 2) to include the reparameterization trick to make it more clear (see new paper revision).\n\n**W4: I am not sure of the significance of proposition 1. I do not see why using this optimal predictor can help achieve better generalization ability of the SSL pretraining on downstream tasks.**\n\nThe main goal of Proposition 1 is to provide insight to what is learned with StoP in a simple setting (one input and one output). In this case, we show that the optimal predictor explicitly models location uncertainty by performing spatial smoothing. We do not claim this property leads to better generalization (although we do see empirical downstream gains). \n\nTo summarize, we think Proposition 1 provides a nice further analysis, but we are open to moving this into the appendix."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699835583149,
                "cdate": 1699835583149,
                "tmdate": 1699835583149,
                "mdate": 1699835583149,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UFnI1NMF52",
                "forum": "jLnygpRFYm",
                "replyto": "puwvdym51j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1616/Reviewer_Hczn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1616/Reviewer_Hczn"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for the response! After reading the rebuttal, I think some of my concerns are addressed (empirical evidence showing the benefits of using learned Sigma vs the fixed Sigma). However, in terms of reusing the matrix A to $s_x$, I am still not convinced (W2). The current version of doing this projection lacks clear motivation and thus leaving it hard to judge the correctiveness. In this regard, I am afraid I agree with reviewer gTwQ, and I look forward to a better justification of the formulation."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625367779,
                "cdate": 1700625367779,
                "tmdate": 1700627474432,
                "mdate": 1700627474432,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OqjVskd1hM",
                "forum": "jLnygpRFYm",
                "replyto": "puwvdym51j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1616/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1616/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, thank you for the reply and we are happy that some of your previous concerns are resolved. \n\n> However, in terms of reusing the matrix $A$ to $s_x$, I am still not convinced (W2). The current version of doing this projection lacks clear motivation and thus leaving it hard to judge the correctiveness.\n\nThe reason for applying matrix $A$ to $s_x$ is to prevent the stochastic positional embeddings from collapsing into deterministic positional embeddings. Let's begin by describing why this collapse phenomenon happens, and subsequently, we will outline how the use of $A$ with $s_x$ provides an effective solution to address it.\n\n## Stochastic positional embeddings collapse to deterministic\n\nBy using the reparametrization trick, we generate stochastic positions as follows: $\\hat{\\psi}_i = An_i + \\psi_i$ (Eq 3).\n\nIt's important to note that $A$ regulates the noise level, and this noise disrupts the positional embeddings of the masked tokens. Therefore, better MIM predictions may be achieved without the presence of noise. Consequently, during training there is a risk of collapse into deterministic positional embeddings by setting $A=0$.\n\nOur experimental results confirm this. Without introducing a mechanism to prevent collapse, the empirical results resemble those obtained using deterministic Sine-Cosine features (see, for example, Table 6, under \"Sine Cosine\").\n\n## Preventing Collapse\n\nHence, in order to effectively capture location uncertainty through stochastic positional embeddings, it is crucial to prevent the occurrence of this collapse. While there might exist other ideas to address this issue, we employ a simple yet effective approach. This approach stands out as it doesn't necessitate additional losses, hyperparameters, or even learned weights.\n\nThe idea is to use the matrix $A$ to project both $n$ and $s_x$. It's worth noting that in MIM models, there is a linear projection of $s_x$ from the encoder's dimension to the predictor's dimension and we replace it with $A$. For a detailed view of the differences between StoP and MIM, please refer to the revised paper, Algorithm 1.\n\n## How does reusing $A$ to both $s_x$ and $n$ prevents collapse while promoting the modeling of location uncertainty?\n\nUsing $A$ to project $s_x$ serves as a preventive measure against setting $A=0$, as doing so would eliminate crucial context information, making the MIM prediction task impossible. Or as pointed out by reviewer gTwQ: \"StoP effectively prevents $A=0$ as it will lead to $As_x=0$\". \n\nHowever, the model has to learn a matrix A that doesn't excessively amplify $s_x$ as this would result in amplifying the noise $n$ as well. Excessive amplification of the noise would camouflage the positional embeddings of the masked tokens, making their location very uncertain.\n\n## Summary\n\nTo summarize, without introducing a mechanism to prevent collapse, the positional embeddings become deterministic. We proposed to mitigate that by applying the same matrix $A$  both to the noise $n$ and to $s_x$. To learn a good $A$, the model has to trade off the importance of input context and the certainty in the masked tokens location. \n\nPlease let us know if there are any other concerns, and we are open to hear more feedback and provide further clarification if needed. We discuss this topic at length in the recent revision (Section 3.2: \u201cAvoiding a degenerate deterministic solution\u201d)."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641408092,
                "cdate": 1700641408092,
                "tmdate": 1700641789973,
                "mdate": 1700641789973,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]