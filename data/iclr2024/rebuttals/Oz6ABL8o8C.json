[
    {
        "title": "Unified Interpretation of Smoothing Methods for Negative Sampling Loss Functions in Knowledge Graph Embedding"
    },
    {
        "review": {
            "id": "vQWsxlsFvZ",
            "forum": "Oz6ABL8o8C",
            "replyto": "Oz6ABL8o8C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission896/Reviewer_8iQT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission896/Reviewer_8iQT"
            ],
            "content": {
                "summary": {
                    "value": "Knowledge Graphs (KGs) are vital in NLP, but creating them manually has limitations, leading to KG Completion (KGC) using KG Embedding (KGE) and Negative Sampling (NS) to handle many entities while reducing computational costs. The challenge of sparsity due to low link appearance frequencies in KGs is addressed through various smoothing methods like Self-Adversarial Negative Sampling (SANS), with this paper offering theoretical insights and introducing Triplet-based SANS (T-SANS), showing improved performance on multiple datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The author provides clear mathematical problem formulation."
                },
                "weaknesses": {
                    "value": "The motivation of this study is a little bit confusing. And the contribution is not clearly articulated."
                },
                "questions": {
                    "value": "Is the focus of this study to improve interpretation  or  model performance of KGE?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission896/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698450626840,
            "cdate": 1698450626840,
            "tmdate": 1699636016492,
            "mdate": 1699636016492,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yZRlBgq6IX",
                "forum": "Oz6ABL8o8C",
                "replyto": "vQWsxlsFvZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Appreciating your concern: motivation and contributions of our work explained"
                    },
                    "comment": {
                        "value": "## Response to Reviewer @8iQT\nDear 8iQT, we highly appreiciate your valuable comments on our work. Based on feedbacks from you, we have carefully made revisions to our paper, and we are more than committed to following your suggestions and enhance our work more. \n\n### **Weaknesses:**\n>#### The motivation of this study is a little bit confusing. And the contribution is not clearly articulated.\n- Our motivation is to solve the problem in KGC caused by the sparsenes of KGs with our propsed theoretically appropriate smoothing method, T-SANS.\n- As stated in Section $\\S1$, for doing that, we mainly contributed in:\n  - By focusing on the smoothing targets, we theoretically reveal the difference between SANS and subsampling and induce a new NS loss, Triplet-based SANS (T-SANS), that can cover the smoothing target of both SANS and subsampling. \n  - We theoretically and empirically verify how our proposed method can potentially cover the conventional methods and improve KGC performance on sparse KGs.\n- To suport the explanation of the above motivation, we added the explanation to $\\S3.1$ in the updated version of our paper. Besides, the difference of T-SANS and conventional subsampling methods are discussed in $\\S3.3$\n\n### **Questions:**\n>#### Is the focus of this study to improve interpretation or model performance of KGE?\n- The main focus is to provide a systematic interpretation, by doing this, we propose the T-SANS and the unified loss function from the viewpoint of smoothing methods that formulate our findings and even help improve the performance of KGC."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission896/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699960728183,
                "cdate": 1699960728183,
                "tmdate": 1700476336520,
                "mdate": 1700476336520,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4TPv8elS4h",
                "forum": "Oz6ABL8o8C",
                "replyto": "vQWsxlsFvZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "content": {
                    "title": {
                        "value": "OpenReview Deadline Confirmation"
                    },
                    "comment": {
                        "value": "### **We appreciate your insightful review and feedback comments. Now, the deadline for the discussion period only left 18.5hours. To clarify what part of your concerns remains, would it be possible for you to state them?**"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission896/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621115908,
                "cdate": 1700621115908,
                "tmdate": 1700673864547,
                "mdate": 1700673864547,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SCaK9aHlQA",
                "forum": "Oz6ABL8o8C",
                "replyto": "vQWsxlsFvZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Summing up our updates for this paper"
                    },
                    "comment": {
                        "value": "Dear 8iQT, \n\nTo further refine our manuscript and address the insightful feedback from all reviewers, we have made several meaningful enhancements that we hope will not only meet but exceed your expectations:\n\n1. We've expanded Appendix B.2 to include a comprehensive analysis of the training loss and validation MRR curves for each method to provide a clearer, more nuanced understanding of our methods.\n2. In an effort to robustly validate our approach, we've conducted additional experiments using the recent Knowledge Graph Embedding model, HousE (Li et al., 2022). These experiments yielded consistent conclusions with that in our current paper, where our T-SANS method achieving significant improvements in MRR on the FB15k-237 dataset: 24.4% over SANS and 28.6% over NS. This not only reinforces the findings in our paper but also broadens the scope of our method's applicability. We're in the process of finalizing these results, post the completion of experiments with various random seeds, and plan to incorporate them into the updated version of our paper. We anticipate that this comprehensive suite of experiments will take about one more week.\n3. To enhance the reader's experience and understanding, we've carefully revised $\\S3$, which outlines our proposed method. These revisions include not only textual improvements for better flow and comprehension but also examples to aid in grasping the concepts for researchers in other fields. \n\nOur goal with these revisions is to not only address the valuable feedback from reviewers but also to contribute a paper that is engaging, informative, and a pleasure to read. We believe these updates will substantially elevate the quality of our work and hopefully resonate positively with you."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission896/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669879534,
                "cdate": 1700669879534,
                "tmdate": 1700698375616,
                "mdate": 1700698375616,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HkT9AVwxgn",
            "forum": "Oz6ABL8o8C",
            "replyto": "Oz6ABL8o8C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission896/Reviewer_4kKk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission896/Reviewer_4kKk"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a unified interpretation of smoothing methods, SANS and subsampling, for negative sampling loss function in KGE. Authors emphasize the importance of smoothing both p(x,y), p(y|x), and p(x) in the loss function to deal with the data sparsity of KG. Based on the analysis of SANS and subsampling negative sampling loss function, authors propose a new negative sampling function T-SANS, which integrate both subsampling and SANS in the loss function. Experiments show that with T-SANS as the negative sampling method, the KGE models generally performs better than SANS or existing subsampling negative sampling methods, especially on the extreme unbalanced and sparse KGs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The topic is interesting and worth to investigate since negative sampling methods significantly affects the KGE performance for KGC tasks.\n2. The paper tried to find a uniform loss function representation for SANS and subsampling negative sampling methods, which is good. \n3. T-SANS performs better than SANS and subsampling methods supports the importance of smoothing both p(x,y), p(y|x), and p(x) in the loss function."
                },
                "weaknesses": {
                    "value": "1. My main concern of the paper is limited novelty contribution. T-SANS adds the subsampling based on SANS, referring to $p_{\\theta}(x;\\gamma)$ in Equation (12), which is the key difference between T-SANS and SANS. While as mentioned by the author in the footnote, Sun et al. (2019); Zhang et al. (2020b) use subsampling in their released implementation without referring to it in their paper. Thus, if I understood correctly,  I would like to say the actual implementation of methods of Sun et al. (2019); Zhang et al. (2020b) is very similar to T-SANS. Thus the novelty of this paper is limited. \n2. The work is motivated by that conventional works use SANS and subsampling with no theoretical background, and authors believed there is room for further performance improvement. It is unclear the why the lack of the theoretical background lead to potential performance improvement.\n3. Some parts of the paper is not clearly explained or inaccurate and need further improvement, such as\n* the $^{-\\alpha}$ in Equation (4) is unexplained \n* statement in page 5 that \"using Eq. (11) causes an imbalanced loss between the first and second terms since the sum of p\u03b8 (x, yi ) on \u03bd number of negative samples is not always 1\" is not accurate, since in the implementation of the model, there usually is a softmax function among over all the negative samples for a positive triple, which will make the sum of $p_{\\theta} (x, y_i )$ to \u03bd number of negative samples to 1. \n* the caption of Figure 4 is the same as Figure 3."
                },
                "questions": {
                    "value": "1. What is the key/significant difference between T-SANS and the actually implementation of Sun et al. (2019); Zhang et al. (2020b) methods, i.e. SANS with subsampling? \n2. Should the caption of  Figure 4 to be updated?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission896/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission896/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission896/Reviewer_4kKk"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission896/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676742640,
            "cdate": 1698676742640,
            "tmdate": 1699636016422,
            "mdate": 1699636016422,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eRbEMXOFJC",
                "forum": "Oz6ABL8o8C",
                "replyto": "HkT9AVwxgn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Appreciating your attention: the novelty and contributions of our work explained"
                    },
                    "comment": {
                        "value": "## Response to Reviewer @4kKk\nDear 4kKk, I appreciate you spending time reading our work and I am dedicated to enhancing my paper for readers. Would you allow me to draw your attention and ask for your additional consideration of this paper? Because your valuable insights are greatly cherished, and I believe revisiting certain aspects of our hard work could bring further clarity and great value to you too. \nAdditionally, based on feedback from all reviewers, the paper has been enhanced by further meticulous revisions. I assure you that our current paper has the potential to earn your approval.\n\n### **Weaknesses:**\n>#### My main concern of the paper is limited novelty contribution. T-SANS adds the subsampling based on SANS, referring to $p_\\theta(x;y)$ in Equation (12), which is the key difference between T-SANS and SANS. While as mentioned by the author in the footnote, Sun et al. (2019); Zhang et al. (2020b) use subsampling in their released implementation without referring to it in their paper. Thus, if I understood correctly, I would like to say the actual implementation of methods of Sun et al. (2019); Zhang et al. (2020b) is very similar to T-SANS. Thus the novelty of this paper is limited. \n- T-SANS does not applied the subsampling implemented by previous work and is completely different from SANS. Please allow me to explain the differences as below:\n    - As discussed in $\\S2.2$ and $\\S2.3.1$, respectively, Eq. (2) denotes NS loss: \\begin{equation} \\ell_{\\text{NS}}(\\mathbf{\\theta}) = -\\frac{1}{|D|}\\sum_{(x,y) \\in D} \\Bigl[\\log(\\sigma(s_{\\mathbf{\\theta}}(x,y)+\\tau)) + \\frac{1}{\\nu}\\sum_{y_{i}\\sim U}^{\\nu}\\log(\\sigma(-s_{\\theta}(x,y_i)-\\tau))\\Bigr], \\end{equation} Eq. (3) denotes conventional subsampling methods: \\begin{align} &\\ell_{\\text{SUB}}(\\mathbf{\\theta}) \\\\ =&-\\frac{1}{|D|}\\!\\!\\sum_{(x,y) \\in D} \\!\\!\\Bigl[A(x,y;\\alpha)\\log(\\sigma(s_{\\theta}(x,y)\\!+\\!\\tau)) \\!+\\!\\frac{1}{\\nu}\\!\\sum_{y_{i}\\sim U}^{\\nu}B(x,y;\\alpha)\\log(\\sigma(-s_{\\theta}(x,y_i)\\!-\\!\\tau))\\Bigr], \\end{align} where Eq. (4) Eq. (6) Eq. (7) denotes different conventional subsampling strategies based on count in Eq. (5). From above, we can understand conventional subsampling methods mean applying term $A$ and $B$ on NS loss using counted triplets and queries frequencies. While T-SANS does not apply these couting strategies. \n    - As discussed in $\\S2.3.2$ and $\\S3.2$, respectively, Eq. (8) denotes SANS loss: \\begin{align} &\\ell_{\\text{SANS}}(\\mathbf{\\theta}) =-\\frac{1}{|D|}\\sum_{(x,y) \\in D} \\Bigl[\\log(\\sigma(s_{\\theta}(x,y)+\\tau)) +\\!\\! \\sum_{y_{i}\\sim U}^{\\nu}p_{\\theta}(y_i|x;\\beta)\\log(\\sigma(-s_{\\theta}(x,y_i)-\\tau))\\Bigr], \\end{align} Eq. (12) denotes T-SANS loss, and Eq. (13) denotes the detailed calculation for $p_{\\mathbf{\\theta}}(x;\\gamma)$ which is innovated by us:\n\\begin{align}\n&\\ell_{\\text{T-SANS}}(\\mathbf{\\theta}) \\nonumber\\\\\n=& -\\frac{1}{|D|}\\sum_{(x,y) \\in D} \\!\\!\\!p_{\\theta}(x;\\gamma)\\Bigl[\\log(\\sigma(s_{\\theta}(x,y)+\\tau)) +\\!\\! \\sum_{y_{i}\\sim U}^{\\nu}p_{\\theta}(y_i|x;\\beta)\\log(\\sigma(-s_{\\theta}(x,y_i)-\\tau))\\Bigr],\\\\\n&p_{\\mathbf{\\theta}}(x;\\gamma) = \\sum_{y_{i}\\in D} p_{\\mathbf{\\theta}}(x,y_{i};\\gamma),\\:\\:\\:\\:\\:p_{\\mathbf{\\theta}}(x,y_i;\\gamma) = \\frac{\\exp{(\\gamma s_{\\theta}(x,y_i))}}{\\sum_{(x',y')\\in D}\\exp{(\\gamma s_{\\theta}(x',y'))}}, \n\\end{align} From above, we can understand that T-SANS and SANS are completely different, with SANS only taking into account the conditional probability of negative samples and T-SANS being a loss function that considers the joint probability of the pair of queries and their answers. \n    - Thus, our proposal is novel, with the key innovation in weighting through $p_{\\mathbf{\\theta}}(x;\\gamma)$.\n- We also conducted additional experiments and illustrated the performance of T-SANS compared with conventional subsampling methods and SANS in Figure 3(a) to show their difference in essence. \n- Since the first version of our paper did not emphasize the difference between T-SANS and SANS, we added the explanation to $\\S3.1$. Besides, the difference of T-SANS and conventional subsampling methods is discussed in $\\S3.3$.\n\n>#### The work is motivated by that conventional works use SANS and subsampling with no theoretical background, and authors believed there is room for further performance improvement. It is unclear the why the lack of the theoretical background lead to potential performance improvement.\n- Since the previous smoothing methods are not comprehensive, the lack of theoretical background will lead to mistakes in applying a proper conventional smoothing strategy. If the smoothing strategy is not proper, the performance is not optimized. Besides, the SANS does not consider the query probability thus is weak to sparse KGs. On the contary, our proposed method are built upon a comprehensive theoretical foundation that combines both subsampling and enhanced SANS, thus can help improve the KGC performance. The experimental results showing the improvement can be seen in Figure 3,4."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission896/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699959513106,
                "cdate": 1699959513106,
                "tmdate": 1700476262734,
                "mdate": 1700476262734,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Trfmd2DA8P",
                "forum": "Oz6ABL8o8C",
                "replyto": "HkT9AVwxgn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Appreciating your attention: notations of our work explained"
                    },
                    "comment": {
                        "value": "(Continuing the Response to Reviewer @4kKk)\n>#### Some parts of the paper is not clearly explained or inaccurate and need further improvement, such as: (1) the $-\\alpha$ in Equation (4) is unexplained; (2) statement in page 5 that \"using Eq. (11) causes an imbalanced loss between the first and second terms since the sum of $p_\\theta(x,y_i)$ on $\\nu$ number of negative samples is not always 1\" is not accurate, since in the implementation of the model, there usually is a softmax function among over all the negative samples for a positive triple, which will make the sum of $p_\\theta(x,y_i)$ to $\\nu$ number of negative samples to 1. (3) the caption of Figure 4 is the same as Figure 3.\n- As explained in the first version of our paper, $\\alpha$ is a temperature term to adjust the frequecy of triplets and queries. Note that we incorporate $\\alpha$ into Eq. (3) to consider various loss functions even though Kamigaito & Hayashi (2022a;b) do not consider $\\alpha$. Eq. (4-7) shows several assumptions for deciding Eq. (3), thus the annotation of $\\alpha$ holds the same meaning as explained by us when introducing Eq. (3).\n- In Eq. (11), we mean directly calculating $p_\\theta(x,y_i)$, because the score function in KGE model aims to approximate the plausibility of an example, so the most direct way to calculate plausibility in KGE models is using $p_\\theta(x,y_i) = s_{\\theta}(x,y_i)$, where softmax function is not applied. Instead, as we stated below Eq. (11), we choose to calculate this probability using the decomposition $p_{\\mathbf{\\theta}}(x,y)=p_{\\mathbf{\\theta}}(y|x)p_{\\mathbf{\\theta}}(x)$ so that the sum of $p_{\\mathbf{\\theta}}(y|x)$ of all negative samples is always 1 but the sum of $p_{\\mathbf{\\theta}}(x,y)$ is not always 1 by $p_{\\mathbf{\\theta}}(x)$. \n- Following your reminding, we have modified the captions for Figure 3 and 4.\n- Considering your first review is not by the accurate understanding in our paper, would it be possible to increase the score based on the updated information? Also, we hope to have the opportunity to hear more of your feedback."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission896/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699959768342,
                "cdate": 1699959768342,
                "tmdate": 1700476302794,
                "mdate": 1700476302794,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tACXeIQSxO",
                "forum": "Oz6ABL8o8C",
                "replyto": "HkT9AVwxgn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "content": {
                    "title": {
                        "value": "OpenReview Deadline Confirmation"
                    },
                    "comment": {
                        "value": "### **We appreciate your insightful review and feedback comments. Now, the deadline for the discussion period only left 18.5hours. To clarify what part of your concerns remains, would it be possible for you to state them?**"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission896/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621087123,
                "cdate": 1700621087123,
                "tmdate": 1700674026653,
                "mdate": 1700674026653,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "15QzCm3VSB",
                "forum": "Oz6ABL8o8C",
                "replyto": "tACXeIQSxO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission896/Reviewer_4kKk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission896/Reviewer_4kKk"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledgement"
                    },
                    "comment": {
                        "value": "Thanks for the clarification. I would like to keep my original score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission896/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660284113,
                "cdate": 1700660284113,
                "tmdate": 1700660284113,
                "mdate": 1700660284113,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LZiXExM40H",
            "forum": "Oz6ABL8o8C",
            "replyto": "Oz6ABL8o8C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission896/Reviewer_s2ka"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission896/Reviewer_s2ka"
            ],
            "content": {
                "summary": {
                    "value": "The paper delves into the significance of Knowledge Graphs (KGs) in Natural Language Processing (NLP) tasks. The primary focus is on Knowledge Graph Completion (KGC), which aims to automatically complete KGs by scoring their links using Knowledge Graph Embedding (KGE). The paper discusses the challenges posed by the sparsity of KGs and the role of Negative Sampling (NS) loss in addressing these challenges. The paper introduces smoothing methods like Self-Adversarial Negative Sampling (SANS) and subsampling to tackle the sparsity issue. The main contribution is a theoretical interpretation of these smoothing methods and the introduction of a new NS loss called Triplet-based SANS (T-SANS). Experimental results on various datasets demonstrate the effectiveness of T-SANS."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1 The paper provides a comprehensive theoretical understanding of smoothing methods for NS loss in KGE.\nS2 The paper presents experimental results on multiple datasets, showcasing the effectiveness of T-SANS."
                },
                "weaknesses": {
                    "value": "W1 While T-SANS aims to improve upon existing methods, the computational overhead, especially in terms of memory usage and processing time, might not be thoroughly addressed.\n\nW2 While the paper provides a comprehensive theoretical understanding of smoothing methods for NS loss in KGE, it might be too dense for a broader audience. The depth of the theoretical content might make it less accessible to practitioners or researchers from adjacent fields.\n\nW3 How does T-SANS handle extremely sparse datasets compared to other methods? Is there a threshold of sparsity beyond which T-SANS might not be as effective?\n\nW4 How generalizable is T-SANS to other related tasks beyond KGC? Has it been tested on tasks other than KG embedding?"
                },
                "questions": {
                    "value": "Q1 How does T-SANS handle extremely sparse datasets compared to other methods? Is there a threshold of sparsity beyond which T-SANS might not be as effective?\n\nQ2 How generalizable is T-SANS to other related tasks beyond KGC? Has it been tested on tasks other than KG embedding?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission896/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734083468,
            "cdate": 1698734083468,
            "tmdate": 1699636016348,
            "mdate": 1699636016348,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fx3LwnHqg9",
                "forum": "Oz6ABL8o8C",
                "replyto": "LZiXExM40H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Experiments on extremely sparse datasets explained and accessibility improved"
                    },
                    "comment": {
                        "value": "## Response to Reviewer @s2ka\nDear s2ka, we would like to express our sincere appreciation for your invaluable feedback on our paper. We have made improvements to our paper based on your valuable input. Would it be possible to increase the score based on our additional work?\n\n### Weaknesses:\n>#### W1 While T-SANS aims to improve upon existing methods, the computational overhead, especially in terms of memory usage and processing time, might not be thoroughly addressed.\n- Following your suggestion, we are trying to demonstrate the actual memory usage and processing time now. \n\n>#### W2 While the paper provides a comprehensive theoretical understanding of smoothing methods for NS loss in KGE, it might be too dense for a broader audience. The depth of the theoretical content might make it less accessible to practitioners or researchers from adjacent fields.\n- Please allow me to cite an example to show the difference between our T-SANS and conventional smoothing mothods: Our T-SANS can work even when the entity or relations included in the target triplet appear more than once, which is theoretically different from conventional approaches.\n- Following your suggestions, we acknowledged the importance of accessibility and added this example in $\\S3.3$ to make it easier for a wider audience to comprehend and apply our methods.\n\n### Questions:\n>#### W3 How does T-SANS handle extremely sparse datasets compared to other methods? Is there a threshold of sparsity beyond which T-SANS might not be as effective?\n- T-SANS can effectively leverage model-predicted probabilities, enabling it to handle both normally distributed and extremely sparse datasets. In contrast, baseline methods struggle to adequately consider the frequency of sparse triples, resulting in sub-optimal performance on such challenging datasets.\n- To thoroughly evaluate T-SANS, we conducted experiments on three extremely sparse datasets derived from FB15k-237, WN18RR, and YAGO3-10. As outlined in $\\S6$ and illustrated in Figure 4, T-SANS consistently demonstrates improved Knowledge Graph Completion (KGC) performance in scenarios where KGs are extremely sparse. Further, by comparing with the results showcased in Figure 3, we can understand the trend that the greater the dataset sparsity, the more T-SANS outperforms conventional methods. \n- Following your viewpoint, we have added a concise comparision when analyzing in $\\S6$ and moved the picture positions for better accessibility.\n\n>#### W4 How generalizable is T-SANS to other related tasks beyond KGC? Has it been tested on tasks other than KG embedding?\n- Regarding generalizability, since word embeddings and item recommendations are similar to the special case of KGs whose triplets have the same relationships, our work may inspire researchers in these fields. Test in a new task would need much more work, so we planned to put it into future work.\n- Following your viewpoint, we added this note in $\\S8$ to provide more value of our work and to inspire other researchers."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission896/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699952933000,
                "cdate": 1699952933000,
                "tmdate": 1700476078593,
                "mdate": 1700476078593,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lKNojjfudS",
                "forum": "Oz6ABL8o8C",
                "replyto": "LZiXExM40H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Memory and time cost enhanced"
                    },
                    "comment": {
                        "value": "Dear s2ka, for your first suggestion, we have conducted detailed analysis in the appendix of our paper. \n>#### W1 While T-SANS aims to improve upon existing methods, the computational overhead, especially in terms of memory usage and processing time, might not be thoroughly addressed.\n- Following your point of view, we have updated our paper by doing supplementary work: we analysed memory usage in $\\S3.2$, and illustrated convergence curves for actual training loss and validation MRR in Appendix $\\S B.2$ for each setting. By doing this, we can understand that the T-SANS has the same memory and time cost as SANS and NS."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission896/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700211036153,
                "cdate": 1700211036153,
                "tmdate": 1700476221717,
                "mdate": 1700476221717,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gYdlQy81Vx",
                "forum": "Oz6ABL8o8C",
                "replyto": "LZiXExM40H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "content": {
                    "title": {
                        "value": "OpenReview Deadline Confirmation"
                    },
                    "comment": {
                        "value": "### **We appreciate your insightful review and feedback comments. Now, the deadline for the discussion period only left 18.5hours. To clarify what part of your concerns remains, would it be possible for you to state them?**"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission896/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621050403,
                "cdate": 1700621050403,
                "tmdate": 1700674007170,
                "mdate": 1700674007170,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EhwvJD7I0S",
                "forum": "Oz6ABL8o8C",
                "replyto": "gYdlQy81Vx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission896/Reviewer_s2ka"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission896/Reviewer_s2ka"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "I appreciate the thorough responses. My opinion of the paper remains unchanged."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission896/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632538936,
                "cdate": 1700632538936,
                "tmdate": 1700632538936,
                "mdate": 1700632538936,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TDgEjtXqzv",
            "forum": "Oz6ABL8o8C",
            "replyto": "Oz6ABL8o8C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission896/Reviewer_fiTR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission896/Reviewer_fiTR"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates how different smoothing methods affect the negative sampling losses for knowledge graph embedding. It introduces a new triplet-based self-adversarial negative sampling method that can adjust the frequencies of triplets, queries, and answers in the training data. It evaluates the proposed method on three benchmark datasets with five base models and demonstrates its effectiveness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Negative sampling is a crucial technique for learning KG embeddings. This paper offers a valuable insight into the smoothing methods for learning loss in KGE. I think it is an interesting and relevant work for the KGE community.\n\n2. Based on the comparison and analysis of existing smoothing methods, the paper proposes triplet-based SANS, which can outperform other baselines on three datasets."
                },
                "weaknesses": {
                    "value": "1. In my view, the proposed method is incremental work based on previous studies. It is an extension of SANS.\n\n2. Another weakness is that the selected KGE models in the experiments are old. Some popular or recent models, such as TuckER [1] and HousE [2], are not included, which, in my view, may weaken the soundness the work.\n\n[1] Ivana Balazevic, Carl Allen, Timothy M. Hospedales: TuckER: Tensor Factorization for Knowledge Graph Completion. EMNLP/IJCNLP (1) 2019: 5184-5193\n\n[2] Rui Li, Jianan Zhao, Chaozhuo Li, Di He, Yiqi Wang, Yuming Liu, Hao Sun, Senzhang Wang, Weiwei Deng, Yanming Shen, Xing Xie, Qi Zhang: HousE: Knowledge Graph Embedding with Householder Parameterization. ICML 2022: 13209-13224"
                },
                "questions": {
                    "value": "1. Why are some results on YGAO3-10 missing? I think it would be better to produce the results using open-source implementations.\n\n2. Is it possible to provide any analysis or experimental results to assess the effect of negative sampling on the convergence rate?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission896/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834216235,
            "cdate": 1698834216235,
            "tmdate": 1699636016257,
            "mdate": 1699636016257,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zvxk5IF53P",
                "forum": "Oz6ABL8o8C",
                "replyto": "TDgEjtXqzv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Novelty of our work"
                    },
                    "comment": {
                        "value": "## Response to Reviewer @fiTR\nDear fiTR, we greatly appreciate your reviewing our paper, and we are committed to improve following your insightful suggestions.\n\n### **Weaknesses:**\n>#### 1. In my view, the proposed method is incremental work based on previous studies. It is an extension of SANS.\n- Although the name T-SANS gives the impression that it is just an extension of SANS, this name was given out of respect for SANS. In fact, they are completely different, with SANS only taking into account the conditional probability of negative samples and T-SANS being a loss function that considers the joint probability of the pair of queries and their answers. This difference is large from the viewpoint of loss functions [1].\n    - [1]: Zhuang Ma, Michael Collins. \"Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency\", https://aclanthology.org/D18-1405/\n- We conducted experiments and illustrated the performance differences of T-SANS compared with SANS in Figure 3, which verified their difference in the actual data. \n- However, Reviewer @4kKk also misunderstood this point, so we added a concise explanation emphasizing the difference in $\\S3.1$.\n\n>#### 2. Another weakness is that the selected KGE models in the experiments are old. Some popular or recent models, such as TuckER [1] and HousE [2], are not included, which, in my view, may weaken the soundness the work.\n- Following your suggestion, we are trying to implement TuckER and HouseE now. Would it be possible to increase the score based on the results of the additional experiments?\n\n### **Questions:**\n>#### 1. Why are some results on YGAO3-10 missing? I think it would be better to produce the results using open-source implementations.\n- Since YAGO3-10 is larger than FB15k-237 and WN18RR, it requires a lot of computational resources. To deal with this problem, we made the decision not to train the YAGO3-10 dataset on all models but only on two best-performing models whose efficient hyperparameters on YAGO3-10 are provided by previous work.\n\n>#### 2. Is it possible to provide any analysis or experimental results to assess the effect of negative sampling on the convergence rate?\n- Following your valuable suggestion, we believe that this analysis will further provide valuable insights and enhancing the credibility of our work, thus we are wholeheartedly committed to conducting this analysis. Would it be possible to increase the score based on the results of the additional analysis?"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission896/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699951173501,
                "cdate": 1699951173501,
                "tmdate": 1700475898649,
                "mdate": 1700475898649,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "snKNJSMci6",
                "forum": "Oz6ABL8o8C",
                "replyto": "TDgEjtXqzv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Convergence analysis conducted in Appendix B.2"
                    },
                    "comment": {
                        "value": "Dear fiTR, for your second qustion, we have conducted additional analysis.\n\n>#### Is it possible to provide any analysis or experimental results to assess the effect of negative sampling on the convergence rate?\n- By following your valuable suggestion, as is refered to in $\\S5.2$ of the updated revision of our paper, we conducted convergence analysis in Appendix $\\S B.2$ using Figures 5,6, and 7 of training loss curves and validation MRR curves for each smoothing method for each experimental setting. \n- From Figure 5,6,and 7, we can understand that the convergence of our T-SANS loss is as well as SANS and NS loss on datasets FB15k-237, WN18RR, and YAGO3-10 for each KGE model.\n- We believe that this analysis will further underscore our dedication to providing valuable insights and enhancing the credibility of our work. Thank you again!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission896/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700199548981,
                "cdate": 1700199548981,
                "tmdate": 1700476027838,
                "mdate": 1700476027838,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F9Xvb5qgCR",
                "forum": "Oz6ABL8o8C",
                "replyto": "TDgEjtXqzv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our methods verified on HousE (Li et al., 2022)"
                    },
                    "comment": {
                        "value": "Dear fiTR, for your second suggestion, we have verified our method by conducting addtional expeirments for HousE. \n>#### Another weakness is that the selected KGE models in the experiments are old. Some popular or recent models, such as TuckER [1] and HousE [2], are not included, which, in my view, may weaken the soundness the work.\n- We would like to share with you that we have verified our method for the most recent KGE model HousE (Li et al., 2022), and that the experimental results show that our T-SANS gains 24.4% and 28.6% improvement in MRR on dataset FB15k-237 compared with SANS and NS, respectively. Thus, the conclusion is consistent with that in our paper. \n- We are also conducting implementation for TuckER (Balazevic et al., 2019). \n- However, because of the computing resource limitation, we will need one more week to finish all the experiments. If our paper is accepted, I am sure that we will show all the experimental results expected. Would it be possible to increase the score based on the results of the additional experiments?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission896/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475756996,
                "cdate": 1700475756996,
                "tmdate": 1700698858505,
                "mdate": 1700698858505,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cXKly19DFe",
                "forum": "Oz6ABL8o8C",
                "replyto": "TDgEjtXqzv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission896/Authors"
                ],
                "content": {
                    "title": {
                        "value": "OpenReview Deadline Confirmation"
                    },
                    "comment": {
                        "value": "### **We appreciate your insightful review and feedback comments. Now, the deadline for the discussion period only left 18.5hours. To clarify what part of your concerns remains, would it be possible for you to state them?**"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission896/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621016861,
                "cdate": 1700621016861,
                "tmdate": 1700673988147,
                "mdate": 1700673988147,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]