[
    {
        "title": "Measuring Local and Shuffled Privacy of Gradient Randomized Response"
    },
    {
        "review": {
            "id": "oGnGklKs1f",
            "forum": "Oe4XdjTP4y",
            "replyto": "Oe4XdjTP4y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2925/Reviewer_eRua"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2925/Reviewer_eRua"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies local differential privacy (LDP) guarantees in the federated learning distributed setting. First, the paper empirically considers the privacy loss for the gradient randomized responses including in the well-known LDP-SGD (Duchi et. al. 2018, Erlingsson et. al. 2020), which first computes a clipped gradient and then samples a random unit vector, signed by a function of the clipped gradient as the output. The paper then considers various adversaries in federated learning to produce a worst-case attack that reaches the theoretical limits of LDP-SGD. \n\nThe paper first claims that the worst-case inputs that match the theoretical upper bounds of LDP-SGD are achieved when the gradients have norms that match or exceed the clipping threshold $L$, possibly resulting in the incorrect sign. It then shows that for increasing values of the privacy parameter $\\varepsilon$, the distributions become easier to distinguish. \n\nThe paper then studies a number of attacks, including settings where 1) some labels are flipped, 2) some gradients are flipped, 3) the client and the server collude to flip a gradient, which is compounded by a malicious global model sent from a server, and 4) the client produces a dummy gradient."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Experiments performed over a large number of datasets\n- Collusion attack and dummy gradient attack empirically reveal vulnerabilities of LDP-SGD\n- The experiments observed privacy amplification through shuffling"
                },
                "weaknesses": {
                    "value": "- Both 1) gradients becoming more distinguishable in experiments as the privacy parameter increases and 2) privacy amplification under shuffling matches existing theory and perhaps is not entirely surprising\n- Limited conceptual or theoretical novelties"
                },
                "questions": {
                    "value": "Were there any characterizations observed for the privacy-convergence or privacy-utility tradeoffs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697863272816,
            "cdate": 1697863272816,
            "tmdate": 1699636236151,
            "mdate": 1699636236151,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B6GFaighF9",
                "forum": "Oe4XdjTP4y",
                "replyto": "oGnGklKs1f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2925/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer"
                    },
                    "comment": {
                        "value": "We thank the reviewer for carefully reviewing our paper. We would like to answer the questions you gave.\n\n\n\n\n>\u00a0Both 1) gradients becoming more distinguishable in experiments as the privacy parameter increases and 2) privacy amplification under shuffling matches existing theory and perhaps is not entirely surprising\n\nIn addition to observations of the empirical epsilon, it is also important to discover worst-case scenarios.\n\nThe discovery of a worst-case attack allows the client can verify that the randomization algorithm satisfies the epsilon-LDP.\n\nWe also suggested the empirical epsilon in the shuffle model (Table 3) that the theoretical value has room for improvement.\n\n\n\n\n>\u00a0Were there any characterizations observed for the privacy-convergence or privacy-utility tradeoffs?\n\nThis may be out of focus as we are not discussing utilities.\n\nIf we have misunderstood, please let us know."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700356152518,
                "cdate": 1700356152518,
                "tmdate": 1700356152518,
                "mdate": 1700356152518,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NoSoMG3KnZ",
            "forum": "Oe4XdjTP4y",
            "replyto": "Oe4XdjTP4y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2925/Reviewer_HQzw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2925/Reviewer_HQzw"
            ],
            "content": {
                "summary": {
                    "value": "The paper looks at empirically measuring differential privacy (DP) level in federated learning (FL) under local DP (LDP) directly, and when the clients jointly communicate all results via a trusted shuffler. The authors focus on the gradient randomized response mechanism as the common LDP mechanism, find a pair of gradients corresponding to the worst-case under the chosen mechanism, and empirically measure how successful membership inference attacks. From the results they convert to empirical ADP guarantees via existing techniques. The paper considers the resutls under 5 different adversaries, including the most powerful adversary allowed by DP."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The topic of empirically measuring DP protection level is important and topical.\n\n* The paper is fairly clear to read.\n\n* While the paper relies on many existing techniques developed for empirical DP estimation, the considered DP mechanism is somewhat different than the existing ones, and the results on shuffle DP seem novel."
                },
                "weaknesses": {
                    "value": "* Comparisons to existing DP bounds in the shuffle model only use the (loose) analytical bound from Feldman et al. 2023.\n\n* In my opinion, the overall story of clients testing by themselves in FL (e.g. abstract) is not convincing or necessary. Instead, this is much more convincing simply as an exploration of empirical privacy in the FL setting, regarless of whether this could be done by the clients bythemselves or by someone else.\n\n* After reading the paper, I still have several questions for the authors (see below for particulars)."
                },
                "questions": {
                    "value": "1) First adversary settings (benign, label flip) seem too benign: I would argue that in most cases two random samples from the data will give overpositive results on the privacy level, which is a worst-case guarantee, in many cases possibly even with label flips. Considering this, comments such as in Relaxations of privacy parameters in Sec6.3 seem overconfident. Have you checked what would be the actual worst-case in the data for benign or label-flip? (On a related note, see also question 8)\n\n2) On distinguishing the gradients: do you use cos-similarity  as the similarity metric in all cases, even in the benign setting? Is this still optimal way to do it?\n\n3) Sec 3.1: on the difference in worst-case w.r.t Gaussian mechanism in the centralized setting: despite several statements about the worst case being entirely different compared to the Gaussian mechanism, to me it seems like the given worst case (maximal l2-norm grads pointing to opposite directions) should work as is also for the Gaussian mechanism under replace neighbourhood, and vice versa. Did I misread this?\n\n4) In the empirical shuffling experiments, do you include delta also in the empirical results (it is not included e.g. in Sec 4.1)?\n\n5) Provide some metric of variability for the empirical results, e.g., Fig4, or mention that the variability is small-enough to be ignored.\n\n6) Sec6.2: \"These results suggest that privacy amplification by shuffling may be an improvement over the state-of-the-art.\" What does this mean?\n\n7) Given that we know that the analytical bound for shuffle DP is loose (see e.g. Feldman et al. 2023, Koskela et al 2023), is there some reason not to use numerical accounting when comparing against existing shuffle DP bounds to get at least somewhat tighter bounds?\n\n8) Considering comments like Sec6.3 Relaxations of privacy parameters: how did you choose the norm clipping value for the experiments? One would expect that in the more benign settings choosing a large value leads to small empirical epsilon due to most gradients not hitting the clipping value, i.e., being farther from the worst-case, but this value does not explicitly show up in the results in any way. If so, one could basically choose any empirical epsilon between 0 and the one corresponding to the worst-case as a result of this proposed measurement just by tuning the clipping value. Which of these values would make sense?\n\n#### Minor comments/requests (no reason to comment on this)\ni) Please state the neighbourhood definition explicitly in defining DP.\n\nReferences:\nFeldman et al. 2023: Stronger privacy amplification by shuffling...\nKoskela et al. 2023: Numerical accounting in the shuffle model of DP"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676370604,
            "cdate": 1698676370604,
            "tmdate": 1699636236067,
            "mdate": 1699636236067,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X7MrICPQPS",
                "forum": "Oe4XdjTP4y",
                "replyto": "NoSoMG3KnZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2925/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for carefully reviewing our paper. We would like to answer the questions you gave.\n\n>\u00a0In my opinion, the overall story of clients testing by themselves in FL (e.g. abstract) is not convincing or necessary. Instead, this is much more convincing simply as an exploration of empirical privacy in the FL setting, regarless of whether this could be done by the clients bythemselves or by someone else.\n\nIn collaborative learning like FL, the server often provides a randomized mechanism to all clients to orchestrate the learning process.\u00a0\nEven when the privacy of the client has been preserved, clients will be concerned about how well the randomized mechanism protects their gradient (or data).\n\nAccording to a previous survey[xiong2020], some users who did not allow information sharing claimed two reasons: they did not trust the DP techniques and did not trust the app or tech company.\nWe believe that to encourage more users to contribute data, it is necessary not only to provide a clear explanation of LDP but also to verify that the randomized mechanism is credible.\n\n[xiong2020] Xiong, Aiping, et al. \"Towards effective differential privacy communication for users\u2019 data sharing decision and comprehension.\"\u00a02020 IEEE Symposium on Security and Privacy (SP). IEEE, 2020.\n\n\n\n\n>\u00a0First adversary settings (benign, label flip) seem too benign: I would argue that in most cases two random samples from the data will give overpositive results on the privacy level, which is a worst-case guarantee, in many cases possibly even with label flips. Considering this, comments such as in Relaxations of privacy parameters in Sec6.3 seem overconfident. Have you checked what would be the actual worst-case in the data for benign or label-flip? (On a related note, see also question 8)\n\n>\u00a0Considering comments like Sec6.3 Relaxations of privacy parameters: how did you choose the norm clipping value for the experiments? One would expect that in the more benign settings choosing a large value leads to small empirical epsilon due to most gradients not hitting the clipping value, i.e., being farther from the worst-case, but this value does not explicitly show up in the results in any way. If so, one could basically choose any empirical epsilon between 0 and the one corresponding to the worst-case as a result of this proposed measurement just by tuning the clipping value. Which of these values would make sense?\n\nWe thank the reviewer for the helpful comments.\n\nHere, we present additional experimental results on the relationship between the norm clipping value and empirical epsilon.\n\nFor FEMNIST dataset and theoretical epsilon = 1,\u00a0\n\n- Benign setting\n  - clipping size = 1, empirical epsilon = 0.32 (std=0.081)\n  - clipping size = 0.1, empirical epsilon = 0.32 (std=0.053)\n- Label flip\n  - clipping size = 1, empirical epsilon = 0.56 (std=0.351)\n  - clipping size = 0.1, empirical epsilon =0.43 (std=0.135)\n\nFrom these results, we believe that even if gradient norm projection is avoided in benign setting and label flip, the clipping size is unlikely to affect the empirical epsilon because the gradient is rarely inverted in these scenarios.\n\n\n\n\n>\u00a0On distinguishing the gradients: do you use cos-similarity as the similarity metric in all cases, even in the benign setting? Is this still optimal way to do it?\n\nAs in [Nasr 2021], we also considered a method to compare the increase or decrease in Loss due to the chosen gradient.\n\nHowever, such a distinguishing algorithm was no more effective than cosine similarity.\n\nThis is because, in LDP-SGD (Algorithm 1), the only object that depends on the data is the sign of the inner product in the computation of $\\hat{z}$."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700355852209,
                "cdate": 1700355852209,
                "tmdate": 1700355852209,
                "mdate": 1700355852209,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1skCBMz4Li",
                "forum": "Oe4XdjTP4y",
                "replyto": "v6uIzygoPC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2925/Reviewer_HQzw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2925/Reviewer_HQzw"
                ],
                "content": {
                    "title": {
                        "value": "Small clarifications"
                    },
                    "comment": {
                        "value": "Thanks for the rebuttal, to clarify further:\n\ni) on the worst-case:\nI do not understand your point. For the Gaussian the worst-case under replace neighbourhood assuming sensitivity $2\\lambda$ is e.g. $(\\lambda, 0, \\dots,  0)$ vs $(-\\lambda, 0, \\dots,  0)$, i.e., grads pointing in opposite directions with norm $L=2\\lambda$. You state in the paper that this is exactly the worst-case for LDP-SGD or do you not?\n\nii) on the numerical accounting: as far as I can see, the analytical bound is loose even without compositions, see Feldman et al. 2023 (e.g. Sec.6 in v2 on arXiv), Koskela et al. 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644277398,
                "cdate": 1700644277398,
                "tmdate": 1700644277398,
                "mdate": 1700644277398,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0br1bFbohb",
                "forum": "Oe4XdjTP4y",
                "replyto": "X7MrICPQPS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2925/Reviewer_HQzw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2925/Reviewer_HQzw"
                ],
                "content": {
                    "title": {
                        "value": "Further clarification"
                    },
                    "comment": {
                        "value": "A small comment on the more benign settings: on a general level, to me this looks something like avg empirical privacy, which I think is not a good proxy for DP, i.e., for the worst-case. The point in the earlier comment was that by setting the clipping bound you can affect how many grads have a high probability of getting flipped, and this is not reflected in any way in the measurement of empirical privacy. If you think about the use case of convincing clients that the mechanism is safe, \nto me it seems like using your method I could get significantly lower empirical $\\epsilon$ in the benign case to convince the clients by setting clipping norm to match the actual worst case for the testing. When you then pick a random sample from the data to check the bound, it is quite likely to not be the worst-case, and since gradients typically are concentrated around 0, I would expect a random sample to have a considerably higher probability of getting flipped than the worst-case. Is this reasoning wrong?\n\nPlease also clarify in the paper explicitly when your proposed method is optimal and when it is not."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646265398,
                "cdate": 1700646265398,
                "tmdate": 1700646265398,
                "mdate": 1700646265398,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "z2qd0V4DgX",
            "forum": "Oe4XdjTP4y",
            "replyto": "Oe4XdjTP4y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2925/Reviewer_MdJg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2925/Reviewer_MdJg"
            ],
            "content": {
                "summary": {
                    "value": "This work extends the line of privacy auditing research to LDP. They analyze the worst-case gradient pair of LDP-SGD mechanism, and use the worst-case pair to design a simple distinguishing game for measuring the lower bound of epsilon through the classic Clopper-Pearson bound. The paper discusses different capability of the adversary. The paper also extends the attack method to the shuffle model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper has a clear roadmap and is very easy to follow."
                },
                "weaknesses": {
                    "value": "1. I am not too sure about the motivation of privacy auditing for LDP. The point of LDP is that the clients do not trust the central server and want to do the randomization on their own. I think the authors' intention was to verify the privacy guarantee of the local randomizer the client uses, but why can't the clients just run the local randomization program they trust (e.g., the one implemented by themselves)? I think the authors should be very clear about the motivation and assumptions for the scenario they consider here. \n\n2. The technical contribution of this work is relatively low. My feeling is that the only notable contribution in this work is the discovery of the worst-case gradient pair for the LDP-SGD algorithm (and the result for that is also not too surprising). Happy to be corrected on this point. \n\n3. The paper has quite a few places that lack mathematical rigor. For example, I didn't find where the author defines $\\tilde{g}_1$ and $\\tilde{g}_2$ in Section 3.2. The assumption for Proposition 4.1 should be clearly stated (the loss is binary cross-entropy). Also, it seems equation 6 is missing something (I guess it's indicator function)?\n\n4. I am not entirely sure about what is the message of Section 3.2 is trying to convey. I feel like it's quite obvious that when $\\epsilon$ is small, it's hard to distinguish, and when $\\epsilon$ is large, it's easier to distinguish. What sounds interesting is the difference when using different $d$, but I don't find a discussion for it in the paper."
                },
                "questions": {
                    "value": "Does the dimension $d$ impact the attack performance (if fix the number of trials)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698716389748,
            "cdate": 1698716389748,
            "tmdate": 1699636235989,
            "mdate": 1699636235989,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m99IEwxSQe",
                "forum": "Oe4XdjTP4y",
                "replyto": "z2qd0V4DgX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2925/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer"
                    },
                    "comment": {
                        "value": "We thank the reviewer for carefully reviewing our paper. We would like to answer the questions you gave.\n\n>\u00a0I am not too sure about the motivation of privacy auditing for LDP. The point of LDP is that the clients do not trust the central server and want to do the randomization on their own. I think the authors' intention was to verify the privacy guarantee of the local randomizer the client uses, but why can't the clients just run the local randomization program they trust (e.g., the one implemented by themselves)? I think the authors should be very clear about the motivation and assumptions for the scenario they consider here.\n\nIn collaborative learning like FL, the server often provides a randomized mechanism to all clients to orchestrate the learning process.\u00a0\nEven when the privacy of the client has been preserved, clients will be concerned about how well the randomized mechanism protects their gradient (or data).\n\nAccording to a previous survey[xiong2020], some users who did not allow information sharing claimed two reasons: they did not trust the DP techniques and did not trust the app or tech company.\nWe believe that to encourage more users to contribute data, it is necessary not only to provide a clear explanation of LDP but also to verify that the randomized mechanism is credible.\n\n\n\n\n> The technical contribution of this work is relatively low. My feeling is that the only notable contribution in this work is the discovery of the worst-case gradient pair for the LDP-SGD algorithm (and the result for that is also not too surprising). Happy to be corrected on this point.\n\nWe also introduce a way to measure empirical privacy amplification by shuffling.\n\nThe empirical epsilon in the shuffle model we present in Table 3 suggests that the theoretical value has room for improvement.\n\n\n\n\n>\u00a0The paper has quite a few places that lack mathematical rigor. For example, I didn't find where the author defines \\tilde{g_1}\u00a0and \\tilde{g_2}\u00a0in Section 3.2. The assumption for Proposition 4.1 should be clearly stated (the loss is binary cross-entropy). Also, it seems equation 6 is missing something (I guess it's indicator function)?\n\nWe apologize for the lack of explanation.\u00a0\n\n means the output of the LDP-SGD.\n\nDue to space limitations, the assumption for Proposition 4.1 is shown in Appendix C.\n\nSimilar to Equation 4, Equation 6 is the distinguishing algorithm.\n\nEquation 6 distinguishes the input from the shuffler output (the characteristics of the distribution of the gradient), as also shown in Figure 1(b).\n\nExplanations for these additions were added during the rebuttal period.\n\n\n\n\n>\u00a0I am not entirely sure about what is the message of Section 3.2 is trying to convey. I feel like it's quite obvious that when epsilon\u00a0is small, it's hard to distinguish, and when epsilon\u00a0is large, it's easier to distinguish. What sounds interesting is the difference when using different epsilon, but I don't find a discussion for it in the paper.\n\n> Does the dimension d\u00a0impact the attack performance (if fix the number of trials)?\n\nWe apologize for the lack of explanation.\n\nFirst, there is the property that higher dimensional vectors have lower cosine similarity.\n\nHowever, if the gradient is in the opposite direction, the distribution is clearly divided around 0.\n\nThis suggests that determining a non-zero threshold and predicting the original gradient from the cosine similarity may not discriminate well depending on the dimension.\n\n\n\n\n[xiong2020] Xiong, Aiping, et al. \"Towards effective differential privacy communication for users\u2019 data sharing decision and comprehension.\"\u00a02020 IEEE Symposium on Security and Privacy (SP). IEEE, 2020.\n\n\n\n\nPlease let us know if anything else is unclear."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700119105380,
                "cdate": 1700119105380,
                "tmdate": 1700214693801,
                "mdate": 1700214693801,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qA9Fhiuljt",
            "forum": "Oe4XdjTP4y",
            "replyto": "Oe4XdjTP4y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2925/Reviewer_MFPE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2925/Reviewer_MFPE"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies schemes for auditing the empirical privacy parameters of the LDP mechanism (Specifically PrivUnit) and the shuffled model. The idea is to estimate the false positive rate (FPR) and true positive rate (TPR) by running the PrivUnit mechanism multiple times with half of the runs on gradient $g_1$ and the other half of runs on gradient $g_2=-g_1$. Then, the empirical estimate of $\\varepsilon_0$ is obtained from the estimated FPR and TPR. This is exactly the standard black-box algorithm in estimating the empirical $\\varepsilon$ in the central DP (except the worst case neighboring datasets)."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The LDP and the shuffled model can be seen as a special case of the central DP mechanisms. In other words, the $\\varepsilon_0$-LDP mechanism is also $\\varepsilon_0$-DP in the central DP model ( similarly for the shuffled model). Hence, I don't understand the main difference between the scheme proposed in this paper and the schemes in the literature for CDP."
                },
                "weaknesses": {
                    "value": "I disagree with the authors in the introduction that the previous studies estimate the empirical privacy of the Gaussian mechanism. There are some studies that consider the general $\\varepsilon$-DP mechanism, e.g., Algorithm 2 in [Jagielski 2020] is generic and isn't dedicated to the Gaussian mechanism. Also [Steinke 2023] proposed a scheme for empirically estimating $\\varepsilon$ for a generic DP mechanism in $\\mathcal{O}(1)$ training run.\n\nIt is not clear to me the novelty of this work.  The paper combines ideas from the existing work in the literature. The techniques used for CDP can be applied to empirically estimate $\\varepsilon_0$ in LDP and $\\varepsilon$ in the shuffled model. The only difference is in constructing the worst-case neighboring data points in the LDP which is straightforward for the considered PrivUnit mechanism [Duchi et. al. 2018]. \n\nThe paper focuses mainly on empirically estimating $\\varepsilon_0$ of a special version of the PrivUnit mechanism [Duchi et. al. 2018 ] which is order optimal only in the high privacy regime $\\varepsilon_0\\leq 1$. It is better to focus on the PrivUnit2  [Bhowmick 2019] which is optimal for all privacy regimes. What about a generic $\\epsilon_0$-LDP mechanism? Are there any ideas on how to handle this case?\n\n\n \n\n\n\n\nMatthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing differentially private machine learning: How private is private sgd? Advances in NeurIPS 2020\n\nSteinke, Thomas, Milad Nasr, and Matthew Jagielski. \"Privacy Auditing with One (1) Training Run.\" arXiv preprint arXiv:2305.08846 (2023)."
                },
                "questions": {
                    "value": "Please, check my comments in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805826425,
            "cdate": 1698805826425,
            "tmdate": 1699636235919,
            "mdate": 1699636235919,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nQYIdNyWPK",
                "forum": "Oe4XdjTP4y",
                "replyto": "qA9Fhiuljt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2925/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful comments. Please find our replies to the comments below.\n\n> There are some studies that consider the general \\epsilon-DP mechanism, e.g., Algorithm 2 in [Jagielski 2020] is generic and isn't dedicated to the Gaussian mechanism. Also [Steinke 2023] proposed a scheme for empirically estimating epsilon for a generic DP mechanism in O(1) training run.\n\nIndeed, the methods of [Jagielski 2020] and [Steinke 2023] are applicable to any mechanism.\n\nHowever, in those papers, only the empirical epsilon of the Gaussian mechanism is observed, and the worst-case values for the Gaussian mechanism are taken into account.\n\nSince the worst-case of LDP-SGD is different from that of the Gaussian mechanism, the empirical epsilon presented by [Jagielski 2020] and [Steinke 2023] does not yield similar results for any mechanism.\n\n\n\nWe claim that our contribution is that we devised the adversary for mechanisms like LDP-SGD and PrivUnit.\n\nHowever, observing empirical epsilon alone is not enough; it is also important to discover the worst-case scenario.\n\nThe discovery of a worst-case attack allows the client can verify that the randomization algorithm satisfies the epsilon-LDP.\n\nAccording to a previous survey[xiong2020], some users who did not allow information sharing claimed two reasons: they did not trust the DP techniques and did not trust the app or tech company.\nWe believe that to encourage more users to contribute data, it is necessary not only to provide a clear explanation of LDP but also to verify that the randomized mechanism is credible.\n\nWe also introduced a way to measure empirical privacy amplification by shuffling.\n\nThe empirical epsilon in the shuffle model we present in Table 3 suggests that the theoretical value has room for improvement.\n\n\n\n[xiong2020] Xiong, Aiping, et al. \"Towards effective differential privacy communication for users\u2019 data sharing decision and comprehension.\" 2020 IEEE Symposium on Security and Privacy (SP). IEEE, 2020.\n\n\nPlease let us know if anything else is unclear."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700115962451,
                "cdate": 1700115962451,
                "tmdate": 1700355378145,
                "mdate": 1700355378145,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LEq3Vow84G",
            "forum": "Oe4XdjTP4y",
            "replyto": "Oe4XdjTP4y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2925/Reviewer_Hrxc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2925/Reviewer_Hrxc"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an approach enabling a user to locally compute a lower bound on the privacy provided through an approach such as gradient randomized response, which is a mechanism ensuring local differential privacy in stochastic gradient descent. More precisely, the setting considered is that of federated learning and the objective is to be able to audit the privacy guarantees, thus obtaining a lower bound on epsilon through the use of privacy attacks conducted on the client-side. The approach proposed can also be adapted to the privacy via shuffling technique."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors have done a good review of the related work on privacy auditing and clearly position their work with respect to the state-of-the-art. They have also clearly explained how the LDP-SGD algorithm works and conduct a thorough analysis of the worst-case attack. I have particularly appreciate Figure 3, which clearly illustrates the impact of the privacy parameter on the distinguishability of gradients. \n\nOne of the main contribution of the paper is the proposition of five crafting algorithms for the auditing phase. These approaches have been tested with a wide range of datasets. The results obtained clearly demonstrate that some of these approaches provide non-trivial lower bound when epsilon is large."
                },
                "weaknesses": {
                    "value": "Although Table 2 aims at classifying the different attacks proposed in terms of adversary power, this issue should be discussed more in the paper. In particular, it is not clear for me about all the assumptions that are needed in practice for these attacks to be implemented in real-life. It would be great of the authors could expand a bit more on these aspects. The impact on the utility of the model of these different approaches should also be discussed. \n\nWith respect to the experiments conducted, more details are needed to be able to understand them. For instance, in Figure 4 for the collusion attack it seems that the measured privacy is above the theoretical one for FEMNIST and CelebA, which seems strange. In addition, in Table 3, the measured epsilons seem to be too low to provide a meaningful theoretical guarantee. \n\nA few typos :\n-\u00ab\u00a0our proposed privacy test has a novelty to discuss\u00a0\u00bb -> \u00ab\u00a0one of the novelty of our proposed privacy test is that it discusses\u00a0\u00bb\n-\u00ab\u00a0We utlize the state-of-the-art privacy\u00a0\u00bb -> \u00ab\u00a0We utilize the state-of-the-art privacy\u00a0\u00bb\n-\u00ab\u00a0Recuired clients\u00a0\u00bb -> \u00ab\u00a0Required clients\u00a0\u00bb\n-\u00ab\u00a0may still be issues running on a smartphone\u00a0\u00bb -> \u00ab\u00a0may still be issues running it on a smartphone\u00a0\u00bb\n-\u00ab\u00a0poising effects\u00a0\u00bb -> \u00ab\u00a0poisoning effects\u00a0\u00bb"
                },
                "questions": {
                    "value": "Please see the main comments in the weaknesses section.\nOne additional question : How is the reference Evfimievski 2003 also related to local differential privacy as cited in the introduction?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2925/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2925/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2925/Reviewer_Hrxc"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2925/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699655610361,
            "cdate": 1699655610361,
            "tmdate": 1699655610361,
            "mdate": 1699655610361,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T2tfz0Y1JI",
                "forum": "Oe4XdjTP4y",
                "replyto": "LEq3Vow84G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2925/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer"
                    },
                    "comment": {
                        "value": "Thank you for your valuable review.\n\nWe address some of your specific concerns below.\n\n>\u00a0Although Table 2 aims at classifying the different attacks proposed in terms of adversary power, this issue should be discussed more in the paper. In particular, it is not clear for me about all the assumptions that are needed in practice for these attacks to be implemented in real-life. It would be great of the authors could expand a bit more on these aspects. The impact on the utility of the model of these different approaches should also be discussed.\n\n\n\n\nWe explain what each attack means in real life.\n\nFirst of all, clients may be restricted from accessing memory even on their own devices.\n\nGradient flip/collusion/dummy assumes that memory access is allowed and that the client can modify the gradient.\nOn the other hand, the benign setting is for the case where no data access is allowed, and label flip is for the case where only the input data to the gradient is accessible.\n\n\n\n\n> With respect to the experiments conducted, more details are needed to be able to understand them. For instance, in Figure 4 for the collusion attack it seems that the measured privacy is above the theoretical one for FEMNIST and CelebA, which seems strange.\n\nWith epsilon=4, the error is 0.84% for a confidence level of 95% and 1000 trials, so there are cases where the empirical value is slightly higher than the theoretical value.\n\nThe more attempts the client makes, the smaller this error will be.\n\n\n\n\n> In addition, in Table 3, the measured epsilons seem to be too low to provide a meaningful theoretical guarantee.\n\nLet us explain the reasons for the discrepancy between theoretical and empirical values.\nThe current theoretical value of privacy amplification by shuffling is still in the process of development and is uncertain.\nTherefore, as more research is done to analyze the theoretical value, it is expected to gradually approach the empirical value.\n\n\nWe apologize for the typo.\n\nPlease let us know if anything else is unclear."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700115261724,
                "cdate": 1700115261724,
                "tmdate": 1700115261724,
                "mdate": 1700115261724,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ocaHJrGTnq",
                "forum": "Oe4XdjTP4y",
                "replyto": "T2tfz0Y1JI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2925/Reviewer_Hrxc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2925/Reviewer_Hrxc"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their answer that have clarified some issues I had. However, based on the many other important points raised by other reviewers, I am not updating my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690159270,
                "cdate": 1700690159270,
                "tmdate": 1700690159270,
                "mdate": 1700690159270,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]