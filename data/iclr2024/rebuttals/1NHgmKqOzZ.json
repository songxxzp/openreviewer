[
    {
        "title": "Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality"
    },
    {
        "review": {
            "id": "0oYgYheJsv",
            "forum": "1NHgmKqOzZ",
            "replyto": "1NHgmKqOzZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6473/Reviewer_xT2K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6473/Reviewer_xT2K"
            ],
            "content": {
                "summary": {
                    "value": "The significant contribution of this work lies in the introduction of Progressive Dataset Distillation (PDD). It is a novel methodology in the field of dataset distillation. PDD effectively addresses the limitations inherent in existing DD works by recognizing that relying solely on a single synthetic subset for distillation does not lead to optimal generalization. \n\nSpecifically, PDD innovatively synthesizes multiple small sets of synthetic images, with each set being conditioned on the knowledge acquired from the preceding sets. The alteranated updated model is then trained on the cumulative union of these subsets. This sophisticated approach results in a noteworthy performance improvement."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea presented in this work is both intriguing and groundbreaking. As far as my knowledge extends, this is the first instance where the synthesis of multiple small sets of distilled images has been proposed. The authors have adeptly addressed the challenge of minimizing bias and ensuring training stability by introducing a conditioning mechanism for each distilled set, based on the knowledge accumulated from the preceding sets.\n\n2. The clarity and coherence of the writing are commendable. The motivation behind the research is robust, and the overall structure of the paper is meticulously organized.\n\n3. The experimental results provided in the paper are highly promising and effectively illustrate the efficacy of the proposed solution."
                },
                "weaknesses": {
                    "value": "N.A."
                },
                "questions": {
                    "value": "Is it possible to provide additional experimental results about \"DM+PDD\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6473/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698594650943,
            "cdate": 1698594650943,
            "tmdate": 1699636724493,
            "mdate": 1699636724493,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RWvTZfF5aw",
                "forum": "1NHgmKqOzZ",
                "replyto": "0oYgYheJsv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6473/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6473/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xT2K"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback on our work and for recognizing its novelty and significance in the field of dataset distillation. We appreciate your comments on the clarity of our writing and the robustness of our research motivation.\n\n**1. Additional Experimental Results on \"DM+PDD\":**\nCertainly! We have conducted experiments with DM on CIFAR-10 with IPC=50. Note that DM does not distill the *training dynamics* but rather enforces the similarity between features extracted from synthetic and real samples by enormous models with *random weights*. This is why we did not consider DM initially in our experiments, as our goal is to capture *longer-term training dynamics on full data*. Our experiments show that for IPC=50, DM hits an accuracy of 63.0% while DM+PDD can reach 63.4%, showing a slight 0.4% improvement in performance. This mild improvement is expected, as DM does not capture any training dynamics of training on full data, and only captures similarity between features based on random weights."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6473/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527524355,
                "cdate": 1700527524355,
                "tmdate": 1700527524355,
                "mdate": 1700527524355,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SdkBvKw41k",
            "forum": "1NHgmKqOzZ",
            "replyto": "1NHgmKqOzZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6473/Reviewer_rp5C"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6473/Reviewer_rp5C"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a dataset distillation method. The core idea is to freeze the previously distilled subset and then only optimize the new subset. This method can be applied to several dataset distillation methods and show marginal improvement over the original method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The overall writing is clear.\n2. The comparison with a few previous is clear to validate the effectiveness of the method.\n3. The explanation looks sufficient."
                },
                "weaknesses": {
                    "value": "1. Some important reference is missing such as DEARM [a]. Since this is the SOTA method in dataset distillation, the authors should cite and compare the paper.\n2. After Eq.4, the authors claim PDD can be used to **any** dataset distillation method. Therefore, it is necessary to do experiment if the PDD can augment DREAM.\n3. The accuracy is not as good as DREAM. For example, on CIFAR-10 IPC-10, DREAM can achieve 69.4% accuracy. But the proposed PDD can just achieve 67.9%. On CIFAR-100 IPC-10, DREAM can achieve 46.8% accuracy. But the proposed PDD can just achieve 45.8%. \n4. Based on question 3, why does the proposed method have bad performance on IPC-10?\n5. The idea is too straightforward. Is it possible to update the previous frozen subset with small learning rate?\n\n\n I will increase the score if the DREAM-related experiments are added.\n\n[a] DREAM: Efficient Dataset Distillation by Representative Matching, ICCV 2023"
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6473/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6473/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6473/Reviewer_rp5C"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6473/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698742953948,
            "cdate": 1698742953948,
            "tmdate": 1700730390923,
            "mdate": 1700730390923,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "r5sajaXdV5",
                "forum": "1NHgmKqOzZ",
                "replyto": "SdkBvKw41k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6473/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6473/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rp5C"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's feedback and recognition of the clarity and effectiveness of our method. We address the specific concerns and suggestions as follows:\n\n**1. Missing Reference to DREAM [a]:**\nThank you for pointing out the missing reference. However, we would like to highlight that our submission to ICLR is concurrent with the public release (Aug 2023) of DREAM\u2019s ICCV version. This timing meant we were not aware of the advancements presented in DREAM by the time we submitted our work. \n\nPer your request, we have:\n1. included and briefly discussed DREAM [a] in the revised manuscript (in Section 2; updates are highlighted in pink). \n2. conducted experiments using DREAM [a] as the base method on CIFAR-10 (please refer to Question 2, and also the appendix). \n\nHowever, this timing and the short period of rebuttal still result in a constraint on our ability to conduct additional experiments on CIFAR-100 and Tiny-ImageNet. We are committed to extending our research and will diligently work to incorporate these datasets into our analysis. We will share these expanded results as soon as they are available. \n\n[a] DREAM: Efficient Dataset Distillation by Representative Matching, ICCV 2023\n\n**2. Experiment with DREAM as the Base Method:**\nCertainly! We have conducted a set of experiments with DREAM as the base method on CIFAR-10 with IPC values of 10 and 50, respectively. The results are summarized in the table below. Using their provided implementation, we observed a slightly lower performance of DREAM, compared to what is reported in their paper (which might be due to the difference between computational devices and environments). However, using the same implementation and computational framework, we see that PDD can further improve the performance of DREAM beyond the current SOTA. These results have also been included in the Appendix and will be further incorporated into the main text. \n\n| Method | IPC=10 | IPC=50 | \n| :-: | :-: |  :-: |\n| Accuracy of DREAM (reproduced) | 68.7 | 74.8 | \n| Accuracy of DREAM + PDD | 69.3 | 76.7 | \n| Improvement of PDD | 0.6 | 1.9 |\n| Standard Deviation | 0.3 | 0.2 | \n\n**3 & 4. Performance Comparison with DREAM**\nWe respectfully disagree that our method has a poor performance when IPC equals 10 due to the following reasons:\n\n1. Firstly, we argue that it is not a fair comparison to contrast the accuracy of DREAM with the numbers in Table 1, as they deploy MTT or IDC as base methods. \n2. Secondly, our method consistently improves the performance of base methods with substantial performance gaps. Specifically, PDD substantially improves the performance of IDC and MTT on CIFAR-10 by 0.4% and 1.6%, respectively. These improvements in performance are also acknowledged by Reviewer xT2K and S7ej. \n3. Moreover, it is important to note that we only observe a smaller improvement for the combination of PDD and IDC on CIFAR-10 when IPC equals 10. This is likely due to IDC\u2019s inability to distill longer training trajectories on real images (merely 4 epochs). Consequently, even extending to multiple stages is not enough for distilling more difficult images. For example, setting 5 stages approximately distills merely 20 epochs on real images, which is deemed insufficient. This is likely a limitation embedded in IDC, yet not in our PDD method. \n4. Finally, the additional experiment results we provided in Question 2 show that PDD can also improve the performance of DREAM on CIFAR-10 with IPC=10 and 50.  \n\nFinally, we reiterate that our contribution lies in the revelation that a multi-stage synthesis approach can enhance the ability of base methods to capture training dynamics more effectively when compared to their single-stage counterparts. While distilling different subsets for different stages of training may seem intuitive, **it has not been previously explored**. **We are the first to introduce this multi-stage synthesis framework, which is acknowledged as \u201cintriguing and groundbreaking\u201d by Reviewer xT2K and \u201cinteresting\u201d by Reviewer S7ej.** \nMore importantly, our framework not only improves performance on standard IPCs, such as 10 and 50, but also represents **a unique advance by enabling the generation of a large number of samples for the first time**. This can equip any base method with the capability to significantly reduce the performance gap compared to training with complete datasets."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6473/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527444881,
                "cdate": 1700527444881,
                "tmdate": 1700527444881,
                "mdate": 1700527444881,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nX1LQJ4WzE",
                "forum": "1NHgmKqOzZ",
                "replyto": "SdkBvKw41k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6473/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6473/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rp5C (cont'd)"
                    },
                    "comment": {
                        "value": "**5. Idea of Updating Previous Frozen Subset:**\nWe emphasize that our approach, while simple, is demonstrably effective and can be seamlessly integrated with any base method to enhance performance and scalability. This strength has been acknowledged by Reviewer S7ej.\n\nRegarding the suggestion to update the previously frozen subset with a small learning rate, it is important to note that such an approach would void the scalability (i.e. generating larger synthetic data) enabled by our method since the samples distilled at previous stages need to be further optimized at later stages, requiring additional computational cost and memory consumption. \n\nNevertheless, we have conducted an experiment that allows images synthesized at early stages to be updated during late stages with a smaller learning rate (i.e., 10% of the original learning rate). We choose MTT as the base method and set up $5$ stages with a per-stage IPC of $10$, resulting in a total IPC of $50$. The trained model hits a test accuracy of 69.1%, which is inferior to our proposed method. \n\nThe decline in performance can likely be attributed to the fact that expert trajectories, i.e., the subsequent model checkpoints obtained by training on the full real dataset and synthetic samples (if applicable, from previous stages), are not derived from a consistent set of synthetic samples. Note that we first train on synthetic examples produced in previous stages to generate expert trajectories and synthesize data for the next stage. If the synthesized images of the previous stages are updated, the distilled samples from stage 1, which serve as the foundation for generating trajectories in stages 2 to 5, vary with each subsequent stage. When the samples synthesized in stage 1 are updated in stage 2, they gradually deviate from the initial synthetic samples (i.e., those employed to generate the expert trajectories for stage 2). Hence, training on the synthesized images that are being updated results in different training dynamics each time. This eventually leads to discrepancy and ultimately results in a decrease in performance. The above discussion has been incorporated into our manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6473/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527490454,
                "cdate": 1700527490454,
                "tmdate": 1700527490454,
                "mdate": 1700527490454,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CjTvbK6BXs",
                "forum": "1NHgmKqOzZ",
                "replyto": "r5sajaXdV5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6473/Reviewer_rp5C"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6473/Reviewer_rp5C"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your effort in responding.\n1. DREAM [a] was publicly released on Feb. 2023 on Arxiv and was also open-sourced then. This is half a year before their ICCV version.\n2. The authors fail to show the experiments on CIFAR-100 and ImageNet. So, I am slightly doubtful about the proposed method's scalability.\n3. The improvement over DREAM is marginal\u2014just 0.6% on IPC-10. It shows the power of the proposed method to some extent. \n4. If the authors provide accuracy improvement on **CIFAR-100 on IPC-10**, which does not require much computation resources. I will raise my score to 5.\n\n\n\n[a] DREAM: Efficient Dataset Distillation by Representative Matching, ICCV 2023"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6473/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535633972,
                "cdate": 1700535633972,
                "tmdate": 1700535633972,
                "mdate": 1700535633972,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ziDCLt96X9",
                "forum": "1NHgmKqOzZ",
                "replyto": "QswbIBPALL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6473/Reviewer_rp5C"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6473/Reviewer_rp5C"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. I will raise my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6473/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730357771,
                "cdate": 1700730357771,
                "tmdate": 1700730357771,
                "mdate": 1700730357771,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Dkcj1MvGm1",
            "forum": "1NHgmKqOzZ",
            "replyto": "1NHgmKqOzZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6473/Reviewer_S7ej"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6473/Reviewer_S7ej"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a dataset distillation algorithm that generates synthetic data in a progressive manner: the next batch of synthetic data would be dependent on previous batches. The training using the distilled datasets also contains several stages. The training data for each stage come from the corresponding batch of the distilled dataset and its previous batches in a cumulative way. Experiments demonstrate that the proposed strategy improves the baseline method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of progressive dataset distillation is interesting. This can 1) capture the training dynamic of neural networks better as demonstrated by authors, 2) reduce the complexity of training the whole synthetic dataset together, and 3) serve as a strong method for slimmable dataset condensation [a].\n\n[a] Slimmable Dataset Condensation, CVPR 2023."
                },
                "weaknesses": {
                    "value": "1. Many places are unclear.\n    * Fig. 1 needs some explanations. How do the results come in detail, like what's the IPC of each stage, and how to conduct multi-stage training? Although some of these questions are answered in the following parts, the writing is not coherent.\n    * The networks for the next stage come from the training results with previous batches of synthetic data. In IDC, the networks are periodically re-initialized randomly. In MTT, the networks come from checkpoints of training with original datasets. We have to conduct some modifications to these baselines before using PDD. These operations are unclear.\n2. A comment: this method makes an assumption on downstream training using synthetic datasets: models must be trained in a multi-stage way using the provided multi-stage synthetic data, which would introduce a lot of hyperparameters, especially in the cross-architecture setting and make the dataset distillation less elegant. Given that the performance gain is not significant in most cases, the practical value of the proposed method is somewhat limited.\n3. Through the results in Tab. 6, the effect of discarding easy-to-learn examples at later stages is not significant. More evidence is necessary to demonstrate the effectiveness."
                },
                "questions": {
                    "value": "Please refer to the Weaknesses part. I would also like the authors to discuss more benefits of PDD as mentioned in the 1st point of Strengths in the camera-ready version or in the next revision cycle."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6473/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6473/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6473/Reviewer_S7ej"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6473/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828710305,
            "cdate": 1698828710305,
            "tmdate": 1700625341970,
            "mdate": 1700625341970,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P3WQ4QArYQ",
                "forum": "1NHgmKqOzZ",
                "replyto": "Dkcj1MvGm1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6473/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6473/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer S7ej"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the constructive feedback. We address your concerns point-to-point below. \n\n**1. Further explanation:**\nWe\u2019d like to note that the details of our method are accurately specified in the paper. We will further elaborate below:\n\n*1.1. Explanation of PDD for Figure 1:*\nThank you for your feedback regarding Figure 1. This Figure is mainly the motivation of our method, and hence the details are specified later. To enhance the clarity, we have updated the caption of Figure 1 to: \u201cOur proposed multi-stage dataset distillation framework, PDD, improves the state-of-the-art algorithms by iteratively distilling smaller synthetic subsets that capture longer training dynamics on full data. In the setting shown in the figure, PDD uses MTT as its base distillation method to incrementally generate $5$ synthetic subsets of size $10$, where each subset captures $15$ epochs of training on full data.\u201c \n\n*1.2. Clarification about Base Method Modification*\nWe clarify that PDD seamlessly integrates with the base methods with hardly any modification and without introducing any additional hyperparameters (which is why our paper does not include discussion on extra hyperparameters):\n \nFor the first stage, we distill datasets using the base methods in their original form. \nStarting from the second stage, we distill with the same base algorithm. However, the model weights employed in these stages are those that have been trained on examples distilled in the earlier stages. For instance, we train the newly sampled networks with previous synthesized samples for IDC, and we start new collections of training trajectories from weights trained with previous samples for MTT. \n**These operations are detailed in our pseudocode and also formulated in Equation (4)**. This ensures a smooth integration of PDD with existing frameworks, enhancing their effectiveness without the need for fundamental modifications to the base methods.  \n\n**2. Practicality of Multi-Stage Training:**\nOur method does not introduce additional hyperparameters: IPC per stage and the number of stages are determined based on the available budget, and PDD inherits all the rest of hyperparameters, such as learning rate, weight decay, number of training steps, etc., from the base method. This ensures that the practicality of our method is maintained by leveraging existing parameters, thereby simplifying the adoption and integration of PDD in various settings.\n\nFurthermore, we wanted to emphasize that our method brings significant performance gain for the following reasons: \n1. Our results have already outperformed current state-of-the-art methods in multiple scenarios, even when using less powerful base methods. For instance, on CIFAR-10 with an IPC of 50, our approach combining PDD and IDC surpasses DREAM, the current SOTA by 1.7%. Similarly, on CIFAR-100 with an IPC of 50, PDD + IDC outperforms DREAM by 0.5%. Notably, PDD can be also easily combined with DREAM to improve its performance and achieve SOTA, as we will report in our response to reviewer rp5C.\n2. Across other settings, our methods consistently enhance the performance of the base methods, approaching the performance level of the current state-of-the-art benchmarks. \n\n**3. Discarding Easy-to-Learn Examples:**\nDiscarding difficult-to-learn examples is very effective in improving the efficiency of our method, and also improves its performance for smaller values of P. Table 6 shows that around 2/3 of examples can be discarded in the first stage and around 2/3 of examples can be discarded in the second and third stages. That is, only 1/3 of the (easiest-to-learn) data is required for the first stage and another 1/3 (easy but not easiest examples) is required for the second and third stages. Discarding difficult-to-learn examples not only considerably improves the distillation efficiency, but also improves the performance of dataset distillation, especially in earlier stages. This is because easy-to-learn examples contribute the most to learning in the early stages. Hence, focusing the distillation on such examples in earlier stages improves both the efficiency and the quality of the generated synthetic examples, as is evident in Table 6, P=1 and P=3."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6473/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527301501,
                "cdate": 1700527301501,
                "tmdate": 1700527301501,
                "mdate": 1700527301501,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z2t1YbFitu",
                "forum": "1NHgmKqOzZ",
                "replyto": "Dkcj1MvGm1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6473/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6473/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer S7ej (cont'd)"
                    },
                    "comment": {
                        "value": "**4. More Discussion on Benefits of PDD:**\nThank you for the suggestion to elaborate on the benefits of PDD in our manuscript. In the revised version, we discussed the advantages of our method again in both the introduction and conclusion sections (highlighted in pink). Specifically, our original introduction highlights that PDD captures the training dynamics of neural networks over longer intervals more effectively. We have added the following suggestion by the reviewer to our introduction: Importantly, PDD reduces the complexity associated with training the entire synthetic dataset simultaneously, and can serve as an effective base method for slimmable dataset condensation [a] which handles changes in the budget for storage or transmission.\n\nWe hope that these revisions and clarifications will address the concerns raised and demonstrate the value of our proposed PDD method. We appreciate the opportunity to improve our manuscript and thank the reviewer for their valuable input.\n\n[a] Slimmable Dataset Condensation, CVPR 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6473/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527335389,
                "cdate": 1700527335389,
                "tmdate": 1700527335389,
                "mdate": 1700527335389,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CljwosGeF0",
                "forum": "1NHgmKqOzZ",
                "replyto": "P3WQ4QArYQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6473/Reviewer_S7ej"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6473/Reviewer_S7ej"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the response to my concerns. I think this time the contribution and technical details are better clarified. Most of my concerns are alleviated. Currently, I still think Fig. 1 is misleading because the total IPC of PDD is 50 but the baseline is 10, which is an unfair comparison. It would be better if the authors could also mark the accuracy of 20, 30, 40, and 50 IPC on the plot. At least 50 IPC is necessary for a fair comparison.  \n\nOverall, I choose to raise my score to 6."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6473/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625324317,
                "cdate": 1700625324317,
                "tmdate": 1700625324317,
                "mdate": 1700625324317,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]