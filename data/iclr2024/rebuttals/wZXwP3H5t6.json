[
    {
        "title": "Faster and Accurate Neural Networks with Semantic Inference"
    },
    {
        "review": {
            "id": "l4BtZRr6gQ",
            "forum": "wZXwP3H5t6",
            "replyto": "wZXwP3H5t6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6252/Reviewer_wsaJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6252/Reviewer_wsaJ"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method to make DDN inference more efficient by selecting for each input, using the features of early layers, which units to use in the rest of layers.\nThey do so by finding clusters in the training dataset such that each cluster is assigned a subnetwork within the whole DNN.\nAt inference time, a sample is assigned to a cluster and, thus, to a subnetwork.\nThe result show that a substantial reduction in inference time can be obtained while sacrificing some accuracy on CIFAR100 using VGG and ResNet DNNs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "With the activations in the high-level layers of DNNs (and not so much the lower-level ones) known to be sparse, it seems reasonable to perform predictive pruning conditioned on the low-level activation of a sample."
                },
                "weaknesses": {
                    "value": "Although I understand the motivation and the gist of the method (Fig 3 conveys it quite well), I struggled to follow some of the details. These are my main issues with the paper:\n1- In Eq 1, is L_eval is a loss (where lower is better), it seems trivial to find subnetworks that perform less well than the original DNN. So I imagine L_eval is not a loss (as the L would suggest), but some performance metric where higher is better. Even in that case the problem is not fully defined, since F_yi = F would satisfy Eq 1 while not being an useful solution.\n2- I find the writing of the paper very hard to follow. How are the semantic clusters built? I\u2019m not sure I could follow the explanation in section 4 and algo 1, which is a bit too cluttered. Something as important for the method as the Discriminative Capability Score is not actually explained anywhere. Same issue with the training of the route predictor.\n3- Even in section 3, which introduces the main setting of the methodology, the authors already mention many elements that would rather pertain to the experimental section, like specific architectures and hyperparameter choices.\n4- The authors claim to obtain SOTA results while using relatively old datasets and architectures. I don\u2019t think such a choice invalidates the contribution, but I would weigh the claims accordingly.\n5- What is Fig 1top supposed to convey? It doesn\u2019t seem to show anything. If the idea is to show the difference in sparsity, a simply plot with the sparsity level of each layer would work better."
                },
                "questions": {
                    "value": "Overall, I think it is likely that the contribution of this paper could be valuable, but the presentation and understandability needs to be substantially improved.\n\nThe authors would probably be interested in this paper:\nYe, Mao, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, and Qiang Liu. \"Good subnetworks provably exist: Pruning via greedy forward selection.\" In International Conference on Machine Learning, pp. 10820-10830. PMLR, 2020."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6252/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6252/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6252/Reviewer_wsaJ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6252/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697914958101,
            "cdate": 1697914958101,
            "tmdate": 1700746360991,
            "mdate": 1700746360991,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2OHFdF3rEu",
                "forum": "wZXwP3H5t6",
                "replyto": "l4BtZRr6gQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6252/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6252/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> In Eq 1, is L_eval is a loss (where lower is better), it seems trivial to find sub-networks that perform less well than the original DNN. So I imagine L_eval is not a loss (as the L would suggest), but some performance metric where higher is better. Even in that case the problem is not fully defined, since F_i = F would satisfy Eq 1 while not being a useful solution.\n\n We thank the reviewer for this valuable comment. Yes, the evaluation metric shown in Semantic DNN Subgraph Problem (SDSP) does not correspond to the loss function. We define it as the evaluation criterion used for scoring the network and its subgraphs on their respective datasets. In our work, we have chosen the evaluation metric as accuracy. We understand the confusion regarding the notation which would not arise if the property of the evaluation metric (higher value means better performance) was mentioned. We have updated the manuscript accordingly. We have changed the notation of the evaluation criterion to $\\mathcal{B}_{eval}$ and added \n\n*A higher value of $\\mathcal{B}_{eval}$ is assumed to correspond to better performance*. \n\nAs mentioned in the comment, $\\mathcal{F}_{\\gamma_i} = \\mathcal{F}$ would satisfy Equation 1. In our case, the problem definition would be more precise if we used the word *proper subgraph*. This would exclude the trivial solution and would solve the confusion. We have modified the problem statement to incorporate this discussion.\n> I find the writing of the paper very hard to follow. How are the semantic clusters built? I\u2019m not sure I could follow the explanation in section 4 and algo 1, which is a bit too cluttered.\n\nWe are sorry that Section 4 and Algorithm 1 have been hard to follow. We have updated the manuscript and tried to make the steps easier to follow. We hope that it will clarify the questions that the reviewer might have regarding the procedure.\n> Something as important for the method as the Discriminative Capability Score is not actually explained anywhere.\n\nSection 4 describes the procedure to obtain the discriminative capability score. The section builds up the intuition behind the discriminative capability score and describes it right before the equation 3. We have updated the manuscript to make it easy to follow the procedure and find the description.\n> Same issue with the training of the route predictor**\n\nWe have added more details to the *Semantic Route Predictor* subsection of Section 5 to illustrate the training of the semantic route predictor. Specifically, we add that \n\n*To train the auxiliary classifier $\\boldsymbol{\\chi}$, the section of the base model up to the $M-1$-th layer of $\\mathcal{F}$ is frozen and the classifier is trained in supervised fashion using {$\\mathbf{A}_{M-1}^j, \\gamma_m^j$} , where  j=1...|D| as the dataset.*\n\n*Here, $\\mathbf{A}_{M-1}^j$, and $\\gamma_m^j$ are respectively the activation of the $M-1$-th layer of the base model and the ground truth semantic cluster for the j-th sample. As we are considering a pre-trained base DNN, we train the auxiliary classifier separately from the base network using the activations obtained from the $M-1$-th layer.*\n\n> The authors claim to obtain SOTA results while using relatively old datasets and architectures. I don\u2019t think such a choice invalidates the contribution, but I would weigh the claims accordingly.\n\nWe thank you for your comment. The setting we are proposing is new and it requires a score-based approach to select the filters in each layer to form the subgraph. As a result, we have chosen the [1], [2] for comparison as recent methods. Apart from that, we compare with the recent [3] which is a published work in ICLR 2023 to prove the efficacy of DCS as a pruning method. We are currently running SINF on ImageNet-1k and will report the results as soon as we obtain them.\n\n[1] Sui, Y., Yin, M., Xie, Y., Phan, H., Aliari Zonouz, S. and Yuan, B., 2021. Chip: Channel independence-based pruning for compact neural networks. Advances in Neural Information Processing Systems, 34, pp.24604-24616.\n[2] Lin, M., Ji, R., Wang, Y., Zhang, Y., Zhang, B., Tian, Y. and Shao, L., 2020. Hrank: Filter pruning using high-rank feature map. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 1529-1538).\n[3] Murti, C., Narshana, T. and Bhattacharyya, C., 2022, September. TVSPrune-Pruning Non-discriminative filters via Total Variation separability of intermediate representations without fine tuning. In The Eleventh International Conference on Learning Representations."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700421564403,
                "cdate": 1700421564403,
                "tmdate": 1700421564403,
                "mdate": 1700421564403,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FGCapLhGQf",
                "forum": "wZXwP3H5t6",
                "replyto": "l4BtZRr6gQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6252/Reviewer_wsaJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6252/Reviewer_wsaJ"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their responses. I would like to clarify and follow up on some of the issues.\n\n- Eq (1): I'm not sure writing \"a proper subgraph\" solves the issue. If I understand well an efficiency is the main objective, it would make more sense to aim at minimizing the number of nodes in each $\\mathcal{F}_{\\gamma_i}$ subject to Eq (1). I would suggest to revise Section 3 accordingly.\n- Clarity: I apologize if my comment was unclear. I meant that DCS is not introduced as a high-level concept, and its connection to solving Eq (1) is not explained in an intuitive manner, with Algo. 1 being so cluttered it makes it very hard for a reader to understand the gist. I would strongly suggest the authors to revise the whole paper to allow for a better flow, such that every section/paragraph leads naturally to the following. Similarly, Fig 1 needs to be improved as per my comment in the previous round."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642338894,
                "cdate": 1700642338894,
                "tmdate": 1700642421509,
                "mdate": 1700642421509,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "66PpDJHb97",
            "forum": "wZXwP3H5t6",
            "replyto": "wZXwP3H5t6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6252/Reviewer_n596"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6252/Reviewer_n596"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces Semantic Inference (SINF), a novel method for drastically reducing the inference complexity of DNNs. SINF is based on the assumption that semantically similar inputs should share a significant number of filter activations; these semantically similar inputs can be viewed as \"clusters\". In turn, appropriate cluster-specific sub-graphs of the base network can be detected and then executed for inference based on the assigned cluster of each object. To do so, the considered approach introduces Discriminative Capability Score (DCS)  , a general purpose method aiming to find filters that can distinguish between semantically similar classes. In experimental evaluations on the CIFAR-10/100 datasets using different benchmark architectures (VGG16, VGG19, RN50), DCS yielded significant improvements compared to other SOTA methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces a novel approach for using sub-graphs of the network to perform inference based on a partition of the inputs via semantic similarity. The idea is very interesting and intuitive, potentially contributing to a different inference methods for DNNs."
                },
                "weaknesses": {
                    "value": "Despite the novelty of the approach, there are some significant issues that need to be addressed, and the main element missing from the main text, at least in my personal view, is clarity. Up until (and including) Section 3, the paper is well-written and easy to follow. The intuition and motivation are very clear. \n\nAfter that, the presentation of the method is very confusing. \n\nWe begin with the definition of the DCS procedure, where the notation keeps changing. The authors start with a given layer $l$ and semantic cluster $\\gamma_m$ (where up to this point, it is not clear if $\\gamma_m$ comes from some ground truth information or is computed as the introduction suggested). For each datapoint belonging to the semantic cluster, the activations for each feature map $\\boldsymbol A_{l, c_i}^j$ are computed and adaptive pooling is perfomed. Then, the feature map is flattened by the dependence on $l$ is gone. This is the first instance that breaks the flow of reading the paper. This trends continues by introducing $\\boldsymbol F^j$ which concatenates all the features for all the filters in the layer and $N_f$ is introduced (again layer-specific with no dependence). The same goes for $\\boldsymbol W*$ that seems to be dependent on both $\\gamma_m$ and $l$. Using some normalization, we then compute the desired DCS value, which again is layer and cluster specific but this is not clear from the notation. \n\nThen, the SINF procedure is presented. The authors present the steps, most of which are not yet introduced. It would be better to first introduce the procedures and then tie them together, instead of noting that everything will be explained later. \n\nHere, the authors note that before deployment, the DCS is used to construct semantic subgraphs. Again is not yet clear how are the partitions obtained. Do the authors use the training set of CIFAR-10/100 that are already somehow prepartitioned into semantic clusters, e.g., the superclasses of CIFAR-100, and use algorithm 2 to extract the subgraphs? Because this is the first step of the approach, that always takes as an input the Partitioned Data. \n\nMoving on to the process of extracting the sub-graphs, we begin with $L$ and $M$, the last layer and the layer before the Common Feature Extractor. In this context, if $L$ is the last layer of (assuming) the backbone network, isn't $M$ also the layer of said network? In Fig.3 there is no illustration of what $L$ and $M$ since they backbone is not shown. The input is just passed through the common feature extractor and a sub-graph is selected without a corresponding backbone. \n\nThe authors use $X$ to denote the number of retained filters in a layer $X$. Again confusing notation, $X$ are the data, and even though it's easy to understand the difference, clarity is reduced. The authors can just use $r_l$ for any layer $l$. The authors note that for each layer $M \\leq l \\leq L$ the $r_l$ is calculated; however, in Algorithm 2, $r_L$ and $r_M$ are given as an input and also updated in the process. At the same time the authors note \"This procedure is performed for different values of $r_L$ and $r_M$. In this paper, $r_L$ is set between 90% and 10%, with steps of 10, while $r_M$ is set between 10% and 1%, with steps of 2\". Why is this procedure performed? Is this some kind of initialization? What happens with the multiple different values? Do the authors select some kind of best performing model? And in this context, what is the intuition behind the computation of $r_l$? It seems to be dependent on the current layer number $M, L, r_M$, and  $r_L$ but how the values affect the results is not clear. \n\nTurning to the classifier, the authors note that the classifier is attached to some $l$-th layer. This layer is the earliest layer that provides good prediction for semantic routes is chosen. What does it mean that the layer provides good predictions for semantic routes? Do the authors try to match the output of this classifier to the potential $K$ clusters (which at this point I will assume stem from ground truth information)? How was the $75$% accuracy was decided? What if for some setting the $l$-th layer is the last layer of the network? What is the impact of different values for this threshold? \n\nIn the experimental section, the authors introduce the notation for the confidence threshold as $\\gamma$, which is also used for the partitions $\\gamma_i$. Again, even though it is something distinct, notation clarity should be improved. \n\nOther points:\n\nIn the context of pruning methods, there exist a plethora of methods that perform pruning end-to-end during training, balancing accuracy and sparsity. Characteristic examples include [1,2,3]. However, one argument that the authors make is that all the (referenced) methods require fine-tuning after pruning, which may be true for the methods that the authors cite but not for all pruning methods. Unless the authors refer to post-hoc pruning methods, in which again there exist methods that consider solvers to balance accuracy and sparsity without any further fine-tuning [4].\n\nI find the definition of the Semantic DNN Subgraph Problem a bit misdefined. The equation itself seems to indicate that the average loss of the partitions should be less or equal to the full network loss; this however can be true for any configuration of the sub-graphs (even random) and restricts solutions that can even perform better, e.g., due to avoiding parameter redundancy. The authors note that $\\mathcal{L}$ is the evaluation metric, but the $\\mathcal{L}$ notation is commonly the loss function in the literature and not the classification accuracy that the authors consider. And in the regression setting, e.g., using MSE as an evaluation metric leads to a different interpretation of the problem setting.\n\nThe fact that the evaluation is only assessed on CIFAR-10/100 undermines the impact of the approach. Indeed, these datasets are considered \"small\" datasets and the authors should assert the generalization capabilities and performance on more demanding datasets and settings, such as ImageNet-1k. At the same time, evidently the algorithms depends on ground truth information about the semantic clusters of the data. If we do not have access to this kind of information, it's not clear: (i) what semantic clusters we should consider, and (ii) how will the algorithm perform in this setting. \n\nOverall, despite the novelty of the proposed framework, presentation and clarity should be greatly improved. At the same time, important  details (like the $r_l$ computation) are not justified, while important ablation studies in the various effects of parameters/settings are missing. \n\n[1] Louizos et al., Bayesian Compression for Deep Learning, In Proc. NIPS 2017\n\n[2] Panousis et al., Nonparametric Bayesian Deep Networks with Local Competition, In Proc. ICML 2019 \n\n[3] Neklyudov et al., Structured bayesian pruning via log-normal multiplicative noise, In Proc. NIPS 2017 \n\n[4] Wong et al., Leveraging sparse linear layers for debuggable deep networks. In Proc. ICML, 2021."
                },
                "questions": {
                    "value": "Please see the Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6252/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6252/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6252/Reviewer_n596"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6252/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698095217859,
            "cdate": 1698095217859,
            "tmdate": 1699636683984,
            "mdate": 1699636683984,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cZRftljBrt",
                "forum": "wZXwP3H5t6",
                "replyto": "66PpDJHb97",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6252/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6252/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> The authors start with a given layer $l$ and semantic cluster $\\gamma_m$ (where up to this point, it is not clear if $\\gamma_m$ comes from some ground truth information or is computed as the introduction suggested [...] the authors note that before deployment, the DCS is used to construct semantic subgraphs. Again is not yet clear how are the partitions obtained.\n\nThank you for your comment. In Section 3, we have stated that we assume that the $\\gamma_m$ clusters are defined based on application-level similarities (e.g., classes related to flowers, insects, etc.) or pre-defined at the dataset level (e.g., as in the CIFAR100 dataset). In this case, the 20 superclasses in CIFAR100 form the semantic clusters we are considering. We are using this dataset to experiment on especially because it is pre-partitioned into semantic clusters. We have remarked this aspect in the updated manuscript we have submitted as part of the rebuttal.\n\n> For each data-point belonging to the semantic cluster, the activations for each feature map $A_{l,c_i}^j$ are computed and adaptive pooling is performed. Then, the feature map is flattened by the dependence on $l$ is gone.\n\nWe are sorry to see that the notation might have been confusing. Algorithm 1 is designed for a specific cluster and a specific layer and it is specifically mentioned in the algorithm heading. That is why the dependence on the layer and cluster is not explicitly mentioned for the feature vector and subsequent symbols. The dependence is maintained in the notation for the other quantities before so as not to disrupt the already defined quantities. However, we agree that keeping the dependence on the layer and semantic cluster can make it easier to follow the algorithm. We have made changes to reflect the dependence on layer and semantic cluster in the manuscript and we have reuploaded it for the reviewer\u2019s convenience.\n> Moving on to the process of extracting the sub-graphs, we begin with $L$ and $M$, the last layer and the layer before the Common Feature Extractor. In this context, if $L$ is the last layer of (assuming) the backbone network, isn't $M$ also the layer of said network? In Fig.3 there is no illustration of what $L$ and $M$ since they backbone is not shown. The input is just passed through the common feature extractor and a sub-graph is selected without a corresponding backbone.\n\nThe $L$ and $M$ are part of the same backbone network. As defined in Section 3, we extract the sub-graph from a pre-trained network. For the sake of adaptive inference, we keep the first part as a common feature extractor and the rest of the network is used for the specialization for different semantic clusters. In this case, $L$ is the last layer of the backbone network and $M$ is the layer just before the common feature extractor from the end.\n\nThe first $l$ layers of the backbone network are frozen and used as the common feature extractor. The motivation is that the earlier layers of a DNN focus on features which overlap among multiple classes and possibly semantic clusters. From that perspective, $M$ would be the $l+1\\ th$ layer of the back-bone and $L$ would be the last layer as mentioned. Through experimentation, we have found that at least the first 5 convolution layers are required to obtain a reasonable prediction accuracy on the semantic clusters for VGG16 and VGG19 and the first 2 layers are required for ResNet50. Therefore, the value of $M$ is 6 for VGG models and 22 for ResNet50. We have updated the manuscript clarifying the relation between the base DNN, the common feature extractor and the extracted sub-graph.\n\n> The authors note that \"This procedure is performed for different values of $r_L$ and $r_M$. In this paper, $r_L$ is set between 90\\% and 10\\%, with steps of 10, while $r_M$ is set between 10\\% and 1\\%, with steps of 2\". Why is this procedure performed? Is this some kind of initialization? What happens with the multiple different values? Do the authors select some kind of best performing model? \n\nWe thank the reviewer for pointing this out and we have fixed the notation in the updated version of the manuscript we have uploaded as part of the rebuttal.\n\nThe procedure of extracting sub-graph is performed for varying values of $r_L$ and $r_M$. This is because there can be multiple sub-graphs that satisfy our accuracy requirement. However, those sub-graphs will vary in their size and latency. By trying  different values of $r_L$ and $r_M$, we can obtain several solutions satisfying our performance requirement and choose the optimum one based on additional requirements, for example, sub-graph size, latency. In other words,  we are selecting the best performing DNN."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700432538391,
                "cdate": 1700432538391,
                "tmdate": 1700432538391,
                "mdate": 1700432538391,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lhtXsV9hg4",
            "forum": "wZXwP3H5t6",
            "replyto": "wZXwP3H5t6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6252/Reviewer_NR9c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6252/Reviewer_NR9c"
            ],
            "content": {
                "summary": {
                    "value": "1) Key ideas: the submitted paper studies the problem of pruning approaches of deep neural networks, with a focus on searching cluster-specific subgraphs for inference (without retraining or fine-tuning) that faster neural network without drastic accuracy loss. \n\n2) Contributions: The authors propose a new inference framework to divide the DNN into subgraphs according to the semantic cluster the object belongs to. This process is complemented by common feature extractor, semantic route predictor and feature router modules.  Additionally, a new discriminative capability score (DCS) is proposed to find the subgraphs, which is also applied as a pruning criterion and achieve state-of-the-art performance.\n\n3) Their significance: the most significant contribution is the results: 1. DCS outperforms existing state-of-the-art discriminative algorithms and pruning criterion. 2. SINF reduces the inference time with limited accuracy loss and showing significant improvement considering per-cluster accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper shows impressive results of reducing inference time with limited accuracy loss (Figure 4 and Figure 6) and the proposed DCS shows state-of-the-art performance (Figure 5 and Table 1).\n2. Visualization and quantization analysis and verification have been conducted on the proposed Semantic DNN Subgraph Problem. Figure 1 shows intuitive and credible results.\n3. The paper provides detailed descriptions of the proposed core algorithms (how to extract subgraphs for semantic clusters, algorithm2) and theories (how to compute discriminative capability score, algorithm1)."
                },
                "weaknesses": {
                    "value": "1. Lack the comparison with previous quantization and pruning approaches. There are many quantization and pruning approaches that can speed up the inference without fine-tuning or retraining. Can do more comparison experiments with SINF and show the comparable results of the whole pipeline.\n2. Table 1 lack the results of ResNet50 on CIFAR100.\n3. Lack the analysis of the poor improvement in Table1, where there is almost no improvement for VGG19 on CIFAR100 and only a slight improvement on CIFAR10."
                },
                "questions": {
                    "value": "Is there any proof or metric to evaluate the intrinsic redundancy in latent representations? The idea of intrinsic redundancy is interesting but somehow blurred, it will be great if there is any explicit metric can be proposed or discussed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6252/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698853025937,
            "cdate": 1698853025937,
            "tmdate": 1699636683850,
            "mdate": 1699636683850,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "muFrzmgRC8",
                "forum": "wZXwP3H5t6",
                "replyto": "lhtXsV9hg4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6252/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6252/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Regarding Weakness 1\n> There are many quantization and pruning approaches that can speed up the inference without fine-tuning or retraining. Can do more comparison experiments with SINF and show the comparable results of the whole pipeline.\n\nWe thank you for your comment. To the best of our knowledge, the previous work that does not require fine tuning is very limited -- we were only able to find [1] as a recent paper that does not require fine-tuning. We would be happy to compare against other non fine-tuning techniques if the reviewer shared these papers with us. We would like to clarify that *we are not considering pruning at train-time or retraining approaches*. We are assuming that we start with a pre-trained DNN and extract the sub-graphs corresponding to the semantic clusters defined by the task at hand.\n\nFor the extraction of the sub-graph part, we have compared against metrics which are usable in our scenario. For the pruning part, we are considering the most recent work closest to our setting for comparison. Notwithstanding, we point out that designing a pruning algorithm is out of the scope of this paper. Our goal is to design a dynamic DNN which can *immediately* adapt to changing requirements. For this reason, we cannot fine-tune the DNN at run time.\n\nRegarding quantization, as pointed out in the background section (Section II), we believe our proposed work is orthogonal to that approach and can be used on top of quantization and coding approaches to further improve the performance.\n\n[1] Murti, C., Narshana, T. and Bhattacharyya, C., 2022, September. TVSPrune-Pruning Non-discriminative filters via Total Variation separability of intermediate representations without fine tuning. In The Eleventh International Conference on Learning Representations.\n\n# Regarding Weakness 2\n> Table 1 lacks the results of ResNet50 on CIFAR100.\n\nThank you for pointing this out. We are comparing our approach to the work which has been published in the last ICLR 2023 [1] . To make a fair comparison, we are using the same DNN model as in [1]. Since the work does not provide any performance result for ResNet50 on CIFAR100, we are not comparing for the same.\n\n# Regarding Weakness 3\n> Lack the analysis of the poor improvement in Table 1, where there is almost no improvement for VGG19 on CIFAR100 and only a slight improvement on CIFAR10.\n\nWe thank you for the opportunity to clarify this point. A possible explanation is that since both our technique and the state of the art IterTVSPrune have pruned a significant amount of weights \u2013 respectively about 50% and 60% for CIFAR10 and CIFAR 100% \u2013 the DNN has reached a lower bound on its predictive capability. In other words, this means that the DNN cannot be pruned more without compromising the accuracy. In the other DNNs (VGG16 and ResNet50) the amounts of weights pruned is less aggressive (up to 40% for VGG16 and up to 35% for ResNet50) so eventually there could be room for improvement. This aspect is nevertheless intriguing and we plan to delve deeper into this aspect in future work. We will include these discussions in the final manuscript. \n\n# Regarding the Question\n> Is there any proof or metric to evaluate the intrinsic redundancy in latent representations? \n\nIn Section 3, we have used $L_1$ distance to quantify the similarity of the filter response for inputs of two different classes (otter and seal). We have also utilized the overlapping between filter activation for two classes to quantify this similarity. Although these metrics do not exactly quantify the intrinsic redundancy, it is part of our current work to characterize the redundancy in a more formal manner."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700432730797,
                "cdate": 1700432730797,
                "tmdate": 1700432730797,
                "mdate": 1700432730797,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rknb54HGh3",
            "forum": "wZXwP3H5t6",
            "replyto": "wZXwP3H5t6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6252/Reviewer_wsBg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6252/Reviewer_wsBg"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the Semantic Inference (SINF) framework to accelerate the execution of DNN. Leveraging the intrinsic redundancy in latent representations, the authors propose the Discriminative Capability Score (DCS) to identify subgraphs within large DNNs to discriminate between members of specific semantic clusters. They validate the approach using the CIFAR100 dataset and compare the performance of SINF across popular DNN architectures like VGG16, VGG19, and ResNet50."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Novelty in Approach: The paper leverages intrinsic redundancy in latent representations, offering an interesting perspective on accelerating DNNs.\n\nVersatility: The approach is tested across multiple popular DNN architectures, indicating its adaptability."
                },
                "weaknesses": {
                    "value": "However, there are several drawbacks.\n\n## Major\n\n1. There are a lot of existing pruning methods, e.g., DepGraph: Towards Any Structural Pruning\nbut this paper does not provide any comparison to existing works\n\n2. The acceleration on CIFAR10/CIFAR100 is not satisfied.\nFor example, DepGraph achieves an 8.92\u00d7 acceleration with a 3.11 loss in accuracy on CIFAR100 with VGG19.\nHowever, the results provided by the author (~1.72x and 2.83 loss on acc) are worse than previous work.\n\n3. The writing is confusing, it is hard to find the main results.\n\n4. This method applies an additional predictor to discriminate which hyper-class the input should be, which introduces extra inference cost and training effort. Does the provided inference time include the inference time on the additional predictor? Another concern is that the extra network will hurt model generalization ability.\n\n## Minor\n\n1. The authors provided an anonymous GitHub link to share code, but it is an empty link."
                },
                "questions": {
                    "value": "Please refer to my detailed comments in the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6252/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699057381944,
            "cdate": 1699057381944,
            "tmdate": 1699636683748,
            "mdate": 1699636683748,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZCol5AT1A3",
                "forum": "wZXwP3H5t6",
                "replyto": "rknb54HGh3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6252/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6252/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Regarding Weakness 1\n\n> There are a lot of existing pruning methods, e.g., DepGraph: Towards Any Structural Pruning but this paper does not provide any comparison to existing works.**\n\nWe thank the reviewer for this comment. Although the end goal of pruning and our proposed approach is to decrease the computational burden of mobile devices related to executing deep neural networks (DNNs), the primary objective of our work is to do so *without the need to fine-tune the DNN*, as in real-world mobile systems the DNN has to be adapted immediately to changing requirements.\n\nThe first key reason behind this choice is that fine-tuning DNNs takes a significant amount of time. For example, fine-tuning one of our sub-graphs even for 20 epochs takes on average a minute, which is just for CIFAR-100. For 20 sub-graphs corresponding to 20 semantic clusters, it takes about 20 minutes. In real-world dynamic mobile scenarios where priorities might change, it may be infeasible and energy-expensive to continuously fine-tune the model. For example, if we are deploying a drone for surveillance in a mountain area, that would encounter certain classes (e.g., animals). However, if the UAV operates in an urban area, then it would require a different set of classes (e.g., cars). Given the dynamic nature of the system, allowing fast switching becomes a compelling necessity.\n\nAnother reason is that fine-tuning the DNNs for one set of classes can degrade its performance in other tasks due to catastrophic forgetting [1,2]. Conversely, our approach is to *pre-compute and then select at runtime* the sub-graph with the capability of achieving sufficient accuracy on the given set of semantic classes. \n\nDepGraph and many other pruning approaches rely on fine-tuning to improve performance. For this reason, we compare to the most recent work for pruning which adopts same scenario as ours [3] in **Table 1**. We also compare the proposed discriminative capability score (DCS) with other score-based metrics usually used for pruning in **Figure 4**.\n\n[1] Pomponi, J., Scardapane, S. and Uncini, A., 2022. Centroids Matching: an efficient Continual Learning approach operating in the embedding space. Transactions on Machine Learning Research.\n[2] Davari, M., Asadi, N., Mudur, S., Aljundi, R. and Belilovsky, E., 2022. Probing representation forgetting in supervised and unsupervised continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 16712-16721).\n[3] Murti, C., Narshana, T. and Bhattacharyya, C., 2022, September. TVSPrune-Pruning Non-discriminative filters via Total Variation separability of intermediate representations without fine tuning. In The Eleventh International Conference on Learning Representations.\n\n# Regarding Weakness 2\n\n> The acceleration on CIFAR10/CIFAR100 is not satisfied. For example, DepGraph achieves an 8.92\u00d7 acceleration with a 3.11 loss in accuracy on CIFAR100 with VGG19. However, the results provided by the author (~1.72x and 2.83 loss on acc) are worse than previous work.\n\nAlthough the review is correct, we point out that conversely from DepGraph and other pruning approaches in literature, our results are obtained *without fine-tuning*. When considering the DCS score for pruning, and enabling fine-tuning, our proposed DCS strategy achieves 9.08x speed up for only 3.27% loss in accuracy which is comparable to DepGraph.\n\n# Regarding Weakness 3\n\n>The writing is confusing, it is hard to find the main results.\n\nWe are sorry to know that the reviewer has found the writing to be confusing and main results hard to find. We have updated the manuscript and tried to highlight the main results. We hope that it will make it the results easier to find."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700433857921,
                "cdate": 1700433857921,
                "tmdate": 1700433896825,
                "mdate": 1700433896825,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]