[
    {
        "title": "Addressing Real-Time  Fragmentary Interaction Control Problems via Muti-step Representation Reinforcement Learning"
    },
    {
        "review": {
            "id": "taxKt7Kdcn",
            "forum": "dkn9cEOQkU",
            "replyto": "dkn9cEOQkU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3319/Reviewer_XM6T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3319/Reviewer_XM6T"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for real-time control scenarios in which interation between the execution devices and the computation node is lossy. The question here is what actions should be taken by the execution devices when the latest inference result has not arrive yet. The algorithm involves planning a sequence of future actions instead of a single action and learning a latent representation for lists of future actions in an unsupervised manner. Then an RL policy is trained to solve the task with the action space being the latent representation. Experimental results illustrate the effectiveness of using latent representations as action spaces in both simulation environments and the real world."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n2. The motivation is clear and the method is clean."
                },
                "weaknesses": {
                    "value": "1. The experiemnt part may not fully match with the motivation of the paper. Generally speaking, simulation environments such as Mujoco do not require the use of framentary control.\n2. The real-world robotic control experiment lack important details, including the interaction pattern between the executor and the agent in the real-world experiment."
                },
                "questions": {
                    "value": "1. Why does the method significantly outperform TD3 with advanced decision? Please explain the comparison between the baselines in more details.\n2. What is the interaction pattern between the executor and the agent in the real-world experiment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3319/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698564273483,
            "cdate": 1698564273483,
            "tmdate": 1699636281399,
            "mdate": 1699636281399,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Q7cMmyReqU",
                "forum": "dkn9cEOQkU",
                "replyto": "taxKt7Kdcn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3319/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3319/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your valuable comments. Hope that our answers can address your concerns."
                    },
                    "comment": {
                        "value": "# Response\nThank you for your valuable comments. Hope that our answers can address your concerns.\n## The experiment part may not fully match the motivation of the paper. Generally speaking, simulation environments such as Mujoco do not require the use of fragmentary control. \nFragmented interactions exist in many virtual scenarios. For example, remote control of NPCS: In the game, the cloud server needs to control the terminal NPC in real time. NPC stalling can significantly degrade the user experience. So we follow works with related evaluation scenarios (e.g.RTAC[1]RDAC[2]). We used mainstream virtual scenarios (Mujoco, D4RL) to simulate remote control of NPC tasks instead of using Mujoco directly. The details of constructing fragmented interactions simulation tasks are described in the experiment section Sec 4.2: In real-world tasks, an execution time for one step is about 0.05s to 0.25s. And most of the prohibited interaction duration is 0.5s to 2s. Thus, we set the interval step length as 8 to simulate the real-world setting.  We will highlight the task settings in the new version. And put it at the top of the experiment section.\n\n[1] Chris Pal, et al. \"Real-time reinforcement learning.\" NeurIPS (2019).\n\n[2] Ramstedt, Simon, et al. \"Reinforcement learning with random delays.\" arXiv preprint arXiv:2010.02966 (2020).\n## The real-world robotic control experiment lacks important details, including the interaction pattern between the executor and the agent in the real-world experiment.\nThe 21-joint snake robotic uses a rolling gait. The algorithm is arranged in the cloud server, and the decision end interacts with the snake robot through the local area network. [This is the interaction pattern figure.](https://anonymous.4open.science/r/ICLR2024-C0F6/interaction_pattern.png)\n\nSnake robot tracking is a periodic information acquisition task. The reason for this phenomenon is not packet loss or delay, but the unique rolling gait of the snake robot. This gait of the snake robot is the most efficient gait for outdoor work, which includes the rolling of the head sensor. However, this gait causes the next observation to be acquired only when the head sensor rolls to a position parallel to the ground. If the rolling motion is interrupted midway, the next state cannot be obtained, leading to ineffective shaking of the robot. \n\nThe snake robot needs to scroll from the starting point to the target point in a scene with a size of 5 square meters. Each observation step is 54d, and the action sequence time step is 20, so the action sequence dimension is 1.08k. The control frequency of the real interaction is 20 Hz. We describe the snake robot tracking task in detail and provide several [visualization results](https://anonymous.4open.science/r/ICLR2024-C0F6/) in the new version. \n## Why does the method significantly outperform TD3 with advanced decision?\nExplanation:\n- Td3-advance decision needs to output the whole action sequence (concatenate c steps), and the long action sequence will increase the output dimension (output_dim=c * single_action_dim). This increases the difficulty of the action space exploration. Thus, the agent cannot learn the optimal policy (mentioned in the introduction).\n- Our method represents diverse action sequences in low-dimensional space. Our method reduces the difficulty of exploration and performs better.\n- Ours is a plug-in method and can be applied to all major reinforcement learning methods. To ensure experimental fairness, all methods use Td3. This eliminates the framework advantages that Td3 brings to advance deisicion.\n\nVerified by experiment:\n- Following results show the sensitivity of three methods to the action sequence dimension. The first table runs on Walker and the second runs on Maze-hard.\n- The exploration ability of Td3-advance decision decreases significantly with the increase of sequence length.\n- TD3-frameskip has better exploration ability than TD3- advanced decision. This is because frameskip reduces the spatial dimension of exploration by sacrificing action variety (repeating the same action).\n- Our method has the best performance by reducing the dimension of exploration space and ensuring the diversity of action through the representation of action space.\n\n| method     | 12 dim| 24dim| 36 dim|\n| :-----------: | :-----------: | :------------: | :-----------: |\n| Ours |271.2 \u00b1 11.4|258.3 \u00b115.2|246.5 \u00b1 21.3|\n| TD3- frameskip |231.2 \u00b1 14.6|112.1 \u00b1 3.2|88.7 \u00b1 2.5|\n| TD3- advanced decision  |83.5 \u00b1 5.7|31.9 \u00b1 7.4|85.1 \u00b1 2.8|\n\n| method     | 18 dim| 36 dim| 54 dim|\n| :-----------: | :-----------: | :------------: | :-----------: |\n| Ours |4715.6 \u00b1 343.1|4613.2 \u00b1 362.7|4215.9 \u00b1 428.3|\n| TD3- frameskip |3714.7 \u00b1 252.1|941.6 \u00b1 603.2|691.3 \u00b1 122.4|\n| TD3- advanced decision |2368.2 \u00b1 316.4|913.2 \u00b1 592.1|718.3 \u00b1 176.8|\n\n## If our reply addresses your concerns, we would appreciate it if you could kindly consider raising the score."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3319/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700403466324,
                "cdate": 1700403466324,
                "tmdate": 1700403466324,
                "mdate": 1700403466324,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CzieCMMXbK",
                "forum": "dkn9cEOQkU",
                "replyto": "taxKt7Kdcn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3319/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3319/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Please allow us to analyze your question again"
                    },
                    "comment": {
                        "value": "## We think that the supplementary experiment is inadequately described. Please Allow us to analyze it again.\n## Why does the method significantly outperform TD3 with advanced decision?\nExplanation:\n- Td3-advance decision needs to output the whole action sequence (concatenate c steps), and the long action sequence will increase the output dimension (output_dim=c * single_action_dim). This increases the difficulty of the action space exploration. Thus, the agent cannot learn the optimal policy (mentioned in the introduction).\n- Our method represents diverse action sequences in low-dimensional space. Our method reduces the difficulty of exploration and performs better.\n- Ours is a plug-in method and can be applied to all major reinforcement learning methods. To ensure experimental fairness, all methods use Td3. This eliminates the framework advantages that Td3 brings to advance deisicion.\n\nVerified by experiment:\n- Following results show the sensitivity of three methods to the action sequence dimension. The first table runs on Walker and the second runs on Maze-hard. Walker's single-step action dimension is 3. To analyze the sensitivity of the methods to the action space dimension, we set the number of decision steps as follows: 4 (dim = $4\\times3$), 8 (dim = $8\\times3$), 12 (dim = $12\\times3$). Maze-hard's single-step action dimension is 2. This navigational task requires the agent to change its action in a specific state (e.g., change the directionto to avoid running into a wall). Excutor with actions repeating hard to get to the end of the maze, and therefore fail to get the maximum reward resulting in an inaccurate Q-value fit. Thus, when the action sequence of the policy is internally homogeneous,  long decision steps lead to training instability. We set the number of decision steps as follows: 9 (dim = $9\\times2$), 18 (dim = $18\\times2$), 27 (dim = $27\\times3$).\n- The exploration ability of Td3-advance decision decreases significantly with the increase of sequence length.\n- TD3-frameskip has better exploration ability than TD3- advanced decision. This is because frameskip reduces the spatial dimension of exploration by sacrificing action variety (repeating the same action).\n- Our method has the best performance by reducing the dimension of exploration space and ensuring the diversity of action through the representation of action space.\n\n| method     | 12 dim| 24dim| 36 dim|\n| :-----------: | :-----------: | :------------: | :-----------: |\n| Ours |271.2 \u00b1 11.4|258.3 \u00b115.2|246.5 \u00b1 21.3|\n| TD3- frameskip |231.2 \u00b1 14.6|112.1 \u00b1 3.2|88.7 \u00b1 2.5|\n| TD3- advanced decision  |83.5 \u00b1 5.7|31.9 \u00b1 7.4|85.1 \u00b1 2.8|\n\n| method     | 18 dim| 36 dim| 54 dim|\n| :-----------: | :-----------: | :------------: | :-----------: |\n| Ours |4715.6 \u00b1 343.1|4613.2 \u00b1 362.7|4215.9 \u00b1 428.3|\n| TD3- frameskip |3714.7 \u00b1 252.1|941.6 \u00b1 603.2|691.3 \u00b1 122.4|\n| TD3- advanced decision |2368.2 \u00b1 316.4|913.2 \u00b1 592.1|718.3 \u00b1 176.8|"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3319/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632883508,
                "cdate": 1700632883508,
                "tmdate": 1700632883508,
                "mdate": 1700632883508,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WM9S2oQ9XA",
            "forum": "dkn9cEOQkU",
            "replyto": "dkn9cEOQkU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3319/Reviewer_cdnx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3319/Reviewer_cdnx"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for solving real-time control tasks where the delay or the loss of the network packets may lead to fragmentary interaction. Specifically, this could happen when the remote controller fails to receive the observation on time and thus can not issue a new command to the robot. Without receiving correct commands at the correct timestep, the robot may standstill by doing nothing or repeating the last action. Both may induce a failure in task completion.\n\nTo address this, the paper proposes to generate an action sequence instead of a single action step when making decisions. Thus when the remote controller can not produce the new action sequence due to failing to receive the new observation, the robot can still realize what to do according to the remaining commands in the action sequence received last time. This is a bit like how traditional planning algorithms such as MPC work.\n\nTo achieve this, a modified VAE is trained to construct a latent action space, which serves as the action space for RL methods like TD3. Every time the actor-network chooses an action from the latent action space, the latent variable will be converted to a robot command sequence through the decoder. \n\nIn the experiments, several Mujoco environments are constructed to simulate the fragment interaction situation. The results show that the proposed method can overcome this problem and compete with the agents trained and deployed in an ideal environment. Besides, a robot snake experiments are conducted to show it can be applied to real robots."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Investigating how to build a system robust to fragmentary interaction or latency is important in the robotics system.\n2. The paper is easy to understand.\n3. The Mujoco experiments show that the method works well and is comparable to baselines in an ideal environment. The ablation study shows the importance of different modules"
                },
                "weaknesses": {
                    "value": "The main weakness of this paper is the poor robot evaluations. As the main motivation of the method is to address a practical issue in the real-world robot learning environment, a comprehensive real-world evaluation should be conducted on a platform where the fragmentary interaction problem indeed exists and is critical to the robot's performance. \n\nThe paper only contains a short section about the snake robot experiment with simple proprioceptive observations. In this setting, the delay or the loss of the network packets rarely happens as the bandwidth should be enough for transmitting the small amount of data consisting of only 54-dimensional vectors without any high-dimensional images and lidar results. Also, if the snake fails to receive any commands, simply stopping by doing nothing and waiting for the new commands is acceptable. It is not like a legged robot, which may easily fall down if it can not receive a stable command stream. \n\nIn a word, my main concern is that it is not verified on robot platforms that indeed suffer from this problem like quadrupedal robots with multi-modal perception. Otherwise, I cannot believe the proposed method can solve the claimed problem."
                },
                "questions": {
                    "value": "As a robotics-related submission, it is usually good to include demo videos. Sometimes, it is even more important than the paper itself. Could you please share more visualization results of the snake experiment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3319/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698690443481,
            "cdate": 1698690443481,
            "tmdate": 1699636281299,
            "mdate": 1699636281299,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kLOd06XpR3",
                "forum": "dkn9cEOQkU",
                "replyto": "WM9S2oQ9XA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3319/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3319/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Response\nThanks for your valuable advice on robot learning. Hope that our answers will address your concerns. \n## Details of snake robot experiment.\nSnake robot point tracking is a periodic information acquisition task. The reason for this phenomenon is not packet loss or delay, but the unique rolling gait of the snake robot. The rolling gait of the snake robot is the most efficient gait for outdoor work at present, which includes the rolling of the head sensor. However, this gait causes the next observation to be acquired only when the head sensor rolls to a position parallel to the ground. If the rolling motion is interrupted midway, the next state cannot be obtained, leading to agent-ineffective shaking.  Our control system is the first to complete the rolling gait tracking task. We describe the snake robot tracking task in detail and provide several [visualization results](https://anonymous.4open.science/r/ICLR2024-C0F6/) in the new version. The snake robot needs to scroll from the starting point to the target point in a scene with a size of 5 square meters. Each observation step is 54d, and the action sequence time step is 20, so the action sequence dimension is 1.08k. The control frequency of the real interaction is 20 Hz.\n\nIn addition to robot scenarios, fragmented interactions also exist in many virtual scenarios. For example, remote control of NPCS: In the game, the cloud server needs to control the terminal NPC in real-time. NPC stalling can significantly degrade the user experience. So we follow works with related evaluation scenarios (e.g.RTAC[1]RDAC[2]). We used mainstream virtual scenarios (Mujoco, D4RL) to simulate remote control of NPC tasks. Our method outperforms in these tasks.\n\n[1] Chris Pal, et al. \"Real-time reinforcement learning.\" Advances in neural information processing systems 32 (2019).\n\n[2] Ramstedt, Simon, et al. \"Reinforcement learning with random delays.\" arXiv preprint arXiv:2010.02966 (2020).\n## It is usually good to include demo videos.\nThanks for your advice. We are willing to provide the visualization results, we provide a [demo video and several visual descriptions](https://anonymous.4open.science/r/ICLR2024-C0F6/) of the task in the new version. \n## If our reply addresses your concerns, we would appreciate it if you could kindly consider raising the score."
                    },
                    "title": {
                        "value": "Thanks for your valuable comments. Hope that our answers will address your concerns."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3319/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700402751072,
                "cdate": 1700402751072,
                "tmdate": 1700402834054,
                "mdate": 1700402834054,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lPFGvJsLqr",
                "forum": "dkn9cEOQkU",
                "replyto": "kLOd06XpR3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3319/Reviewer_cdnx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3319/Reviewer_cdnx"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thank you for the response.\n\nI checked the videos but could not get a clear conclusion. It is just a moving robot snake without anything informative. A demo video at least is supposed to:\n\n1) Put a start point mark and a destination mark on the ground to indicate what the task is\n2) Have a baseline without the proposed method. The result should be that with the proposed method, the snake manages to reach the destination. \n\nThe robotics research is demo-centric. Due to the limited real-world robot evaluation, I would like to keep my score unchanged."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3319/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600355647,
                "cdate": 1700600355647,
                "tmdate": 1700600355647,
                "mdate": 1700600355647,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tyfFqhqH5J",
            "forum": "dkn9cEOQkU",
            "replyto": "dkn9cEOQkU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3319/Reviewer_GKKU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3319/Reviewer_GKKU"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a representation learning method for reinforcement learning to handle real-time fragmentary interaction control problems. The authors proposed a novel problem formulation in the MDP, where the interaction between agents and environments might be fragmentary.The agents need to make multi-step decisions on potentially insufficient observation to handle the frame skip and package loss. The authors proposed a VAE-based approach to learn the multi-step latent representation and use the representation with RL to handle the fragmentary interaction problem. Empirical results have shown the effectiveness compared to intuition-based baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The problem formulation is novel and significant. Fragmentary interaction is indeed an important problem in real-world high-frequency control problems.\n2. The presentation is excellent, the problem formulation is clear and multiple figures help clarify the problems.\n3. The proposed algorithm is solid and performs well empirically."
                },
                "weaknesses": {
                    "value": "1. The authors might need to connect more with existing problem formulations. The FIMDP looks related to partially observable MDPs and MDP with reward delays. I can get a rough sense that there are differences between FIMDP and these existing problem formulations, but not very clear. The authors should add a clear discussion to distinguish FIMDP from the existing related problem formulations."
                },
                "questions": {
                    "value": "1. The questions are also related to weakness, what are the differences between FIMDP and POMDP, or MDP with reward delays?\n2. If we have/learned a world model for the environment, can we do the model-based predictions, like model predictive control to solve the FIMDP? (this might be the most straightforward method that first came into my mind.) How does this compare to learning the multi-step representations?\n\nIt is a good paper. I will consider improving my score if the questions are appropriately addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3319/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803892406,
            "cdate": 1698803892406,
            "tmdate": 1699636281218,
            "mdate": 1699636281218,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2JRHLB9wJs",
                "forum": "dkn9cEOQkU",
                "replyto": "tyfFqhqH5J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3319/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3319/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your recognition of our work. We hope our reply can address all your concerns."
                    },
                    "comment": {
                        "value": "# Response\nThanks for your recognition of our work. We hope our reply can address all your concerns.\n## The questions are also related to weakness, what are the differences between FIMDP and POMDP, or MDP with reward delays?\nWe compare FIMDP with reward delayed MDP and POMDP in the new version:\n- The main difference between FIMDP and delay MDP: In FIMDP All environmental information is delayed (observation, action sequence, reward). Besides, agents are not allowed to pause midway. However, in reward delay MDP agents just need to address the reward delay. And reward delay MDP does not consider the harm caused by the agent stalled in the middle.\n- POMDP does not involve delay, the agent gets a local observation at each step. In contrast, the FIMDP has a delay in obtaining observations, but the agent can obtain global observations\n\n## 2. If we have/learned a world model for the environment, can we make the model-based predictions, like model predictive control to solve the FIMDP? (this might be the most straightforward method that first came into my mind.) How does this compare to learning the multi-step representations?\nThis question is valuable. In fact, we originally wanted to build a world model to achieve comprehensive environmental perception. However, the interval of FIMDP is too long and some tasks have random intervals. It is difficult to construct effective dynamics models and reward models under delayed conditions, which can be summarized as we cannot model random delays effectively. Therefore, we chose a method that is easier to train and robust to delay: construct an action latent space. To prove that our approach is more efficient than the model-based approach in FIMDP tasks. The following results show the performance of the original MBPO[1] based world model method (Average of the 10 runs. Interval is 10). In the future, we will further think about how to make Model-based methods effective in FIMDP tasks.\n\n| Methods     | Walker| maze-hard| HalfCheetah|\n| :-----------: | :-----------: | :------------: | :-----------: |\n| MBPO-based |3781.2 \u00b1 217.5|203.6 \u00b1 15.8|6153.1 \u00b1 381.2|\n| Ours  |4715.6 \u00b1 343.1|271.2 \u00b1 11.4|7012.1 \u00b1 131.4|\n\n[1] Janner, Michael, et al. \"When to trust your model: Model-based policy optimization.\" Advances in neural information processing systems 32 (2019).\n## If you think the above response addresses your concerns, we would appreciate it if you could kindly consider raising the score."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3319/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700402663202,
                "cdate": 1700402663202,
                "tmdate": 1700402663202,
                "mdate": 1700402663202,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S9cEP9aGTA",
                "forum": "dkn9cEOQkU",
                "replyto": "2JRHLB9wJs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3319/Reviewer_GKKU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3319/Reviewer_GKKU"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response! \n\nI have a complex feeling about the paper. \n\nOn the one hand, I like the paper because this is a realistic problem in the real world. It hasn't been studied according to the literature review. The methods proposed look good. Accepting this paper definitely helps motivate more people to focus on this problem. \n\nOn the other hand, after reading other reviewers' comments, the experimental results are indeed not strong enough to sell the methods. The video is also not very helpful.\n\nI finally decided to keep my score, It is already a paper above the accept threshold. If the authors could further polish the paper and provide stronger empirical results, this work would have a broader impact."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3319/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712025371,
                "cdate": 1700712025371,
                "tmdate": 1700712025371,
                "mdate": 1700712025371,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KDM6ObVeYu",
            "forum": "dkn9cEOQkU",
            "replyto": "dkn9cEOQkU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3319/Reviewer_3YNf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3319/Reviewer_3YNf"
            ],
            "content": {
                "summary": {
                    "value": "This work primarily focuses on real-time reinforcement learning for high-frequency robot control tasks, where the information transmission is not entirely reliable. The communication between the action executor and agent in reinforcement learning may be affected by packet loss and latency, potentially impacting the effectiveness of policy execution. In contrast to previous methods that directly generate multi-step action sequences, this paper proposes using sc-VAE to generate an intermediate representation in place of an action sequence. During actual execution, this intermediate representation is used to generate the corresponding action sequence. The paper introduces additional regularization for the influence of actions on the environment within the generated intermediate representation.\n\nThe proposed method's performance is tested in various Mujoco task environments and a real-world snake robot control task. The results indicate that MARS outperforms the advanced decision method that directly generates action sequences and a simple frame-skip method which makes decisions with lower frequency. Further ablation studies confirm that the proposed method can consider the influence of the environment when generating intermediate representations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strengths of this work are as follows:\n\n1. The paper provides a detailed introduction to the background of the real-time RL problem, and the research objectives are clear.\n\n2. The proposed method in the paper exhibits excellent generability and can work with various reinforcement learning optimization algorithms.\n\n3. The paper offers experimental results on real robots, demonstrating the practicality of the proposed method."
                },
                "weaknesses": {
                    "value": "The weaknesses of this work are as follows:\n\n1. The soundness of the paper is limited. The method is based on sc-VAE, and the primary claim that \"the action sequences decoded by the close points in the latent space should have a similar     influence on the environment\" relies on empirical evidence and lacks theoretical explanation (refer to question 1).\n\n2. The paper lacks explanations for some critical aspects of the experiments. For more details, please refer to question 2."
                },
                "questions": {
                    "value": "1. Why is clustering representations of actions that have similar environmental effects better than clustering action sequences with similar values or rewards? Can you provide a more in-depth explanation and analysis?\n\n2. In the random FIMDP tasks mentioned in the paper, is the number of decision steps fixed within one trial or randomly decided during execution? As shown in Figure 11, a larger interval leads to lower performance, how will the method perform if trained with longer action sequences but executed with a shorter interval, compared with training with shorter action sequences?\n\n3. The paper mentions that MARS has better stationarity, but it doesn't provide relevant explanations and proofs. Additionally, in a real-time RL setting, where it's not guaranteed that the actions actually executed by the executor strictly match the policy's output, do the collected trajectory samples inherently lack stationarity? (you can regard the trajectory as being sampled from a rapidly changing environment transition probability)\n\n4. I noticed some interesting results. Why does it appear that MARS has a more pronounced advantage over frame-skips in simpler tasks than in more complex tasks?\n\n5. The application form of the method needs further clarification. Does the decoder need to be run on the execution device? If so, does this mean that latent representations will also be lost?\n\n6. Should the \"Musar\" method in Fig.5 and 6 be referred to as \"MARS\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3319/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3319/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3319/Reviewer_3YNf"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3319/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821711180,
            "cdate": 1698821711180,
            "tmdate": 1700669068197,
            "mdate": 1700669068197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "radnQVbHqO",
                "forum": "dkn9cEOQkU",
                "replyto": "KDM6ObVeYu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3319/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3319/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the objective and in-depth comments. Hope that our reply can ease your concerns."
                    },
                    "comment": {
                        "value": "# Response\n## Why is clustering representations of actions that have similar environmental effects better than clustering action sequences with similar values or rewards? \nExplanation is as follows (highlight in new version):\n- Get accurate Q value is difficult: In sparse reward environments (such as FIMDP), reward and Q are difficult to obtain and the evaluation of Q values in the early stage of training is inaccurate. In contrast, environmental dynamic is more reliable and accessible.\n- Environmental dynamic contains more information: The same reward or Q value may correspond to different environmental changes, but the same environmental change must have the same reward or Q value.\n- Environmental dynamic is reward-agnostic: In FIMDP, rewards are sparse. Environment dynamics do not require a per-step reward. Therefore, environmental dynamic representation is more robust in FIMDP.\n\nFurther, we compare these 3 representational learning methods. We only changed the clustering representations.\n\n|Method|Halfcheetah|Walker|maze-hard|\n| :- | :- | :- | :- |\n|Env dynamic (ours)|7012.1\u00b1131.4|4821.6\u00b1427.6|311.4\u00b116.3|\n|Q|6386.1\u00b1412.7|4021.6\u00b1313.7|275.2\u00b113.7|\n|reward|6618.1\u00b1372.7|4188.3\u00b1185.5|253.9\u00b121.5|\n## Is the number of decision steps fixed within one trial? How will MARS perform if trained with longer action sequences but executed with a shorter interval, compared with training with shorter action sequences?\nThe number of decision steps is fixed. The following results show that reducing the number of steps as much as possible can improve scores (Average of the 10 runs. Interval is 6). As the number of steps increases, both the sequence dimension and the reward sparsity increase. These lead to the difficulty in exploration.\n|task|6 step|12 step|18 step|\n| :-| :-| :- | :- |\n|Walker|4715.6\u00b1343.1|4613.2\u00b1362.7|4215.9\u00b1428.3|\n|maze-hard |271.2\u00b111.4|258.3\u00b115.2|246.5\u00b121.3|\n## How to guarantee that the actions actually executed by the executor strictly match the policy's output, do the collected trajectory samples inherently lack stationarity? \nWe use the physical clocks on both devices(a common method in real-time control, hightlght in the new version). If the actions are obsolete, lose them. We retain the time stamp and execution flag of each action, which makes actions executed in strict accordance with the timestamp order. When the new sequence arrives at the executor, the previous sequence will be replaced, and the execution flag of the unexecuted action will be False. Each latent space action reward is the sum of the executed action reward in the corresponding sequence. Results show the effect of the alignment method (average of the 10 runs, Interval is 6). \n\n|Method|Walker|maze-hard|\n| :- | :- | :- |\n|Ours|4463.2\u00b1362.7|311.4\u00b116.3|\n|without alignment|4168.3\u00b1372.6|213.1\u00b116.7|\n## MARS has a more advantage over frame-skips in simple tasks than in complex tasks?\nBecause the VAE hyperparameters of MARS are not optimized in difficult tasks. We adjust the VAE hyperparameters while keeping the remaining hyperparameters unchanged for both methods \uff08following table). By doing this MARS has a more pronounced advantage on complex tasks. (Old score of MARS in maze-Hard:315.2\u00b113.9 maze-Hard:6417.3\u00b1317.3)\n\n| method| maze-hard|HalfCheetah|\n| :- | :- | :-| \n|Ours|356.4\u00b116.3|8631.5\u00b1265.2|\n|TD3- frameskip|285.7\u00b112.5|5842.3\u00b1306.7|\n## MARS has better stationarity.  \nExplanation:\n- Frameskip leads to internal homogeneity of the action sequence and the inability to change the action at key states. Thus, the policy is unstable.\n- Advance decision needs to output the whole action sequence (concatenate c steps), which will increase the output dimension. This increases the difficulty of the action space exploration. Thus, the agent cannot learn the optimal policy.\n- Our method represents diverse action sequences in low-dimensional space. RL algorithms only need to learn policies in the latent action space. Our method reduces the difficulty of exploration and performs better.\n\nFurther, we compare the policy stability of all methods for sequence length. Ours provides high stability.\n| method|6 step|12 step|18 step|\n| :- | :-| :- | :- |\n|Ours|4715.6\u00b1343.1|4613.2\u00b1362.7|4215.9\u00b1428.3|\n|TD3- frameskip|3714.7\u00b1252.1|941.6\u00b1603.2|195.7\u00b172.5|\n|TD3- advanced decision|2368.2\u00b1316.4|913.2\u00b1592.1|718.3\u00b1176.8|\n## Does the decoder need to be run on the execution device? Does this mean that latent representations will also be lost?\nWe do not deploy the decoder to the executor. The agent makes c step decision based on each timestep information, so each time step (t+i) will receive c times in the future. Besides, our decision steps are set at maximum intervals to ensure that the executor receives the next sequence before the previous sequence is completed.\n## Should the \"Musar\" be referred to as \"MARS\"?\nCorrect this in the new version.\n\n## If our reply addresses your concerns, we would appreciate it if you could kindly consider raising the score."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3319/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700405602060,
                "cdate": 1700405602060,
                "tmdate": 1700471475822,
                "mdate": 1700471475822,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8lgkGiCnBy",
                "forum": "dkn9cEOQkU",
                "replyto": "radnQVbHqO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3319/Reviewer_3YNf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3319/Reviewer_3YNf"
                ],
                "content": {
                    "title": {
                        "value": "Reply for the author"
                    },
                    "comment": {
                        "value": "I appreciate the detailed response from the authors. The author's replies addressed some of my concerns and clarified certain ambiguities in the paper. After reviewing the questions from other reviewers and the author's responses, I have slightly and cautiously raised my score. My understanding of this paper is that the author effectively improves exploration and learning efficiency by learning a more compact latent representation rather than the original action sequence, as opposed to directly optimizing the action sequence. The focus of the paper is, in fact, slightly different from the core issue of real-time RL. Looking forward, in addition to addressing the mentioned points in the rebuttal, the author could consider whether placing the decoder on the executor side is feasible. Additionally, encoding key future actions in the action sequence could be explored to ensure the preservation of critical actions even in the event of communication loss. This would align more with real-time specified methods."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3319/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670693588,
                "cdate": 1700670693588,
                "tmdate": 1700670693588,
                "mdate": 1700670693588,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]