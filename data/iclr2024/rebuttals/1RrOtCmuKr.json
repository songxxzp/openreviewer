[
    {
        "title": "Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings"
    },
    {
        "review": {
            "id": "BFiqQlv2lk",
            "forum": "1RrOtCmuKr",
            "replyto": "1RrOtCmuKr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2665/Reviewer_fJGw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2665/Reviewer_fJGw"
            ],
            "content": {
                "summary": {
                    "value": "The novel ALAM framework introduced in this paper leverages average quantization and a simple sensitivity calculation method to reduce memory usage in LLMs without affecting their training efficacy. This approach minimizes gradient variance by compressing activations to their group average values, allowing for effective compression to less than 1 bit. Additionally, ALAM employs a novel sensitivity calculation that uses the L2 norm of parameter gradients, greatly reducing memory overhead. In testing, ALAM achieves up to a 12.5\u00d7 compression rate for activation memory in LLMs without sacrificing accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.The paper applied a re-order method on neurons to reduce the memory overhead, which is quite novel.\n\n2.This paper jointly optimized the mapping and codebooks, and the proximal search method can be used to the modified gradient update method.\n\n3.The evaluation demonstrates the effectiveness."
                },
                "weaknesses": {
                    "value": "1.It is hard to understanding the background part, especially the challenges of conventional approach. It would be better to provide a straightforward illustration with a figure or algorithm.\n2.Some typos. In the beginning of sec3.2, \u201cthe\u201d should be \u201cThe\u201c; \u201cram\u201d in sec 3.1 should be \u201cRAM\u201d; Something is missing in Equation 3.\n3.The metric in evaluation part is accuracy and compression ratio. Can the proposed idea bring more benefits like accelerating the inference/training/finetuning?"
                },
                "questions": {
                    "value": "1.It would be better to explain why the second term $log(\\Omega(C))\\Omega(W)$dominates. It seems a common sense but I cannot see it in the context.\n\n2.Can you provide some evaluation results in inference or training or finetuning performance in efficiency (throughput or energy efficiency)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2665/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2665/Reviewer_fJGw",
                        "ICLR.cc/2024/Conference/Submission2665/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2665/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698670987017,
            "cdate": 1698670987017,
            "tmdate": 1700275589829,
            "mdate": 1700275589829,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DQSvAItUF0",
                "forum": "1RrOtCmuKr",
                "replyto": "BFiqQlv2lk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2665/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2665/Authors"
                ],
                "content": {
                    "title": {
                        "value": "response to reviewer fJGw"
                    },
                    "comment": {
                        "value": "### It is hard to understanding the background part, especially the challenges of conventional approach. It would be better to provide a straightforward illustration with a figure or algorithm.\n\nWe introduced a novel figure to highlight these elements. First, we illustrate the challenges of the baseline approach and second, our contributions both in terms of multiple codebooks/scalings and gradient updates.\n\n### Some typos. In the beginning of sec3.2, \u201cthe\u201d should be \u201cThe\u201c; \u201cram\u201d in sec 3.1 should be \u201cRAM\u201d; Something is missing in Equation 3. \n\nThank you for pointing these out. We fixed these typos in the revision version and doubled checked the remainder of the article.\n\n### The metric in evaluation part is accuracy and compression ratio. Can the proposed idea bring more benefits like accelerating the inference/training/finetuning?\n\nthe primary aim of the proposed JLCM method is to preserve the accuracy of the model while significantly reducing its memory footprint. However, as illustrated in Table 2, memory limitations can force the weights to be loaded on-the-fly for inference, leading to severe computational burden: this is especially true on large models (see last row in Table 2 for instance), as the architectures become more and more parameter hungry.\n\nIn this context, using JLCM can translate in huge latency improvements at inference time, e.g. on low-power or low-memory hardware where the original model cannot fit entirely, or if we consider very large models (e.g. LLMs).\n\nSecondly, to answer your question, and although not studied in the present paper, one could decide to only optimize the codebooks during a post-hoc fine-tuning phase. In this case ,the proposed method would behave similarly to a foldeable adapter (e.g. LoRa [1], which can allow to finetune or fewer examples or using less memory), which could be an interesting lead for future work.\n\n### It would be better to explain why the second term $\\log(\\Omega(C))\\Omega(W)$ dominates. It seems a common sense but I cannot see it in the context.\n\nTo answer you concern, we clarified this point in the revised version of the paper. In short, for a dense layer of weights $W \\in \\mathbb{R}^{n_i \\times n_o}$, we have $\\Omega(W) = n_i \\times n_o$ which is often larger than $100000$ and can grow to several millions in some LLMs. On the other end, $\\log(\\Omega(C))$ corresponds to the size of our codebooks which is empirically bounded by $\\Omega(C) \\leq 16$, i.e. we have $\\log(\\Omega(C)) \\leq 4 \\ll 100000 \\leq \\Omega(W)$.\n\n### Can you provide some evaluation results in inference or training or finetuning performance in efficiency (throughput or energy efficiency)?\n\nIn terms of latency, we refer to Table 2 in our original submission (see also the answer to your previous question, in short JLCM has the potential to increase the latency in the cases where the model is too big to be loaded on device at once, and memory swaps have to be done). This was shown in SQueeze LLM [2]. With respect to throughput, we share these new results\n\n| model | GPU (A100) | GPU (A100) with offload on disk |\n| :---: | :---: | :---: |\n| ViT b16  | 1333.33 | 280.70 |\n| LLama 7B | 28.57 | 1.33 |\n\nIn both cases, we measure the impact of a model not fitting on the device cache (e.g. the V-RAM for a GPU) and thus needing data transfers from disk. This occurs when a model is too large to be fully loaded on the device cache (e.g. V-RAm for a GPU). In such circumstances, we need to offload some paramters on the disk and load them when necessary (this process is optimized with the accelerate library from huggingface). In this experiment, we imitate the situation where the GPU has 2GB of VRAM (simulating the memory on a smartphone). Still, having the model fully loaded leads to massive throughput benefits. This can be acheived through memory footprint compression as in JLCM.  We hope that these results further motivate the relevance of the proposed JLCM method.\n\n### references \n\n[1] Hu, Edward J., et al. \"Lora: Low-rank adaptation of large language models.\" arXiv preprint arXiv:2106.09685 (2021).\n\n[2] Kim, Sehoon, et al. \"SqueezeLLM: Dense-and-Sparse Quantization.\" arXiv preprint arXiv:2306.07629 (2023)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2665/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699870535800,
                "cdate": 1699870535800,
                "tmdate": 1699870535800,
                "mdate": 1699870535800,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TyoOYeozXS",
            "forum": "1RrOtCmuKr",
            "replyto": "1RrOtCmuKr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2665/Reviewer_P3Eg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2665/Reviewer_P3Eg"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a series of improvements to codebook-based weight compression schemes for deep neural networks with the aim of enabling larger models to fit in the limited on device storage for accelerators like GPUs. Their method is based on three core changes relative to existing methods. First, they apply a neuron re-ordering to group weights with similar distributions. This allows for finer-grained application of codebooks to weights without increasing the storage overhead of the compressed representation. Second, they propose to jointly learn the codebook and codebook mappings, similar to gradient-based post-training quantization schemes. Lastly, they modify the gradient update for the codebook mappings to enable more effective optimization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Despite my lack of experience with the topic of this paper I found the text reasonably easy to follow. I think the paper is well written and the methods appear to be sound to me. In particular, the impact of part 3 of the proposed method (improved gradient estimator) seems to be considerable based on the ablation results presented in Table 4."
                },
                "weaknesses": {
                    "value": "I am not an expert on compression of neural network weights for storage optimization but I did not identify any particular weaknesses in the methodology. The technique appears to be reasonable and the results appear to be sound based on my review of the paper."
                },
                "questions": {
                    "value": "The neuron permutations you use in your method remind me of the channel permutations that are used by N:M sparsification methods [1]. Drawing this connection could be interesting in your related work section.\n\nIn a number of places you use \u201cGo\u201d as a unit - is this intentional? Was \u201cGo\u201d supposed to be \u201cGB\u201d?\n\nIn Table 2, I\u2019m curious to understand how you implemented offloading of parameters to disk. Are transfers from disk to GPU pipelined with computation to hide as much transfer latency as possible? What batch sizes were used for each model?\n\n[1] https://proceedings.neurips.cc/paper_files/paper/2021/hash/6e8404c3b93a9527c8db241a1846599a-Abstract.html"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2665/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698704052536,
            "cdate": 1698704052536,
            "tmdate": 1699636206918,
            "mdate": 1699636206918,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iOX3tkN1Hk",
                "forum": "1RrOtCmuKr",
                "replyto": "TyoOYeozXS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2665/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2665/Authors"
                ],
                "content": {
                    "title": {
                        "value": "response to reviewer P3Eg"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their interest in the method and we appreciate the comment on readibility, especially provided this is not their exact domain of expertise.\n\n### The neuron permutations you use in your method remind me of the channel permutations that are used by N:M sparsification methods [1]. Drawing this connection could be interesting in your related work section.\n\nWe agree that our paper would benefit from this and other references on weight permutation. They were added directly in the corresponding section in the methodology.\n\n### In a number of places you use \u201cGo\u201d as a unit - is this intentional? Was \u201cGo\u201d supposed to be \u201cGB\u201d?\n\nThis was indeed a typo that was corrected in the revised version.\n\n### In Table 2, I\u2019m curious to understand how you implemented offloading of parameters to disk. Are transfers from disk to GPU pipelined with computation to hide as much transfer latency as possible? What batch sizes were used for each model?\n\nThe implementation of this offloading can be achieved using the accelerate library from huggingface, from our experiments it is well optimized in order to hide as much as possible the cost of data transfers. \n\nRegarding the batch-size, we measure the latency which is defined using a batch size of 1 (table 2). On the flip side, if we want to maximize the throughput (number of predictions per second with optimal batch size = 32), we observe very similar results (especially on larger models). In this experiment, we imitate the situation where the GPU has 2GB of VRAM (simulating the memory on a smartphone).\n\n| model | GPU (A100) | GPU (A100) with offload on disk |\n| :---: | :---: | :---: |\n| ViT b16  | 1333.33 | 280.70 |\n| LLama 7B | 28.57 | 1.33 |\n\nWe hope that these answers address your questions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2665/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699870495652,
                "cdate": 1699870495652,
                "tmdate": 1699870495652,
                "mdate": 1699870495652,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Gscr8RywTf",
            "forum": "1RrOtCmuKr",
            "replyto": "1RrOtCmuKr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2665/Reviewer_kEDS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2665/Reviewer_kEDS"
            ],
            "content": {
                "summary": {
                    "value": "This paper compresses memory footprint in deep neural networks inference by a codebook-based approach, mainly solving the granularity problem and the training problem suffered by the previous works. This paper tried two methods to solve the granularity problem, setting per-channel scaling factors with one codebook or using multiple codebooks with weight matrix reordering. To enable proximal search thus solving the training problem, this method uses custom gradient updates inspired by STE."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. In the experiment section, this method shows advantages over baselines.\n\n2. This paper provides a detailed analysis of memory usage in different settings."
                },
                "weaknesses": {
                    "value": "The paper has some unclear statements that require more explanation:\n\n1.  The settings for the number of codebooks and the number of scaling factors should be further explained. Although the authors mention in the paper that these values are determined by compression goal, the process needs more details and therelationship with the weight distribution needs to be explained as well.\n\n2. This paper does not explain how to cluster the neurons. There is also no data to prove that \u201ctwo neurons far from each other in the weight matrix can be similarly distributed\u201d.\n\n3. The meaning of the X-axis in Figure 1 is confusing. For different curves, the X-axis seems to have different meanings. These should be marked on the figure. \n\n4. For the LLM experiments in Table 6, how are the results compared with more recent works, e.g., SqueezeLLM, AWQ, etc."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2665/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811971293,
            "cdate": 1698811971293,
            "tmdate": 1699636206845,
            "mdate": 1699636206845,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S90vEWw6xj",
                "forum": "1RrOtCmuKr",
                "replyto": "Gscr8RywTf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2665/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2665/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer kEDS"
                    },
                    "comment": {
                        "value": "We would like to thank you for your interest in the proposed method and for highlighting the empricial benefits offered by the proposed approach. In the following, we share our responses to your concerns.\n\n### The settings for the number of codebooks and the number of scaling factors should be further explained. Although the authors mention in the paper that these values are determined by compression goal, the process needs more details and therelationship with the weight distribution needs to be explained as well.\n\nIn our original submission, we define the memory footprint of a weight tensor $W \\in \\mathbb{R}^{n_i \\times n_o}$ based on the cost of encoding a single scalar value (in 16 bits floating point) mutliplied by the number of weights (end of introduction of section 3.2), i.e.\n$$\nM = \\Omega(W) \\times 16 = n_i \\times n_o \\times 16\n$$\nNow assuming that we know the target hardware capacities (in terms of memory) $M_{\\text{capa}}$ and the original memory footprint of the original model $M_{\\text{ref}}$, we can derive the target compression goal $\\alpha\\geq\\frac{M_{\\text{ref}}}{M_{\\text{capa}}}$. Now, we have two cases: multi-codebooks and multi-scalings. In our original submission, we define the memory cost for both as follows\n$$\n\\begin{cases}\n\\text{multi-codebooks:} & \\Omega(W) \\times \\log(\\Omega(C)) + k \\times \\Omega(C) \\times 16 \\\\\n\\text{multi-scalings:} & \\Omega(W) \\times \\log(\\Omega(C)) + (\\Omega(s) + \\Omega(C)) \\times 16\n\\end{cases}\n$$\nwhere $k$ is the number of codebooks and $\\Omega(s)$ the number of scaling terms. If we combine this equation with the constraint of $\\alpha$, we can derive the number of codebooks and scaling terms immediately (as in equation 3 of the original paper)\n$$\n\\begin{cases}\n\\text{multi-codebooks:} & \\Omega(W) \\times 16 = \\alpha \\left(\\Omega(W) \\times \\log(\\Omega(C)) + k \\times \\Omega(C) \\times 16\\right) \\\\\n\\text{multi-scalings:} & \\Omega(W) \\times 16 = \\alpha \\left(\\Omega(W) \\times \\log(\\Omega(C))+ (\\Omega(s) + \\Omega(C)) \\times 16\\right)\n\\end{cases}\n$$\nThe multi-codebook encoding is more robust to weight tensor with a lot of variation per-neuron in their distribution. On the flip side, multi-scaling encoding is more robust to weight tensors with signifant shifts in their support range but low variation in their distribution.\n\nWe aknowledge that our paper would benefit from these details and we added them to our rebuttal revision.\n\n\n### This paper does not explain how to cluster the neurons. There is also no data to prove that \u201ctwo neurons far from each other in the weight matrix can be similarly distributed\u201d.\n\nIn order to cluster neurons, we use the same clsutering technique $\\mathcal{C}$ for the weights and the neurons. This information was missing above equation 4. We added it in the revised article. In order to re-order the output neurons of a layer, one simply needs to re-order the input neurons of the subsequent layers. We added references which detail this process which has been widely adopted in the literature \\[1,2,3\\]. Similarly, the fact that two neurons of the same layer can be similalry distributed and be \"far from each other\" (i.e. in term of indexes) was illustrated in DFQ [4]. For the sake of having a self-contained paper, we added these elements of clarification in the revised manuscript.\n\n### The meaning of the X-axis in Figure 1 is confusing. For different curves, the X-axis seems to have different meanings. These should be marked on the figure.\n\nThis figure was indeed confusing, we updated it for a novel one which illustrates both our main contributions at once. Please refer to the revised article."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2665/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699870437982,
                "cdate": 1699870437982,
                "tmdate": 1699870437982,
                "mdate": 1699870437982,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]