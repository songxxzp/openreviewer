[
    {
        "title": "Visual Evidence Prompting Mitigates Hallucinations in Multimodal Large Language Models"
    },
    {
        "review": {
            "id": "A5hv0ULHLz",
            "forum": "xh3XUaB8M9",
            "replyto": "xh3XUaB8M9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3272/Reviewer_bJ8t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3272/Reviewer_bJ8t"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores to mitigate hallucinations of LVLMs by a few visual knowledge evidence prompting provided as small visual models. Experiments on three large language models demonstrate its effectiveness on object hallucinations as well as relation hallucinations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- well organized and easy to follow. \n- a new idea to mitigate hallucinations of LVLMs."
                },
                "weaknesses": {
                    "value": "- The authors are recommended to provide the performance comparisons on object and relation hallucinations where only object labels in the input image are provided. \nIf the performance is also good, please make explanations on the necessity of the proposed method. \n\n- The authors are recommended to provide the hallucinations mitigations on open vocabulary objects and few-shot relations. \n\n- make explanations on the decline of 'at' or slight improvement on 'under', displayed in Figure 4. \n\n- make necessary analysis for situations where the performance of RelTR on SGG is significantly higher than motifs, while the performance of hallucination mitigations on LVLMs is equal or even opposite, displayed as in Table 9."
                },
                "questions": {
                    "value": "As aforementioned"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3272/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698651703578,
            "cdate": 1698651703578,
            "tmdate": 1699636275902,
            "mdate": 1699636275902,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mb8DHf7PBL",
                "forum": "xh3XUaB8M9",
                "replyto": "A5hv0ULHLz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer bJ8t"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for taking the time to review. We appreciate that you find our approach a new idea, well-organized and easy to follow. According to your valuable comments, we provide detailed feedback.\n\n**Q1:** Only providing object labels:\n> The authors are recommended to provide the performance comparisons on object and relation hallucinations where only object labels in the input image are provided. If the performance is also good, please make explanations on the necessity of the proposed method.\n\n***Ans for Q1:***\n\nThanks for the inspiring and interesting comments!\n\nWe conduct validation experiments using the detr-resnet-101 model to provide object labels as evidence for relation hallucination.\n\n| Relation Hallucination | baseline | + object label |+ relation label |\n| --- | ---| --- | --- |\n| Qwen-VL-Chat | 63.62 | 71.41 | 75.68 |\n| mPLUG-Owl| 62.58 | 66.88 | 68.46 |\n\nThe results show that providing object labels as evidence also has some improvement although not as effective as relation label. We suppose it is because object labels themselves contain crucial object information from the image, which leads to mitigating relation hallucination. This result not only validates the necessity of relation labels but also further verifies that our approach is orthogonal to the specific task. We have added the above discussions in the revision (Appendix A.4).\n\n**Q2:** Open-vocabulary objects and few-shot relations:\n> The authors are recommended to provide the hallucinations mitigations on open vocabulary objects and few-shot relations.\n\n***Ans for Q2:***\n\nThanks for your insightful question. We conduct experiments in response to your valuable advice. The following discussions are added in the revision (Appendix A.5).\n\na) open vocabulary objects: We construct a new out-of-domain object hallucination dataset using the test sets from Object365 [A]. This dataset is divided into two parts. One part includes 80 objects that appear in COCO, while the other portion consists of objects that do not appear in COCO. The performance of these two parts are shown in the table below. It can be observed that there is a consistent improvement in performance for both in-domain and out-of-domain object categories.\n| Model |  | In-domain objects | Out-of-domain objects |\n| --- | --- | --- | --- |\n| mPLUG-owl | baseline | 58.68 | 48.45 |\n|  | **+ visual evidence** | **65.38** | **60.87** |\n| Qwen-VL-Chat | baseline | 74.64 | 67.87 |\n|  | **+ visual evidence** | **79.77** | **75.10** |\n\nb) few-shot relation: We chose the bottom-10 tail relations as defined in [C] of VG to construct a medium-sized relation hallucination dataset with 1006 samples. We used OpenPSG as the SGG model and conducted experiments on Qwen-VL-Chat and mPLUG-Owl. The experiment results are shown in the table below, and it can be seen that our framework still achieves significant improvements in few-shot relations.\n\n| Model | | Accuracy (%) |\n| --- | --- | --- |\n| mPLUG-Owl | baseline | 61.23 |\n|  | **+ visual evidence** | **67.89** |\n| Qwen-VL-Chat | baseline | 55.67 |\n|  | **+ visual evidence** | **68.63** |\n\n> [A] Objects365: A Large-scale, High-quality Dataset for Object Detection  \n> [B] Simple Open-Vocabulary Object Detection with Vision Transformers  \n> [C] Learning from the Scene and Borrowing from the Rich: Tackling the Long Tail in Scene Graph Generation  \n\n**Q3:** Figure 4 explanation:\n> make explanations on the decline of 'at' or slight improvement on 'under', displayed in Figure 4.\n\n***Ans for Q3:***\n\nThanks for your careful review. We conducted statistical analysis on the samples of these two relations. It is observed that \"at\" is naturally ambiguous with \"on\", \"next to\", and \"in\", and \"under\" is ambiguous with \"below\" and \"around\". We will carefully review the collected data to minize the confusions.\n\n**Q4:** Analysis of Table 9 in Appendix:\n> make necessary analysis for situations where the performance of RelTR on SGG is signi\u00eccantly higher than motifs, while the performance of hallucination mitigations on LVLMs is equal or even opposite, displayed as in Table 9.\n\n***Ans for Q4:***\n\nThank you for your meticulous review. In the Table 9, different scene graph generation models (RelTR, MOTIFS and OpenPSG) have comparable improvements on mPLUG-Owl and Qwen-VL-Chat. For example, RelTR achieves 5.92% and MOTIFS achieves 5.8% improvement on mPLUG-Owl. RelTR achieves 11.35% and MOTIFS achieves 12.55% improvement on Qwen-VL-Chat. The gains brought by different scene graph generation models to LVLM are within a stable range (saturated).\n\n| LVLM | Small model | mAP | Accuracy (%) |\n| --- | --- | --- | --- |\n| mPLUG-owl | baseline | - | 62.58 |\n|  | RelTR | 18.9 |  68.50 (+5.92) |\n|  | MOTIFS | 20.0 | 68.38 (+5.8) |\n|  | OpenPSG | 28.4 | 68.25 (5.67)|\n| Qwen-VL-Chat | baseline | - | 63.62 |\n|  | RelTR | 18.9 | 74.97 (+11.35) |\n|  | MOTIFS | 20.0 | 75.80 (+12.18) |\n|  | OpenPSG | 28.4 | 76.17 (+12.55) |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3272/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700283895040,
                "cdate": 1700283895040,
                "tmdate": 1700283895040,
                "mdate": 1700283895040,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NU5DgMPTy0",
                "forum": "xh3XUaB8M9",
                "replyto": "A5hv0ULHLz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely looking forward to your feedbacks"
                    },
                    "comment": {
                        "value": "Dear reviewer bJ8t,\n\nWe sincerely apologize for inconveniencing you, but we have no other choice at the end of the discussion period. We sincerely hope you can understand.\n\nThank you for taking the time to review our work. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work. Your support or feedback are very important to us. We greatly appreciate your constructive comments and efforts.\n\nBest regards\n\nAuthors of #3272"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3272/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700474005939,
                "cdate": 1700474005939,
                "tmdate": 1700474005939,
                "mdate": 1700474005939,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2QSh9oMhfs",
                "forum": "xh3XUaB8M9",
                "replyto": "A5hv0ULHLz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely looking forward to your feedbacks"
                    },
                    "comment": {
                        "value": "Dear reviewer bJ8t,\n\nWe sincerely apologize for inconveniencing you again, but we really have no other choice as the discussion period is drawing to a close (only 1 days left). We sincerely hope you can understand.\n\nThank you for taking the time to review our work. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work. Your support or feedback are very important to us. We greatly appreciate your constructive comments and efforts.\n\nSorry again for inconveniencing you.\n\nBest regards\n\nAuthors of #3272"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3272/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560311631,
                "cdate": 1700560311631,
                "tmdate": 1700560311631,
                "mdate": 1700560311631,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hO7zxAbXOd",
                "forum": "xh3XUaB8M9",
                "replyto": "A5hv0ULHLz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely request for an opportunity to discuss with you"
                    },
                    "comment": {
                        "value": "Dear reviewer bJ8t,\n\nDeep apologies for the repeated interruption. Sorry so much!\n\nWe are truly honored that you have reviewed our paper and provided valuable constructive suggestions. Sincerely thanks for these constructive suggestions which greatly aid in our paper\u2019s refinement, we have diligently addressed every concern and question you raised during the initial review. We genuinely hope our responses have resolved your concerns and provided satisfactory explanations. We sincerely appreciate your dedication and valuable time.\n\nTo further improve our work, we sincerely hope for a valuable opportunity to discuss with you. It would be greatly appreciated if we could have a chance to hear your feedback. Your feedback is highly valuable to our paper and greatly contributes to the entire community.\n\nVery sorry for inconveniencing you again. Hope you have a good day!\n\n\nBest regards and many thanks,\n\nAuthors of #3272"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3272/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665251142,
                "cdate": 1700665251142,
                "tmdate": 1700665251142,
                "mdate": 1700665251142,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1rRx0H2SxD",
            "forum": "xh3XUaB8M9",
            "replyto": "xh3XUaB8M9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3272/Reviewer_YNjN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3272/Reviewer_YNjN"
            ],
            "content": {
                "summary": {
                    "value": "This work explores visual evidence prompting and shows how small visual models complement the LVLMs by effectively extracting contextual information from images to generate precise answers. Experiments on both object and relation halluaciations shows the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper proposes a simple method on mitigating LVLM's object and relation hallucination problem.\n- The proposed method shows improvement for all the models and for both object and relation hallucinations.\n- The authors also propose a dataset and benchmark for relation hallucinations.\n- The authors also provide in-depth analysis on visual evidence prompting."
                },
                "weaknesses": {
                    "value": "There are some unanswered questions regarding visual evidence, please refer to the **Questions** section."
                },
                "questions": {
                    "value": "1. Have the authors explored questions regarding how overlapped between objects in questions and objects in evaluation datasets' questions?\n1.1 What is the current stats on how overlapping between objects in questions and objects in evaluation datasets' questions?\n1.2 What if visual evidence prompt contains objects that are not in the question? what if the prompt contains objects that are not in the questions exclusively?\n1.3 What if the object names are switched to the synonyms to the objects appear in question?\n\n\n2. Another question is regarding the pixel locations of detected objects? does it matter to change the scale of the image? for example, `cup <0, 284, 133, 424>'  to '<0 /width, 284 / height, 133 / width, 424 / height>'. or to '<0 /width * 2, 284 / height * 2, 133 / width * 2, 424 / height *2>'. I wonder to what extend LVLMs will differentiate object relations"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3272/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3272/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3272/Reviewer_YNjN"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3272/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698732545328,
            "cdate": 1698732545328,
            "tmdate": 1700631911295,
            "mdate": 1700631911295,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0rbkd7wntW",
                "forum": "xh3XUaB8M9",
                "replyto": "1rRx0H2SxD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer YNjN"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for taking the time to review. We appreciate that you find our approach simple, experimental improvement, contributions of dataset and benchmark and the in-depth analysis. According to your valuable comments, we provide detailed feedback.\n\n**Q1:** Overlap between objects:\n> Have the authors explored questions regarding how overlapped between objects in questions and objects in evaluation datasets' questions?   \n> 1.1 What is the current stats on how overlapping between objects in questions and objects in evaluation datasets' questions?  \n> 1.2 What if visual evidence prompt contains objects that are not in the question? what if the prompt contains objects that are not in the questions exclusively?  \n> 1.3 What if the object names are switched to the synonyms to the objects appear in question?\n\n***Ans for Q1:***\n\nThanks for the inspiring and interesting comments! \n\na) We calculate the current stats of the overlap using detr-resnet-101. In the 3,000 visual evidence prompts, there are 298 prompts that contains object that are not in the question (Type A), and 1,202 prompts that contain objects that are not in the questions exclusively (Type B).\n\n\n|  | LVLM right->right | LVLM right->wrong | LVLM wrong->right | LVLM wrong->wrong |\n| -------- | -------- | -------- | -------- | -------- |\n| Type A | 139 (46.7%)     | 8 (2.7%)    | 110 (36.9%)     | 41  (13.8%)   |\n| Type B | 415 (34.5%)     | 22 (1.8%)    | 563 (46.8%)     | 202  (16.8%)   |\n\nb) Following Figure 3 in the draft, we calculate the stats of samples which were initally answered corrently/wrongly and answer correctly/wrongly after provided with Type A/B prompts. In the 298 Type A prompts, 110 of which (36.9%) allievates the hallucination of LVLM with detr-resnet-101 on Qwen-VL-Chat. In the 1,202 Type B prompts, 563 of which (46.8%) allievates the hallucination of LVLM.\n\nc) With the help of ChatGPT, we manually change the object appear in question to its synonyms respectively. The evaluation of object hallucination slightly decreases from 87.70% to 86.53% on Qwen-VL-Chat and from 78.38% to 71.54% on mPLUG-Owl, but there is still a non-trival improvement over the baseline especially on mPLUG-Owl, the results are shown in the table below.\n| Model | Setting | Accuracy (%) |\n| ---| --- | --- |\n| mPLUG-Owl | baseline | 57.29 |\n|  | before switching to synonyms | 78.38 |\n|  | after switching to synonyms | **71.54** |\n| Qwen-VL-Chat | baseline | 81.23 |\n|  | before switching to synonyms | 87.70 |\n|  | after switching to synonyms | **86.53** |\n\n\nIn summary, with the LVLM's intrinsic robust ability to understand the output visual evidence of small models, visual evidence prompting is also robust in terms of the specific words used for objects in the given problem. We have add the above discussions in the revision (Appendix A.3).\n\n**Q2:** Scale of Image:\n> does it matter to change the scale of the image? for example, \"cup <0, 284, 133, 424>\" to \"<0 /width, 284 / height, 133 / width, 424 / height>\". or to \"<0 /width * 2, 284 / height * 2, 133 / width * 2, 424 / height *2>\". I wonder to what extend LVLMs will di\u00eberentiate object relations\n\n***Ans for Q2:***\n\nThanks for your careful review. In Qwen-VL-Chat, for any given bounding box, a normalization process is applied (within the range [0, 1000)). \nFor example, x_input = x_pixel / width * 1000, y_input = y_pixel / height * 1000. So the scale of the image does not differentiate any object relations. We clarify this setting in Sec. 4.1.3 in the revision."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3272/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700283643856,
                "cdate": 1700283643856,
                "tmdate": 1700283643856,
                "mdate": 1700283643856,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7tgQPYDp6H",
                "forum": "xh3XUaB8M9",
                "replyto": "1rRx0H2SxD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely looking forward to your feedbacks"
                    },
                    "comment": {
                        "value": "Dear reviewer YNjN,\n\nWe sincerely apologize for inconveniencing you, but we have no other choice at the end of the discussion period. We sincerely hope you can understand.\n\nThank you for taking the time to review our work. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work. Your support or feedback are very important to us. We greatly appreciate your constructive comments and efforts.\n\nBest regards\n\nAuthors of #3272"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3272/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700473964258,
                "cdate": 1700473964258,
                "tmdate": 1700473964258,
                "mdate": 1700473964258,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zC7JEVJezW",
                "forum": "xh3XUaB8M9",
                "replyto": "1rRx0H2SxD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely looking forward to your feedbacks"
                    },
                    "comment": {
                        "value": "Dear reviewer YNjN,\n\nWe sincerely apologize for inconveniencing you again, but we really have no other choice as the discussion period is drawing to a close (only 1 days left). We sincerely hope you can understand.\n\nThank you for taking the time to review our work. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work. Your support or feedback are very important to us. We greatly appreciate your constructive comments and efforts.\n\nSorry again for inconveniencing you.\n\nBest regards\n\nAuthors of #3272"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3272/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560255282,
                "cdate": 1700560255282,
                "tmdate": 1700560255282,
                "mdate": 1700560255282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EVdjTO4gbv",
                "forum": "xh3XUaB8M9",
                "replyto": "zC7JEVJezW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3272/Reviewer_YNjN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3272/Reviewer_YNjN"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks authors for the response & clarification. The additional analysis and experiments on object overlap is indeed interesting and shows the proposed method still outperforms baselines. But the authors are encouraged to keep improving the method and making the approach truly generalizable.\n\nMy rating is updated based on the provided response."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3272/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631891080,
                "cdate": 1700631891080,
                "tmdate": 1700631891080,
                "mdate": 1700631891080,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3uhZaF6kfV",
            "forum": "xh3XUaB8M9",
            "replyto": "xh3XUaB8M9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3272/Reviewer_w1s8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3272/Reviewer_w1s8"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the object hallucinations in large-VLM. It points out that the vision model detects the explicit objects that can be used for visual prompts and mitigates the issue of object hallucination. Sufficient experiments shed light on how the vision model and the visual prompts affect object-level hallucination."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper has good motivation, i.e., the vision model affects the hallucinations and the detection results can be used as visual prompting for LLM. The experiments also demonstrate the effectiveness of the proposed idea. The paper is also well-written."
                },
                "weaknesses": {
                    "value": "The authors utilize small detection and scene graph models as evidence, potentially introducing domain-specific knowledge that skews the results. It's crucial to clarify the differences in training data between these small models and the evaluation data. Additionally, the paper should address whether the proposed method remains effective when applied to out-of-domain models.\n\nThe authors should extend their comparisons to include boosting and bagging methods, especially considering the paper primarily demonstrates performance improvements for yes/no VQA questions. This would provide a more robust validation of the proposed method.\n\nGiven that the small detection models used in the study are contrasted with large language models (LLMs), it would be beneficial to also present results using larger versions of these detection models. This would ensure a fair and comprehensive comparison.\n\nThe paper should incorporate a wider array of multilingual large language models (MLLMs), particularly those trained on Visual Genome (VG) data. A comparison with models like LLaVA 1.5, which achieves state-of-the-art performance with a minimal domain data, is essential. The authors should delve into why evidence-based methods might be superior to sampling fine-tuning techniques in this context."
                },
                "questions": {
                    "value": "See the weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3272/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3272/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3272/Reviewer_w1s8"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3272/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698906384513,
            "cdate": 1698906384513,
            "tmdate": 1700505243331,
            "mdate": 1700505243331,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O0UmKqyBMX",
                "forum": "xh3XUaB8M9",
                "replyto": "3uhZaF6kfV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer w1s8 (part 1)"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for taking the time to review. We appreciate that you find our approach motivation good, experimental effectiveness and well-written. According to your valuable comments, we provide detailed feedback.\n\n**Q1:** Out-of-domain data:\n> The authors utilize small detection and scene graph models as evidence, potentially introducing domain-speci\u00ecc knowledge that skews the results.  \n> a) It's crucial to clarify the differences in training data between these small models and the evaluation data.  \n> b) Additionally, the paper should address whether the proposed method remains effective when applied to out-of-domain models.\n\n***Ans for Q1:***\n\na) Thanks for pointing out this potentially confusing point. There is no overlap between the training data and the evaluation data. We are using the open-source small models trained on the COCO  or Visual Genome training set. The test set we have constructed is derived from the COCO validation set or Visual Genome test set. We have clarified this in the revision in Sec. 4.1.1.\n\nb) Thanks for your insightful question. We have incorporated this discussion into our revision in Appendix A.1. Specifcally, we collect 2,540 additional samples from open-world datasets and scenarios (Object365 [A] and OpenImage [B]) to further evaluate the genaliation ability of our method quantitatively. \n\n|  | Model | Setting | Accuracy (%) |\n| --- | --- | --- | --- |\n| Object Hallucination (2000 OOD samples)| mPLUG-Owl | baseline | 52.04 |\n|  |  | **+ visual evidence** | **62.46** |\n|  | Qwen-VL-Chat | baseline | 70.25 |\n|  |  | **+ visual evidence** | **76.74** |\n| Relation Hallucination (540 OOD samples) | mPLUG-Owl | baseline | 58.52 |\n|  |  | **+ visual evidence** | **72.41** |\n|  | Qwen-VL-Chat | baseline | 73.93 |\n|  |  | **+ visual evidence** | **75.98** |\n\nThe above results present the comparison with baseline results for the evaluation on out-of-domain datasets. After incorporating visual evidence prompting, all models enables more precise discernment of object or relation presence within the image.\n\nBesides Figure 5 in the draft that showing some out-of-domain cases, we also follow CLIP and select 2 samples from 10 open-world datasets (without groundtruth) for qualititive analysis, including CLEVER and Caltech 101. These 20 cases are in the revised Appendix F.1.\n\nThe contribution of our method lies in combining small and large models, utilizing the domain-specific knowledge of small models to complement the large models. In practical applications, it is possible to customize different small models to tailor different domain knowledge. The Figure 3 in the draft verified that if the small models struggle to effciently capture visual evidence, incorrect evidence has limited impact on cases that are already correct.\n\n> [A] Objects365: A Large-scale, High-quality Dataset for Object Detection  \n> [B] The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale\n\n**Q2:** Boosting and bagging methods:\n> The authors should extend their comparisons to include boosting and bagging methods, especially considering the paper primarily demonstrates performance improvements for yes/no VQA questions. This would provide a more robust validation of the proposed method.\n\n***Ans for Q2:***\n\nThanks for your insightful comments. In response to the suggestion, we conduct the experiments of utilizing the conventional bagging methods [A] between LVLM and VQA models which does a plurality vote when predicting. Combining the Qwen-VL-Chat and N2NMNs, the evaluation accuracy of object and relation hallucination slightly increases from 81.23% to 83.02% and from 63.62% to 67.91%. This result further validates the effectiveness of our framework.\n\n| | baseline | bagging | Visual Evidence Prompting |\n| --- | --- | --- | --- |\n| Object Hallucination | 81.23 | **83.02** | **87.70** |\n| Relation Hallucination | 63.62 | **67.91** | **75.68** |\n\n> [A] Bagging predictors"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3272/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700282848819,
                "cdate": 1700282848819,
                "tmdate": 1700282848819,
                "mdate": 1700282848819,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SJ71C68O0p",
                "forum": "xh3XUaB8M9",
                "replyto": "3uhZaF6kfV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely looking forward to your feedbacks"
                    },
                    "comment": {
                        "value": "Dear reviewer w1s8,\n\nWe sincerely apologize for inconveniencing you, but we have no other choice at the end of the discussion period. We sincerely hope you can understand.\n\nThank you for taking the time to review our work. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work. Your support or feedback are very important to us. We greatly appreciate your constructive comments and efforts.\n\nBest regards\n\nAuthors of #3272"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3272/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700473924875,
                "cdate": 1700473924875,
                "tmdate": 1700473924875,
                "mdate": 1700473924875,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JBgOpf6Wd7",
                "forum": "xh3XUaB8M9",
                "replyto": "SJ71C68O0p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3272/Reviewer_w1s8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3272/Reviewer_w1s8"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for your detailed reply. My concerns are addressed and raise my score.\n\nBest regards"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3272/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505211651,
                "cdate": 1700505211651,
                "tmdate": 1700505211651,
                "mdate": 1700505211651,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yPDEcfRu9j",
            "forum": "xh3XUaB8M9",
            "replyto": "xh3XUaB8M9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3272/Reviewer_rr7K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3272/Reviewer_rr7K"
            ],
            "content": {
                "summary": {
                    "value": "The research addresses the persistent issue of hallucination in Large Vision-Language Models (LVLMs, such as GPT-3) where these models tend to make predictions of objects and relations that do not exist in the input images. The study highlights that while traditional small visual models produce professional and accurate outputs, they lack the ability to effectively interact with humans.\n\nThe primary focus of the research is to investigate how small visual models can complement LVLMs by extracting contextual information from images to generate precise answers. The proposed approach, known as \"visual evidence prompting,\" demonstrates a natural mitigation of hallucination in LVLMs. This technique involves providing visual knowledge as context to reduce the hallucination problem. The study includes experiments conducted on three large language models, showing improved performance in addressing object hallucinations and a new benchmark for relation hallucinations.\n\nOverall, the research aims to serve as a baseline for challenging hallucination benchmarks and emphasizes the significance of exploring and analyzing the substantial visual evidence concealed within small visual models before fine-tuning LVLMs. The paper presents an intriguing approach to mitigating hallucination in LVLMs, but there may be room for further clarification and validation of the proposed method in different scenarios and datasets. Additionally, it would be beneficial to provide a more detailed discussion of potential applications and practical implications of the findings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation is clear\n2. The presentation is good"
                },
                "weaknesses": {
                    "value": "In light of the issue of object and relation hallucinations within the existing LVLM model, this paper employs the object and relation information captured by the visual component of the mini model as a contextual query to mitigate the LVLM hallucination problem. However, this approach gives rise to two significant concerns:\n\nEfficiency: What degree of efficiency degradation can be expected with the introduction of new mini models for object detection and scene graph generation into the original method?\n\nDomain Compatibility: Is there any overlap between the datasets used for training the small target detection and scene graph generation models and the LVLM hallucination test set? \nIn the context of an open-world scenario where the small model might struggle to efficiently capture target and relation information, to what extent will the acquisition of false evidence affect the LVLM's otherwise accurate responses? It's worth noting that despite the findings presented in Figure 3, my skepticism regarding this issue persists."
                },
                "questions": {
                    "value": "na"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "na"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3272/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698999299203,
            "cdate": 1698999299203,
            "tmdate": 1699636275651,
            "mdate": 1699636275651,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hXp4VpmOAk",
                "forum": "xh3XUaB8M9",
                "replyto": "yPDEcfRu9j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer rr7K"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for taking the time to review. We appreciate that you find our approach intriguing, the motivation clear, and the presentation good. According to your valuable comments, we provide detailed feedback.\n\n**Q1:** Efficiency:\n> What degree of efficiency degradation can be expected with the introduction of new mini models for object detection and scene graph generation into the original method?\n\n***Ans for Q1:***\nThanks for your constructive comments. We compute the average inference time of several small models on one A100 GPU.\n\n| yolos-small | detr-resnet-101 | RelTR | OpenPSG |\n| -------- | -------- | -------- | -------- |\n|0.193s     | 0.163s     | 0.394s     |0.218s|\n\nMeanwhile, the average time for one inference of Qwen-VL-Chat and mPLUG-Owl is:\n\n| Qwen-VL-Chat | mPLUG-Owl |\n| -------- | -------- |\n| 1.077s    | 1.157s     |\n\nAfter incorporating the small models, the relative increase in inference time is an average of 21.78%. Moreover, in multi-turn dialogue scenarios, the image is only uploaded once, regardless of how many questions are asked. The small model only needs to perform inference once, making the cost of inference relatively smaller.\n\n**Q2:** Domain Compatibility:\n> a) Is there any overlap between the datasets used for training the small target detection and scene graph generation models and the LVLM hallucination test set?  \n> b) In the context of an open-world scenario where the small model might struggle to efficiently capture target and relation information, to what extent will the acquisition of false evidence affect the LVLM's otherwise accurate responses?\n\n***Ans for Q2:***\n\na) Thanks for pointing out this potentially confusing point. There is no overlap between the datasets used for training small models and the LVLM hallucination test sets. We are using the open-source small models trained on the COCO  or Visual Genome training set. The test set we have constructed is derived from the COCO validation set or Visual Genome test set. We have clarified this in the revision in Sec. 4.1.1.\n\nb) Thanks for your insightful question. We conduct more evaluations and incorporate the discussions into our revision in Appendix A.1. \n* Firstly, we collect 2,540 additional samples from open-world datasets and scenarios to further quantitatively evaluate the genaliation ability of our method. More details of these two out-of-domain datasets Object365 [A] and OpenImage [B]are added in the revision (Appendix A.1). In 20.6% of the images, small model captures incorrect or partial correct object or relation information. With these visual evidences, **only 8%** of the false evidence confuse the LVLM and change the response from collect to wrong. \n* Secondly, besides Figure 3, we follow CLIP and select 2 samples from 10 open-world datasets (without groundtruth) for qualititive analysis. These 20 cases are in the revised Appendix F.1. From these results and cases, it can be seen that in **open-world scenarios, incorrect evidence still has limited impact on cases that are already correct**. \n* Thirdly, The fifth prompt template in Table 3 in the draft shows that we can tell the LVLM that *the evidence might be wrong and keep your answer if you think the evidence is wrong or missing.*\n\nIn summary, quantitative and qualititive analysis show that false evidence has limited affect on LVLM's accurate responses with the help of carefully designed prompt.\n\n> [A] Objects365: A Large-scale, High-quality Dataset for Object Detection  \n> [B] The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3272/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700282486144,
                "cdate": 1700282486144,
                "tmdate": 1700282486144,
                "mdate": 1700282486144,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vn5iFBN4mK",
                "forum": "xh3XUaB8M9",
                "replyto": "yPDEcfRu9j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely looking forward to your feedbacks"
                    },
                    "comment": {
                        "value": "Dear reviewer rr7K,\n\nWe sincerely apologize for inconveniencing you, but we have no other choice at the end of the discussion period. We sincerely hope you can understand.\n\nThank you for taking the time to review our work. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work. Your support or feedback are very important to us. We greatly appreciate your constructive comments and efforts.\n\nBest regards\n\nAuthors of #3272"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3272/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700473881306,
                "cdate": 1700473881306,
                "tmdate": 1700473881306,
                "mdate": 1700473881306,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XU3ZtUp1jf",
                "forum": "xh3XUaB8M9",
                "replyto": "yPDEcfRu9j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely looking forward to your feedbacks"
                    },
                    "comment": {
                        "value": "Dear reviewer rr7K,\n\nWe sincerely apologize for inconveniencing you again, but we really have no other choice as the discussion period is drawing to a close (only 1 days left). We sincerely hope you can understand.\n\nThank you for taking the time to review our work. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work. Your support or feedback are very important to us. We greatly appreciate your constructive comments and efforts.\n\nSorry again for inconveniencing you.\n\nBest regards\n\nAuthors of #3272"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3272/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560212168,
                "cdate": 1700560212168,
                "tmdate": 1700560212168,
                "mdate": 1700560212168,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sFSkNFBrIp",
                "forum": "xh3XUaB8M9",
                "replyto": "yPDEcfRu9j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3272/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely request for an opportunity to discuss with you"
                    },
                    "comment": {
                        "value": "Dear reviewer rr7K,\n\nDeep apologies for the repeated interruption. Sorry so much!\n\nWe are truly honored that you have reviewed our paper and provided valuable constructive suggestions. Sincerely thanks for these constructive suggestions which greatly aid in our paper\u2019s refinement, we have diligently addressed every concern and question you raised during the initial review. We genuinely hope our responses have resolved your concerns and provided satisfactory explanations. We sincerely appreciate your dedication and valuable time.\n\nTo further improve our work, we sincerely hope for a valuable opportunity to discuss with you. It would be greatly appreciated if we could have a chance to hear your feedback. Your feedback is highly valuable to our paper and greatly contributes to the entire community.\n\nVery sorry for inconveniencing you again. Hope you have a good day!\n\n\nBest regards and many thanks,\n\nAuthors of #3272"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3272/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665179533,
                "cdate": 1700665179533,
                "tmdate": 1700665179533,
                "mdate": 1700665179533,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]