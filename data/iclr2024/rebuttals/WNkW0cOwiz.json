[
    {
        "title": "Lipschitz Singularities in Diffusion Models"
    },
    {
        "review": {
            "id": "dWS4dZDVrv",
            "forum": "WNkW0cOwiz",
            "replyto": "WNkW0cOwiz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1058/Reviewer_gGRw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1058/Reviewer_gGRw"
            ],
            "content": {
                "summary": {
                    "value": "The paper elaborates upon an important observation concerning the presence of infinite-Lipschitz constants in the diffusion process, made earlier by (Song et al., 2021a; Vahdat et al., 2021). It also proposes a simple yet effective approach to address this challenge."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Theorem 3.1 is a nice piece of rigorous analysis of diffusion models, albeit indebted to Song et al.\n\nThe proposed approach to address this infinite-Lipschitz challenge, which is based on improving the resolution of the discretisation, does indeed seem to be effective. \n\nNumerical results in Figures 3 and 4 seem quite impressive."
                },
                "weaknesses": {
                    "value": "The observation concerning the presence of infinite-Lipschitz constants in the diffusion process is not original (Song et al., 2021a; Vahdat et al., 2021). Concerning it has been observed before, the authors should like to tone down their claims of having observed it first. \n\nSome of the English is stilted (\"vexing propensity of diffusion models\" in the abstract, \"Recently, there have been massive variants that significantly promote the development of diffusion models\" on page 3)."
                },
                "questions": {
                    "value": "How would you describe the differences in your observation and those of (Song et al., 2021a; Vahdat et al., 2021)? \n\nYou could make your observation more original by noting that the infinite Lipchitz constants mean the SDE need not have a unique strong solution (\u00d8ksendal, 2003). Exhibiting multiple solutions would indeed be of interest."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1058/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698005299056,
            "cdate": 1698005299056,
            "tmdate": 1699636032033,
            "mdate": 1699636032033,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BccsRhvLw5",
                "forum": "WNkW0cOwiz",
                "replyto": "dWS4dZDVrv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, we sincerely appreciate your valuable comments. Thank you for acknowledging the soundness of our research and the efficacy of our approach. Such recognition greatly boosts our confidence. Additionally, we are grateful for your insightful opinion, \u201cthe infinite Lipchitz constants mean the SDE need not have a unique strong solution\u201d. We are deeply inspired by it.\n\n**Q1: The observation concerning the presence of infinite-Lipschitz constants in the diffusion process is not original. How would you describe the differences in your observation and those of (Song et al., 2021a; Vahdat et al., 2021).**\n\n**A1:** Thank you for reminding us of these related works. The aforementioned works indeed observed numerical instability issues near zero point, which are of great importance. **However, it is distinct from our work.** The numerical issues observed by previous works are mainly caused by the **singularity of the Gaussian transition kernel $q_{0t}(x_t|x_0)$ as $\\sigma_t \\rightarrow 0$**. However, our observation is *the infinite Lipschitz constants of the noise prediction model $\\epsilon_\\theta(x, t)$ w.r.t time variable $t$*, which are caused by **the explosion of ${\\rm d}\\sigma_t / {\\rm d}t$, but not $\\sigma_0=0$**. **To the best of our knowledge, the infinite Lipschitz constants are not pointed out by these works**. Besides, large Lipschitz constants near zero will pose a significant threat to both training and inference: during training, it can hurt the training on other timesteps due to the smooth nature of the neural network; during inference, it can affect the accuracy of numerical integration. **Such instability issues can not be well solved by the methods of previous works**. **Therefore, our work explores a different topic from previous works**. We believe the observation of the instability issues caused by the infinite Lipschitz constants, combined with our proposed solution to mitigate the instability, constitutes a good contribution to the diffusion model community.\n\n**Detailed analysis**\nWhile we have added further discussion in the related-work section (**Section 2**) of our paper, we also give a brief summary of the numerical issue observed by each related work and our work as below:\n\n**Song et al., 2021a:** In this work, it is mentioned in **section 4.3** that numerical instabilities exist when $t\\rightarrow 0$. However, the authors don't point out what kind of numerical instability issue it is, and they don't provide any further analysis of the reasons behind the problem.\n\n**Vahdat et al., 2021:** In this work, it is pointed out in **Appendix G.4** that integration of probability flow ODE should be cut off close to zero since $\\sigma_t$ goes to zero at $t=0$. However, there is no explanation why $\\sigma_t \\rightarrow 0$ makes integration cutoff necessary.\n\n**Song et al., 2021b:** This work is cited by Vahdat et al., 2021. To the best of our knowledge, this is the first work that proposes to cut off near zero during training and sampling for diffusion models. Specifically, it is pointed out in **Appendix C** that the variance of $x_t$ in the Gaussian perturbation kernel $p_{0t}(x_t | x_0)$ vanishes as $t \\rightarrow 0$. This vanishing variance can cause numerical instability issues for training and sampling at $t=0$. Therefore, computation is restricted to $t \\in [\\epsilon, 1]$ for a small $\\epsilon > 0$, *i.e.*, the cutoff strategy mentioned in Vahdat et al., 2021. Indeed, when the variance of $x_t$, denoted as $\\sigma_t^2$, goes to zero, the Gaussian perturbation kernel $p_{0t}(x_t | x_0)=\\mathcal{N}(x_t; \\alpha_t x_0, \\sigma_t^2 {\\bf I}) \\propto \\frac{1}{\\sigma_t^d}\\exp(-\\frac{1}{2\\sigma_t^2}\\Vert x(t) - \\alpha_t x_0\\Vert^2)$ will degrade to a Dirac kernel $\\delta(x_t - \\alpha_t x_0)$, and its log gradient $\\nabla_{x_t} \\log p_{0t}(x_t | x_0) = -\\frac{1}{\\sigma_t^2}(x_t - \\alpha_t x_0)$ is not well defined when $\\sigma_t=0$. As the score-based model $s_\\theta(x, t)$ is directly optimized by denoising score matching with learning objective $\\mathcal{L}_ t^{\\rm score} = \\mathbb{E}_ {x_ 0, x_ t}[\\Vert s_ \\theta(x, t) - \\nabla_ {x_ t} \\log p_ {0t}(x_ t | x_ 0)\\Vert _ 2^2 ]$, an ill-defined $\\nabla_{x_t} \\log p_{0t}(x_t | x_0)$ at $t=0$ can cause numerical issues, as mentioned in Song et al., 2021b."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448639135,
                "cdate": 1700448639135,
                "tmdate": 1700448639135,
                "mdate": 1700448639135,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l3Y54OBfWY",
                "forum": "WNkW0cOwiz",
                "replyto": "dWS4dZDVrv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Our work:** Different from previous works, the focus of our work is the noise prediction model $\\epsilon_\\theta(x, t) = -\\sigma_t \\nabla_{x_t} \\log p_t(x_t)$, and its learning objective at any $t$ is $\\mathcal{L}_ t^{\\rm noise}(\\theta) = \\mathbb{E}_ {x_0, \\epsilon}[\\Vert \\epsilon_ \\theta(\\alpha_t x_0 + \\sigma_t \\epsilon, t) - \\epsilon \\Vert_ 2^2]$. Although $\\mathcal{L}_ t^{\\rm noise}$ only differs from $\\mathcal{L}_ t^{\\rm score}$ by a scalar transformation $\\epsilon = -\\sigma_t \\nabla \\log p_{0t}(\\alpha_t x_0 + \\sigma_t \\epsilon | x_0)$ when $\\sigma_t > 0$, $\\mathcal{L}_t^{\\rm noise}$ exhibits better numerical properties at $t=0$ since $\\epsilon$ is just a sample of Gaussian distribution, and it is well defined when $\\sigma_t=0$. From this point of view, noise prediction model should alleviate the numerical issues mentioned in Song et al., 2021b.  However, our analysis reveals that noise prediction model suffers from another numerical issue, *i.e.*, the infinite Lipschitz constants near zero. This problem is caused by the explosion of $\\frac{{\\rm d} \\sigma_t}{{\\rm d} t}$ near $t=0$, but not the vanishing variance $\\sigma_t^2 \\rightarrow 0$ as indicated in previous works. \n\n**Please let us know if there are any of your concerns, we are very willing to discuss them with you**\n\n**Q2: Some of the English is stilted.**\n\n**A2:** Thank you for incorporating the suggestions. Here's the revised version of the sentences:\n+ \"In this paper, we explore a perplexing tendency of diffusion models: they often display the infinite Lipschitz property of the network with respect to the time variable near the zero point.\"\n+ \"The significant advancements of diffusion models have been witnessed in recent years in the domain of image generation.\"\nBesides, we have relocated the second sentence to the beginning of the related works (Section 2) to enhance the logical flow and clarity."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448755010,
                "cdate": 1700448755010,
                "tmdate": 1700448755010,
                "mdate": 1700448755010,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VU6KDi2RlM",
                "forum": "WNkW0cOwiz",
                "replyto": "dWS4dZDVrv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q3: You could make your observation more original by noting that the infinite Lipchitz constants mean the SDE need not have a unique strong solution (\u00d8ksendal, 2003). Exhibiting multiple solutions would indeed be of interest.**\n\n**A3:** Thank you for your kind suggestion. It is really an interesting problem. According to Theorem 5.2.1 (Existence and uniqueness theorem for stochastic differential equations) in the book you mentioned (\u00d8ksendal, 2003), for a general SDE ${\\rm d} X_t = b(t, X_t){\\rm d}t + \\sigma(t, X_t){\\rm d}B_t$, an unique strong solution exists if measurable functions $b(\\cdot, \\cdot)$ and $\\sigma(\\cdot, \\cdot)$ are bounded and globally Lipschitz in state. Thus, if the infinite Lipschitz constants in time can break either of these two conditions, the SDE may not have a unique strong solution.\n\nHowever, in diffusion models, for any $\\epsilon > 0$, a unique strong solution exists in $[\\epsilon, T]$ since the above two conditions are satisfied. Multiple solutions can only exist at the zero point. Thus, we can cut off $[0, \\epsilon]$ as proposed in previous milestone works (Song et al., 2021a,b; Vahdat et al., 2021), and this will not hurt the practical performance in most cases with a suitable $\\epsilon$.\n\nIt is worth noting that the cut-off $\\epsilon$ should be small enough to avoid performance degradation. Thus, near $t=\\epsilon$, the Lipschitz constants in time may still be large, and this can also lead to numerical instability in both training and inference. As demonstrated in our work, the instability issues can be significantly mitigated with our proposed method.\n\nAll in all, the uniqueness of the solution is an important and interesting question, and thank you for your suggestion again. We will add this discussion to our paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700457239727,
                "cdate": 1700457239727,
                "tmdate": 1700457239727,
                "mdate": 1700457239727,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9GvNqZOCi6",
                "forum": "WNkW0cOwiz",
                "replyto": "dWS4dZDVrv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, We would like to know if our response has dispelled all your concerns. If we have dispelled all your concerns, would you please raise our score? If there are any of your concerns, please let us know. We are very willing to discuss them with you."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700457727686,
                "cdate": 1700457727686,
                "tmdate": 1700457882688,
                "mdate": 1700457882688,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aDtZ6Ljrn8",
                "forum": "WNkW0cOwiz",
                "replyto": "dWS4dZDVrv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, we notice that the response period is nearing its end. If you still have any concerns, we would greatly appreciate hearing from you. We eagerly await your response."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635158492,
                "cdate": 1700635158492,
                "tmdate": 1700635424013,
                "mdate": 1700635424013,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uRmiVcf9KH",
            "forum": "WNkW0cOwiz",
            "replyto": "WNkW0cOwiz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1058/Reviewer_qCuR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1058/Reviewer_qCuR"
            ],
            "content": {
                "summary": {
                    "value": "This paper deals with an exploding Lipschitz constant in the function a neural network is asked to learn in a DDPM model, and the negative effects of trying to learn a function with such.\n\nThe authors present an argument based on taking time derivative of the quantity $-\\sigma_t \\nabla \\log q_t(x)$, where the $\\sigma_t$ are the standard deviations of the forward noising process in the time-discrete forward noising process in the DDPM formulation and $q_t$ is the density of the data distribution diffused to discrete time step $t$.\n\nThey demonstrate that the Lipshitz constant in time explodes to infinity for a most parameter settings of common noising schedules under the DDPM/VPSDE setting.\n\nThe authors propose a method for fixing this issue by applying a transform to the time input of the score network, tying together multiple timestep near t=0 to have the same score.\n\nThey demonstrate significant empirical benefit over a range of diffusion modelling tasks.\n\nThe authors also discuss a number of other possible methods to alleviate the issue of learning high lipshitz constants in diffusion models, but show that these methods despite being theoretically attractive, do not perform as well in practise."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The method proposed is simple to implement.\n2) The method clearly demonstrates significant empirical benefit.\n3) The authors discuss alternative proposals and show these are less effective"
                },
                "weaknesses": {
                    "value": "1) The only weakness I would like to highlight is the discussion of the alternative methods presented.\n - I believe 1 of the methods from the appendix is not mentioned in the main text - namely the Remp method (D.3.3).\n - It would be nice to see an expanded discussion of these with some small experiment to show the quantitative difference between the proposed method and these other methods. I appreciate the space limitation, but I think this is really an interesting point."
                },
                "questions": {
                    "value": "1. Could the authors highlight better which lipshitz constant is is that is important, and why we care about it? While I understand I believe which and why it is cared about, it is perhaps not the clearest from reading the paper. The sentence in the abstract \"they frequently exhibit the infinite Lipschitz near the zero point of timesteps\" is a good example of this - it does not specify _what_ function has high lipshitz constant, or why indeed that matters. From reading the paper in depth, the authors care about the lisphitz constant of the quantity $-\\sigma_t \\nabla \\log q_t(x)$ as a) neural networks find it difficult to learn high lipshitz constant functions, and this is the function we are asking the score net to learn, and b) because this quantity is involved in the reverse rollouts, having a term with high lipshitz constant makes discretising the SDE challenging to do accurately, but this should be apparent from the abstract/introduction."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1058/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1058/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1058/Reviewer_qCuR"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1058/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698670571623,
            "cdate": 1698670571623,
            "tmdate": 1699636031960,
            "mdate": 1699636031960,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l4fUUqfnfh",
                "forum": "WNkW0cOwiz",
                "replyto": "uRmiVcf9KH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, We extend our gratitude for acknowledging the significance of our research. Your commendation of our proposed method for its simplicity and efficacy, as well as your appreciation of our analysis of alternative methods, serve as a profound source of motivation for us. In light of your invaluable feedback, we have diligently incorporated several enhancements into our paper. We firmly believe that these refinements shall considerably augment the overall quality of our work.\n\n**Q1: \"Remap\" is not mentioned in the main text.**\n**A1:** Yes, thank you very much for reminding us. We didn't mention \"Remap\" in our main text because of space constraints. we have added a brief discussion of Remap in the \"Comparison with some alternative methods\" part of **Section 4**. This discussion aims to introduce the concept of Remap and highlight its limitations. Our intention is to draw the readers' attention to this intriguing alternative approach.\n\n**Q2: It would be nice to see an expanded discussion of these with some small experiment to show the quantitative difference between the proposed method and these other methods.**\n**A2:** Thank you for your advice, we have incorporated a new figure (**Figure 3** of the updated PDF) in the \"Comparison with some alternative methods\" part of **Section 4**, to visually represent the quantitative evaluation of these alternative methods. By including this visual representation, we aim to facilitate a rapid understanding of the performance of these alternative methods for the readers.\n\n**Q3: Could the authors highlight better which lipshitz constant is important, and why we care about it?**\n**A3:** Thank you for your valuable suggestion. We have made the necessary adjustments in the **abstract** and **introduction** by explicitly mentioning which specific Lipschitz constants we are concerned with. Additionally, we have included a concise discussion in the **introduction** to elucidate the importance of these Lipschitz constants. Our aim with these modifications is to enhance the readers' comprehension of our work and minimize any potential confusion."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413402759,
                "cdate": 1700413402759,
                "tmdate": 1700413402759,
                "mdate": 1700413402759,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JbAYMMfuW7",
                "forum": "WNkW0cOwiz",
                "replyto": "uRmiVcf9KH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, We would like to know if our response has dispelled all your concerns. If there are any of your concerns, please let us know. We are very willing to discuss them with you."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700457089810,
                "cdate": 1700457089810,
                "tmdate": 1700457771231,
                "mdate": 1700457771231,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1KIyF4ZSWp",
            "forum": "WNkW0cOwiz",
            "replyto": "WNkW0cOwiz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1058/Reviewer_DUEU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1058/Reviewer_DUEU"
            ],
            "content": {
                "summary": {
                    "value": "This paper demonstrates theoretically (and confirms empirically) that the limit of the Lipschitz constant of the noise prediction network for a timestep of zero is infinite. Such a result is a source of instability for using diffusion models in many generative tasks, and the authors propose a technical solution to alleviate this issue and confirm the superiority of the approach with extensive numerical simulations."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is an excellent paper, and the presentation is very well carried out. The authors point out a very interesting theoretical property that could explain some practical instabilities encountered in DDPM samples. They then present a practical solution to the problem. The authors' contribution is excellent for the community, as reducing the instabilities in the generative process, such as diffusion models, has important practical consequences."
                },
                "weaknesses": {
                    "value": "This paper as it is impeccable in terms of presentation and contribution, both theoretically and practically. The only drawback is that no open-source code is available to experiment with their approach."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1058/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698774759169,
            "cdate": 1698774759169,
            "tmdate": 1699636031873,
            "mdate": 1699636031873,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NH2u6PQtJy",
                "forum": "WNkW0cOwiz",
                "replyto": "1KIyF4ZSWp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate your kind words and recognition of our work. We are highly encouraged by your kind words \"The authors' contribution is excellent for the community, as reducing the instabilities in the generative process, such as diffusion models, has important practical consequences.\" We have anonymously submitted the core part of our code as supplementary material and will make it publicly available once the paper is accepted. Thank you once again for your support."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413238235,
                "cdate": 1700413238235,
                "tmdate": 1700413238235,
                "mdate": 1700413238235,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "syx7W5AtBo",
            "forum": "WNkW0cOwiz",
            "replyto": "WNkW0cOwiz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1058/Reviewer_gj5E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1058/Reviewer_gj5E"
            ],
            "content": {
                "summary": {
                    "value": "Diffusion models, utilizing stochastic differential equations to generate images, have become a leading type of generative model. However, their underlying diffusion process hasn't been thoroughly examined. This paper reveals a concerning tendency in diffusion models: they often display infinite Lipschitz (for $\\sigma_{t} \\cdot \\text{score function}$) near the initial timesteps. Through theoretical and empirical evidence, the presence of these infinite Lipschitz constants is confirmed, which can jeopardize the stability and precision of the models during training and inference. To combat this, the paper introduces a new method, E-TSDM, that uses quantization to reduce these Lipschitz issues. Tests on various datasets support the presented theory and approach, potentially offering a deeper understanding of diffusion processes and guiding future diffusion model design."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper highlights a unique and previously unexplored challenge with DDPM: the instability encountered when learning $\\epsilon_{\\theta} = \\sigma_{t} \\cdot \\nabla \\log q_{t}(x)$ during the time steps where $\\sigma_{t}$ is minimal. One might naturally question why DDPM doesn't directly learn $\\nabla \\log q_{t}(x)$. I conjecture that the optimization process for learning $\\nabla \\log q_{t}(x)$, which involves solving $E\\|\\nabla \\log q_{t}(x) - \\frac{1}{\\sigma_{t}} \\|^2$, becomes problematic with a small $\\sigma_{t}$. As a workaround, DDPM employs a transformation to learn $\\sigma_{t}\\cdot \\nabla \\log q_{t}(x)$ directly. However, this paper reveals the inherent price of such an approach (no free lunch indeed).\n\nThe paper validates the infinite Lipschitz problem with $\\epsilon_{\\theta}$ both theoretically and empirically. Moreover, it introduces E-TSDM, an innovative solution that essentially employs a quantization strategy when $\\sigma_{t}$ is minimal, particularly during the initial t=100 steps. Comprehensive experiments demonstrate E-TSDM's enhanced stability and performance, even setting a new benchmark for FFHQ 256\u00d7256.\n\nThe paper's novelty is commendable, presenting a compelling and succinct argument with an impressive practical performance. Its insights could significantly influence the diffusion model community. I'm inclined to strongly endorse its acceptance."
                },
                "weaknesses": {
                    "value": "- One minor suggestion is to avoid saying $t$ being small (rather, it is about $\\sigma_{t}$ being small). Since $t$ is in fact $0, 1, 2, 3, .. 100.$ \n- May add more discussions to the alternative approaches (see Questions below). \n- It may be worth showing that directly learning $\\nabla \\log q_{x}(t)$ with the least square is prohibitve."
                },
                "questions": {
                    "value": "I am looking for comments from the authors on a few alternative methods:\n1. Learning $\\nabla \\log q_{x}(t)$ directly with weighted least square: can we reduce the weight of the least square when $\\sigma_t$ is small, e.g., learn $E \\sigma_{t}^2\\|\\nabla \\log q_{x}(t) - \\frac{1}{\\sigma_{t}}I\\|^2$?\n2. For Eq (9), what if we only learn $\\epsilon_{\\theta}(\\alpha_{f_T(t)}x_0 + \\sigma_{f_T(t)}\\epsilon, f_{T}(t))$, i.e., only learn the score function for time $f_{T}(t)$ and only use those time steps to do sampling? \n3. Is that $\\sigma_{t}$ an input of the neural network: what if we learn $\\epsilon_{\\theta}(x, \\sigma_{t})$ (the intuition is that $\\sigma_{t}$ will help adjust the network Lipschize automatically). \n\nMinor comments:\n- Eq 10, are $\\beta_{t}$ and $\\eta_{t}$ defined?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1058/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1058/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1058/Reviewer_gj5E"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1058/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820591544,
            "cdate": 1698820591544,
            "tmdate": 1699636031787,
            "mdate": 1699636031787,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QxGtgYnLDm",
                "forum": "WNkW0cOwiz",
                "replyto": "syx7W5AtBo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Esteemed reviewer, we express our heartfelt gratitude for your recognition of the novelty, effectiveness, and contribution of our work. Additionally, we extend our utmost appreciation for your invaluable advice. We firmly believe that these suggestions hold the potential to enhance the quality of our paper.\n\n**1. This work reveals the inherent price of predicting noise.**\nYour astute observation regarding our work's revelation of the inherent cost associated with predicting noise instead of directly learning $\\nabla \\log q_ t(x)$ is greatly appreciated. We wholeheartedly concur with your insightful perspective. In response to this, we have included detailed discussions about this viewpoint within **Section 2** (Related Works) of our paper. Specifically, we begin by introducing the challenges associated with directly learning the score function, followed by an exposition on commonly employed noise-prediction models. Ultimately, we emphasize that our research reveals the inherent drawbacks of noise-prediction models.\n\n**2. Reply for questions about alternative methods**\nFollowing your recommendations, we have incorporated four additional experiments on FFHQ $256\\times 256$ into our research. The evaluation metric used for these experiments is FID-10k. To ensure fairness, we have maintained the same experimental settings as those employed in the baseline.\n\n**Q1: Directly learning $\\nabla \\log q_t(x)$ with least square and with weighted least square.**\n**A1:** In accordance with your analysis, directly learning $\\nabla \\log q_t(x)$ with least square presents challenges, especially with a small $\\sigma_t$. We have implemented this approach and present a quantitative comparison in the table below. The experimental results affirm that directly learning $\\nabla \\log q_t(x)$ using the least square method is not feasible.\n\nMoreover, we have also attempted to learn $\\nabla \\log q_t(x)$ using a weighted least square approach. Specifically, we aimed to learn $\\theta^* = \\arg\\min_ {\\theta} \\mathbb{E}_ t \\left[ \\sigma_t^2\\mathbb{E}_ {x}\\left[ \\Vert s_ \\theta(x, t) - \\nabla \\log q_ {0t}(x|x_0) \\Vert_2^2\\right] \\right] $. Intuitively, the weighted least square method should outperform the original least square method, as it mitigates the influence of problematic intervals. As depicted in the subsequent table, learning $\\nabla \\log q_t(x)$ with the weighted least square method yields significantly superior results compared to direct learning with the least square method.\n\n**Table1. Quantitative comparison between directly learning $\\nabla \\log q_t(x)$ with the least square and the baseline on FFHQ $256\\times 256$.**\n| Method | FID-10k |\n| --- | --- |\n| Learning $\\nabla \\log q_t(x)$ with LS | 97.96|\n| Learning $\\nabla \\log q_t(x)$ with weighted LS | 13.87 |\n| Baseline | 9.50 |\n\n**Q2: What if we only learn the score function for time $f_T(t)$ and only use those time steps to do sampling.**\n**A2:** This method exhibits inferior performance compared to the baseline. The experimental results for FFHQ $256\\times 256$ are presented in the table below. Specifically, when $t \\ge 100$, we train the score function for $t=100, 101, \\dots, T-1$, and when $t < 100$, we only train the score function for $t = 0, 20, 40, 60, 80$. During inference, we exclusively employ the timesteps of $f_T(t)$ for sampling. Likewise, the baseline method also restricts sampling to the timesteps of $f_T(t)$. The experimental results, depicted in the subsequent table, demonstrate the inferior performance of this method compared to the baseline.\n\nAlthough sampling in our method is limited to the timesteps of $f_T(t)$ ($t = 0, 20, 40, 60, 80$ when $t < 100$), training on additional timesteps can provide valuable information and enhance the network's denoising capabilities. By solely learning from the timesteps of $f_T(t)$, the network struggles to capture information from other timesteps, resulting in subpar performance. On the other hand, if we adopt our proposed method to train on all timesteps but limit sampling to the timesteps of $f_T(t)$ , then it becomes the situation of DDIM sampling. The outcomes of DDIM are presented in **Table 3** of our paper. However, if we extend the sampling to all timesteps, it will further enhance the performance.\n\n**Table2. Quantitative comparison between only training for time $f_T(t)$ and the baseline on FFHQ $256\\times 256$.**\n| Method | FID-10k |\n| --- | --- |\n| Training on all $f_T(t)$, sampling on $f_T(t)$ | 48.15|\n| Training on all $t$, sampling on $f_T(t)$ (Baseline) | 21.75 |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700412931190,
                "cdate": 1700412931190,
                "tmdate": 1700412931190,
                "mdate": 1700412931190,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ucwQmS25fz",
                "forum": "WNkW0cOwiz",
                "replyto": "syx7W5AtBo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q3: What if we learn $\\epsilon_\\theta(x, \\sigma_t)$.**\n**A3:** Based on our understanding, this method aims to replace the network's conditional input $t$ with its corresponding $\\sigma_t$. If there are any misunderstandings, please let us know.\n\n**1) This method can be classified as \"Remap\" mentioned in our paper.**\nIn reality, this method can be classified as one of the alternative methods mentioned in our paper, *i.e.*, Remap (Please refer to the \"Comparison with some alternative methods\" part in **Section 4** and **Section D.3.3** in the appendix). The core idea behind Remap is to design a remap function $\\lambda=f(t)$ as the network's conditional input instead of directly utilizing $t$, *i.e.*, $\\epsilon_\\theta(x_t, \\lambda)$. In this particular case, we set $\\lambda = \\sigma_t$.\n\n**2) Analysis of Remap, where the remap function is $\\lambda = \\sigma_t$**\nThe performance of Remap relies heavily on whether sampling is performed uniformly based on $t$ or $\\lambda$ during both training and inference. **1) Uniformly sampling $t$:** As discussed in our paper, if the sampling strategy remains consistent (uniformly sampling $t$) during both training and inference, Remap has no impact on $\\frac{\\partial \\epsilon_\\theta(x_t, t)}{\\partial t}$. As a result, Remap may yield similar performance to the baseline. **2) Uniformly sampling $\\lambda:$** However, if there is a change in the sampling strategy (uniformly sampling $\\lambda=f(t)$, i.e. $\\sigma_t$ here) during training **or** inference, the inference results may deteriorate, similarly to the examples shown in our paper. This occurs because the equivalent schedule may compel the network to focus on specific stages of the entire process. For instance, taking $\\lambda=\\sigma_t$ as an example, where $\\sigma_{999} = 1.0000$, $\\sigma_{782}= 0.9990$, and $\\sigma_{0}= 0.01$. If we uniformly sample $\\sigma_t$ during training, timesteps falling within the range $t\\in(782, 999)$ will rarely undergo training. We provide additional examples of remap functions, with a quantitative evaluation provided in **Figure 3**, along with a detailed analysis in the \"Comparison with some alternative methods\" section in **Section 4** and **Section D.3.3** in the appendix.\n\nBesides, it is important to note that $\\lambda=\\sigma_t$ is a special remap function. The difference of $\\sigma_{t}$ and $\\sigma_{t-1}$ gets extremely small near $t=T$. This characteristic may hinder the network's training process, as it is hard for the network to distinguish adjacent timesteps with extremely similar conditional inputs. Consequently, learning $\\epsilon_\\theta(x, \\sigma_t)$ may yield poorer performance than the baseline, as confirmed by the experimental results presented in the following table.\n\n**Table3. Quantitative comparison between using $\\sigma_t$ as conditions and the baseline on FFHQ $256\\times 256$.**\n| Method | FID-10k |\n| --- | --- |\n| $\\epsilon_\\theta(x, \\sigma_t)$ | 16.41|\n| Baseline | 9.50 |\n\n\n**3. Reply for other questions**\n\n**Q4:Avoid saying t being small.**\n**A4:** Thank you, we appreciate your reminding. To prevent any potential misunderstandings, we replace \"small $t$\" with \"small $\\sigma_t$\".\n\n**Q5: Eq10, are $\\beta_t$ and $\\eta_t$ defined?**\n**A5:** Yes, thank you for pointing out this oversight. we have included their definition ($\\beta_t = 1 - \\frac{\\alpha_t}{\\alpha_{t-1}}$, and $\\eta_t^2 = \\beta_t$.) after Equation (10)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413198457,
                "cdate": 1700413198457,
                "tmdate": 1700413198457,
                "mdate": 1700413198457,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sbNDmkN0wO",
                "forum": "WNkW0cOwiz",
                "replyto": "syx7W5AtBo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1058/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, We would like to know if our response has dispelled all your concerns. If there are any of your concerns, please let us know. We are very willing to discuss them with you."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700457062593,
                "cdate": 1700457062593,
                "tmdate": 1700457790052,
                "mdate": 1700457790052,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vFOizWR3HT",
                "forum": "WNkW0cOwiz",
                "replyto": "sbNDmkN0wO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1058/Reviewer_gj5E"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1058/Reviewer_gj5E"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "I am grateful for the authors' comprehensive and insightful responses. They took great care in addressing my questions and the results highlighted their contribution again."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716379782,
                "cdate": 1700716379782,
                "tmdate": 1700716379782,
                "mdate": 1700716379782,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]