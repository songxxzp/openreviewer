[
    {
        "title": "RegionSpot: Unleashing the Power of Frozen Foundation Models for Open-World Region Understanding"
    },
    {
        "review": {
            "id": "HAFLpvjlqq",
            "forum": "9Wy6pLNQcG",
            "replyto": "9Wy6pLNQcG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2923/Reviewer_3vj6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2923/Reviewer_3vj6"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to combine SAM and CLIP to improve region-level visual understanding. Specifically, the proposed RegionSpot method freezes the whole SAM and CLIP models, and adds new layers to them to let the position-aware tokens from SAM interact with the image-level features from CLIP, thus leading to region-level semantic tokens. After training the new layers on Object365, OpenImages, and V3D, the method shows decent performance on LVIS open-world object recognition."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The motivation of combining the capabilities of SAM and CLIP is clear and makes sense.\n- The method is simple and easy to implement, as most of the parameters are frozen and only several new layers are trained."
                },
                "weaknesses": {
                    "value": "- The claim of 6.5% and 14.8% improvements over GLIP makes no sense. The comparison is not fair. The paper only compares with GLIP-Tiny, which is way smaller than the proposed RegionSpot. GLIP-Tiny uses Swin-Tiny which has 29M parameters, while RegionSpot-BB has at least 160M parameters. According to the GLIP paper, GLIP-L achieves 26.9 AP on LVIS, which is better than the best RegionSpot-BL's 23.7 AP.\n- Evaluation is limited. The paper only tests on open-world LVIS recognition. It would be more convincing to do a more comprehensive evaluation.\n- It is not appropriate to claim a zero-shot recognition on LVIS, as RegionSpot is trained on the combination of Object365, OpenImages, and V3D, which shares a lot of common categories, objects, and scenes with LVIS.\n- Please check the reference carefully. For example, \" However, the use of ROIAlign (Ren et al., 2015) for region feature extraction...\". The ROIAlign is not proposed in Ren et al., 2015."
                },
                "questions": {
                    "value": "- The paper only experiments with different CLIP models, i.e., CLIP-base and large. What about using larger SAM models, e.g., RegionSpot-LL? \n- More details can be reported, e.g., the number of new parameters."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2923/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634964990,
            "cdate": 1698634964990,
            "tmdate": 1699636236111,
            "mdate": 1699636236111,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7ZjVsA3ULd",
                "forum": "9Wy6pLNQcG",
                "replyto": "HAFLpvjlqq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2923/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2923/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer 3vj6"
                    },
                    "comment": {
                        "value": "Thank you for the helpful review. We carefully address your questions and comments below and have updated the submission pdf accordingly.\n\n> The claim of 6.5% and 14.8% improvements over GLIP makes no sense. The comparison is not fair. The paper only compares with GLIP-Tiny, which is way smaller than the proposed RegionSpot. GLIP-Tiny uses Swin-Tiny which has 29M parameters, while RegionSpot-BB has at least 160M parameters. According to the GLIP paper, GLIP-L achieves 26.9 AP on LVIS, which is better than the best RegionSpot-BL 23.7 AP.\n\nThanks for pointing out this. To address this question, we have now provided extra comparison under GLIP protocol. This table below shows that our method is superior over GLIP-L despite using less training data (3M vs 27M) and less learnable parameters (35M vs. 289M). Both conditions are related to the training cost directly, which means our training is significantly more efficient.\n\nTable 1: Comparison under GLIP protocol.\n| Method | Training Data | Data Size | Fully finetune | Training Time | MiniValAPr | MiniValAPall | Val APr | Val APall |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| GLIP-L | FourODs,GoldG,Cap24M | 27M | Yes | 120K | 28.2 | 37.3 | 17.1 | 26.9 |\n| RegionSpot-BL | O365,OI,V3DET | 3M | No | 0.2k | 33.2 | 36.9 | 30.2 | 29.8 |\n\n\n> Evaluation is limited. The paper only tests on open-world LVIS recognition. It would be more convincing to do a more comprehensive evaluation.\n\nThanks. For more comprehensive evaluation, we also perform experiments under the VILD protocol. The results, presented in Table 2 (Response to all reviewers), show that our method achieves a 19.7% APr, surpassing F-VLM 18.6% APr by 1.1% APr. Notably, F-VLM is also based on a Frozen Foundation model. This comparison demonstrates our model capability to effectively leverage pre-trained knowledge.\n\n> It is not appropriate to claim a zero-shot recognition on LVIS, as RegionSpot is trained on the combination of Object365, OpenImages, and V3D, which shares a lot of common categories, objects, and scenes with LVIS.\n\nGreat point. It's true that recent large models, such as CLIP, are typically trained on vast datasets that encompass a wide range of concepts. Consequently, when these models are utilized for a downstream task, they often encounter concepts that were already part of their training data, though it is unclear which specific concepts. While this may not strictly adhere to the definition of zero-shot learning, it is widely acknowledged and applied in practice. Our setup aligns with GLIP in the utilization of extensive training data, a practice that is not novel but well-established.\nAdditionally, we extend our evaluation using the protocol established by VILD, ensuring a robust and comprehensive assessment of our method. This further demonstrates our method can fully unleash pretrained knowledge.\n\n> Please check the reference carefully. For example, \" However, the use of ROIAlign (Ren et al., 2015) for region feature extraction...\". The ROIAlign is not proposed in Ren et al., 2015.\n\nThanks, we will fix all in the revision.\n\n> The paper only experiments with different CLIP models, i.e., CLIP-based and large. What about using larger SAM models, e.g., RegionSpot-LL ?\n\nThanks for this great suggestion. To address this, we have conducted additional experiments with varying SAM model backbones. Our findings are summarized as follows:\n1. Role of SAM in Position-Aware Knowledge and Mask Generation: Our results indicate that the use of larger SAM models (e.g., SAM-L) improves mask AP due to the higher quality of mask generation. However, for box AP, there is even some slight drop in the improvement. This is because the SAM mask token primarily contributes position-aware knowledge, which is already sufficiently captured by either ViT-B or ViT-L.\n2. Choice of SAM Model: Given our focus on region recognition, we opted for SAM-B, balancing performance and computational efficiency.\n\n| SAM-Model | Box AP_rare | Mask AP_rare |\n| --- | --- | --- |\n| ViT-B | 24.9 | 22.8 |\n| ViT-L | 24.7 | 23.6 |\n\n> More details can be reported, e.g., the number of new parameters.\n\nGreat point! As suggested, we provide the number of learnable parameters as following:\n|  | Leanble Parameter(M) |\n| --- | --- |\n| RegionSpot-BB | 22 |\n| RegionSpot-BL | 35 |\n\nWe hope the above information addresses all the concerns and provides further insight into our work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2923/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700521781299,
                "cdate": 1700521781299,
                "tmdate": 1700521781299,
                "mdate": 1700521781299,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ppfk3IeGbO",
                "forum": "9Wy6pLNQcG",
                "replyto": "HAFLpvjlqq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2923/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2923/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Last discussion request before the discussion period ends"
                    },
                    "comment": {
                        "value": "Dear Reviewer\uff0c\n\nThank you for dedicating your time and expertise to review our paper and for participating in the rebuttal process. Your feedback has been instrumental in enhancing the quality of our work.\n\nWe hope that our responses in the rebuttal have satisfactorily addressed your concerns. If there are any remaining issues or new points of discussion, we are fully prepared to engage in further dialogue. Given the constraints of the review timeline, we would greatly appreciate it if you could review our revised responses at your earliest convenience.\n\nShould you find that our revisions and clarifications have resolved the initial concerns, we would be grateful for your reconsideration of the initial rating. However, if there are still aspects that require clarification, we welcome the opportunity to discuss them in the remaining time.\n\nWe sincerely appreciate your time and thoughtful consideration."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2923/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668363730,
                "cdate": 1700668363730,
                "tmdate": 1700668363730,
                "mdate": 1700668363730,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AzarjLvsFa",
            "forum": "9Wy6pLNQcG",
            "replyto": "9Wy6pLNQcG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2923/Reviewer_Tjjg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2923/Reviewer_Tjjg"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a open-world region recognition architecture, named RegionSpot, designed to integrate position-aware localization knowledge from SAM and CLIP."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The method performs open world object detection through pre-trained vision foundation model SAM and CLIP. SAM is a good foundation model with excellent performance. This work sucessfully adopts SAM into object detection (requiring object recognition). This idea is novel and good. Introducing vision foundation models into new tasks is an important thing, I think.\n2. The experimental results are good."
                },
                "weaknesses": {
                    "value": "Most of the problems come from the experiment part.\n1. This method still requires existinig region proposal generation models, like ground-truth, RPN, GLIP, which makes this method extremely limited. From this view, this method is even not complete, since you cannot find such a region understanding task in reality. In addition, the pre-extracted regions are also meaningless, since SAM can also perform the region proposal generation task. Therefore, the main experimental setting is meaningless and unreasonable.\n2. There are also many alternatives to perform the so-called region understanding task with SAM and CLIP. For example, SAM can directly extract region proposals and CLIP (or RegionCLIP) can predict the category tags of them. We can also add some projection layers in this way, freeze most parameters and finetuning a small part of parameters for efficient training. The author should perform experiments to compare with baselines like this. Otherwise, the effect of the position-aware localization cannot be seen.\n3. The experiment in Table 1 is also unfair. RegionCLIP simply performs image-text pre-training, without object detection finetuning. However, RegionSpot performs something about detection training. Therefore, the author should finetuning RegionCLIP on the same detection datasets for a fair comparison.\n4. The author should also compare with some more recent methods, like GLIP v2, Grounding DINO and so on."
                },
                "questions": {
                    "value": "Most of the problems come from the experiment section. The author should provide more additional results to make the paper accepted,"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2923/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2923/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2923/Reviewer_Tjjg"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2923/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752028698,
            "cdate": 1698752028698,
            "tmdate": 1699636236004,
            "mdate": 1699636236004,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NhTjBWuEc5",
                "forum": "9Wy6pLNQcG",
                "replyto": "AzarjLvsFa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2923/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2923/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer Tjjg"
                    },
                    "comment": {
                        "value": "Thank you for the helpful review. We carefully address your questions and comments below and have updated the submission pdf accordingly.\n> This method still requires existinig region proposal generation models, like ground-truth, RPN, GLIP, which makes this method extremely limited. From this view, this method is even not complete, since you cannot find such a region understanding task in reality. In addition, the pre-extracted regions are also meaningless, since SAM can also perform the region proposal generation task. Therefore, the main experimental setting is meaningless and unreasonable. \n\nAppologies for the confusion. We highlight the following points:\n\n1. Same as our work, several recent works such as RegionCLIP and DetPro also use extrenel region proposal, as we all focus on recognizing the region of interest. As showin in ViLD and F-VLM, existing region proposals methods are already highly generic across different domains, whilst recognizing the detected regions is relatively outpaced. That's why we focus on the latter.\n2. Our method fully preserves SAM flexible prompting capability, which enables RegionSpot to perform interactive region recognition, extract region-specific semantic features, and object localization.\n3. Proposal generation from SAM: Whilst SAM can generate region proposals, its recall is still significantly inferior than other alternatives such as GLIP, as shown in the table below, leading to less competitive final results.\n\n| Proposal | AP_rare | Recall_200 |\n| --- | --- | --- |\n| SAM | 10.6 | 0.4 |\n| RPN | 10.9 | 0.53 |\n| GLIP | 20.0 | 0.75 |\n\n\n> There are also many alternatives to perform the so-called region understanding task with SAM and CLIP. For example, SAM can directly extract region proposals and CLIP (or RegionCLIP) can predict the category tags of them. We can also add some projection layers in this way, freeze most parameters and finetuning a small part of parameters for efficient training. The author should perform experiments to compare with baselines like this. Otherwise, the effect of the position-aware localization cannot be seen.\n\nThanks for such detailed suggestions. In our submission, we already provided two variants of this suggested baseline by feeding ground-truth proposals in the first two rows of Table 1. They can be considered as the upper bounds. Following the suggestion, we further tested with SAM region proposals along with CLIP. The table below shows simply streamlining the off-the-shelf foundation models is still largely inferior than our method. We have added this new baseline in the revised version.\n\n|  | Proposals | AP_rare |\n| --- | --- | --- |\n| SAM + CLIP | SAM | 8.6 |\n| SAM + CLIP w/ projector | SAM | 9.6 |\n| RegionSpot-BB | SAM | 10.6 |\n| RegionSpot-BB | GLIP | 20.0 |\n\n\n> The experiment in Table 1 is also unfair. RegionCLIP simply performs image-text pre-training, without object detection finetuning. However, RegionSpot performs something about detection training. Therefore, the author should finetuning RegionCLIP on the same detection datasets for a fair comparison.\n\nThank you for the excellent suggestion. Given that RegionCLIP requires extensive computational resources and time for re-training, we conducted experiments following the ViLD protocol for a fair comparison. The results, presented in Table 2 (Response to all reviewers), demonstrate that our method achieves a 19.7%, outperforming RegionCLIP 17.1% by 2.6% APr. This is noteworthy as we achieve these results without refining the backbone using large-scale image-text pairs.\n\n> The author should also compare with some more recent methods, like GLIP v2, Grounding DINO and so on.\n\nThank you for the valuable suggestion. As GLIP v2 does not operate in a zero-shot manner on LVIS, we have included the more robust Grounding DINO large model in Table 1 (Response to all reviewers) for a direct comparison. Please refer to this for detailed insights."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2923/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700521668757,
                "cdate": 1700521668757,
                "tmdate": 1700522739185,
                "mdate": 1700522739185,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bUi5Zshro6",
                "forum": "9Wy6pLNQcG",
                "replyto": "AzarjLvsFa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2923/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2923/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Last discussion request before the discussion period ends"
                    },
                    "comment": {
                        "value": "Dear Reviewer\uff0c\n\nThank you for dedicating your time and expertise to review our paper and for participating in the rebuttal process. Your feedback has been instrumental in enhancing the quality of our work.\n\nWe hope that our responses in the rebuttal have satisfactorily addressed your concerns. If there are any remaining issues or new points of discussion, we are fully prepared to engage in further dialogue. Given the constraints of the review timeline, we would greatly appreciate it if you could review our revised responses at your earliest convenience.\n\nShould you find that our revisions and clarifications have resolved the initial concerns, we would be grateful for your reconsideration of the initial rating. However, if there are still aspects that require clarification, we welcome the opportunity to discuss them in the remaining time.\n\nWe sincerely appreciate your time and thoughtful consideration."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2923/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668345531,
                "cdate": 1700668345531,
                "tmdate": 1700668345531,
                "mdate": 1700668345531,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Jxkw9EpiMp",
            "forum": "9Wy6pLNQcG",
            "replyto": "9Wy6pLNQcG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2923/Reviewer_UsjU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2923/Reviewer_UsjU"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose RegionSpot, an open-vocabulary detection and instance segmentation approach that leverages frozen foundation models. Specifically, given some set of candidate bounding box proposals, authors use SAM for class-agnostic localization and CLIP features for classification. Importantly, authors only train a small projection and attention module to combine the location queries from SAM with the semantic key/value pairs from CLIP. Authors evaluate their method on LVIS and find that their method beats prior work including RegionCLIP and GLIP."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Simple Approach. Authors propose a simple way of combining two popular off-the-shelf foundational models for vocabulary detection and segmentation that effectively leverages the foundational pre-training of each model.\n- Excellent Training and Data Efficiency. Due to the small size of the RegionSpot attention module, authors can train on 3M data points in 22 hours. (Table 2 is notable)\n- Clear Explanation. Authors present their work in a generally coherent manner."
                },
                "weaknesses": {
                    "value": "- Limited Baseline Comparisons. Despite significant prior work in OVOD [1,2,3], authors primarily only compare with RegionCLIP and GLIP. In reality, prior work [1] significantly outperforms RegionSpot.\n- Unfair Comparisons. Since authors show that pre-training data scale significantly contributes to model performance, comparing RegionSpot, a technically very similar method trained on much less data is unfair. Instead, it would make more sense to evaluate RegionSpot trained on only CC3M. \n- Limited by Quality of Boxes. RegionSpot is always limited by the bounding box proposals provided as input to the system. As authors show in Table 1, the type of proposals has a significant impact on model performance. It would be interesting to evaluate how the impact of using proposals from one of the more performant open-vocabulary models than GLIP (e.g. GroundingDINO).\n\nReferences \n\n[1] Scaling Open-Vocabulary Object Detection. Minderer et. al. ArXiv. \n\n[2] Multi-Modal Classifiers for Open-Vocabulary Object Detection. Kaul et. al. ICML 2023\n\n[3] https://github.com/witnessai/Awesome-Open-Vocabulary-Object-Detection"
                },
                "questions": {
                    "value": "- How to Deal with False Positive Boxes? Since all regions are classified into one of K categories, how are false positive proposals addressed?\n-  Can this method be most improved by better classification (e.g. CLIP) or localization (e.g. SAM)? What are the typical error modes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2923/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2923/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2923/Reviewer_UsjU"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2923/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840022164,
            "cdate": 1698840022164,
            "tmdate": 1699636235936,
            "mdate": 1699636235936,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g2hhs1eyrE",
                "forum": "9Wy6pLNQcG",
                "replyto": "Jxkw9EpiMp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2923/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2923/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer UsjU"
                    },
                    "comment": {
                        "value": "Thank you for the helpful review. We carefully address your questions and comments below and have updated the submission pdf accordingly.\n> Limited Baseline Comparisons. Despite significant prior work in OVOD,  authors primarily only compare with RegionCLIP and GLIP. In reality, OWL-V2 significantly outperforms RegionSpot.   \n\nThanks for pointing this out. We note that comparing OWL-V2 and RegionSpot directly is not fair in terms of the usage of data scale: WebBI dataset at a size of 2B images vs. 3M images. This big difference in data size makes us hard to draw meaningful insights.Following additional suggestions, we have included more extensive comparison. We add  GLIP-L and Grounding DINO-L in Table 1 ( Response to all reviewers) to verify the efficient training and good performance.\n\n> Unfair Comparisons. Since authors show that pre-training data scale significantly contributes to model performance, comparing RegionSpot, a technically very similar method trained on much less data is unfair. Instead, it would make more sense to evaluate RegionSpot trained on only CC3M. \n\nOur training objectives differ from those of RegionCLIP, which requires large-scale image-text pairs to retrain a region-level backbone. Our goal is to leverage publicly available object detection datasets to train new layers that enable existing foundational models to perform region-level understanding. For fair comparison, we have conducted experiments following the ViLD protocal Please refer to our comprehensive response to all reviewers for detailed results.\n> Limited by Quality of Boxes. RegionSpot is always limited by the bounding box proposals provided as input to the system. As authors show in Table 1, the type of proposals has a significant impact on model performance. It would be interesting to evaluate how the impact of using proposals from one of the more performant open-vocabulary models than GLIP (e.g. GroundingDINO).\n\nThanks for this insight. We have now evaluated the effect of input proposals on performance. As GroundingDINO comes with no open-source for LVIS evaluation, we instead utilize GLIP-L as the proposal generator. Our results underscore two key findings:\n1. There is a significant improvement when the quality of prompt boxes is improved.\n2. Despite GLIP-L generating high-quality boxes, it faces challenges in region recognition. Notably, RegionSpot achieves superior performance, exceeding GLIP-L by 13.1 in AP_r metric.\n|  | Proposal | AP_rare |\n| --- | --- | --- |\n| GLIP-L | GLIP-L | 17.1 |\n| RegionSpot-BL | GLIP-T | 24.9 |\n| RegionSpot-BL | GLIP-L | 30.2 |\n\n\n> How to Deal with False Positive Boxes? Since all regions are classified into one of K categories, how are false positive proposals addressed? \n\nWe treat the background as a special class, which aids in mitigating issues associated with false positives.\n\n> Can this method be most improved by better classification (e.g. CLIP) or localization (e.g. SAM)? What are the typical error modes?\n\nThanks for the great question. We would do more evaluation on their respective effects once more options become available. From our (not exhaustive) observation, it is hard to draw the typical error modes though."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2923/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700521542662,
                "cdate": 1700521542662,
                "tmdate": 1700521542662,
                "mdate": 1700521542662,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6EcSF9EumM",
            "forum": "9Wy6pLNQcG",
            "replyto": "9Wy6pLNQcG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2923/Reviewer_PEMU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2923/Reviewer_PEMU"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the challenging task of region recognition, particularly, open-world object detection. The authors present a new region recognition framework, combining a vision foundation model, i.e., SAM, and a vision-language foundation model, i.e., CLIP. In this framework, localization knowledge and semantic conception are integrated to promote each other. Experimental results and analyses conducted on the LVIS detection dataset demonstrate its effectiveness and generalization."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Originality**: The paper proposes a lightweight knowledge integration module to unleash the ability of both SAM and CLIP models for open-world object detection tasks.\n\n**Quality**: The paper provides a thorough experimental evaluation of **RegionSpot** on a challenging object detection dataset, i.e., LVIS. The authors also conduct various ablation studies to analyze the impact of different components of **RegionSpot**, such as the scale of training data, different representations from the CLIP model, selection of position-aware tokens, etc. \n\n**Clarity**: The paper also provides sufficient background information and related work to situate the contribution of **RegionSpot** in the context of existing literature on region understanding and zero-shot recognition."
                },
                "weaknesses": {
                    "value": "**Major Issues**:\n\n**Insufficient novelty and contribution**: The newly proposed **RegionSpot** framework lacks justification for its design. The pipeline of fine-tuning a lightweight module while frozen SAM and CLIP models seems natural and basic. Additionally, only conducting experiments on object detection tasks is not convening.\n\n**Insufficient results for experiments**: Although the authors claim that \"This implementation effectively facilitates the fusion of semantic and location information in a manner that is amenable to learning and yields substantive efficacy.\", they provide no experimental results. Also, the motivation is not clear. For example, why do the authors serve the localization feature as the role of `query`? what if the ViL feature assumes as the `query`? \n\n**Minor Issues**:\n\n**Excessive statement**: The authors claim that \"Our model's flexible architecture allowed us to seamlessly replace one-hot labels with class name strings.\". This may be overemphasizing their contribution.\n\n**Grammar and minor errors**:\n- In section **Position-aware tokens selection in SAM**, a grammatical error is present in the sentence: \"Surprisingly, although it can outperform GLIP (i.e., 17.2 vs. 18.6).\"\n- In section **Prompt enginerring**, there is a discrepancy in the reported increase: \"an increase of 1.4 AP\" \u2014 should this be 1.6 AP? Additionally, clarification is needed regarding whether the method in the last line of Table 5 indeed represents the baseline with both prompts."
                },
                "questions": {
                    "value": "1. My major concern is the contributions of combining SAM and CLIP for object detection tasks.\n\n2. The authors should discuss the limitations and potential negative societal impact in the Conclusion.\n\n3. Please also refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2923/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699601351964,
            "cdate": 1699601351964,
            "tmdate": 1699636235874,
            "mdate": 1699636235874,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NCyare2aZO",
                "forum": "9Wy6pLNQcG",
                "replyto": "6EcSF9EumM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2923/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2923/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the helpful review. We carefully address your questions and comments below and have updated the submission pdf accordingly.\n\n> The newly proposed RegionSpot framework lacks justification for its design.  Although the authors claim that \"This implementation effectively facilitates the fusion of semantic and location information in a manner that is amenable to learning and yields substantive efficacy.\", they provide no experimental results. Also, the motivation is not clear. For example, why do the authors serve the localization feature as the role of query? what if the ViL feature assumes as the query?\n\n1. Our motivation is to integrate position-aware localization knowledge from a localization foundation model (e.g., SAM) with semantic information extracted from a ViL model in a frozen to achieve the region understanding.  As Reviewer Tjjg  said, introducing vision foundation models into new tasks is an important thing.\n2. In our framework, the mask tokens from SAM, which act as queries, inherently localize objects but do not convey semantic information. Our method thus uses these tokens as query to find the corresponding semantic details from the ViL feature map, enhancing the semantic understanding at a regional level. Using the ViL feature as a query, on the other hand, would be illogical for our purpose, as our objective is to augment already localized specific regions with semantic information from the ViL features.\n\n> Only conducting experiments on object detection tasks is not convening. \n\nAs we focus on region recognition, object detection is the most important and most suitable problem for evaluation in computer vision, per our knowledge. Also, we have evaluated the Instance Segmentation task with positive comparisons. We are open to evaluate more suitable problems if suggested kindly.\n\n> Overemphasizing the contribution about \"Our model's flexible architecture allowed us to seamlessly replace one-hot labels with class name strings.\"\n\nWe would tune done and refine this description. To further clarify: Using text embedding of class name strings instead of one-hot labels is the enabler for good zero-shot performance as exemplified with recent foundation models like CLIP and ALIGN (both function at the image level). In the domain of open-world object recognition, this approach is similarly employed to attain zero-shot capabilities, such as ViLD and GLIP. We have simply adhered to this established method.\n> Ablation Study of Prompt engineering - \"clarification is needed regarding whether the method in the last line of Table 5 indeed represents the baseline with both prompts.\"\n\nYes, we will clarify in the revision.\n> The authors should discuss the limitations and potential negative societal impact in the Conclusion.\n\nThank you for emphasizing the need to address limitations and potential negative societal impacts in our Conclusion. We recognize that our method, while offering advancements in open world region understanding, has limitations, including its dependency on external region proposal mechanisms. This could potentially limit its versatility or introduce biases depending on the proposal generation source. To achieve more efficient AI, there is good amount of space to compress our whole architecutre by using smaller faster-to-run compoents whilst paying little cost in recognition performance. Moreover, we will discuss possible negative societal impacts, such as concerns over privacy, the ethical use of recognition technologies, and the imperative for responsible, transparent deployment of such tools. Your feedback is invaluable in ensuring our research comprehensively addresses these critical and broader implications.\n\n> Grammar mistakes\n\nThanks, we will fix all in the revision."
                    },
                    "title": {
                        "value": "Official Response to Reviewer PEMU"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2923/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700521308656,
                "cdate": 1700521308656,
                "tmdate": 1700521407049,
                "mdate": 1700521407049,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TqdHab4OUE",
                "forum": "9Wy6pLNQcG",
                "replyto": "6EcSF9EumM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2923/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2923/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Last discussion request before the discussion period ends"
                    },
                    "comment": {
                        "value": "Dear Reviewer\uff0c\n\nThank you for dedicating your time and expertise to review our paper and for participating in the rebuttal process. Your feedback has been instrumental in enhancing the quality of our work.\n\nWe hope that our responses in the rebuttal have satisfactorily addressed your concerns. If there are any remaining issues or new points of discussion, we are fully prepared to engage in further dialogue. Given the constraints of the review timeline, we would greatly appreciate it if you could review our revised responses at your earliest convenience.\n\nShould you find that our revisions and clarifications have resolved the initial concerns, we would be grateful for your reconsideration of the initial rating. However, if there are still aspects that require clarification, we welcome the opportunity to discuss them in the remaining time.\n\nWe sincerely appreciate your time and thoughtful consideration."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2923/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668241858,
                "cdate": 1700668241858,
                "tmdate": 1700668241858,
                "mdate": 1700668241858,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]