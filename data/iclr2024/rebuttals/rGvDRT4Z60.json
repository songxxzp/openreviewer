[
    {
        "title": "FairPATE: Exposing the Pareto Frontier of Fairness, Privacy, Accuracy, and Coverage"
    },
    {
        "review": {
            "id": "8IuiGuz9Hc",
            "forum": "rGvDRT4Z60",
            "replyto": "rGvDRT4Z60",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses the problem of interactions between fairness, privacy, and accuracy constraints for PATE (Private Aggregation of Teacher Ensembles) type algorithms for differentially private learning. PATE style of algorithms first uses the private data (partitioned into several small partitions) to train base classifiers (teachers). Then, the algorithm uses these teachers to label some public data in a privacy preserving way. In particular, it cleverly chooses which points it can label without sacrificing too much privacy. Then another classifier (student) is trained on this newly privately labelled dataset, which is then released to the user. In this paper, this labelling step is used to also incorporate privacy constraints. Finally, the paper uses empirical evidence to suggest that their algorithm achieves a better privacy fairness accuracy trade-off than Loewy et. al. 2023."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper is written quite clearly and is easily readable. The arguments of the authors come out clearly without ambiguity and the reader can easily follow the train-of-thought. I appreciated that very much.\n* I also found the main algorithmic idea of this paper quite nice. The idea of that the algorithm chooses which points to label not only on the bassi of the privacy constraint but also the fairness constraint is quite neat and could be useful in other contexts. I appreciated this."
                },
                "weaknesses": {
                    "value": "Despite the interesting idea of the paper, I am unable to support this paper for acceptance. The four main reasons are as follows (in decreasing order of severity).\n\n1. __W1__ Significance: The paper aims to convey the significance of its contribution through experimental results (and I don't think there is anything wrong with it and in general support extensive empirical results), but the experiments present a rather bleak picture of the advantages of FairPATE. \n    * **Minimal improvements** Most importantly, there are rarely any results where the improvements of PATE over baselines is larger than 1\\%. For example in 1. Credit card dataset and 2. parkinson's dataset, there results differ by less than $1\\%$.\n    * **Several examples of underperformance** Several examples also show that FairPATE performs significantly worse than competitor. Examples include Demographic Parity for Adult dataset and UTK Face ($\\epsilon=5$).\n    * **Misleading regarding diversity of experiments** In the introduction, the paper sells itself regarding performing a wide range of experiments but results on nearly half of these datasets are hidden away in the appendix without any mention in the main text and without comparisons. Experiments on **CheXpert, CelebA, FairFace, and Retired-Adults** are not available in the main text and it is very hard to interpret the results presented for this in the appendix.  In fact, there are **no results of FAIR-PATE on CheXpert** in the paper \n    * Please show comparison on these for demographic parity and equalized odds (similar to Figure 2,3 with comparison to Loewe et. al.'s algorithm) on these four datasets. In addition, there are results in the literature that run DP algorithms on CelebA and CheXpert. Comparisons should be made to them to show what is the sacrifice being made compared to state-of-the-art.\n\n2. __ W2__ **Wrong Conclusion from Theorem 2** I did not go through the detail of Theorem 1 but I assume it is correct and follows from Group privacy arguments. However, the sentence above Theorem 1 (Point C2) claims that the result proves that  \" pre-processing ... will necesarrily degrade the guarantee of the .... private learning mechanism\". How ever this is not what the Theorem shows. The theorem only proves a privacy guarantee of $M\\odot P_{\\text{pre}}$ not that this is the tightest privacy guarantee possible for the composed mechanism. Am I missing something ?\n\n3. __W3__ **Abstaining from prediction for fairness reasons** The introduction justifies this as _\"if a decision cannot be made without violating a pre-specified fairness metric, then the model can refuse to answer at which point the decision can be relegated to a human judge\"_. If I have understood this correctly, this is a flawed argument. For example, consider the situation where there are group of data points from the majority group, on which if the algorithm predicts correctly the fairness will be violated and hence the algorithm abstains. Nevertheless, the prediction is easy for this group and the judge nevertheless predicts on them correctly and the overall fairness is still violated. Isn't this pointless then to relegate this to the judge ? Intuitively, it appears that any problem can be made fair this way by simply refusing to predict on certain majority groups thereby artificially boosting the fairness of the algorithm. The correct fairness solution should aim to improve performance on the minority group instead.\n\n\n4. __W4__  **Unfair Comparisons** Fair-PATE  and Loewy et. al's algorithm do not work under the same restrictions of differential privacy. Specifically, Fair-PATE uses public unlabelled data and Loewy et. al doesn't and we know that (even a small amount of) public data can severely help in improving accuracy of DP models (perhaps fairness too). Hence, it appears to me that Loewy et. al. uses a significantly stricter notion of privacy and achieves nearly comparable result and in some cases even better (for tabular datasets, there are no comparison on vision datasets with Loewy et. al.)."
                },
                "questions": {
                    "value": "* **Motivation** I understand that there are several works showing that privacy incurs a sacrifice in fairness. However, one possible approach to this problem is simply improving the accuracy and perhaps the resultant accuracy also leads to high fairness. In fact Berrada et. al. (2023) makes this argument with some experiments in their paper. I did not see an argument in this paper why this approach is not expected to solve all problems. Why should there be a trade-off between privacy and fairness ? Is it known in the literature and are there theoretical studies arguing about the necessity of a trade-off ?\n\n* **Missing baselines** Why is there no comparison with the algorithm of Zhang et. al. (2021), Kulynuych et. al. (2022) and Berrada et. al. (2023) ? They are mentioned in one or two sentences at the very end of the paper but without any comprehensive empirical comparison.\n* **FairDP-SGD\" What is FairDP-SGD ? It doesn't seem to have been defined anywhere ? Is it an existing work ? Where is FAIR-PATE's result on CheXpert ?\n\n* In addition to this, please also also address __W1,W2,W3,W4__."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9455/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699190636228,
            "cdate": 1699190636228,
            "tmdate": 1699637190299,
            "mdate": 1699637190299,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sO0uHsJt8I",
                "forum": "rGvDRT4Z60",
                "replyto": "m7O0uYOVWk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThanks for the response. Just to clarify when you say\n\n'''Finally, all PATE-based frameworks assume access to an unlabeled public dataset on which privacy accounting does not take place. This is akin to pre-training DP-SGD models on public data before any privacy accounting occurs\u2014although this requires labels while PATE-based models do not.'''\n\n__Do you mean PATE uses the same kind of data, but without labels, as DP-SGD normally uses for pre-training ?__\n\nFrom my understanding, the most popular examples of DP-SGD using pre-training is ImageNet pre-trained for private fine-tuning on CIFAR or JFT pre-training for fine-tuning on ImageNet and similar for NLP tasks. \n\nIs your public data from a very different distribution compared to the test set ? That's not the impression I got from reading the draft."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700300816060,
                "cdate": 1700300816060,
                "tmdate": 1700300816060,
                "mdate": 1700300816060,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hodaXVcdji",
                "forum": "rGvDRT4Z60",
                "replyto": "8IuiGuz9Hc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. Below we address each point raised in \"Weaknesses\" section.\n\n- **Several examples of underperformance**\u00a0Several examples also show that FairPATE performs significantly worse than competitor. Examples include Demographic Parity for Adult dataset and UTK Face ().\n\n\t- For the concerns about significance, please see our General Response.\n\n\t- We have noted clearly in the main paper that for the Adult dataset, under Demographic Parity DP-Fermi performs better than FairPATE for smaller epsilon values (Figure 10). However that trend is not consistent over different datasets (on UTKFace, FairPATE does better for lower epsilons and can even go lower in terms of achievable classification error\u2014see Figure 6). We provide a discussion on the cause of these performance differentials in response to **Reviewer XkhF.**\n\n- **Misleading regarding diversity of experiments**\u00a0In the introduction, the paper sells itself regarding performing a wide range of experiments but results on nearly half of these datasets are hidden away in the appendix without any mention in the main text and without comparisons. Experiments on\u00a0CheXpert, CelebA, FairFace, and Retired-Adults\u00a0are not available in the main text and it is very hard to interpret the results presented for this in the appendix. In fact, there areno results of FAIR-PATE on CheXpert\u00a0in the paper\n\n\t- The CheXpert results were already present in the paper (in the last figure of the appendix) but was unfortunately mislabeled. We apologize for the error.  The figure's label has been corrected in the updated manuscript.\n\n\t- We currently have results of FairPATE on CheXpert, CelebA and FairFace in Section H in the appendix.  Unfortunately given the space constraints we cannot add all the aforementioned results in the main text.\n\n- Please show comparison on these for demographic parity and equalized odds (similar to Figure 2,3 with comparison to Loewe et. al.'s algorithm) on these four datasets. In addition, there are results in the literature that run DP algorithms on CelebA and CheXpert. Comparisons should be made to them to show what is the sacrifice being made compared to state-of-the-art.\n\n\t- DP-FERMI is an expensive algorithm to run. On CelebA, it requires 33 hours to train one model for 200 epochs as specified in the paper without parameter tuning on an NVIDIA A100; while the model has at least 4 hyper-parameters to tune. Nevertheless, we are currently in the process on running DP-FERMI on CelebA, and we will report the results as we acquire them in the coming days.\n\n- **W2 Wrong Conclusion from Theorem 2**\u00a0I did not go through the detail of Theorem 1 but I assume it is correct and follows from Group privacy arguments. However, the sentence above Theorem 1 (Point C2) claims that the result proves that \" pre-processing ... will necesarrily degrade the guarantee of the .... private learning mechanism\". How ever this is not what the Theorem shows. The theorem only proves a privacy guarantee of\u00a0\u00a0 $M \\odot P_{\\mathrm{pre}}$ not that this is the tightest privacy guarantee possible for the composed mechanism. Am I missing something ?\n\n\t- We are not sure we follow the reviewer's comments here. Does the reviewer take issue with the phrasing of the claim \" pre-processing ... will necesarrily degrade the guarantee of the .... private learning mechanism\"?\n\n\t- If yes, then we apologize for the confusion. We can adjust the claim as\n\t  > pre-processing ... will necessarily degrade the guarantee of the .... composed preprocessor and private learning mechanism  \n\t- Of course, the private learning algorithm does not degrade on its own; but via the inclusion of a fairness pre-processor, we have shown that the joint fair and private mechanism will have degraded privacy guarantee which is always an upper bound on the privacy leakage\n\n\t- We are also unsure of what the reviewer mean by the \"tightest privacy guarantee possible for the composed mechanism.\" Do they mean that another $\\mathcal{P}_\\text{pre}$ could possibly reduce the privacy cost? If yes, then given that the pre-processor is simply ensuring the fairness definition holds (via a filter that class balances the dataset conditioned on the sensitive attribute) can the reviewer recommend another pre-processor?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700304529756,
                "cdate": 1700304529756,
                "tmdate": 1700304529756,
                "mdate": 1700304529756,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4rH0hOwbwX",
                "forum": "rGvDRT4Z60",
                "replyto": "8IuiGuz9Hc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses (Part 2)"
                    },
                    "comment": {
                        "value": "- **W3**\u00a0**Abstaining from prediction for fairness reasons**\u00a0The introduction justifies this as\u00a0*\"if a decision cannot be made without violating a pre-specified fairness metric, then the model can refuse to answer at which point the decision can be relegated to a human judge\"*. If I have understood this correctly, this is a flawed argument...\n\n\t- We think there has been a misunderstanding regarding a) purpose of the reject option for fairness, and b) the inference-time nature of the IPP algorithm that implements it. What the reject-option enables is for a human (e.g. a trained judge) to take over the decision making in case of a fairness constraint violation. **The presumption here is that a trained judge is better placed to make impactful decisions than the algorithm.** Therefore, the human judge can choose to accept violations of the fairness constraint on a case-by-case basis. We do not believe the human judge should be placed under the same constraints as the algorithm given the vast difference in training, experience and accountability faced by the human judge versus the algorithm.\n\n-  Consider the situation where there are group of data points from the majority group, on which if the algorithm predicts correctly the fairness will be violated and hence the algorithm abstains\n\t- Please note that IPP is an **inference-time** algorithm. It is incorrect to say that the \"algorithm predicts correctly that the fairness will be violated\" because there is no prediction involved in this process. For instance, using Demographic Parity, the fairness metric requires that the rates of acceptance be similar for different subgroups. Since IPP is releasing decisions, it can keep an exact record of the decisions released (this is what $m(z,k)$ counter keeps in Algorithm 1). Therefore, the rates calculated based on this record is exact and so are the detected violations that would prompt the algorithm to reject a certain query.\n\n- Intuitively, it appears that any problem can be made fair this way by simply refusing to predict on certain majority groups thereby artificially boosting the fairness of the algorithm. The correct fairness solution should aim to improve performance on the minority group instead.\n\t- Our standard fairness metric does not require a notion of majority vs. minority. At any given decision point, our algorithm (IPP) bounds the worst-case fairness violations experienced by any subpopulation. Furthermore, the reject-option is not a silver bullet; as it comes with a trade-off with coverage (1-rejection rate). Therefore the more rejections translate more work for the human judge. It is, therefore, in the best interest of deployer of the upstream algorithm (FairPATE or DP-FERMI, or any other fairness-aware) algorithm to minimize fairness violations at training-time; because an unfair algorithm will increase the work that human judges would need to do\u2014 eliminating the need for the fair algorithm in the first place.\n\n- **W4**\u00a0 **Unfair Comparisons**\u00a0Fair-PATE and Loewy et. al's algorithm do not work under the same restrictions of differential privacy. Specifically, Fair-PATE uses public unlabelled data and Loewy et. al doesn't and we know that (even a small amount of) public data can severely help in improving accuracy of DP models (perhaps fairness too). Hence, it appears to me that Loewy et. al. uses a significantly stricter notion of privacy and achieves nearly comparable result and in some cases even better (for tabular datasets, there are no comparison on vision datasets with Loewy et. al.).\n\n\t- We wish to clarify and disentangle two issues that the reviewer points out: a) access to public data, b) access to additional data. All of our baselines experiments are conducted using the exact data splits of Lowy et al. 2023. This means that we do not use any additional training data compared to our baselines. Instead, we use the training data available to us and partition it into public and private. Consistent with the original PATE framework, we do not use the labels of the public dataset; instead we train teacher models on the private partition and label the public points under privacy and fairness constraints. In Figure 1.a, a DP-SGD method like Lowy et al. 2023 replaces the shaded Private Model. Therefore, we respectfully disagree with the reviewer's assessment that the comparisons are not fair data-wise. As a matter of fact, in our experiments, DP-Fermi receives additional information compared to FairPATE (namely, the labels of the public data partition)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700305047245,
                "cdate": 1700305047245,
                "tmdate": 1700305047245,
                "mdate": 1700305047245,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lm7TklCVUO",
                "forum": "rGvDRT4Z60",
                "replyto": "8IuiGuz9Hc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response To Questions"
                    },
                    "comment": {
                        "value": "- Motivation.\u00a0I understand that there are several works showing that privacy incurs a sacrifice in fairness. However, one possible approach to this problem is simply improving the accuracy and perhaps the resultant accuracy also leads to high fairness. In fact Berrada et. al. (2023) makes this argument with some experiments in their paper. I did not see an argument in this paper why this approach is not expected to solve all problems. Why should there be a trade-off between privacy and fairness ? Is it known in the literature and are there theoretical studies arguing about the necessity of a trade-off ?\n\n\t- Berrada et al. 2023 only consider unfairness with respect to loss disparity. This particular fairness metric is inherently a measure of the generalization of the model. That is, if a model is well generalized, then it is also well-generalized for different subpopulations of the data; therefore, it is only natural that it would exhibit lower loss disparity. As a result Berrada et al. 2023 do not consider (or rather, need to consider) an unfairness mitigation step. Our results (and that of Lowy et al. 2023 and others) show that for other group fairness metrics, this is not the case. That is, fairness and accuracy do not align so well all the time.\n\n\t- In terms of theoretical results Cummings et al. 2019 provide an the impossibility result for DP learning and exact fairness; while also demonstrating a positive result for the case of approximate fairness (our settings). More recently, Mangold et al. 2023 have showed that \"the fairness (and accuracy) costs induced by privacy in differentially private classification vanishes at a $O(\\sqrt{p}/n)$ rate, where n is the number of training records, and p the number of parameters.\" This is achieved through a proof of pointwise Lipschitz smoothness of group fairness metrics with respect to the model where the pointwise Lipschitz constant explicitly depends on the confidence margin of the model, and may be different for each sensitive group. Therefore, in the presence of minority groups in the data, we can expect a non-zero gap between private and non-private models.\n\n- Missing baselines.\u00a0Why is there no comparison with the algorithm of Zhang et. al. (2021), Kulynuych et. al. (2022) and Berrada et. al. (2023) ? They are mentioned in one or two sentences at the very end of the paper but without any comprehensive empirical comparison.\n\n\t- Kulynych et. al 2021 does not account for the privacy leakage of its importance sampling step therefore a fair comparison with methods such as ours or Lowy et al. 2023 is not possible\n\n\t- Berrada et al. 2023 only considers loss parity, and has no unfairness mitigation for two of our gorup fairness metrics (see our previous answer for more details)\n\n\t- Zhang et al. 2021 implement early stopping to save on privacy budget; however, they do not measure the privacy leakage of their early stopping criterion therefore the privacy loss reported is underestimated\n\n- What is FairDP-SGD ? It doesn't seem to have been defined anywhere ? Is it an existing work ? Where is FAIR-PATE's result on CheXpert ?\n\n\t- This is an error on our part. The figure in the appendix was mislabelled. We apologize for the confusion. The figure's label has been corrected in the updated manuscript.\n\nWe once again thank the reviewer for their detailed feedback and kindly ask them to consider raising their score if we have addressed their concerns successfully. We are happy to answer further questions."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700305377688,
                "cdate": 1700305377688,
                "tmdate": 1700305377688,
                "mdate": 1700305377688,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RsZ1LOVDol",
                "forum": "rGvDRT4Z60",
                "replyto": "YNWlv7w4rJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification. I think the way the above (original comment) statement is made is misleading and I would suggest that the authors update it to reflect what is mentioned in the clarification."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653700730,
                "cdate": 1700653700730,
                "tmdate": 1700653700730,
                "mdate": 1700653700730,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oo6ORCxeyS",
                "forum": "rGvDRT4Z60",
                "replyto": "hodaXVcdji",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response. I have read through the entire comment in response to my questions. I will try to keep my replies brief and provide suggestions on what I think can improve the manuscript in subsequent drafts.\n\n[W1]  Currently the paper presents a new algorithm for improving fairness-accuracy pareto frontier but the main weakness  here is that in a significant number of situations this improvement does not happen but rather a degradation happens. I would recommend that the authors rethink about what the main contribution is, perhaps identify more clearly in the main text where they expect an improvement and where they don't and then show that through experiments. Doing this discussion through rebuttal is not the best as this a major point of this paper. \n\nMoreover, i had noticed the plots in Appendix H, but the plots were not very informative regarding how important the improvement is due to the missing comparisons. Again I appreciate that the authors are running the experiments but this is an important contribution of the paper and hence the experiments for this should not really be done during the rebuttal and it should be possible to present the \"improvements of the algorithm on seven distinct datasets\" in the main paper. Perhaps, the paper needs a restructuring if all the exerimental improvements obtained by a novel algorithm cannot be presented in the main pages of the paper."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654626157,
                "cdate": 1700654626157,
                "tmdate": 1700654626157,
                "mdate": 1700654626157,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3e9BVbXn5D",
                "forum": "rGvDRT4Z60",
                "replyto": "oo6ORCxeyS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 2"
                    },
                    "comment": {
                        "value": "My point in weakness 2 is that the result shows that the composed mechanism is at least $(\\epsilon,\\delta)$ DP. \n\nHowever the argument in the text says that the \"will necessarily degrade the guarantee\". \n\nThe first sentence is an upper bound on the privacy guarantee of the mechanism whereas the second sentence is a sentence akin to a lower bound."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654800108,
                "cdate": 1700654800108,
                "tmdate": 1700654800108,
                "mdate": 1700654800108,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g0rXelwiLv",
                "forum": "rGvDRT4Z60",
                "replyto": "4rH0hOwbwX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
                ],
                "content": {
                    "comment": {
                        "value": "I am a bit confused with what the authors mean by \"Please note that IPP is an inference-time algorithm. It is incorrect to say that the _\"algorithm predicts correctly that the fairness will be violated because there is no prediction involved in this process\"_ and _\"Since IPP is releasing decisions, it can keep an exact record of the decisions released (this is what \n counter keeps in Algorithm 1). Therefore, the rates calculated based on this record is exact and so are the detected violations that would prompt the algorithm to reject a certain query.\"_\n\nIf the model is creating an output, which is then judged by IPP whether to release or not - isn't this an example of prediction ? Or pehaps I am misunderstanding something"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655117962,
                "cdate": 1700655117962,
                "tmdate": 1700655117962,
                "mdate": 1700655117962,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l9HYiaQ5qh",
                "forum": "rGvDRT4Z60",
                "replyto": "lm7TklCVUO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
                ],
                "content": {
                    "comment": {
                        "value": "Regarding W4, can the authors clarify whether the number of __private__ data (excluding public unlabelled data) points seen by FairPATE and Loewy et. al. are exactly same ?"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655229661,
                "cdate": 1700655229661,
                "tmdate": 1700655229661,
                "mdate": 1700655229661,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mxU8CH1YDD",
                "forum": "rGvDRT4Z60",
                "replyto": "lm7TklCVUO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
                ],
                "content": {
                    "title": {
                        "value": "Regarading existing literature"
                    },
                    "comment": {
                        "value": "* Kulynych et. al. - Do the authors mean that Algorithm in Kulynych et. al. (which does DP importance sampling) does not respect the $(\\epsilon,\\delta)$-DP ?\n* Berrada et. al. - Perhaps the authors can then compare their method with the algorithm in berrada et. al. and show that Berrada et. al.'s method indeed suffers in DP and EO. This would be a strong and important point to make showing the necessity of this algorithm.\n* I am a bit confused what the authors mean about Cummings et. al's result. If their paper shows a negative result about pure DP but positive result about Approximate DP, then this is arguing against the existence of unfairness problem in approx DP ?"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655855160,
                "cdate": 1700655855160,
                "tmdate": 1700655855160,
                "mdate": 1700655855160,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0lpY90OOio",
                "forum": "rGvDRT4Z60",
                "replyto": "Za13H9epWI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for engaging.\n\nRegarding W1 and W2. I maintain my concern about W1 that this is not something that can solved with minor edits and clarification but needs restructuring. The comments here in the rebuttal is a good place to start in my opinion.\n\nRegarding W2, if the authors agree with the difference between what the result conveys and what a lower bound would convey, perhaps the author can include a lower bound result in the work to make their original point, which is I think important to the work that pre-processing indeed necessarily costs privacy.\n\nRegarding IPP, I do not agree with the characterisation. This is prediction with abstention or selective classification, as the authors describe. And it is not clear to me from the text, how IPP guarantees that it will not simply relegate examples that are 1) easy to classify 2) and classification will lead to unfairness to the human classifier even though there are other examples in the test set which are more nuanced and should have been relegated to the human classifier. An easy-to-see instance is that first the model can do usual classification without abstention, then check its fairness, then figure out which examples it can refuse to classify in order to stay within this budget (note that there can be many such sets; the algorithm can choose a set randomly), and then abstain from predicting for these.\n\nFinally, regarding W4, I see the stress the authors are putting on the absence of labels and using it to say that PATE sees stricly less information than Loewy et. al. I also reject this characterisation by arguing that the PATE algorithm seems (unlabelled) data points whose privacy it does not need to preserve whereas Loewy et. al. needs to preserve privacy of every data point it sees."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740703616,
                "cdate": 1700740703616,
                "tmdate": 1700740703616,
                "mdate": 1700740703616,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qqFIZSiB3N",
                "forum": "rGvDRT4Z60",
                "replyto": "XPtuqeCcWi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_p5Rv"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the correction. I would recommend adding an \"addendum\" to the previous comment so that people reading this conversation do not get the wrong information.\n\nI would also recommend adding your own comparisons and analysis in the paper about Kulynych et. al. as opposed to quoting the authors, in particular empirical comparisons.\n\nThank you for adding E.3, I beleive this should go in the paper and you can compare their \"extended DP-SGD\" with your algorithm on all the vision datasets you use for all three metrics.\n\nFinally, Regarding Cummings et. al. result, I agree their negative result is more important which is why i was surprised to see your mention their positive result in the previous message. However, their negative result only deals with pure DP and not with approximate DP and I would recommend searching for a negative result from literature that also shows a trade-off with privacy and fairness."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740948251,
                "cdate": 1700740948251,
                "tmdate": 1700740948251,
                "mdate": 1700740948251,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nPlMdbTUzW",
            "forum": "rGvDRT4Z60",
            "replyto": "rGvDRT4Z60",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9455/Reviewer_Mex5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9455/Reviewer_Mex5"
            ],
            "content": {
                "summary": {
                    "value": "The work considers the inclusion of fairness constraints into a method for differentially private\n(DP) training (or data generation from private data) based on transfer learning. The paper argues\nthat in this \"PATE\" approach which accounts of privacy concerns using DP, there is only one\nsensible place to incorporate fairness using an intervention (i.e. adjusting what/whether) data\nproceeds to subsequent PATE steps. This step is the point after the transfer from an ensemble of\nteachers is made. The paper puts a mechanism there that will reject some queries/instances if they\nresult in violations of fairness which is a function of all of the prior decisions of the\nmechanism. The work evaluates this approach relative to 2 other DP-based systems that incorporate\nfairness showing mostly preferable trade-offs between fairness, accuracy, and privacy; though this\nbenefit is small."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "+ Fairly well written and easy to follow.\n\n+ The points of intervention discussions give a nice overview of the PATE approach and ways in\n  which additional mechanisms can be independently injected. Note, however, the independent\n  intervention assumption is a weakness below.\n\n+ Rejection for fairness does give additional options for achieving fairness though this too comes\n  with a weakness below."
                },
                "weaknesses": {
                    "value": "- The implications of rejecting for fairness are not considered. Rejection for privacy has\n  implications in terms of privacy budget and likewise rejections for fairness come with\n  implications and ignoring them might be responsible for the observed gains on the Pareto\n  frontier. Consider the noted rejection example:\n\n    \"If at inference-time a decision cannot be made without violating a pre-specified fairness\n     metric, then the model can refuse to answer, at which point that decision could be relegated\n     to a human judge\"\n\n  The important implication here is that there will still be a judgement; it is just that the model\n  will not be making it. Regardless of whether the result of the human judgement will produce fair\n  or unfair overall statistics (that consider ultimate judgement whether by model or human), those\n  decisions need to be incorporated into subsequent fairness calculus. Even if a query is rejected\n  due to privacy, and if a decision is made for it subsequently, it would need to be accounted for\n  in subsequent fairness decisions.\n\n  Suggestion: incorporate ultimate decisions, whether by model or human, into the rejection\n  mechanism; i.e. update counts m(z, k) based on human decisions. Given that humans might put the\n  group counts into already violating territory, it may be necessary to rewrite Line 7 of Algorithm\n  1 to check whether the fairness criterion is improving or not due to the decision and allow\n  queries that improve statistics even though those statistics already violate \u03b3 threshold.\n  Handling rejection in experiments will also need to be done but unsure what the best approach\n  there would be. Perhaps a random human decision maker?\n\n- In arguments for intervention points, assumptions are made which preclude solutions. They assume\n  the intervention need to be made independent of other mechanisms in PATE. That is, they cannot\n  consider information internal to decision making that is not described by Figure 1 like\n  individual teacher outputs. This leaves the possibility that some fairness methods might be able\n  to integrated with PATE in a closer manner than the options described. One example is that they\n  might include the teacher outputs instead of operating on the overall predicted class like\n  Algorithm 1 assumes presently. C3 in particular suggests that some interventions will not account\n  for privacy budget correctly due to special circumstances and suggests at Point 4, they can be\n  budgeting can be handled correctly. Nothing is stopping a design from refunding privacy budget if\n  a query is rejected subsequently to an intervention point.\n\n  Suggestion: rephrase arguments for why some intervention points are bad to make sure they don't\n  also make assumptions about how the interventions are made and whether they can interact with\n  privacy budget.\n\n- Results in the Pareto frontier show small improvements, no improvements, and in some cases worse\n  results than prior baselines.\n\n  Suggestion: Include more experimental samples in the results to make sure the statistical\n  validity of any improvement claims is good. This may require larger datasets. Related, the\n  experiments show error bars but how they are derived is not explained.\n\n- Comparisons against methods in which rejection due to fairness is not an option may not be fair.\n\n  Suggestion: either integrate suggestion regarding accounting for rejection above, or incorporate\n  some form of rejection (or simulate it) in the existing methods being compared to. It may be that\n  the best methodology is not FairPATE but some existing baselines if adjusted to include fairness\n  rejection option.\n\nSmaller things:\n\n- Rejection rate is not shown in any experiments. One could view a misclassification as a\n  rejection, however. Please include rejection rates or view them as misclassifications in the\n  results.\n\n- The distribution whose fairness need to be protected is left to be guessed by the reader. For\n  privacy, it is more clear that it is the private data that is sensitive and thus privacy\n  budgeting is done when accessing that private data as opposed to the public data. For fairness,\n  the impact on individuals in the private dataset seems to be non-existent as the decisions for\n  them are never made, released, or implemented in some downstream outcome. I presume, then, it is\n  the fairness needs to be respected on the public data.\n\n  Algorithm 1 and several points throughout the work hint at this. However, there is also the\n  consideration of intervention points 1,2,3 which seem odd as they points seen before any\n  individual for whom fairness is considered is seen. That is, fairness about public individuals\n  cannot be made there, independent of any other issues such as privacy budgeting. Further, Theorem\n  1 discusses a demographic parity pre-processor which achieves demographic parity on private data\n  which I presume is irrelevant.\n\n- The statement\n\n    \"PATE relies on unlabeled public data, which lacks the ground truth labels Y\"\n\n  is a bit confusing unless one has already understood that fairness is with respect to public\n  data. PATE also relies on private labeled data to create the teachers.\n\n- The Privacy Analysis paragraph could be greatly simplified to just the last sentence regarding\n  post-processing.\n\nSmallest things:\n\n- Double \"violations\" near \"violations of demographic disparity violations\".\n\n- The statement \"DP that only protects privacy of a given sensitive feature\" might be\n  mischaracterizing DP. It is not focused on features or even data but rather the impact of\n  *individuals* on visible results."
                },
                "questions": {
                    "value": "Question A: Is reasonable to ignore downstream decisions from queries rejected due to fairness\n  (i.e. contrary to my suggestion in the weaknesses above)?\n\nQuestion B: C1 makes a point that adding privacy after fairness may break fairness. What about in\n  expectation? Were one to view the demographic statistics defining fairness measures in\n  expectations, wouldn't they remain fair?\n\nQuestion C: Theorem 1 makes a statement about a pre-processor inducing privacy parameter\n  degradation but FairPATE (or PATE) appears to fit the definition of a pre-processor. If the point\n  of the Theorem is to argue against pre-processors, isn't it also arguing against PATE/FairPATE?\n  Unrelated, what is \"ordering defined over the input space X\" and why is it necessary?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9455/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9455/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9455/Reviewer_Mex5"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9455/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699237849013,
            "cdate": 1699237849013,
            "tmdate": 1699637190200,
            "mdate": 1699637190200,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jyrmWCHu28",
                "forum": "rGvDRT4Z60",
                "replyto": "nPlMdbTUzW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. Below we address each point raised in \"Weaknesses\" section. We kindly note that in the following, we have grouped the reviewer's comments that addressed a common concern:\n\n- The implications of rejecting for fairness are not considered. Rejection for privacy has implications in terms of privacy budget and likewise rejections for fairness come with implications and ignoring them might be responsible for the observed gains on the Pareto frontier. Consider the noted rejection example: ...\n\n\t- Our understanding of the reviewer's remark here is that in a human-in-the-loop setting, the mistakes of the human would ultimately be mistakes of the end-to-end system; and therefore, measuring the efficacy of the system should consider the human error as well. While we agree with the reviewer, we believe that measuring the end-to-end error of the human-in-the-loop system should be the topic of a future study.\n\n\t- We have added the following to our discussion in Section 7:\n\t  > We note that in a human-in-the-loop system, the mistakes of the human would ultimately be mistakes of the end-to-end system; and therefore, measuring the efficacy of the system should consider the human error as well. We have shown that FairPATE and IPP enable such applications but whether such a system is the appropriate choice for a given application is out of the scope of the current study.  \n\n- In arguments for intervention points, assumptions are made which preclude solutions. They assume the intervention need to be made independent of other mechanisms in PATE. That is, they cannot consider information internal to decision making that is not described by Figure 1 like individual teacher outputs. This leaves the possibility that some fairness methods might be able to integrated with PATE in a closer manner than the options described. One example is that they might include the teacher outputs instead of operating on the overall predicted class like Algorithm 1 assumes presently. C3 in particular suggests that some interventions will not account for privacy budget correctly due to special circumstances and suggests at Point 4, they can be budgeting can be handled correctly. Nothing is stopping a design from refunding privacy budget if a query is rejected subsequently to an intervention point.\n\n\t- Regarding the suggested alternative scheme, we would like to to ensure that we understand the premise correctly: is the reviewer suggesting that we run PATE, accept and reject queries according to standard PATE analysis, and then apply the fairness intervention and reject possibly some more queries; and come back to give back some of the privacy budget?\n\n\t- If the answer is yes, then we note that standard PATE privacy analysis in Papernot et al. 2016 (summarized  in Section B in the appendix) is based on the probability of answering a certain query. To recoup the \"unused\" privacy budget one needs to find the difference between the privacy budget if the sample would have been answered and the privacy budget if the sample wouldn't have been answered. Note that the this is exactly what FairPATE already performs. Therefore, the suggested scheme is functionally equivalent to FairPATE but presents additional complexity.\n\n- Results in the Pareto frontier show small improvements, no improvements, and in some cases worse results than prior baselines.\n- Suggestion: Include more experimental samples in the results to make sure the statistical validity of any improvement claims is good. This may require larger datasets. Related, the experiments show error bars but how they are derived is not explained.\n\n\t- We have included more results in the revised manuscript. Please see our responses to **Reviewers XkhF and p5Rv.**\n\n- Comparisons against methods in which rejection due to fairness is not an option may not be fair.\n- Suggestion: either integrate suggestion regarding accounting for rejection above, or incorporate some form of rejection (or simulate it) in the existing methods being compared to. It may be that the best methodology is not FairPATE but some existing baselines if adjusted to include fairness rejection option.\n\n\t- The reviewer is correct that the IPP algorithm is an inference-time algorithm, therefore, its use is not limited to a FairPATE model.\n\t- We have run additional experiments on other models with the IPP;  and included the results in Section E.1 in the appendix. We find that FairPATE with IPP outperforms baselines (with IPP) in most regions fairness-privacy settings in both accuracy and coverage. See Figure 11 for a 2d Pareto surface, or Figure 12 for a corresponding 3d plot which showcases the accuracy performance improvements better."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700303333776,
                "cdate": 1700303333776,
                "tmdate": 1700303333776,
                "mdate": 1700303333776,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5siQtBHolj",
                "forum": "rGvDRT4Z60",
                "replyto": "nPlMdbTUzW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses (Part 2)"
                    },
                    "comment": {
                        "value": "Smaller things:\n- Rejection rate is not shown in any experiments. One could view a misclassification as a rejection, however. Please include rejection rates or view them as misclassifications in the results.\n\n\t- We do report **coverage** which is 1-rejection rate for the results that use the IPP. See Figure 5 where coverage is marked by the **color**; similarly the Pareto frontiers in the appendix all include coverage encoded in color.\n\n- The distribution whose fairness need to be protected is left to be guessed by the reader. For privacy, it is more clear that it is the private data that is sensitive and thus privacy budgeting is done when accessing that private data as opposed to the public data. For fairness, the impact on individuals in the private dataset seems to be non-existent as the decisions for them are never made, released, or implemented in some downstream outcome. I presume, then, it is the fairness needs to be respected on the public data.\n- Algorithm 1 and several points throughout the work hint at this. However, there is also the consideration of intervention points 1,2,3 which seem odd as they points seen before any individual for whom fairness is considered is seen. That is, fairness about public individuals cannot be made there, independent of any other issues such as privacy budgeting. Further, Theorem 1 discusses a demographic parity pre-processor which achieves demographic parity on private data which I presume is irrelevant.\n- The statement \"PATE relies on unlabeled public data, which lacks the ground truth labels Y\" is a bit confusing unless one has already understood that fairness is with respect to public data. PATE also relies on private labeled data to create the teachers.\n\n\t- A fairness intervention and a privacy intervention have different goals w.r.t. the data distribution they target. In private learning, the goal is to protect the privacy of the sensitive training data only. In fair learning, the end goal is to ensure the algorithmic decisions (at inference-time) are fair. Within the context of FairPATE; from a private learning perspective, PATE (and FairPATE by extension) protect private teacher data (see Figure 1.a) and not the public unlabeld data. From a fair learning perspective, a FairPATE model is just like any other fairness-aware model is trained such that it achieves fairness on training data, **with the expectation that that behavior generalizes to inference-time.** These are standard assumptions which is why we have refrained from repeating them.\n\nSmallest things:\n\n- Double \"violations\" near \"violations of demographic disparity violations\".\n- The statement \"DP that only protects privacy of a given sensitive feature\" might be mischaracterizing DP. It is not focused on features or even data but rather the impact of\u00a0*individuals*\u00a0on visible results.\n\t- We thank the reviewer for their keen eye. We have fixed the typo and revised the sentence to\n\t  > ..DP that only protects privacy of individuals with respect to particular sensitive features..."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700303442157,
                "cdate": 1700303442157,
                "tmdate": 1700303442157,
                "mdate": 1700303442157,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "68xsL4U5am",
                "forum": "rGvDRT4Z60",
                "replyto": "nPlMdbTUzW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Questions"
                    },
                    "comment": {
                        "value": "- Question A: Is reasonable to ignore downstream decisions from queries rejected due to fairness (i.e. contrary to my suggestion in the weaknesses above)?\n\n\t- We believe the alternative algorithm suggested is functionally similar to FairPATE. Please see our answer above.\n\n- Question B: C1 makes a point that adding privacy after fairness may break fairness. What about in expectation? Were one to view the demographic statistics defining fairness measures in expectations, wouldn't they remain fair?\n\n\t- The aforementioned fairness constraints are rate constraints which are in and of themselves expected values. If the constraints remain effective post DP noising; it means that we have retained at least group membership information. This is inconsistent with the standard neighboring relationship that our standard privacy definition adopts (i.e., one-sample difference between two datasets) therefore it is a weaker privacy notion. We conjecture that this weaker privacy notion would be closer to the DP w.r.t. sensitive attributes (which defines the group membership), but that it is possibly even weaker because prior theoretical work with DP w.r.t. sensitive attributes (namely, Mozannar et al. 2020) have already shown that a second post-processing step is necessary to ensure the fairness constraint is satisfied post DP-noising.\n\n- Question C: Theorem 1 makes a statement about a pre-processor inducing privacy parameter degradation but FairPATE (or PATE) appears to fit the definition of a pre-processor. If the point of the Theorem is to argue against pre-processors, isn't it also arguing against PATE/FairPATE? Unrelated, what is \"ordering defined over the input space X\" and why is it necessary?\n\n\t- Theorem 1 does not apply to FairPATE because it does not pre-process the training data as described in theorem 1. FairPATE is a pre-processor from the point of view of the student which only sees public queries. An algorithm that pre-processes the teacher data would indeed fit Theorem 1 because it sees private sensitive data (this is intervention point 1 in Figure 1.a).\n\n\t- The assumption on the ordering is a technicality that simplifies the the proof.  In practice, one can almost always assume such an ordering exists. An example of such ordering would be to order images based on their pixel values in some specified order of height, width and channel starting by checking the first pixel, then the second pixel, and so on.\n\n\nWe once again thank the reviewer for their detailed feedback and kindly ask them to consider raising their score if we have addressed their concerns successfully. We are happy to answer further questions."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700303671055,
                "cdate": 1700303671055,
                "tmdate": 1700303671055,
                "mdate": 1700303671055,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yub0t46EhK",
            "forum": "rGvDRT4Z60",
            "replyto": "rGvDRT4Z60",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9455/Reviewer_XkhF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9455/Reviewer_XkhF"
            ],
            "content": {
                "summary": {
                    "value": "The proposes a framework to integrate fairness into PATE. The proposed method is a simple adaptation of PATE which incorporates fairness constraints into the model's query rejection mechanism."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper tackles a highly relevant issue in ML, addressing both theoretical and practical implications of fairness and privacy.\n- The proposed framework is a simple adaptation of the existing PATE. Simplicity is a plus in my book."
                },
                "weaknesses": {
                    "value": "- Fairness is \"enforced\" by adding parity constraint within the aggregator which acts as a rejection sampling step on the basis of fairness. The general idea was proposed in several other works in the past, including the ones cited by the authors and against which the authors compare. It is thus difficult to understand what is new in the proposed framework. An explicit mention would help.\n- A discussion on when to use the proposed framework in contrast to other frameworks is absent (see also my questions below).\n- The experimental analysis should be improved. Some figures are misleading, e.g., same colors used for different algorithms (see questions below)."
                },
                "questions": {
                    "value": "- I can't judge what is the impact of IPP (the rejection step added at inference time) on the overall results. Can you provide some ablation study showing how the framework performs on various datasets with different majority/minority distributions with and without IPP? \n- How does the framework work in case of some distribution shift? This is especially important in the context of my question above. \n- For the other datasets reported in the appendix the trends shown reverts, e.g., DP-Fermi produces better tradeoffs than FairPATE. Can you discuss why? What feature of the dataset makes this possible?\n- Fig. 2 and 3 use orange colors for two different algorithms (Tran et al (Fig 2) and Jagielski et al. (Fig 3)). The authors should report all algorithms in all figures or justify their absence.\n- Why Tran et and Jagielski et al. are not reported for the UTK-dataset experiment? \n- Paper [Learning with Impartiality to Walk on the Pareto Frontier of Fairness, Privacy, and Utility](https://arxiv.org/pdf/2302.09183.pdf) discusses a similar topic (although the contributions from this work are different) and it could be added to your Related work section.\n\nMinor comments:\n\nA lot of the cited papers have appeared in conferences. But the authors cite their arxiv version. I suggest to update the references accordingly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9455/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699269096649,
            "cdate": 1699269096649,
            "tmdate": 1699637189990,
            "mdate": 1699637189990,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Jdw2fAW5kA",
                "forum": "rGvDRT4Z60",
                "replyto": "yub0t46EhK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. Below we address each point raised in \"Weaknesses\" section.\n\n- Fairness is \"enforced\" by adding parity constraint within the aggregator which acts as a rejection sampling step on the basis of fairness. The general idea was proposed in several other works in the past, including the ones cited by the authors and against which the authors compare. It is thus difficult to understand what is new in the proposed framework. An explicit mention would help.\n\t- The reviewer is correct that our work enforces fairness on the level of sample selection. However, this is done within a differentially private learning setup. The only prior work that we are aware of that has a sampling step for fairness within a private-learning setup is Kulynych et. al 2021 but importantly that work does not consider the privacy cost of the fairness importance sampling it performs while our work does.\n\t- We have added an extended related works section in Section I in the appendix with a more detailed comparison to Kulynych et. al 2021.\n- A discussion on when to use the proposed framework in contrast to other frameworks is absent (see also my questions below).\n\t- We have addressed the reviewer's concern. Please see our General Response.\n- The experimental analysis should be improved. Some figures are misleading, e.g., same colors used for different algorithms (see questions below).\n\t- We have re-generated the figures in question (3 and 4) and updated the manuscript accordingly."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301630594,
                "cdate": 1700301630594,
                "tmdate": 1700301704768,
                "mdate": 1700301704768,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XqxJwfBiW3",
                "forum": "rGvDRT4Z60",
                "replyto": "yub0t46EhK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Questions"
                    },
                    "comment": {
                        "value": "We kindly note that in the following, where appropriate, we have grouped the reviewer's questions together:\n\n- I can't judge what is the impact of IPP (the rejection step added at inference time) on the overall results. Can you provide some ablation study showing how the framework performs on various datasets with different majority/minority distributions with and without IPP?\n- How does the framework work in case of some distribution shift? This is especially important in the context of my question above.\n\t- We have run additional experiments on FairPATE+IPP and have added the results Section E.2 in the appendix of the updated manuscript. We observe that under class-imbalance for every percentage of coverage degradation, the fairness-privacy curve improves consistently. For the balanced case, we observe that the disparity levels are much higher. This is because random (non-stratified) sampling is not conducive to demographic parity.  IPP's overall behavior is similar as before but the curves are much closer to each other and the rejection rates are higher due to high disparity levels of the initial models caused by random sampling.\n\n- For the other datasets reported in the appendix the trends shown reverts, e.g., DP-Fermi produces better tradeoffs than FairPATE. Can you discuss why? What feature of the dataset makes this possible?\n\t- We thank the reviewer for the insightful question. Adult is a label-imbalanced dataset and we believe this is the reason its results stand out. Let us elaborate on this point: FairPATE achieves better fairness by pre-processing (a sample-level mitigation). As a result, if the dataset is imbalanced; especially if it is conditionally imbalanced, that is  $\\mathbb{P}[Y = 1 \\mid Z = 0] - \\mathbb{P}[Y = 1 \\mid Z = 1] > \\gamma,$ then fairness of a model trained on such data would also be bounded from below: $\\mathbb{P}[\\hat{Y} = 1 \\mid Z = 0] - \\mathbb{P}[\\hat{Y} = 1 \\mid Z = 1] > \\alpha > 0.$  This follows from Shamsabadi et al. 2023 Theorem 1, if we assume that the ground truth labels $Y$ are coming from a data oracle and the predicted labels $\\hat{Y}$ are from the \"surrogate model\". Since the oracle model is not fair; then the actual model trained on those will similarly not be fair (in the demographic parity sense).\n\n\t- DP-FERMI on the other hand has a model-level mitigation with a fairness regularization term scaled with $\\lambda \\in \\mathbb{R}^+$ . For a large enough $\\lambda$ , DP-FERMI can close the fairness gap on a particular dataset. However, this comes at a cost to generalization. Prior works in algorithmic fairness have noted this trade-off under the notion of \"stability\" and \"generalization of fairness\" (see for instance, Huang and Vishnoi 2020). We note that in Section D in the appendix, we also present a weight-space mechanism  (namely, model fair-tuning) and show it to be similarly effective in reducing the residual fairness gap of a FairPATE student model.\n\n\n- Fig. 2 and 3 use orange colors for two different algorithms (Tran et al (Fig 2) and Jagielski et al. (Fig 3)). The authors should report all algorithms in all figures or justify their absence.\n\t- We have regenerated and replaced Figure 3 to ensure distinct colors. We apologize for the confusion.\n\t- As to the reason why Figure 3 does not include Tran 21 et al, we have already made an explicit note in the empirical section that we report the same  baseline results as reported by Lowy et al. 2023 (in 2nd line of SOTA Baseline Comparisons) including that of Tran et al 2021.\n\n- Why Tran et and Jagielski et al. are not reported for the UTK-dataset experiment?\n\n\t- Both methods are evaluated on tabular data and are not suitable for deep learning models. In particular, Lowy et al. 2023 report that they have attempted this to no avail:\n\t  > We observed that the baselines were very unstable while training and mostly gave degenerate results(predicting a single output irrespective of the input). By contrast, our method was able to obtain stable and meaningful tradeoff curves.  \n\t    \n\t  Given that DP-FERMI reports better fairness-accuracy trade-offs at every privacy level compared to these baselines; we found it sufficient to test FairPATE against the new state-of-the-art (DP-FERMI); and report the other baselines for which we had reported data.\n\nMinor comments:\n\n- A lot of the cited papers have appeared in conferences. But the authors cite their arxiv version. I suggest to update the references accordingly.\n\t - We thank the reviewer for their careful reading of the paper. We have replaced the arxiv papers with conference counterparts.\n\nWe once again thank the reviewer for their detailed feedback and kindly ask them to consider raising their score if we have addressed their concerns successfully. We are happy to answer further questions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302523436,
                "cdate": 1700302523436,
                "tmdate": 1700302523436,
                "mdate": 1700302523436,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7SRmvUI4Bd",
                "forum": "rGvDRT4Z60",
                "replyto": "XqxJwfBiW3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_XkhF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9455/Reviewer_XkhF"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your replies. They are appreciated. \nI suggest to make it much more explicit the point discussed here regarding distribution shift and trends regarding different datasets, as the current results showcased (if one would read the main section only) may be misleading or at least report only a portion of the story.\n\nI also strongly suggest the authors to include the evaluation of Mozannar et al. al and Tran et al. in all of the experiments (including those based on CNNs)! \nIn particular, notice that the claim by Lowey et al. is erroneous and both algorithms are reported on the UTK datasets in their original papers: \n- [Tran and Fioretto, On the Fairness Impacts of Private Ensembles Models](https://www.ijcai.org/proceedings/2023/0057.pdf) See figure 7 (right)\n- [Tran et al, SF-PATE: Scalable, Fair, and Private Aggregation of Teacher Ensembles](https://www.ijcai.org/proceedings/2023/0056.pdf) See figure 2(c)"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657034176,
                "cdate": 1700657034176,
                "tmdate": 1700657034176,
                "mdate": 1700657034176,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]