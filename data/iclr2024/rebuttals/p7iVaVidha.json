[
    {
        "title": "OfflineLight: An Offline Reinforcement Learning Model for Traffic Signal Control"
    },
    {
        "review": {
            "id": "OGyqBBbKp9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6980/Reviewer_aRRe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6980/Reviewer_aRRe"
            ],
            "forum": "p7iVaVidha",
            "replyto": "p7iVaVidha",
            "content": {
                "summary": {
                    "value": "RL methods have gained popularity in the traffic signal control (TSC) problem, since they can learn from the traffic environment\u2019s feedback and adjust the traffic signal policies accordingly.\nTo avoid trial and error training, this paper considers offline RL for adaptive and efficient TSC.\nTo this end, this paper introduces a general offline actor-critic framework  (Offline-AC)  and develop an adaptive decision-making model, namely OfflineLight, based on the Offline-AC for TSC.\nThe proposed method improves the generalization ability under a variety of traffic conditions.\nThe authors also release the first offline dataset for TSC."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper collect and release the first offline dataset for TSC problem, which should benefit the community.\n2.  The proposed method shows reasonable performance in the experiments over several datasets."
                },
                "weaknesses": {
                    "value": "1. Several math formulas are incorrect. For example, Eq. 1 should be something like $G = E_{\\rho_{0}} E_{a_0, a_1,\\ldots, \\sim \\pi}[\\sum_{t=0}^\\infty \\gamma^t r_t]$, where $\\rho_0$ is the initial state distribution. Also problematic is the definition of $\\pi^*$ two lines after Eq. 2 and the $Q(s,a)$ in Section 4.2.2 (missing the expectation and what is $r^n$?).\n2. The proposed \"offline Actor-Critic framework\" may not be considered as the contribution of this paper. For example, Eq. 3 in this paper is the same as Eq. 7 in BRAC [1]. Eq. 5 is the same as Eq. 2 in CQL [2].\n3. There are many typos and inaccurate words/punctuation marks/notations in the paper, making the paper hard to follow. \n4. In Table 1, CQL has similar overall performance but shorter error bars than the proposed method OfflineLight, so it may be overclaim to say that the proposed method \"shows more robust performance among offline-RL methods.\"\n5. The main paper is not self-contained, the authors may remove/reduce Section 5 and move the results of Section 6.6 & 6.7 onto the main paper.\n\n\n[1] Wu, Yifan, George Tucker, and Ofir Nachum. \"Behavior regularized offline reinforcement learning.\" arXiv preprint arXiv:1911.11361 (2019).\n\n[2] Kumar, Aviral, et al. \"Conservative q-learning for offline reinforcement learning.\" Advances in Neural Information Processing Systems 33 (2020): 1179-1191."
                },
                "questions": {
                    "value": "1. What is the reward function you use for the RL formulation of the TSC problem?\n2. What this the meaning of this sentence: \"the Critic is applied to address overestimated values of the Critic\"?\n3. I don't understand this sentence in Section 4.3.1: \"where $\\pi_{\\theta'}$ is the target policy network by constraining the policy.\" Wouldn't the \"target policy network\" being an exponential moving average of the learning policy as in BEAR [3]?\n4. For the sentence \"In general, the Actor implementation applies PSR to overfitting to narrow peaks in the value estimate...\" Why applying PSR to **overfitting** to narrow peaks?\n5. \"Our method provides new insights into the OOD problem of offline RL\" --- Could you be more specific about what are the new insights?\n6. How many seeds do you use in composing Table 1?\n\n[3] Kumar, Aviral, et al. \"Stabilizing off-policy q-learning via bootstrapping error reduction.\" Advances in Neural Information Processing Systems 32 (2019)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6980/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6980/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6980/Reviewer_aRRe"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6980/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697260996075,
            "cdate": 1697260996075,
            "tmdate": 1700723043546,
            "mdate": 1700723043546,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mKXxzxmKhU",
                "forum": "p7iVaVidha",
                "replyto": "OGyqBBbKp9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6980/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6980/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to viewer aRRe"
                    },
                    "comment": {
                        "value": "The authors express gratitude for the valuable comments and will address them individually.\n\n- Q1\uff1aWhat is the reward function?\n\n  **Response:** The reward function in our decentralized MARL approach focuses on maximizing individual rewards for each traffic signal controller (TSC), as detailed in Equation 2. The metric used is the vehicle queue length (QL), with shorter queues indicating higher efficiency and rewards.\n\n- Q2: What is the meaning of \"the Critic is applied to address overestimated values of the Critic\"?\n\n  **Response:** Apologies for the misunderstanding due to the inappropriate terminology used. The correct term should be 'overestimated values of the Q value,' not 'overestimated values of the Critic.' We have corrected for this in the revision.\n\n- Q3: In 4.3.1: \" where  $\\pi_{\\theta'}$  is the target policy network by constraining the policy.\"\n\n  **Response:** Apologies for the lack of clarity in our description within Section 4.3.1 of the 'BEAR' paper, particularly regarding the target policy network. To clarify, the implementation of the target policy network in our paper is indeed similar to that in BEAR. We integrate noise into the learning policy to expand the range of action selection.  Specifically, we implement this by calculating the KL divergence. We will clarify in the revision when the paper is accepted. \n\n- Q4: Why applying PSR to overfitting to narrow peaks?\n\n  **Response:** Firstly, the primary function of PSR in our Actor implementation is not to directly address overfitting to narrow peaks in the value estimate, but rather to enhance the policy's robustness and generalization. The reference to \u2018narrow peaks\u2019 is in the context of the value function landscape. In some scenarios, the value function might exhibit sharp peaks, indicating a high estimated reward for very specific actions or action sequences. Without appropriate regularization, the Actor might overfit to these narrow peaks, leading to a brittle policy that performs well in the training environment but fails to generalize to slightly different or unseen situations.\n\n- Q5:  What are the new insights?\n\n  **Response:** Our method introduces a unique computational paradigm that simultaneously considers constraints on policy (action) and value, focusing on the trade-off between these two aspects. This approach is particularly insightful for addressing the OOD challenges in offline RL. By balancing the constraints on policy and value, our method effectively mitigates the common pitfalls associated with distributional shifts, which are prevalent in offline RL scenarios. OfflineLight, under this paradigm, has set a new benchmark in traffic signal control (TSC) within offline RL.\n\n- Q6:  How many seeds do you use in composing Table 1?\n\n  **Response:** To ensure a fair and consistent comparison across all experimental runs, we employed the same seed for each iteration of our experiments. Specifically, we conducted a total of 80 rounds of training, with each round followed by a testing phase. Additionally, our code has been made publicly available to facilitate transparency and reproducibility. \n\n- W1: Several math formulas are incorrect. \n\n  **Response:** We apologize for our carelessness and thank you for your attention. There are several typos in the formula. In Eq.1, $\\gamma^{n}$ should be $\\gamma^{t}$,\n  and $r^{n}$ of $Q(s,a)$ in Section 4.2.2 should be $r^{t^{'}}$.\n\n- W2: What is the  \"offline Actor-Critic framework\"'s contribution? \n\n  **Response:** We believe that the main contribution of this paper is the innovation of proposing a new framework. It actually encourages novel approaches and breakthroughs in optimizing and balancing these constraints, which is a significant aspect of our work. Our OfflineLight is an example algorithm for offline Actor-Critic framework, which  lies in integrating and leveraging these known techniques to push the boundaries of performance in TSC. \n\n- W3: The paper is hard to follow.\n\n  **Response**:  We have carefully reviewed and polished our manuscript, making structural adjustments as per your suggestions. \n\n- W4: CQL has similar overall performance. \n\n  **Response:** Our analysis across seven datasets demonstrates that OfflineLight outperforms CQL in five out of these seven datasets. In some cases, the improvements brought by OfflineLight involve significant reductions in Average Travel Time (ATT) by several tens of seconds. While in other datasets, the reduction in ATT may appear marginal (a few seconds), hence it is important to consider the broader impact in urban traffic systems. When translated into real-world implications, these time savings can result in considerable reductions in fuel consumption and subsequent decreases in air pollution. \n\n- W5: The authors may remove/reduce Section 5 and Section 6.\n\n  **Response:** Based on feedback, Sections 5 and 6 will be reduced, with essential results from Sections 6.6 & 6.7 integrated into the main paper for coherence."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6980/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389901939,
                "cdate": 1700389901939,
                "tmdate": 1700389901939,
                "mdate": 1700389901939,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zDGl9xOrUv",
                "forum": "p7iVaVidha",
                "replyto": "mKXxzxmKhU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6980/Reviewer_aRRe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6980/Reviewer_aRRe"
                ],
                "content": {
                    "title": {
                        "value": "Responses to the authors"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you so much for your reply. \n\nAfter reading other reviewers' reviews and your responses. I do have some follow-up questions.\n\n1. Could you elaborate more on \"*the Critic is applied to address overestimated values of the Q value*\"? I still cannot understand this statement even after your clarification in the rebuttal. What is your definition of Critic? How is it different from a (conservative/penalized) Q-function?\n\n2. For your statement that \"*... leading to a brittle policy that performs well in the training environment but fails to generalize to slightly different or unseen situations,*\" this is not the goal of standard RL, which is indeed to maximize performance in the training environment. Could you be more specific on the goal of your RL method? As an aside, there is a huge pool of papers on RL-agent generation, multi-task learning, fast adaptation in the multi-environment setting, and so on, which are not related to this paper and should not be confused with the proposed method.\n\n3. Why do you want to simultaneously considers constraints on policy and value? Why isn't constraining one of them sufficient for offline-RL training as demonstrated in CQL, BRAC, BEAR and so on.\n\n4. About novelty: \nCould you elaborate more on how is the proposed method different from the papers I listed in the **Weaknesses** section of my previous review?"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6980/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530981152,
                "cdate": 1700530981152,
                "tmdate": 1700530981152,
                "mdate": 1700530981152,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7WjZUyO9tG",
                "forum": "p7iVaVidha",
                "replyto": "odM1qUBenZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6980/Reviewer_aRRe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6980/Reviewer_aRRe"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you so much for the reply. Below are my comments to some of your misunderstandings.\n\n> simultaneously constraining policy and value...\n\nAs far as I know, it can be redundant to simultaneously constraint both policy and value function(s). For example, as suggested in the CQL paper, if we constraint the Q-function, then the policy learning can be left unconstrained and thereby optimized towards a better optimal (guided by the conservative Q-function).\n\n>  CQL ... does not integrate policy constraints. OfflineLight transcends this limitation...\n\nAs far as I know, for policy optimization, no constraint should be a advantage rather than a disadvantage.\n\n> In parallel, methods such as TD3+BC [1], which combines the existing TD3 and BC approaches, have led to surprising and noteworthy results.\n\nI agree. But the title of the TD3+BC paper is \"*A **Minimalist** Approach to Offline Reinforcement Learning*\", which is different from your proposal and contribution. Therefore, I don't think your paper is in parallel to TD3+BC, and the comparison with it may not be fair.\n\n***\nNevertheless, I agree that the empirical result of this paper has its own merit. I have increased my rating to 5."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6980/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723005530,
                "cdate": 1700723005530,
                "tmdate": 1700723005530,
                "mdate": 1700723005530,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QNJJ1fvgla",
            "forum": "p7iVaVidha",
            "replyto": "p7iVaVidha",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6980/Reviewer_46VN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6980/Reviewer_46VN"
            ],
            "content": {
                "summary": {
                    "value": "This paper creates an offline dataset (TSC-OID) collected with trained reinforcement learning policies on real-life datasets for traffic signal control task (TSC) to stress the problem that the trial and error training procedure is unsuitable for the traffic signal control problem. Based on the collected dataset, the author also proposed a novel offline-AC algorithm to train offline agents, taking a conservative exploration strategy. The result shows that the offline trained agents have performance close to SOTA online training agents and, at the same time, have the ability to transfer to unseen datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation for this work is good. TSC is not an environment like a based environment; the traditional trial and error exploration strategy is unsuitable for this high-stack training strategy. The idea of taking online training of TSC to offline training is necessary and important\n2. The paper is mostly well-written, with a clear description of the motivation and algorithm. Most of the motivation for designing the offline-AC is clearly explained.\n3. The experiment result is consistent with the claims made in the paper, and the transferability discussion is essential to justify the motivation, which is why we need to train an agent offline with a dataset collected from trained RL agents."
                },
                "weaknesses": {
                    "value": "1. The novelty is not clear for this work. The offline-AC brings in ideas from policy smooth regularization (PSR), and conservative Q-learning combines these ideas and transfers them into the application of traffic signal control. Though the attempts are reasonable, the novelty is not recognized as a novelty closely related to representation learning.\n2. Though the algorithm is well explained, the most crucial part is that the dataset is not well explained. Some details are very critical to judge the quality of this dataset. For example, how many policies did the author use to collect the data? Does this policy train on all datasets? These are the major components of this paper but need to be well discussed."
                },
                "questions": {
                    "value": "1. For the dataset, how did the author collect data? Is the policy trained on some other dataset or all the datasets used later? Is the RL agent already well-trained before interacting with the environment? If it is trained on all datasets, then the transferability evaluation is not feasible to evaluate the generalization of this offline-AC performance. Could the author give a very detailed description of this part?\n2. In the appendix Figure 4. The comparison seems not convincing. If the advanced MPLight performs very well on the transferred dataset, then the transfer ratio is also low. Could the author provide another metric to evaluate the performance?\n3. In section 4.2.1, the J(\\theta) is calculated over n and t. But in the formula, Q is independent of n and t. Should the action and state subscribe with n and t?\n4. How could the author conclude the Discussion \u201cWhy is the Offline-AC the abstract method?\u201d. To justify this idea, more experiment results should be conducted."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6980/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6980/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6980/Reviewer_46VN"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6980/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698554454320,
            "cdate": 1698554454320,
            "tmdate": 1700542305625,
            "mdate": 1700542305625,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QT4BTrFoiq",
                "forum": "p7iVaVidha",
                "replyto": "QNJJ1fvgla",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6980/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6980/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to viewer 46VN"
                    },
                    "comment": {
                        "value": "We thank you for your insightful reviews and address your concerns as follows.\n\n- Q1&W2\uff1aDescribe Offline Dataset Collection and Training Procedure? \n\n  **Response:** \n\n  1.Data Collection Method: Our approach to data collection prioritizes diversity to enhance the model's generalization capabilities. We used a variety of states and rewards, acknowledging that the optimal choices in existing works are still ambiguous. Our offline dataset comprises interactions generated by different policies, akin to the data buffers in off-policy reinforcement learning methods like Q-learning or DDPG. We engage various RL methods to interact with the CityFlow traffic simulator, recording diverse interaction data. This process encompasses different frameworks, traffic states, and rewards, detailed in Table 3 in Appendix A.2.\n\n  2.States of RL Agents: The RL agents employed in our study were trained from scratch. This approach ensures that the distribution of rewards does not converge prematurely, allowing us to discern the impacts of different actions more effectively. If agents had been well trained beforehand, the reward distribution would have stabilized, making it difficult to assess the relative merits of their actions.\n\n  3.Training on Datasets: Our training was conducted on a limited portion of the available datasets. For example, OfflineLight-JN1 was trained on only 20\\% of all data from TSC-OID. Despite this limited training, our model demonstrates robustness, as it can be effectively deployed on various other traffic topologies and flows without extensive retraining. \n\n- Q2:In the appendix Figure 4. The comparison seems not convincing. \n\n  **Response:** It is essential to clarify the interpretation of the transfer ratio used in our study. The transfer ratio, defined as $t_{transfer}/t_{train}$ , inversely correlates with performance: The higher the ratio, the lower the performance.\n  The MPLight  has shown the best transferability performance. But the comparison demonstrates that advanced MPLight does not outperform OfflineLight in Figure 4. The seemingly low transfer ratio for MPLight does not indicate superior performance but rather the opposite. This is a crucial aspect of our analysis, highlighting the effectiveness of OfflineLight in comparison to MPLight under the given metric. \n\n  To further solidify our argument about the generalization capabilities of OfflineLight, we plan to include several classic RL methods in our analysis. \n\n- Q3:In section 4.2.1, should the action and state subscribe with n and t?\n\n  **Response:** It is crucial to note that the Q value in our formulation indeed corresponds to each time step t in the $n^{\\text{th}}$ batch. While in Equation (3), for the sake of simplicity, the Q value is not explicitly subscripted with n and t , the calculations preceding this formulation have taken these variables into account. The decision to present the Q value without direct reference to n and t in the equation was a deliberate choice aimed at simplifying the representation and making the equation more readable. However, this should not be misconstrued as an oversight or a simplification in the actual computational process. We will certainly clarify this in the revision.  \n\n- Q4:\u201cWhy is the Offline-AC the abstract method?\u201d. \n\n  **Response:**\n\n  1.Offline-AC (Actor-Critic) should be recognized as a computational paradigm within offline RL, uniquely integrating constraints on actions and values. This paradigm is not merely a specific algorithm or technique but represents a broader conceptual approach.\n\n  2.Methods like CQL and BEAR can be viewed as specific instances of the Offline-AC framework. OfflineLight is also implemented under this paradigm. These examples demonstrate the versatility and applicability of the Offline-AC approach in various contexts.\n\n- W1:The novelty is not clear for this work.\n\n  **Response:**\n\n  1. Innovation of Offline-AC: Offline-AC is a computational paradigm within offline RL that emphasizes innovation in the constraints on actions and values. This framework is not only applying existing methods; instead, it actually encourages novel approaches and breakthroughs in optimizing and balancing these constraints, which is a significant aspect of our work. \n\n  2. Innovation in OfflineLight:Our model, OfflineLight, utilizing established concepts such as PSR and Conservative Q values, achieves SOTA results in offline RL, yielding impressive outcomes. This achievement itself signifies novelty. In parallel, methods such as TD3+BC [1], which combines the existing TD3 and BC approaches, have led to surprising and noteworthy results. Similarly, OfflineLight's innovation lies in integrating and leveraging these known techniques to push the boundaries of performance in TSC.  Moreover, our method can be evidenced in our real implementation at some cities. \n\n  [1] A Minimalist Approach to Offline Reinforcement Learning."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6980/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389282850,
                "cdate": 1700389282850,
                "tmdate": 1700389282850,
                "mdate": 1700389282850,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lgfnodJ3bx",
                "forum": "p7iVaVidha",
                "replyto": "QT4BTrFoiq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6980/Reviewer_46VN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6980/Reviewer_46VN"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. My concerns are addressed by the rebuttal and I now raise my score. My remaining suggestion is to differentiate the novelty of this work in the paper by including your response to my W1 in the paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6980/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542269680,
                "cdate": 1700542269680,
                "tmdate": 1700542269680,
                "mdate": 1700542269680,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XzTjfrjqU6",
            "forum": "p7iVaVidha",
            "replyto": "p7iVaVidha",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6980/Reviewer_Twe3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6980/Reviewer_Twe3"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Offline-AC, a general offline actor-critic framework for traffic signal control, addressing the limitations of traditional RL methods that rely on costly trial and error training. They also propose OfflineLight, an adaptive decision-making model based on Offline-AC. Additionally, the paper presents TSC-OID, the first offline dataset for traffic signal control, generated from state-of-the-art RL models, and demonstrates through real-world experiments that Offline RL, especially OfflineLight, can achieve high performance without online interactions with the traffic environment and offers impressive generalization after training on only 20% of the TSC-OID dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It's evident that this paper addresses a significant challenge in the field of RL-based traffic signal control by focusing on training policies using offline datasets. This approach is highly practical, as acquiring online samples from high-fidelity traffic simulators like CityFlow and SUMO can be challenging, particularly in scenarios involving large and dense traffic networks. The fact that the proposed offline dataset is publicly accessible is a commendable aspect, as it not only supports the research presented in the paper but also encourages and facilitates further studies and advancements in this area."
                },
                "weaknesses": {
                    "value": "To the best of my understanding, OfflineLight treats each traffic signal as an independent RL agent. Nevertheless, since there are multiple agents, it is crucial to clarify the definitions of state, action, and reward in order to comprehend the problem thoroughly. It would be highly beneficial if there were a well-defined problem formulation, possibly following a POMDP framework. Additionally, I'm interested in gaining a clearer understanding of the objective function. It seems unusual to aim for maximizing the expected reward over historical trajectories when the rewards for these trajectories are already given. Both Offline RL and Online RL generally share a common objective, which is to find an optimal policy that maximizes the expected return within the true MDP. I would appreciate a more detailed elaboration on your objective.\n\nIt's worth noting that OfflineLight does not take into account the interactions between multiple traffic lights. In contrast, many studies in Traffic Signal Control (TSC) have explored such interactions using various techniques like the CTDE framework and graph neural networks. I would like to suggest that integrating these approaches could substantially enhance the performance of the proposed model.\n\nRegarding offline RL, it's important to emphasize the significance of the size and quality of the offline dataset for the algorithm's performance. Unfortunately, there is a lack of analysis regarding the dataset's quality. Providing statistics on the offline dataset, such as the maximum reward contained within it, would greatly enhance the understanding of its characteristics."
                },
                "questions": {
                    "value": "I have some detailed questions regarding your work:\n\n1. State and Reward Definitions: Could you please provide more information about the definition of the state? Is the state considered a global state or local information for each agent? Additionally, is the reward shared among all agents or individually assigned? This is a critical matter to address, as most RL-based Traffic Signal Control methods operate in a decentralized Multi-Agent Reinforcement Learning (MARL) framework with a Partially Observable Markov Decision Process (POMDP) setting.\n\n2. Offline Dataset Collection Procedure: I'm interested in understanding the specifics of the offline dataset collection procedure. According to appendix B.2, the offline dataset is collected through three epochs of training. However, this may seem insufficient to attain high-reward solutions. Furthermore, I couldn't find information about which RL method is used to generate the dataset for each scenario. Lastly, could you provide some statistics regarding the reward distribution in the dataset, as the quality of the dataset is crucial for Offline RL performance.\n\n3. Experiment Section: Can you provide more details about the training and evaluation procedures in your experiments? I'm particularly curious about how the offline RL models are trained and evaluated in the New York scenario, given that there is no available offline dataset. Please elaborate on this aspect.\n\nI hope you can provide more insight into these questions to better understand your work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6980/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6980/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6980/Reviewer_Twe3"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6980/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698742003040,
            "cdate": 1698742003040,
            "tmdate": 1700718935912,
            "mdate": 1700718935912,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1SPw334vrX",
                "forum": "p7iVaVidha",
                "replyto": "XzTjfrjqU6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6980/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6980/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to viewer Twe3"
                    },
                    "comment": {
                        "value": "Thank you for the insightful comments. We herein address them one by one.\n\n- Q1&W1\uff1aDefinitions of State and Reward.\n\n  **Response:**\n\n  We appreciate your suggestion regarding the need of more detailed information about the state definition in our methodology. Our approach is grounded in a decentralized MARL framework, operating within the confines of a POMDP. This is an integral aspect of our research design. In Section 3.3 of our paper, we provided a comprehensive formulation that delineates the essential elements of our approach, particularly as they pertain to the POMDP context. This section is crafted to offer clarity and depth to our methodological framework.\n\n  Each agent in our model concentrates on local traffic information regarding state representation. This includes NV,  NV-segments, QL, EP,  ERV, and TMP for training process.\n\n  All these state variables are integrated into our OfflineLight model. However, in practical applications, we primarily utilize the queue length (QL) as the state input. This decision is based on our assessment of QL's relevance and effectiveness in capturing the traffic state's critical aspects for our model's objectives. This can be evidenced in our real implementation at some cities. \n\n  \n\n- Q2&W3:Describe Offline Dataset Collection Procedure.\n\n  **Response:**  We appreciate your comment and would like to provide clarifications as follows:\n\n  1. Only Three Epochs of Training:\n     (1) Our dataset is unique as it incorporates real-world traffic topology and flow data. This approach ensures that each epoch encompasses a significantly large volume of data. The substantial data volume per epoch compensates for what may initially appear as a limited number of epochs.\n     (2) The initial training epochs are particularly crucial for our analysis. In these stages, the reward distribution exhibits a wide range of variability, which is fundamental for algorithmic guidance. This concept is somewhat similar to the Advantage mechanism in Actor-Critic methods like A2C, where the difference between Q-values and value functions is most pronounced in the early stages of learning.\n\n  2. RL Methods Used: To address your query regarding the specific RL methods used for dataset generation in each scenario, we would direct your attention to Appendix Table 5 of our documentation. \n\n  3. Reward Distribution: We show the distribution of reward of our offline dataset in an anonymous URL : https://anonymous.4open.science/r/OfflineLight_rebuttal-3BA9/README.md.\n\n   \n\n- Q3:Provide more details about the training and evaluation procedures in your experiments?  \n\n  **Response:** We actually detailed the OfflineLight training process in Appendix A. It can be found that OfflineLight quickly converges in the first three training episodes and shows exceptionally stable and uniformly smooth learning curves across five real-world datasets. One of our main contributions is the generalization of the model. Although OfflineLight-JN1 trained in 20\\% of all data from TSC-OID, it can be deployed directly on other traffic topologies and flows. Intuitively pleasing but with little surprise, OfflineLight-JN1 has almost achieved a similar performance to the SOTA RL methods. Even if the New York's topology and flows have not been collected to generate the offline dataset, we can directly deploy to its scenario.  Because the OfflineLight is decentralized MARL with individually reward, the difference of traffic topology has little influence on the final performance.\n\n- W2:OfflineLight does not take into account the interactions between multiple traffic lights.\n\n  **Response:**\n\n  We acknowledge the potential benefits of integrating approaches that consider interactions between multiple agents.  In our next phase of development, we aim to explore the incorporation of offline multi-agent interactions to further improve the results in TSC. However, the current OfflineLight has the advantages as follows: \n\n   1. Our OfflineLight has shown superior performance in offline RL scenarios. Notably, it outperforms methods like CoLight and AttendLight (baseline methods in this paper), which do take into account the interactions between various intersections. This outcome indicates that, in certain contexts, the decentralized approach of OfflineLight can be more effective than interaction-centric models. Moreover, OfflineLight is decentralized MARL with individual reward, and the difference of traffic topology has little influence on the final performance.\n\n  2. Comparison with CTDE Methods: We have conducted comparative analyses with methods such as CTDE. Interestingly, OfflineLight exhibited better performance even in these comparisons. MPLight and CoLight are typical CTDE methods, but our OfflineLight performs better. Additionally, a significant advantage of OfflineLight is that it does not require interaction with the environment, which can be a crucial factor in real-world applicability."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6980/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388116000,
                "cdate": 1700388116000,
                "tmdate": 1700388116000,
                "mdate": 1700388116000,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4Qpzo0HTii",
                "forum": "p7iVaVidha",
                "replyto": "1SPw334vrX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6980/Reviewer_Twe3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6980/Reviewer_Twe3"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response."
                    },
                    "comment": {
                        "value": "Although the rebuttal has addressed my concerns, I still believe this paper lacks novelty and applicability in real problem setting. I have raised my score to 5."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6980/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718914352,
                "cdate": 1700718914352,
                "tmdate": 1700718914352,
                "mdate": 1700718914352,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]