[
    {
        "title": "Probabilistic Adaptation of Black-Box Text-to-Video Models"
    },
    {
        "review": {
            "id": "YXtu8zFco3",
            "forum": "pjtIEgscE3",
            "replyto": "pjtIEgscE3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4192/Reviewer_4TFV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4192/Reviewer_4TFV"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates how to adapt a pre-trained text-to-video model to a specific domain without modifying the pretrained model. The authors propose Video Adapter, which trained a task-specific small video model and leverage pre-trained models to guide generation during inference."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Since this framework only requires training a small domain-specific text-to-video model, the domain-specific model can be adapted to any pretrained models as long as their input and output definitions are same.\n\nExperiments clearly demonstrate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "The authors assume pretrained model weights are not accessible. Table 1 and figure 9 verifies the authors\u2019 claim. However, it is not clear how does the model perform compared to other methods, such as LoRA and prefix-tuning if pretrained model weights are accessible."
                },
                "questions": {
                    "value": "Method in Sec 3 is not limit to text-to-video generation and should also be applicable to text-to-image generation. Have you considered other use cases?\n\nOnly two models (a pretrained model and a domain-specific model) are involved in derivation in Sec. 3. If we have more than 1 domain-specific model, can we easily extend this method to multiple domain-specific models?\n\nI know the assumption here is that pretrained models are not accessible during training. But what if we can access the pre-trained models during training, will it outperform LoRA and prefix-finetuning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4192/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4192/Reviewer_4TFV",
                        "ICLR.cc/2024/Conference/Submission4192/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4192/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698625403791,
            "cdate": 1698625403791,
            "tmdate": 1700631697186,
            "mdate": 1700631697186,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uAClYHIhWl",
                "forum": "pjtIEgscE3",
                "replyto": "YXtu8zFco3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4192/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4192/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "> Comparison to parameter efficient finetuning.\n\nThe problem setting of VideoAdapter is that pretrained model weights are **NOT** accessible. This scenario is common in large language models (e.g., GPT-4, Bard), and large text-to-video models are heading in the same direction. Parameter efficient finetuning (e.g., LoRA, null-text inversion, prefix-tuning) requires access to pretrained model weights, whereas VideoAdaptor does not. Therefore VideoAdapter is not comparable to parameter efficient finetuning.\n\nNevertheless, we conducted comparisons to LoRA and null-text inversion out of curiosity (prefix-tuning is omitted since it has only been applied to language models). For LoRA, we use rank 1 and rank 64 to compare to the smaller and larger task-specific VideoAdapter model. For null-text inversion, we use an unconditional null embedding of size [64, 4096] (the same dimension as the original text embeddings). We report the video modeling metrics in the following table: \n\n\n| Method | Bridge FVD &darr; | Bridge FID &darr; | Ego4D FVD &darr; | Ego4d IS &uarr; |\n| ----------- | ----------- | ----------- | ----------- | ----------- |\n| VideoAdapter Small | 177.4 | 37.6 | 156.3 | 2.8 |\n| LoRA-Rank1 | 170.2 | 32.2 | 74.5 | 3.4 |\n| Small (no adaption)  | 165.5 | 30.1 | 65.1 | 3.3 |\n| Video Adapter Large | 148.1 | 29.5 | 52.5 | 3.5 |\n| LoRA-Rank64 | 165.5 | 31.6 | 50.3 | 3.5 |\n| Null-text inversion | 288.8 | 40.2 | 90.2 | 3.1 |\n\nWe observe that LoRA-Rank1 performs slightly better than VideoAapter (small). However, In the LoRA-Rank1 case, LoRA still performs worse than training a small domain specific model. In this case, VideoAdapter can simply use the small model without the pretrained prior. In comparison, we found LoRA-Rank64 leads to mixed results when compared to VideoAdaptor (large), i.e., LoRA outperforms VideoAdapter on Ego4D but not on Bridge data. We found that null-text inversion performs the worst, potentially due to limited flexibility of null-embeddings during finetuning.\n\nOur results illustrate that VideoAdapter, despite requiring only black-bo adaptation without access to pretrained model weights performs better than Null-text inversion and very comparably to LoRA finetuning (with pretrained model weights).\n\n> Use case in text-to-image adaptation.\n\nVideoAdapter is indeed applicable to the text-to-image setting, as text-to-image is a special case of text-to-video. We focused on the text-to-video setting because the pretrained text-to-video models are generally much bigger than pretrained text-to-image models, which requires even more efficient adaptation.\n\n> Composing multiple models.\n\nThis is not only possible but in favor of VideoAdapter compared to other adaptation methods (e.g., efficient finetuning), since VideoAdapter can compose arbitrary black-box models at inference time. We provide an additional analysis on adaptation in Appendix D."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502339175,
                "cdate": 1700502339175,
                "tmdate": 1700502339175,
                "mdate": 1700502339175,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "79S9g4oQL7",
                "forum": "pjtIEgscE3",
                "replyto": "uAClYHIhWl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4192/Reviewer_4TFV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4192/Reviewer_4TFV"
                ],
                "content": {
                    "comment": {
                        "value": "Authors solve all my concerns and I would increase my rating."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631670237,
                "cdate": 1700631670237,
                "tmdate": 1700631670237,
                "mdate": 1700631670237,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iuysOVRvXO",
            "forum": "pjtIEgscE3",
            "replyto": "pjtIEgscE3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4192/Reviewer_nNvZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4192/Reviewer_nNvZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new method called Video Adapter to adapt a large pretrained text-to-video diffusion model for new transfer and enviroment. Video Adapter trained a small domain-specific model to provide the score to guide the inversion trajectory of large-scale pretrained model. The proposed method was successful in many downstream tasks including animation, style transfer and egocentric modeling."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is easy to follow, and the overall writing is good.\n2. The proposed method, Video Adapter, is a simple yet effective way to guide the inversion trajectory to adapt for new scenarios."
                },
                "weaknesses": {
                    "value": "1. My major concern is the lack of comparison with other baseline models like Lora, null-text inversion and prefix-tuning. Surely, these methods I mentioned above were designed for image adaptation but, correct me if I am wrong, it seems that they can be easily transplanted to the video diffusion model to serve as baselines models. Authors should explain more about why such comparison is not necessary.\n\n2.In table 1, small+pretrained demonstrate the best quantitative performance on both Bridge and Ego4D. However, these numbers are very close to only small variant. Considering the poor generative quality of small model in Figure 7 and 8, it is questionable whether small+pretrained can generate high-quality videos consistently."
                },
                "questions": {
                    "value": "1. The prior strength weight is constant during the sampling. It would be interesting to see if that the prior strength weight is changing with t could be helpful.\n\nI would increase my rating if my concerns are properly addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4192/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4192/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4192/Reviewer_nNvZ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4192/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782177757,
            "cdate": 1698782177757,
            "tmdate": 1700684631035,
            "mdate": 1700684631035,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rz8JWtvHVk",
                "forum": "pjtIEgscE3",
                "replyto": "iuysOVRvXO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4192/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4192/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "> Comparison to parameter efficient finetuning.\n\nThe problem setting of VideoAdapter is that pretrained model weights are **NOT** accessible. This scenario is common in large language models (e.g., GPT-4, Bard), and large text-to-video models are heading in the same direction. Parameter efficient finetuning (e.g., LoRA, null-text inversion, prefix-tuning) requires access to pretrained model weights, whereas VideoAdaptor does not. Therefore VideoAdapter is not comparable to parameter efficient finetuning.\n\nNevertheless, we conducted comparisons to LoRA and null-text inversion out of curiosity (prefix-tuning is omitted since it has only been applied to language models). For LoRA, we use rank 1 and rank 64 to compare to the smaller and larger task-specific VideoAdapter model. For null-text inversion, we use an unconditional null embedding of size [64, 4096] (the same dimension as the original text embeddings). We report the video modeling metrics in the following table: \n\n\n| Method | Bridge FVD &darr; | Bridge FID &darr; | Ego4D FVD &darr; | Ego4d IS &uarr; |\n| ----------- | ----------- | ----------- | ----------- | ----------- |\n| VideoAdapter Small | 177.4 | 37.6 | 156.3 | 2.8 |\n| LoRA-Rank1 | 170.2 | 32.2 | 74.5 | 3.4 |\n| Small (no adaption)  | 165.5 | 30.1 | 65.1 | 3.3 |\n| Video Adapter Large | 148.1 | 29.5 | 52.5 | 3.5 |\n| LoRA-Rank64 | 165.5 | 31.6 | 50.3 | 3.5 |\n| Null-text inversion | 288.8 | 40.2 | 90.2 | 3.1 |\n\nWe observe that LoRA-Rank1 performs slightly better than VideoAapter (small). However, In the LoRA-Rank1 case, LoRA still performs worse than training a small domain specific model. In this case, VideoAdapter can simply use the small model without the pretrained prior. In comparison, we found LoRA-Rank64 leads to mixed results when compared to VideoAdaptor (large), i.e., LoRA outperforms VideoAdapter on Ego4D but not on Bridge data. We found that null-text inversion performs the worst, potentially due to limited flexibility of null-embeddings during finetuning.\n\nOur results illustrate that VideoAdapter, despite requiring only black-bo adaptation without access to pretrained model weights performs better than Null-text inversion and very comparably to LoRA finetuning (with pretrained model weights).\n\n> Poor video quality of small model in Figure 7 and 8.\n\nWe used the larger version of the small model, i.e., Small (L), in Figure 7 and Figure 8. We found that for harder dataset (e.g., Ego4D), too small of an adapter model fails to adapt to the target domain. The size of the adapter should in fact be determined by the complexity of the video domain one wishes to adapt to, and VideoAdapter enables such flexibility during adaptation. We also found that video modeling metrics sometimes do not faithfully reflect sample quality, especially in failing to reflect whether the generated videos corresponds to the given text instruction.\n\n> Tunable prior strength.\n\nWe have indeed found tunable prior strength to be helpful. For instance, we set the prior strength to 0 in the last 100 timesteps of diffusion (last paragraph in Appendix B.1), which we found to result in better generation quality than a fixed prior strength. In general, we found that  higher strengths at higher noise levels were helpful \u2013 perhaps due to more accurate score functions at higher timesteps. We have made tunable prior strength more explicit in Section 3.2 (Page 4)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502315202,
                "cdate": 1700502315202,
                "tmdate": 1700502315202,
                "mdate": 1700502315202,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7CPffLcxvO",
                "forum": "pjtIEgscE3",
                "replyto": "rz8JWtvHVk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4192/Reviewer_nNvZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4192/Reviewer_nNvZ"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "The rebuttal addressed most of my concerns. Therefore I'd like increase my rating.\nFor the 2nd weakness I have asked in my review, recent results in stable video diffusion also demonstrate the ineffectiveness of FVD as it cannot properly evaluate whether a model generate authetic video frames. I think the numbers in Fig 7 and 8 are another example of this ineffectiveness"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684604327,
                "cdate": 1700684604327,
                "tmdate": 1700684604327,
                "mdate": 1700684604327,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OGFgSdzrj3",
            "forum": "pjtIEgscE3",
            "replyto": "pjtIEgscE3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4192/Reviewer_yyCL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4192/Reviewer_yyCL"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an adapter-based video-generation model training methodology. They use a large pre-trained videogen model and use the output scores from the model (at each step of denoising in diffusion models) to help train a smaller model for a specific task.\nThe black box large pre-trained model has been used as a prior to learn a distribution of new stylistic videos in a finetuning-free, probabilistic composition framework. The small model is learned via MLE and the black box model EBM interpretation is used to sample from it. To probabilistically adopt the pre trained black-box model to new data, the denoising prediction is changed to the sum of predictions from both the black-box pre-trained model and the task-specific small model. The combined model is further refined by MCMC sampling. \n The authors show successful experiments on controlled video generation, egocentric videos, and realistic robotic videos from simulation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors tackle a novel problem of building proprietary video models which has not been addressed before in the video generation domain. The videos provided as supplementary material look great.\n2. The paper is well-written and self-explanatory with well-cited results and related work"
                },
                "weaknesses": {
                    "value": "The paper is based on the fact that there will be more closed source companies willing to open source their diffusion scores."
                },
                "questions": {
                    "value": "The description of \u201cControllable video synthesis\u201d is not very clear in terms of inputs. The text description as inputs is not provided very clearly hence someone without knowing the problem statement of this particular problem has difficulty in understanding what are the inputs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4192/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830324467,
            "cdate": 1698830324467,
            "tmdate": 1699636385606,
            "mdate": 1699636385606,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HjmctqsIEi",
                "forum": "pjtIEgscE3",
                "replyto": "OGFgSdzrj3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4192/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4192/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for your positive evaluation of the paper. We address each of the questions and concerns below.\n\n> Companies Willing to Opensource Diffusion Scores\n\nWe agree that it is not entirely clear if companies would be willing to open-source predicted diffusion scores, but believe that there is no free lunch \u2013 only with more access to the generation process would we be able to more effectively adapt models. The aim of our paper is to demonstrate that with relatively black-box access (only access to the score predictions during generation) we can already achieve effective adaptation of diffusion models, motivating companies to enable access to score predictions. There is also some precedent to releasing some intermediate generation information from companies \u2013 LLM APIs for instance often release intermediate logit likelihoods.\n\n> Textual Clarification\n\nWe have updated \u201cControllable video synthesis\u201d to \u201cAdapting to Specific Video Domains\u201d for clarity."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502265345,
                "cdate": 1700502265345,
                "tmdate": 1700502265345,
                "mdate": 1700502265345,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lqvwVRvrwV",
            "forum": "pjtIEgscE3",
            "replyto": "pjtIEgscE3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4192/Reviewer_Z2pV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4192/Reviewer_Z2pV"
            ],
            "content": {
                "summary": {
                    "value": "The Video Adapter framework addresses the challenge of adapting black-box, large text-to-video models to specific domains such as robotics and animation, without access to the models' weight parameters. This innovative approach, inspired by the prompting mechanism in large language models, uses the score function of a pretrained video diffusion model as a probabilistic prior, guiding the generation of a smaller, task-specific video model. Remarkably, this smaller model, with only 1.25% of the parameters of the original, can generate high-quality and domain-specific videos across diverse applications. The authors advocate for private enterprises to provide access to the scores of video diffusion models, enhancing the adaptability of large pretrained text-to-video models, with video demos available in the supplementary material."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The tackled problem is important and practical.\n2. The proposed idea is intuitive.\n3. Some of the results look interesting."
                },
                "weaknesses": {
                    "value": "1. The current manuscript could really use a better position to avoid over-claiming.\n\na. Currently the paper highlights the ability of adapting black-box pretrained large text-video model. However, it relaxes the assumption of obtaining intermediate generation results of the black-box model, which is not realistic, and even contradicts with the term \"black-box\". It is really inappropriate to just use an existing term in a completely different setting.\n\nb. The current manuscript also highlights parameter efficiency. However, the current evidence is not enough to really claim this. There has been some many parameter efficient fine-tuning techniques and the comparison with such existing method is really needed to claim superiority from this part. Specifically, comparisons with LoRA and other SOTA PEFT methods should be at least provided to help the audience really know the performance of existing methods.\n\nc. The current setup is actually not that highlighting efficiency. The largest small model can even take half of the computation of the large model. It would be much more rigorous if the authors can set a computation budget and compare methods under a fixed budget.\n\n2. Evaluation of the video generation. From qualitative inspect, I don't really think Video Adapter in Figure 8 is really better. Is there any case study further examining the human preference and the score obtained from automatic evaluation, especially on datasets like Ego4D?"
                },
                "questions": {
                    "value": "Please check weakness for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4192/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698848051852,
            "cdate": 1698848051852,
            "tmdate": 1699636385548,
            "mdate": 1699636385548,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wIpeJWprbt",
                "forum": "pjtIEgscE3",
                "replyto": "lqvwVRvrwV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4192/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4192/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for your constructive evaluation of the paper. We address each of the questions and concerns below.\n\n> Black Box Terminology.\n\nWe believe that our approach does act in a \u201cblack box\u201d manner since it does not require access or knowledge about any of the weights of a neural network (treating the neural network itself as a \u201cblack box\u201d), but rather only access to predictions from the neural network (albeit at each point of sample generation). While this uses more information than having only the final generated sample, there is no free lunch, and the greater the information given, the more effectively models are able to be adapted. There is precedence for giving access to such intermediate predictions in black-box models, for example the chatGPT API gives intermediate prediction logits.\n\nHowever, if the reviewer feels strongly about this, we are happy to adopt another word to describe our approach.\n\n> Parameter Finetuning\n\nThe problem setting of VideoAdapter is that pretrained model weights are NOT accessible. This scenario is common in large language models (e.g., GPT-4, Bard), and large text-to-video models are heading in the same direction. Parameter efficient finetuning (e.g., LoRA, null-text inversion, prefix-tuning) requires access to pretrained model weights, whereas VideoAdaptor does not. Therefore VideoAdapter is not comparable to parameter efficient finetuning.\n\nNevertheless, we conducted comparisons to LoRA and null-text inversion out of curiosity (prefix-tuning is omitted since it has only been applied to language models). For LoRA, we use rank 1 and rank 64 to compare to the smaller and larger task-specific VideoAdapter model. For null-text inversion, we use an unconditional null embedding of size [64, 4096] (the same dimension as the original text embeddings). We report the video modeling metrics in the following table: \n\n\n| Method | Bridge FVD &darr; | Bridge FID &darr; | Ego4D FVD &darr; | Ego4d IS &uarr; |\n| ----------- | ----------- | ----------- | ----------- | ----------- |\n| VideoAdapter Small | 177.4 | 37.6 | 156.3 | 2.8 |\n| LoRA-Rank1 | 170.2 | 32.2 | 74.5 | 3.4 |\n| Small (no adaption)  | 165.5 | 30.1 | 65.1 | 3.3 |\n| Video Adapter Large | 148.1 | 29.5 | 52.5 | 3.5 |\n| LoRA-Rank64 | 165.5 | 31.6 | 50.3 | 3.5 |\n| Null-text inversion | 288.8 | 40.2 | 90.2 | 3.1 |\n\nWe observe that LoRA-Rank1 performs slightly better than VideoAapter (small). However, In the LoRA-Rank1 case, LoRA still performs worse than training a small domain specific model. In this case, VideoAdapter can simply use the small model without the pretrained prior. In comparison, we found LoRA-Rank64 leads to mixed results when compared to VideoAdaptor (large), i.e., LoRA outperforms VideoAdapter on Ego4D but not on Bridge data. We found that null-text inversion performs the worst, potentially due to limited flexibility of null-embeddings during finetuning.\n\nOur results illustrate that VideoAdapter, despite requiring only black-bo adaptation without access to pretrained model weights performs better than Null-text inversion and very comparably to LoRA finetuning (with pretrained model weights).\n\n> Computational Efficiency.\n\nIn the Bridge setting, our small model is substantially smaller in size than our original pretrained model. However, in more complex domains, it is necessary to use larger models to capture the characteristics of that domain, and thus our larger model is similar in size to our original pretrained model.  In the table above, however, we have equated the computational budget across both our approach and parameter-efficient adaptation methods and find that our approach performs comparably to such approaches.\n\n> Video Generation Qualitative Comparison\n\nWe have added a video comparison in https://drive.google.com/file/d/16O_RsGhwDhUmUsHTf8lSW8LvvCbEQ01r/view?usp=sharing. From these video results, it\u2019s clear that our approach substantially better than the pretrained model without adaptation."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502239103,
                "cdate": 1700502239103,
                "tmdate": 1700633392440,
                "mdate": 1700633392440,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]