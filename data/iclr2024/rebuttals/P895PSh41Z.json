[
    {
        "title": "Relaxed State-Adversarial Offline Reinforcement Learning: A Leap Towards Robust Model-Free Policies from Historical Data"
    },
    {
        "review": {
            "id": "TqZG10LDs9",
            "forum": "P895PSh41Z",
            "replyto": "P895PSh41Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1710/Reviewer_6xeb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1710/Reviewer_6xeb"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduce a model-free offline RL approach, RAORL, by framing the problem within a state adversarial context. RAORL avoids model uncertainty issues and eliminates the need for explicit environmental modeling. The method ensures policy robustness, providing performance guarantees. Empirical evaluations demonstrate that RAORL consistently meets or surpasses the performance of existing SOTA methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The studied problem model-free robust offline RL is important.\n* The proposed method and the theoretical analysis seems reasonable.\n* The empirical results on the D4RL benchmark and a robustness task shows the efficacy of the proposed method."
                },
                "weaknesses": {
                    "value": "I appreciate the authors' efforts in investigating the studied problem; however, I think the current version requires several revisions and improvements.\n\n* The methods section appears somewhat unclear. It is recommended that the authors include a subsection on the implementation of RAORL, detailing the learning objectives for both value functions and policy.\n\n* RORL [1] is already a model-free state-adversarial method that achieves state-of-the-art performance on the MuJoCo benchmark and robust experiments. A thorough discussion and comparison with this work are necessary. Moreover, the proposed method seems quite similar to the adversarial state training in S4RL [2]. A more comprehensive discussion with closely related works is required to emphasize the significance of the method; otherwise, the novelty may be questioned.\n\n* It appears that the method can be applied to different base algorithms besides ReBrac. Incorporating the method with approaches like ATAC and IQL could enhance the comprehensiveness of the experiments and support the claims made in the paper.\n\n* In the state perturbation experiments, a comparison with the prior state adversarial baseline RORL is needed.\n\n* The impact of each hyperparameter, $\\epsilon$ and $\\alpha$, on the performance of RAORL remains unclear.\n\n* The authors should provide more implementation details in the Appendix to facilitate reproduction of the results.\n\n\n[1] Yang, R., Bai, C., Ma, X., Wang, Z., Zhang, C., & Han, L. (2022). Rorl: Robust offline reinforcement learning via conservative smoothing. Advances in Neural Information Processing Systems, 35.\n\n[2] Sinha S, Mandlekar A, Garg A. S4rl: Surprisingly simple self-supervision for offline reinforcement learning in robotics[C]//Conference on Robot Learning. PMLR, 2022."
                },
                "questions": {
                    "value": "My questions are the same in the \"Weakness\" part.\n\n* The authors should consider adding a subsection in the method section that details the learning objectives for both value functions and policy.\n\n* A comprehensive discussion and comparison with RORL and S4RL are essential.\n\n* Incorporating the method with approaches like ATAC and IQL could improve the comprehensiveness of the experiments.\n\n* A comparison with the prior state adversarial baseline RORL is necessary in the state perturbation experiments.\n\n* A hyperparameter study about the $\\epsilon$ and $\\alpha$ would be beneficial.\n\n* To facilitate the reproduction of results, the authors should provide more implementation details in the Appendix."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1710/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1710/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1710/Reviewer_6xeb"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1710/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698227577054,
            "cdate": 1698227577054,
            "tmdate": 1699636099674,
            "mdate": 1699636099674,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iigLvpVuM7",
                "forum": "P895PSh41Z",
                "replyto": "TqZG10LDs9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank reviewer 6xeb for the insightful comments. Below, we address the questions raised by the reviewer. We hope the replies could help the reviewer further recognize our contributions. Thank you.\n\n> Q: It appears that the method can be applied to different base algorithms besides ReBrac. Incorporating the method with approaches like ATAC and IQL could enhance the comprehensiveness of the experiments and support the claims made in the paper.\n\nA: We are currently conducting experiments to test our method with different base algorithm IQL, to further validate its applicability and effectiveness. The following table are results from integrating the Relaxed Adversarial State technique with IQL are promising. These experiments, run on three different seeds, show notable improvements in performance.\n\n|                          | IQL            | IQL+RA         |\n|--------------------------|----------------|----------------|\n| Halfcheetah-medium-expert| 86.7           | **93.3 \u00b1 1.5**     |\n| hopper-medium-expert     | 109.6          | **112 \u00b1 1.9**      |\n| walker2d-medium-expert   | 91.5           | **112.4 \u00b1 0.6**    |\n\n\n> Q: In the state perturbation experiments, a comparison with the prior state adversarial baseline RORL is needed.\n\nA: Our methodology is primarily based on the ReBrac approach, making the performance comparison between our method and RORL particularly crucial. RORL utilizes an ensemble-based model, with \"RORL-n\" indicating the use of n ensemble models. In the following analysis on Hopper-medium-expert over 4 seeds, we demonstrate that our method achieves comparable results to RORL-10, which involves an ensemble size five times larger than ours. Furthermore, our approach notably outperforms RORL-2, which has the same numbers of critics of our model.\n\n| Attack magnitude | RAORL | ReBrac | RORL-10 | RORL-2 |\n|--------|-------|--------|---------|--------|\n| 0.025  | 72.1  | 67.1   | 75.8    | 48.1   |\n| 0.05   | 61.2  | 40.2   | 65.0    | 33.2   |\n| 0.075  | 53.1  | 35.3   | 53.5    | 32.6   |\n| 0.1    | 41.7  | 30.1   | 44.3    | 29.1   |\n\n> Q: The impact of each hyperparameter,  and , on the performance of RAORL remains unclear.\n\nA: In RAORL, we introduce two critical hyperparameters: the radius of the uncertainty set and the relaxed parameter alpha. The radius of the uncertainty set determines its size, directly influencing the range of potential state-action pairs considered during policy training. The relaxed parameter alpha, on the other hand, plays a pivotal role in defining the distribution over the uncertainty set. Varied values of alpha represent different prior assumptions about the environment. For instance, setting alpha to 1 suggests a belief that the nominal MDP is the most likely scenario, indicating confidence in the representativeness of the training data. Conversely, an alpha value of 0 implies a lean towards the worst-case MDP, preparing the policy for the most challenging possible scenarios."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576238195,
                "cdate": 1700576238195,
                "tmdate": 1700576789499,
                "mdate": 1700576789499,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3r9a7j81d8",
                "forum": "P895PSh41Z",
                "replyto": "TqZG10LDs9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank reviewer 6xeb for the insightful comments. Below, we address the questions raised by the reviewer. We hope the replies could help the reviewer further recognize our contributions. Thank you.\n\n\n> Q: The methods section appears somewhat unclear. It is recommended that the authors include a subsection on the implementation of RAORL, detailing the learning objectives for both value functions and policy.\n\nA: Our algorithm can be found in algorithm 1. Specifically, inspired by TD3+BC (Fujimoto & Gu, 2021) and ReBrac (Tarasov et al., 2023), Objective function for the policy: \n\n$$\n\\pi = argmax_{\\pi} \\mathbb{E}_{(s,a) \\sim D} \\left[ \\lambda Q(s, \\pi(s)) - (\\pi(s) - a)^2 \\right]\n$$\n\nThe default values of the hyper-parameters($\\lambda$) were used in the experiments (same as ReBrac (Tarasov et al., 2023)).\n\nObjective function for the average scenario value function is followed the lemma2:\n\n$$\nL_{TD}(\\theta) = E_{(s,a,s') \\sim D} \\left[ \\left( r(s,a) + \\gamma \\big( \\alpha Q_{\\bar{\\theta}}(s', \\pi(s\u2019)) + (1-\\alpha) Q_{\\bar{\\theta}}(adv(s'), \\pi(s\u2019)) \\big) - Q_{\\theta}(s, a) \\right)^2 \\right],\n$$\n\nWe set perturbation radius from the set {0.03, 0.05, 0.08, 0.1} multiplied by state differences (in Section 4.4). The setting of attack budgets were chosen to reflect the mean magnitude of the state affected by actions taken in each environment.\n\n\n\n\n> Q: A thorough discussion and comparison with this work are necessary. Moreover, the proposed method seems quite similar to the adversarial state training in S4RL [2]. A more comprehensive discussion with closely related works is required to emphasize the significance of the method; otherwise, the novelty may be questioned.\n\nA: The main contribution of RAORL over S4RL is a theoretical guarantee for the lower bound of performance, further justifying the use of state-adversaries . Besides, we acknowledge the importance and \u201crelaxed\u201d technique beyond state-adversarial technique. In S4RL, the state-adversarial technique was identified as the most effective augmentation method based on their experiments. However, applying the state-adversarial technique in a straightforward manner can lead to overly cautious results (Lien et al., 2023). This is primarily because the state-adversarial technique inherently prepares for the worst-case scenario. Therefore, in practical applications, particularly in real-world problems, it becomes essential to adopt a \"relaxed\" version of the state-adversarial technique. The following table shows a comparative study of RAORL and S4RL in various experimental environments.\n\n|                          | RAORL         | S4RL           |\n|--------------------------|---------------|----------------|\n| Halfcheetah-medium       | **66.1**          | 48.6           |\n| Halfcheetah-medium-replay| **51.0**          | **51.7**         |\n| Halfcheetah-medium-expert| **107.0**        | 78.1           |\n| hopper-medium            | **102.3**         | 81.3           |\n| hopper-medium-replay     | **100.4**        | 36.8           |\n| hopper-medium-expert     | 108.9         | **117.9**         |\n| walker2d-medium          | 86.8          | **93.1**           |\n| walker2d-medium-replay   | **85.0**          | 35.0           |\n| walker2d-medium-expert   | **112.2**        | 107.1          |\n| Average                  | **90.9**          | 61.6           |\n\n\n|                       | RAORL         | S4RL          |\n|-----------------------|---------------|---------------|\n| antmaze-umaze         | **97.5**      | 94.1          |\n| antmaze-umaze-diverse | 87.7          | **88.0**      |\n| antmaze-medium-play   | **91.5**      | 61.6          |\n| antmaze-medium-diverse| **86.7**      | 82.3          |\n| antmaze-large-play    | **71.2**      | 25.1          |\n| antmaze-large-diverse | **68**        | 26.2          |\n| Average               | **83.8**      | 62.8          |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576690896,
                "cdate": 1700576690896,
                "tmdate": 1700627521191,
                "mdate": 1700627521191,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hRfmzprSBy",
                "forum": "P895PSh41Z",
                "replyto": "3r9a7j81d8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1710/Reviewer_6xeb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1710/Reviewer_6xeb"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for your response. After carefully reading the response, my concerns persist. First, the paper was not updated. The discussion for the learning objectives, discussions about RORL and S4RL, and new results are not included in the revision. Moreover, the paper continues to lack the necessary implementation details that would enable the replication of the results, along with comprehensive studies on the introduced hyperparameters. Therefore, I will keep my current rating."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624343808,
                "cdate": 1700624343808,
                "tmdate": 1700624343808,
                "mdate": 1700624343808,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZmQSaE1Nzw",
            "forum": "P895PSh41Z",
            "replyto": "P895PSh41Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1710/Reviewer_K6G5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1710/Reviewer_K6G5"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the Relaxed State-Adversarial Offline Reinforcement Learning (RAORL) method, a model-free offline RL approach that deals with model uncertainty issues by using a state adversarial context. By eliminating the requirement for explicit environmental modeling, RAORL offers a robust and adaptive policy formulation. It addresses challenges related to offline RL's dataset limitations and ensures performance in real-world applications without online interactions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The primary contribution of this paper is the introduction of a robust offline reinforcement learning method that offers a lower bound guarantee. This aspect distinguishes the work and highlights its potential significance in the field of reinforcement learning."
                },
                "weaknesses": {
                    "value": "1. The quality of writing in this paper could benefit from further refinement.  Some explanations are unclear.  For instance, in Lemma 1, formula 1 and formula 2 look identical, I had thought that formula 1 might have been inadvertently repeated.  But it turns out there are slight differences. I think there should be a better presentation such as using the absolute difference is bounded. Furthermore, the content of Lemma 1 raises questions for me.  The textual description seems to discuss the relationship of the policy's value learned from two different properties of P_B.  However, the formula appears to depict the policy's value learned from U and P_B.  I hope the authors recognize this distinction.  One refers to the value function learned under a specific P_B, while the other pertains to the average value function learned on U.\n\n2. While this paper offers a plethora of proofs and new definitions, it lacks a coherent logical thread to effectively connect them. Additionally, the absence of adequate textual explanations for the theorems and lemmas makes the paper hard to follow and comprehend. There is no formal problem formulation for the problem the paper wants to solve. A more structured approach and clear expositions would greatly enhance its readability.\n\n3. This paper introduces the concept of the Relaxed State-Adversarial Transition Kernel. However, the name doesn't seem to genuinely capture the essence of this transition kernel. I'm uncertain where the \"State-Adversarial\" aspect is present. The approach of crafting a new transition kernel using a linear combination appears to be already employed by many robust RL under model uncertainty methods, such as the R-contamination uncertainty set. You may check this paper \"Policy Gradient Method For Robust Reinforcement Learning\"."
                },
                "questions": {
                    "value": "Please see the Weaknesses, I will decide my final score after rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Non."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1710/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698339108568,
            "cdate": 1698339108568,
            "tmdate": 1699636099592,
            "mdate": 1699636099592,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v1t69uM19u",
                "forum": "P895PSh41Z",
                "replyto": "ZmQSaE1Nzw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank reviewer K6G5 for the insightful comments. Below, we address the questions raised by the reviewer. We hope the replies could help the reviewer further recognize our contributions. Thank you.\n\n> Q: The quality of writing in this paper could benefit from further refinement. Some explanations are unclear. For instance, in Lemma 1, formula 1 and formula 2 look identical, I had thought that formula 1 might have been inadvertently repeated. But it turns out there are slight differences. I think there should be a better presentation such as using the absolute difference is bounded. Furthermore, the content of Lemma 1 raises questions for me. The textual description seems to discuss the relationship of the policy's value learned from two different properties of P_B. However, the formula appears to depict the policy's value learned from U and P_B. I hope the authors recognize this distinction. One refers to the value function learned under a specific P_B, while the other pertains to the average value function learned on U.\n\nA: We revise Lemma 1 in absolute difference and clarify the details in Lemma 1 as follows.\n\n1. Relevance of U and P_B: As shown in Figure 1,  U represents the set of all possible real-world transition kernels, encompassing a wide range of scenarios and environmental dynamics. In contrast, P_B is specific to the offline dataset and represents the transitions recorded in that particular dataset.  \n\n2. Objective of Lemma 1: The lemma intends to quantify the performance gap between a policy learned exclusively from the offline dataset (P_B) and the policy evaluated against the universal set of transition kernels (U). This gap highlights the potential discrepancies and challenges in applying policies learned in a limited offline context to a broader range of real-world scenarios. \n\n3. Lemma 1 in absolute difference :  \\begin{align}\n\\left| J_{\\rho_0^B}(\\pi, P_B) -E_{P_0 \\sim U}(J_{\\rho_0}(\\pi, P_0)) \\right| \\leq & \\frac{2R_{\\text{max}}}{1-\\gamma} E_{P_0 \\sim U}[D_{\\text{TV}}(\\rho_0,\\rho_0^B)]  + \\frac{2 \\gamma R_{\\text{max}}}{(1-\\gamma)^2} \\beta + \\frac{2R_{\\text{max}}}{1-\\gamma} E_{P_0 \\sim U}{E}[\\gamma^{\\text{T}_{V}^\\pi}].\n\\end{align}\n\n\n> Q: While this paper offers a plethora of proofs and new definitions, it lacks a coherent logical thread to effectively connect them. Additionally, the absence of adequate textual explanations for the theorems and lemmas makes the paper hard to follow and comprehend. There is no formal problem formulation for the problem the paper wants to solve. A more structured approach and clear expositions would greatly enhance its readability.\n\nA: In the second paragraph of Section 4, we provided a logical thread of sections of 4.1, 4.2 and 4.3. In Section 4.1, we systematically measure the performance discrepancies between the policy shaped solely from the offline dataset ($P_B$) and its application in the universal set of transition kernels (U). Sections 4.2 and 4.3 further build upon this by demonstrating methods to enhance policy performance.\n\n> Q: This paper introduces the concept of the Relaxed State-Adversarial Transition Kernel. However, the name doesn't seem to genuinely capture the essence of this transition kernel. I'm uncertain where the \"State-Adversarial\" aspect is present. The approach of crafting a new transition kernel using a linear combination appears to be already employed by many robust RL under model uncertainty methods, such as the R-contamination uncertainty set. You may check this paper \"Policy Gradient Method For Robust Reinforcement Learning\".\n\nA: The term \"State-Adversarial\" in our Relaxed State-Adversarial Transition Kernel is defined in Definition 2 of our paper. It refers to the process where state transitions are adversarially perturbed towards those with lower value estimates, a concept critical for robustness in continuous state-action spaces. Our kernel thus represents a bridge between standard and adversarial MDPs, a novel contribution particularly tailored for continuous environments.\nWhile the \"Relaxed\" aspect of our kernel does resemble approaches like the R-contamination uncertainty set found in robust RL literature, the \"State-Adversarial\" facet specifically addresses continuous domains. This differentiates our work from that in \"Policy Gradient Method For Robust Reinforcement Learning,\" which primarily addresses discrete spaces (as their algorithm requires the summation over all possible states and actions) and does not extend to the continuous settings that our paper focuses on."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575535424,
                "cdate": 1700575535424,
                "tmdate": 1700575535424,
                "mdate": 1700575535424,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nyBj5fwjYO",
                "forum": "P895PSh41Z",
                "replyto": "v1t69uM19u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1710/Reviewer_K6G5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1710/Reviewer_K6G5"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the response!  I have one followup question: if state-adversarial means the process where state transitions are adversarially perturbed towards those with lower value estimates, shouldn't it be transition-adversarial?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670825353,
                "cdate": 1700670825353,
                "tmdate": 1700670825353,
                "mdate": 1700670825353,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0Gr01GvZWg",
            "forum": "P895PSh41Z",
            "replyto": "P895PSh41Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1710/Reviewer_8FWn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1710/Reviewer_8FWn"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Relaxed State-Adversarial Offline Reinforcement Learning (RAORL), a model-free approach for offline RL. RAORL reframes the policy problem as a state-adversarial optimization challenge, eliminating the need for explicit environmental modeling and addressing model uncertainty issues. The method guarantees policy robustness and adaptability to dynamic transitions. Empirical evaluations on the D4RL benchmark demonstrate RAORL's superiority over baseline methods in continuous-control tasks, highlighting its potential for risk-sensitive applications."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper introduces an intriguing concept by integrating adversarial robust reinforcement learning with offline model-free reinforcement learning within a state-adversarial optimization framework.\n\n2. The paper's solid theoretical foundation and validation of the reality gap contribute to its strength."
                },
                "weaknesses": {
                    "value": "1. The paper's structure is not well-organized, making it challenging to grasp the primary contributions and follow the narrative. Furthermore, the writing is convoluted, containing multiple unclear sentences that hinder comprehension.\n\n2. The experimental results lack persuasiveness. Firstly, the authors do not compare their work with state-of-the-art algorithms such as SAC-N, EDAC [1], and LB-SAC [2]. Even when implementing the algorithms based on https://github.com/tinkoff-ai/CORL, this paper's results are inferior to previous methods in most Mujoco and Adroit environments. Hence, I believe there are significant limitations in the paper's empirical contributions.\n\n3. Figure 2 should clarify the color codes representing different algorithms and indicate the corresponding perturbation magnitudes. The paper should explain how optimal state perturbations were determined in the robustness analysis. The connection between the state perturbations in the experiments and the real-world environments, as claimed in the paper, seems tenuous. I have reservations about the practical significance of this paper.\n\n4. While the paper focuses on model-free offline RL, I find it puzzling that model-based offline RL is included in the related work. Robust RL works like [3, 4] (and others) dealing with state adversaries seem more relevant, yet the authors do not discuss them in the related work section.\n\n[1] Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble. Gaon An, Seungyong Moon,  Jang-Hyun Kim, Hyun Oh Song. NeurIPS 2021.\n\n[2] Q-Ensemble for Offline RL: Don\u2019t Scale the Ensemble, Scale the Batch Size. Alexander Nikulin, Vladislav Kurenkov, Denis Tarasov, Dmitry Akimov, Sergey Kolesnikov.\n\n[3] Robust Reinforcement Learning on State Observations with Learned Optimal Adversary. Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh. ICLR 2021.\n\n[4] Efficient Adversarial Training without Attacking: Worst-Case-Aware Robust Reinforcement Learning. Yongyuan Liang, Yanchao Sun, Ruijie Zheng, Furong Huang. Neurips 2022."
                },
                "questions": {
                    "value": "1. How does RAORL effectively bridge the performance gap in real-world applications, as claimed in the introduction?\n\n2. How to determine the attack budget ($\\epsilon$) in RAORL?\n\n3. I am seeking clarity regarding the primary advantage of RAORL. Is it intended to be more effective than other offline model-free algorithms, aimed at improving adversarial robustness, or targeting enhanced offline performance? While the paper mentions these advantages, the results do not strongly demonstrate superior performance or robustness with enough evidence and explanation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed.",
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1710/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1710/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1710/Reviewer_8FWn"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1710/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810987300,
            "cdate": 1698810987300,
            "tmdate": 1700661634214,
            "mdate": 1700661634214,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rIRksiZcBp",
                "forum": "P895PSh41Z",
                "replyto": "0Gr01GvZWg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank reviewer 8FWn for the insightful comments. Below, we address the questions raised by the reviewer. We hope the replies could help the reviewer further recognize our contributions. Thank you.\n\n> Q: The paper's structure is not well-organized, making it challenging to grasp the primary contributions and follow the narrative. Furthermore, the writing is convoluted, containing multiple unclear sentences that hinder comprehension.\n\nA: In the second paragraph of Section 4, we provided a logical thread of sections of 4.1, 4.2 and 4.3. In Section 4.1, we systematically measure the performance discrepancies between the policy shaped solely from the offline dataset (P_B) and its application in the universal set of transition kernels (U). Sections 4.2 and 4.3 further build upon this by demonstrating methods to enhance policy performance.\n\n\n> Q: The experimental results lack persuasiveness. Firstly, the authors do not compare their work with state-of-the-art algorithms such as SAC-N, EDAC [1], and LB-SAC [2]. Even when implementing the algorithms based on https://github.com/tinkoff-ai/CORL, this paper's results are inferior to previous methods in most Mujoco and Adroit environments. Hence, I believe there are significant limitations in the paper's empirical contributions.\n\nA: SAC-N, EDAC, and LB-SAC are all ensemble-based algorithms where the ensemble sizes are 500, 50, and 50, respectively. These ensemble-based algorithms often require extensive computational resources due to their reliance on large ensembles for uncertainty estimation (Nikulin et al., 2022). We did not include these algorithms in the comparative study because our method is not only model-free but also ensemble-free.\n\n> Q: Figure 2 should clarify the color codes representing different algorithms and indicate the corresponding perturbation magnitudes. The paper should explain how optimal state perturbations were determined in the robustness analysis. The connection between the state perturbations in the experiments and the real-world environments, as claimed in the paper, seems tenuous. I have reservations about the practical significance of this paper.\n\nA: In the caption of Figure 2, we have clarified \u201cThe blue and red solid lines depict the average performances of RAORL and ReBrac, respectively, in the presence of state perturbations\u201d. \n\nIn offline reinforcement learning (RL), a common and practical challenge arises when data collected from one system (Machine A) is used to train an agent that will be deployed on a different but similar system (Machine B). Even minor differences between these two machines can lead to distinct Markov Decision Processes (MDPs), posing a significant challenge in terms of MDP generalization. This situation underscores the importance of developing RL agents that can generalize effectively across varying MDPs. In essence, the agent must be capable of adapting to the nuances and potential discrepancies between the training environment (Machine A) and the deployment environment (Machine B).  The experiments in Figure 2 simulate this situation.\n\n> Q: While the paper focuses on model-free offline RL, I find it puzzling that model-based offline RL is included in the related work. Robust RL works like [3, 4] (and others) dealing with state adversaries seem more relevant, yet the authors do not discuss them in the related work section.\n\nA: The inclusion of model-based offline RL in the related work is to present a comprehensive background, as it also addresses offline scenarios. We acknowledge the relevance of robust RL literature, particularly works that handle state adversaries. We will incorporate references [3, 4] into our discussion on robust reinforcement learning in Section 2.3 to provide a more complete overview of the field and its developments.\n\n> Q: How does RAORL effectively bridge the performance gap in real-world applications, as claimed in the introduction?\n\nA: RAORL utilizes a strategic training approach that involves the uncertainty set of an offline dataset, as defined in Definition 4 of our paper. This approach ensures that RAORL is not confined to the narrow bounds of the data it was trained on but instead has exposure to a wider array of potential scenarios within its uncertainty set. Moreover, Theorem 3 in our paper underpins this methodology with a solid theoretical foundation, demonstrating how training within this uncertainty set can reduce the discrepancy between the policy's performance in the training data and its expected performance in real-world conditions. \n\n> Q: How to determine the attack budget () in RAORL?\n\nA: We set perturbation radius from the set {0.03, 0.05, 0.08, 0.1} multiplied by state differences (in Section 4.4). The setting of attack budgets were chosen to reflect the mean magnitude of the state affected by actions taken in each environment."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577053695,
                "cdate": 1700577053695,
                "tmdate": 1700636437042,
                "mdate": 1700636437042,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RuaUCJt90n",
                "forum": "P895PSh41Z",
                "replyto": "0Gr01GvZWg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "> Q: I am seeking clarity regarding the primary advantage of RAORL. Is it intended to be more effective than other offline model-free algorithms, aimed at improving adversarial robustness, or targeting enhanced offline performance? While the paper mentions these advantages, the results do not strongly demonstrate superior performance or robustness with enough evidence and explanation.\n\nA: The primary objective of RAORL is to offer an effective and efficient solution in the realm of offline model-free algorithms.  It significantly reduces the computational and memory overhead typically associated with ensemble methods. In terms of performance and robustness, RAORL is designed to be competitive with other state-of-the-art offline model-free algorithms."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577094880,
                "cdate": 1700577094880,
                "tmdate": 1700577094880,
                "mdate": 1700577094880,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VjFgPqQoLi",
                "forum": "P895PSh41Z",
                "replyto": "RuaUCJt90n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1710/Reviewer_8FWn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1710/Reviewer_8FWn"
                ],
                "content": {
                    "title": {
                        "value": "Thanks to the response"
                    },
                    "comment": {
                        "value": "I appreciate the response from the authors, which has led to improvements in the presentations of this paper. While I still have some concerns regarding the motivation and novelty, I have decided to raise my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661601736,
                "cdate": 1700661601736,
                "tmdate": 1700661601736,
                "mdate": 1700661601736,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5RRNDd3ink",
            "forum": "P895PSh41Z",
            "replyto": "P895PSh41Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1710/Reviewer_YpUx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1710/Reviewer_YpUx"
            ],
            "content": {
                "summary": {
                    "value": "The paper takes an adversarial approach to offline RL in order to account for the ambiguity in transition dynamics that arises from limited samples. The authors present theory outlining performance guarantees for a \u201crisk-aware\u201d policy which optimizes the average performance across an uncertainty set of plausible transition dynamics which have low TV distance to the empirical distribution. They propose to build this uncertainty set by constructing dynamics which are perturbed to be pessimistic with respect to the policy\u2019s value function. The practical implementation of this pessimism can be achieved in continuous state spaces using the Fast Gradient Sign Method to compute adversarial perturbations. The resulting algorithm, Relaxed State-Adversarial Offline RL (RAORL), is evaluated on the D4RL benchmark, where it achieves strong results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The algorithm adapts a previous online robust RL algorithm, Relaxed State-Adversarial Policy Optimization (RAPPO), to the offline setting.\n* The paper presents theoretical guarantees for an idealized version of the algorithm.\n* The performance of RAORL on D4RL is strong compared to prior methods, and the state-adversarial training improves (or doesn\u2019t hurt) performance compared to the base algorithm, ReBrac. The gain over ReBrac is most apparent on the AntMaze tasks."
                },
                "weaknesses": {
                    "value": "* As far as I can tell, the RAORL algorithm is not substantially different from RAPPO, other than that (i) it is applied offline, (ii) a different solver is used (ReBrac vs. PPO), although this is necessitated by (i), and (iii) the relaxation parameter $\\alpha$ is tuned as a hyperparameter rather than being updated as part of the algorithm. It would be helpful for the reader if you describe the distinction more clearly.\n* Similarities between this paper\u2019s theoretical results and those of the MoREL paper (Kidambi et al. 2020) are not highlighted. For example, Lemma 1 here looks nearly identical to Theorem 1 in that paper, with the addition of expectation over the uncertainty set $\\mathcal{U}$. It would be helpful for the reader if you include some comments on the similarities/differences, perhaps in an appendix.\n* Although it is repeatedly mentioned that one of the primary motivations for avoiding a model-based algorithm is that they struggle with stochastic environments, all the experiments are conducted on deterministic tasks.\n* I felt that the self-promotional language should be toned down significantly in the writing of this paper (e.g. \u201cA Leap Towards\u201d, \u201cinnovative\u201d, \u201ctop contender\u201d, \u201csuperior\u201d, \u201cimpressive\u201d, etc.). I understand that some degree of salesmanship is standard in today\u2019s research environment, but in my opinion it is more appropriate for Twitter than in the paper itself."
                },
                "questions": {
                    "value": "* Am I missing other differences between RAORL and RAPPO?\n* In Definition 1, is the expectation over $s,a$ taken over all transitions in the offline dataset? (I think the answer is yes, but wanted to confirm. The notation could be modified to clarify this.)\n* In Definition 3, do the transition probabilities need to be re-normalized if the argmin defining $Z^\\pi_\\sigma$ contains more than one state? If we are assuming the argmin is always unique, perhaps this should be noted in the paper.\n* The definition of $p_r$ (in Lemma 3 in Appendix A.2) is given as \u201cthe probabitlity [sic] of $P \\in \\mathcal{U}_r$ for every $P \\in \\mathcal{U}$\u201d. I am not sure how to interpret this?\n* Could you describe more precisely how the perturbations were chosen in section 5.3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1710/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820414808,
            "cdate": 1698820414808,
            "tmdate": 1699636099413,
            "mdate": 1699636099413,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MctdLHw6oB",
                "forum": "P895PSh41Z",
                "replyto": "5RRNDd3ink",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank reviewer YpUx for the insightful comments. Below, we address the questions raised by the reviewer. We hope the replies could help the reviewer further recognize our contributions. Thank you.\n\n\n> Q: As far as I can tell, the RAORL algorithm is not substantially different from RAPPO, other than that (i) it is applied offline, (ii) a different solver is used (ReBrac vs. PPO), although this is necessitated by (i), and (iii) the relaxation parameter is tuned as a hyperparameter rather than being updated as part of the algorithm. It would be helpful for the reader if you describe the distinction more clearly.\n\nA: RAORL is tailored for offline RL, focusing on adaptability across multiple domains using static data, training within State-Adversarial uncertainty sets to ensure robust performance across a range of MDPs. Theorem 3 in our work theoretically validates the efficacy of RAORL in these offline scenarios. On the other hand, RAPPO is concerned with online settings, aiming to enhance both average and worst-case performance concurrently. These distinct goals highlight the fundamental differences in the application and contribution of RAORL compared to RAPPO.\n\n\n> Q: Similarities between this paper\u2019s theoretical results and those of the MoREL paper (Kidambi et al. 2020) are not highlighted. For example, Lemma 1 here looks nearly identical to Theorem 1 in that paper, with the addition of expectation over the uncertainty set.  It would be helpful for the reader if you include some comments on the similarities/differences, perhaps in an appendix.\n\nA: Our Lemma 1 and MoREL's Theorem 1 differ primarily in their treatment of transition probabilities. MoREL assumes known transition probabilities, using them to create a pessimistic MDP. In Theorem 1, MoREL work with the constraint that the Total Variation Distance (DTV) between the model transition probability $\\hat{P}(\\cdot|s,a)$ and the true probability $P(\\cdot|s,a)$ is less than or equal to $\\alpha$. However, in practice, $P(\\cdot|s,a)$ is unknown since we only have access to the transition kernel derived from the offline dataset. In contrast, RAORL operates without this assumption, reflecting real-world scenarios where true dynamics are often unknown. \n\n\n> Q: In Definition 1, is the expectation over taken over all transitions in the offline dataset? (I think the answer is yes, but wanted to confirm. The notation could be modified to clarify this.)\n\nA: In Definition 1, the expectation indeed goes beyond the transitions present in the offline dataset. It encompasses all possible transition kernels within a universal uncertainty set, representing potential dynamics in real-world environments. This approach extends our analysis beyond limited empirical data, preparing the model for a broader range of scenarios.\nHowever, it's important to note that while the concept of a universal uncertainty set might seem overly extensive, in practice, we refine this to a more manageable subset, denoted as $U_r$ in section 4.2 and $U_{\\epsilon}$  in section 4.3. This refinement process involves constraining the uncertainty set based on realistic assumptions and characteristics observed in the offline data, thereby making the set both practical and relevant to real-world applications. \n\n\n> Q: In Definition 3, do the transition probabilities need to be re-normalized if the argmin defining contains more than one state? If we are assuming the argmin is always unique, perhaps this should be noted in the paper.\n\nA: The problem can be easily solved by setting a rule, such as picking the state that has the smallest value in the first dimension if there are multiple choices. In our implementation, the worst state is determined using the FGSM method. Although there can be multiple states that are equally worse, the FGSM finds only one of them. We will clarify this issue in the revision. \n\n> Q: The definition of  (in Lemma 3 in Appendix A.2) is given as \u201cthe probabitlity [sic] of  for every \u2208\u201d. I am not sure how to interpret this?\n\nA: In Lemma 3, $ p_r$ represents the likelihood of each possible transition kernel $ P $ being within a robust subset $U_r $, which is taken into account by the robust policy $\\pi_r$.\n\n> Q: Could you describe more precisely how the perturbations were chosen in section 5.3?\n\nA: We defined the perturbation magnitude linearly with 20 points in the range $[0, 0.1]$. A perturbation of 0 signifies no perturbation, while a perturbation of 0.1 implies a perturbation strength of $0.1 \\times | \\text{ActualNextState} -  \\text{State}|$, thereby ensuring that the norm of the difference between the actual state and the perturbed state is at most $0.1 \\times  | \\text{ActualNextState} -  \\text{State}|$.\n\nAs depicted in Figure 2, we analyzed the effect of different perturbation magnitudes on agent performance. The results demonstrate that RAORL maintains robust performance even under significant state perturbations."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575013033,
                "cdate": 1700575013033,
                "tmdate": 1700575013033,
                "mdate": 1700575013033,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "czCRiVvug7",
                "forum": "P895PSh41Z",
                "replyto": "MctdLHw6oB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1710/Reviewer_YpUx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1710/Reviewer_YpUx"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for your response. My questions are mostly answered. However my first question was more asking about the *algorithmic* differences, rather than differences in setting. Perhaps more directly, how is RAORL different from RAPPO if you were to apply both to a static offline dataset?\n\nAlso, I disagree that \"MoREL assumes known transition probabilities\". It is estimating the dynamics from data. The divergence between model probabilities and true probabilities can be bounded using standard concentration inequalities."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691192183,
                "cdate": 1700691192183,
                "tmdate": 1700691192183,
                "mdate": 1700691192183,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]