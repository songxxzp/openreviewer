[
    {
        "title": "How Well Do Supervised Models Transfer to 3D Image Segmentation?"
    },
    {
        "review": {
            "id": "DrODpbuLWX",
            "forum": "AhizIPytk4",
            "replyto": "AhizIPytk4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission742/Reviewer_osvs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission742/Reviewer_osvs"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel dataset (IMAGENETCT-9K) containing 9,262 CT volumes along with their respective voxel-level masks. Furthermore, the paper pretrains various models on this dataset and fine-tunes it on other publicly available benchmarks, achieving SOTA performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The dataset appears to be comprehensive and holds great promise. I believe that making this dataset and the pretrained weights publicly available can contribute to advancements in the field."
                },
                "weaknesses": {
                    "value": "The weakness could relate to the details of the pretraining strategy. Typically, image-wise [1, 2] or pixel-wise [3] pre-training relies on the InfoNCE loss for clustering embedding samples in the latent space, rather than directly applying penalties based on labels via cross-entropy loss. It would be more interesting to observe results achieved through category-guided InfoNCE loss [4, 5] pre-training using this dataset.\n\nAdditionally, there exist many promising domain transfer methods, yet the paper appears to lack exploration in this area. The current approach appears to be straightforward fine-tuning on other datasets, such as TotalSegmentator and JHH.\n\n[1] Self-training with Noisy Student improves ImageNet classification\n\n[2] Unsupervised Learning of Visual Features by Contrasting Cluster Assignments\n\n[3] Dense Contrastive Learning for Self-Supervised Visual Pre-Training\n\n[4] Supervised Contrastive Learning\n\n[5] Exploring Cross-Image Pixel Contrast for Semantic Segmentation"
                },
                "questions": {
                    "value": "I don't have a lot questions since the paper's primary contribution lies in the dataset. I would like to confirm whether the dataset and the pre-trained weights will be made accessible to the public."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I missed the data privacy of the patients throughout the processes of data collection, storage, and sharing in the paper. I have observed a 'pending' status in Table 5 of your Appendix A and I believe it is essential for the authors to address this issue appropriately."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission742/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission742/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission742/Reviewer_osvs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission742/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698502902760,
            "cdate": 1698502902760,
            "tmdate": 1699636001221,
            "mdate": 1699636001221,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VuTRTGEVDL",
                "forum": "AhizIPytk4",
                "replyto": "DrODpbuLWX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission742/Reviewer_osvs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission742/Reviewer_osvs"
                ],
                "content": {
                    "comment": {
                        "value": "My questions are well solved."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632395514,
                "cdate": 1700632395514,
                "tmdate": 1700632395514,
                "mdate": 1700632395514,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ec5b1D3Wxa",
                "forum": "AhizIPytk4",
                "replyto": "DrODpbuLWX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer osvs (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for acknowledging our general response. We appreciate your thoughtful feedback and accolades on our contribution: *\u201cthe dataset appears to be comprehensive and holds great promise\u2026making this dataset and the pretrained weights publicly available can contribute to advancements in the field.\u201d*\n\n---\n\n> **Q1.** The weakness could relate to the details of the pretraining strategy. Typically, image-wise [1, 2] or pixel-wise [3] pre-training relies on the InfoNCE loss for clustering embedding samples in the latent space, rather than directly applying penalties based on labels via cross-entropy loss. It would be more interesting to observe results achieved through category-guided InfoNCE loss [4, 5] pre-training using this dataset.\n\nThanks for your suggestion. We must admit that category-guided InfoNCE loss was not our first try by default because it is not a standard approach for medical segmentation tasks as of now. [[Ma et al., MEDIA 2021](https://www.sciencedirect.com/science/article/pii/S1361841521000815)] have listed all the popular losses for medical segmentation tasks, each of these loss functions may have their own strengths to specific scenarios, such as dealing with unbalanced class. Therefore, we initially selected a combination of Dice loss and binary cross-entropy loss, the most commonly used segmentation loss, as recommended by the introduction and Figure 1 in [[Ma et al., MEDIA 2021](https://www.sciencedirect.com/science/article/pii/S1361841521000815)].\n\nBut we are very much interested in exploring more loss options. Category-guided InfoNCE loss, as you suggested, is potentially good because it can cluster embedding samples in the latent space and has proven effective for supervised learning in natural image classification [4] and segmentation tasks [5]. Thanks for sharing the references. The references [1, 2, 3], however, discussed self-supervised learning, so their applicability and effectiveness in the context of supervised learning for segmentation tasks might not have been thoroughly explored. Following your suggestion, a comprehensive comparison between InfoNCE loss and our dice + bce loss will be included in our final version.\n\n---\n\n> **Q2.** Additionally, there exist many promising domain transfer methods, yet the paper appears to lack exploration in this area. The current approach appears to be straightforward fine-tuning on other datasets, such as TotalSegmentator and JHH.\n\nThe current approach is pretty robust (evidenced in Table 3) because our dataset covers a variety of domains (i.e., 68 hospitals with different scanners and protocols). Therefore, models pre-trained on this dataset are expected to be generalizable for novel domains, e.g., TotalSegmentator, FLARE\u201923, and JHH. These three datasets are completely unseen during the pre-training and represent a variety of diversity. Specifically, the TotalSegmentator dataset represents for the Central European population from Switzerland, the FLARE\u201923 dataset represents for East Asian population from China, and the JHH dataset represents for another population (anonymous for peer review). Our models achieve comparable or even superior performance to the IID counterparts (Table 3). Therefore, domain transfer becomes less important if the model is pre-trained on large and diverse datasets (elaborated in the next two points).\n\n1. The domain transfer problem could be solved by methodology innovation, as you suggested, and also by training AI models on enormous datasets. This point has been more clear recently demonstrated by large language models (GPT) and vision foundation models (SAM), which show incredible performance in \u201cnew domain\u201d. However, this achievement may not be directly attributed to method-driven solutions for domain transfer, but simply because the AI might have been trained on similar sentences or images. This was also pointed out by Yann Lecun\u2014*\u201c[beware of testing on the training set](https://twitter.com/ylecun/status/1723752958037315874)\u201d*\u2014in response to the incredible results achieved by GPT.\n\n2. In some sense, our paper explores dataset-driven solutions for domain transfer. The robust performance of our models when direct inference on multiple domains could also be attributed to our large-scale, fully-annotated medical dataset\u2014as one of our major contributions. The release of this dataset can foster AI models that are more robust than the majority of existing models that are only trained on a few hundred CT volumes from limited domains. We completely agree that existing domain transfer methods could be supplemented with direct inference and fine-tuning to further improve AI performance.\n\n---\n\n**Reference**\n\n- Ma, Jun, Jianan Chen, Matthew Ng, Rui Huang, Yu Li, Chen Li, Xiaoping Yang, and Anne L. Martel. \"Loss odyssey in medical image segmentation.\" *Medical Image Analysis* (2021)."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635977072,
                "cdate": 1700635977072,
                "tmdate": 1700732250944,
                "mdate": 1700732250944,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "baHWrchN6k",
                "forum": "AhizIPytk4",
                "replyto": "DrODpbuLWX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer osvs (2/2)"
                    },
                    "comment": {
                        "value": "> **Q3.** I would like to confirm whether the dataset and the pre-trained weights will be made accessible to the public.\n\nWe confirm that the dataset, the pre-trained weights, and the source code will be made accessible to the public upon the acceptance of the paper. Pre-trained weights have already been pre-leased for peer review and public audience through the links in the general responses. Feel free to test it out. And we have already attached our source code to the supplementary material. The dataset, however, cannot be pre-leased because we are in the process of licensing our ImageNetCT-9K.\n\n---\n\n> **Q4.** I missed the data privacy of the patients throughout the processes of data collection, storage, and sharing in the paper. I have observed a 'pending' status in Table 5 of your Appendix A and I believe it is essential for the authors to address this issue appropriately.\n\nEnsuring the data privacy of the patients is our top priority. We collected over 9K CT volumes from publicly available datasets (Table 1 and Appendix B.1 Table 5) and annotated 257K organ/tumor masks in addition to what public datasets already provided. The total size of our ImageNetCT-9K is around 500 GB. We commit to releasing the entire dataset to the public upon the acceptance of the paper. Currently, ImageNetCT-9K is in the process of acquiring a proper license, so we put a \u201cpending\u201d status for now."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638202571,
                "cdate": 1700638202571,
                "tmdate": 1700638202571,
                "mdate": 1700638202571,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "N3RHFQ9taA",
            "forum": "AhizIPytk4",
            "replyto": "AhizIPytk4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission742/Reviewer_mSzX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission742/Reviewer_mSzX"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the transfer learning ability of self-supervised and fully-supervised foundational models for medical image segmentation. For this purpose, the authors collect a very large, labeled 3D CT scans to be used for pre-trainining. The results show that supervised models have better transfer learning ability compared to self-supervised counterparts, by also saving a significant GPU time. Moreover, since the collected data is very diverse and large, it generalizes well to OOD dataset even surpasses the performance of the models trained on the OOD datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Lack of large annotated datasets is a huge problem in medical imaging due to the cost of collecting labelled data. Publicly available pretrained models on such large datasets are huge assets for medical imaging. \n\n- The paper presents extensive experiments showing the benefit of supervised pre-training compared to the unsupervised counterparts."
                },
                "weaknesses": {
                    "value": "- UniverSeg [1] is another paper that trains networks on a very large dataset, 22K scans, which is even larger than this paper. Although Arxiv version of UniverSeg is available since April 2023, I see this work and UniverSeg as concurrent works since UniverSeg is recently presented in ICCV. However, I still think that mentioning UniverSeg and discussing the similarities/differences in the final version would be useful.\n\n[1] https://universeg.csail.mit.edu/\n\n- How do the models trained on the collected large CT dataset generalize to novel modalities such as MRI?"
                },
                "questions": {
                    "value": "Please address my concerns in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission742/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698872212794,
            "cdate": 1698872212794,
            "tmdate": 1699636001154,
            "mdate": 1699636001154,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xZ4p8YHvCS",
                "forum": "AhizIPytk4",
                "replyto": "N3RHFQ9taA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mSzX"
                    },
                    "comment": {
                        "value": "We appreciate the acknowledgment of the scientific impact of our ImageNetCT-9K as *\u201clack of large annotated datasets is a huge problem in medical imaging\u2026\u201d*. Thank you so much for praising our pre-trained models as *\u201chuge assets for medical imaging\u201d*.\n\n---\n\n> **Q1.** UniverSeg [1] is another paper that trains networks on a very large dataset, 22K scans, which is even larger than this paper. Although Arxiv version of UniverSeg is available since April 2023, I see this work and UniverSeg as concurrent works since UniverSeg is recently presented in ICCV. However, I still think that mentioning UniverSeg and discussing the similarities/differences in the final version would be useful.\n\nThank you for sharing UniverSeg with us\u2014its relevance to our work is significant and appreciated. We have now included a discussion of UniverSeg, along with other concurrent and related works, in the revised Section 2. These works share a level of **similarity** with ours, aiming to develop medical AI models that can segment multiple anatomical structures and generalize to out-of-distribution datasets.\n\n**Opinions on UniverSeg.** UniverSeg demonstrates an impressive ability to generalize across various unseen tasks and modalities without the need for fine-tuning, offering computational efficiency. However, its current performance falls short of the benchmarks set by fine-tuned models (as their upper bound reference). **In contrast**, our research aims to advance the fine-tuning performance by pre-training models on larger, per-voxel annotated datasets. We believe that our ImageNetCT-9K dataset could also further enhance UniverSeg's generalization ability.\n\n**Perspectives on additional related and future work** The forthcoming public release of ImageNetCT-9K and SuPreM is anticipated to positively influence Segment Anything Models (SAM) within the medical field. We hold the same belief with [[Kirillov et al. ICCV 2023](https://openaccess.thecvf.com/content/ICCV2023/papers/Kirillov_Segment_Anything_ICCV_2023_paper.pdf)] that the foundation models for image segmentation require supervised training on board data at scale, and we both contribute to a large-scale annotated dataset in the respective field. In addition, we already have some proof of concept that the model tends to understand **objectness** in a broader sense through full supervision in Table 4.\n\n---\n\n> **Q2.** How do the models trained on the collected large CT dataset generalize to novel modalities such as MRI?\n\nThis is a very good point. We think transfer learning across different imaging modalities, such as from CT to MRI, might be less effective compared to transfers within the same modality, primarily due to the significant differences in their imaging techniques. The discrepancies in image acquisition methods between CT and MRI result in distinct intensity values and ranges. Nonetheless, our pre-trained model could still be valuable for abdominal MRI applications. This is because the underlying anatomical structures remain consistent across both CT and MRI, allowing for the potential transfer of shared knowledge.\n\nGiven the constraints of time, we are unable to include this specific experiment in the rebuttal period. However, we plan to incorporate a study focused on **abdominal MRI tasks** in the final version of our paper. With the release of our ImageNetCT-9K and SuPreM, we look forward to promoting a collaborative effort to thoroughly evaluate the capabilities of transfer learning. This includes exploring a wider range of medical modalities, such as T1, T1c, T2, Flair, and Ultrasound, and extending into general 3D vision tasks involving diverse data formats like point clouds, voxel occupancy grids, meshes, and implicit surface models (e.g., signed distance functions). We have now included in the future work section and Appendix F.4.\n\n---\n\n**Reference**\n- Kirillov, Alexander, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao et al. \"Segment anything.\" *ICCV* (2023)."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634520827,
                "cdate": 1700634520827,
                "tmdate": 1700634520827,
                "mdate": 1700634520827,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nEc7rLsPrs",
                "forum": "AhizIPytk4",
                "replyto": "xZ4p8YHvCS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission742/Reviewer_mSzX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission742/Reviewer_mSzX"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for the well-prepared rebuttal. I am still suggesting acceptance of this paper."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639591558,
                "cdate": 1700639591558,
                "tmdate": 1700639591558,
                "mdate": 1700639591558,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rYEpxAPsYf",
            "forum": "AhizIPytk4",
            "replyto": "AhizIPytk4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission742/Reviewer_bY7Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission742/Reviewer_bY7Q"
            ],
            "content": {
                "summary": {
                    "value": "The authors collected publicly available and private CT data, and obtained over 9000 CT data cases by manually correcting pseudo-labels. These data can support the segmentation of 32 organs and a small number of tumors. Through experiments, it was discovered that the supervised pre-training method outperforms self-supervised pre-training and supervised pre-training with fewer samples."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors collected over 9000 cases of publicly available and private 3D CT datasets, and manually corrected annotation errors, making it the largest dataset for multi-organ segmentation currently available."
                },
                "weaknesses": {
                    "value": "1. The paper explores the transferability of supervised learning by combining multiple 3D CT segmentation datasets. Similar work has been done in various fields, such as training various tasks and data in computer vision, which has shown improved results across tasks. This paper incorporates 3D CT data and tasks, with the only difference being the collection of more publicly available and private data, without bringing new insights or technological innovations to the community.\n2. The conclusion that supervised pre-training has an advantage over other pre-training methods in 3D medical imaging is generalized to general 3D vision tasks in an inconsistent manner. The introduction discusses pre-training strategies for 3D vision tasks, but the experiments are all conducted on medical images. It is well known that there are significant differences between medical images and natural images, and whether the experimental conclusions on 3D CT data can be extended to other 3D vision tasks is not explored in the paper.\n3. The fused dataset of over 9000 CT cases collected by the authors includes segmentation of 32 organs and tumors, but the evaluation in the experimental section focuses more on organs, lacking an evaluation of tumor segmentation performance. Comparatively, organ segmentation is less challenging in terms of generalization, while tumor segmentation is more complex. In the external dataset, more focus should be given to tumor segmentation, as it is more susceptible to a series of generalization issues caused by differences in populations, devices, and diseases in practical application scenarios.\n4. Unfair comparison in the experimental section is a fatal flaw. Although the authors claim that collecting 9000 data is their contribution, the same 9000 data were not used for pre-training when comparing with other self-supervised/supervised pre-training methods. Therefore, it is not rigorous to conclude that SPT is superior to other supervised/self-supervised methods.\n5. The results of fine-tuning SPT on 63 novel classes are not impressive. Although there is no comparison with totalsegmentator, the performance of totalsegmentator trained on 1000 data seems to surpass what is reported in Table 4. For example, totalsegmentator can achieve over 95% Dice on iliopsoas, while it is less than 90% in the paper."
                },
                "questions": {
                    "value": "1. The paper does not clearly explain how the three expert radiologists collaborated to clean the data, including how they worked together, established uniform standards, and how they corrected tumor masks in the pseudo labels. What was the time cost involved, and so on?\n2. The \"novel datasets\" claimed in Table 3 is inappropriate. It should be referred to as the external dataset."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This paper uses internal medical data for training, so it may need ethical approval for use."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission742/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission742/Reviewer_bY7Q",
                        "ICLR.cc/2024/Conference/Submission742/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission742/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698921491329,
            "cdate": 1698921491329,
            "tmdate": 1700749072834,
            "mdate": 1700749072834,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X5ym5cloo3",
                "forum": "AhizIPytk4",
                "replyto": "rYEpxAPsYf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bY7Q (1/6)"
                    },
                    "comment": {
                        "value": "We would like to thank you for your diligent efforts and constructive suggestions on our paper, which have helped us think more deeply. In the following, we have provided a point-by-point response to all questions raised.\n\n---\n\n> **Q1.** The paper explores the transferability of supervised learning by combining multiple 3D CT segmentation datasets. Similar work has been done in various fields, such as training various tasks and data in computer vision, which has shown improved results across tasks. This paper incorporates 3D CT data and tasks, with the only difference being the collection of more publicly available and private data, without bringing new insights or technological innovations to the community. \n\nCreating large-scale (9K) per-voxel annotated medical datasets of over 25 classes and making this resource publicly available takes a village. Your acknowledgment of our dataset as *\u201cthe largest dataset for multi-organ segmentation currently available\u201d* is greatly appreciated. While it is common in computer vision to improve performance through joint training on a combination of existing datasets, our study overcomes technical barriers and brings new insights as follows.\n\n1. ImageNetCT-9K is NOT a simple combination of existing datasets. We have now included Appendix B.1 Figure 5, in conjunction with Table 5, to better illustrate the evolution from public datasets to our ImageNetCT-9K. The 9K CT volumes in the combination of public datasets only contain a total of **39K** annotated organ masks, while our ImageNetCT-9K provides **296K** annotated organ/tumor masks for these CT volumes, substantially increasing the number of masks by **7.6** times.\n\n2. Creating **296K** high-quality organ/tumor masks for 9K CT volumes requires extensive medical knowledge and annotation cost (much more difficult than annotating natural images). Based on our experience and those reported in [[Park et al., Diagnostic and interventional imaging 2020](https://pubmed.ncbi.nlm.nih.gov/31358460/)], trained radiologists annotate abdominal organs at a rate of 30\u201360 minutes per organ per three-dimensional CT volume. This translates to **247K** human hours for completing ImageNetCT-9K. We employed a highly efficient annotation method, combining AI with the expertise of three radiologists using active learning (details in **Q6**), to overcome this challenge and produce the largest annotated dataset to date.\n\n3. ImageNetCT-9K can be used as a training resource and a testbed for AI algorithms. As acknowledged by Reviewer fMfJ, it can *\u201chave a good impact on the whole community.\u201d* Our study introduces **new insights** on the use of ImageNetCT-9K in transfer learning. We delve into a key debate within general computer vision: the comparative effectiveness of representation learning via human-annotated data (supervised pre-training) versus raw data (self-supervised pre-training). As Reviewer fMfJ acknowledged, *\u201dthis paper concluded on the debate of whether self-supervised or supervised pre-training lead to better performance and data efficiency. This debate **had not be resolved** without the invention of a fully-annotation dataset of such scale.\u201d* We are committed to making ImageNetCT-9K publicly available, thereby enabling further research to derive insights from various angles and foster technological innovations. Reviewer mSzX recognized that *\u201dpublicly available pretrained models on such large datasets are huge assets for medical imaging.\u201d*\n\n\n---\n\n**Reference**\n\n- Park, S., L. C. Chu, E. K. Fishman, A. L. Yuille, B. Vogelstein, K. W. Kinzler, K. M. Horton et al. \"Annotated normal CT data of the abdomen for deep learning: Challenges and strategies for implementation.\" *Diagnostic and interventional imaging* (2020)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624738643,
                "cdate": 1700624738643,
                "tmdate": 1700625155815,
                "mdate": 1700625155815,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hkXM7qvE6E",
                "forum": "AhizIPytk4",
                "replyto": "rYEpxAPsYf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bY7Q (2/6)"
                    },
                    "comment": {
                        "value": "> **Q2.** The conclusion that supervised pre-training has an advantage over other pre-training methods in 3D medical imaging is generalized to general 3D vision tasks in an inconsistent manner. The introduction discusses pre-training strategies for 3D vision tasks, but the experiments are all conducted on medical images. It is well known that there are significant differences between medical images and natural images, and whether the experimental conclusions on 3D CT data can be extended to other 3D vision tasks is not explored in the paper.\n\nWe completely agree with your comments and have now revised our title, abstract, introduction, and conclusion to narrow down our current research scope to 3D medical imaging. \n\nThe potential applications and implications of our dataset have now been included in the future work section. Given that our dataset includes detailed per-voxel annotations for 25 organs and tumors, it enables the automatic generation of 3D shape representations. These representations can be formatted as point clouds, voxel occupancy grids, meshes, and implicit surface models (e.g., signed distance functions), each catering to different algorithmic needs. We anticipate our dataset could be useful for a variety of other 3D medical vision tasks [[Li et al., 2023](https://arxiv.org/abs/2308.16139)], such as pose estimation, surface reconstruction, depth estimation, etc. \n\n---\n\n> **Q3.** The fused dataset of over 9000 CT cases collected by the authors includes segmentation of 32 organs and tumors, but the evaluation in the experimental section focuses more on organs, lacking an evaluation of tumor segmentation performance. Comparatively, organ segmentation is less challenging in terms of generalization, while tumor segmentation is more complex. In the external dataset, more focus should be given to tumor segmentation, as it is more susceptible to a series of generalization issues caused by differences in populations, devices, and diseases in practical application scenarios.\n\n**Tumor-related tasks** were evaluated in Figure 1. Due to space constraints and the breadth of our experiments, we initially averaged the performance metrics for both tumor and organ segmentation in Figure 1, for which we apologize if this led to any confusion. We completely agree with you that evaluating tumor-related tasks is much more significant and challenging than organ tasks. We have now explicitly reported tumor segmentation performance in Table 4 and tumor classification performance in Figure 3. \n\nWe would like to stress the challenges in benchmarking tumor segmentation/classification, particularly due to the scarcity of annotations in publicly available datasets (often limited to hundreds of tumors). To overcome this limitation, we employed our proprietary dataset, which comprises **3,577** annotated pancreatic tumors, including detailed sub-types: **1,704 PDACs**, **945 Cysts**, and **928 PanNets**. This extensive dataset enabled us to thoroughly assess the transfer learning ability of our pre-trained models in tumor-related tasks. Notably, the transfer learning results detailed in Appendix E.4 Figure 13 demonstrate a sensitivity of 86.1% and specificity of 95.4% for PDAC detection. This performance surpasses the average radiologist's performance in PDAC identification by 27.6% in sensitivity and 4.4% in specificity, as reported in [[Cao et al., Nat Med 2023](https://www.nature.com/articles/s41591-023-02640-w)]. This is one of the demonstration how our pre-trained models can be deployed for clinical applications. If we have an honor to present this work at ICLR, we can perhaps elaborate more.\n\n**Generalization issues.** Thanks for bringing this into our attention. Through our experiment, we have shown the generalizability in terms of populations in Table 3 (i.e. TotalSegmentator (representing the Central European population from Switzerland) and FLARE\u201923 (the East Asian population from China)). Moreover, our proprietary dataset contains CT scans taken by a variety of vendors, e.g. Siemens, GE, Philips, and Toshiba, as well as scanners 16-/64-slice MDCT and Dual-source MDCT. The promising results in all three external datasets\u2014with various populations, devices, and diseases\u2014suggest the clinical impact of our pre-trained models in practical application scenarios.\n\n---\n\n**Reference**\n\n- Li, Jianning, Antonio Pepe, Christina Gsaxner, Gijs Luijten, Yuan Jin, Narmada Ambigapathy, Enrico Nasca et al. \"MedShapeNet--A Large-Scale Dataset of 3D Medical Shapes for Computer Vision.\" *arXiv* (2023).\n- Cao, Kai, Yingda Xia, Jiawen Yao et al. Large-scale pancreatic cancer detection via non-contrast CT and deep learning. *Nature Medicine* (2023)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625096903,
                "cdate": 1700625096903,
                "tmdate": 1700730978892,
                "mdate": 1700730978892,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "t7oZhWzHav",
                "forum": "AhizIPytk4",
                "replyto": "rYEpxAPsYf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bY7Q (3/6)"
                    },
                    "comment": {
                        "value": "> **Q4.** Unfair comparison in the experimental section is a fatal flaw. Although the authors claim that collecting 9000 data is their contribution, the same 9000 data were not used for pre-training when compared with other self-supervised/supervised pre-training methods. Therefore, it is not rigorous to conclude that SPT is superior to other supervised/self-supervised methods. \n\nThank you for your valuable feedback. Ensuring a fair and rigorous comparison is our top priority. Therefore, we must clarify that while we have developed and released a suite of models pre-trained on 9K data, these models were NOT used for comparisons against other self-supervised/supervised pre-training methods within this paper. The only experiment that used 9K-models was the direct inference on external datasets (Table 3), where we benchmarked against methods trained specifically on those datasets, representing an upper bound in performance. This benchmark is to ensure that the released models are the most effective and robust ones (as we could provide) for the research community to directly use.\n\nFor the other experiments, our aim was to evaluate the efficacy of supervised pre-training relative to other pre-training methods, so we designed them within a controlled setting. \n\n1. For supervised pre-training, the largest study to date was by [[Liu et al., ICCV 2023](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_CLIP-Driven_Universal_Model_for_Organ_Segmentation_and_Tumor_Detection_ICCV_2023_paper.pdf)], which was developed on 3,410 (2,100 for training and 1,310 for validation) annotated CT volumes. For self-supervised pre-training, the largest one was by [[Tang et al., CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.pdf)], which was trained on 5,050 unannotated CT volumes. To do a rigorous comparison, we **benchmarked** with these advanced pre-training methods by pre-training our model using 2,100 CT volumes (*same as Liu et al. and fewer than Tang et al.*) in the Table 2, Figure 1 and Appendix C.2 Figure 8.\n2. We further **scaled down** the number of CT volumes to 21 to explore the edge of our supervised pre-training method. Surprisingly, Figure 2a shows these experiments and demonstrates that the model trained with 21 CT volumes, 672 masks, and 40 GPU hours shows a transfer learning ability similar to [[Tang et al., CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.pdf)] trained with 5,050 CT volumes and 1,152 GPU hours.\n3. Lastly, we **scaled up** the SOTA self-supervised method [[Tang et al., CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.pdf)] by pre-training it on the same 9K CT volumes. Under this similar setting, our supervised pre-training 9K model substantially outperforms the SOTA self-supervised 9K-model (see Appendix C2 Table 8). \n\n---\n\n**Reference**\n\n- Liu, Jie, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Bennett A Landman, Yixuan Yuan, Alan Yuille, Yucheng Tang, and Zongwei Zhou. \"Clip-driven universal model for organ segmentation and tumor detection.\" *ICCV* (2023).\n- Tang, Yucheng, Dong Yang, Wenqi Li, Holger R. Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, and Ali Hatamizadeh. \"Self-supervised pre-training of swin transformers for 3d medical image analysis.\" *CVPR* (2022)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630355684,
                "cdate": 1700630355684,
                "tmdate": 1700630355684,
                "mdate": 1700630355684,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fR9c9P1Oon",
                "forum": "AhizIPytk4",
                "replyto": "rYEpxAPsYf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bY7Q (4/6)"
                    },
                    "comment": {
                        "value": "For your convenience, we have summarized all the models we compared in Appendix C.2 Table 8. A concise version is provided as follows, where the name with a star (*) denotes it is **implemented and pre-trained** by us.\n\n- **Self-supervised pre-training**\n\n| name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| backbone &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| params &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| pre-trained data &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| paper &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| \n|  ----  | ----  |  ----  |  ----  |  ----  |  \n| Models Genesis | U-Net | 19.08M | 623  | [Zhou et al.](http://www.cs.toronto.edu/~liang/Publications/ModelsGenesis/MICCAI_2019_Full.pdf) |\n| UniMiSS | U-Net | 19.08M | 5,022 | [Xie et al.](https://link.springer.com/chapter/10.1007/978-3-031-19803-8_33) |\n| NV | Swin UNETR | 62.19M | 5,050  | [Tang et al.](https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.pdf) |\n| NV* | Swin UNETR | 62.19M | 1,000  | [Tang et al.](https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.pdf) |\n| NV* | Swin UNETR | 62.19M | 3,000  | [Tang et al.](https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.pdf) |\n| NV* | Swin UNETR | 62.19M | 5,050  | [Tang et al.](https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.pdf) |\n| NV* | Swin UNETR | 62.19M | 9,262  | [Tang et al.](https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.pdf) |\n\n- **Supervised pre-training**\n\n| name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| backbone &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| params &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| pre-trained data &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| paper &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| \n|  ----  | ----  |  ----  |  ----  |  ----  |  \n| Med3D | U-Net | 19.08M | 1,638 | [Chen et al.](https://arxiv.org/pdf/1904.00625.pdf) | \n| DoDNet | U-Net | 19.08M | 920 | [Zhang et al.](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_DoDNet_Learning_To_Segment_Multi-Organ_and_Tumors_From_Multiple_Partially_CVPR_2021_paper.pdf)  | \n| DoDNet* | U-Net | 19.08M | 920 | [Zhang et al.](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_DoDNet_Learning_To_Segment_Multi-Organ_and_Tumors_From_Multiple_Partially_CVPR_2021_paper.pdf)  | \n| Universal Model | U-Net | 19.08M | 3,410 | [Liu et al.](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_CLIP-Driven_Universal_Model_for_Organ_Segmentation_and_Tumor_Detection_ICCV_2023_paper.pdf)  | \n| Universal Model | Swin UNETR | 62.19M | 3,410 | [Liu et al.](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_CLIP-Driven_Universal_Model_for_Organ_Segmentation_and_Tumor_Detection_ICCV_2023_paper.pdf)  | \n| SuPreM* | U-Net | 19.08M | 9,262  | ours | \n| SuPreM* | Swin UNETR | 62.19M | 9,262  | ours | \n| SuPreM* | Swin UNETR | 62.19M | 21  | ours | \n| SuPreM* | Swin UNETR | 62.19M | 2,100  | ours | \n| SuPreM* | SegResNet | 470.13M | 9,262  | ours | \n\n---\n\n**Reference**\n\n- Tang, Yucheng, Dong Yang, Wenqi Li, Holger R. Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, and Ali Hatamizadeh. \"Self-supervised pre-training of swin transformers for 3d medical image analysis.\" *CVPR* (2022).\n- Zhou, Zongwei, Vatsal Sodha, Md Mahfuzur Rahman Siddiquee, Ruibin Feng, Nima Tajbakhsh, Michael B. Gotway, and Jianming Liang. \"Models genesis: Generic autodidactic models for 3d medical image analysis.\" *MICCAI* (2019).\n- Xie, Yutong, Jianpeng Zhang, Yong Xia, and Qi Wu. \"Unimiss: Universal medical self-supervised learning via breaking dimensionality barrier.\" *ECCV* (2022).\n- Chen, Sihong, Kai Ma, and Yefeng Zheng. \"Med3d: Transfer learning for 3d medical image analysis.\" *arXiv* (2019).\n- Zhang, Jianpeng, Yutong Xie, Yong Xia, and Chunhua Shen. \"DoDNet: Learning to segment multi-organ and tumors from multiple partially labeled datasets.\" *CVPR* (2021).\n- Liu, Jie, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Bennett A Landman, Yixuan Yuan, Alan Yuille, Yucheng Tang, and Zongwei Zhou. \"Clip-driven universal model for organ segmentation and tumor detection.\" *ICCV* (2023)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630788159,
                "cdate": 1700630788159,
                "tmdate": 1700630788159,
                "mdate": 1700630788159,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u4bt3URo2q",
                "forum": "AhizIPytk4",
                "replyto": "rYEpxAPsYf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bY7Q (5/6)"
                    },
                    "comment": {
                        "value": "> **Q5.** The results of fine-tuning SPT on 63 novel classes are not impressive. Although there is no comparison with totalsegmentator, the performance of totalsegmentator trained on 1000 data seems to surpass what is reported in Table 4. For example, totalsegmentator can achieve over 95% Dice on iliopsoas, while it is less than 90% in the paper.\n\nIn TotalSegmentator, the labels were largely generated by a **single** nnU-Net re-trained continually (see Figure 1b in [Wasserthal et al.](https://pubs.rsna.org/doi/full/10.1148/ryai.230024)). Depending solely on nnU-Net could introduce a potential label bias favoring the nnU-Net architecture. This means two points. \n\n1. Their ground truth (revised pseudo labels) could be biased to the nnU-Net architecture. nnU-Net trained and tested on this dataset will achieve an **unreachable** performance as shown in Totalsegmentator [github](https://github.com/wasserth/TotalSegmentator/blob/master/resources/evaluate_results.txt). For example, they report the DSC score of 0.894 for pancreas segmentation, this number has never been reached literature. The MSD top 1 result (**[0.828](https://decathlon-10.grand-challenge.org/evaluation/challenge/leaderboard/)**); TCIA Pancreas-CT Dataset top 1 result (**[0.845](https://paperswithcode.com/sota/pancreas-segmentation-on-tcia-pancreas-ct)**); even the FELIX Project (producing the largest pancreas dataset in USA) only achieves **[0.87](https://www.medrxiv.org/content/10.1101/2022.09.24.22280071v1)** DSC score. None of these advanced benchmarks have achieved 0.894 reported in Totalsegmentator. Therefore, it is sensible to assume the ground truth is biased to the nnU-Net architecture (including iliopsoas that you mentioned and many other classses). This is the reason why we do not make comparisons with Totalsegmentator. \n\n2. Due to the potential label biases, whenever TotalSegmentator is employed for benchmarking, nnU-Net and models building upon nnU-Net would always outperform other segmentation architectures (e.g., UNETR, TransUNet, SwinUNet, etc.). This observation has also been made in several publications that used the TotalSegmentator dataset. For example, Table 3 in [[Huang et al., 2023](https://arxiv.org/abs/2304.06716)] showed that nnFormer, UNETR, and Swin UNETR were all outperformed by nnU-Net and models building upon nnU-Net in TotalSegmentator. More importantly, the average DSC achieved by our model on TotalSegmentator is also much higher than [[Huang et al., 2023](https://arxiv.org/abs/2304.06716)] as compared in the following table.\n\n| method &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  | organ &nbsp; &nbsp; &nbsp; | vertebrae &nbsp; &nbsp; &nbsp; | cardiac &nbsp; &nbsp; &nbsp; | muscle &nbsp; &nbsp; &nbsp; | \n|  ----  | ----  |  ----  |  ----  |  ----  | \n| [Huang et al., 2023](https://arxiv.org/abs/2304.06716) | 89.82 | 90.43 | 90.89 | 88.83 |\n| Ours | **92.09** | **91.29** | **92.21** | **95.40** | \n\nTherefore, we believe that the segmentation results in TotalSegmentator reported in our paper are **compelling** and should be considered a **faithful** benchmark.\n\n---\n\n**Reference**\n\n- Wasserthal, Jakob, Hanns-Christian Breit, Manfred T. Meyer, Maurice Pradella, Daniel Hinck, Alexander W. Sauter, Tobias Heye et al. \"Totalsegmentator: Robust segmentation of 104 anatomic structures in ct images.\" *Radiology: Artificial Intelligence* (2023).\n- Huang, Ziyan, Haoyu Wang, Zhongying Deng, Jin Ye, Yanzhou Su, Hui Sun, Junjun He et al. \"STU-Net: Scalable and Transferable Medical Image Segmentation Models Empowered by Large-Scale Supervised Pre-training.\" *arXiv* (2023)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631020537,
                "cdate": 1700631020537,
                "tmdate": 1700631020537,
                "mdate": 1700631020537,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tzBYyRrmuT",
                "forum": "AhizIPytk4",
                "replyto": "rYEpxAPsYf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bY7Q (6/6)"
                    },
                    "comment": {
                        "value": "> **Q6.** The paper does not clearly explain how the three expert radiologists collaborated to clean the data, including how they worked together, established uniform standards, and how they corrected tumor masks in the pseudo labels. What was the time cost involved, and so on?\n\nThank you very much for the suggestion. We have now enclosed this information in the revised Section 3.1 and Appendix B.3.\n\n**Automated organ annotations.** Our annotation pipeline involved an interactive segmentation approach, an integration of AI algorithms and human expertise, which premises to improve the efficiency while upholding high-quality annotations. *One senior radiologist* revised the annotations predicted by our AI models, and in turn, the AI models improved their predictions by learning from these revised annotations. This interactive process continued to enhance the quality of annotations until no major revision is needed. Subsequently, *five junior radiologists* examine the final visualizations for accuracy (examples of the rendered images are illustrated in Appendix B.3 Figure 7). The junior radiologists were responsible for reviewing the correctness of the annotations and marking the patient ID for any major discrepancies. Such cases are then reviewed by the senior radiologist. Our uniform annotation standards, largely overlapping with those in [[Ma et al., FLARE 2022](https://arxiv.org/abs/2308.05862)], require trained radiologists to spend approximately 30\u201360 minutes annotating each organ in a three-dimensional CT volume. \n\n**Automated (pseudo) tumor annotations.** We have established uniform annotation standards for tumors, with both senior and junior radiologists actively refining and adhering to these guidelines.\n\n- Liver tumors: Liver tumors include primary tumor lesions and metastases in the liver. Annotations should encompass the entire tumor, including any invasive parts, necrosis, hemorrhage, fibrous scars, and calcifications. Healthy areas or unrelated lesions are not included.\n\n- Kidney tumors: Kidney tumors include both benign and malignant tumor lesions growing in the kidneys. The entire tumor and its invasive parts to surrounding areas, plus internal changes like necrosis and calcification, should be annotated. Exclude healthy structures.\n\n- Pancreatic tumors: Pancreatic tumors include all benign and malignant tumor lesions growing in the pancreas. Annotations cover the whole tumor and its invasive growth into adjacent areas, including changes like cysts, necrosis, and calcification. Exclude healthy structures.\n\n- Colon tumors: Colon tumors include all benign and malignant tumor lesions developing from the colon wall. The entire tumor and its invasion into nearby structures, along with internal changes like necrosis, should be annotated, excluding healthy areas.\n\n- Hepatic vessel tumors: Hepatic vessel tumors include all primary tumor lesions developing from the intrahepatic vessel wall and tumor thrombus in intrahepatic vessels. Annotations should include the tumor within the vessels, excluding external parts and unrelated lesions.\n\nOverall, our ImageNetCT-9K dataset offers **51.8K** pseudotumor masks visually inspected by radiologists, though without biopsy confirmation. While these masks lack pathological validation, we anticipate they will serve as a valuable foundation for expanding precise tumor annotations in future research.\n\n---\n\n> **Q7.** The \"novel datasets\" claimed in Table 3 is inappropriate. It should be referred to as the external dataset.\n\nThanks for your suggestion, we have revised the \u201cexternal dataset\u201d in Table 3 in the new version.\n\n---\n\n> **Q8.** This paper uses internal medical data for training, so it may need ethical approval for use.\n\nThe internal medical data has received IRB approval for use. In Appendix B.1 Table 5, we've detailed the source and permissions for data release. Our approach involves disseminating only the annotations of the CT volumes, which users can combine with the original CT volumes obtained from their original sources. All data created and licensed out by us will be in separate files, ensuring no modifications to the original CT volumes. Legal consultations confirm our permission to distribute these **annotations** under the licenses of each dataset. Upon acceptance of the paper, we will release the entire ImageNetCT-9K dataset to the public. This dataset will provide **296K** organ/tumor masks and **3.7M** annotated images that are taken from **68** hospitals worldwide. And this dataset will continue to expand with the collective effort from the community.\n\n---\n\n**Reference**\n\n- Ma, Jun, Yao Zhang, Song Gu, Cheng Ge, Shihao Ma, Adamo Young, Cheng Zhu et al. \"Unleashing the strengths of unlabeled data in pan-cancer abdominal organ quantification: the flare22 challenge.\" *arXiv* (2023)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631169154,
                "cdate": 1700631169154,
                "tmdate": 1700631169154,
                "mdate": 1700631169154,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "T6KM3fkRwU",
            "forum": "AhizIPytk4",
            "replyto": "AhizIPytk4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission742/Reviewer_esgA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission742/Reviewer_esgA"
            ],
            "content": {
                "summary": {
                    "value": "This paper evaluates supervised and self-supervised feature learning approaches on 3D CT data. The work first contributes a dataset of CT scans (9000 samples) of different organs, and taken from different hospitals. Next, the authors train different segmentation learning models on this data, both supervised and self-supervised, and report general trends such as sample size efficiency and transferability. The main conclusion is that there is a benefit to having supervised 3D datasets, and that self-supervision can be inferior despite the recent successes of self-supervised models in the vision community. The authors will make the dataset and trained models public."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The dataset that the authors collected seems like it will be a valuable resource for researchers, particularly in medical imaging. The data can serve to train general-purpose 3D features and comes with annotations.\n\n- I do like the point that self-supervision has its limits and that there is a benefit to simply having large supervised data. This is particularly relevant with the current interest in the vision community on self-supervised learning representations.\n\n- Experiments are reasonable and include sufficient prior models."
                },
                "weaknesses": {
                    "value": "- The title may be a little misleading. If I understood it correctly, the word \"transfer\" in the title is not referring to transfer learning in this case, but simply asking whether supervision is also good for 3D data as it has been for 2D data. When first reading the title, I thought you were exploring transferring 2D supervised models to 3D tasks. Others may also make that incorrect assumption.\n\n- The use of the word \"ImageNet\" in the dataset name may want to be reconsidered. Besides being a large dataset with a variety of anatomy, the link to ImageNet is a bit weak and may also suggest properties that the dataset does not have (e.g., per-image classification labels). \n\n- CTs are very specific types of 3D data -- it's difficult to make a claim for all 3D data from these experiments alone. I think the paper could be improved by focusing the message more narrowly, perhaps on medical imaging segmentation. \n\n- To me, the results are not surprising -- if you have a such a large dataset with rich segmentation labels, then it should do better than self-supervision. This is in contrast to classification, where this may not be true. I think the main point of the paper should be that this is a new dataset that will be valuable to the community, not that rich supervision is useful in segmentation."
                },
                "questions": {
                    "value": "1. Is there anything surprising from the results? I think it is clear that if you have rich segmentation labels you can learn a better segmentation model than via self-supervised learning."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission742/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698930927208,
            "cdate": 1698930927208,
            "tmdate": 1699636000994,
            "mdate": 1699636000994,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m6l6Q47Ogf",
                "forum": "AhizIPytk4",
                "replyto": "T6KM3fkRwU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer esgA (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for recognizing the value of our dataset, the quality of our presentation, and the extensiveness of our experiments.\n\n---\n\n> **Q1.** The title may be a little misleading. If I understood it correctly, the word \"transfer\" in the title is not referring to transfer learning in this case, but simply asking whether supervision is also good for 3D data as it has been for 2D data. When first reading the title, I thought you were exploring transferring 2D supervised models to 3D tasks. Others may also make that incorrect assumption.\n\nThe word \u201ctransfer\u201d in the title refers to transfer learning, a procedure that involves two stages. **Firstly**, a model is pre-trained on a pretext task, specifically organ segmentation in our paper. **Secondly**, the model is transferred (fine-tuned) to multiple target tasks, including new datasets, classes, and tasks. The target task performance can indicate whether the model pre-training is helpful compared with (1) learning the model from scratch and (2) other publicly available pre-trained models. In this paper, we have assessed the transfer learning ability of models under **five distinct settings**. \n1. Transfer to different datasets (domains) to segment the same organs\u2014classes that were used for pre-training (Appendix E.2 Table 10).\n2. Transfer to segmentation tasks of organs, muscles, vertebrae, and cardiac structures\u2014classes that were not used for pre-training (revised Table 4; Appendix E.3 Table 11).\n3. Transfer to segmentation tasks of pancreatic tumor segmentation\u2014more challenging classes that were not used for pre-training (revised Table 4; Appendix E.3 Table 11).\n4. Transfer to few-shot segmentation tasks using only a limited number of annotated CT volumes\u2014classes that were not used for pre-training (Figure 1; Figure 2b; Appendix D.1 Figure 9).\n5. *New:* Transfer to classification tasks that identify fine-grained tumors, including PDAC, Cyst, and PanNet in JHH (Figure 3).\n\nThank you for your valuable feedback regarding our title. We agree the previous title can be confusing and de-appreciate our contribution. We have now revised it to **\u201cHow Well Do Supervised 3D Models Transfer to Medical Imaging Tasks?\u201d** and are actively considering further refinements for greater clarity. One of the contributions of our study, as described in Section 3.2, is the systematic benchmarking of transfer learning performance, particularly from 3D segmentation tasks to a wider range of 3D imaging tasks. Another contribution is the creation of large-scale, per-voxel annotated ImageNetCT-9K, which in turn, makes the transfer learning benchmarking possible.\n\n---\n\n> **Q2.** The use of the word \u201cImageNet\u201d in the dataset name may want to be reconsidered. Besides being a large dataset with a variety of anatomy, the link to ImageNet is a bit weak and may also suggest properties that the dataset does not have (e.g., per-image classification labels).\n\nOur objective in developing ImageNetCT-9K is to drive algorithmic advancements and set new benchmarks in the field of 3D medical imaging. In many ways, our dataset echoes the early days of ImageNet, as both datasets emerged at times when large-scale data, diverse classes, and detailed labels were sparse in their respective fields. The limitations of publicly available datasets have been summarized with statistics in Appendix B.1 Table 5 and Figure 5.\n\nSegmentation is often conceptualized as per-voxel classification. In the medical domain, segmentation holds the same fundamental importance as classification does in general computer vision [[Ma & Wang, Nature Methods 2023](https://www.nature.com/articles/s41592-023-01885-0)]. We bet that ImageNet-like datasets in the medical domain should be formed as per-voxel segmentation labels. Our dataset aligns with this vision by providing per-voxel labels, offering a level of detail far surpassing ImageNet's per-image labels. Concretely, the per-voxel labels in our dataset (**272.7B annotated voxels**) are much more extensive than the per-image labels in ImageNet (**14M annotated images**).\n\nWe really appreciate your suggestions and are actively looking for better names for our dataset to deliver our vision more precisely for the medical imaging field and to inspire further research endeavors towards this end.\n\n---\n\n**Reference**\n\n- Ma, Jun, and Bo Wang. \"Towards foundation models of biological image segmentation.\" *Nature Methods* (2023)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608865870,
                "cdate": 1700608865870,
                "tmdate": 1700608865870,
                "mdate": 1700608865870,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M7MRjh6Oum",
                "forum": "AhizIPytk4",
                "replyto": "T6KM3fkRwU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer esgA (2/2)"
                    },
                    "comment": {
                        "value": "> **Q3.** CTs are very specific types of 3D data -- it's difficult to make a claim for all 3D data from these experiments alone. I think the paper could be improved by focusing the message more narrowly, perhaps on medical imaging segmentation.\n\nWe completely agree with your comments and have now revised our title, abstract, introduction, and conclusion to narrow down our current scope to 3D medical imaging. \n\nGiven that our dataset includes detailed per-voxel annotations for 25 organs and tumors, it enables the automatic generation of 3D shape representations. These representations can be formatted as point clouds, voxel occupancy grids, meshes, and implicit surface models (e.g., signed distance functions), each catering to different algorithmic needs. We anticipate our dataset could be useful for a variety of other 3D medical vision tasks [[Li et al., 2023](https://arxiv.org/abs/2308.16139)], such as pose estimation, surface reconstruction, depth estimation, etc. Since these studies go far beyond the scope of the current manuscript and our expertise, we would like to leave the investigation as an independent work in the future. The potential applications and implications of our dataset have now been included in the future work section and Appendix F.4.\n\n---\n\n> **Q4.** Is there anything surprising from the results? I think it is clear that if you have rich segmentation labels you can learn a better segmentation model than via self-supervised learning. I think the main point of the paper should be that this is a new dataset that will be valuable to the community, not that rich supervision is useful in segmentation.\n\nThank you for recognizing the value of our dataset. As detailed in **Q1**, we have systematically benchmarked the transfer learning ability of public models and are preparing to release a suite of models that are pre-trained on our extensive, annotated dataset. The download links are available in our common responses. These two contributions align closely with the conference of *Learning Representation* (IC*LR*) and our submission is categorized into the *\u201cdatasets and benchmarks\u201d* primary area.\n\nThe debate between the effectiveness of representation learning using human-annotated data (supervised pre-training) versus raw data (self-supervised pre-training) has been a longstanding topic in general computer vision, as we review in Section 2. As acknowledged by Reviewer fMfJ, *this paper concluded on the debate of whether self-supervised or supervised pre-training leads to better performance and data efficiency. This debate had not be resolved without the invention of a fully-annotation dataset of such scale.* Our constructed dataset enables this crucial comparison and promises to be a valuable asset for future algorithm benchmarking in the field.\n\nBased on our comparison, two observations are particularly surprising. Please note that in this comparison, the model is transferred to the classes and datasets that differ from those used for pre-training.\n\n1. Efficiency in pretext task: Supervised pre-training requires **99.6%** fewer data and **96.5%** less computation than self-supervised pre-training. The model trained with 21 CT volumes, 672 masks, and 40 GPU hours shows a transfer learning ability similar to that trained with 5,050 CT volumes and 1,152 GPU hours. See details in Figure 2a.\n\n2. Efficiency in target task: Supervised pre-training requires **50%** fewer manual annotations for fine-grained tumor classification than self-supervised pre-training. This is particularly critical for tumor imaging tasks because annotating tumors requires much more effort and often relies on the availability of pathology reports. See details in Figure 2b and Appendix D.1 Figure 9.\n\n---\n\n**Reference**\n\n- Li, Jianning, Antonio Pepe, Christina Gsaxner, Gijs Luijten, Yuan Jin, Narmada Ambigapathy, Enrico Nasca et al. \"MedShapeNet--A Large-Scale Dataset of 3D Medical Shapes for Computer Vision.\" *arXiv* (2023)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609045267,
                "cdate": 1700609045267,
                "tmdate": 1700609045267,
                "mdate": 1700609045267,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CauaXiMXvW",
            "forum": "AhizIPytk4",
            "replyto": "AhizIPytk4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission742/Reviewer_fMfJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission742/Reviewer_fMfJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new large-scale computed tomography (CT) dataset for medical image segmentation, which is so-far the one with the highest number of annotated scans. The authors, employing this benchmark, draw several insights where most of them are revealed for the first time. More specifically, the authors show that supervised pre-training is more effective and efficient compared with self-supervised counterpart given similar training circumstances. Their released models can effectively serve as a foundation model for transfer learning, helping to reduce the computational load and improve the segmentation accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I really enjoy reading this paper and the contribution it brings. First, the authors would release a large-scale volumetric segmentation dataset with unprecedented number of pixelwisely labeled ground truth. This dataset comes from some publically available dataset and self-constructed ones with semi-annotated tools and interactive segmentation with radiologists. Second, the paper concluded on the debate of whether self-supervised or supervised pre-training lead to better performance and data efficiency. This debate had not be resolved without the invention of a fully-annotation dataset of such scale. Third, the authors release their pre-trained models so that one can easily fine-tune the model efficiently. This can also have a good impact for the whole community. \n\nThis paper is well structure and written. The notation is clear, and the experimental setups are carefully noted. \n\nExperimental results are intensive and convincing. I checked the attached code and it seems to be solid."
                },
                "weaknesses": {
                    "value": "I do not remark any major issue as the drawback of this paper."
                },
                "questions": {
                    "value": "I haven't had many questions regarding this paper. \n\n- Could the authors explain why the performance \"scratch\" out-performed most of the pre-training method in Tab. 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission742/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698995022421,
            "cdate": 1698995022421,
            "tmdate": 1699636000928,
            "mdate": 1699636000928,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "msWtNi5KAQ",
                "forum": "AhizIPytk4",
                "replyto": "CauaXiMXvW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fMfJ"
                    },
                    "comment": {
                        "value": "We are grateful for your accolades on our contributions, *\u201c...a foundation model for transfer learning\u2026\u201d*, *\u201c...concluded on the debate\u2026\u201d*, *\u201c...the invention of a fully-annotation dataset of such scale\u201d*, and potential impact, *\u201cone can easily fine-tune the model efficiently\u201d*. \n\n---\n\n> **Q1.** Could the authors explain why the performance \"scratch\" out-performed most of the pre-training method in Tab. 2?\n\nThe goal of Table 2 is to provide a practical benchmark for the transfer learning ability of readily available pre-trained models. Our intent is not to compare the specific pre-training methodologies of each model for two primary reasons. **Firstly**, the majority of researchers tend to fine-tune pre-existing models rather than retrain them from scratch due to convenience and accessibility. **Secondly**, reproducing these models would require specialized hyper-parameter tuning and varied computational resources. For example, models like Swin UNETR [[Tang et al., CVPR 2023](https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.pdf)] are pre-trained using large-scale GPU clusters at NVIDIA, making them challenging for us to faithfully retrain. Considering both practical user scenarios and computational constraints, we decided to directly use their released models and fine-tune them with consistent settings on the same datasets.\n\nUsing existing pre-trained models can inevitably lead to certain problems. For example, the U-Net family has seen numerous variations over the years [[Siddique et al., 2021](https://ieeexplore.ieee.org/abstract/document/9446143)]. Pre-trained models released before 2021 typically employed a basic version of U-Net (e.g., [[Zhou et al., 2019](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7405596/)] and [[Chen et al., 2019](https://arxiv.org/abs/1904.00625)] in Table 2). On the other hand, our U-Net benefits from a more advanced code base, thanks to the [MONAI](https://monai.io/) platform at NVIDIA, which includes enhanced architectures and advanced training optimization strategies. Consequently, our U-Net, even trained from scratch, is capable of surpassing the performance of these older baseline models.\n\n---\n\n**Reference**\n\n- Tang, Yucheng, Dong Yang, Wenqi Li, Holger R. Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, and Ali Hatamizadeh. \"Self-supervised pre-training of swin transformers for 3d medical image analysis.\" *CVPR* (2022).\n- Siddique, Nahian, Sidike Paheding, Colin P. Elkin, and Vijay Devabhaktuni. \"U-net and its variants for medical image segmentation: A review of theory and applications.\" *Ieee Access* (2021).\n- Zhou, Zongwei, Vatsal Sodha, Md Mahfuzur Rahman Siddiquee, Ruibin Feng, Nima Tajbakhsh, Michael B. Gotway, and Jianming Liang. \"Models genesis: Generic autodidactic models for 3d medical image analysis.\" *MICCAI* (2019).\n- Chen, Sihong, Kai Ma, and Yefeng Zheng. \"Med3d: Transfer learning for 3d medical image analysis.\" *arXiv* (2019)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606934003,
                "cdate": 1700606934003,
                "tmdate": 1700606934003,
                "mdate": 1700606934003,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]