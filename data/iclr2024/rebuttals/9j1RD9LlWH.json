[
    {
        "title": "Bayesian Optimization through Gaussian Cox Process Models for Spatio-temporal Data"
    },
    {
        "review": {
            "id": "mO6GAzjO9b",
            "forum": "9j1RD9LlWH",
            "replyto": "9j1RD9LlWH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6205/Reviewer_dgpT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6205/Reviewer_dgpT"
            ],
            "content": {
                "summary": {
                    "value": "The paper seeks to build a novel Bayesian Optimization approach that is based on Gaussian Cox Processes for spatio-temporal data. The authors emphasize that Gaussian Cox Processes have never been used within BO settings. The posterior distribution is computed using Laplace approximation and a change of kernel that enables to transform the inference problem into a kernel regression problem. The latter kernel is computed using Nystr\u00f6m approximation. Then the authors present several acquisition functions that are built using posterior mean and posterior variance of the Gaussian Cox Process. Finally the authors illustrate the efficiency of their approach on two types of experiments. First, they show the quality of the mean estimation on synthetic data (4.1.1) and real word data (4.2.1). Second, they show how the BO with UCB function performs for some synthetic dataset (4.1.2) and real world dataset (4.2.2)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written and the overall approach is scientifically sound and compelling. \nThe motivation is clear, which is to provide a BO framework with Gaussian Cox processes.\nAlthough Laplace approximation is a standard tool, the authors provide an elegant way to derive the maximum of the log likelihood through a trick that transform the problem into a standard kernel regression problem so that they can use the representer theorem. The main results are clearly detailed. The use of Nystrom approximation is standard, but Lemma 3 helps the reader to figure out what has been implemented. \nThe numerical experiments section show that the authors made great effort to compare all the components of their methodology with some of the state-of-the-art approaches. In addition, this has been done for simulated data and real-world data."
                },
                "weaknesses": {
                    "value": "Although the paper is well written and has many interesting components, there are a couple of points that need to be detailed.\n\nMajor comments:\n\n- My first question is general. In a standard BO setting, we aim at minimizing a function which is costly to evaluate. It seems that this is not the objective of the presented BO problem. It would have been good to make a clear distinction between the two problems.  \n\n - In the literature review, the authors write that \"existing works mostly concentrate on the mean estimation\". I am not an expert in Cox models, but it seems that this has been investigated in [1]. This means that this approach could have also been tested in the numerical experiments with Bayesian Optimization. It also seems that this cited paper uses some tools (Laplace approximation, eigenfunctions decomposition) similar to the ones in the paper. Would it be possible to highlight the main modeling differences with this paper? \nWhat is the theoretical/computational benefit of the paper's approach compared to the cited paper? \n\n-  In Equation (5), the authors claim that the rest of the terms of the likelihood are dominated by the first term when $n$ is large. However, this assumption is not always true in a BO setting (it is not in the numerical experiments). Could the authors comment that point and provide more details?\n\n- The authors claim that they can use the Nystrom approximation to compute the next approximation from new samples in an incremental fashion. Is this step used in the experiment? How does it work in practice? \n\nMinor comments:\n\n- After reading the proof of Lemma 2, there is a point I did not understand. Why does the function $h$ belong to $\\mathcal{H}$? This fact does not look straightforward to me. Perhaps I missed something in the development.\n\n- In the numerical experiments, the authors do not report the results of PIF in Table 2. Is there a reason for that? I've not found it in the paper.\n\n- In figure 2c, it looks like the maximum of the acquisition function is around t=40. This means that we have to sample around $t=40$. However I don't see any sample around $t=40$ in figure 2d. \n\n\n[1] Hideaki Kim. Fast Bayesian Inference for Gaussian Cox Processes via Path Integral Formulation. Advances in Neural Information Processing Systems  2021."
                },
                "questions": {
                    "value": "See questions above.\nIs the code publicly available?\nDepending on author responses, I would change my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6205/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6205/Reviewer_dgpT",
                        "ICLR.cc/2024/Conference/Submission6205/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6205/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698250335205,
            "cdate": 1698250335205,
            "tmdate": 1700728318809,
            "mdate": 1700728318809,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5VvFVsofF1",
                "forum": "9j1RD9LlWH",
                "replyto": "mO6GAzjO9b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6205/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6205/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer dgpT (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive suggestions. Our answers to your questions are provided below.\n\n>Reviewer: In a standard BO setting, we aim at minimizing a function which is costly to evaluate. It seems that this is not the objective of the presented BO problem. It would have been good to make a clear distinction between the two problems.\n\n\n***Response***: Thank you for the question. We would like to clarify that this is indeed our objective. However, expensive-to-evaluate functions relating to discrete point observations over time and space, such as arrivals, events, and occurrences, must be modeled as a Gaussian Cox process, which is a Poisson process modulated by a latent Gaussian process. Standard BO relies on a Gaussian prior and cannot model such expensive-to-evaluate functions with respect to point observation and spatio-temporal data. It requires using doubly stochastic process models, i.e., Gaussian Cox process models considered in this paper. \n\nWe clarify that the paper makes two related contributions: (1) a Gaussian Cox process model for expensive-to-evaluate functions relating to discrete point observations over time and space, and (2) a BO framework that builds on such models.\n\nWe will also add two illustrative examples to demonstrate the unique advantages of using Gaussian Cox process models for BO. (1) Consider a problem where we need to find the time and location of maximum truck arrivals from video feeds collected by tens of thousands of geo-distributed cameras in a city. This is an expensive-to-evaluate function since it requires performing analytics on a huge volume of video feeds. It is natural to model the truck arrivals (over time and location) as a Gaussian Cox process. We can sample a small set of video clips and use them to estimate the latent intensity, thus supporting efficient BO. (2) Consider the problem of finding the time and location of maximum bird-related incidents in a city. Again, this is an expensive-to-evaluate function, and we have to estimate a latent intensity function from limited samples. We can use the proposed method to model the incidents through a Gaussian Cox process model and then support a BO to solve this problem. For problems requiring estimating a latent intensity function from point observations, we need to use BO with Gaussian Cox process models as proposed in this paper.\n\n>Reviewer: In the literature review, the authors write that \"existing works mostly concentrate on the mean estimation\". I am not an expert in Cox models, but it seems that this has been investigated in [1]. This means that this approach could have also been tested in the numerical experiments with Bayesian Optimization. It also seems that this cited paper uses some tools (Laplace approximation, eigenfunctions decomposition) similar to the ones in the paper. Would it be possible to highlight the main modeling differences with this paper? What is the theoretical/computational benefit of the paper's approach compared to the cited paper?\n\n***Response***: Thanks for the question. In the paper, we have compared our results with those reported in [1]. Specifically, this is included in Table 2 with respect to three commonly used synthetic functions and the IQL metric. Regarding the approaches and modeling differences, we would like to clarify that the previous paper [1] uses path integral to obtain an approximation of the solution. In contrast, our approach is to recast the problem into a new RKHS through a change of kernel technique so it becomes easier to handle. It also allows applying the representer theorem to ensure the uniqueness of the estimated mean and covariance. Additionally, our approach enables the use of Nystrom approximation for efficient numerical computation (in Section 3.3), given the change of kernel and the new RKHS. These fundamental differences allow our proposed solution to achieve more favorable results than [1], as demonstrated in Table 2, and make our framework align more with the BO design."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700128849022,
                "cdate": 1700128849022,
                "tmdate": 1700128849022,
                "mdate": 1700128849022,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fL1zcR6wjw",
                "forum": "9j1RD9LlWH",
                "replyto": "5FxeJnAWWP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6205/Reviewer_dgpT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6205/Reviewer_dgpT"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the responses"
                    },
                    "comment": {
                        "value": "I thank the authors for the responses.\n\nI am still not convinced about an answer of the authors.\n When the number of samples is low, I don't see any reasons to neglect the prior term in the likelihood function. Indeed, there are some practical cases where the number of acquired data points grows very quickly. However, there is also a large number of problems where acquiring data is costly and the purpose of using BO approach is then to make this acquisition intelligent with a low number of data points. \n\nI'd consider increasing my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644879899,
                "cdate": 1700644879899,
                "tmdate": 1700644879899,
                "mdate": 1700644879899,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hv8Usm6sO9",
                "forum": "9j1RD9LlWH",
                "replyto": "mO6GAzjO9b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6205/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6205/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dgpT"
                    },
                    "comment": {
                        "value": "Thank you for your comment. We will certainly consider discussing the approximation related to a few samples and intelligent acquisition strategy with a low number of data points in future work. For now, we have added an experiment starting with a small number of events to observe each step in Fig. 10, Appendix N. The experimental results show that our method can be applied to cases where input events are low without significant estimation deviation from the true intensity."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717225187,
                "cdate": 1700717225187,
                "tmdate": 1700717465903,
                "mdate": 1700717465903,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Z9BhsCMzZD",
            "forum": "9j1RD9LlWH",
            "replyto": "9j1RD9LlWH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6205/Reviewer_bJz1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6205/Reviewer_bJz1"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel framework for conducting BO by leveraging Cox processes. This approach hinges on a Laplace approximation of the likelihood and uses kernel techniques to transform the optimization problem into a RKHS. The framework is empirically evaluated across a range of scenarios, encompassing well-known synthetic functions and real-world databases. The results of numerical experiments indicate that this approach exhibits competitive performance in comparison to other state-of-the-art methods. Unlike the other frameworks, it stands out by enabling BO within the context of Cox process-based models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Theoretical contributions bring together techniques from the machine learning and functional analysis communities. Lemmas and other theoretical developments can be easily verified thanks to the clarity of the discussions. The diversity of examples, which take into account well-studied synthetic functions and real databases, makes it possible to assess the competitiveness of the framework in relation to the literature. The paper is generally well-written and well-organized."
                },
                "weaknesses": {
                    "value": "Although the strengths lie in both the theoretical and numerical aspects, the motivation for performing BO in point processes lacks practical utility. For example, in the spatio-temporal application describing tornadoes in the USA, I fail to see how new events (tornadoes involving damage) can be sampled sequentially to promote active learning of the intensity function $\\lambda$. I assume that for illustrative purposes, the authors considered adding the \"closest event\" available in the database that matches the BO's suggestion. Is this correct? If this is the case, and if the size of the database allows tractable implementations, we can consider all events for inference of $\\lambda$. If the model cannot handle the whole database, the BO schema is an interesting idea that promotes a threshold between inference quality and the number of observed point events. However, what can be done if no similar events are recorded in the database?  Can the authors give further details on the practical utility of their framework?\n\nThe authors have suggested publishing the Python codes in a Github repository, but there is no evidence of their existence. I suggest sharing an anonymous repository (e.g. via https://anonymous.4open.science/) for further examination."
                },
                "questions": {
                    "value": "**Questions**\n- Are the results in Table 2 consistent, i.e. similar results are obtained for a different seed? If no, the authors must consider several random replicates and provide the mean +- std of the results\n- In Figure 2, at the initial step, the UCB acquisition function suggests adding new events at $t > 90$ (since we seek to maximize such criterion) but they are added somewhere else. Similarly, in step 14, the UCB targets the instants around $t = 40$ but events are again added somewhere else. Besides the authors argue that \"the algorithm keeps sampling by maximizing UCB acquisition function and then improving the estimation based on new samples observed\", the plots do not validate their point. Can the authors further explain the results while clarifying my concern? Is it possible to add extra plots at consecutive steps (e.g. steps 1 and 2) for a better understanding of the BO's choice?\n- In the experiments, the choice of the hyperparameters $w_1, w_2, w_3$ is not discussed. Can the authors precise their values in each experiment and explain how they were tuned? \n- The authors approximate the integral $\\int_{\\mathcal{S}} \\kappa(g(t)) dt$ using an $m$-partition Riemann sum to obtain a closed-form of the posterior covariance. Since such approximation depends on $m$, can the authors discuss the quality of the approximation in terms of $m$ and precise how they tune that value in the experimental setup? Can they also discuss the scalability of the approximation when $d$ increases?\n- The limitations of the proposed framework are not discussed in the paper. Can the authors add a remark on this subject?\n\n**Other minor remarks**\n- Page 3, Table 1: the derivatives of the link functions need to be checked. For instance, $\\dot{\\kappa}(x) = 2x$ (quadratic case), $\\dot{\\kappa}(x) = \\frac{e^{-x}}{(1+e^{-x})^2}$ (sigmoidal), $\\ddot{\\kappa}(x) = \\frac{e^{-x}}{(1+e^{-x})^2}$ (softplus), ...\n- Page 3, Section 3.1: $\\Sigma$ is a **CO**variance\n- Page 3, Section 3.1, after Eq. (2): $\\lambda(t) = \\kappa(g(t)) \\to \\lambda(t)$ (it has been already defined before Eq.(1) ) \n- Page 4, after Eq. (7): However, Equation equation (7)\n- Page 4, after Eq. (8): $\\eta_i$ and $\\phi_i(\\cdot)$ need to be defined in the main part of the paper (they were defined in the supplementary material)\n- Page 5, Eq. (10): $\\Lambda = \\operatorname{diag}(\\lambda_1, \\ldots, \\lambda_m)$ needs to be defined\n- Page 7, Section 4: To precise that further details on the \"evaluation setup\" are given in Appendix G\n- Page 7, Section 4.1.1: to indicate the number of events considered in each toy example\n- Page 9, Figure 5: to indicate the iteration step in each panel \n- In Appendix C, Eq. (26): $h(t_j) = \\langle h, \\tilde{k}(t_j, \\cdot) \\rangle_{\\mathcal{H}_{\\tilde{k}}}$ ($j$ rather than $i$)\n- In Appendix C, Eq. (32): the first line must be $\\sum_{i=1}^{n} \\log(\\kappa(g(t_i))) - \\sum_{j=1}^m \\kappa(g(t)) \\Delta t$. Then, the sign of $\\ddot{\\kappa}^2(\\hat{g}_i) \\Delta$ must be inverted.\n- In Appendix C, Eq. (32): given the proposed notation, it is not clear that the dimension of $\\nabla_{\\hat{g}}^{2} \\Psi(\\hat{g})$ matches the dimension of the $d \\times d$ matrix $\\Sigma$. Can the authors clarify this and/or propose a more readable notation?\n- In the References: laplace $\\to$ Laplace (Illian et al., 2012), bayesian $\\to$ Bayesian (Kim, 2021), to add all the authors in (Lai et al., 1985), to complete the reference (Stanton et al., 2022), to be consistent with the names of the journals and conferences and the style of displaying them."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6205/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6205/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6205/Reviewer_bJz1"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6205/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698407733190,
            "cdate": 1698407733190,
            "tmdate": 1699636676342,
            "mdate": 1699636676342,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1w908NR2OQ",
                "forum": "9j1RD9LlWH",
                "replyto": "Z9BhsCMzZD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6205/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6205/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer bJz1 (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for pointing out and correcting the minor mistakes in our manuscript, which will definitely make the paper much more robust. We have updated our manuscript after checking all the remarks you kindly provided. Besides, we provide our responses regarding your concerns as follows.\n\n>Reviewer: The authors considered adding the \"closest event\" available in the database that matches the BO's suggestion. Is this correct? If this is the case, and if the size of the database allows tractable implementations, we can consider all events for inference of. If the model cannot handle the whole database, the BO schema is an interesting idea that promotes a threshold between inference quality and the number of observed point events. However, what can be done if no similar events are recorded in the database? Can the authors give further details on the practical utility of their framework?\n\n***Response***:  Thank you for the question. We will certainly clarify our motivation in the final version. BO based on Gaussian Cox process models is useful when the target function (or dataset, as the reviewer mentioned) is expensive to evaluate. \n\nWe will add two illustrative examples where the target function is hard to evaluate (either because it requires performing expensive analytics on a known dataset or due to the fact that obtaining the complete dataset is expensive). (1) Consider a problem where we need to find the time and location of maximum truck arrivals from video feeds collected by tens of thousands of geo-distributed cameras in a city. This is an expensive-to-evaluate function since it requires performing analytics on a huge volume of video feeds. It is natural to model the truck arrivals (over time and location) as a Gaussian Cox process. We can sample a small set of video clips and use them to estimate the latent intensity, thus supporting efficient BO. (2) Consider the problem of finding the time and location of maximum bird-related incidents in a city. Again, this is an expensive-to-evaluate function, and we have to estimate a latent intensity function from limited samples. We can use the proposed method to model the incidents through a Gaussian Cox process model and then support a BO to solve this problem.\n\nWe also need to clarify that our proposed BO leverages Gaussian Cox process models to estimate the target function (or dataset as mentioned by the reviewer) from limited samples available. It is a Bayesian method and does not require similar data. The results support the development of acquisition functions that balance exploration (by collecting new samples) and exploitation (to optimize performance objectives) in BO.\n\n>Reviewer: Are the results in Table 2 consistent, i.e., similar results are obtained for a different seed? If no, the authors must consider several random replicates and provide the mean +- std of the results.\n\n***Response***: Thank you for your suggestion. Table 4 was included in the appendix as the completed version of Table 2 with standard deviation. Due to the page limit, we shortened the table in the main text to save space. We will also add a description in the corresponding section for clarification and refer readers to the full version of Table 4 in the appendix.\n\n>Reviewer: In Figure 2, at the initial step, the UCB acquisition function suggests adding new events at $t>90$ (since we seek to maximize such criterion) but they are added somewhere else. Similarly, in step 14, the UCB targets the instants around $t=40$ but events are again added somewhere else. Can the authors further explain the results while clarifying my concern? Is it possible to add extra plots at consecutive steps (e.g. steps 1 and 2) for a better understanding of the BO's choice?\n\n***Response***: Sorry about the confusion. In Fig. 2, the black vertical bars at the bottom of each subplot denote the ground-truth events that are not visible to BO. Only the red vertical bars are regions sampled already and thus visible in BO. For instance, at the initial step, our proposed algorithm tries to build a Gaussian Cox process model based on the observed events shown by red vertical bars ($t=25$ and $t=60$).\n\nThe green curve depicting the UCB acquisition function anticipates the region of interest for the next step based on our posterior estimates of the current step. In Fig. 2(c), the maximum UCB acquisition value indicates that the algorithm plans to explore around $t=40$ in the upcoming step 15. However, since no events are observed near $t=40$ when we sample, no new observations (i.e., vertical red bars) are added at $t=40$. The Gaussian Cox process model is adjusted based on this new information, and the UCB acquisition function is updated accordingly to guide future samples/steps. We have added the complete consecutive step-wise figure (Fig. 8 in Appendix L, providing a comprehensive demonstration of the BO procedure."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127579808,
                "cdate": 1700127579808,
                "tmdate": 1700127986373,
                "mdate": 1700127986373,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gGTLYh7Mo2",
                "forum": "9j1RD9LlWH",
                "replyto": "Z9BhsCMzZD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6205/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6205/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer bJz1 (2/2)"
                    },
                    "comment": {
                        "value": ">Reviewer: In the experiments, the choice of the hyperparameters $\\omega_1, \\omega_2, \\omega_3$ is not discussed. Can the authors precise their values in each experiment and explain how they were tuned?\n\n***Response***: $\\omega_1, \\omega_2, \\omega_3$ are weights for mean and covariance terms to balance their contributions in the acquisition function. When these values are small, BO will place higher emphasis on the mean, favoring exploitation in the process. In contrast, when they are large, BO favors exploration by placing higher emphasis on large variance. In our experiments, we maintain the fixed values for all these parameters, specifically as 0.8.\n\nFurthermore, additional experiments were conducted to assess the impact. In the experiments, we add two extra levels (0.6 and 1.0) for $\\omega_3$ to compare with the original $\\omega_3=0.8$ in Fig. 6 as an example. The results are shown in Fig. 9 in Appendix M, where if we increase the $\\omega_3$ value, the acquisition function will encourage exploring areas with high uncertainty earlier. \n\n>Reviewer:  The authors approximate the integral $\\int_\\mathcal{S} \\kappa(g(\\boldsymbol{t})) d\\boldsymbol{t}$ using an $m$-partition Riemann sum to obtain a closed-form of the posterior covariance. Since such approximation depends on $m$, can the authors discuss the quality of the approximation in terms of $m$ and precise how they tune that value in the experimental setup? Can they also discuss the scalability of the approximation when $d$ increases?\n\n***Response***: Since the exact closed-form solutions are not tractable, we discretize the integral term for obtaining the computable closed-form covariance. In the experiment, we set $m$ as the grid size. Therefore, a higher $m$ leads to a finer granularity but higher computation complexity; otherwise a lower $m$ will increase the computational efficiency but with slightly lower quality. To explore the impact of this value, we conduct an additional experiment and show the results in the following table, where we used the synthetic intensity and ran BO for 10 steps. In the table, the runtime will increase with a higher $m$, while the performance does not deviate much.\n\n| $m$ | 200 | 400 | 600 | 800 |\n| :---  | :----: | :---: | :---: | :---: | \n| step 5 $l_2$-norm | 8.35 (3.1) | 10.54 (3.43) | 12.99 (2.78) | 11.91 (2.98) |\n| step 10 $l_2$-norm | 4.68 (1.84) | 5.38 (2.21) | 4.42 (1.02) | 4.08 (1.12) |\n| runtime | 6.03 (0.12) | 15.98 (0.23) | 34.81 (0.09) | 71.61 (0.20) |\n\nRegarding the scalability of the approximation, our solution computes the kernel matrix of each dimension separately and then applies the Kronecker product to construct the final kernel matrix as $K = K_1 \\otimes \\dots \\otimes K_d$. For example, given $d=2$ where we have two kernel matrices with size $m_1$ and $m_2$, the time complexity will be $O(m_1^2 m_2^2)$.\n\n>Reviewer: The limitations of the proposed framework are not discussed in the paper. Can the authors add a remark on this subject?  \n\n***Response***: Thanks for the question. For future work, we will analyze more complex scenarios with the high-dimensional Gaussian Cox process model, and we will try to add some discussions about this in the final version.\n\n>The code is made available via the [link](https://anonymous.4open.science/r/gaussian_cox_bo-6926)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127976010,
                "cdate": 1700127976010,
                "tmdate": 1700128336978,
                "mdate": 1700128336978,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OzyODfsleu",
                "forum": "9j1RD9LlWH",
                "replyto": "1w908NR2OQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6205/Reviewer_bJz1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6205/Reviewer_bJz1"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for addressing many of my concerns. \n\n> Response: Thank you for your suggestion. Table 4 was included in the appendix as the completed version of Table 2 with standard deviation. Due to the page limit, we shortened the table in the main text to save space. We will also add a description in the corresponding section for clarification and refer readers to the full version of Table 4 in the appendix.\n\nIs it possible to precise the number of replicates used to build Table 4? Can the standard deviations be considered when highlighting the best results? For instance, for $\\lambda_1$, it is not very clear that the model **Ours (q)** outperforms the model **PIF (s)** due to the larger error intervals when comparing the $IQL_{.85}$ criterion. Similarly, for $\\lambda_2$ and $IQL_{.50}$, both **Ours (q)** and **STVB** are competitive (I would say that the latter is better).\n\nIncreasing the number of replicates may help to make error intervals finer."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497528033,
                "cdate": 1700497528033,
                "tmdate": 1700497528033,
                "mdate": 1700497528033,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "73duLPDPw0",
                "forum": "9j1RD9LlWH",
                "replyto": "3iMOCNdMlY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6205/Reviewer_bJz1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6205/Reviewer_bJz1"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for this clarification. When judging the best model, I suggest considering the median rather than the mean if the focus is only on model accuracy (ignoring the dispersion associated with error intervals). The former statistic is known to be more robust. In my opinion, it's fairer to compare models on the basis of both accuracy and dispersion. This can achieved by considering boxplots. For instance, Table 4 (using 10 replicates) can be completed (or replaced) by boxplots (baselines vs error criterion) for each intensity function $\\lambda_i$."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644037933,
                "cdate": 1700644037933,
                "tmdate": 1700644037933,
                "mdate": 1700644037933,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4qqIMcTbW3",
            "forum": "9j1RD9LlWH",
            "replyto": "9j1RD9LlWH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6205/Reviewer_kMEd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6205/Reviewer_kMEd"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors propose a novel method to estimate the posterior mean and covariance of the gaussian cox process model.\n\nThey do this by first approximating the posterior $p(g|{t_i})$ via Laplace approximation, and then using BIC to further simplify the computation. This is in terms of $\\hat g$, which must be solved for by minimizing Eq. 6. To do this, they use RKHS along with a transformation of kernel to make the problem computationally cheap to solve. Once this is done, the posterior mean and covariance can be estimated by $\\hat g$ and the expression in Eq. (9). For kernels that cannot be expanded explicitly, they also discretize and use a Nystrom approximation.\n\nWith a way of estimating posterior mean and covariance, one now is free to choose an acquisition function for the specific problem being solved. The authors discuss various settings in which different acquisition can be applied within this framework.\n\nExperiments are carried out showing both the modelling of the latent intensity, as well as the full framework applied in various spatiotemporal settings."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper claims to be the first work on BO using Gaussian Cox Process models. I could not disprove this claim through a short search, and if true, I think shows a clear strength in its originality. Every claim seemed technically sound and I could not find any glaring problems, and there were a myriad of experiments demonstrating the method in various synthetic and real world settings. The results present are qualitatively and quantitatively compelling, and the whole paper is relatively clear to understand and well written."
                },
                "weaknesses": {
                    "value": "Because many other people have not used Gaussian Cox Process models for BO before, I wonder how much modelling the latent intensity actually helps. I did not see any results or discussion on this, but it feels like a useful comparison to make to show that using GCP is actually more performant than standard BO."
                },
                "questions": {
                    "value": "-I'm slightly confused about Section 3.4 in that it seems like one can choose any acquisition function that would solve their problem. What about using Gaussian Cox enables us to do this in contrast to standard BO?\n\n-Were there any experiments done which could find the posterior mean and covariance in closed form (without Nystrom approximation)? I don't have good intuition for how much expressivity is lost in doing this approximation.\n\n-The paper analyzes this method on spatial-temporal data, but couldn't I use this method with any temporal data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6205/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6205/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6205/Reviewer_kMEd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6205/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788632764,
            "cdate": 1698788632764,
            "tmdate": 1699636676221,
            "mdate": 1699636676221,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Gk2Sroh0UR",
                "forum": "9j1RD9LlWH",
                "replyto": "4qqIMcTbW3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6205/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6205/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer kMEd (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your appreciation of our work. We provide our answers to your questions as follows.\n\n>Reviewer: Because many other people have not used Gaussian Cox Process models for BO before, I wonder how much modeling the latent intensity actually helps. I did not see any results or discussion on this, but it feels like a useful comparison to make to show that using GCP is actually more performant than standard BO.\n\n\n***Response***: Thanks for your question. The Gaussian Cox process is the suitable model for addressing problems involving discrete point observations over time and space, such as arrivals, events, and occurrences. This process, characterized by a Poisson process modulated by a latent Gaussian process, proves essential for scenarios where the standard BO relying on a Gaussian prior falls short. The latter cannot effectively model expensive-to-evaluate functions in the context of point observation and spatio-temporal data. Thus, the estimation of latent intensity from discrete point observations becomes necessary, requiring doubly stochastic process models. Gaussian Cox process models are considered as the golden standard for spatio-temporal data [1,2]. This is what motivates our work.\n\nWe will provide two illustrative examples demonstrating the unique advantages of using Gaussian Cox process models for BO. (1) Consider a problem where we need to find the time and location of maximum truck arrivals from video feeds collected by tens of thousands of geo-distributed cameras in a city. This is an expensive-to-evaluate function since it requires analyzing a huge volume of video feeds. It is natural to model the truck arrivals (over time and location) as a Gaussian Cox process, as proposed in this paper. We can sample a small set of video clips and use them to estimate the latent intensity, thus supporting efficient BO. (2) Consider the problem of finding the time and location of maximum bird-related incidents in a city. Again, this is an expensive-to-evaluate function, and we need to estimate a latent intensity function from limited samples. We can use the proposed method to model the incidents through a Gaussian Cox process model and then support a BO to solve this problem. For problems requiring estimating a latent intensity function from point observations, we need to use the Gaussian Cox process models proposed in this paper.\n\n>Reviewer: I'm slightly confused about Section 3.4 in that it seems like one can choose any acquisition function that would solve their problem. What about using Gaussian Cox enables us to do this in contrast to standard BO?\n\n***Response***: As mentioned in the previous answer, for problems relating to discrete point observations over time and space, such as arrivals, events, and occurrences, they must be modeled as a (doubly stochastic) Gaussian Cox process. Gaussian process models in Standard BO are insufficient. In this paper, we first develop a Gaussian Cox process model to estimate the latent intensity from point observations and then, in Section 3.4, show that various acquisition functions can be developed in our proposed BO framework, supporting standard acquisition functions like UCB as well as more specialized acquisition functions defined on the latent intensity function, such as maximum idle time detection and change point detection, fully exploiting the temporal properties of the underlying data. \n\n>Reviewer: Were there any experiments done which could find the posterior mean and covariance in closed form (without Nystrom approximation)? I don't have a good intuition for how much expressivity is lost in doing this approximation.\n\n***Response***:  This is a great question. We have conducted additional experiments to assess the approximation gap. It uses a synthetic set of point events and an RBF kernel, comparing the mean intensity estimations with and without Nystrom approximation. As shown in the following table, the approximation is quite accurate.\n\n| Trials    | 1 | 2 | 3 | average |\n| ----------- | :----------: | :-----------: | :-----------: | :-----------: |\n| $l_2$-norm | 0.79| 1.33 | 0.89 | 1.00 (0.23) |\n\nWe would also like to note that Nystrom approximation can significantly reduce the computational complexity from $O(n^3)$ to $O(n^2m+m^3)$, where $m$ is the number of grid size and smaller than the number of events $n$ [3]. The theoretical analysis of the approximation gap has been discussed in [4]. Additionally, the Nystrom approximation proves beneficial when obtaining a closed-form solution for a specific kernel is not feasible through explicit Mercer\u2019s expansion. We will cite these results and explain them in the final version."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700126170376,
                "cdate": 1700126170376,
                "tmdate": 1700126170376,
                "mdate": 1700126170376,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0lp6omp7Uv",
                "forum": "9j1RD9LlWH",
                "replyto": "DCqawwLDpR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6205/Reviewer_kMEd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6205/Reviewer_kMEd"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their rebuttals. My questions were answered for the most part, and I think that the current score I have given is fair. The primary reason for this is that I am uncertain about the significance of this work to the BO community. It is, however, a sound and mathematically interesting paper."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679630755,
                "cdate": 1700679630755,
                "tmdate": 1700679630755,
                "mdate": 1700679630755,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]