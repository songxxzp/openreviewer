[
    {
        "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation"
    },
    {
        "review": {
            "id": "1C74UDZ2SM",
            "forum": "gC6JTEU3jl",
            "replyto": "gC6JTEU3jl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2493/Reviewer_MvCd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2493/Reviewer_MvCd"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a novel method for blockwise pruning of LLMs. It was evaluated with generation tasks, as well as for zero-shot downstream tasks, and outperformed SparseGPT and Wanda baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method outperformed recent baselines.\n- The motivation for this research is clear, and the proposed method is helpful for practitioners."
                },
                "weaknesses": {
                    "value": "Authors claimed BESA has different advantages compared to other baselines. E.g., the fact that BESA differentiably optimizes masks, unlike SparseGPT. However, it is not clear whether other methods could or could not use such specific techniques and what makes BESA better than them. The current ablation study does not answer these questions."
                },
                "questions": {
                    "value": "Please refer to the weaknesses Section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2493/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697635274151,
            "cdate": 1697635274151,
            "tmdate": 1699636186123,
            "mdate": 1699636186123,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LQL2gBrwKY",
                "forum": "gC6JTEU3jl",
                "replyto": "1C74UDZ2SM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2493/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MvCd"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful feedback. We appreciate your engagement with our rebuttal and the points you've raised. We have carefully considered each of your concerns and addressed them as follows:\n\n**Q1:** \"Authors claimed BESA has different advantages ... does not answer these questions.\"\n\n**A1:** Thank you for your insightful suggestion. In response, we have conducted experiments using three additional pruning techniques commonly employed in computer vision and natural language processing. These methods, namely iterative pruning (proposed in the Lottery Ticket Hypothesis [A]), threshold pruning [B], and direct mask learning, have been implemented in a blockwise manner for a fair comparison. For iterative pruning, we employ a step-wise approach where we iteratively prune the LLM block with a step of 10% according to the metric Wanda or Weight magnitude. During each iteration, we update the weights with the reconstruction loss. The results are reported in the table under \"Iterative Prune\". Then for threshold pruning, we learn a threshold value for each layer within the block while maintaining the block sparsity at 50%. The results are reported in the table under \"Threshold Prune\". For direct mask learning, we directly learn a sparse mask for the LLM block. The results are reported in the table under \"Direct Mask Learning\". We opt for iterative pruning as it stands out as one of the most representative methods for pruning, incorporating weight updating. Additionally, we select threshold pruning for its cutting-edge capabilities, showcasing state-of-the-art performance in computer vision models. Lastly, we choose direct mask learning as it represents the most straightforward and directly differentiable pruning method. Notably, the original \u201cIterative prune\u201d and \u201cThreshold prune\u201d methods use the weight value as a measure of importance. In contrast, we provide the versions that use the weight or Wanda importance metric.\n\nThe perplexity results on WikiText2, C4, and PTB datasets are presented below:\n\n|  | BESA | Iterative prune (Wanda) | Iterative prune (Weight) | Threshold prune (Wanda) | Threshold prune (Weight) | Direct mask learning |\n| --- | --- | --- | --- | --- | --- | --- |\n| WikiText2 ppl | 6.86 | 6.88 | 123.48 | 2984.61 | 20351.98 | 1457.53 |\n| C4 ppl | 8.96 | 9.08 | 102.11 | 2191.59 | 21203.34 | 371.47 |\n| PTB ppl | 66.96 | 57.71 | 3929.01 | 4600.38 | 32474.87 | 3927.83 |\n\nThese results underscore the unique advantages of our method, particularly its ability to achieve substantial reductions in perplexity. We believe that this thorough exploration of differentiable methods provides a comprehensive understanding of the strengths and capabilities of BESA compared to other baselines.\n\nAdditionally, we have re-implemented our algorithm in the style of structured pruning and compared it with LLM-Pruner [C], the latest structured pruning algorithm we found. The comparison includes WikiText2 perplexity and zero-shot downstream task performance and is conducted on LLaMA-7B with 20% sparsity without tuning. The table below shows the results, highlighting that our method significantly reduces WikiText2 perplexity by at least 40%, while maintaining comparable performance in zero-shot downstream tasks compared to LLM-Pruner's various weight importance settings.\n\n|  | WikiText2 ppl | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e | ARC-c | OBQA | Average |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| LLM-Pruner (Vector) | 22.28 | 61.44 | 71.71 | 57.27 | 54.22 | 55.77 | 33.96 | 38.40 | 53.25 |\n| LLM-Pruner (Element2) | 19.77 | 59.39 | 75.57 | 65.34 | 61.33 | 59.18 | 37.12 | 39.80 | 56.82 |\n| LLM-Pruner (Element1) | 19.09 | 57.06 | 75.68 | 66.80 | 59.83 | 60.94 | 36.52 | 40.00 | 56.69 |\n| BESA | 11.38 | 66.91 | 72.96 | 61.63 | 58.80 | 62.84 | 34.39 | 35.40 | 56.13 |\n\nIf you have any further inquiries or require additional clarification, please don't hesitate to ask.\n\n[A] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations, 2019.\n\n[B] Kusupati, Aditya and Ramanujan, Vivek and Somani, Raghav and Wortsman, Mitchell and Jain, Prateek and Kakade, Sham and Farhadi, Ali. Soft Threshold Weight Reparameterization for Learnable Sparsity. Proceedings of the International Conference on Machine Learning, 2020.\n\n[C] Xinyin Ma, Gongfan Fang, and Xinchao Wang. LLM-Pruner: On the Structural Pruning of Large Language Models. Advances in Neural Information Processing Systems, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484857229,
                "cdate": 1700484857229,
                "tmdate": 1700534416092,
                "mdate": 1700534416092,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bOgTCPRYKl",
            "forum": "gC6JTEU3jl",
            "replyto": "gC6JTEU3jl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2493/Reviewer_JpTs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2493/Reviewer_JpTs"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a blockwise model pruning framework for compressing LLMs, which searches for optimal pruning rates for each layer in a differentiable manner. The authors have done experiments on different models and datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors demonstrate the practical speedup of the pruned model in a hardware simulator.\n2. The method is parameter-efficient and easy to optimize.\n3. The authors have done extensive experiments on language modeling and few-shot learning benchmark datasets. The authors also explore models with different numbers of parameters from 7b to 70b."
                },
                "weaknesses": {
                    "value": "1. It would be better to involve the computational cost of attention weight in Table 4.\n2. It would be better to have an ablation study of block-wise pruning. Maybe directly pruning all the models instead of layer by layer.\n3. There has been a lot of research on pruning in CV and NLP. The baselines are far from complete, such as \"The Lottery Ticket Hypothesis\" and its following works. While considering not much work on LLM, I would not penalize too much on it.\n\nOverall, I think the experiments are solid. The novelty is ok, but it would be better to explore more classical pruning methods."
                },
                "questions": {
                    "value": "The model seems not significantly better than SparseGPT with a smaller number of parameters. Could you have significant test?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2493/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821804759,
            "cdate": 1698821804759,
            "tmdate": 1699636186041,
            "mdate": 1699636186041,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LlWWz8n7ik",
                "forum": "gC6JTEU3jl",
                "replyto": "bOgTCPRYKl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2493/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JpTs (part 1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful feedback. We appreciate your engagement with our rebuttal and the points you've raised. We have carefully considered each of your concerns and addressed them as follows:\n\n**Q1:** \"It would be better to involve the computational cost of attention weight in Table 4.\"\n\n**A1:** We acknowledge the importance of providing information on the computational cost of running our pruned model in ViTCoD. In response to your suggestion, we have revised Table 4 to include the FLOPs (floating-point operations) of the pruned model. The updated table now offers a more comprehensive view of the efficiency gains achieved by our method. Please refer to the revised table below.\n\n| Layer name | q_proj | k_proj | v_proj | o_proj | gate_proj | up_proj | down_proj |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| Dense Runtime | 4096 | 4096 | 4096 | 4096 | 10128 | 10128 | 10128 |\n| Average Sparse Runtime | 2232.31 | 2230.50 | 2720.59 | 2698.53 | 5207.53 | 5125.0 | 6850.03 |\n| Dense mFLOPs | 16.78 | 16.78 | 16.78 | 16.78 | 45.09 | 45.09 | 45.09 |\n| Average Sparse mFLOPs | 8.55 | 8.27 | 9.29 | 10.88 | 37.36 | 35.34 | 27.63 |\n| Speedup (Runtime) | 1.83x | 1.84x | 1.51x | 1.52x | 1.94x | 1.98x | 1.48x |\n\nTable 4: Runtime (cycles), computational cost (FLOPs), and speedup across various layers in LLaMA-7B. The term \"FLOPs\" refers to floating-point operations while \"cycles\" denotes the number of instruction cycles necessary for the ViTCoD accelerator to perform the associated computational workloads.\n\n**Q2:** \"It would be better to have an ablation study of block-wise pruning. Maybe directly pruning all the models instead of layer by layer.\"\n\n**A2:** Good suggestion. Your insightful question highlights the potential for valuable insights by examining the direct pruning performance across all models. Regrettably, conducting such an experiment proves challenging due to computational limitations, even with our exploration of state-of-the-art strategies such as [microsoft's DeepSpeed](https://github.com/microsoft/DeepSpeed), the int-8 training scheme, and the int8 optimizer provided by the [bitsandbytes library](https://github.com/TimDettmers/bitsandbytes/tree/main). We believe that directly pruning all the models instead of layer by layer could lead to better performance, which can be derived from the experiments in Table 6 and Figure 5 of the Appendix. Table 6 shows that performing our BESA with a larger learning granularity leads to better performance, which is led by the lower quantization error when pruning the model with  a larger learning granularity. Hence,  directly pruning all the models would result in a lower quantization error and thus better performance."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484153521,
                "cdate": 1700484153521,
                "tmdate": 1700534297423,
                "mdate": 1700534297423,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hInGBk4ZS1",
                "forum": "gC6JTEU3jl",
                "replyto": "bOgTCPRYKl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2493/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JpTs (part 2/3)"
                    },
                    "comment": {
                        "value": "**Q3:** \"There has been a lot of research on pruning in CV and NLP. The baselines ...\"\n\n**A3:** Thanks for the suggestion. In response, we have conducted experiments using three additional pruning techniques commonly employed in computer vision and natural language processing. These methods, namely iterative pruning (proposed in the Lottery Ticket Hypothesis [A]), threshold pruning [B], and direct mask learning, have been implemented in a blockwise manner for a fair comparison. For iterative pruning, we employ a step-wise approach where we iteratively prune the LLM block with a step of 10%. During each iteration, we update the weights with the reconstruction loss. The results are reported in the table under \"Iterative Prune\". Then for threshold pruning, we learn a threshold value for each layer within the block while maintaining the block sparsity at 50%. The results are reported in the table under \"Threshold Prune\". For direct mask learning, we directly learn a sparse mask for the LLM block. The results are reported in the table under \"Direct Mask Learning\". We opt for iterative pruning as it stands out as one of the most representative methods for pruning, incorporating weight updating. Additionally, we select threshold pruning for its cutting-edge capabilities, showcasing state-of-the-art performance in computer vision models. Lastly, we choose direct mask learning as it represents the most straightforward and directly differentiable pruning method. Notably, the original \u201cIterative prune\u201d and \u201cThreshold prune\u201d methods use the weight value as a measure of importance. In contrast, we provide the versions that use the weight or Wanda importance metric.\n\nThe perplexity results on WikiText2, C4, and PTB datasets are presented below:\n\n|  | BESA | Iterative prune (Wanda) | Iterative prune (Weight) | Threshold prune (Wanda) | Threshold prune (Weight) | Direct mask learning |\n| --- | --- | --- | --- | --- | --- | --- |\n| WikiText2 ppl | 6.86 | 6.88 | 123.48 | 2984.61 | 20351.98 | 1457.53 |\n| C4 ppl | 8.96 | 9.08 | 102.11 | 2191.59 | 21203.34 | 371.47 |\n| PTB ppl | 66.96 | 57.71 | 3929.01 | 4600.38 | 32474.87 | 3927.83 |\n\nThese results underscore the unique advantages of our method, particularly its ability to achieve substantial reductions in perplexity. We believe that this thorough exploration of differentiable methods provides a comprehensive understanding of the strengths and capabilities of BESA compared to other baselines.\n\n[A] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations, 2019.\n\n[B] Kusupati, Aditya and Ramanujan, Vivek and Somani, Raghav and Wortsman, Mitchell and Jain, Prateek and Kakade, Sham and Farhadi, Ali. Soft Threshold Weight Reparameterization for Learnable Sparsity. Proceedings of the International Conference on Machine Learning, 2020."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484260789,
                "cdate": 1700484260789,
                "tmdate": 1700534254459,
                "mdate": 1700534254459,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8WrzSh3HGI",
            "forum": "gC6JTEU3jl",
            "replyto": "gC6JTEU3jl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2493/Reviewer_qTC9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2493/Reviewer_qTC9"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel pruning technique called Blockwise Parameter-Efficient Sparsity Allocation (BESA) for compressing large language models (LLMs). BESA aims to address the computational footprint and memory consumption issues associated with LLMs by optimizing pruning rates across different layers in a differentiable manner. The proposed method achieves state-of-the-art performance in pruning various LLMs, such as LLaMA1 and LLaMA2, and efficiently prunes them on a single A100 GPU."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- BESA is the first differentiable pruning algorithm for LLMs, which allows for efficient optimization of pruning rates across layers.\n- The method is parameter-efficient and easy to optimize, exhibiting high efficiency and effectiveness in pruning various LLMs.\n- BESA achieves state-of-the-art performance in pruning various LLMs, such as LLaMA1 and LLaMA2, with reduced performance degradation after pruning.\n- The proposed method can be jointly optimized with weight-only quantization techniques, further enhancing the compression ratio and efficiency of LLMs."
                },
                "weaknesses": {
                    "value": "- The paper does not provide a detailed analysis of the trade-offs between different pruning rates and their impact on model performance, which could be useful for understanding the optimal pruning strategy.\n- The paper does not provide a comprehensive comparison of BESA with other pruning techniques, such as structured pruning, which could help in understanding the relative strengths and weaknesses of the proposed method."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2493/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698854186158,
            "cdate": 1698854186158,
            "tmdate": 1699636185977,
            "mdate": 1699636185977,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P4f0NKMfhX",
                "forum": "gC6JTEU3jl",
                "replyto": "8WrzSh3HGI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2493/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qTC9"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful feedback. We appreciate your engagement with our rebuttal and the points you've raised. We have carefully considered each of your concerns and addressed them as follows:\n\n**Q1:** \"The paper does not provide a detailed analysis of the trade-offs between different pruning rates and their impact on model performance, ... \"\n\n**A1:** We apologize for any confusion caused by the placement of ablation studies in our paper. On page 7, in Section 4.2, we discussed and conducted experiments with varying sparsities, which are visually represented in Fig. 3. However, we acknowledge that the figure is presented on page 12 due to the limit of space. From Fig.3, we can see that BESA consistently outperforms other baselines such as SparseGPT and Wanda. The gap is particularly pronounced at large sparsities.\n\n**Q2:** \"The paper does not provide a ... structured pruning ... of the proposed method.\"\n\n**A2:** Your suggestion to include a comparison with structured pruning methods is valid, and we appreciate the insight. Therefore, we conducted experiments about comparing our method with LLM-Pruner [A], the latest structured pruning algorithm we found. After checking the experiments shown in LLM-Pruner, we observed a dramatic decrease in both WikiText2 perplexity and zero-shot performance of some downstream tasks with structured pruning. For a fair comparison, we reimplemented our method in the manner of structured pruning. Specifically, we block-wise pruned the LLM model in the dimension of the attention module's head and the MLP module's hidden dimension. The detailed comparison results are shown in Table 1. The experiment was performed in LLaMA-7B with a sparsity of 20% without tuning. From the table below, it is evident that our method can reduce WikiText2 perplexity by at least 40% while achieving comparable performance in zero-shot downstream tasks compared to LLM-Pruner's different weight importance settings.\n\n| Methods | WikiText2 ppl | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e | ARC-c | OBQA | Average |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| LLM-Pruner (Vector) | 22.28 | 61.44 | 71.71 | 57.27 | 54.22 | 55.77 | 33.96 | 38.40 | 53.25 |\n| LLM-Pruner (Element2) | 19.77 | 59.39 | 75.57 | 65.34 | 61.33 | 59.18 | 37.12 | 39.80 | 56.82 |\n| LLM-Pruner (Element1) | 19.09 | 57.06 | 75.68 | 66.80 | 59.83 | 60.94 | 36.52 | 40.00 | 56.69 |\n| BESA | 11.38 | 66.91 | 72.96 | 61.63 | 58.80 | 62.84 | 34.39 | 35.40 | 56.13 |\n\nTable 1: Evaluation of WikiText2 perplexity (ppl, lower is beter) and performance on zero-shot downstream tasks (higher is better) for LLaMA-7B at 20% sparsity without tuning the pruned model.\n\n[A] Xinyin Ma, Gongfan Fang, and Xinchao Wang. LLM-Pruner: On the Structural Pruning of Large Language Models. Advances in Neural Information Processing Systems, 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483937711,
                "cdate": 1700483937711,
                "tmdate": 1700483937711,
                "mdate": 1700483937711,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GXhzGGYe9J",
            "forum": "gC6JTEU3jl",
            "replyto": "gC6JTEU3jl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2493/Reviewer_VXVS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2493/Reviewer_VXVS"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the weight pruning problem of large language models. In order to solve the problems of significant output disturbance and the need for careful hyperparameters tuning in existing layer-wise methods, a novel LLM pruning technique named block-wise parameter-efficient sparsity allocation (BESA) is proposed, with two distinctive characters: minimizing pruning error for individual blocks and ensuring the layer-specific sparsity differentiable. Finally, this paper verified the performance and efficiency of the method through detailed experiments on strong baselines."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper introduces a novel approach, BESA, for compressing Large Language Models through block-wise pruning with a differentiable sparsity allocation, which maintains the performance of LLMs well and improves computational efficiency compared to existing methods. Besides, this paper is well written, clearly explaining the method of block-wise tuning and parameter-efficient sparsity learning through detailed mathematical and textual expression. Finally, credible experimental design and solid experimental results and increase the credibility of the paper."
                },
                "weaknesses": {
                    "value": "There may be a few things that need to be modified or clarified clearly. In page 3 BLOCK-WISE PRUNING equation (1), it is better to add the meaning of \u201cW\u201d together with \u201cM, F, X, \u2026\u201d; Since the article mentioned that existing methods require meticulous hyperparameter tuning, adding the sensitivity of some vital hyperparameters of the proposed model and clarify the advantage in the appendix will make this paper more convincing."
                },
                "questions": {
                    "value": "1)Considering the abstract mentions existing methods require meticulous hyperparameter tuning, it would be better to study the vital hyperparameter\u2019s sensitivity of the proposed method and experiments are needed, which will make this paper more convincing. \n2) Due to the uniqueness of the proposed method BESA, which seeks the optimal pruning rate for each layer, compared to existing methods, will implementing specialized neural network accelerators (ViTCoD in the experiments) consume additional time or make model faster than others?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2493/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699195082380,
            "cdate": 1699195082380,
            "tmdate": 1699636185889,
            "mdate": 1699636185889,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GtgkjnBORk",
                "forum": "gC6JTEU3jl",
                "replyto": "GXhzGGYe9J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2493/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VXVS"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful feedback. We appreciate your engagement with our rebuttal and the points you've raised. We have carefully considered each of your concerns and addressed them as follows:\n\n**Q1:** \"There may be a few things ... add the meaning of \u201cW\u201d together with \u201cM, F, X, \u2026\u201d;\"\n\n**A1:** Sorry for any confusion caused by our paper. To clarify, \"W\" represents all linear weights in a transformer block, \"M\" refers to the learnable binary mask of these weights, \"X\" means the input tokens, and \"F\" is the mapping function that gets the forward result of given \"W\" and \"X\".\n\n**Q2:** \"Considering the abstract mentions existing methods ... this paper more convincing.\"\n\n**A2:** Thank you for pointing it out. We provide a comprehensive ablation study of the hyperparameters used in our method on page 12, Appendix Sec. A, which is inclusive of the size of the calibration set, epochs, the step of sparsity candidates, learning granularity, and importance metric.\n\n**Q3:** \"Due to the uniqueness of the proposed method BESA, ... make model faster than others?\"\n\n**A3:** Good question. Comparing the runtime of the model pruned by our method and other methods in the ViTCoD accelerator can make our experiments more reasonable and convincing. An updated version of Table 4 in our original paper is shown below.\n\n| Layer name | q_proj | k_proj | v_proj | o_proj | gate_proj | up_proj | down_proj |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| Dense Runtime | 4096 | 4096 | 4096 | 4096 | 10128 | 10128 | 10128 |\n| Average Runtime (SparseGPT) | 2952.22 | 2932.0 | 3041.31 | 2950.13 | 7941.88 | 7865.81 | 7441.44 |\n| Average Runtime (Wanda) | 2887.84 | 2871.34 | 3000.91 | 2461.59 | 7701.41 | 7670.84 | 7388.97 |\n| Average Runtime (BESA) | 2232.31 | 2230.50 | 2720.59 | 2698.53 | 5207.53 | 5125.0 | 6850.03 |\n| BESA Speedup | 1.83x | 1.84x | 1.51x | 1.52x | 1.94x | 1.98x | 1.48x |\n\nTable 4: Runtime (cycles) and speedup across various layers in LLaMA-7B. The term \"cycles\" denotes the number of instruction cycles necessary for the ViTCoD accelerator to perform the associated computational workloads.\n\n\nBased on the results shown in the previous table, although our method shares the same sparsity with other  methods, we obtain better performance in the ViTCoD accelerator because of our nonuniform sparsity within the block. As shown in Appendix Sec.B, ViTCoD incorporates a dense engine and a sparse engine to perform computation simultaneously and assign the computation workloads to different engines with respect to their sparsity, the model pruned by our methods can have a more balanced workload assignment."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483814700,
                "cdate": 1700483814700,
                "tmdate": 1700483839143,
                "mdate": 1700483839143,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]