[
    {
        "title": "Characterizing Robust Overfitting in Adversarial Training via Cross-Class Features"
    },
    {
        "review": {
            "id": "sgyggmXgLa",
            "forum": "ncbDXOdURn",
            "replyto": "ncbDXOdURn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission843/Reviewer_iycg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission843/Reviewer_iycg"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors primarily concentrate on addressing the robust overfitting problem. In pursuit of this objective, they identify intriguing properties within both cross-class and class-specific features. Then, both empirical and theoretical evidence show that knowledge distillation helps mitigate robust over\ufb01tting by preserving the observed properties. The experiments further verify this."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Understanding robust overfitting via cross-class and class-specific features is interesting.\n1. The empirical and theoretical evidence could support the recent common sense that knowledge distillation is beneficial to mitigate robust overfitting.\n1. The experiments show that a weight-average guided knowledge distillation method can boost robustness."
                },
                "weaknesses": {
                    "value": "1. In Eq. 10, the authors use label smoothing to simulate knowledge distillation. I am wondering if this simulation is reasonable. The authors only said that ''due to symmetry'', but it is difficult to understand. \n\n1. On page 6, the authors claimed that \"... derive a large enough logit\" and \"... Since the final checkpoint makes decisions based only on these limited features, the logit for the correct class is not large enough\". These two conclusions are very difficult to understand because I find it challenging to infer them from the existing experiments. Figure 4 only shows the correlation matrix which cannot reflect the logit information.\n\n1. In Equation 7, why do the class-specific or cross-class features only appear in robust features?\n\n1. The results shown in Section 4.2 are somehow weak I think. For example, it is difficult to see that feature $x_{E,i}$ will dominate the whole training objective when $w_1>0$.\n\n1. There are many adversarial robust distillation methods in previous works, so it's better to compare them in details like [a].\n\n[a] Revisiting Adversarial Robustness Distillation: Robust Soft Labels Make Student Better. ICCV 2021."
                },
                "questions": {
                    "value": "Apart from the issues mentioned in the Weaknesses section, there is another concern: this paper mainly focuses on robust overfitting. How about catastrophic overfitting in Fast-AT?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission843/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698664858296,
            "cdate": 1698664858296,
            "tmdate": 1699636011809,
            "mdate": 1699636011809,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PsRtEnliPk",
                "forum": "ncbDXOdURn",
                "replyto": "sgyggmXgLa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer iycg (part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback. We have revised our manuscript based on your comments and address your concerns as follows.\n\n---\n\n**Q1**: In Eq. 10, the authors use label smoothing to simulate knowledge distillation. I am wondering if this simulation is reasonable. The authors only said that ''due to symmetry'', but it is difficult to understand.\n\n**A1**: Thanks for pointing that out. In this context, the term 'symmetry' specifically refers to the symmetry of logits for the other two classes when taking the expectation in the loss function (equation 10). When considering data from class $y$, both the distribution of features $x_{E,i}$  and $x_{C_i}$ for the other two classes, as well as their respective weights $w_1$ and $w_2$, exhibit symmetry respectively. Consequently, after applying knowledge distillation, the expectation for logits of the other two classes in the objective loss function (equation 10) becomes identical. To simplify this process, we can employ label smoothing. \n\nWe have incorporated this discussion into our revision.\n\n---\n\n**Q2**: On page 6, the authors claimed that \"... derive a large enough logit\" and \"... Since the final checkpoint makes decisions based only on these limited features, the logit for the correct class is not large enough\". These two conclusions are very difficult to understand because I find it challenging to infer them from the existing experiments. Figure 4 only shows the correlation matrix which cannot reflect the logit information.\n\n**A2**: Thank you for your careful reading. We have toned down such claims in the updated paper as\n\n> Since the final checkpoint makes decisions based only on these limited features, it fails to leverage comprehensive features for classification, making the model more vulnerable to adversarial attacks on these samples.\n\n---\n\n**Q3**: In Equation 7, why do the class-specific or cross-class features only appear in robust features?\n\n**A3**: Thanks for your careful reading. We want to emphasize that our claim does not assert that features of this nature exclusively appear in robust features. As clarified earlier, our data model, inspired by [1], primarily centers on the inherent relationship between these two types of robust features, and non-robust features do not affect our main claims. To illustrate, consider a non-robust, class-specific feature $x_n$ with a Gaussian mean $\\eta$ (as introduced in [1]), where $\\eta$ is a small positive value. For adversarial training with perturbation bound $\\epsilon>\\eta$ (generally holds since $\\eta$ is small), the weight $w_n$ for $x_n$ will be set to 0, as demonstrated by the proof of Theorem 1. Consequently, under adversarial training, $x_n$ does not exert an influence on the decision logic. This same conclusion extends to cross-class features as well.\n\n[1] Robustness may be at odds with accuracy. ICLR 2019\n\n---\n\n**Q4**: The results shown in Section 4.2 are somehow weak I think. For example, it is difficult to see that feature $x_{E,i}$ will dominate the whole training objective when $w_1>0$.\n\n**A4**: Thank you for your valuable insights. We would like to clarify that the assumption regarding the dominance of $x_{E,i}$ is primarily supported by our empirical findings, which reveal a gradual forgetting of cross-class features $x_{C,i}$ and an increasing dominance of class-specific features $x_{E,i}$ during adversarial training (AT). These observed tendencies of these features during AT lend credibility to the reasonable supposition that feature $x_{E,i}$ will indeed dominate the training objective."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512398821,
                "cdate": 1700512398821,
                "tmdate": 1700512398821,
                "mdate": 1700512398821,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "378OdqJYJn",
                "forum": "ncbDXOdURn",
                "replyto": "sgyggmXgLa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer iycg (part 2/2)"
                    },
                    "comment": {
                        "value": "**Q5**: There are many adversarial robust distillation methods in previous works, so it's better to compare them in details like [a].\n\n**A5**: Thank you for your thoughtful suggestions. The primary distinction between our method and adversarial robust distillation methods lies in their respective purposes and settings. Our method is specifically designed to address robust overfitting, whereas robust distillation methods are focused on distilling adversarial robustness from pre-trained teacher models. Furthermore, it's noteworthy that our proposed method is entirely trained from scratch, in contrast to robust distillation methods that require a pre-trained teacher.\n\n---\n\n**Q6**: This paper mainly focuses on robust overfitting. How about catastrophic overfitting in Fast-AT?\n\n**A6**: Thank you for offering this valuable perspective. It's essential to note that the majority of existing papers [2-4] on understanding robust overfitting do not explicitly delve into the topic of catastrophic overfitting in the context of Fast-AT, which indeed remains an open research area.\n\nIn line with your suggestion, we have extended our investigations to include Fast-AT for the CIFAR-10 dataset, employing an $\\ell_\\infty$-norm perturbation bound of $\\epsilon=8/255$. The detailed results are presented in **Appendix D.6**. Notably, after catastrophic overfitting, there is a significant reduction in the usage of cross-class features. This observation aligns with our understanding, indicating that the model also tends to forget cross-class features after experiencing catastrophic overfitting.\n\n---\n\n[a] Revisiting Adversarial Robustness Distillation: Robust Soft Labels Make Student Better. ICCV 2021\n\n[2] Understanding Robust Overfitting of Adversarial Training and Beyond. ICML 2022\n\n[3] On the Onset of Robust Overfitting in Adversarial Training. arxiv preprint\n\n[4] Understanding and combating robust overfitting via input loss landscape analysis and regularization. Pattern Recognition"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512444839,
                "cdate": 1700512444839,
                "tmdate": 1700512444839,
                "mdate": 1700512444839,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZGSyaixYey",
                "forum": "ncbDXOdURn",
                "replyto": "sgyggmXgLa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Could you please have a look at our rebuttal?"
                    },
                    "comment": {
                        "value": "Dear Reviewer iycg, thanks for your time reviewing our paper. We have meticulously prepared a detailed response addressing your concerns and revised our paper accordingly. Could you please have a look to see if there are further questions? Your invaluable input is greatly appreciated. Thank you once again, and we hope you have a wonderful day!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696436496,
                "cdate": 1700696436496,
                "tmdate": 1700696436496,
                "mdate": 1700696436496,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eKguNbvkjs",
            "forum": "ncbDXOdURn",
            "replyto": "ncbDXOdURn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission843/Reviewer_quy6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission843/Reviewer_quy6"
            ],
            "content": {
                "summary": {
                    "value": "As claimed in this paper, robust overfitting, which although has been widely discussed, has not been fully understood. In this paper a new interpretation in the view of feature attribution is proposed and then a follow-up method is designed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Robust overfitting is an interesting phenomena happened in adversarial training. Investigating it in the view of feature attribution may bring new understanding for it."
                },
                "weaknesses": {
                    "value": "- Robust overfitting is specific to adversarial training. Thus, the discussion should be emphasized more on special properties of adversarial attack/examples/training. However, the idea given in this paper fails in this aspect: in regular training, it is also the case that the model first learn cross class information and then focusing on a specific model, e.g., the well-known neural collapse saying that the features of examples of one class will collapse to a direction.\n\n- The unclearness to the regular training also could be observed in their proposed method: stochastic weight averaging (SWA) is a standard technique to enhance generalization capability in regular training. In the current experiment, it is hard to say the advantage of the proposed WAKE comes from SWA or it comes specific property of adversarial training, then now it cannot well support the main contribution.\n\n- Another weakness is that the metric used to support the conclusion is not very convincing. For example, it is believed that features in different layers capture different information, e.g., cross-class or in-class information. Simply put all features together seems too weak."
                },
                "questions": {
                    "value": "- I want to see specific properties in adversarial training. Thus, could I see the performance of the same experiment but on regular training?\n\n- There are many metric that can measure the information about class and examples learned by DNNs. Could the authors use other metric and also observe the similar phoneme as measured by CAS\uff1f"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission843/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764022815,
            "cdate": 1698764022815,
            "tmdate": 1699636011720,
            "mdate": 1699636011720,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fvJsTvnIBQ",
                "forum": "ncbDXOdURn",
                "replyto": "eKguNbvkjs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer quy6"
                    },
                    "comment": {
                        "value": "Thank you for your detailed and valuable feedback. We have revised our manuscript based on your comments and addressed your concerns as follows.\n\n---\n\n**Q1**: Robust overfitting is specific to adversarial training. Thus, the discussion should be emphasized more on special properties of adversarial attack/examples/training. However, the idea given in this paper fails in this aspect: in regular training, it is also the case that the model first learn cross class information and then focusing on a specific model, e.g., the well-known neural collapse saying that the features of examples of one class will collapse to a direction.\n\n**A1**: Thank you for reminding this viewpoint. \n\nFirst, we want to emphasize that we have indeed taken into account the distinction in the explanation for robust overfitting between adversarial training and regular training. In Section 3.3 and illustrated in Figure 3, our experiments encompass different values of $\\epsilon$, with a small $\\epsilon$ (2/255) representing a scenario closer to regular training. Gradually increasing $\\epsilon$ can be seen as a transition towards more rigorous adversarial training. Notably, the significance of cross-class feature forgetting becomes more pronounced as $\\epsilon$ increases. This observation aligns with the phenomenon that robust overfitting is uniquely evident in adversarial training scenarios.\n\nFollowing your suggestions, we have extended our experimental scope to include regular training on the CIFAR-10 dataset. The experimental settings mirror those outlined in Section 5, with the sole distinction being the absence of perturbations in regular training. The results are presented in **Appendix D.5**. Specifically, considering that regular training prioritizes natural generalization and exhibits minimal robustness, we have calculated the feature attribution vectors **using clean examples**. These vectors were computed for epochs $\\\\{50, 100, 150, 200\\\\}$. Notably, the results reveal a lack of clear differences between them, particularly in the latter stages (150th and 200th), where the training tends to converge. This observation is consistent with the characteristic of regular training, which typically does not exhibit overfitting.\n\n---\n\n**Q2**: The unclearness to the regular training also could be observed in their proposed method: stochastic weight averaging (SWA) is a standard technique to enhance generalization capability in regular training. In the current experiment, it is hard to say the advantage of the proposed WAKE comes from SWA or it comes specific property of adversarial training, then now it cannot well support the main contribution.\n\n**A2**: Thank you for pointing this out. It's essential to recall that in our comparison with the baseline KDWSA [1], which combines knowledge distillation (KD) and stochastic weight averaging (SWA), the performance of WAKE is notably superior to KDSWA, not to mention using SWA alone. To provide further clarification, we have added a table below for a direct comparison. The experimental settings are identical to those in Table 1 for the CIFAR-10 dataset. Consequently, it is clear that the improvement achieved by WAKE is not solely attributed to SWA; rather, it stems from the specific properties of adversarial training.\n\n| Method | Robust Acc (Best) | Robust Acc (Last) |\n| --- | --- | --- |\n| AT+KD | 48.6 | 46.0 |\n| AT+SWA | 49.2 | 47.1 |\n| AT+KDSWA | 49.8 | 49.6 |\n| AT+WAKE | 50.4 | 50.1 |\n\n---\n\n**Q3**: Another weakness is that the metric used to support the conclusion is not very convincing. For example, it is believed that features in different layers capture different information, e.g., cross-class or in-class information. Simply put all features together seems too weak.\n\n**A3**: Thanks for your kind suggestion. Firstly, it's important to clarify that CAS does not aggregate all features. Instead, CAS exclusively utilizes the features from the penultimate layer, just before the linear classification head. This selection is based on the understanding that features in this layer hold the highest influence on the classification logits, providing a clear reflection of the decision-making process.\n\n**Q4**: There are many metric that can measure the information about class and examples learned by DNNs. Could the authors use other metric and also observe the similar phoneme as measured by CAS\uff1f\n\n**A4**: We appreciate your suggestion regarding alternative metrics to quantify the usage of cross-class features, but we do not know which metric you are referring. Could you clarify which specific metric you are referring? This will assist us in addressing your suggestion more accurately.\n\n---\n\n[1] Robust overfitting may be mitigated by properly learned smoothening. ICLR 2021\n\n---\n\nWe truly appreciate your valuable and detailed feedback. If you have any further questions or concerns, please let us know."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512295628,
                "cdate": 1700512295628,
                "tmdate": 1700512295628,
                "mdate": 1700512295628,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ff7pXUR3Kv",
                "forum": "ncbDXOdURn",
                "replyto": "fvJsTvnIBQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission843/Reviewer_quy6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission843/Reviewer_quy6"
                ],
                "content": {
                    "title": {
                        "value": "thanks for the explanation"
                    },
                    "comment": {
                        "value": "Thanks for the authors' response, especially the clarification on which features are used and the direct comparison. As the supplementary table shows, the proposed WAKE is better than others, however not significant, and more importantly the improvement does not come from decreasing the generalization gap. I think all the reviewers have consistent concerns about the motivation, the algorithm, and the experiments. So I would like to keep my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580640691,
                "cdate": 1700580640691,
                "tmdate": 1700580640691,
                "mdate": 1700580640691,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OilYhkLnDg",
            "forum": "ncbDXOdURn",
            "replyto": "ncbDXOdURn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission843/Reviewer_NSUP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission843/Reviewer_NSUP"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the effects of cross-class features and specific-class features to the performance of adversarial training. The conclusion is that the cross-class features are more robust and important to the robust accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The empirical finding is interesting and possibly helpful in practice to improve the performance of adversarial training."
                },
                "weaknesses": {
                    "value": "- The theories developed are very restrictive and under extremely unrealistic assumptions\n   -  Data have 6 features and each features follow a Gaussian with mean zero.\n   -  It only considers a linear model with two parameters $w_1$ for the exclusive feature and $w_2$ for the shared feature.\n\n- The theoretical results obtained are humble and do not really reflect the empirical finding. Moreover, the statement of Theorem 2 is ambiguous. It is not clear what it does mean by \"a larger w2 increases the possibility of the model distinguishing the adversarial examples from any other given class\".\n\n- When taking average of feature vectors for a class to visualize the matrix $C$ and CAS, this cannot reflect the tendency/behavior of each feature vector. To me, a better way is to compute and visualize instance-wisely.\n\n- The proposed practical method lacks novelty and does not have a strong connection to theory developed and experiments are humble."
                },
                "questions": {
                    "value": "What does it mean by \"a larger w2 increases the possibility of the model distinguishing the adversarial examples from any other given class\" in Theorem 2?\n\nCan you compute and visualize C and CAS instance-wisely? For example, do it for a pair of feature vectors in class i and class j, and then take average."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission843/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793107126,
            "cdate": 1698793107126,
            "tmdate": 1699636011610,
            "mdate": 1699636011610,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "49NJSLVw4s",
                "forum": "ncbDXOdURn",
                "replyto": "OilYhkLnDg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NSUP (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your detailed and valuable feedback. We have revised our manuscript based on your comments and addressed your concerns as follows.\n\n---\n\n**Q1**: The theories developed are very restrictive and under extremely unrealistic assumptions.\n\n  - Data have 6 features and each features follow a Gaussian with mean zero.\n  - It only considers a linear model with two parameters\u00a0$w_1$\u00a0for the exclusive feature and\u00a0$w_2$\u00a0for the shared feature.\n\n**A1**: We appreciate your feedback, but we have to point out this is an unjustified criticism to our paper. \n\n- Firstly, our choice of data distribution is rooted in a well-established model from [1], which separates robust and non-robust features through similar Gaussian distributions. This model has been widely adopted in subsequent studies, such as [2], for the analysis of adversarial robustness.\n- Moreover, though [1] considers $d$ non-robust features, they simplified the linear function due to symmetry and they only use **2 parameters** (one for robust features and another for non-robust features) in the **linear model**. Therefore, we believe modeling the **6 features** among 3 classes is sufficient to analyze the relationship between cross-class features and adversarial robustness.\n- Additionally, in the realm of adversarial robustness, many theoretical analyses, including but clearly not limited to [3,4,5], are also based on **linear models.** It is worth noting that linear models are sufficient to convey valuable insights, and we don't perceive the analysis within a linear model as a weakness of our paper.\n\n---\n\n**Q2**: The theoretical results obtained are humble and do not really reflect the empirical finding. Moreover, the statement of Theorem 2 is ambiguous. It is not clear what it does mean by \"a larger w2 increases the possibility of the model distinguishing the adversarial examples from any other given class\".\n\n**A2**: We appreciate your feedback, but the theoretical results are well aligned with the empirical findings.\n\n- Theorem 1 explains why larger $\\epsilon$ in adversarial training forgets more cross-class features by showing cross-class features are more sensitive to robust loss in adversarial training, which is consistent with Figure 3.\n- Theorem 2 explains why forgetting cross-class features leads to a decrease of robustness by showing larger weight of cross-class features $w_2$ can bring better robustness, which is consistent with the overall finding of this paper.\n- Theorem 3 and Corollary 1 show how knowledge distillation mitigates robust overfitting which is consistent with Figure 4(b)(c).\n\nAs for theorem 2, following your feedback, we have refined the claim in Theorem 2 to make it clearer in our revised paper, and we copied it below:\n\n>  For any class $y$, consider weights $w_1 > 0$, $w_2 \\in [0, w_1]$, and $\\epsilon \\in (0, \\frac{\\mu}{2})$. When sampling $x$ from the distribution of class $y$, increasing the value of $w_2$ enhances the possibility of the model assigning a higher logit to class $y$ than to any other classes $y'\\ne y$ under adversarial attack. In other words, the probability $\\Pr_{x\\sim\\mathcal D_y}[f_w(x+\\delta))\\_{y}>f_w(x+\\delta)\\_{y'},\\forall \\delta:\\|\\delta\\|_\\infty\\le\\epsilon]$ monotonically increases with $w_2$ within the range $[0, w_1]$.\n\n\n\n---\n\n[1] Robustness may be at odds with accuracy. ICLR 2019\n\n[2] Adversarial Examples Are Not Bugs, They Are Features. NeurIPS 2019\n\n[3] Theoretically Principled Trade-off between Robustness and Accuracy. ICML 2019\n\n[4] To be Robust or to be Fair: Towards Fairness in Adversarial Training. ICML 2020\n\n[5] On the Tradeoff Between Robustness and Fairness. NeurIPS 2023"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512064202,
                "cdate": 1700512064202,
                "tmdate": 1700512064202,
                "mdate": 1700512064202,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HurOGMPBbk",
                "forum": "ncbDXOdURn",
                "replyto": "OilYhkLnDg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NSUP (2/2)"
                    },
                    "comment": {
                        "value": "**Q3**: When taking average of feature vectors for a class to visualize the matrix and CAS, this cannot reflect the tendency/behavior of each feature vector. To me, a better way is to compute and visualize instance-wisely.\n\n**A3**: Thank you for your thoughtful suggestion. Our choice of using vectors over classes is based on the belief that averaging over samples can yield a more comprehensive and holistic understanding of the attribution for different classes.\n\nFollowing your suggestion, we additionally calculated feature attribution correlation matrices on an instance-wise basis for both $\\ell_\\infty$ and $\\ell_2$ adversarial training. The results are presented in **Appendix D.4**. To elaborate, when considering classes $i$ and $j$, for each sample $x$ from class $i$, we identify its most similar sample $x'$ from class $j$.  We then calculate their cosine similarity and average the results over all samples in class $i$. \n\nIn this context, $x'$ can be interpreted as the sample in class $j$ that shares the most cross-class features with $x$ among all samples in class $j$. This metric provides a meaningful way to quantify the utilization of cross-class features. We did attempt to average over all sample pairs $(x, x')$ in classes $i$ and $j$, but due to high variance among samples, each element in the correlation matrix $C$ hovered near 0 throughout all epochs in adversarial training, rendering it unable to provide meaningful information.\n\nConsistent with the results for class-wise attribution vectors, it is still observed that there is a significant decrease in the usage of cross-class features from the best checkpoint to the last for both $\\ell_\\infty$ and $\\ell_2$-AT. This observation further substantiates our understanding of robust overfitting.\n\n---\n\n**Q4**: The proposed practical method lacks novelty and does not have a strong connection to theory developed and experiments are humble\n\n**A4**: \n\n- In terms of novelty, it's crucial to highlight a key distinction of our method from existing distillation methods. Unlike approaches like KDWSA, which necessitates a pre-trained teacher model, our proposed method, WAKE, stands out in its efficiency and ease of implementation. WAKE does not rely on a pre-trained teacher model, making it a more straightforward and resource-efficient solution. Regarding your criticism on novelty, could you kindly specify any existing works that significantly overlap with our proposed WAKE?\n- In terms of connection to theory, in this paper, we showed the importance of cross-class features for improving adversarial robustness, and the effectiveness of knowledge distillation to preserve such features. Thus, a natural solution is finding a better distillation teacher model for more precise cross-class feature information. This shows a clear connection between our theory and the proposed method.\n- In terms of experiments, we also conducted comprehensive experiments which are shown in the Appendix. First, we delved into empirical investigations to uncover the relationship between cross-class features and robust overfitting (as detailed in Appendix D). Additionally, we conducted a series of experiments for WAKE (as detailed in Appendix F). These encompassed examinations involving $\\ell_2$-norm, synergies with other adversarial training methods, and the exploration of alternative model architectures.\n\n---\n\nWe truly appreciate your valuable and detailed feedback. If you have any further questions or concerns, please let us know."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512091275,
                "cdate": 1700512091275,
                "tmdate": 1700512982595,
                "mdate": 1700512982595,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zY1RcbyfYt",
                "forum": "ncbDXOdURn",
                "replyto": "OilYhkLnDg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Could you please have a look at our rebuttal?"
                    },
                    "comment": {
                        "value": "Dear Reviewer NSUP, thanks for your time reviewing our paper. We have meticulously prepared a detailed response addressing your concerns and revised our paper accordingly. Could you please have a look to see if there are further questions? Your invaluable input is greatly appreciated. Thank you once again, and we hope you have a wonderful day!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696379211,
                "cdate": 1700696379211,
                "tmdate": 1700696379211,
                "mdate": 1700696379211,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2fOeoL4Rr5",
                "forum": "ncbDXOdURn",
                "replyto": "49NJSLVw4s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission843/Reviewer_NSUP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission843/Reviewer_NSUP"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your feedback"
                    },
                    "comment": {
                        "value": "Thanks the authors for answers my questions. Currently, I am keen on to keep my score. The reasons include:\n- I am not very convincing of using $A_i(x)$ to represent the features in $g(x)$ that characterize the class $i$. I agree that the if $x$ has the class $i$ and the classifier correctly predicts $x$, many value $W_{ij}g(x)_j$ receive remarkable values. Moreover, to compute $A^y$, you attack the test examples $x$ in the class $y$ to gain $x^a$, evaluate the attribution vector of $x^a$ for the class $y$ (i.e., $A_y(x^a)$), and take average of them to yield $A^y$. My concern is that after attacking, $x^a$ has the predicted label $y^a \\neq y$, hence if $A_y(x^a)$ makes sense in this context. Moreover, you take average of all $A_y(x^a)$ where $x^a$ can be attacked to different labels. It is hard for me to interpret the meaning of this average.\n-  I cannot classify this work to a work with strong contributions to theory or proposing a novel practical method. With respect to the first perspective, the assumptions made in this work are super-unrealistic to me, while the proposed practical method is limited in novelty with just small improvements comparing to AT + KDSWA."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696408055,
                "cdate": 1700696408055,
                "tmdate": 1700696408055,
                "mdate": 1700696408055,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZCmrlfxhTm",
            "forum": "ncbDXOdURn",
            "replyto": "ncbDXOdURn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission843/Reviewer_fn47"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission843/Reviewer_fn47"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of robust overfitting in adversarial training (AT), where during adversarial training, the test robust accuracy of a model gradually decreases after a certain epoch (checkpoint) although the training robust accuracy continues to increase. This leads to a large robust generalization gap. The paper first provides a novel explanation for this phenomenon from the perspective of feature attribution. They divide the features learned by a model (robust DNN classifier) into `class-specific or exclusive features` and `cross-class features`. As their names suggest, class-specific features are informative for the prediction of a specific class, whereas cross-class features are informative for the prediction of multiple classes. \n\nThey provide empirical evidence to show that models trained using AT tend to rely more on cross-class features when observed at the best checkpoint (epoch where the test robust accuracy is highest). However, when robust overfitting occurs at later epochs/checkpoints, they observe that the model tends to rely less on cross-class features and more on class-specific features. In order to quantify the cross-class feature usage, they propose a feature attribution vector (per sample and per class) and a feature attribution correlation matrix. The feature attribution correlation matrix helps visualize the extent to which a model depends on cross-class features, and it is summarized by a metric named class attribution similarity (CAS). By observing the correlation matrix and CAS metric and at the best checkpoint and the final checkpoint across a number of datasets, architectures, and norm types, they conclude that there is consistently a higher dependence on cross-class features at the best checkpoint, but this dependence decreases at the final checkpoint, leading to robust overfitting. Therefore, it is crucial to preserve the model\u2019s dependence on cross-class features to mitigate this phenomenon. \n\nThey provide a theoretical analysis to back this observation, based on a simple linear model with Gaussian class-conditional distributions and decoupled class-specific and cross-class features. Finally, they show that knowledge distillation can be effective at preserving the cross-class features. Therefore, they propose a knowledge distillation based adversarial training method where a weight-averaged model acts as the teacher for knowledge distillation. This method is shown to have improved adversarial robustness in their experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The explanation for robust overfitting based on cross-class and class-specific features is intuitively clear and is backed well by empirical results. \n\n2. The visualization of robust overfitting using the feature attribution correlation matrix and the CAS metric are useful tools for understanding this phenomenon. Sections 3.2 and 3.3 do a good job of considering multiple scenarios to understand the effect of cross-class features, and have pointers to the Appendix for additional results.\n\n3. The weight average guided knowledge distillation method for adversarial training mitigates the issue  of robust overfitting and has improved performance on multiple datasets and architectures."
                },
                "weaknesses": {
                    "value": "1. The theoretical analysis section is weak for the following reasons: lack of clarity in the presentation and theorem statements. The synthetic model is quite simple and can be presented in a better way (see my comments in the `Questions` section).\n\n2. The proposed weight average guided knowledge distillation method (WAKE) is not clearly described. Is the weight-averaged model $\\bar{\\theta}$ created on the fly during adversarial training, similar to the self ensemble adversarial training (SEAT) method of (Wang & Wang, 2022)? The piecewise linear scheduling for $\\lambda$ should be described precisely. More clarity is needed on these aspects, and an algorithm block would also be helpful (appendix can be used if space is limited).  \n\n3. Minor: experiments in the main paper are limited, and only two baselines are used for comparison. However, there are more experiments and other baselines like TRADES in the appendix. \n\n4. Code has not been made available. \n\nWould consider raising my score if Sections 4 and 5 are improved.\n\n**UPDATE:** Increased my score to 6 after reading the author responses and revised paper."
                },
                "questions": {
                    "value": "1. In Eqn (3), the max over the perturbation $\\delta$ should include all three loss terms. It seems like the max is only over the cross-entropy loss term. Please make it clear by moving the $(1 - \\lambda_1 - \\lambda_2)$ term inside the max and using parentheses around the three loss terms. \n\n2. The terms $\\ell_{CE}$ and $\\mathcal{KD}$ in Eqn (3) should be defined as the cross entropy loss and the Kullback-Leibler divergence respectively. \n\n3. Nit: the $\\times$ operator is not needed in Eqn (4).\n\n4. First paragraph of section 3.1:\n    - The notation $f(\\cdot) = W \\circ g(\\cdot)$ seems incorrect. It should be $f(\\cdot) = W g(\\cdot)$, i.e. a matrix multiplication. \n    - Please define the notation that $W[i]^T$ is the $i$-th row of $W$. \n    - The dot is not needed for inner products and scalar products. For instance, it can be $f(x)_i = W[i]^T g(x)$ rather than $f(x)_i = W[i]^T \\cdot g(x)$. Similarly, it can be $g(x)_j \\\\,W[i, j]$ rather than $g(x)_j \\cdot W[i, j]$.\n\n5. Referring to the subsection `Knowledge distillation mitigates robust overfitting`, the following statement is not clear: \u201c. . . using knowledge distillation can preserve the weight of these features by smoothing the labels of the overlapping classes\u201d. \n\n### On Section 4\nThe description of the synthetic model is not clear in Section 4.1. Some suggestions below:\n\n6. Use of $i$ to index the class is confusing. Instead, $y$ could be used to denote the class and $i$ or $j$ to index the features.\n\n7. It is mentioned that the data distribution $\\mathcal{D}_i = \\\\{ x\\_{Ej}, x\\_{Cj} \\\\,|\\\\, 1 \\leq j \\leq 3 \\\\} \\in \\mathrm{R}^6$. How can the data distribution be a set of features which lives in $\\mathrm{R}^6$? I can follow what is implied, but it is not formally correct.\n\n8. In Eqn (7), the probability distributions are actually **class-conditional** rather than marginal. That is, for the exclusive features: $x\\_{Ej} \\\\,|\\\\, y = i \\sim \\mathcal{N}(\\mu, \\sigma^2)$ when $j = i$ and $0$ when $j \\neq i$. Similarly, for the cross-class features: $x\\_{Cj} \\\\,|\\\\, y = i \\sim \\mathcal{N}(\\mu, \\sigma^2)$ when $j \\neq i$ and $0$ when $j = i$. Specifying it this way makes it clear why these features are defined as exclusive and cross-class.\n\n9. In Eqn (9), it would be better to say the outer expectation is over the class prior distribution since $\\mathrm{E}_i[\\cdot]$ is pretty vague. \n\n10. In Eqn (9), it is not clear how the cross-entropy loss simplifies to the term $\\max_{j \\neq i} f_w(x + \\delta)_j - f_w(x + \\delta)_i$. It seems. to me that if we simplify the cross-entropy loss, it reduces to $\\log(\\sum_j e^{f_w(x + \\delta)_j}) - f_w(x + \\delta)_i$. Same question applies to Eqn (10).\n\n11. What is the significance of introducing the regularization term in this objective?\n\n12. In Section 4.2, 2nd line: should it be `abandon $x_C$` rather than `abandon $x_E$`?\n\n13. The statements of theorem 2 and theorem 3 are vague. They could be stated more formally. In theorem 3, what is $\\epsilon_0$? \n \n14. In the subsection `Knowledge distillation preserves cross-class features`, could the authors clarify how label smoothing applies here due to symmetry? And what does symmetry refer to here?\n\n### On Section 5\n15. As pointed out in point 2 under `Weaknesses`: \nIs the weight averaged model $\\bar{\\theta}$ created on the fly during adversarial training, similar to the self ensemble adversarial training (SEAT) method of (Wang & Wang, 2022)? The piecewise linear scheduling for $\\lambda$ should be described precisely. More clarity is needed on these aspects, and an algorithm block would also be helpful (appendix can be used if space is limited). \n\n16. In Eqn (11), please clarify that the max over $\\delta$ applies to both the loss function terms. \n\n17. What is the temperature parameter $T = 2$ referred to in section 5.2 under settings?\n\n18. Finally, the number of references seems to be small given the breadth of literature in this area."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission843/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission843/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission843/Reviewer_fn47"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission843/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698879747175,
            "cdate": 1698879747175,
            "tmdate": 1700709438024,
            "mdate": 1700709438024,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "J7uZiIzjt8",
                "forum": "ncbDXOdURn",
                "replyto": "ZCmrlfxhTm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fn47 (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your detailed and valuable feedback. We have revised our manuscript based on your comments and address your concerns as follows.\n\n---\n\n**Q1**: In Eqn (3), the max over the perturbation\u00a0$\\delta$\u00a0should include all three loss terms. It seems like the max is only over the cross-entropy loss term. Please make it clear by moving the\u00a0$(1-\\lambda_1-\\lambda_2)$\u00a0term inside the max and using parentheses around the three loss terms.\n\n**A1**: Thanks for your careful reading. We have fixed this typo in Equation (3) in our manuscript.\n\n---\n\n**Q2**: The terms  $\\ell_{CE}$ and $\\mathcal{KD}$ in Eqn (3) should be defined as the cross entropy loss and the Kullback-Leibler divergence respectively.\n\n**A2**: Thank you for your valuable comment. We'd like to clarify that $\\mathcal {KD}$ is not the Kullback-Leibler divergence; rather, it represents the knowledge distillation function. This function initially computes the distilled probability prediction for both the student and teacher models by dividing the temperature $T$. Subsequently, it calculates the KL-divergence between these predictions:\n\n$\\mathcal{KD}(f(\\boldsymbol\\theta_{\\text{student}};x), f(\\boldsymbol\\theta_{\\text{teacher}};x)) = \\text{KL}[\\text{softmax}(\\frac{f(\\boldsymbol\\theta_{\\text{student}};x)}{T}),\n\\text{softmax}(\\frac{f(\\boldsymbol\\theta_{\\text{teacher}};x)}{T}$)]\n\nwhere $\\text{KL}(\\cdot,\\cdot)$ is the Kullback-Leibler divergence and $T$ is the distillation temperature. We have clarified this in page 3 in our revision and added the details in Appendix G.2.\n\n---\n\n**Q3**: Nit: the\u00a0$\\times$\u00a0operator is not needed in Eqn (4).\n\n**A3**: Thanks for pointing that out. We have removed that $\\times$ operator in Eqn (4) to improve readability.\n\n---\n\n**Q4**: First paragraph of section 3.1: Notations for $f(\\cdot), W$ and $f(x)_j$.\n\n**A4**: Thank you for providing such detailed feedback. Based on your suggestions, we have addressed the typos in Section 3.1 on page 4 to enhance the overall readability of the manuscript.\n\n---\n\n**Q5**: Referring to the subsection\u00a0``Knowledge distillation mitigates robust overfitting``, the following statement is not clear: \u201c. . . using knowledge distillation can preserve the weight of these features by smoothing the labels of the overlapping classes\u201d.\n\n**A5:** Thanks for your careful reading. In order to provide further clarity regarding the effectiveness of knowledge distillation for preserving cross-class features, we have refined the claim in our revised paper as follows:\n\n> In the process of AT with knowledge distillation, the teacher model adeptly captures the cross-class features present in the training data, and provides more precise labels by considering both class-specific and cross-class features. This stands in contrast to vanilla AT with one-hot labels, which primarily emphasizes class-specific features and may inadvertently suppress cross-class features in the model weights. The incorporation of cross-class features, backed by both our empirical findings and theoretical insights highlighting their significance for enhanced robustness, enables knowledge distillation to effectively mitigate robust overfitting by preserving these crucial features.\n\n---\n\n**Q6**: Use of $i$ to index the class is confusing. Instead, $y$ could be used to denote the class and or to index the features.\n\n**A6**: Thanks for your kind consideration. Following your suggestion, we have replaced the index $i$ for classes with $y_i$ to differentiate them from the index of features in the revised paper.\n\n---\n\n**Q7**: It is mentioned that the data distribution\u00a0$\\mathcal{D}\\_i = \\\\{ x\\_{E_j}, x\\_{C\\_j} | 1 \\leq j \\leq 3 \\\\} \\in \\mathrm{R}^6$. How can the data distribution be a set of features which lives in\u00a0$R^6$? I can follow what is implied, but it is not formally correct.\n\n**A7**: Thanks for your careful checking. In Section 4.1, page 7 of our revised paper, we redefined the features for each sample as living in $\\mathbb R^6$, and defined the distribution for the features independently in equation (7).\n\n---\n\n**Q8**: In Eqn (7), the probability distributions are actually\u00a0**class-conditional**\u00a0rather than marginal. That is, for the exclusive features: $x_{Ej} \\|\\ y = i \\sim \\mathcal{N}(\\mu, \\sigma^2)$ when $j=i$ and 0 when $j\\ne i$. Similarly, for the cross-class features: $x_{Cj} \\|\\ y = i \\sim \\mathcal{N}(\\mu, \\sigma^2)$ when $j\\ne i$ and 0 when $j=i$. Specifying it this way makes it clear why these features are defined as exclusive and cross-class.\n\n**A8**: Thank you for your kind suggestion. We have revised the definition in equation (7) as per your suggestion to make it clearer."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511798510,
                "cdate": 1700511798510,
                "tmdate": 1700511798510,
                "mdate": 1700511798510,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iSeqXwVgeM",
                "forum": "ncbDXOdURn",
                "replyto": "ZCmrlfxhTm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fn47 (2/3)"
                    },
                    "comment": {
                        "value": "**Q9**: In Eqn (9), it would be better to say the outer expectation is over the class prior distribution since $\\mathrm{E}_i[\\cdot]$ is pretty vague.\n\n**A9**: Thanks for pointing this out. Following your suggestion, we have modified $\\mathbb E_i$ as $\\mathbb E_{y_i\\sim\\{y_1,y_2,y_3\\}}$ in both equations (9) and (10) to clarify the class prior for the expectation.\n\n---\n\n**Q10**: In Eqn (9), it is not clear how the cross-entropy loss simplifies to the term $\\max_{j \\neq i} f_w(x + \\delta)_j - f_w(x + \\delta)_i$.\n\n**A10**: Thanks for your careful consideration, but we didn't claim that we apply cross-entropy loss in this framework, which may be hard to analyze. Instead, we use this simple loss function: $\\max_{j\\ne i} [f_w(x+\\delta)\\_j]-f_w(x+\\delta)\\_{y_i}$, where $y_i$ is the index of the true class. Both cross-entropy and this loss function encourage the model to increase the logit margin between the correct class $f_w(x+\\delta)_{y_i}$ and any other class $f_w(x+\\delta)_j\\quad (j\\ne y_i)$.\n\n---\n\n**Q11**: What is the significance of introducing the regularization term in this objective?\n\n**A11**: Thanks for your careful reading, we explain the significance of this regularization term as follows. Adding this regularization term prevents both $w_1$ and $w_2$ from going infinitely large. For example, by multiplying a positive term $k>0$ to both $w_1$ and $w_2$, the weights become $kw_1$ and $kw_2$, and these parameters can also cause the loss function to be multiplied by $k$ due to the linearity of $\\mathcal L$, but this will not change the classification results due to the linearity of $f_w$. Therefore, for any $w_1$ and $w_2$ that imply a negative loss function $\\mathcal L(f_{w_1,w_2})$, multiplying $k>1$ makes $\\mathcal L(f_{kw_1,kw_2})<\\mathcal L(f_{w_1,w_2})$. Thus, there is no optimal solution for $\\min \\mathcal L(f_w)$ without a regularization term.\n\n---\n\n**Q12**: In Section 4.2, 2nd line: should it be ``abandon $x_C$`` rather than ``abandon $x_E$``?\n\n**A12**: Thank you for your careful reading. It is exactly ``abandon $x_C$`` rather than ``abandon $x_E$``, and we have fixed this typo in our revision.\n\n---\n\n**Q13**: The statements of theorem 2 and theorem 3 are vague. They could be stated more formally.\n\n**A13**: Thank you for your valuable comment, we have clarified the possibly confusing part in the statements in our revised paper and explained them as follows. \n\n- We have revised the claim in Theorem 2 more clearly as\n    \n    > For any class $y$, consider weights $w_1 > 0$, $w_2 \\in [0, w_1]$, and $\\epsilon \\in (0, \\frac{\\mu}{2})$. When sampling $x$ from the distribution of class $y$, increasing the value of $w_2$ enhances the possibility of the model assigning a higher logit to class $y$ than to any other classes $y'\\ne y$ under adversarial attack. In other words, the probability $\\Pr_{x\\sim\\mathcal D_y}[f_w(x+\\delta))\\_{y}>f_w(x+\\delta)\\_{y'},\\forall \\delta:\\|\\delta\\|_\\infty\\le\\epsilon]$ monotonically increases with $w_2$ within the range $[0, w_1]$.\n    \n- For $\\epsilon_0$ in Theorem 3, it is exactly the $\\epsilon_0$ derived in Theorem 1. We have added this explanation in our revision.\n\n---\n\n**Q14**: In the subsection ``Knowledge distillation preserves cross-class features``, could the authors clarify how label smoothing applies here due to symmetry? And what does symmetry refer to here?\n\n**A14**: Thank you for your careful reading. In this context, the term 'symmetry' specifically refers to the symmetry of logits for the other two classes when taking the expectation in the loss function (equation 10). When considering data from class $y$, both the distribution of features $x_{E,i}$  and $x_{C_i}$ for the other two classes, as well as their respective weights $w_1$ and $w_2$, exhibit symmetry respectively. Consequently, after applying knowledge distillation, the expectation for logits of the other two classes in the objective loss function (equation 10) becomes identical. To simplify this process, we can employ label smoothing. \n\nWe have incorporated this discussion into our revision."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511877444,
                "cdate": 1700511877444,
                "tmdate": 1700511877444,
                "mdate": 1700511877444,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uFOmYTNXxp",
                "forum": "ncbDXOdURn",
                "replyto": "ZCmrlfxhTm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fn47 (3/3)"
                    },
                    "comment": {
                        "value": "**Q15**: As pointed out in point 2 under Weaknesses: Is the weight averaged model created on the fly during adversarial training, similar to the self ensemble adversarial training (SEAT) method of (Wang & Wang, 2022)? The piecewise linear scheduling should be described precisely. More clarity is needed on these aspects, and an algorithm block would also be helpful (appendix can be used if space is limited).\n\n**A15**: Thank you for bringing this to our attention. The weight-averaged model, denoted as $\\bar\\theta$, is dynamically constructed on-the-fly and continues until the warm-up procedure concludes.\n\nDuring the warm-up stage of the weight-averaged model, the piecewise linear schedule for $\\lambda$ is defined as follows: $\\lambda$ starts at 0 and linearly increases to 1 until a certain epoch. Subsequently, $\\lambda$ remains constant at 1 until the end of the training process.\n\nIn response to your suggestion, we have incorporated a detailed algorithm outlining our proposed method in **Appendix G.1.**\n\n---\n\n**Q16**: In Eqn (11), please clarify that the max over\u00a0$\\delta$ applies to both the loss function terms.\n\n**A16**: Thanks for your careful reading. We have corrected this in our revision.\n\n---\n\n**Q17:** What is the temperature parameter $T$ referred to in section 5.2 under settings?\n\n**A17**: Thanks for your kind consideration, $T$ is the temperature for the knowledge distillation function. We have added more details on knowledge distillation in **Appendix G.2**, as discussed in our response in **A2**.\n\n---\n\n**Q18**. Finally, the number of references seems to be small given the breadth of literature in this area.\n\n**A18**. Thanks for the kind advice. We have added additional related work in Appendix H.\n\n---\n\n### Minor comments\n\n**W3**: Experiments in the main paper are limited, and only two baselines are used for comparison. However, there are more experiments and other baselines like TRADES in the appendix.\n\n**A**: Thank you for recognizing that more comprehensive experiments are included in the appendix, where experiments over $\\ell_2$-norm AT, more model architectures, and combinations with TRADES are included. As the proposed knowledge distillation method is designed to mitigate robust overfitting, to the best of our knowledge, KDSWA is the **only** existing knowledge distillation method for the same purpose. Thus, we only consider vanilla training and KDSWA as baselines.\n\n---\n\n**W4**: Code has not been made available.\n\n**A**: We commit that we will release our code upon publication.\n\n---\n\nWe truly appreciate your valuable and detailed feedback. If you have any further questions or concerns, please let us know."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511923349,
                "cdate": 1700511923349,
                "tmdate": 1700511923349,
                "mdate": 1700511923349,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4NGsylQDNf",
                "forum": "ncbDXOdURn",
                "replyto": "ZCmrlfxhTm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Could you please have a look at our rebuttal?"
                    },
                    "comment": {
                        "value": "Dear Reviewer fn47, thanks for your time reviewing our paper. We have meticulously prepared a detailed response addressing your concerns and revised our paper accordingly. Could you please have a look to see if there are further questions? Your invaluable input is greatly appreciated. Thank you once again, and we hope you have a wonderful day!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696343871,
                "cdate": 1700696343871,
                "tmdate": 1700696343871,
                "mdate": 1700696343871,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2IqtSLBz90",
                "forum": "ncbDXOdURn",
                "replyto": "4NGsylQDNf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission843/Reviewer_fn47"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission843/Reviewer_fn47"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you for the detailed responses and revision to the paper, including the addition of Algorithm 2 for WAKE. Most of my concerns have been clarified. However, some of the changes made to the equations are not consistent with my suggestions. Please see below and address them in the next revision.\n\nOverall, I found the paper to make an interesting contribution by providing a novel understanding and knowledge distillation-based training approach for the robust overfitting problem. Hence, I will update my rating to 6."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709259395,
                "cdate": 1700709259395,
                "tmdate": 1700709259395,
                "mdate": 1700709259395,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FiTXbsA690",
                "forum": "ncbDXOdURn",
                "replyto": "ZCmrlfxhTm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission843/Reviewer_fn47"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission843/Reviewer_fn47"
                ],
                "content": {
                    "title": {
                        "value": "Suggested changes (part 1)"
                    },
                    "comment": {
                        "value": "Regarding Equations 3, 11, and 10, I meant changing them as follows because even in the revised version, it is not clear that the inner `max` over $\\delta$ applies to all three terms.\n\n**Equation 3**\\\n$\\min_{\\theta} \\mathbb{E}_{(x,y) \\sim \\mathcal{D}\\_{train}} \\left[  \\max\\limits\\_{\\\\|\\delta\\\\|_p \\le \\epsilon} \\tilde{\\ell}(\\theta \\~;\\~ \\theta_1, \\theta_2, x + \\delta, y) \\right] $, where \\\n$\\tilde{\\ell}(\\theta \\~;\\~ \\theta_1, \\theta_2, x + \\delta, y) \\~=\\~ (1 - \\lambda_1 - \\lambda_2) \\ell\\_{CE}(f(\\theta, x + \\delta), y) \\~+\\~ \\sum\\limits\\_{i=1}^2 \\lambda_i \\\\, \\mathcal{KD}( f(\\theta, x + \\delta), f(\\theta_i, x + \\delta) )$\n\n**Equation 11**\\\n$ \\max\\limits\\_{\\\\|\\delta\\\\|_p \\le \\epsilon} \\tilde{\\ell}(\\theta \\~;\\~ \\bar{\\theta}, x + \\delta, y) $, where \\\n$\\tilde{\\ell}(\\theta \\~;\\~ \\bar{\\theta}, x + \\delta, y) = (1 - \\lambda) \\ell\\_{CE}( f(\\theta, x + \\delta), y ) \\~+\\~  \\lambda \\\\,\\mathcal{KD}( f(\\theta, x + \\delta), f(\\bar{\\theta}, x + \\delta) )$\n\nSimilar suggestion for **Equation 10** as well."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712209675,
                "cdate": 1700712209675,
                "tmdate": 1700712231340,
                "mdate": 1700712231340,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3rUzDoidbk",
                "forum": "ncbDXOdURn",
                "replyto": "ZCmrlfxhTm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission843/Reviewer_fn47"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission843/Reviewer_fn47"
                ],
                "content": {
                    "title": {
                        "value": "Suggested changes (part 2)"
                    },
                    "comment": {
                        "value": "In the earlier version, the class label was denoted by $y = i$, which I found to be more clear than the current version (which denotes the label by $y_i$). For instance, the notation $x_{E_i}$ is clearer than $x_{E_{y_i}}$, and $f_w(x)\\_i$ is clearer than $f_w(x)\\_{y_i}$.\n\nAs suggested in my original review, Eqn 7 is perhaps more clear when expressed as follows. The conditional distribution $\\mathcal{D}\\_i$ for class $i$ is defined as: \\\n$x_{E_j} \\~|\\~ y = i \\~\\sim \\begin{cases} \\mathcal{N}(\\mu, \\sigma^2) \\~\\~&\\text{ if  } j = i  \\\\\\\\  0 \\~\\text{ w.p. } 1  \\~\\~&\\text{ if  } j \\neq i \\end{cases}$\n\n$x_{C_j} \\~|\\~ y = i \\~\\sim \\begin{cases} \\mathcal{N}(\\mu, \\sigma^2) \\~\\~&\\text{ if  } j \\neq i  \\\\\\\\  0 \\~\\text{ w.p. } 1  \\~\\~&\\text{ if  } j = i \\end{cases}$ \\\nwhere $j \\in \\\\{1, 2, 3\\\\}$.\n\n### Regarding Equation 9\nMy suggestion in the original review is a simple change, to indicate that the outer expectation is over the prior distribution on class labels $p_Y$, i.e.\\\n$\\mathcal{L}(f_w) = \\mathbb{E}\\_{y \\sim p_Y}[ \\mathbb{E}\\_{x \\sim \\mathcal{D}_y}[  \\cdots ] ] + \\frac{\\lambda}{2} \\\\|w\\\\|^2$\n\nThat's all the changes I want to suggest. Glad to know that you found my feedback useful. Good luck!"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726249143,
                "cdate": 1700726249143,
                "tmdate": 1700726288191,
                "mdate": 1700726288191,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]