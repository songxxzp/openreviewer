[
    {
        "title": "MissDiff: Training Diffusion Models on Tabular Data with Missing Values"
    },
    {
        "review": {
            "id": "Nkz1mbwiCp",
            "forum": "vULHgaoASR",
            "replyto": "vULHgaoASR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4832/Reviewer_JZFN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4832/Reviewer_JZFN"
            ],
            "content": {
                "summary": {
                    "value": "In the paper, the authors first comment on the limitations of two existing frameworks: (1) impute and then generate framework is constructing a complete training dataset first, but is biased. (2)generate then impute framework needs MCAR and MAR conditions.  The authors propose a simple MissDiff algorithm that utilizes a masking function on the loss function to handle the missing data values. The authors examine the effectiveness of MissDiff, by theoretically verifying that MissDiff can learn the oracle score on the complete data. Furthermore, MissDiff is an upper bound for the negative likelihood on the observed data.  The experiment shows that MissDiff performs comparably better to various imputation approaches in the imputation tasks, and MissDiff is performing better than impute-and -then-generate or generate-then-impute frameworks training with vanilla diffusion."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1. The methodology is given in a clear manner; the proper notations are used and the authors provide solid explanations, examples and proofs to each of the claims.\n\nS2. The authors have provided a theoretical proof to justify the effectiveness of the masking mechanism by examining the loss functions and verifying that optimal solution of MissDiff can learn the oracle score on the complete data and MissDiff is also the upper bound of the negative likelihood of the generative model on the observed data.\n\nS3. The simulation results seem promising. The authors have provided convincing figures to show that under fidelity evaluation, MissDiff performs better than Diff-delete and Diff-mean under different data missing scenarios."
                },
                "weaknesses": {
                    "value": "W1. The experimental setting for Experiment 1 should be detailed described in the main text or appendix. It is confusing to the reviewer how MissDiff\u2019s RMSE and error rate are provided (if the MissDiff is doing unconditional generation according to Algorithm 2), and what is the training hyper parameters that are associated with MissDiff in this scenario? It is also confusing how RMSE evaluation of MIssDiff on MINIC4ED dataset is acquired (see Table 3). The authors should consider providing more experimental details.\n\nW2. Table 2 shows that MissDiff\u2019s utility is significantly lower than Diff-mean for column missing scenario, the authors should have provided an explanation for the inferior performance of MissDiff for column missing Census data.\n\nW3. In terms of writing, Algorithm 2 is not different from the existing variance preserving sampling method. Therefore, it is redundant to include Algorithm 2 in the main text. The reviewer suggest to move the Algorithm 2 into the appendix."
                },
                "questions": {
                    "value": "Q1. What do the numbers mean in the parenthesis for Table 1?\n\nQ2. The authors mention that they hope to evaluate MissDiff with more complicated data types such as video or language data, but the reviewer is wondering how missing values are defined in videos and images. It seems trivial to handle the missing values in video and images due to the amount of redundant information the videos and images contain. 70% of missing pixels can still generate reliable images in masked auto-encoders. For languages, missing values are also beneficial to generative models. Masked-out tokens are important for language model training, \n\nQ3. The reviewer is wondering how MissDiff can handle the categorical features and nonnumerical features in Tabular Data. While the categorical features and non-numerical features could be treated as continuous features according to Equation 4, the performance is not as good as directly performing discrete diffusion models (D3PM [1], tauLDR[2]). Could MissDiff be adapted to discrete diffusion models?\n\n[1] Austin, Jacob, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. 2023. \"Structured Denoising Diffusion Models in Discrete State-Spaces.\" arXiv preprint.  https://arxiv.org/abs/2107.03006.\n\n[2] Campbell, Andrew, Joe Benton, Valentin De Bortoli, Tom Rainforth, George Deligiannidis, and Arnaud Doucet. 2022. \"A Continuous Time Framework for Discrete Denoising Models.\" arXiv preprint.  https://arxiv.org/abs/2205.14987."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/a."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4832/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4832/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4832/Reviewer_JZFN"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4832/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771817246,
            "cdate": 1698771817246,
            "tmdate": 1699636466662,
            "mdate": 1699636466662,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F9zPC8kNwe",
                "forum": "vULHgaoASR",
                "replyto": "Nkz1mbwiCp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4832/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4832/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments, and we appreciate the time you spent on the paper. Below we address the concerns and comments that you have provided.\n\n**Q**: *The experimental setting for Experiment 1 should be detailed described in the main text or appendix.*\n\n**A**: Thank you for your suggestion. The experimental setting for Section 4.1 can be found in Appendix B.1 in the revised paper (updated on openrview). \n\n**Q**: *It is confusing to the reviewer how MissDiff\u2019s RMSE and error rate are provided ... what is the training hyper parameters that are associated with MissDiff in this scenario?*\n\n**A**: Thank you very much for your question. We have added the imputation algorithm of MissDiff in Algorithm 2, which can be found in Appendix B.3. Since MissDiff models the score for complete data distribution, we can adopt the same model and training hyperparameters for imputation and generation tasks. The hyperparameters are described in Appendix B.4.\n\n**Q**: *It is also confusing how RMSE evaluation of MIssDiff on MINIC4ED dataset is acquired (see Table 3).*\n\n**A**: The RMSE in Table 3 is different from imputation tasks. As mentioned in \"Evaluation Criterion\" paragraph in section 4.2, we follow the evaluation criterion as Xu et al. (2019); Kim et al. (2023); Kotelnikov et al. (2022) that use {\\it utility} to evaluate the performance of the generated complete data. We train a downstream model (e.g., XGBoost model) on generated data for regression tasks and report the RMSE of the predicted value against the oracle value on the real (test) data. \n\n**Q**: *Table 2 shows that MissDiff\u2019s utility is significantly lower than Diff-mean for column missing scenario.*\n\n**A**: We believe the column missing mechanism described in Appendix B.1 is actually a special scenario. Most specifically, the mask $\\mathbf m$ (indicator of missing values) for each row (sample) would depend on the masks of other rows as well, since the missing rate for each column is fixed. It leads to dependence between missing samples. We further note that in our population objective function eq (4), as a standard practice, we regard the sample pair (m,x) are iid and the expectation in (4) is taken with respect to this joint distribution. When the sample size of the dataset is relatively small, such sample dependence is more evident, and MissDiff is not as good as Diff-mean. However, when there is a sufficiently large number of samples, as in Table 3,8, and 11 for the MIMIC4ED dataset (22 times larger than the Census dataset), such dependence becomes very weak and the joint sample pairs (m,x) are close to independent; in these cases, MissDiff performs better. \n\n**Q**: *It is redundant to include Algorithm 2 in the main text.*\n\n**A**: Thank you for your suggestion. We have moved Algorithm 2 to Appendix B.3. For completeness, we have also added the algorithm for imputation in Appendix B.3.\n\n**Q**: *What do the numbers mean in the parenthesis for Table 1?*\n\n**A**: The numbers are the standard deviation of five independent trials. We add the explanation to the caption of Table 1.\n\n**Q**: *How missing values are defined in videos and images? How to evaluate MissDiff with more complicated data types such as video or language data?*\n\n**A**: Thank you very much for your question. \n\n- For video and image data, we can either regard the missing frames or the missing pixels in each frame as the missing data. It is a good point of view that the amount of redundant information in video and images is high. We think MissDiff would still be useful against VAE-based or GAN-based methods since our approach can learn the complete data distribution from incomplete data (low-quality video data) and generate complete data in a unified framework. \n- For the language data, missing values could be missing words or sentences. We would like to emphasize that missing values in language data are typically more challenging to deal with as compared with the tabular case we considered: (1) there are strong temporal dependencies in language data, (2) the length of the language data is unfixed. \n\nAs a special and simple example, our method can be extended for tabular data that contains text information (for instance, the diagnosis note from the doctor). Some of such descriptions might be missing. A language embedding model could potentially be incorporated into our proposed method to deal with the missingness of such short-length text information in certain columns. Nevertheless, this would need careful design case by case."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469525830,
                "cdate": 1700469525830,
                "tmdate": 1700469832753,
                "mdate": 1700469832753,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DdlIxXd97H",
                "forum": "vULHgaoASR",
                "replyto": "NNhdhy8JC8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4832/Reviewer_JZFN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4832/Reviewer_JZFN"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the detailed responses. My overall rating remains."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703130243,
                "cdate": 1700703130243,
                "tmdate": 1700703130243,
                "mdate": 1700703130243,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1ZnNyasXu4",
            "forum": "vULHgaoASR",
            "replyto": "vULHgaoASR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4832/Reviewer_bE5M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4832/Reviewer_bE5M"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a new diffusion-based generative framework, MissDiff, specifically designed for learning from incomplete data and generating synthetic complete data. They highlight the common issue of incomplete data in various real-world applications, such as healthcare and finance, particularly when dealing with tabular datasets. The authors illustrate the drawbacks of existing two-stage inference frameworks, noting they are either biased or computationally taxing.\n\nThe proposed solution, MissDiff, is presented as a unified and computationally friendly framework. It models the score of complete data distribution by denoising score matching on data with missing values. The authors provide a theoretical justification for MissDiff's effectiveness and stress that the proposed training objective serves as an upper bound for the negative likelihood of observed data.\n\nIn the case of incomplete training data, MissDiff can be employed for synthetic data generation and missing value imputations based on the learned generative model. The authors conclude by stating that extensive experiments on imputation tasks and generation tasks show that MissDiff outperforms existing state-of-the-art approaches on multiple tabular datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Good coverage of existing methods\n- Certainly a practically relevant problem to address."
                },
                "weaknesses": {
                    "value": "- The technical description is difficult to follow, e.g. in section 2.2. or 3.2. I think it can be well understood by someone who is already familiar with the material but it doesn't do a great job building this up for the less versed reader.\n- 1.1. \"evidential low bound\", I think you mean evidence lower bound.\n- How do you tune hyperparameters, especially for competing methods? I think it's hard to compare against baselines without this.\n- Claiming that existing approaches are either biased or expensive seems very general.\n\n## Stylistic critique\n- The abstract is a bit vague, e.g. \"beyond missing value imputation\" - not sure what that means.\n- Intro: \"It is known that ...\" remove. Just state the claim.\n- Footnote 3, you mean to say you \"use them interchangeably\"."
                },
                "questions": {
                    "value": "- I don't think that work like the VAEs for missing data can be described as \"generate-than-impute\". In (at least some) of these models, there is no separate imputation step, but they learn a joint density marginalizing over missing value. I may be wrong. Can you clarify?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4832/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782369499,
            "cdate": 1698782369499,
            "tmdate": 1699636466577,
            "mdate": 1699636466577,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RIHGiFoRV5",
                "forum": "vULHgaoASR",
                "replyto": "1ZnNyasXu4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4832/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4832/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments, and we appreciate the time you spent on the paper. Below we address the concerns and comments that you have provided.\n\n**Q**: *How do you tune hyperparameters, especially for competing methods?*\n\n**A**: The hyperparameters of our method can be found in Appendix B.4 (revised version updated on openreview). In short, the optimizer, learning rate, batch size, and training epochs are the same as Zheng & Charoenphakdee (2022) for fair comparison. The hyperparameters of other baseline methods can also be found in Appendix B.4.\n\n**Q**: *I don't think that work like the VAEs for missing data can be described as \"generate-than-impute\". In (at least some) of these models, there is no separate imputation step, but they learn a joint density marginalizing over missing value.*\n\n**A**: Thank you very much for your question. We clarify this point below. VAE-based methods (especially Mattei Frellsen (2019)) maximize the likelihood of the observed value to train an encoder projecting $x^{\\text{obs}}$ to feature $z$ and a decoder projecting feature $z$ to $x^{\\text{obs}}$. For the imputation tasks, they learn the distribution $p(x^{\\text{missing}}|x^{\\text{obs}})$ by marginalizing over latent variable $z$. However, the previous VAE-based method cannot easily deal with the generation tasks without adopting \"generate-then-impute\" framework. The key reason is they model the distribution $p(x^{\\text{obs}}|z)$ by a student $t$ distribution with location, scale, and degrees of freedom outputted by the decoder, which has limited representation power for the real distribution. We believe directly sampling from $p(x^{\\text{obs}}|z)$ for generating tasks will have poor performance. Therefore, a practical solution for generating is first generating a set of missing data and then adopting VAE-based methods to impute them.\n\n**Q**: *Claiming that existing approaches are either biased or expensive seems very general.*\n\n**A**: We provide a detailed explanation of this claim in Section 3.1 and Section 1.1.\n\n**Q**: *The technical description is difficult to follow; typos; Stylistic critique*\n\n**A**: Thank you very much for your suggestion. The typos are carefully checked and corrected. To improve the readability of Section 2.2 and 3.2, we have added specific references to the main technical equations, including the forward and backward process, the score matching objective, and variance preserving SDE."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468973632,
                "cdate": 1700468973632,
                "tmdate": 1700469788862,
                "mdate": 1700469788862,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZYa4AX8Wc0",
                "forum": "vULHgaoASR",
                "replyto": "RIHGiFoRV5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4832/Reviewer_bE5M"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4832/Reviewer_bE5M"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thank you for the kind response. My overall rating remains."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511017816,
                "cdate": 1700511017816,
                "tmdate": 1700511017816,
                "mdate": 1700511017816,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xLloG895Tm",
            "forum": "vULHgaoASR",
            "replyto": "vULHgaoASR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4832/Reviewer_EFAN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4832/Reviewer_EFAN"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the diffusion model in the presence of missing data. The basic idea is to compute the score matching objective on observed entries. The authors proved that the solution is the true score function and this objective upper bounds the negative log-likelihood. Experiments are conducted on tabular data imputation and generating new data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposes a new method to train a diffusion model on missing data. The training objective is simple and the authors provide theoretical guarantees."
                },
                "weaknesses": {
                    "value": "1. There are several inaccurate claims about previous literature and related work, which decrease the validation of the paper.\n- In Section 1.1, why did the authors claim that Mattei & Frellsen (2019); Ipsen et al. (2020b) train multiple decoders? From my understanding, they proposed multiple imputation via sampling multiple latent factors, rather than training multiple decoders.\n- Ipsen et al. (2020a) was published in ICLR 2022.\n- In the last line of Page 3, the authors said the references require MCAR assumption. However, I suppose most of them assumed the MAR case. Moreover, one of the contributions of Li et al. (2019) is to deal with NMAR data.\n2. The authors did not mention what kind of missing mechanism was used in Section 4.1. Moreover, it seems that they only compare with baselines under one missing mechanism in this section.\n3. Why did not the authors compare with CSDI_T in Section 4.2? Why are the results of STaSy not included in Table 4?"
                },
                "questions": {
                    "value": "While Tashiro et al. (2021) adopted conditional score matching, the work goes back to unconditional score matching. Can the authors provide more discussions about these two approaches and what are the advantages of using unconditional scores?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4832/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816530263,
            "cdate": 1698816530263,
            "tmdate": 1699636466479,
            "mdate": 1699636466479,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Jko6olxJC0",
                "forum": "vULHgaoASR",
                "replyto": "xLloG895Tm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4832/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4832/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments, and we appreciate the time you spent on the paper. Below we address the concerns and comments that you have provided.\n\n**Q**: *Why did the authors claim that Mattei & Frellsen (2019); Ipsen et al. (2020b) train multiple decoders?*\n\n**A**: Thank you very much for pointing this out. We agree that Mattei & Frellsen (2019); Ipsen et al. (2020b) only train one decoder and we revise the argument in Section 1.1 accordingly in the revised paper (updated on openreview). More specifically, different from Nazabal et al. (2018); Ma et al. (2020) that training a different VAE independently to each data dimension, Mattei & Frellsen (2019); Ipsen et al. (2020b) only need to train one decoder and sample multiple latent factors from the variational distribution.\nHowever, these two works model the distribution $p(x^{\\text{obs}}|z)$ by a student $t$ distribution with location, scale, and degrees of freedom outputted by the decoder, which has limited representation power for directly generating complete new samples. Therefore, their methods still need a two-stage inference framework for generating complete new samples, i.e., generating the samples containing missing values and adopting the proposed single imputation or multiple imputation methods to obtain complete data.  \n\n**Q**: *The authors said the references require MCAR assumption. However, I suppose most of them assumed the MAR case. Moreover, one of the contributions of Li et al. (2019) is to deal with NMAR data.*\n\n**A**: Thank you very much for your comments. We agree that the architecture in Li et al. (2019) can be easily extended to MAR and MNAR cases. However, the theoretical guarantees only held for the MCAR case, which is the same as Yoon et al. (2018a). In Li & Marlin (2020), the authors assume MCAR but can still be unbiased if the missing mechanism is MAR. In Ipsen et al. (2022) and Mattei & Frellsen (2019), the authors require MAR assumptions. We have modified the last sentence on page 3 to M(C)AR accordingly.\n\n**Q**: *The authors did not mention what kind of missing mechanism was used in Section 4.1. Moreover, it seems that they only compare with baselines under one missing mechanism in this section.*\n\n**A**: We follow the same experimental setup as Zheng & Charoenphakdee (2022) for the imputation tasks. The missing mechanism is MCAR and the missing ratio is 0.2. We clarify the experimental setup in Appendix B.1. Due to the time limit, we were unable to finish the comparison with six baseline methods on six datasets under different missing mechanisms. We believe the comparison under the same setting with previous work can demonstrate the effectiveness of MissDiff for imputation tasks.\n\n**Q**: *Why did not the authors compare with CSDI\\_T in Section 4.2? Why are the results of STaSy not included in Table 4?*\n\n**A**: Thank you very much for your suggestion. We have added the comparison with CSDI\\_T in Table 2, 4, 7, 9 and STaSy-delete and STaSy-mean in Table 4, 9. MissDiff outperforms CSDI\\_T on 8 out of 10 results and outperforms STaSy-delete and STaSy-mean by a large margin.  \n\n**Q**: *While Tashiro et al. (2021) adopted conditional score matching, the work goes back to unconditional score matching. Can the authors provide more discussions about these two approaches and what are the advantages of using unconditional scores?*\n\n**A**: Thank you very much for raising this important question. First of all, there are various conditional scores (depending on which information is conditioned on), making them difficult to learn and analyze. In Tashiro et al. (2021) and its tabular variant CSDI\\_T, there were no theoretical guarantees on whether the learned conditional score satisfied the optimality condition. In our work, we provide theoretical guarantees on the unconditional score learning from incomplete data. Secondly, conditional score matching performs better in time series imputation tasks than unconditional score matching, which is not necessarily the case for tabular data. We believe that is because there may exist some complex or irregular dependencies between different columns in tabular data, e.g., some features might be redundant (uninformative). Therefore, MissDiff can achieve better performance than CSDI\\_T in the imputation tasks and generation tasks."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468583775,
                "cdate": 1700468583775,
                "tmdate": 1700469771976,
                "mdate": 1700469771976,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0AnA1SK4qV",
                "forum": "vULHgaoASR",
                "replyto": "Jko6olxJC0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4832/Reviewer_EFAN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4832/Reviewer_EFAN"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I would like to keep my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538243271,
                "cdate": 1700538243271,
                "tmdate": 1700538243271,
                "mdate": 1700538243271,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Az86tyZxrE",
            "forum": "vULHgaoASR",
            "replyto": "vULHgaoASR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4832/Reviewer_JCWu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4832/Reviewer_JCWu"
            ],
            "content": {
                "summary": {
                    "value": "The authors of the paper developed MissDiff, a diffusion-based method for imputing incomplete tabular data under various missingness scenarios (MCAR, MAR, MNAR) and generating data. MissDiff most appealing feature is that it imputes missing values learns the (estimated) generative distribution (i) at a unified (i.e., not multiple-stage) manner, and (b) without the computational burden of training additional networks, as opposed to the corresponding state-ot-the-art methods. The authors performed experiments comparing their method to others both with respect to data imputation and generation tasks across several datasets where, in the majority of the experiments, their method outperfomed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "--method handles tabular data;\n--method imputes and generates data in without needing to train extra networks or diving the task into imputing and generating;\n--both theoretical and experimental justification is presented;\n--method seems to perform well under various missingness scenarios;"
                },
                "weaknesses": {
                    "value": "-- inconsistency of word \"complete\" : page 3, paragraph 2.1 : \"complete d-dimensional data ... m=(m_1, ...,m_d) in {0,1}^d\",  page 4, \"framework for learning on incomplete data\"\n-- more metrics need to be presented at the experiments section, eg RMSE is not necessarily representative in high-dimensional cases were the distribution is complex-multimodal;\n-- typos"
                },
                "questions": {
                    "value": "-- at fidelity testing (section 4.3, Figure 1) it seems that the method's performance improves as missingness rate increases in (0.1-0.6) and then decreases in the row- and column- missing setups: could you provide some insights on why this happens (also could you also add the case where missingness rate is 90% in the first case) ?\n-- at table 2 it seems that MissDiff and Diff-Mean seem to have similar performance -- could you comment on that and explore more datasets?\n-- could you compare your method with VAE- and GAN-based methods as well (more specifically Mattei and Frelsen, Li and Marlin) ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4832/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4832/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4832/Reviewer_JCWu"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4832/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699622952374,
            "cdate": 1699622952374,
            "tmdate": 1700701651404,
            "mdate": 1700701651404,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JihJPiqdEL",
                "forum": "vULHgaoASR",
                "replyto": "Az86tyZxrE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4832/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4832/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments and suggestions. We appreciate the time you spent on the paper. Below we address the concerns and comments that you have provided.\n\n**Q**: *Inconsistency of word \"complete\".*\n\n**A**: Thank you for your comments. Our proposed framework is learning from incomplete data and generating complete data. Therefore, in Section 2.1, we first define the complete (unobserved) data distribution $p_0(x)$ and then define the incomplete (observed) data samples $S^{\\text {obs}}$. We have made this more clear in the revision (updated on openreview). \n\n**Q**: *More metrics need to be presented at the experiments section, eg RMSE is not necessarily representative in high-dimensional cases were the distribution is complex-multimodal.*\n\n**A**: Thank you for your good suggestion. For the generation task, we adopt two types of criteria, fidelity and utility, to evaluate the quality of the synthetic data generated. For the imputation task, we adopted RMSE in order to compare with previous works that evaluated the imputation performance mainly by RMSE. \n\n**Q**: *Could you provide some insights on why  the method's performance improves as missingness rate increases in (0.1-0.6)?*\n\n**A**: For the slight performance improvement with missing rate increase in (0.1-0.6), it is the authors' conjecture that this is a phenomenon due to the unique structure of certain tabular datasets. For this simulated Bayesian network dataset, the dependencies between different columns are demonstrated in Figure 2: some features might be uninformative, for instance, variables C1, C2, and D1 are all uninformative to the value of D3, given that D2 is observed. This implies that for some rows with missing C1, C2, and D3 values, the model still has enough information to learn the full dependence between variables D3 and D2. Moreover, the model can potentially learn the distribution of D3|D2 better in such cases since other redundant variables are excluded. We conjecture this is the reason why we see a little improvement in the performance. \n\nNevertheless, we would like to emphasize that the improvement in performance is very small, after considering the observation noise, the authors believe it is more appropriate to conclude that the performance of the proposed method remains more ${\\it stable}$ (as compared with other baselines) for missing rates in 0.1-0.6. Moreover, the performance starts to decrease when we increase the missing rate to 0.8, since in such case, we only have one variable left in each row and thus it is reasonable to expect worse performance.\n\n**Q**: *Could you add the case where missingness rate is 90\\% in the first case?*\n\n**A**: As mentioned in Appendix B.1 (which is Appendix B.2 in the revised version), there are only five variables (columns) in the data generated by Bayesian Network (three categorical variables and two continuous variables). Therefore, in the row missing mechanism, we only have the missing ratio is [0.2,0.4,0.6,0.8]. For the column missing or the independent missing mechanisms, we set the missing ratio to be [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468024997,
                "cdate": 1700468024997,
                "tmdate": 1700468024997,
                "mdate": 1700468024997,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bXvbiVkiLC",
                "forum": "vULHgaoASR",
                "replyto": "LygsOvHvtS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4832/Reviewer_JCWu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4832/Reviewer_JCWu"
                ],
                "content": {
                    "title": {
                        "value": "Final decision"
                    },
                    "comment": {
                        "value": "We thank the authors for their response\nGiven that they did not provide empirical results, as requested, against competing methods, the final score will be 6: marginally above the acceptance threshold as we recognize the potentials of this work but there is more to be explored in order to assess its performance compared to existing methods."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701625459,
                "cdate": 1700701625459,
                "tmdate": 1700701625459,
                "mdate": 1700701625459,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]