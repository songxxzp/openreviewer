[
    {
        "title": "Centroid- and Orientation-aware Feature Learning"
    },
    {
        "review": {
            "id": "nHNKcGOZ3r",
            "forum": "fmAzKz9DJs",
            "replyto": "fmAzKz9DJs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2862/Reviewer_sBsU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2862/Reviewer_sBsU"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for learning representations where image moments (centroid and orientation) are explicitly disentangled from the rest of the representation. A loss term comparing the image moments is introduced and its contribution is gradually decreased during learning.\n\nExperimental results on six datasets are provided to show that the proposed method compares favorably to six recently proposed methods that also seek to disentangle translation and rotation representations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**S1.** The proposed method compares favorably to recently proposed methods. In particular, translation and rotation are effectively disentangled while the model is more computationally efficient than most other baselines."
                },
                "weaknesses": {
                    "value": "**W1.** In general, the presentation could be significantly improved. Some examples:\n- It seems to me that the key contribution of the method is the introduction of loss L_m but the discussion does not make this clear.\n- Along the same lines, it is suggested (end of section 2.3) that the \u201cprimary focus\u201d of the paper might be to achieve disentanglement (and indeed, some experiments also suggest that). However, it is not clear how this is pursued besides obtaining moments and orientation.\n- The theorems in section 3.1 are barely referenced in subsequent sections.\n- The experimental results need further details and discussion (more on this below).\n- Some sentences are hard to understand, e.g.: in the abstract \u201ctraining and inference performance\u201d (what is the metric?), third-to-last sentence in paragraph preceding eq. (13) (on \u201csubtle inaccuracy\u201d).\n\n**W2.** The motivation seems disconnected from the experimental validation. It is stated that learning of centroids and orientations \u201cunderpins\u201d a number of downstream tasks. It is not clear what this means nor is it clear what the level of success of the proposed approach would be in this regard.\n\nThe downstream task experiments are perhaps most interesting but barely any details of the experimental setup are provided. A lot of space is taken by visual results but I would suggest the downstream task results are much more important.\n\n**W3.** It is unclear what tables 3 and 4 convey when comparing different models as the optimal latent dimension is model dependent. For instance, for models TARGET-VAE and IRL-INR the original authors showed results for d >=32 (but it is suggested d=2 in the experiments in the present submission)."
                },
                "questions": {
                    "value": "**Q1.** Is the method pursuing disentanglement of all features or mainly/only obtaining moments and orientation? In case of the former, I would say this is not clear in the presentation, could you outline how this is pursued?\n\n**Q2.** Why are baselines not compared with representation dimension as in the original papers?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2862/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698538426503,
            "cdate": 1698538426503,
            "tmdate": 1699636229670,
            "mdate": 1699636229670,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ho0f0VFCBi",
                "forum": "fmAzKz9DJs",
                "replyto": "nHNKcGOZ3r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2862/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2862/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors - Thank you for the review"
                    },
                    "comment": {
                        "value": "We appreciate the time you've dedicated to reviewing our paper. Please find below our detailed response to your comments:\n\n1. **Improvement of our paper (W1)**\n\n* We apologise for the omission of this proof since translational equivariance in CNN is a well-known concept in the community, and the proof is similar to the Theorem 3.2. We have now added the proof of theorem 3.1 in the Appendix. \n\n* Related to \u2018training and inference performance\u2019: We apologise for the confusion. It refers to the training and inference time. We have changed it in the paper.\n\n* Related to L_m loss: We highlight that \n> Although image moments capture the centroid and orientation features with a high degree of accuracy, they can still have a degree of subtle inaccuracy. Therefore, we utilize the image moments as guiding principles during the initial stages of learning.\n\n\n\n2. **Problem statement and motivation of our paper (W2)**\n\nThank you for the comments. We apologize if the motivation seems disconnected from the experimental validation. We have changed the first paragraph of the introduction to clarify our problem as follows: \n\n\u2018The distinct 2D projections that arise from the rotation of a 3D object can be seen as unique categories of in-plane rotation. Each category corresponds to a specific orientation in the 3D space. This principle is widely applied in fields such as computer graphics, computer vision, and related areas where the understanding and manipulation of 3D objects in a 2D images are essential. In this context, robust learning of centroids and orientations of objects in images, along with other features, is important, especially when objects are captured with 3D shifts and rotations such as protein, and galaxy studies.\u2019\n\nTherefore, we demonstrate that the capability of the model to learn key features, including centroids and orientations in the experimental validation. Table 3 and 4 show that DAE and beta-VAE have poor performance in factorizing two datasets, XYRCS and dSprites. On the other hand, the disentanglement scores of the proposed model on these datasets demonstrate that the model effectively learn all features including centroids and orientations.  \n\n\n3. **Optimal latent dimension for TARGET-VAE and IRL-INR (W3, Q2)**\n\nTarget-VAE and IRL-INR conducted their experiments with large latent dimension, pursuing two objectives: 1) learning centroids and orientation, and 2) reconstructing the original data. Therefore, the two methods do not address the disentanglement scores in their paper, while they included the centroids and the orientation correlation. Therefore, the latent dimensions chosen in their respective papers are optimal for reconstructing the original data but may not effectively factorize all features. In contrast, our goal is not only to learn centroids and orientation but also to independently learn the other features while successfully reconstructing the original data. To show this, we chose a smaller latent dimension because modularity (or independence) and compactness are important in disentanglement properties. Figure 2, 4, 5, and 7 along with Table 3 and 4 demonstrate the capability of the model to factorize all the features while reconstructing the original data. \n\n\n4. **Objective of our paper (Q1)**\n\nWe pursue disentanglement of all features. DAE already shows an ability to disentangle features. However, it fails factorizing all features when the data has a rotational feature. Therefore, we introduce a new architecture outlined in the paper with $L_{moment}$ to explicitly learn centroids and rotations, leading to fully factorized features. \n\n5. **Contributions of our paper (W1)**\n\nOur contribution is not limited on $L_{moment}$. We:\n* Introduce a translational and rotational equivariant encoder.\n* Propose an architecture that eliminates computationally expensive decoder, such as spatial or hypernetwork decoders.\n* Drastically reduce computational costs while achieving superior disentanglement scores. \n\n\n\n6. **Further discussion**\n\nDespite detailed problem statement, clear motivation and challenges around robustly learning centroid and orientations, especially in real images, theoretical framework, and extensive experimental results (using appropriate datasets), we received a score of three. We would greatly appreciate more detailed feedback that led to this score on these specific areas. Understanding these details will help us improve and refine our paper. Your insights are invaluable, and we are eager to make the necessary updates based on your feedback."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2862/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700102895764,
                "cdate": 1700102895764,
                "tmdate": 1700601824622,
                "mdate": 1700601824622,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ACtRouoPcd",
                "forum": "fmAzKz9DJs",
                "replyto": "t4pX9Ee05v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2862/Reviewer_sBsU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2862/Reviewer_sBsU"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for the rebuttal and additional results.\n\nThe rebuttal adds some clarity. I have spent an additional few hours trying to understand the contribution of the submission and still have several questions.\n\nFurther, all reviewers seem aligned that the submission is difficult to understand. Yet, the rebuttal insists \"detailed problem statement, clear motivation\" were provided. This is dismissive of the reviewers' comments. I would discourage such interaction. In fact, even the new language provided in the rebuttal lacks clarity, e.g., \"Each category corresponds to in the 3D space.\" -- this is not a proper sentence, nor is it clear.\n\nThe rebuttal claims there are contributions besides L_m:\n\n1. \"Introduce a translational and rotational equivariant encoder.\"\n\nUnclear what is novel. The architecture is based on DAE. Interestingly the new results in the rebuttal show that the alleged contributions hurt DAE performance (e.g., compare the performance of DAE in the submission to the performances in the rebuttal).\n\n2. \"Propose an architecture that eliminates computationally expensive decoder, such as spatial or hypernetwork decoders.\"\n\nThe decoder contribution would need to be presented in detail and experimental results included to make it clear what the achieved improvements are. I would also note that prior work, e.g., [Chat and Thiyagalingam ICML 2023] use the same decoder across baselines in the interest of a fair evaluation. The present submission would do well to incorporate evaluations with a constant decoder.\n\n3. \"Drastically reduce computational costs while achieving superior disentanglement scores.\"\n\nUnclear where the reduced computational costs would come from (besides the change in decoder).\n\nGiven consideration to all of the above, I chose to keep my current score for the time being."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2862/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590552811,
                "cdate": 1700590552811,
                "tmdate": 1700590552811,
                "mdate": 1700590552811,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "avLj7xCyQ7",
                "forum": "fmAzKz9DJs",
                "replyto": "nHNKcGOZ3r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2862/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2862/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer"
                    },
                    "comment": {
                        "value": "Thank you for your response, and we appreciate that your views, and glad we added some clarity. Please find below our detailed response to your additional comments:\n\n1. **Comments on \u2018Each category corresponds to in the 3D space.\" -- this is not a proper sentence, nor is it clear\u2019:**\n\nThank you for your feedback, and we apologize for the missing word. The original sentence was: \n\u2018The distinct 2D projections that arise from the rotation of a 3D object can be seen as unique categories of in-plane rotation. Each category corresponds to *a specific orientation* in the 3D space.\u2019\n\nWe have revised all the response. We truly appreciate your feedback.\n\n2. **Introduce a translational and rotational equivariant encoder. (Q1)**\n\nWe would like to emphasize that the primary objective of DAE is to disentangle all features. However, as illustrated in Table 3 and 4, and Figure 9 and 10, DAE has poor performance in factorizing all features when the data has a rotational feature. Additionally, partial modifications to DAE may negatively impact its overall performance. This forms the motivation for the proposed work, CODAE, where we introduce two aspects, which we believe underpins the novelty: (i) a translational and rotational equivariant encoder, and (ii) a loss function based on image moments. \nWe are in agreement that neither the translational and rotational equivariant encoder nor image moments alone can improve the disentanglement scores, for the reasons of: \n\n* The translational and rotational equivariant encoder, designed following the Theorem 3.2, has the limitation in practice due to the impossibility of integration on S1. The computations are performed on a subset of S1, limiting rotational equivariance.\n* While image moments capture the centroid and orientation features with a high degree of accuracy, they can still have a degree of subtle inaccuracy.\n\nTherefore, we use the image moments as guiding principles during the initial stages of learning. Our evaluation results show that the combination of a translational and rotational equivariant encoder with image moments significantly improves disentanglement scores, particularly when the data involves rotational features. Figure 5 illustrates that the capability of the proposed architecture to learn semantic representations of objects when the objects of interest are positioned and oriented arbitrarily within the image.\n\nWe will add these as part of the modified version of the manuscript to improve the clarity.\n\n3. **Propose an architecture that eliminates computationally expensive decoder, such as spatial or hypernetwork decoders. (Q2)**\n\nWhile we understand the rationale for the suggestion, we politely would like to disagree with the reviewer\u2019s comments mainly because different architectures rely on its own decoder design. More specifically:\n\n* The contribution of Spatial-VAE lies in its spatial decoder.\n* The contribution of Target-VAE is its specialized encoder, built on top of the spatial decoder from Spatial-VAE.\n* The contribution of IRL-INR is its hypernetwork decoder.\n\nAs such, each model's distinctive feature is embedded in its decoder design. As such, it is almost impossible to rely on the same decoder unless we extensively modify their respective designs, distorting their purposes, which would be very unfair comparison. Given this, our method of comparison is fair retaining their design in the referenced papers [1, 2,3]. However, we used the same decoder for CODAE, DAE and $\\beta$-VAE, which is valid and permissible. \n\n\n4. **Drastically reduce computational costs while achieving superior disentanglement scores. (Q3)**\n\nContinuing from the previous response, each model has its own encoder or decoder model (as intended than modified or distorted) to learn semantic representations of objects when the objects of interest are positioned and oriented arbitrarily within the image. Our proposal in the paper involves designing a translational and rotational equivariant encoder while avoiding spatial or hypernetwork decoders. We believe this approach provides a clear rationale for reducing computational costs. \n\n\n\n[1] Explicitly disentangling image content from translation and rotation with spatial-vae, Neurips 2019\n\n[2] Unsupervised object representation learning using translation and rotation group equivariant vae, Neurips 2022\n\n[3] Rotation and translation invariant representation learning with implicit neural representations, ICML 2023"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2862/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601729839,
                "cdate": 1700601729839,
                "tmdate": 1700643695519,
                "mdate": 1700643695519,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LUfuIyf2l2",
            "forum": "fmAzKz9DJs",
            "replyto": "fmAzKz9DJs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2862/Reviewer_8vxB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2862/Reviewer_8vxB"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to disentangle the data into invariant and equivariant components. It builds up on DAE and introduces image moment based losses. The results are promising and the evaluation is extensive."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The method is simple to understand and well-written.\n- Evaluations are extensive for the disentanglement property."
                },
                "weaknesses": {
                    "value": "- Due to the deterministic and simple nature of the moment computation, it could be easy for the neural network to learn z_eq. Therefore, a result on ablating L_{moment} could be interesting to see the emergent properties just based on the reconstruction loss, and also could be a baseline, since moment loss is the only new component here. As a corollary, the moment loss could also be applied over other baselines to evaluate how much does it contribute in improving their performance.  \n- Novelty of the moment loss is very limited, as it is a widely known concept in the community.  \n- Results on 3D datasets, such as 3D airplanes, 3D teapots, 3D face could test the method more robustly as the shape also changes.  \n- GF-Score could be reported as proposed in the DAE paper."
                },
                "questions": {
                    "value": "- How is the moment computed for other factors such as shape and color? Is anything more than centroid and orientation that is part of z_{eq} on these datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2862/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2862/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2862/Reviewer_8vxB"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2862/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698845473526,
            "cdate": 1698845473526,
            "tmdate": 1699636229593,
            "mdate": 1699636229593,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NxAgf4a6Lw",
                "forum": "fmAzKz9DJs",
                "replyto": "LUfuIyf2l2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2862/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2862/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors - Thank you for the review"
                    },
                    "comment": {
                        "value": "We appreciate the time you've dedicated to reviewing our paper. Please find below our detailed response to your comments:\n\n1. **Ablation study (W)**\n\nWe conduct the ablation study, and the results indicate that the disentanglement scores increase with the inclusion of the $L_{moment}$. We will include the ablation study table in the paper and post it separately in the response soon. \n\n\n2. **Contributions of our paper (W)**\n\nWhile the concept of moment loss is widely recognized in the community, this paper introduces the first method to incorporate image moments into the learning mechanism -  which is not common. In addition, our contribution is not limited on $L_{moment}$. We:\n* Introduce a translational and rotational equivariant encoder.\n* Propose an architecture that eliminates computationally expensive decoder, such as spatial or hypernetwork decoders.\n* Drastically reduce computational costs while achieving superior disentanglement scores. \n\n\n3. **Additional experiments (W)**\n\nThank you for the comments. We have conducted additional experiments and new results will be posted separately soon.\n\n\n4. **GF-Score (W)**\n\nThank you for the comments. We have added the scores in the paper and will post it separately in the response soon. \n\n\n5. **Image moments for other factors such as shape and color**\n\nWe do not compute moments for other factors such as shape and color. Based on disentanglement literature (beta-VAE and DAE), these features can be learned. However, the table 3 and 4 show that these models struggle to learn features like shape and color when objects are captured with orientations. Therefore, the primary object of this paper is to guide the network to learn centroids and orientations using image moments, leading to the proposed model to effectively learn other features as part of $z_{inv}$.\n\n\n6. **Further discussion**\n\n Despite detailed problem statement, clear motivation and challenges around robustly learning centroid and orientations, especially in real images, theoretical framework, and extensive experimental results (using appropriate datasets), we received a score of three. We would greatly appreciate more detailed feedback that led to this score on these specific areas. Understanding these details will help us improve and refine our paper. Your insights are invaluable, and we are eager to make the necessary updates based on your feedback."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2862/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700102310863,
                "cdate": 1700102310863,
                "tmdate": 1700102310863,
                "mdate": 1700102310863,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vAHX5fdu79",
            "forum": "fmAzKz9DJs",
            "replyto": "fmAzKz9DJs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2862/Reviewer_ULgT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2862/Reviewer_ULgT"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to learn image representations with translational and rotational invariance and equivariance properties, under the guidance of the centroid and orientation information of images.  The model is trained with simple image reconstruction loss in the space of pixel intensity and image moments.  Experiments on several datasets (such as 5HD and MINIST) demonstrate that the proposed method outperforms existing methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It is technically reasonable to guide the learning of equivariant features with some spatial image statistics (such as image centroid). \n2. The paper is well-organized and easy to follow.  \n3. The performance across multiple benchmarks consistently shows the improvement of the proposed method over existing works."
                },
                "weaknesses": {
                    "value": "The main technical contribution of this work is to guide the learning of equivariant features with some spatial image statistics (such as image centroid). However, all the experiments are conducted on toy datasets such as MNIST digits which contain very simple 2D objects and almost uniform background region. This is also manifested in the evaluation scores. For example, in Table 1, all methods achieve over 97% accuracy. This leaves a question mark on how useful the proposed method is in practice where natural images are way more complicated and whether the simple spatial statistics are still sufficient."
                },
                "questions": {
                    "value": "How is the performance of the proposed method on natural image datasets, such as CIFAR or ImageNet where objects and background are more complicated and rotations are almost 3D (instead of just 2D in-plane rotation)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2862/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2862/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2862/Reviewer_ULgT"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2862/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699045888000,
            "cdate": 1699045888000,
            "tmdate": 1699636229531,
            "mdate": 1699636229531,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QKXFdqwOGc",
                "forum": "fmAzKz9DJs",
                "replyto": "vAHX5fdu79",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2862/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2862/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors - Thank you for the review"
                    },
                    "comment": {
                        "value": "Thank you for your comments and questions. \n\n1. **All the experiments are conducted on toy datasets such as MNIST digits which contain very simple 2D objects and almost uniform background region (W).**\n\nFirst of all, we would like to politely highlight that the statement \u201call the experiments are conducted on toy datasets such as MNIST digits which contain very simple 2D objects and almost uniform background region.\u201d Undermined all the efforts and work presented in this paper. In contrary to the statement, the evaluation includes two real world datasets, such as EMPIAR-10029 (protein) and Galaxy-Zoo datasets, where objects and background are complicated, and hence do not have a uniform background. Figures 4 and 5 clearly show that our proposed model learns key features of complicated datasets including centroids and orientations. Secondly, aside from Table 1, the disentanglement scores in Table 3 and 4 are far better than those presented in the literature and those achieved by the baselines presented in the paper. As such, we find the dismissal not only disheartening but also expects an unrealistic accuracy on these tasks. Please see our detailed responses below: \n\n\n* However, all the experiments are conducted on toy datasets such as MNIST digits which contain very simple 2D objects and almost uniform background region.\n\n> Please see the opening response above. Evaluations include rather complex EMPIAR and GalaxyZoo datasets. In particular the EMPIAR (10029) dataset includes 3D rotation.\n\n\n* How is the performance of the proposed method on natural image datasets, such as CIFAR or ImageNet\u2026..\n\n> The primary objective of the proposed work is to design a method that learns key features including centroid and orientation. Our selection of datasets is based on the literature and on existing work on disentanglement methods, but now includes additional 3D datasets as requested by the Reviewer 8vxB (3D datasets, such as 3D airplanes, 3D teapots, 3D face). As such, including CIFAR or ImageNet is beyond the scope of disentanglement studies. \n\n\n2. **Problem statement and motivation of our paper**\n\nWe agree that we could have phrased the problem statement much more clearly, which, based on the comments, rephrased as follows: \n\u2018The distinct 2D projections that arise from the rotation of a 3D object can be seen as unique categories of in-plane rotation. Each category corresponds to a specific orientation in the 3D space. This principle is widely applied in fields such as computer graphics, computer vision, and related areas where the understanding and manipulation of 3D objects in 2D images are essential. In this context, robust learning of centroids and orientations of objects in images, along with other features, is important, especially when objects are captured with 3D shifts and rotations such as protein, and galaxy studies.\u2019\n\n\n3. **Further discussion**\n\n We feel that the score of three (3) is purely based on misunderstanding of the difficulty of the challenges around robust learning of the centroids and orientations (please see [1, 2, 3]), our evaluation process (such as missing the fact that we included real-world and complex datasets) or overlooking the comparison in Table 3 and 4. As such, we feel that the lower scores are not justified.  We believe additional datasets, and more extensive evaluation, and our responses above clear these issues. We will be happy to receive more detailed feedback that led to this score on these specific areas. \n\n[1] Explicitly disentangling image content from translation and rotation with spatial-vae, Neurips 2019\n\n[2] Unsupervised object representation learning using translation and rotation group equivariant vae, Neurips 2022\n\n[3] Rotation and translation invariant representation learning with implicit neural representations, ICML 2023"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2862/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700101478387,
                "cdate": 1700101478387,
                "tmdate": 1700601793563,
                "mdate": 1700601793563,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]