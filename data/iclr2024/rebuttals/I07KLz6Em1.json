[
    {
        "title": "QuantEase: Optimization-based Quantization for Large Language Models"
    },
    {
        "review": {
            "id": "Hu7z4CU3f7",
            "forum": "I07KLz6Em1",
            "replyto": "I07KLz6Em1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8739/Reviewer_Y8vm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8739/Reviewer_Y8vm"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the method entitled QuantEase based on Coordinate Descent techniques that avoids matrix inversion and decomposition. Also, the paper proposes an outlier-aware approach by employing a sparse matrix with just a few non-zeros."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors propose derives a closed-form solution for fast update and presents a convergence analysis of QuantEase. Also, QuantEase can quantize up to OPT 66B on a single NVIDIA V100 32GB GPU."
                },
                "weaknesses": {
                    "value": "It is dubious whether the outlier-aware approach could be also accelerated like other approaches such as GPTQ and AWQ due to the presence of a sparse matrix, $\\hat{H}$. To validate the effectiveness of the outlier-aware approach, it seems to be required to measure the inference latency of the outlier-aware version of QuantEase.\n\u00a0\n\nAll experiments are based on perplexity, which is insufficient to assess whether QuantEase is effective or not. The zero-shot performance of common sense reasoning tasks or the five-shot accuracy of MMLU seems to be needed. \n\u00a0\n\nIn addition, all experiments are conducted for OPT and BLOOM models. The experiments for Llama or Llama 2 models should be necessary to justify the effectiveness of QuantEase."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8739/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8739/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8739/Reviewer_Y8vm"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8739/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698647688825,
            "cdate": 1698647688825,
            "tmdate": 1699637096719,
            "mdate": 1699637096719,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BLWBIhwV0l",
                "forum": "I07KLz6Em1",
                "replyto": "Hu7z4CU3f7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8739/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments. Here are our responses:\n\n1. **On the inference speed of outlier-aware QuantEase** - Please note that the correct algorithm to compare outlier-aware QuantEase with is SpQR.  As we note in Remark 2 of the paper, the setup of dividing weights into a set of quantized weights and a few outliers is standard and similar to the setting studied by SpQR. Therefore, our outlier-aware version enjoys the same speed-ups shown by SpQR using the implementation provided by them.\n\n2. **On perplexity vs. zero-shot performance** - Please note Figures 1 and 3 in the paper already measure zero-shot performance on the LAMBADA benchmark. From the figures, it is clear that **QuantEase outperforms other methods on zero-shot tasks**. Note that this is the setup also used by the GPTQ paper and related papers. \n\n3. **On experiments conducted for Llama** - We tried to get access to Llama weights by asking Meta, but they did not respond to our request. We note that OPT and BLOOM are publicly-available and cover a wide range of model sizes, making them suitable for our comparisons. Moreover, the GPTQ paper also reports results using the same models.\n\nBased on these clarifications, we would like to ask the reviewer to kindly consider increasing their score."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700095314030,
                "cdate": 1700095314030,
                "tmdate": 1700095314030,
                "mdate": 1700095314030,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CGxuQyg7iH",
                "forum": "I07KLz6Em1",
                "replyto": "Hu7z4CU3f7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8739/Reviewer_Y8vm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8739/Reviewer_Y8vm"
                ],
                "content": {
                    "comment": {
                        "value": "1. At the following url: https://openreview.net/forum?id=Q1u25ahSuy, some reviewers raised questions about the inference speed of SpQR, and those questions seems not to be dealt with yet. For this reason, until now, I cannot expect the speed-up of the outlier-aware version of QuantEase like a normal 3-bit or 4-bit weight quantization approach.\n\n2. Furthermore, the accuracy of QuantEase seems not to be better than existing methods based on experimental results that authors provided. First of all, many LLM papers like Llama papers [1, 2] measures the zero-shot performance of common sense reasoning tasks including BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-e, ARC-c, and OBQA. In addition, from my experience, the zero-shot performance of common sense reasoning tasks and the five-shot accuracy of MMLU are weakly correlated to the perplexity of WikiText2. In this sense, measuring the perplexity of WikiText2 and the accuracy of LAMBADA only seems not to be sufficient at all. \n\n[1] LLaMA: Open and Efficient Foundation Language Models\n\n[2] Llama 2: Open Foundation and Fine-Tuned Chat Models\n\n3. Using OPT and BLOOM only is too outdated because there are a couple of ways to use Llama models such as [3] and [4]. I cannot understand why the authors should have access to Llama weights by asking Meta, which appears to make no sense.\n\n[3] https://huggingface.co/huggyllama\n\n[4] https://huggingface.co/meta-llama\n\nAccordingly, I keep my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547495649,
                "cdate": 1700547495649,
                "tmdate": 1700547609415,
                "mdate": 1700547609415,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aMKW2oc9Py",
            "forum": "I07KLz6Em1",
            "replyto": "I07KLz6Em1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8739/Reviewer_3JsL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8739/Reviewer_3JsL"
            ],
            "content": {
                "summary": {
                    "value": "QuantEase is novel quantization method similar to GPTQ [1], but instead of Hessian-based second order optimization, QuantEase uses Coordinate Decent (CD) for much faster training. In addition to the improvement over GPTQ, QuantEase also provide an outlier-aware solution with sparse integration similar to SPQR [2].\n\n* [1] Frantar, E., Ashkboos, S., Hoefler, T. and Alistarh, D., 2022. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323.\n* [2] Dettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev, D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T. and Alistarh, D., 2023. SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression. arXiv preprint arXiv:2306.03078."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. QuantEase demonstrated the cheaper and faster coordinate decent method can achieve better result over Hessian-based GPTQ. GPTQ requires Cholesky decomposition of the Hessian matrix which is a big overhead for Neural Networks. Removing such overhead but achieving similar or better performance is a great contribution to quantization research.\n\n2. QuantEase's coordinate decent approach is quite orthogonal to other modern quantization techniques, such as integrating with sparsity as this work demonstrated, AWQ [1], and sub-channel quantization (used in GPTQ as well). The addictive impact of this work is promising.  \n\n* [1] Lin, J., Tang, J., Tang, H., Yang, S., Dang, X. and Han, S., 2023. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. arXiv preprint arXiv:2306.00978."
                },
                "weaknesses": {
                    "value": "1. All the experiments were based on per-channel quantization. Nevertheless, SOTA quantization research (GPTQ and AWQ) set the baseline with sub-channel quantization with group size of 128. Author justified their choice of comparing per-channel baseline for computational efficiency, yet there were no quantitative support for the argument. Providing a runtime benchmark would be a good support.\n\n2. Coordinate decent is often treated as approximation of Hessian based optimization. This work demonstrated CD performs better than GPTQ's Hessian. While it is encouraging, we'd like to see some explanation why it is the case."
                },
                "questions": {
                    "value": "Please explain why CD outperforms GPTQ's Hessian based optimization."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8739/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8739/Reviewer_3JsL",
                        "ICLR.cc/2024/Conference/Submission8739/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8739/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698717131795,
            "cdate": 1698717131795,
            "tmdate": 1700498311523,
            "mdate": 1700498311523,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gZ1j7Tkm57",
                "forum": "I07KLz6Em1",
                "replyto": "aMKW2oc9Py",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8739/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their encouraging and insightful comments. Below, we provide a point-by-point response to your comments.\n\n1. **Quantization with grouping** - Our claim that grouping can lead to slower inference is based on Table 3 of [3], which shows GPTQ with 128 grouping can be about an order of magnitude slower (in terms of inference latency) compared to ordinary GPTQ. Nevertheless, we had explored the performance of QuantEase with grouping, before deciding to not use grouping due to its overhead. These results are reported below. In these experiments, we use 3-bit quantization with either 128 or 256 grouping. Here, we report perplexity on wikitext2 datasets.  The first table below shows the results for the OPT family, and the second one for the BLOOM family.\n\n| Algorithm  | 350m | 1.3b | 2.7b | 6.7b |\n| --- | ----------- | -----------  | ----------- | ----------- |\n| GPTQ (g128) | $27.69_{0.27}$ |$16.29_{0.12}$ | $13.37_{0.06}$ |$11.37_{0.03}$|\n| QuantEase (g128) | $27.09_{0.07}$ | $16.01_{0.07}$| $13.54_{0.03}$ |$11.27_{0.01}$ |\n| GPTQ (g256) | $29.29_{0.32}$ | $17.22_{0.04}$ | $13.84_{0.04}$ | Did not run |\n| QuantEase (g256) | $27.89_{0.22}$ |$16.70_{0.06}$ | $13.79_{0.10}$ | Did not run |\n\nWe note that for the OPT family, except for the case of OPT-2.7b with 128-grouping, QuantEase outperforms GPTQ. \n\n| Algorithm  | 560m | 1b1 | 1b7 | 3b |\n| --- | ----------- | -----------  | ----------- | ----------- |\n| GPTQ (g128) | $25.77_{0.02}$ | $20.06_{0.02}$ | $17.30_{0.04}$ | $14.64_{0.01}$   |\n| QuantEase (g128) | $25.43_{0.03}$ | $19.66_{0.06}$ | $16.76_{0.02}$ |$14.59_{0.02}$ |\n| GPTQ (g256) |  $26.89_{0.06}$ | $20.79_{0.01}$ | $17.83_{0.04}$ |$14.81_{0.03}$ |\n| QuantEase (g256) | $26.38_{0.08}$ | $20.23_{0.03}$ | $17.13_{0.03}$ |$14.79_{0.02}$  |\n\nFor the BLOOM family, we note that QuantEase outperforms GPTQ in all cases.\n\n2. **CD vs. Hessian-based optimization**- We use cyclic CD where we optimize one coordinate at a time instead of all variables in one shot. Our single coordinate optimization does involve the hessian. CD is well-known to perform very well compared to gradient/hessian based methods when the optimization along a single coordinate is cheap, which is the case for our problem (see section 3.2 of the paper for the efficiency of our CD updates). In this sense, QuantEase is similar to, and motivated by Liblinear [1] and Improved Glmnet [2] which are classic methods from machine learning where CD works extremely effectively.\n\n\n3. **Why does Coordinate Descent (CD) outperform GPTQ** - This is an interesting but difficult question. Here we provide some points that differentiate the operation characteristics of CD-based QuantEase and GPTQ, which helps us to better understand the difference in performance.\n* First, as we note in Section 2, CD forms a descent procedure that decreases the objective value (i.e., the quantization error) over iterations, while ensuring the solution remains feasible (i.e., quantized). Specifically, after we complete a pass of CD over all coordinates, we achieve a feasible and quantized solution However, as CD updates preserve feasibility of the solution, we can perform another pass of CD on the coordinates, which by construction of CD, decreases (or at least, does not increase) the objective. This allows us to further decrease the quantization error over multiple passes, which is what we do. On the other hand, we note that GPTQ, by construction, stops after looping once over all columns of weights. GPTQ updates can also break the feasibility of the solution which shows GPTQ is not a descent procedure.\n* Second, GPTQ is based on Optimal Brain Surgeon (OBS)[4] and Optimal Brain Compression (OBC)[5] updates that were originally derived for neural network sparsification and pruning, rather than quantization. However, due to the discrete quantization structure, GPTQ uses an approximation to the exact OBS updates that have been derived before (this is discussed in details in Appendix A.1 of the paper). This can partially explain why GPTQ can lose in optimization performance. We note that as a by-product of these OBS-based updates, GPTQ requires the inversion of the Hessian, while our CD-based does not need that.\n\nIn the end, we would like to ask the reviewer to kindly revisit their evaluation and consider increasing the score. \n\n\n[1] C.J. Hsieh et al, A Dual Coordinate Descent Method for Large-scale Linear SVM, ICML 2008.\n\n[2] G.X. Yuan et al, An Improved GLMNET for L1-regularized Logistic Regression, JMLR 2012.\n\n[3] S. Kim et al., (2023). SqueezeLLM: Dense-and-Sparse Quantization.\n\n[4] Hassibi, Babak, David G. Stork, and Gregory J. Wolff. \"Optimal brain surgeon and general network pruning.\" IEEE international conference on neural networks. IEEE, 1993.\n\n[5] Frantar, Elias, and Dan Alistarh. \"Optimal brain compression: A framework for accurate post-training quantization and pruning.\" Advances in Neural Information Processing Systems 35 (2022): 4475-4488."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700095087347,
                "cdate": 1700095087347,
                "tmdate": 1700095087347,
                "mdate": 1700095087347,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cIirMOmzbE",
                "forum": "I07KLz6Em1",
                "replyto": "aMKW2oc9Py",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8739/Reviewer_3JsL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8739/Reviewer_3JsL"
                ],
                "content": {
                    "comment": {
                        "value": "I thank authors for the rebuttal.\n\nI agree that grouping can introduce extra latency penalties. However, the introduced latency is often smaller on 4-bits rather than 3-bits, due to the instructions needed for decoding 4-bits are much simpler due to its power-of-2 alignment.\n\nHowever, in the new benchmark authors demonstrated with grouping, the performance gain is negligibly small. Comparing this work to novel low bit work such as \"QuIP: 2-Bit Quantization of Large Language Models With Guarantees\", I decided to lower my rating from 6 to 3."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498262578,
                "cdate": 1700498262578,
                "tmdate": 1700498327384,
                "mdate": 1700498327384,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Cru81RFDpx",
                "forum": "I07KLz6Em1",
                "replyto": "NmvaSW5vn1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8739/Reviewer_3JsL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8739/Reviewer_3JsL"
                ],
                "content": {
                    "comment": {
                        "value": "I thank authors for the rebuttal.\n\nThe perplexity difference of GPTQ and QuantEase (with groups) is very small. I suggest the authors to emphasize training time or robustness of the proposed method can achieve similar quality. If the training time significantly dropped, it is considered a good drop-in replacement over GPTQ."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675002988,
                "cdate": 1700675002988,
                "tmdate": 1700675002988,
                "mdate": 1700675002988,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8fIjszPwgQ",
            "forum": "I07KLz6Em1",
            "replyto": "I07KLz6Em1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8739/Reviewer_g6vC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8739/Reviewer_g6vC"
            ],
            "content": {
                "summary": {
                    "value": "Proposes a method which uses coordinate descent techniques to perform layer-wise quantization, achieving 3-bit quantization"
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Interesting coordinate descent formulation, that seems to produce better quantization and is more efficient than another method, gptq.\n- nice presentation of the method, and reasonable set of empirical results"
                },
                "weaknesses": {
                    "value": "- I know it may be difficult getting compute resources, but if at all possible I would have liked to see results for AWQ and GPTQ on OPT-66b rather than just \u201cOOM\u201d. GPU memory constraints in quantization are not common, in my view. I don\u2019t want to fault the authors if they don\u2019t have access to larger compute resources."
                },
                "questions": {
                    "value": "- I am aware of another paper on arXiv called \"QuIP: 2-Bit Quantization of Large Language Models With Guarantees\" claiming 2 bit quantization for LLM models like OPT and LLama2. I know it was released recently over the summer, but could the authors comment on this work? How does AffineQuant perform at 2 bits?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8739/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8739/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8739/Reviewer_g6vC"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8739/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801408204,
            "cdate": 1698801408204,
            "tmdate": 1700715587852,
            "mdate": 1700715587852,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fLNf6aCCdI",
                "forum": "I07KLz6Em1",
                "replyto": "8fIjszPwgQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8739/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our Response"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our paper and your encouraging comments. Below, we provide responses to the two points you raised.\n\n- **AWQ/GPTQ memory issues** - As mentioned in the paper, both AWQ and GPTQ run out of memory on V100 GPUs when quantizing the OPT-66b model, owing to a limited memory of 32GB. Based on feedback from the reviewer, we re-ran the experiments on OPT-66b using A100 GPUs (we have limited availability of these GPUs, which made running these experiments difficult). Following are the results. As we can see, QuantEase outperforms both GPTQ and AWQ for this model.\n\n| Algorithm + bits | PPL (wikitext2) |\n| --- | ----------- |\n| GPTQ3 | $14.13_{0.43}$|\n| AWQ3 | $17.94_{0.18}$ |\n| QuantEase3 | $13.08_{0.38}$ |\n| GPTQ4 | $9.58_{0.05}$|\n| AWQ4 | $9.58_{0.01}$ |\n| QuantEase4 | $9.47_{0.02}$|\n\n\n\n- **Comparison to QuIP and 2-bit quantization** - Thanks for bringing our attention to this interesting paper. It seems the best results in [1] are achieved when incoherence processing is used, for example, in Table 1 of [1], the best performance is always achieved by an algorithm that uses incoherence processing. This is especially true for 2-bit quantization, where non-incoherence methods seem to perform poorly. However, incoherence processing needs projection steps---see line 3 of Algorithm 2 of [1] where quantized weights $W$ are rescaled using two orthogonal matrices $U,V$. As authors note in Section 4 of [1], producing and applying such projections requires a delicate bookkeeping of $U,V$ and possible computational overheads in the inference time.  As far as we could tell, the runtime implications of such overheads have not been explored numerically. This makes the practical applicability of incoherence processing for 2-bit quantization less clear. On the other hand, QuantEase performs standard quantization, which is commonly used in practice. \nHowever, we note that it seems incoherence processing can be treated as a module and integrated to any existing quantization method, as it is applied to GPTQ in [1]. As we see in [1], incoherence processing helps with boosting the performance of GPTQ, and as we show here, QuantEase can perform better than GPTQ. Therefore, we believe adding incoherence projection to QuantEase can lead to better quantization accuracy, although such a study is out of the scope of our paper. \n\n\nBased on our response here, we would like to kindly ask the reviewer to revisit their score. We will be happy to include snippets of this discussion in the final version of the paper, if the paper were to be accepted.\n\n\n\n[1]: QuIP: 2-Bit Quantization of Large Language Models With Guarantees"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700094519181,
                "cdate": 1700094519181,
                "tmdate": 1700094519181,
                "mdate": 1700094519181,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Kk58kVrVsR",
                "forum": "I07KLz6Em1",
                "replyto": "fLNf6aCCdI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8739/Reviewer_g6vC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8739/Reviewer_g6vC"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thanks for providing the additional experiments. \n\nI think it's a valid point about QuIP and it's current lack of practical implementation. \n\nAfter reading the discussion with other reviewers, I agree with the sentiment that the set of experimental results needs to be more expansive, on more models and more tasks. I know the authors have stated their difficulty in obtaining Llama2, and in getting increased computational resources. Unfortunately the state of LLM research these days is quite rapid and incurs a computational cost."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715545228,
                "cdate": 1700715545228,
                "tmdate": 1700715545228,
                "mdate": 1700715545228,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]