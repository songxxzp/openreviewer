[
    {
        "title": "Mitigating Label Noise on Graphs via Topological Curriculum Learning"
    },
    {
        "review": {
            "id": "WmDgNsKnRZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1257/Reviewer_gbE5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1257/Reviewer_gbE5"
            ],
            "forum": "e2rBzbWwGC",
            "replyto": "e2rBzbWwGC",
            "content": {
                "summary": {
                    "value": "This paper studies the problem of robust learning on graphs and proposes a new selection criteria named CBC, which are incorprated into a curriculum learning framework to select confident nodes. Authors experimentally show the superiority of our method compared with state-of-the-art baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation of the work is sound.\n\n2. The paper is well written.\n\n3. The paper is well organized."
                },
                "weaknesses": {
                    "value": "1. The incorporation of curriculum learning to graph robust learning has been studied in [1,2,3]. Although there are some differences, [1] can smoothly adapted into the studied problem. \n\n2. The performance compared with the best baseline is minor in a lot of datasets, e.g., PubMed of less than 1%, which is even bigger than the variance with different GNN architectures in Table 3. \n\n3. The performance with naive curriculum learning (selection with confidence) should be included in Figure 6. \n\n4. The introduction of CBC to topological curriculum learning is not clear. I expect to see the selection criteria more clear. However, authors show the Eqn. 2 with abstract W_\\lamdba. It should be a very simple selection criteria as from Algorithm in Appendix. \n\n[1] OMG: Towards Effective Graph Classification Against Label Noise, TKDE 2023.\n\n[2] Curriculum Graph Machine Learning: A Survey, IJCAI 2023.\n\n[3] CuCo: Graph Representation with Curriculum Contrastive Learning., IJCAI 2021."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1257/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1257/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1257/Reviewer_gbE5"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1257/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697328430577,
            "cdate": 1697328430577,
            "tmdate": 1699636052391,
            "mdate": 1699636052391,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JxkoejuoBO",
                "forum": "e2rBzbWwGC",
                "replyto": "WmDgNsKnRZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The response to Reviewer  gbE5 (Part 1)"
                    },
                    "comment": {
                        "value": "We would like to first thank Reviewer gbE5 for carefully reading our submission and we carefully considered your comments please find the answers to your questions below.\n\n> w1:\n\nWe have thoroughly reviewed listed papers [1, 2, 3], finding that papers [1] and [3] focus on graph classification tasks, which **significantly differ** from our study\u2019s focus on node classification. In graph classification, each graph is treated as an independent entity with a single label, akin to **i.i.d datasets**. Conversely, node classification deals with interdependent labeled nodes within a single interconnected graph, is **not a i.i.d dataset structure**. Therefore, methods suited for graph classification **cannot smoothly transfer** to node classification issues, due to their fundamental distinction within the graph machine learning community. The paper [2] provides a thorough survey of curriculum graph machine learning, but it **does not include** methods for implementing curriculum learning to tackle label noise in node classification task. We have already discussed this point on our **related work (Appendix B.4)**. \n\nWe appreciate the reviewer's recommendation, and for more discussion about the distinction from these works [1, 2, 3], please refer to the **revised Related Work (Appendix B)** section in the updated submission.\n\n[1] OMG: Towards Effective Graph Classification Against Label Noise, TKDE 2023.\n\n[2] Curriculum Graph Machine Learning: A Survey, IJCAI 2023.\n\n[3] CuCo: Graph Representation with Curriculum Contrastive Learning., IJCAI 2021.\n\n> w2: \n\nThe term \"minor\" indeed encompasses a broad assertion. First, it's crucial to highlight that in every cases, our approach **consistently outperforms** existing curriculum learning (CL) methods in terms of average accuracy. Second, **as the second-best baselines change in case-by-case noise levels, it is not reasonable to roughly consider the comparison between RCL with the case-by-case second-best baseline**. It is more fair to compare the **average improvement** between TCL and other baselines across all cases within a single dataset. These calculations, based on the data from our Tables 1 and 2, are **summarized in the table below**. The data presented in the table below illustrates the superiority of our method over the second-best approaches, with an significant improvement margin in all datasets. Notably, **while the second-best methods are different across various datasets, our method consistently emerges the top performance.** This evidence demonstrates that our approach substantially outperforms other baseline methods and is effective on noisy labeled graph.\n\n\n**The average improvement of the following table is calulated by averaging the accuracy improvement between TCL and other methods across all cases.** For example, as we can see, although Me-Momentum is competitive, TCL achieves 6.52\\%, 9.91\\%, and 18.29\\% improvement on average on Facebook, Physics and DBLP datasets.\n\n||  CORA   | CiteSeer  | Pubmed | WiKiCS | Facebook | Physics | DBLP |\n|----|  ----  | ----  | ----  | ----  | ----  | ----  |----  |\n|CP| 4.34  | 5.68 |1.49|4.51|**2.36**|**3.08**|7.72|\n|NRGNN| 3.11  |  **1.28**| 1.60|3.25|7.85|3.84|**1.55**|\n|PI-GNN| 2.40 |4.14|1.21|1.05|3.27|3.69|3.48|\n|Co-teaching+|4.18 |7.56|1.73|4.19|3.18|5.27|11.61|\n|Me-Momentum |**1.34** | 1.31|1.15|**0.97**|6.52|9.91|18.29|\n|MentorNet |5.11 | 8.98 |2.24|5.85|4.63|6.89|14.71|\n|CLNode  |6.22|10.73|**1.09** |1.38|2.91|3.42|3.74|\n|RCL  |15.83 |18.49|5.04|7.83|11.07|10.69|14.84|\n\nIn Tables 1 and 2, where we compare accuracy, we ensure consistency in the backbone model of our work with other baselines to maintain a fair comparison. Hence, the variance across different GNN architectures cannot be considered as a factor to influence the accruacy comparsion between our method and baseline methods."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376917548,
                "cdate": 1700376917548,
                "tmdate": 1700376917548,
                "mdate": 1700376917548,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BVtYvyTSem",
                "forum": "e2rBzbWwGC",
                "replyto": "WmDgNsKnRZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The response to Reviewer gbE5 (Part 2)"
                    },
                    "comment": {
                        "value": "> w3: \n\nActually, **the \"Vanilla\" method in Figure 6 is the naive curriculum learning** that selects nodes according to the confidence during training. We have highlighted this point in the revised caption of Figure 6. \n\nAdditionlly, to further address the concren, we have include two additional curriculum leanring method, e.g. MentorNet and Me-momentum, which use the curriculum learning to address label noise but do not employ the CBC measure. These two methods could be considered as other curriculum learning method without the aid of CBC measure. The updated experiment are now included into **the Figure 10 in Appendix E.1**. The results further demonstrate the effectiveness of our proposed CBC measure.\n\n> w4: \n\nIn the curriculum learning literatrue [1, 2], the key is a measure that can estimate the learning difficulty of each instance. Following the similar spirit, our CBC is worked as a difficulty measurer in the topological curriculum learning. Specifically, the $W_\\lambda$ in Eqn.2 is a weight function for each confident nodes, which is defined by our CBC measurement. This formalization has been widely used in the related curriculum learning literature [1, 2]. With the help of the CBC measure, we can design an robust \"easy-to-hard\" learning curriculum, akin to the approach outlined in **lines 3-6 of Algorithm 1 in Appendix D.3**. \n\nTo address the reviewer's concern, we have **enriched the discussion in the corresponding parts to improve the clarity of CBC for topological curriculum learning** in the revised submission. Thank you very much for the suggestion.\n\n[1] Bengio, Y., Louradour, J., Collobert, R., & Weston, J. (2009, June). Curriculum learning. In Proceedings of the 26th annual international conference on machine learning (pp. 41-48).\n\n[2] Wang, X., Chen, Y., & Zhu, W. (2021). A survey on curriculum learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9), 4555-4576."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377000123,
                "cdate": 1700377000123,
                "tmdate": 1700377000123,
                "mdate": 1700377000123,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2uU6XIYjbo",
                "forum": "e2rBzbWwGC",
                "replyto": "WmDgNsKnRZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewers gbE5,\n\nWe have diligently addressed most of your concerns. If you still have concerns about our paper, please join the rolling discussion, where we await your valuable insights and comments.\n\nThanks."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533299047,
                "cdate": 1700533299047,
                "tmdate": 1700533299047,
                "mdate": 1700533299047,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4RTfnqM6jk",
                "forum": "e2rBzbWwGC",
                "replyto": "WmDgNsKnRZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer gbE5:\n\nOur sincere thanks go out to you for reviewing this paper! We do our best to address the issues raised by relevant work, experimental results and more empirical evidence. The discussion deadline is approaching now. Is there anything unclear in these explanations and descriptions?\n\nIt would be greatly appreciated if your concerns had been addressed. However, if you require any further clarification, we can provide it before the deadline for discussion.\n\nThanks!\n\nAuthors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614116066,
                "cdate": 1700614116066,
                "tmdate": 1700614116066,
                "mdate": 1700614116066,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Hj2Tc0BBaM",
                "forum": "e2rBzbWwGC",
                "replyto": "4RTfnqM6jk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1257/Reviewer_gbE5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1257/Reviewer_gbE5"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your comment. I checked the general response. R2 comment \"However, it does not theoretically or, more shallowly, logically demonstrate or explain the connection between the proposed CBC criterion and noise labels.\" while in general response it says \"supported by theoretical results (R1,R2,R3) \"."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736806058,
                "cdate": 1700736806058,
                "tmdate": 1700736806058,
                "mdate": 1700736806058,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Hb92HaJbca",
                "forum": "e2rBzbWwGC",
                "replyto": "WmDgNsKnRZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1257/Reviewer_gbE5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1257/Reviewer_gbE5"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your comment. However, it seems what R2's really rate about \"theoretical results\" is the other one and I suggested to revise the general response. Moreover, I checked the other's comment with soundness and performance, and decided to keep the rating."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739254888,
                "cdate": 1700739254888,
                "tmdate": 1700739315098,
                "mdate": 1700739315098,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tBS297r4ul",
            "forum": "e2rBzbWwGC",
            "replyto": "e2rBzbWwGC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1257/Reviewer_GKC8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1257/Reviewer_GKC8"
            ],
            "content": {
                "summary": {
                    "value": "This paper study the problem of node classification when the graph data is noisily labeled. The authors introduce the class-conditional betweenness centrality as the measure of the confidence of a node having noise label, and then apply the curriculum learning framework to guide model learning. They further show that this framework minimizes an upper bound of the expected risk under target clean distribution. Experiments on benchmark datasets demonstrate the effective of their proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method is novel and reasonable, which incorporates the graph structural information to judge the confidence of nodes. The theoretical results also provide a suitable explanation for the effectiveness of TCL.\n- The experimental results are substantial and credible, which is sufficient to support the effectiveness of TCL."
                },
                "weaknesses": {
                    "value": "- It seems that there exists some error in the proof of Theorem 1 (See Questions for more detail).\n- The proposed framework relies on the homogeneity of graph data, which could limit its applications in real-world scenarios, where the connected nodes may belong to different classes."
                },
                "questions": {
                    "value": "1. In Eq. (18), the authors use the following inequality:\n$$\n\\frac{1}{m} \\mathbb{E}_{\\sigma} \\left[ \\mathop{\\rm sup}\\_{\\Vert \\mathbf{w} \\Vert \\leq B } \\sum\\_{i=1}^m \\sigma_i \\vert \\mathbf{w} \\mathbf{x}_i \\vert \\right] \\leq \\frac{B}{m} \\mathbb{E}\\_{\\sigma} \\left[ \\mathop{\\rm sup}\\_{\\Vert \\mathbf{w} \\Vert \\leq B} \\Vert \\sum\\_{i=1}^m \\sigma_i \\mathbf{x}_i \\Vert \\vert \\right].\n$$\n\nI don't think this inequality holds true. To remedy this, I recommend the following proof:\n$$\n\\begin{aligned}\n& \\frac{1}{m} \\mathbb{E}_{\\sigma} \\left[ \\mathop{\\rm sup}\\_{\\Vert \\mathbf{w} \\Vert \\leq B } \\sum\\_{i=1}^m \\sigma_i \\vert \\mathbf{w} \\mathbf{x}_i \\vert \\right] \\\\\\\\\n\\leq & \\frac{B}{m} \\mathbb{E}\\_{\\sigma} \\left[  \\sum\\_{i=1}^m \\sigma_i \\Vert \\mathbf{x}_i \\Vert \\right] \\leq \\frac{B}{m} \\mathbb{E}\\_{\\sigma} \\left[ \\left\\vert  \\sum\\_{i=1}^m \\sigma_i \\Vert \\mathbf{x}_i \\Vert \\right\\vert \\right] \\\\\\\\\n= &  \\frac{B}{m} \\mathbb{E}\\_{\\sigma} \\left[ \\sqrt{\\left( \\sum\\_{i=1}^m \\sigma_i \\Vert \\mathbf{x}_i \\Vert \\right)^2} \\right] =  \\frac{B}{m} \\mathbb{E}\\_{\\sigma} \\left[ \\sqrt{ \\sum\\_{i,j=1}^m \\sigma_i \\sigma_j \\Vert \\mathbf{x}_i \\Vert \\Vert \\mathbf{x}_j \\Vert} \\right] \\\\\\\\\n\\leq & \\frac{B}{m} \\sqrt{ \\mathbb{E}\\_{\\sigma}  \\left[ \\sum\\_{i,j=1}^m \\sigma_i \\sigma_j \\Vert \\mathbf{x}_i \\Vert \\Vert \\mathbf{x}_j \\Vert\\right] } = \\frac{B}{m} \\sqrt{ \\sum\\_{i=1}^m \\Vert \\mathbf{x}_i \\Vert^2} \\leq \\frac{BR}{\\sqrt{m}}.\n\\end{aligned}\n$$\nAlso, in the first equation, the term $sgn(\\mathbf{w}_i \\mathbf{x}_i)$ should be corrected as $sgn(\\mathbf{w} \\mathbf{x}_i)$.\n\n2. The noise label situation has a close relation with the out-of-distribution (OOD) situation. Could the proposed framework be applied to OOD generalization [1] or detection [2] tasks?\n\n3. In Algorithm 1, why the authors use a pretrained classifier $f^p_{\\mathcal{G}}$? What pretrain method was used to obtain $f^p_{\\mathcal{G}}$? Do different pretrain methods affect the final performance of the model?\n\n[1] Wu et al, Geometric knowledge distillation for topological shifts. NeurIPS 2022.\n\n[2] Wu et al, Energy-based detection model for OOD nodes on graphs. ICLR 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1257/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1257/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1257/Reviewer_GKC8"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1257/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698478684784,
            "cdate": 1698478684784,
            "tmdate": 1699636052336,
            "mdate": 1699636052336,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "24kxTtUrih",
                "forum": "e2rBzbWwGC",
                "replyto": "tBS297r4ul",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for taking the time to review our manuscript and providing us with your insightful feedback. We carefully considered your comments and please find the answers to your questions below.\n\n> w1 & Q1: \n\nThank you very much for your pretty detailed advice and we already updated our proof in **Appendix C.2** following your advice.\n\n> w2:\n\nWe would like to kindly argue that our proposed framework **still remains effective on heterogeneous graphs**, as even for heterogeneous graphs, the proposed CBC measure can still efficiently distinguish the easy nodes and hard nodes. To address the reviewer's concern, we have included additional toy experiments in **Fig. 11 (Appendix E.2)**. These experiments empirically demonstrate the **effectiveness of our CBC measure in graphs with different homophily ratios**, reinforcing its utility in heterogeneous graph. Besides, we have extended our experimentation to include three commonly used heterogeneous datasets (the DBLP dataset has been included in our experiment) under 30% instance-dependendent label noise. The summary of experimental results in the below table. As can be seen, our method still show the superior performance over a range of baselines. Additionally, it's important to note that the recent research has already demonstrated the capability of GNN to handle heterogeneous graphs [1,2].\n\n|dataset | DBLP   | Chameleon | Squirrel |\n|----|  ----  | ----  | ----  |\n|CP|70.02 | 55.08| 43.42|\n|NRGNN|72.48 |49.02 | 41.35|\n|PI-GNN|71.72 |52.85 |43.31 |\n|Co-teaching+| 66.32| 53.07 | 39.48 |\n|Me-Momentum |59.88  |55.01 | 44.38 |\n|MentorNet | 63.73 |53.73 |39.63 |\n|CLNode  | 72.32|52.85 |35.92 |\n|RCL  | 63.20|52.96 | 40.59 |\n|**TCL**| **74.70**|  **56.17** | **48.03** |\n\n[1] Ma, Y., Liu, X., Shah, N., & Tang, J. Is Homophily a Necessity for Graph Neural Networks?. ICLR 2021.\n\n[2] Luan, S., Hua, C., Xu, M., Lu, Q., Zhu, J., Chang, X. W., ... & Precup, D. (2023). When do graph neural networks help with node classification: Investigating the homophily principle on node distinguishability. NIPS 2023\n\n> Q2: \n\nWe appreciate about the reviewer's question. We guess that the proposed CBC measure may also benefit to the OOD scenarions, as the noise can be intrinsically considered as the OOD data. We **have included extra discussion about the potential connection with the OOD generalization [1] or detection [2], which we place in Appendix A.3** of the revised submission. \n\n[1] Wu et al, Geometric knowledge distillation for topological shifts. NeurIPS 2022.\n\n[2] Wu et al, Energy-based detection model for OOD nodes on graphs. ICLR 2023.\n\n> Q3:\n\nThe pretrained classifier is used to extract the confident nodes from the noisy labeled graph and we obtain a pertrained GNN classifier based on the memorization effect of neural networks. This is because the GNN classifier pre-trained at early epochs would fit the clean data well but not the incorrectly labeled data. Thus, this pre-trained model serves as an efficient tool for identifying confident nodes within the graph. This part has been detailed illustrated in the part of **section 2.3**.\n\nDifferent pretraining methods do not significantly influence the performance of our model. This is primarily because the essence of our TCL framework lies in the implementation of the \"easy-to-hard\" curriculum learning mechanism (the Fig.6). This principle has been empirically substantiated in our extensive ablation study. Consequently, the role of pretraining methods is confined to rigorously extracting confident nodes, thereby ensuring that they do not substantially affect the final outcomes of our TCL framework."
                    },
                    "title": {
                        "value": "The response to Reviewer GKC8"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376641270,
                "cdate": 1700376641270,
                "tmdate": 1700376667354,
                "mdate": 1700376667354,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mxjik7F69b",
                "forum": "e2rBzbWwGC",
                "replyto": "tBS297r4ul",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewers GKC8,\n\nWe have diligently addressed most of your concerns. If you still have concerns about our paper, please join the rolling discussion, where we await your valuable insights and comments.\n\nThanks."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533271921,
                "cdate": 1700533271921,
                "tmdate": 1700533271921,
                "mdate": 1700533271921,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2Z2l6Ewdul",
                "forum": "e2rBzbWwGC",
                "replyto": "tBS297r4ul",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer GKC8:\n\nOur sincere thanks go out to you for reviewing this paper! We do our best to address the issues raised by relevant work, experimental results and more empirical evidence. The discussion deadline is approaching now. Is there anything unclear in these explanations and descriptions?\n\nIt would be greatly appreciated if your concerns had been addressed. However, if you require any further clarification, we can provide it before the deadline for discussion.\n\nThanks!\n\nAuthors"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614088309,
                "cdate": 1700614088309,
                "tmdate": 1700614088309,
                "mdate": 1700614088309,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yDrkDgqAFU",
                "forum": "e2rBzbWwGC",
                "replyto": "tBS297r4ul",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1257/Reviewer_GKC8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1257/Reviewer_GKC8"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Authors"
                    },
                    "comment": {
                        "value": "Thanks the authors for their responses. My concerns are well addressed. Currently, I would like to keep my initial score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648807327,
                "cdate": 1700648807327,
                "tmdate": 1700648862788,
                "mdate": 1700648862788,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "f030v48R6B",
            "forum": "e2rBzbWwGC",
            "replyto": "e2rBzbWwGC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1257/Reviewer_cJzR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1257/Reviewer_cJzR"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Class-conditional Betweenness Centrality (CBC) as a robust measure to address graph label noise and develop a Topological Curriculum Learning (TCL) framework guided by CBC, which enhances model learning and outperforms existing methods both theoretically and experimentally."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors propose a novel metric for assessing the cleanliness of labels, and the paper is clear, understandable, with extensive experiments"
                },
                "weaknesses": {
                    "value": "However, I have some concerns about certain aspects of the paper:\n- This paper seems to address the problem of noisy labels on graphs with the concept of curriculum learning and Pagerank. However, it does not theoretically or, more shallowly, logically demonstrate or explain the connection between the proposed CBC criterion and noise labels. Alternatively, it does not clearly state the rationality of the CBC criterion to eliminate the negative effect of noisy labels. It is insufficient to show the correlation between CBC and class boundary nodes under the setting of noisy labels. Moreover, empirical results on a single dataset are not enough to support some conclusive statements in the paper. \n- Theorem 1 draws the conclusion from the i.i.d data but not the graph-structured data directly. It is better to show a more explicit upper bound from the graph itself. \n- Please explain the meaning of positive/negative samples and error distribution in Theorem 1. In addition, the upper bound relates to the clean distribution, but not the noisy distribution. Is it independent of the noise factor? If so, how can Theorem 1 be used to illustrate the connection between the proposed method and noisy labels?\n- Some of the latest papers on label noise in graphs haven't been adequately mentioned and compared, which weakens the persuasiveness of the paper: (1) ALEX: Towards Effective Graph Transfer Learning with Noisy Labels. MM'23 (2) Learning on Graphs under Label Noise. ICASSP'23 (3) GNN Cleaner- Label Cleaner for Graph Structured Data. TKDE'23"
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1257/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1257/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1257/Reviewer_cJzR"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1257/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698484002517,
            "cdate": 1698484002517,
            "tmdate": 1699636052253,
            "mdate": 1699636052253,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hE3jSaL630",
                "forum": "e2rBzbWwGC",
                "replyto": "f030v48R6B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The response to Reviewer cJzR (Part 1)"
                    },
                    "comment": {
                        "value": "We sincerely thank you for taking the time to review our manuscript and providing us with your insightful feedback. We carefully considered your comments and please find the answers to your questions below.\n\n> w1: \n\nFor the first question, **our proposed CBC measure does not operate in isolation for handling noisy labels; instead, it functions in collaboration with the proposed TCL framework.** With the help of CBC, our TCL is a novel graph curriculum learning framework that can first learn clean easy nodes and then lean noisy hard nodes. This \"easy-to-hard\" training curriculum is effective on handling label noise and depend on the propose of CBC. Consequently, the proposed CBC, in collaboration with the TCL framework, effectively addresses label noise. The development of **both the TCL framework and the CBC measure are interdependent** and constitute the core contributions of our work.\n\nFor the second question, we employed the CBC measure to identify class boundary nodes based on their topological structure. This measure **primarily utilizes topological structure, making it less susceptible to the impact of label noise**. This characteristic has been empirically demonstrated in our work, as evidenced in Figures 2 and 3 with different datasets, such as Cora and WikiCS. These figures, which can be viewed as toy experiments, clearly illustrate that **class boundary nodes are assigned high CBC scores even in scenarios with a high ratio of label noise**. Additionally, to further address concerns regarding this aspect of our research, we have included **an extra experiment in Figure 11 (Appendix E.2)**. In this experiment, we altered the structure of the dataset under label noise conditions, demonstrating that our CBC score can **consistently identify class boundary nodes across various datasets**.\n\n\n> w2: \n\nWe appreciate the reviewer's question about this theory. \n\nIn Theorem 1, we would like to kindly clarity that we used the widely acknowledged **local-dependence assumption** applicable to graph-structured data. This assumption aligns with Markov chain principles, stating that **the node is independent of the nodes that are not included in their two-hop neighbors when utilizing two-layer GNN, which does not means the totally i.i.d w.r.t. each node but means i.i.d w.r.t. subgroups**. The local-dependence assumption is well-established and has been **widely adopted in numerous graph theory studies** [1] [2] [3]. It endows models with desirable properties which make them amenable to statistical inference [2]. Consequently, this assumption forms a sound basis for our Theorem 1, which from risk-consistent statistical theory. The derived upper bound, based on this assumption, is both **reasonable and applicable** within the context of graph theory.\n\nTo clarity the rationality behind the theory deduction, we have added more discussion with similar works for reference in the corresponding parts of the revised submission.  \n\n[1] Wu, T., Ren, H., Li, P., & Leskovec, J. (2020). Graph information bottleneck. Advances in Neural Information Processing Systems, 33, 20437-20448.\n\n[2] Schweinberger, M., & Handcock, M. S. (2015). Local dependence in random graph models: characterization, properties and statistical inference. Journal of the Royal Statistical Society Series B: Statistical Methodology, 77(3), 647-676.\n\n[3] Didelez, V. (2008). Graphical models for marked point processes based on local independence. Journal of the Royal Statistical Society Series B: Statistical Methodology, 70(1), 245-264."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376416199,
                "cdate": 1700376416199,
                "tmdate": 1700376416199,
                "mdate": 1700376416199,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vtfqdKsSSo",
                "forum": "e2rBzbWwGC",
                "replyto": "f030v48R6B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The response to Reviewer cJzR (Part 2)"
                    },
                    "comment": {
                        "value": "> w3: \n\nThe theorey of TCL is based on the binary classification setting and the postive/negative sample correspond to the nodes have postive/negative labels. **The error distirbution refects the difference between the noisy distirbution and the clean distirbution**. Essentially, this error distribution serves as a bridge connecting the noisy and clean distributions in our upper bound and we have **reinforced** this point in our theorem 1 of the revised submission. \n\nIn the last two rows of our upper bound, the KL-divergence between the error distribution E and the clean distribution serves as an indicator of the divergence between the noisy and clean distributions. Essentially, the greater the deviation of the error distribution E from the clean distribution, the larger the difference between the noisy and clean distributions. This increased divergence directly increase the upper bound of the expected risk, making it more challenging to learn a proper classifier.\n\nNote that in the upper bound, the $0 < \\alpha_{\\lambda} <= 1$ is a decresing parameter, diminishing as the volume of training data increases in curriculum learning. Consequently, **training a classifier directly with the entire noisy dataset can lead to suboptimal generalization**. This is primarily due to the exacerbated deviation between the noisy and clean distributions. This reveals the significance of adopting an 'easy-to-hard' learning curriculum that first train model on a subset of easy clean node and progressively incorporating the noisy hard nodes. This training process effectively mitigates the adverse impact of the noisy distribution. Thus, this theory aligns with our proposed method, highlighting its relevance and effectiveness.\n\n\n>w4: \n\nThank you very much for recommending these excellent works. Following the advice, we have added discussion about these three works in **the revised first paragraph of our introduction**. Furthermore, a detailed comparison between these works and our own research can be found in the part of **revised related work (Appendix B.3)**. We summarize the comparison as follows:\n\nThe paper [1] and [2] are basd on the homophily assumption and employ the power of graph contrastive learning to learn robust node representations in the presence of label noise. The paper [3] employs the topological structure of the graph, along with a set of clean nodes, to mitigate label noise in graph data. These three approaches offers unique insights into handling label noise. **Different from these works,** we develop a novel graph curriculum learning framework that can learn easy clean nodes first and then learn from hard noisy nodes, with the aid of the proposed CBC measure that **robustly** identifies the learning difficulty of nodes in a graph with noisy labels. The design of the CBC measure is **novel** in the graph curriculum learning, marking it as a **significant contribution** in this field.\n\nBesides, as these works have not open their official codes, we are still reproducing them to complement the empirical comparison. Once we finished the experiments, we will place the results here and update in our submission. Thank you again for the recommendation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376477733,
                "cdate": 1700376477733,
                "tmdate": 1700376477733,
                "mdate": 1700376477733,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iwjognd4hg",
                "forum": "e2rBzbWwGC",
                "replyto": "f030v48R6B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewers cJzR,\n\nWe have diligently addressed most of your concerns. If you still have concerns about our paper, please join the rolling discussion, where we await your valuable insights and comments.\n\nThanks."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533241087,
                "cdate": 1700533241087,
                "tmdate": 1700533241087,
                "mdate": 1700533241087,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PTT2u5RcaO",
                "forum": "e2rBzbWwGC",
                "replyto": "f030v48R6B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer cJzR:\n\nOur sincere thanks go out to you for reviewing this paper! We do our best to address the issues raised by relevant work, experimental results and more empirical evidence. The discussion deadline is approaching now. Is there anything unclear in these explanations and descriptions?\n\nIt would be greatly appreciated if your concerns had been addressed. However, if you require any further clarification, we can provide it before the deadline for discussion.\n\nThanks!\n\nAuthors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614057469,
                "cdate": 1700614057469,
                "tmdate": 1700614057469,
                "mdate": 1700614057469,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K4BtEEsoYl",
                "forum": "e2rBzbWwGC",
                "replyto": "PTT2u5RcaO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1257/Reviewer_cJzR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1257/Reviewer_cJzR"
                ],
                "content": {
                    "comment": {
                        "value": "The author has partially addressed my concerns and I will keep my score unchanged"
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700742144361,
                "cdate": 1700742144361,
                "tmdate": 1700742144361,
                "mdate": 1700742144361,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LwiqEDZII1",
            "forum": "e2rBzbWwGC",
            "replyto": "e2rBzbWwGC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1257/Reviewer_rtA1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1257/Reviewer_rtA1"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a metric called Class-conditional Betweenness Centrality, which is used for easy-to-hard sample selection in Curriculum Learning for graph data. Based on this, the authors design a Topological Curriculum Learning (TCL) framework and prove that it minimizes an upper bound of the expected risk."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The robustness and effectiveness of the Class-conditional Betweenness Centrality metric have been thoroughly and experimentally validated.\n\n2. The CBC measure and the corresponding Topological Curriculum Learning are intuitive and meaningful in the graph curriculum learning."
                },
                "weaknesses": {
                    "value": "1. The proposed Class-conditional Betweenness Centrality (CBC) is to some extent influenced by the homo. ratio of the dataset. Compared to Cora, conducting toy experiments with certain specifically synthesized data would be more accurate.\n\n2. The paper lacks a more specific introduction to the impact of over-smoothing on Curriculum Learning for graph data, making it somewhat disjointed.\n\n3. The improvements over existing CL methods are not significant."
                },
                "questions": {
                    "value": "1. Fig.6 does not provide a comparison of effectiveness between other Curriculum Learning methods.\n2. Is there any experimental results based on models that can effectively alleviate over-smoothing, used as the GNN backbone?\n3. The features in Fig 2, are their features after being processed by GNN? If not, could authors provide that visualizations?\n4. Will the proposed method be effective on homophily datasets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1257/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722076785,
            "cdate": 1698722076785,
            "tmdate": 1699636052157,
            "mdate": 1699636052157,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dteq4soRhj",
                "forum": "e2rBzbWwGC",
                "replyto": "LwiqEDZII1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The response to Reviewer rtA1 (Part 1)"
                    },
                    "comment": {
                        "value": "We thank Reviewer rtA1 for reviewing our paper and providing helpful feedback on our work. We address many of Reviewer rtA1\u2019s concerns in the general response above.\n\n> w1\uff1a\n\nThank you for the suggestion. To answer the reviewer's question, we have included additional toy experiments in **Fig. 11 (Appendix E.2)** that vary homo. ratios. According to Fig. 11, a decrease in the homophily ratio results in an increased number of nodes near class boundaries, which consequently exhibit higher CBC scores. The experiments demonstrate that our CBC measure can effectively reflect the topology of nodes with different homophily ratios, even as the complexity of the graph increases, which reinforces its utility in diverse graph structures.\n\n> w2:\n\nNote that, this work take advantage of curriculum learning to handle the label noise on graph data. In Graph Neural Networks (GNNs), **\"over-smoothing\"** refers to the phenomenon where, as the network depth increases, node features become increasingly similar. This similarity **poses a challenge when employing curriculum learning with label noise**, making it difficult to distinguish between \"easy\" and \"hard\" nodes due to the homogenization of features caused by over-smoothing. Furthermore, this issue persists even in shallow GNNs, leading to under-confident predictions and complicating the establishment of an \"easy-to-hard\" training curriculum, as noted in previous studies [1][2]. Addressing this challenge, our work introduces a novel methd, which proposes **a robust CBC measure**. This measure effectively distinguishes between \"easy\" and \"hard\" nodes, taking into account the graph structure rather than the prediction of GNNs, thereby **mitigating the over-smoothing problem**.\n\nTo address the concern, we have revised the paper by **emphasizing this aspect in the introduction** for readability.  Additionally, we have included **a more detailed discussion** regarding this part in the **related work (Appendix B.1)**, to provide a comprehensive understanding for the readers.\n\n[1] Wang, X., Liu, H., Shi, C., & Yang, C. (2021). Be confident! towards trustworthy graph neural networks via confidence calibration. Advances in Neural Information Processing Systems, 34, 23768-23779.\n[2] Hsu, H. H. H., Shen, Y., Tomani, C., & Cremers, D. (2022). What Makes Graph Neural Networks Miscalibrated?. Advances in Neural Information Processing Systems, 35, 13775-13786.\n\n>w3: \n\nThe term \"significant\" indeed encompasses a broad assertion. First, it's crucial to highlight that in every cases, our approach **consistently outperforms** existing curriculum learning (CL) methods in terms of average accuracy. Second, **as the second-best baselines change in case-by-case noise levels, it is not reasonable to roughly consider the comparison between RCL with the case-by-case second-best baseline**. It is more fair to compare the **average improvement** between TCL and other baselines across all cases within a single dataset. These calculations, based on the data from our Tables 1 and 2, are **summarized in the table below**. The data presented in the table below illustrates the superiority of our method over the second-best approaches, with an significant improvement margin in all datasets. Notably, **while the second-best methods are different across various datasets, our method consistently emerges the top performance.** This evidence demonstrates that our approach substantially outperforms other baseline methods and is effective on noisy labeled graph.\n\n**The average improvement of the following table is calulated by averaging the accuracy improvement between TCL and other methods across all cases.** For example, as we can see, although Me-Momentum is competitive, TCL achieves 6.52\\%, 9.91\\%, and 18.29\\% improvement on average on Facebook, Physics and DBLP datasets.\n|Baselines|  CORA   | CiteSeer  | Pubmed | WiKiCS | Facebook | Physics | DBLP |\n|----|  ----  | ----  | ----  | ----  | ----  | ----  |----  |\n|CP| 4.34  | 5.68 |1.49|4.51|**2.36**|**3.08**|7.72|\n|NRGNN| 3.11  |  **1.28**| 1.60|3.25|7.85|3.84|**1.55**|\n|PI-GNN| 2.40 |4.14|1.21|1.05|3.27|3.69|3.48|\n|Co-teaching+|4.18 |7.56|1.73|4.19|3.18|5.27|11.61|\n|Me-Momentum |**1.34** | 1.31|1.15|**0.97**|6.52|9.91|18.29|\n|MentorNet |5.11 | 8.98 |2.24|5.85|4.63|6.89|14.71|\n|CLNode  |6.22|10.73|**1.09** |1.38|2.91|3.42|3.74|\n|RCL  |15.83 |18.49|5.04|7.83|11.07|10.69|14.84|"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376046810,
                "cdate": 1700376046810,
                "tmdate": 1700376046810,
                "mdate": 1700376046810,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "re22yhHczJ",
                "forum": "e2rBzbWwGC",
                "replyto": "LwiqEDZII1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The response to Reviewer rtA1 (Part 2)"
                    },
                    "comment": {
                        "value": "> Q1:\n\nThank you for your suggestion. Actually, the purpose of this section is to conduct an ablation study evaluating the \"easy-to-hard\" mechanism with or without the CBC measure (**TCL v.s. Vanilla**). Furthermore, **we follow the reviewer's advice and have included comparisons with two additional curriculum learning methods in the revision**. These experiments and results are now incorporated into **the Figure 10 in Appendix E.1**, which further highlights the advantages of our method.\n\n\n> Q2:\n\nWe would like to kindly clarify some points. First, most models for alleviating over-smoothing are based on adjusting the connections between nodes. This adjustment heavily relies on the presence of correctly labeled nodes [1,2,3]. Therefore, these models are not suited in our setting scenarios. Addressing the over-smoothing issue in scenarios with label noise constitutes another research problem and does not impact the main contribution of our work.\n\nBesides, the primary contribution of our work is the development of a robust CBC measure. This measure leverages curriculum learning to effectively address label noise in graph data. Notably, the CBC measure is independent of trained models and relies solely on topological structure, which is unaffected by the 'over-smoothing' phenomenon and provides another way to distinguish the easy and hard nodes. Therefore, changing the GNN backbone in our experiments does not affect the evaluation of our main contribution.\n\n[1] Zhou, K., Huang, X., Li, Y., Zha, D., Chen, R., & Hu, X. (2020). Towards deeper graph neural networks with differentiable group normalization. Advances in neural information processing systems, 33, 4917-4928.\n\n[2] Chen, D., Lin, Y., Li, W., Li, P., Zhou, J., & Sun, X. (2020, April). Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. In Proceedings of the AAAI conference on artificial intelligence (Vol. 34, No. 04, pp. 3438-3445).\n\n[3] Rusch, T. K., Chamberlain, B., Rowbottom, J., Mishra, S., & Bronstein, M. (2022, June). Graph-coupled oscillator networks. In International Conference on Machine Learning (pp. 18888-18909). PMLR.\n\n> Q3: \n\nYes, as the understanding of the reviewer, the features in Fig. 2 are after being processed by GNN. We **have added the highlight of this point in Figure 2 caption in the revised submission**.\n\n\n> Q4: \n\nWe would like to kindly point out that the most of experiment dataset are common-used homophily datasets, each exhibiting varying degrees of homophily. Across all cases, our method consistently outperforms baseline methods by achieving highest average accuracy. \n\nWe guess that if the question of the reviewer is actually about whether our method could be effective on heterphily dataset. To address this concern, we have included three commonly-heterophilous datasets, e.g., DBLP [1], Chameleon [2], Squirrel [2] under 30% instance-dependent label noise. The summary of experiment results in the below table. Our method **still show the superior performance on heterphily datasets**.\n\n|dataset | DBLP   | Chameleon | Squirrel |\n|----|  ----  | ----  | ----  |\n|CP|70.02 | 55.08| 43.42|\n|NRGNN|72.48 |49.02 | 41.35|\n|PI-GNN|71.72 |52.85 |43.31 |\n|Co-teaching+| 66.32| 53.07 | 39.48 |\n|Me-Momentum |59.88  |55.01 | 44.38 |\n|MentorNet | 63.73 |53.73 |39.63 |\n|CLNode  | 72.32|52.85 |35.92 |\n|RCL  | 63.20|52.96 | 40.59 |\n|**TCL**| **74.70**|  **56.17** | **48.03** |\n\n[1] Fu, X., Zhang, J., Meng, Z., & King, I. (2020, April). Magnn: Metapath aggregated graph neural network for heterogeneous graph embedding. In Proceedings of The Web Conference 2020 (pp. 2331-2341).\n\n[2] Rozemberczki, B., Allen, C., & Sarkar, R. (2021). Multi-scale attributed node embedding. Journal of Complex Networks, 9(2), cnab014."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376253765,
                "cdate": 1700376253765,
                "tmdate": 1700376253765,
                "mdate": 1700376253765,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AvhW34reAH",
                "forum": "e2rBzbWwGC",
                "replyto": "LwiqEDZII1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1257/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewers rtA1,\n\nWe have diligently addressed most of your concerns. If you still have concerns about our paper, please join the rolling discussion, where we await your valuable insights and comments.\n\nThanks."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533216399,
                "cdate": 1700533216399,
                "tmdate": 1700533216399,
                "mdate": 1700533216399,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]