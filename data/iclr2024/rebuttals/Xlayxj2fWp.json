[
    {
        "title": "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text"
    },
    {
        "review": {
            "id": "UXB0YR0j2P",
            "forum": "Xlayxj2fWp",
            "replyto": "Xlayxj2fWp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4658/Reviewer_FS9z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4658/Reviewer_FS9z"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of determining whether a block of text comes from LLM or real human beings. Their crucial observation is that if part of the text is fed to an LLM, the output will exhibit certain behaviors that are distinct from human-generated text and simple scoring rules based on KL divergence or total variations is already a sufficiently good classifier."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduced a simple and neat technique to address an important and practical problem in the LLM era. It looks like one of those \u201cone simple trick\u201d result and gives the audience an aha moment."
                },
                "weaknesses": {
                    "value": "I have mixed feelings about this work. While it is in general quite pressing to understand the structure of GPT/LLM, and the authors indeed made some interesting observations, they seem to be quite explicit that they do not care about the execution quality. For example, Delta should be lower bounded but not upper bounded in order to justify using KL divergence/Total variations (smells like a doable fix to provide the other side of inequality; I dont know). In general there is a feeling that the problem is not studied thoroughly.  IMO, another major limitation is that if we dont know which model is used to generate the text. In general, one is most interested in determining whether a block of text is from some LLM, not a specific collection of 5-10 widely available models.\n\n-----\nupdated: \n\nI read the rebuttal that clarifies my misunderstanding. The current logic in the paper flows as \"let's introduce llh gap, then llh gap implies KL large, then let's forget about KL because it is hard to compute/estimate and go back to llh gap\"; some text in rebuttal appears to be more clear to me. \n\nAlso, it would be helpful to discuss more on why KL estimation is harder than llh gap (as both are effectively relating human and machine distributions) like llh requires smaller sample from human. Nevertheless, I think in general the paper is above iclr bar."
                },
                "questions": {
                    "value": "1. is it possible to fix the Delta. \n2. is it possible to build a stronger result, e.g., so long as an LM is trained through certain mechanism, it will always exhibit the likelihood gap hypothesis?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4658/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4658/Reviewer_FS9z",
                        "ICLR.cc/2024/Conference/Submission4658/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4658/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698800918167,
            "cdate": 1698800918167,
            "tmdate": 1700492063864,
            "mdate": 1700492063864,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CNNir52JP1",
                "forum": "Xlayxj2fWp",
                "replyto": "UXB0YR0j2P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4658/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4658/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We would like first to thank the reviewer for the feedback and admit our interesting observations. Below are our responses.\n\n1. \"they seem to be quite explicit that they do not care about the execution quality...\"\n\n**Response**: **We respectfully disagree with this claim**. Execution quality is our top priority: From TabIe 1, our method achieved SOTA results on 4 datasets and 3 models from OpenAI in both black- and white-box scenarios on 16/16 cases regarding AUROC and 13/16 cases regarding TPR.\\\nTable 3 also confirmed the superior detection quality of our methods on 2 advanced open-sourced models (LLaMa, GPT-NeoX), surpassing baselines in 8/8 cases on AUROC.\\\nThus, we are confident that the execution quality is our top priority, and our experimental results demonstrate our method's much higher detection quality over baselines. \n\n2. \"Delta should be lower bounded but not upper bounded in order to justify using KL divergence/Total variations ...\"\n\n**Response**: **We want to clarify that this bound is not intended for a low bound of Delta**. Looking at the other side of this inequality, our purpose is to prove that $d_{KL(M, H)} \\ge \\frac{2\\Delta^2}{|| log (.|X) ||^2_{\\infty}}$, which means that the difference between the two distributions is significant enough since it is greater than some positive gap.\\\nSo this serves as the low bound of $d_{KL(M, H)}$, rather than the upper bound of $\\Delta$. Therefore, even if the $\\Delta$ is small, the positive gap of the distance will always exist as long as the $\\Delta$ is not zero. We will update this in the revision.\n\n3. \"IMO, another major limitation is that if we dont know which model is used to generate the text....\"\n\n**Response**: We would like to clarify that **we are following the same task-setting as in previous work [1, 2]**: the requirements for known LLM are also the problem settings of the two most famous detection approaches: zero-shot detector DetectGPT [1] and Watermark [2].\\\nAdditionally, as we wrote in the original submission on page 3, \u201cRefer to Appendix B.4 for unknown source model\u201d, **we actually included the experiments for detecting text from unknown sources**. To the best of our knowledge, there are no existing good solutions to detect text from unknown sources robustly. Our paper contributes to this challenging problem by using proxy models in Appendix B.4 to make a step forward. To be specific, we use two proxy models (OPT-125M and GPT2-124M) to estimate the relative probability difference and achieve good performance. So **this is also our unique contribution rather than weakness since previous baselines can not handle this**.\\\nFurthermore, on page 9, we have a paragraph titled \u201cModel Sourcing\u201d to tackle the problem of unknown LLM sources.  To be specific, we can detect the model source given some model candidates through pair-wise comparison by applying our method to those candidates and then rank the scores. Table 2 shows that our method can reliably detect model sources from a collection of 4 models with 85%-99% AUROC score. In comparison, none of the baselines can perform model sourcing. Therefore, **this is our additional unique contribution since previous baselines can not solve it.**\n\n[1] Mitchell, Eric, et al. \"Detectgpt: Zero-shot machine-generated text detection using probability curvature.\" (ICML 2023).\\\n[2] Kirchenbauer, John, et al. \"A watermark for large language models.\" (ICML 2023).\n\nQuestions 1&2: \n\n**Response:** We would like to first thank the reviewer for raising this interesting question.\\\nWe believe our results are already very strong compared with baselines since our experiments validate the effectiveness of our method on both closed models like text-davinci-003, GPT-3.5-turbo, and GPT-4, as well as open-source models such as GPT-NeoX-20B and LLaMa-13B. Thus, it is evident that it holds for those representative models.\\\nHowever, in terms of \u201cso long as an LM is trained through certain mechanism, it will always exhibit the likelihood gap hypothesis and the possibilities of fixing the Delta.\u201d, we believe that it would be a complicated question because **it involves not only the training mechanism but also factors that might be relevant to the training data, model size, model architecture, and alignment, among others**. We would love to extend our results to more models in the future.\\\nConsidering the popularity of those models, especially ChatGPT, we believe that our approach provides a timely solution to this problem. \n\nBesides, we also noticed that **on the OpenAI dev day, the former CEO of OpenAI announced [3] that they would give token logits access on GPT-4 API calls in the future. This will make our white-box detection method also applicable to GPT-4! Although we have not got access to GPT-4 output token logits, we believe this will only make our method more powerful in the future.**\n\n[3] https://openai.com/blog/new-models-and-developer-products-announced-at-devday"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700357953743,
                "cdate": 1700357953743,
                "tmdate": 1700357953743,
                "mdate": 1700357953743,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U39V5k0Pum",
                "forum": "Xlayxj2fWp",
                "replyto": "CNNir52JP1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4658/Reviewer_FS9z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4658/Reviewer_FS9z"
                ],
                "content": {
                    "title": {
                        "value": "Point 2."
                    },
                    "comment": {
                        "value": "Is this the right direction? I am still not completely sure. \n\nDid I understand that correctly that your kl divergence/tv is a tester, i.e., if your kl divergence is large enough, then you output YES (it is from human). \n \nIf I have a blackbox that outputs infinte all the time (say I have a function that's d_{always infinite} that replaces d_{KL}), all the arguments/logics still work (of course infinite > Delta > 0?), which implies I can use d_{always infinite} to do detection, and my detection will always output YES?\n\nI still feel you need the other direction, i.e., there is a gap Delta, which you dont know how to estimate. But you know KL divergence is smaller than the gap, so if KL divergence is large, then gap must also be large so you are confident that this is from human."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364017069,
                "cdate": 1700364017069,
                "tmdate": 1700364017069,
                "mdate": 1700364017069,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TvURctvkG9",
                "forum": "Xlayxj2fWp",
                "replyto": "UXB0YR0j2P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4658/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4658/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional response to Point 2"
                    },
                    "comment": {
                        "value": "**Response**: Thanks for the quick feedback and suggestion!\\\nNO. The kl divergence is not a tester. This kl divergence is intractable for each input text since $\\Delta$ can not be exactly computed, as you also mentioned. Instead, we use it as a theoretical guide to help us empirically detect text.\\\nAs we mentioned, the $\\Delta$ is relevant to different training mechanisms, data, model size, model architecture, alignment, etc. Thus, inherited from the DetectGPT hypothesis, we assume such a gap is large enough for us to distinguish between human and AI-written text. But in practice, we have to turn to alternative calculations, such as our proposed Bscore and Wscore, showing impressive performance over previous baselines, as reported in Table 1. Based on our experimental observation, like Figure 2, and the outperforming performance in Table 1, we believe our hypothesis is reasonable since it can explain our observed empirical results. For example, the detection effectiveness consistently improves as we increase the number of re-generations $N$ in Figure 4, validating our theoretical result in A.2 PRINCIPLED CHOICE OF K. On the other hand, if we can directly compute the kl divergence, we can perfectly detect any text without needing any detection algorithm. But it is impossible. Thus, we believe our method is solid and effective both empirically and theoretically compared with previous baselines. This is also admitted by reviewer 4FGJ: \"The proposed method is simple and effective\", reviewer UDX7: \"The results are strong\" and your kind comment \"a simple and neat technique\".\\\nSo, we believe our theoretical and empirical results echo harmoniously, providing a timely solution to the urgent issue of text detection. This is particularly suitable for our ICLR submission track of societal considerations including fairness, safety, privacy."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700422966612,
                "cdate": 1700422966612,
                "tmdate": 1700423308196,
                "mdate": 1700423308196,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CwJJQDa3LV",
                "forum": "Xlayxj2fWp",
                "replyto": "UXB0YR0j2P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4658/Reviewer_FS9z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4658/Reviewer_FS9z"
                ],
                "content": {
                    "title": {
                        "value": "some followup questions"
                    },
                    "comment": {
                        "value": "I see. I still have some conceptual/intuitive questions and technical questions related to KL and TV discussions. \n\nLet me try to understand the implication of each of your statements: \n\n1. Bottom of page 3: your log likelihood hypothesis claims that the entropy of machine generated text is smaller than human generated text, which I think is intuitive and captures the observation that machine generated texts are more deterministic. \n2. Page 4: then you move to prove that both KL and TV should be sufficiently large. So I start to get confused here: (2a)You proved TV and KL of two distributions are large. Here is my confusion: so long as two distributions are sufficiently different, then KL and TV should always be large. Does that mean the assumption you need is not loglikelihood gap hypothesis, but a simpler assumption that distributions of machine generated text and that of human generated text are sufficiently different. (2b) A technical question: your second inequality (moves from difference between two expectations to TV): how is p(\\cdot |X) defined? I feel the p(\\cdot |X) in two terms are two different functions (representing the conditional pdf for M and H respectively) so you cannot pull out the \u201cshared term\u201d like you did. \n\nTechnicality (2b) is always fixable but my central question is what math story you are trying to highlight from these a few lines. I also notice that you pull out KL from TV, which means you really only need two distributions to be different. Llh gap is a sufficient condition but not something you leveraged in your test?\n\nAlso, how are BScores and WScores related to KL if you claim KL are intractable? I thought those scores are some best effort to approximate KL but then I feel less sure either now. \n\nMy concern is not primarily in the efficacy side but on the explanation side, which I feel is the weaker part of the submission."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700427534660,
                "cdate": 1700427534660,
                "tmdate": 1700427656451,
                "mdate": 1700427656451,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kJNLo4lf5w",
                "forum": "Xlayxj2fWp",
                "replyto": "CwJJQDa3LV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4658/Reviewer_FS9z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4658/Reviewer_FS9z"
                ],
                "content": {
                    "title": {
                        "value": "An additional followup"
                    },
                    "comment": {
                        "value": "Or perhaps I can paraphrase the questions: \n\nYour logic seems to be following this chain: \n\nIf this is from human, there is a positive llh gap, then that implies KL divergence is large, then I can design some scoring heuristics that outputs YES when KL divergence is large. \n\n(or correct me if my understanding is still inaccurate). \nMy questions can be summarized into two parts: \n1. Large KL divergence does not imply a positive llh gap unless you have the other side of inequality, so it does not imply it is from human either (theoretically).   \n2. Large KL does not need to rely on positive llh gap; it only requires that two distributions are sufficiently different.  So llh gap does not feel like a crucial ingredient to build the theory."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700428871250,
                "cdate": 1700428871250,
                "tmdate": 1700428871250,
                "mdate": 1700428871250,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3V2J6x978i",
                "forum": "Xlayxj2fWp",
                "replyto": "UXB0YR0j2P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4658/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4658/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional respoonse to \"some followup questions and An additional followup\""
                    },
                    "comment": {
                        "value": "**Response**: We would like to thank the reviewer for the additional questions, which are very good suggestions to help improve our work. \n\nFirst of all, we would like to **clarify a major misunderstanding that we are not designing scoring heuristics to estimate KL divergence that outputs YES when KL divergence is large.** In our previous response, we also clarified that the kl divergence is not a tester.\\\nPlease see below for more explanations.\n\n1. Yes, we agree with your valuable observation that a large KL divergence does not imply a positive llh gap unless you have the other side of inequality.\\\nOur WScore or BScore is not designed to calculate KL divergence since the human distribution is unknown. Instead, it is used to estimate the $\\Delta$. However, since human distribution is intractable in the llh gap, WSocre or BScore is still a rough estimate for $\\Delta$. Empirical results demonstrate it is very effective, though theoretically, it is not a 100% exact estimate.\n\n2. Yes, we agree with your valuable observation that large KL only requires that two distributions are sufficiently different. Our intent is to show that large $\\Delta$ helps lead to large KL since KL is not directly calculable.\n\nWe re-summarize our intents: First, we assume the human and machine distribution is sufficiently different because otherwise, we can not detect it if they are the same. However, this KL is intractable. Instead, we turn to alternative estimates: large $\\Delta$ leads to large KL. Then, our core contribution is the design of the BScore and WScore to estimate $\\Delta$ roughly. Empirical results show that those two scores are simple and effective for real-world detection.  \n\nWe hope our explanation helps explain your concerns.\n\nThanks,\\\nAuthors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471520721,
                "cdate": 1700471520721,
                "tmdate": 1700471718350,
                "mdate": 1700471718350,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nE1Td6RBNF",
                "forum": "Xlayxj2fWp",
                "replyto": "3V2J6x978i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4658/Reviewer_FS9z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4658/Reviewer_FS9z"
                ],
                "content": {
                    "title": {
                        "value": "continue discuss"
                    },
                    "comment": {
                        "value": "I see. So there are actually two threads: \n\n1. Positive llh gap implies large kl, large tv, so they are statistically distinguishable.\n2. Llh gap is q log p - p log p (q human distribution p from machine), where as kl is p log p - p log q, or q log q - q log p. You have blackbox/whitebox access to LLM so you believe either estimating q log p is easier than p log q, or q log q is easier than p log p. Your scores are designed to estimate/approximate/capture llh gap, not kl."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490698713,
                "cdate": 1700490698713,
                "tmdate": 1700490698713,
                "mdate": 1700490698713,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pZTaUAH0fW",
            "forum": "Xlayxj2fWp",
            "replyto": "Xlayxj2fWp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4658/Reviewer_UDX7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4658/Reviewer_UDX7"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a training-free detection strategy for identifying machine-generated text by an LLM from human-generated text. The method involves truncating the text and feeding the second half to an LLM and analysing the parts through an N-gram analysis. The paper addresses both black-box (probability distribution of LLMs is not known) as well as white-box models (probability distribution is known). The paper shows that the proposed method surpasses SOA performance in distinguishing between machine and human-generated text by evaluating on a number of proprietary (e.g. GPT3+) and open-source(LLAMA, GPT-NeoX) models, on both English and German datasets. The performance is based on the finding that the distribution of machine-generated text and human-generated text are different when given a preceding text. The proposed method also provides explanation and evidence for the prediction, can handle model sourcing and revised text attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- **Significance**: The paper tackles an interesting, relevant problem\n- **Originality**: The proposed mathematical framework used is original\n- **Results**: The results are strong, showing performance improvement over similar detection methods, like GPTZero and OpenAI classifier\n- **Presentation** : The paper has good presentation, and claims follow clearly from experiments"
                },
                "weaknesses": {
                    "value": "- The entire framework is based on the likelihood-gap hypothesis. What is the basis for this assumption? Is this formally acknowledged somewhere else in the literature or verified empirically in this paper (e.g. from Fig 2)? \n- The paper mentions that is able to provide explanations and evidence in the form of n-grams overlaps. Would be good to provide some examples."
                },
                "questions": {
                    "value": "- Asked above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4658/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4658/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4658/Reviewer_UDX7"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4658/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811596173,
            "cdate": 1698811596173,
            "tmdate": 1699636446266,
            "mdate": 1699636446266,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0Sfnt9SUvN",
                "forum": "Xlayxj2fWp",
                "replyto": "pZTaUAH0fW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4658/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4658/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for the positive feedback on our work. Below are our responses.\n\n1. The entire framework is based on the likelihood-gap hypothesis. What is the basis for this assumption? Is this formally acknowledged somewhere else in the literature or verified empirically in this paper (e.g. from Fig 2)?\n\n**Response**: We have both theoretical and experimental bases for this hypothesis.\\\n**Theoretically**: In the baseline work of DetectGPT [1] published on ICML 2023, they also have a likelihood-gap hypothesis (on page 3, section 4): the perturbation discrepancy distance of the machine is positive and large, while it is close to 0 for humans, which means that there is always a positive gap between the human and machine. Therefore, our hypothesis is inherited from theirs.\n\n**Experimentally**: It is also empirically verified by Fig 2, where we can see a clear gap between those two distributions of human and machine. Additionally, our comprehensive experiments in Tab 1 further confirm it by showing that our method achieves SOTA results on 4 datasets and 3 models from OpenAI in both black- and white-box scenarios on 16/16 cases regarding AUROC and 13/16 cases regarding TPR. \n\n[1] Mitchell, Eric, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. \"Detectgpt: Zero-shot machine-generated text detection using probability curvature.\" (ICML 2023).\n\n2. The paper mentions that is able to provide explanations and evidence in the form of n-grams overlaps. Would be good to provide some examples.\n\n**Response**: **We indeed provided examples in our original submission** since this is the unique advantage of our detector over other baselines. On page 8, we had a whole paragraph titled \u2018Explainability\u2019 to talk about the explanations and evidence. In Appendix D, we also had the additional tables 14, 15, and 16 for explanations and evidence.\\\nSpecifically, one example in Figure 1 shows that the typical overlapped n-grams are: \nThe original $y_0$ contains the following sentence: \u201cle analyses that focus on specific neighborhoods or regions may reveal disparities that are not apparent in larger-scale analyses. Therefore, it is important to consider the scale of analysis when\u201d. Among 20 re-generations, we found the following two typical examples with significant overlapped n-grams:\\\nIn $y_1$, the overlapped n-gram is \u201cle analyses that focus on specific neighborhoods or regions may reveal disparities that are not apparent in larger-scale analyses\u201c\\\nIn $y_5$, the overlapped n-gram is \u201cmay reveal disparities that are not apparent in larger-scale analyses. Therefore, it is important to consider the scale of analysis when\u201d\\\nWe hope this example of overlapped n-grams helps. More examples can be found in Appendix D.\n\nAdditionally, we also noticed that on **the OpenAI dev day, the former CEO of OpenAI announced [2] that they would give token logits access on GPT-4 API calls in the future. This will make our white-box detection method also applicable to GPT-4! Although we have not got access to GPT-4 output token logits, we believe this will only make our method more powerful in the future.**\n\n[2] https://openai.com/blog/new-models-and-developer-products-announced-at-devday"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700356402039,
                "cdate": 1700356402039,
                "tmdate": 1700357977917,
                "mdate": 1700357977917,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wSzRY3RQUm",
            "forum": "Xlayxj2fWp",
            "replyto": "Xlayxj2fWp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4658/Reviewer_4FGJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4658/Reviewer_4FGJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of detecting whether a piece of text is generated by human or AI models. The authors proposed a simple method to detect the origin of the text, together with two score functions, for both black-box and white-box detection. The authors conducted extensive experiments, and show their approach outperform the state-of-the-art detecting methods.\n\nAfter rebuttal: I have read the rebuttal and would like to keep my scores."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper is generally well-written and easy-to-follow. The method proposed by the authors is conducted in a training-free fashion, which remove the need of training detection classifier on millions of texts. The proposed method is simple and effective: based on the authors evaluation, it outperforms many existing detecting methods."
                },
                "weaknesses": {
                    "value": "While the proposed method is effective, it not clear how certain terms in the score function is selected---e.g., the f(n) function in BScore. Can authors comment more on that? \n\nAlso, can authors explain a bit more on how P(Y_0 | X) is calculated when Y_0 is human-written text? Does text-davinci-003 allow such calculation?"
                },
                "questions": {
                    "value": "Please see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4658/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4658/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4658/Reviewer_4FGJ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4658/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699051530185,
            "cdate": 1699051530185,
            "tmdate": 1700776590000,
            "mdate": 1700776590000,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VVD6XqxxPl",
                "forum": "Xlayxj2fWp",
                "replyto": "wSzRY3RQUm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4658/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4658/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for the positive feedback on our work. Below are our responses.\n\n1. While the proposed method is effective, it not clear how certain terms in the score function is selected---e.g., the f(n) function in BScore. Can authors comment more on that?\n\n**Response**: For BScore, we aim to make it simple and effective. The basic idea is to assign higher weights to overlapped N-grams with larger N since it would be more unusual for the machine to generate overlapped N-grams with larger N. As we wrote, \u201cIn practice, we set f(n)=n log(n), \u2026 More comparisons on parameter sensitivity can be found in Appendix B.\u201d In Appendix B.8, we demonstrate the effects of 6 different weight functions on additional validation sets and found that f(n)=nlog(n) works well across 2 models and 3 datasets. For other terms like the starting or ending N-grams, we follow the same design principle to ensure it is simple and effective.\nThus, we keep the current choices since they already work well across comprehensive experiments, serving as a strong baseline for future work.\n\n2. Also, can authors explain a bit more on how P(Y_0 | X) is calculated when Y_0 is human-written text? Does text-davinci-003 allow such calculation?\n\n**Response**: Yes, text-davinci-003 allows such calculations. When Y_0 is human-written text, we could get the logits over Y_0 given X by changing the settings in the OpenAI API: set the max_tokens = 0, n= 1, logprobs=5, echo=True. According to the official documentation [1], \n\nmax_tokens = 0: The maximum number of tokens to generate in the completion.\n\nn= 1: How many completions to generate for each prompt.\n\nlogprobs=5: Include the log probabilities on the logprobs of the 5 most likely tokens, as well the chosen tokens.\n\necho=True: Echo back the prompt in addition to the completion \n\nThus, by the above settings, we were able to get all the token probabilities over Y_0 and X on text-davinci-003.\n\nWe also noticed that **on the OpenAI dev day, the former CEO of OpenAI announced [2] that they would give token logits access on GPT-4 API calls in the future. This will make our white-box detection method also applicable to GPT-4! Although we have not got access to GPT-4 output token logits, we believe this will only make our method more powerful in the future**\n\n[1]  https://platform.openai.com/docs/api-reference\n\n[2] https://openai.com/blog/new-models-and-developer-products-announced-at-devday"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700355434852,
                "cdate": 1700355434852,
                "tmdate": 1700355480153,
                "mdate": 1700355480153,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]