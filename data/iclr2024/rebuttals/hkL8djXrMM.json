[
    {
        "title": "Neural Diffusion Models"
    },
    {
        "review": {
            "id": "e6XtHVKbkM",
            "forum": "hkL8djXrMM",
            "replyto": "hkL8djXrMM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7849/Reviewer_vgew"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7849/Reviewer_vgew"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Neural Diffusion Models (NDMs) as an extension of traditional diffusion models. While conventional diffusion models are limited to linear data transformations, NDMs allow for time-dependent non-linear transformations, potentially improving generative distribution training efficiency. The authors propose a variational bound for optimizing NDMs in a simulation-free setting and develop a time-continuous formulation for efficient inference. Experimental results on image generation benchmarks, such as CIFAR-10, downsized ImageNet, and CelebA-HQ, demonstrate that NDM is able to do generative tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces NDM as a framework that extends conventional diffusion models to both discrete and continuous time settings. The upper bound of the negative log-likelihood objective is provided. The authors claim that the generation quality is improved with small to medium steps in terms of log-likelihood. The approach is interesting, even though the idea of using a nonlinear forward process is not novel."
                },
                "weaknesses": {
                    "value": "1. There are a few other learnable forward process which generalizes the diffusion model [1,2]. The authors should consider citing or comparing with them.\n\n2. Actually I am not quite convincing by the effectiveness of proposed method. The numerical value is compared with the DDPM. The authors clearly present the differences with DDIM in Figure 1, but the experiment section is just focus on DDPM for fast sampling. The numerical value provided by authors is not impressive or competitive compared with the other fast sampling techniques on the market which is built on DDPM/DDIM [3,4].\n\n[1]:Dongjun Kim et al. 'Maximum Likelihood Training of Implicit Nonlinear Diffusion Models.'\n\n[2]: Tianrong Chan et al. 'Likelihood Training of Schr\u00f6dinger Bridge using Forward-Backward SDEs Theory.'\n\n[3]: Qinsheng Zhang et al. 'Fast Sampling of Diffusion Models with Exponential Integrator'\n\n[4]: Fan Bao et al. 'Analytic-DPM'"
                },
                "questions": {
                    "value": "1. What is the difference between the NDM compared with [1,2]? What is the benefits over them?\n2. In the experiment section, 'DDPM' represents for the SDE/stochastic model? If it is, it would be great to have the comparison for ODE model which is more favorable for fast sampling for both NDM and DDIM.\n3. How do the authors obtain the $\\hat{x}_{\\theta}$ in eq.10? Does it include the inference of the network? If yes, then why the algorithm is simulation-free? From my understanding, it is implicitly simulating the dynamics which is also the part of reason for heavy training complexity as stated by the authors.\n\n[1]:Dongjun Kim et al. 'Maximum Likelihood Training of Implicit Nonlinear Diffusion Models.'\n\n[2]: Tianrong Chan et al. 'Likelihood Training of Schr\u00f6dinger Bridge using Forward-Backward SDEs Theory.'"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7849/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697815567955,
            "cdate": 1697815567955,
            "tmdate": 1699636961933,
            "mdate": 1699636961933,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GdG20PdOhN",
                "forum": "hkL8djXrMM",
                "replyto": "e6XtHVKbkM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7849/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7849/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their review and suggestions which will help improve the paper. We will include the suggested reference in the revision. Below we address the questions and comments raised in the review.\n\n**Weaknesses and Questions:**\n\n1. We present NDM not merely as a model learning the forward process but as a comprehensive framework that enables the predefined specification of forward dynamics capable of information destruction. Additionally, it facilitates the learning of these dynamics. The concept of a learnable forward process is not novel. However, unlike others, NDM allows time-dependent, non-linear, and learnable data transformations, all while remaining a simulation-free framework, which is an essential factor for the efficiency of the training procedure.\n    \n    For instance, in [1], the authors suggest training a conventional diffusion model in the latent space of a flow model. This model can be viewed as a special case of NDM, where the transformation $F^\\varphi$ corresponds to a time-independent flow.\n    \n    The work [2], like many other Schr\u00f6dinger Bridge models, does not fall into the special case of NDM. However, in contrast to NDM, the approach [2] is not simulation-free. In Schr\u00f6dinger Bridge models, we typically lack access to the distribution $q(\\mathbf{z}^t|\\mathbf{x})$. Consequently, to sample the latent variable $\\mathbf{z}^t$ in training time, we must simulate the full stochastic process, such as the SDE in [2]. This characteristic makes Schr\u00f6dinger Bridge models expensive in training and not simulation-free. In contrast, NDM, by design, has access to $q(\\mathbf{z}^t|\\mathbf{x})$. Thus, with NDM, we can directly sample $\\mathbf{z}^t$ without expending computational resources on simulation, enhancing the efficiency of the training procedure.\n    \n2. While NDM can be thought of as a generalisation of DDIMs, the DDIM-style deterministic sampling effectively implies sampling from a process different from the one with which the model was trained. Thus, for a comparison, we opted to sample from the same processes that the model was trained on, which coincides with the DDPM reverse process. However, since NDMs allow deterministic sampling we acknowledge the interest of this alternative comparison and plan to incorporate it into the camera-ready version of our paper to compare FID scores. The switch from DDPM to DDIM doesn\u2019t affect evaluation of likelihood, since it relies on integration of deterministic process in both cases.\n    \n    Nevertheless, in addition to the comparison with DDPM presented in Table 4, we also provide a comparison of NDM with other newer models in Tables 2 and 3.\n    \n    Simultaneously, there exist studies proposing techniques to enhance the performance of diffusion models or expedite the sampling procedure [3, 4]. However, most of these approaches are orthogonal to NDMs and can be easily adapted for further gains.\n    \n3. We present the derivation of equation 10 in Appendix A.2. The computation of this equation requires the inference of the transformation $F^\\varphi$. Importantly, this does not disrupt the simulation-free paradigm. As described earlier, a non-simulation-free term implies the necessity to simulate certain dynamics during the training procedure. In NDM, we can sample $\\mathbf{z}^t$ with just one inference of $F^\\varphi$, introducing increased complexity only as a constant without altering the order of complexity.\n    \n    In our experiments, the introduction of a learnable $F^\\varphi$ extends the training time by approximately 2.3 times. However, in a non-simulation-free approach with $T$ timestamps, employing a learnable forward process would result in a $T$-fold slowdown.\n    \n\n[1]:Dongjun Kim et al. 'Maximum Likelihood Training of Implicit Nonlinear Diffusion Models.'\n\n[2]: Tianrong Chan et al. 'Likelihood Training of Schr\u00f6dinger Bridge using Forward-Backward SDEs Theory.'\n\n[3]: Qinsheng Zhang et al. 'Fast Sampling of Diffusion Models with Exponential Integrator'\n\n[4]: Fan Bao et al. 'Analytic-DPM'"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7849/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469456914,
                "cdate": 1700469456914,
                "tmdate": 1700469456914,
                "mdate": 1700469456914,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VbR9dSKW6H",
                "forum": "hkL8djXrMM",
                "replyto": "GdG20PdOhN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7849/Reviewer_vgew"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7849/Reviewer_vgew"
                ],
                "content": {
                    "title": {
                        "value": "Questions"
                    },
                    "comment": {
                        "value": "Thanks for the responses.\n\n1. I think the performance comparison with DDIM is absolutely necessary in the revision, and I do not see any difficulties in incorporating it in the current revision. I do not understand why the authors hope not to add them until camera-ready version. The checkpoint and code base are public. You can also grab the numerical value from DDIM paper directly.\n2. I noticed for Cifar10, the FID for DDPM (11.44) with 1000 steps lags significantly compared with the existing literature (~3.x). Could you explain the reasons? \n3. For [1], one needs to propagate the auxiliary network N times in order to get the training source data with size [batch, N, dimension]. I think in your framework, you also need to propagate $F^{\\phi}$ N times in order to train the same amount of source data? Though I agree that it is 'simulation-free', it seems not to reduce any training complexity?\n\n[1] Tianrong Chan et al. 'likelihood Schrodinger Bridge'"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7849/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493871048,
                "cdate": 1700493871048,
                "tmdate": 1700493871048,
                "mdate": 1700493871048,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BEpRShivyJ",
                "forum": "hkL8djXrMM",
                "replyto": "e6XtHVKbkM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7849/Reviewer_vgew"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7849/Reviewer_vgew"
                ],
                "content": {
                    "title": {
                        "value": "Replies"
                    },
                    "comment": {
                        "value": "1. I totally agree with authors' argument that the full ELBO will lead to lower FID and better NLL, however I still insist that the numerical value FID lags too much. A kind suggestion is that, the authors can demonstrate some experimental setup in which the NLL is more important than other quantitative metrics. The image generation tasks rely quite much on FID because the generated quality counts more. The generated quality is not satisfying even though the higher NLL you achieved (Fig.2).\n\n2. At the end of the day, the time dimension will be considered as part of batch size for both NDM and SB. The only difference is that NDM chops the time dimension (just randomly sample one timestep which results in [batch, 1,dimension]), and SB chops the batch dimension into small pieces (which results in [small-batch, T, dimension]). I still want to argue that the proposed method is not really 'simulation-free'.\n\n*************EDIT*******************\nMy apology for the incorrect term. What I would like to argue is that the complexity will still be high. The proposed method is indeed simulation-free but with comparable complexity like simulation-based approaches (SB)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7849/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545635923,
                "cdate": 1700545635923,
                "tmdate": 1700569368650,
                "mdate": 1700569368650,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Jr7ThrN8Tb",
            "forum": "hkL8djXrMM",
            "replyto": "hkL8djXrMM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7849/Reviewer_Y8jp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7849/Reviewer_Y8jp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Neural Diffusion Models, which parameterize the forward diffusion process with a neural network. By training the neural network with the x-prediction network together, the authors yield new generative models that are better than naive diffusion models in terms of likelihood estimation and few-step generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of using parameterized marginal distribution makes a lot of sense, since we have no idea which configuration of the marginal distributions is the best and most of them are handcrafted.\n\n2.The empirical performance successfully demonstrates the effectiveness of the proposed method in likelihood estimation."
                },
                "weaknesses": {
                    "value": "1. The presentation of the paper should be polished. The learning and sampling algorithm is difficult to find. I suggest the authors shorten/defer the discussion section to Appendix and add algorithm boxes in the main text. Moreover, I have several questions on the training process: (1) Are $\\phi$ and $\\theta$ jointly trained or alternatively trained? (2) How are the hyper-parameters of $F_\\phi$ set? Are they similar to the x-prediction network?\n\n2. Certain constraints of $F_\\phi(x_t, t)$ should be satisfied. For example, when $t=0$, I think it must satisfy $F_\\phi(x_0, 0) = x_0$. I wonder how the authors ensure that.\n\n3. The proposed method brings additional computation overhead in the inference time when simulating Eq. (12) and Eq. (13). Because $F_\\phi$ is a neural network and the authors use the same U-Net for both $F_\\phi$ and $\\hat{x}$ , it at least doubles the computation. Visually, Figure 2 shows that $F_\\phi$ actually gives very close prediction to the real data $x$. I guess maybe simply double the size of the original $\\hat{x}_\\theta$ can get the similar results as NDM.\n\n4. Ablation studies are missing. At least the influence of (1) the various choices of $\\alpha_t$ and $\\sigma_t$, and (2) the various choices of the network structure and number of parameters of $F_\\phi$  should be investigated to verify that NDM brings consistent improvement. \n\n5. Relationship to learnable interpolation in [1] should be discussed in more detail, although I notice there are several sentences in the related works. The two methods are actually very close to each other (I think the only difference is the objective).\n\nOverall, I think using learnable marginal distribution for improve likelihood estimation is reasonable, and there is empirical improvement. However, the poor presentation quality and unstatisfying empirical evaluation makes the paper borderline.\n\n[1] Building normalizing flows with stochastic interpolants. https://arxiv.org/abs/2209.15571"
                },
                "questions": {
                    "value": "Please refer to Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7849/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698640405921,
            "cdate": 1698640405921,
            "tmdate": 1699636961826,
            "mdate": 1699636961826,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "npSQvrA3oW",
                "forum": "hkL8djXrMM",
                "replyto": "Jr7ThrN8Tb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7849/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7849/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the suggestions and review which will help to improve the paper. We are glad that the reviewer finds our idea to make sense and empirical results to be illustrative. Below we address the questions and comments raised in the review.\n\n**Weaknesses:**\n\n1. We agree that adding algorithm boxes can help improve readability. We will include these boxes in the camera-ready version if accepted.\n    \n    (1) We optimize the parameters of the forward process $\\varphi$ and parameters of the reverse process $\\theta$ jointly by optimizing the full ELBO objective (eq. 9).\n    \n    (2) As we discuss in Section 4.1, in all our experiments involving learnable transformations, we employ the same neural network architecture and set of hyperparameters for both the transformations $F^\\varphi$ and prediction of datapoints $\\hat{x}^\\theta$.\n    \n2. Upon training NDM with the complete ELBO objective (equation 9), additional restrictions are not necessary. However, as outlined in Appendix C.2, in practice, we ensure that $F^\\varphi(\\mathbf{x}, t) = \\mathbf{x}$ for $t = 0$. This is achieved through the following reparameterization: $F^\\varphi(\\mathbf{x}, t) = (1 - t) \\mathbf{x} + t \\bar{F}^\\varphi(\\mathbf{x}, t)$. This ensures that the reconstruction term $\\mathcal{L}^\\mathrm{rec}$ doesn\u2019t depend on parameters $\\varphi$.\n3. To address the question of whether the improved performance of NDM is due to the proposed method or merely the result of increasing the number of model parameters we provide an ablation study in Appendix D.3, which demonstrates that the improved quality is not a consequence of an increased number of parameters.\n4. We introduce NDMs as a framework that generalises existing approaches and enables further extensions through learnable transformations. The main purpose of our experiments is not to obtain state-of-the-art performance nor uniformly better performance compared to baselines. Instead, our aim is to establish the functionality of NDMs in a broader sense, highlighting their additional flexibility and studying learnt transformations.\n    \n    We believe that NDM with learnable transformations may exhibit enhanced performance with different neural network architectures, improved parameterization, and after hyperparameter tuning. However, we defer this aspect to future research. In this work, we wanted to keep experimental setup simple to showcase the inherent properties of NDM, prioritizing this over achieving superior results through engineering efforts.\n    \n    Nevertheless, as mentioned above, we conducted an ablation study that substantiates the enhanced performance observed in experiments, attributing it to the model itself."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7849/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469357421,
                "cdate": 1700469357421,
                "tmdate": 1700469357421,
                "mdate": 1700469357421,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aBsyEDRGTq",
                "forum": "hkL8djXrMM",
                "replyto": "Z3w7cQ6rYH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7849/Reviewer_Y8jp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7849/Reviewer_Y8jp"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response!"
                    },
                    "comment": {
                        "value": "I appreciate the additional explanations and clarifications provided by the authors. They correct some of my understandings and  point out experimental evidence in the Appendix.\n\nMy final decision will depend on the overall responses and other questions raised by other reviewers. Thank you once again for the response!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7849/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540110773,
                "cdate": 1700540110773,
                "tmdate": 1700540110773,
                "mdate": 1700540110773,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ixhrOBDEIB",
            "forum": "hkL8djXrMM",
            "replyto": "hkL8djXrMM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7849/Reviewer_LvPx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7849/Reviewer_LvPx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes neural diffusion models, or NDMs, extending conventional diffusion models to using learnable non-linear transformation. An objective function is developed within this framework to optimize NDMs, providing an upper bound for the negative log-likelihood.  Empirical studies showcase NDMs' ability to consistently enhance log-likelihood while improving generation quality for scenarios involving a small to medium number of steps."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The background and techniques of NDMs are clearly described. \n- The visualization of the transformed data is insightful and pretty helpful in understanding the benefits of NDMs."
                },
                "weaknesses": {
                    "value": "- It seems that the technical details are similar to the conventional DMs. So, what are the technical challenges and novelties here?\n\n- In my opinion, the argument \"a key limitation of most existing diffusion models is that they rely on a fixed and pre-specified forward process that is unable to adapt to the specific task or data at hand\" is not convincing enough. The extensive empirical studies in the community reflect that conventional DMs have enough flexibility to accommodate diverse data. So, more clearly, I want the authors to clarify what practical consequences would the conventional \"inflexible\" modeling lead to. On the other hand, conventional modeling bears great simplicity, where the denoising process is training-free. Compared to that, NDMs may rely on more complicated training. \n\n- As mentioned in the paper, the prior term and the reconstruction term should also be trained. What are the weights of these objectives in the entire training objective?\n\n- More clarifications should be put on the training cost and stability of NDMs, compared to vanilla DMs. \n\n- Is there an experiment on a larger dataset using a larger model? Are there technical challenges to achieving this?"
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7849/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698656994705,
            "cdate": 1698656994705,
            "tmdate": 1699636961711,
            "mdate": 1699636961711,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HuMjnQ8ExT",
                "forum": "hkL8djXrMM",
                "replyto": "ixhrOBDEIB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7849/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7849/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are glad that the reviewer founds the description of NDM to be clear and the visualizations to be helpful. Below we address the questions and comments raised in the review.\n\n**Weaknesses:**\n\n- The main novelty of our work is NDM as a simulation-free framework that generalises conventional diffusion models and allows non-linear, time-dependent and learnable transformations of the data distribution in both discrete and continuous time.\n    \n    While NDM is a generalisation of conventional diffusion models, it shares many technical similarities with them. However, implementing NDM involves addressing several technical challenges. \n    \n    Firstly, in order to consistently define the forward process, we needed to shift to a non-Markovian definition of the model. This required deriving consistent distributions $q(\\mathbf{z}_t|\\mathbf{x})$ and $q(\\mathbf{z}_s|, \\mathbf{z}_t, \\mathbf{x})$, as well as parameterizing the reverse process and deriving objective.\n    \n    Secondly, to extend NDM to continuous time, we derived stochastic and ordinary differential equations (SDE and ODE) along with the corresponding objective. Lastly, for an efficient practical implementation of the continuous-time model, we devised a method to calculate time derivatives $\\frac{\\partial F_\\varphi}{\\partial  t}$ using Jacobian Vector Product (JVP). Additionally, we applied importance sampling to ensure the stability of the training procedure.\n    \n- Conventional diffusion models indeed exhibit remarkable generalization ability and performance across various domains. However, the forward process in these models implicitly predefines the distribution of latent variables and consequently generative trajectories. The fixed forward process thus determines the complexity of the target for the reverse process and the complexity of the sampling procedure. Consequently, the flexibility of the forward process has the potential to simplify the target for the reverse process, resulting in enhanced performance and simplified trajectories that boost sampling.\n    \n    Recent research in diffusion models indicates that techniques such as diffusion in latent or orthogonal space, a combination of diffusion and blurring, flow matching, and rectified flows contribute to improved performance. These techniques bring modifications to the conventional diffusion process, enhancing sample quality and sampling speed. The NDM framework presented in our work represents the next step in the quest for finding better methods to construct generative dynamics.\n    \n- We don\u2019t utilize any balancing coefficients. We always train models with the pure ELBO (eq. 9). However, thanks to parameterization of the transformation that guarantees $F_\\varphi(\\mathbf{x}, t) = \\mathbf{x}$ for $t = 0$ we don\u2019t need to optimize the reconstruction term (see Appendix C.2, (eq. 37)).\n- In all our experiments, we employ the same neural network parameterization for both transformations, $F_\\varphi$ and prediction of datapoint $\\hat{x}_\\theta$. Therefore, with this parameterization, NDM has twice as many parameters compared to vanilla diffusion models and approximately 2.3 times longer training duration (refer to Section 6). However, in Appendix D.3, we present an ablation study demonstrating that performance improvements are not achieved by increasing the number of parameters.\n    \n    The only additional technique we employed to ensure training stability is importance sampling of time for continuous-time models, as described in Sections 3.2 and 6. No further techniques where necessary to ensure stable training of NDMs. We will clarify it in camera ready.\n    \n- In our paper, we present experimental results on the ImageNet 64 and CelebA-HQ-256 datasets, which involve significantly more computational resources compared to experiments on the standard CIFAR-10 dataset. However, across datasets we consistently vary only the scale of the neural network architecture in all our experiments. Thus, the primary technical challenge we identify for experiments on larger datasets with larger models is the associated computational expense."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7849/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468894186,
                "cdate": 1700468894186,
                "tmdate": 1700468894186,
                "mdate": 1700468894186,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RRlUebDjBo",
                "forum": "hkL8djXrMM",
                "replyto": "HuMjnQ8ExT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7849/Reviewer_LvPx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7849/Reviewer_LvPx"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for the answers. I retain my score and will also consider the comments from other reviewers for the final recommendation."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7849/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631854344,
                "cdate": 1700631854344,
                "tmdate": 1700631854344,
                "mdate": 1700631854344,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v2WyKiwmB1",
            "forum": "hkL8djXrMM",
            "replyto": "hkL8djXrMM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7849/Reviewer_rkQA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7849/Reviewer_rkQA"
            ],
            "content": {
                "summary": {
                    "value": "Diffusion models traditionally use only linear transformations, however using a broader family of transformations can potentially help train generative distributions more efficiently. This work presents Neural diffusion models that generalize existing diffusion models by adding learnable transformation of data which are parameterized with a neural network. The corresponding forward and reverse process are derived by modifying DDPM forward and reverse process. The variational loss objective has been generalized to include learnable transformations. NDM can also be extended to continuous time diffusion models based on previous work by Song et al. 2020. Many previously proposed diffusion models are instances of NDM. Overall, NDM shows gains in NLL and negative ELBO over DDPM."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea of generalizing diffusion models to learnable non-linear transformations is interesting.\n- Many previously proposed diffusion models and flow models are special cases of NDMs with specific choice of transformation.\n- The qualitative results of learned transformations for different datasets in Figure 2 are interesting.\n- NDM provides consistent gains in terms of NLL and NELBO over DDPM (See Table 4 and 7)."
                },
                "weaknesses": {
                    "value": "- One of the primary motivations for learnable transformations is that it simplifies the data distribution and therefore leads to predictions of x that are more aligned with data. Ideally, if transformations indeed helped with simplification of data distribution, one should have observed better quantitive metrics in fewer sampling steps. However, the actual gains in quantitative metrics like NLL and NELBO seem marginal. Further, there seems to be no consistent gains in terms of FID. In addition, as listed in limitations, the model uses 2.3$\\times$ more training time than DDPM (which by itself is slow and needs hundreds of thousands of steps to get good FID). Therefore, I am not sure if the minor gains in NLL and NELBO can be justified when compared to 2.3$\\times$ increase in training time as well as $\\sim 2\\times$ increase in model size.\n- Benefits of learnable transformations hasn\u2019t been explored much in the paper and as a result its benefits remain unclear. There is a toy experiment on training optimal transport on 1D data in the appendix. However, larger scale experiments on real data would make this work much stronger. Overall, I feel that NDMs are well-motivated but it remains unclear from the experimental results that introducing learnable transformations as a standard practice for training diffusion models is beneficial."
                },
                "questions": {
                    "value": "- After going through the implementation details (Section 4.1, Appendix C),  the architectural details of neural network used to model the transformation $F_\\psi$  remains unclear to me. Could the authors further elaborate on these details? \n- Table 4 compares DDPM and NDM. However, NDM has non-Markovian forward and reverse process and for a fair comparison, results for DDIM should also be included, especially in the cases when the number of sampling steps is smaller than the number of training steps."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7849/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785681643,
            "cdate": 1698785681643,
            "tmdate": 1699636961590,
            "mdate": 1699636961590,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xsZPPOLICd",
                "forum": "hkL8djXrMM",
                "replyto": "v2WyKiwmB1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7849/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7849/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are delighted that the reviewer found our idea of generalization and the qualitative results of learnable transformations intriguing. Below we address the questions and comments raised in the review.\n\n**Weaknesses:**\n\n- As soon as NDM with learnable transformation $F_\\varphi$ has a more flexible forward process, an improved likelihood estimation is anticipated, as indeed observed. As illustrated in Section 4.2, the incorporation of learnable transformations consistently enhances both NLL and NELBO across a variety of datasets, including state-of-the-art results on ImageNet-64 dataset.\n    \n    While these learnable transformations simplify the generative process, making the approximation by neural networks easier, it does not necessarily ensure the simplicity of generative trajectories. In theory, it may potentially lead to increased curvature of trajectories. Therefore, the results presented for NDM with a reduced number of steps (Table 4, 1000 \u2192 10 steps) are considered more of an empirical finding rather than an expected property.\n    \n    While extra flexibility of the forward process leads to consistently improved likelihood estimation, it doesn't always lead to improved FID score. This is a well-known attribute of diffusion models trained with the full ELBO objective (eq. 9).\n    \n    We introduce the NDM as a novel framework that generalizes many existing diffusion models and allows for a more complex parameterization of the forward process. Therefore, the primary focus of our experimental section is to demonstrate the feasibility of the framework, rather than presenting a stronger model that surpasses baselines. We believe that the NDM may exhibit enhanced performance with different parameterization and hyperparameter tuning. Therefore, we aimed to maintain a simple experimental setup. Nevertheless, our findings illustrate that the NDM, with learnable transformations, yields improved NLL estimation while remaining comparable in terms of FID score. In addition, in Appendix D.3, we present the ablation study, which demonstrates that the improved quality is not a consequence of an increased number of parameters.\n    \n- While we agree that advantages of NDMs are not yet fully exploited, we consider this a strength of the method rather than a weakness. NDM introduces a new flexible framework for diffusion modelling, derives novel time-discrete and time-continuous bounds on the log-marginal likelihood, studies the type of potential transformations that are learned, delivers promising NLL results on a variety of real and synthetic datasets, as well as discusses its potential limitations. Because of the flexibility of NDMs they enable interesting potential for adapting to the problem at hand and new avenues for future research in generative modelling, like learning dynamic optimal transport that we present in Appendix E.\n\n**Questions:**\n\n- In all our experiments with learnable transformations we use the same neural network parameterization for both transformations $F_\\varphi$ and prediction of the datapoint $\\hat{x}_\\theta$.\n- While NDM can be thought of as a generalisation of DDIMs, the DDIM-style deterministic sampling effectively implies sampling from a process different from the one with which the model was trained. Thus, for a comparison, we opted to sample from the same processes that the model was trained on, which coincides with the DDPM reverse process. However, since NDM allows deterministic sampling we acknowledge the interest of this alternative comparison and plan to incorporate it into the camera-ready version of our paper to compare FID scores. The switch from DDPM to DDIM doesn\u2019t affect evaluation of likelihood, since it relies on integration of deterministic process in both cases."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7849/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468860860,
                "cdate": 1700468860860,
                "tmdate": 1700468860860,
                "mdate": 1700468860860,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "onb0J7DoPx",
                "forum": "hkL8djXrMM",
                "replyto": "xsZPPOLICd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7849/Reviewer_rkQA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7849/Reviewer_rkQA"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you for answering my questions! I would like to retain my score for now."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7849/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594721624,
                "cdate": 1700594721624,
                "tmdate": 1700594721624,
                "mdate": 1700594721624,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]