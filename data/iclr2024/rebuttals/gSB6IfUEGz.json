[
    {
        "title": "VoxGenesis: Unsupervised Discovery of Latent Speaker Manifold for Speech Synthesis"
    },
    {
        "review": {
            "id": "N5obwzEuPy",
            "forum": "gSB6IfUEGz",
            "replyto": "gSB6IfUEGz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3241/Reviewer_cVSz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3241/Reviewer_cVSz"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a speaker creation method that uses a pre-trained speaker embedding model together with Gaussian sampling to generate new speakers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed model performs better than Tacospawn in generating unseen and diverse speakers.\n2. Ablation studies are performed to understand the impact of different design choices.\n3. The paper is in general written clearly with a good amount of illustrations and maths."
                },
                "weaknesses": {
                    "value": "1. I think the novelty is somewhat limited. To me, the proposed method is quite similar to a VAE-based speaker encoder but with the encoder changed to a pre-trained speaker embedding module. The mapping network is almost the same as in StyleGAN and is quite similar in principle to AdaSpeech. My main takeaway is that a pre-trained speaker embedding model as the encoder of a VAE works well for generating unseen speakers, and NFA is a promising speaker embedding model compared to other pre-trained speaker embedding model.\n2. The baselines compared are quite old by deep learning standards. There are a few follow-up works to TacoSpawn. For zero-shot VC there are many more recent works (e.g. ControlVC). The authors state VITS is the \"the state of the art\" Multi-Speaker TTS but it is by no means true in 2023."
                },
                "questions": {
                    "value": "For Table 1, it is not clear to me for each baseline, what is the exact procedure and hyper-params involved to sample speaker embeddings. I hope the authors can clarify."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3241/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698352618471,
            "cdate": 1698352618471,
            "tmdate": 1699636272623,
            "mdate": 1699636272623,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ikmquQoS6z",
                "forum": "gSB6IfUEGz",
                "replyto": "N5obwzEuPy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3241/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3241/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for the clear and constructive feedback. We have addressed the questions as follows:\n> Q1 I think the novelty is somewhat limited. To me, the proposed method is quite similar to a VAE-based speaker encoder but with the encoder changed to a pre-trained speaker embedding module. The mapping network is almost the same as in StyleGAN and is quite similar in principle to AdaSpeech. My main takeaway is that a pre-trained speaker embedding model as the encoder of a VAE works well for generating unseen speakers, and NFA is a promising speaker embedding model compared to other pre-trained speaker embedding model.\n\n**A1** We believe the reviewer missed the main contribution of our paper. If we have to put the takeaway in one sentence. It would be \u201cconstraining speaker embedding distribution into a known form allows speaker generation and speaker attribution manipulation\u201d. Our contribution is not in introducing NFA per se or training speaker embedding in a VAE encoder-like fashion. Rather we identify a principled way to do speaker generation, i.e. by constraining speaker embedding distribution to a known form. \nFor the mapping network, we mainly used it to transform the speaker embedding from an isotropic Gaussian distribution into a non-isotropic one to allow latent manipulation. StyleGAN does not use it for latent manipulation.\n\n> Q2  The baselines compared are quite old by deep learning standards. There are a few follow-up works to TacoSpawn. For zero-shot VC there are many more recent works (e.g. ControlVC). The authors state VITS is the \"the state of the art\" Multi-Speaker TTS but it is by no means true in 2023.\n\n**A2** We have expanded our experiments to include more baselines and datasets. Specifically, we have now incorporated FastSpeech2 and StyleTTS, both are recent popular papers, into our comparisons for multi-speaker TTS. Additionally, we have included ControlVC as an additional voice conversion baseline to provide a broader perspective on our model's performance.\n\n**Comparison of Multi-Speaker TTS Performance on LibriTTS-R, Original LibriTTS, and VCTK, Featuring Benchmarks Against VITS, StyleTTS, and FastSpeech**\n\n| Measurement | Dataset   | NFA-VoxGenesis  | VITS         | StyleTTS     | FastSpeech2  |\n|-------------|-----------|-----------------------------------|--------------|--------------|--------------|\n| Spk. MOS    | LibriTTS-R | 4.03\u00b10.09                         | 3.63\u00b10.2     | 3.68\u00b10.09    | 3.55\u00b10.12    |\n| MOS         | LibriTTS-R | 4.15\u00b10.08                         | 3.8\u00b10.09     | 3.94\u00b10.07    | 3.82\u00b10.07    |\n| Spk. MOS    | LibriTTS   | 4.05\u00b10.06                         | 3.42\u00b10.11    | 3.74\u00b10.06    | 3.77\u00b10.11    |\n| MOS         | LibriTTS   | 4.02\u00b10.08                         | 3.54\u00b10.07    | 3.82\u00b10.08    | 3.72\u00b10.08    |\n| Spk. MOS    | VCTK       | 4.3\u00b10.07                          | 3.95\u00b10.09    | 4.02\u00b10.07    | 3.81\u00b10.06    |\n| MOS         | VCTK       | 4.42\u00b10.09                         | 4.03\u00b10.08    | 4.09\u00b10.06    | 4.18\u00b10.07    |\n\n**Results of Voice Conversion Experiments on LibriTTS-R, Original LibriTTS, and VCTK Datasets**\n\n| Method            | Dataset   | WER  | EER  | MOS        |\n|-------------------|-----------|------|------|------------|\n| NFA-VoxGensis     | LibriTTS-R| 7.56 | 5.75 | 4.21\u00b10.07  |\n| Speech Resynthesis| LibriTTS-R| 7.54 | 6.23 | 3.77\u00b10.08  |\n| ControlVC         | LibriTTS-R| 7.57 | 5.98 | 3.85\u00b10.06  |\n| NFA-VoxGensis     | LibriTTS  | 6.13 | 4.82 | 4.01\u00b10.07  |\n| Speech Resynthesis| LibriTTS  | 6.72 | 5.49 | 3.42\u00b10.04  |\n| ControlVC         | LibriTTS  | 6.43 | 5.22 | 3.56\u00b10.03  |\n| NFA-VoxGensis     | VCTK      | 5.68 | 2.83 | 4.32\u00b10.08  |\n| Speech Resynthesis| VCTK      | 6.15 | 4.17 | 3.58\u00b10.09  |\n| ControlVC         | VCTK      | 6.03 | 3.88 | 3.66\u00b10.07  |\n\n\nWe have checked the follow-up works to Tacospawn. The most popular one is \"Creating new voices using normalizing flows\" [1]. After reading the paper in detail, we found that their way to do speaker generation is entirely the same as Tacospawn. Therefore, we believe it is less interesting to include it.  We are happy to include an additional speaker generation baselines if the reviewer has suggestions.\n\n[1] Bilinski, P., Merritt, T., Ezzerg, A., Pokora, K., Cygert, S., Yanagisawa, K., Barra-Chicote, R., Korzekwa, D. (2022) Creating New Voices using Normalizing Flows. Proc. Interspeech 2022"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3241/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219196990,
                "cdate": 1700219196990,
                "tmdate": 1700240422804,
                "mdate": 1700240422804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OncNNyjnPD",
                "forum": "gSB6IfUEGz",
                "replyto": "ikmquQoS6z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3241/Reviewer_cVSz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3241/Reviewer_cVSz"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the addition results. However I am still not convinced about the reply to Q1. The authors said that the main takeaway should be \u201cconstraining speaker embedding distribution into a known form allows speaker generation and speaker attribution manipulation\u201d, but isn't this just simply the main takeaway of any VAE? For example, when beta-VAE first came out, the main takeaway was that \"constraining the latent distribution allows for disentangled attribute manipulation and image generation.\" Therefore I still think the novelty is limited."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3241/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687198617,
                "cdate": 1700687198617,
                "tmdate": 1700687198617,
                "mdate": 1700687198617,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MN90G7Va6S",
            "forum": "gSB6IfUEGz",
            "replyto": "gSB6IfUEGz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3241/Reviewer_fp4x"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3241/Reviewer_fp4x"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a neural factor analysis (NFA)-based speech synthesis model for tts, vc, and voice generation. They simply disentangle a HuBERT representation for controlling the semantic guided voice representations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "They utilize a neural factor analysis (NFA) for speech representation disentanglement, and adapt the semantic representation with style conditions. This simple modification could improve the TTS and VC performance by adopting it to speech resynthesis frameworks."
                },
                "weaknesses": {
                    "value": "1.\tThe authors should have conducted more comparisons to evaluate the model performance. They only compare it with speech resynthesis, and the result is a little incremental.\n\n2.\tThey utilize a NFA which was proposed in ICML 2023. The contribution is weak.\n\n3.\tIt would be better if you could compare the self-supervised speech representation model for the robustness of your methods by replacing HuBert with any other SSL models.\n\n4.\tUsing HuBERT representation may induce a high WER. Replacing it with ContentVec may improve the pronunciation.\n\n5.\tThe semantic conditioned Transformation is utilized in many works. NANSY++ utilizes a time-varying timbre embedding and this is almost the same with this part."
                },
                "questions": {
                    "value": "1.\tThe authors cited LibriTTS and LibriTTS-R together. Which dataset do you use to train the models? In my personal experience, using LibriTTS-R decreases the sample diversity. Do you have an experience with this?\n\n2.\tAccording to ICLR policy, when using human subjects such as MOS, you may include the evaluation details in your paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3241/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3241/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3241/Reviewer_fp4x"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3241/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698653867888,
            "cdate": 1698653867888,
            "tmdate": 1700624087165,
            "mdate": 1700624087165,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zoJskeGFWO",
                "forum": "gSB6IfUEGz",
                "replyto": "MN90G7Va6S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3241/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3241/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clear and constructive feedback. We have addressed the questions as follows:\n> Q1: The authors should have conducted more comparisons to evaluate the model performance. and the result is a little incremental.\n\nA1: We have expanded our experiments to include more baselines and datasets. Specifically, we have now incorporated FastSpeech2 and StyleTTS into our comparisons for multi-speaker TTS. Additionally, we have included ControlVC as an additional voice conversion baseline to provide a broader perspective on our model's performance.\n\n**Comparison of Multi-Speaker TTS Performance on LibriTTS-R, Original LibriTTS, and VCTK, Featuring Benchmarks Against VITS, StyleTTS, and FastSpeech**\n\n| Measurement | Dataset   | NFA-VoxGenesis  | VITS         | StyleTTS     | FastSpeech2  |\n|-------------|-----------|-----------------------------------|--------------|--------------|--------------|\n| Spk. MOS    | LibriTTS-R | 4.03\u00b10.09                         | 3.63\u00b10.2     | 3.68\u00b10.09    | 3.55\u00b10.12    |\n| MOS         | LibriTTS-R | 4.15\u00b10.08                         | 3.8\u00b10.09     | 3.94\u00b10.07    | 3.82\u00b10.07    |\n| Spk. MOS    | LibriTTS   | 4.05\u00b10.06                         | 3.42\u00b10.11    | 3.74\u00b10.06    | 3.77\u00b10.11    |\n| MOS         | LibriTTS   | 4.02\u00b10.08                         | 3.54\u00b10.07    | 3.82\u00b10.08    | 3.72\u00b10.08    |\n| Spk. MOS    | VCTK       | 4.3\u00b10.07                          | 3.95\u00b10.09    | 4.02\u00b10.07    | 3.81\u00b10.06    |\n| MOS         | VCTK       | 4.42\u00b10.09                         | 4.03\u00b10.08    | 4.09\u00b10.06    | 4.18\u00b10.07    |\n\n**Results of Voice Conversion Experiments on LibriTTS-R, Original LibriTTS, and VCTK Datasets**\n\n| Method            | Dataset   | WER  | EER  | MOS        |\n|-------------------|-----------|------|------|------------|\n| NFA-VoxGensis     | LibriTTS-R| 7.56 | 5.75 | 4.21\u00b10.07  |\n| Speech Resynthesis| LibriTTS-R| 7.54 | 6.23 | 3.77\u00b10.08  |\n| ControlVC         | LibriTTS-R| 7.57 | 5.98 | 3.85\u00b10.06  |\n| NFA-VoxGensis     | LibriTTS  | 6.13 | 4.82 | 4.01\u00b10.07  |\n| Speech Resynthesis| LibriTTS  | 6.72 | 5.49 | 3.42\u00b10.04  |\n| ControlVC         | LibriTTS  | 6.43 | 5.22 | 3.56\u00b10.03  |\n| NFA-VoxGensis     | VCTK      | 5.68 | 2.83 | 4.32\u00b10.08  |\n| Speech Resynthesis| VCTK      | 6.15 | 4.17 | 3.58\u00b10.09  |\n| ControlVC         | VCTK      | 6.03 | 3.88 | 3.66\u00b10.07  |\n\nPerformance wise we believe the improvement is no incremental. Comparing with TacoSpawn, our NFA-VoxGenesis MOS improvement is 0.68, which by any criterion is significant improvement. The discrepancy in MOS is also demonstrated in sample quality in the demo. For voice conversion, the MOS margin with SpeechResynthesis is 0.44, which is also quit considerable improvement. The improvement is relatively small only in multi-speaker TTS. \n\n> Q2: They utilize a NFA which was proposed in ICML 2023. The contribution is weak.\n\nA2: Our primary contribution is not the utilization of NFA. We demonstrate that regular speaker embedding networks, as well as unsupervised contrastive trained embedding networks, can be effectively used for novel speaker generation and speaker editing. The crux of our approach is constraining the speaker distribution p(z) into a form that allows for sampling. We introduce a principled method for speaker generation. Additionally, our work contributes to style control, particularly in independently controlling pitch, tone, and emotion through latent space editing. This contrasts with Style Tokens [1], which have attribute entanglement issues and differs from Style-TTS [2]  which rely on external references for emotion control. Our pitch control also stands apart from pitch-specific techniques like ControlVC [3] and Fastspeech2 [4], which use pitch features and a dedicated pitch encoders.\n\nReference:\n\n[1] Wang, Yuxuan, et al. \"Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis.\"\u00a0International conference on machine learning. PMLR, 2018.\u2028\n\n[2] Li, Yinghao Aaron, Cong Han, and Nima Mesgarani. \"Styletts: A style-based generative model for natural and diverse text-to-speech synthesis.\" arXiv preprint arXiv:2205.15439 (2022).\u2028\n\n[3] Chen, Meiying, and Zhiyao Duan. \"ControlVC: Zero-Shot Voice Conversion with Time-Varying Controls on Pitch and Rhythm.\"\u00a0arXiv preprint arXiv:2209.11866\u00a0(2022).\u2028\n\n[4] Ren, Yi, et al. \"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech.\"\u00a0International Conference on Learning Representations. 2021."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3241/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219785903,
                "cdate": 1700219785903,
                "tmdate": 1700222023352,
                "mdate": 1700222023352,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wFzHrkMkeY",
                "forum": "gSB6IfUEGz",
                "replyto": "MN90G7Va6S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3241/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3241/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">Q3 & Q4: It would be better to compare the self-supervised speech representation model for the robustness of your methods by replacing HuBert with other SSL models. Using HuBERT representation may induce a high WER. Replacing it with ContentVec may improve the pronunciation.\n\nA3 & A4: Thanks for the suggestion on SSL module. We have included results from other SSL-based content extractors, such as wav2vec-BERT and Content-Vec. The impact on WER is obvious. We will incorporate these findings in our revised paper.\n| Speaker Module | Content Model | WER  | EER  | MOS        |\n|----------------|---------------|------|------|------------|\n| NFA            | HuBERT        | 7.56 | 5.75 | 4.21\u00b10.07  |\n| NFA            | w2v-BERT      | 7.22 | 5.63 | 4.1\u00b10.09   |\n| NFA            | ContentVec    | 7.04 | 5.65 | 4.25\u00b10.05  |\n\nQ5: The semantic conditioned Transformation is utilized in many works. NANSY++ utilizes a time-varying timbre embedding and this is almost the same with this part.\n\nA5: We would like to clarify that we do not claim the semantic conditioned Transformation as our innovation. Instead, our contribution lies in constraining speaker embeddings p(z) to be easily sampled before transformation.\n\n> Q6: The authors cited LibriTTS and LibriTTS-R together. Which dataset do you use to train the models? In my personal experience, using LibriTTS-R decreases the sample diversity.\n\nWe used LibriTTS-R to train our model. Because it is a newly released dataset and based on LibriTTS, we cite both papers in case some readers are not familiar with LibriTTS-R. Our observations align with the reviewer: using the original LibriTTS leads to greater diversity in generated speakers compared to models trained with LibriTTS-R. We will include the results on the original LibriTTS in the appendix.\n\n| Method          | Dataset   | FID  | Spk. Similarity | Spk. Diversity | MOS        |\n|-----------------|-----------|------|-----------------|----------------|------------|\n| NFA-VoxGenesis  | LibriTTS-R| 0.14 | 0.3             | 4.17\u00b10.09      | 4.22\u00b10.06  |\n| NFA-VoxGenesis  | LibriTTS  | 0.15 | 0.23            | 4.4\u00b10.07       | 4.03\u00b10.05  |\n\n> Q7: According to ICLR policy, when using human subjects such as MOS, you may include the evaluation details in your paper.\n\nA7: The evaluation process of the Mean Opinion Score (MOS) is in Section 4.2 of our initial submission. This section comprehensively covers the protocols followed during the human subject evaluation."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3241/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220016370,
                "cdate": 1700220016370,
                "tmdate": 1700222245765,
                "mdate": 1700222245765,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1MQqY6rSjG",
                "forum": "gSB6IfUEGz",
                "replyto": "wFzHrkMkeY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3241/Reviewer_fp4x"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3241/Reviewer_fp4x"
                ],
                "content": {
                    "title": {
                        "value": "Thansk for your response"
                    },
                    "comment": {
                        "value": "Thanks for your detailed answer. It took some time to reply this response. I've read the paper and response several times. \n\nI acknowledge that the result is not a little incremental compared to other models you compared. However, there is no recent model. VITS was published in 2021. \n\nThe most weakness of this paper is a lack of survey for recent models. In my opinion, the TTS pipeline is similar to SPEAR-TTS which generates a semantic token from text, and then generates an acoustic or waveform from the semantic token. \n\nIn addition, Tacotron-based semantic token generation may have a problem such as repeating and skipping. It would be better if you could add the experiments for robustness such as WER for TTS results.\n\nI will raise a score from 3 to 5. However, I'm still thinking this paper does not meet the bar for the high standard of ICLR2024."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3241/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624068624,
                "cdate": 1700624068624,
                "tmdate": 1700624068624,
                "mdate": 1700624068624,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eE6aPrlIEf",
                "forum": "gSB6IfUEGz",
                "replyto": "i2QSxZ4Kdu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3241/Reviewer_fp4x"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3241/Reviewer_fp4x"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for your response.\n\nI acknowledge that the main contribution of this paper is not TTS pipeline so I may overstate this issue. However, I think that it is difficult to evaluate the performance of speaker generation and speaker editing so the performance may be judged by TTS or VC results. To be honest, these topic are such minor in speech domain and the speaker generation and controlling are just resulted from training the multi-speaker TTS models.\n\nIt would be better if you could define the speaker generation more precisely in terms of prosody and voice and this paper is more fit for a speech venue such as TASLP, Interspeech, and ICASSP."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3241/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630374381,
                "cdate": 1700630374381,
                "tmdate": 1700630374381,
                "mdate": 1700630374381,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DSRVuTcdym",
            "forum": "gSB6IfUEGz",
            "replyto": "gSB6IfUEGz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3241/Reviewer_dpEi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3241/Reviewer_dpEi"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an unsupervised voice generation model called VoxGenesis by transforming Gaussian distribution to speech distribution. The proposed VoxGenesis can discover a latent speaker manifold and meaningful voice editing directions without supervision, and the latent space uncovers human-interpretable directions associated with specific speaker characteristics such as gender attributes, pitch, tone, and emotion, allowing for voice editing by manipulating the latent codes along these identified directions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. It is interesting to utilize Gaussian distribution transformation for unsupervised voice (speech) synthesis so that the model is able to generate realistic speakers with distinct characteristics like pitch, tone, and emotion."
                },
                "weaknesses": {
                    "value": "1. While the idea is promising, the experimental results seem to be limited. Most of the performances are from the ablation studies. The proposed model should compare the performances with the previous works like SLMGAN [1], StyleTTS [2], and LVC-VC [3]. Moreover, the paper only utilizes one dataset, LibriTTS-R. More extensive experiments on different dataset might be necessary.\n2. The paper can be more curated. While it is well written paper, it slightly lacks in structure. Since the idea is interesting enough, I would consider adjusting the rating if the paper is more well structured and the additional experiments are conducted.\n\n[1] Li, Yinghao Aaron, Cong Han, and Nima Mesgarani. \"SLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs.\" 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). IEEE, 2023.\n\n[2] Li, Yinghao Aaron, Cong Han, and Nima Mesgarani. \"Styletts: A style-based generative model for natural and diverse text-to-speech synthesis.\" arXiv preprint arXiv:2205.15439 (2022).\n\n[3] Kang, Wonjune, Mark Hasegawa-Johnson, and Deb Roy. \"End-to-End Zero-Shot Voice Conversion with Location-Variable Convolutions.\" Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH. Vol. 2023. 2023."
                },
                "questions": {
                    "value": "Please refer to the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3241/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3241/Reviewer_dpEi",
                        "ICLR.cc/2024/Conference/Submission3241/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3241/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752220080,
            "cdate": 1698752220080,
            "tmdate": 1700638217681,
            "mdate": 1700638217681,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "myCaHmsMva",
                "forum": "gSB6IfUEGz",
                "replyto": "DSRVuTcdym",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3241/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3241/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clear and constructive feedback. We have addressed the reviewer's questions as follows:\n> Q1.While the idea is promising, the experimental results seem to be limited. Most of the performances are from the ablation studies.\n\n> Q2. The paper can be more curated. While it is well written paper, it slightly lacks in structure. \n\nA: We have made efforts to improve the structure of our paper by expanding experiments with more datasets and comparison with relevant recent works like StyleTTS [1], ControlVC [2], and FastSpeech2 [3].\n\n**Comparison of Multi-Speaker TTS Performance on LibriTTS-R, Original LibriTTS, and VCTK, Featuring Benchmarks Against VITS, StyleTTS, and FastSpeech**\n| Measurement | Dataset   | NFA-VoxGenesis  | VITS         | StyleTTS     | FastSpeech2  |\n|-------------|-----------|-----------------------------------|--------------|--------------|--------------|\n| Spk. MOS    | LibriTTS-R | 4.03\u00b10.09                         | 3.63\u00b10.2     | 3.68\u00b10.09    | 3.55\u00b10.12    |\n| MOS         | LibriTTS-R | 4.15\u00b10.08                         | 3.8\u00b10.09     | 3.94\u00b10.07    | 3.82\u00b10.07    |\n| Spk. MOS    | LibriTTS   | 4.05\u00b10.06                         | 3.42\u00b10.11    | 3.74\u00b10.06    | 3.77\u00b10.11    |\n| MOS         | LibriTTS   | 4.02\u00b10.08                         | 3.54\u00b10.07    | 3.82\u00b10.08    | 3.72\u00b10.08    |\n| Spk. MOS    | VCTK       | 4.3\u00b10.07                          | 3.95\u00b10.09    | 4.02\u00b10.07    | 3.81\u00b10.06    |\n| MOS         | VCTK       | 4.42\u00b10.09                         | 4.03\u00b10.08    | 4.09\u00b10.06    | 4.18\u00b10.07    |\n\n**Results of Voice Conversion Experiments on LibriTTS-R, Original LibriTTS, and VCTK Datasets**\n| Method            | Dataset   | WER  | EER  | MOS        |\n|-------------------|-----------|------|------|------------|\n| NFA-VoxGensis     | LibriTTS-R| 7.56 | 5.75 | 4.21\u00b10.07  |\n| Speech Resynthesis| LibriTTS-R| 7.54 | 6.23 | 3.77\u00b10.08  |\n| ControlVC         | LibriTTS-R| 7.57 | 5.98 | 3.85\u00b10.06  |\n| NFA-VoxGensis     | LibriTTS  | 6.13 | 4.82 | 4.01\u00b10.07  |\n| Speech Resynthesis| LibriTTS  | 6.72 | 5.49 | 3.42\u00b10.04  |\n| ControlVC         | LibriTTS  | 6.43 | 5.22 | 3.56\u00b10.03  |\n| NFA-VoxGensis     | VCTK      | 5.68 | 2.83 | 4.32\u00b10.08  |\n| Speech Resynthesis| VCTK      | 6.15 | 4.17 | 3.58\u00b10.09  |\n| ControlVC         | VCTK      | 6.03 | 3.88 | 3.66\u00b10.07  |\n\nReference:\n\n[1] Li, Yinghao Aaron, Cong Han, and Nima Mesgarani. \"Styletts: A style-based generative model for natural and diverse text-to-speech synthesis.\" arXiv preprint arXiv:2205.15439 (2022).\u2028\n\n[2] Chen, Meiying, and Zhiyao Duan. \"ControlVC: Zero-Shot Voice Conversion with Time-Varying Controls on Pitch and Rhythm.\" arXiv preprint arXiv:2209.11866 (2022).\u2028\n\n[3] Ren, Yi, et al. \"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech.\" International Conference on Learning Representations. 2021."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3241/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220685625,
                "cdate": 1700220685625,
                "tmdate": 1700221547568,
                "mdate": 1700221547568,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FEMA3VX1LH",
                "forum": "gSB6IfUEGz",
                "replyto": "myCaHmsMva",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3241/Reviewer_dpEi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3241/Reviewer_dpEi"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response, and I have no further questions. I have read the modified version of manuscript, but still the paper seems to be not well curated and not easily readable. While I will raise few individual scores, I will leave my overall score as it is."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3241/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638201153,
                "cdate": 1700638201153,
                "tmdate": 1700638201153,
                "mdate": 1700638201153,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cozv4tAC4E",
            "forum": "gSB6IfUEGz",
            "replyto": "gSB6IfUEGz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3241/Reviewer_dHpK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3241/Reviewer_dHpK"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses on an approach on modeling speaker's voice in speech generation models, as well as its application in voice conversion and multispeaker TTS."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Not clear to me."
                },
                "weaknesses": {
                    "value": "* The clarity of the presentation and writing needs improvement. Overall, it's difficult to read this paper. Some basic writing styles, like missing necessary parentheses on citations, is very bothersome for reading. The description of the method seems over complicated than what the method actually is.\n\n* A lot of technical incorrectness. Just a few examples:\n   - Sec 3.1: \"GAN has been the de facto choice for vocoders.\" This is a false claim and ignores a large arrays of active and important works in the community. There are other popular vocoders choices like WaveNet, WaveRnn, diffusion-based approaches etc.\n   - Sec 3.1: \"A notable limitation in these models is ... learn to replicate the voices of the training or reference speakers rather than creating new voices.\" Another false claim. It improper to state for such an \"limitation\" because that is not the goal of the task of vocoders.\n   - Sec 3.1 \"consequently, a conditional GAN is employed to transform a Gaussian distribution, rather than mapping speech features to waveforms\". This is another improper comparison as what GAN does is to transfer the conditioning features (e.g. speech features) , rather than plain Gaussian noise, to waveforms.\n   - Sec 3.1 \"It is crucial, in the absence of Mel-spectrogram loss, for the discriminator to receive semantic tokens; otherwise, the generator could deceive the discriminator with intelligible speech.\" I don't see the connection.\n   - Sec 4.2 says speaker similarity is evaluated in a 3-point scale from 0 to 1, but Table 3 shows speaker similarity as 4.x and 3.x values."
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3241/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698885209547,
            "cdate": 1698885209547,
            "tmdate": 1699636272341,
            "mdate": 1699636272341,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "svFE9Aelt3",
                "forum": "gSB6IfUEGz",
                "replyto": "cozv4tAC4E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3241/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3241/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Q1 Sec 3.1: \"GAN has been the de facto choice for vocoders.\" This is a false claim and ignores a large arrays of active and important works in the community. There are other popular vocoders choices like WaveNet, WaveRnn, diffusion-based approaches etc.\n\nA1: We do not ignore the large arrays of active and important works in the community. Actually, we explicitly said so during the literature review section. \n\n> \u201cGiven the necessity for vocoders in both VC and TTS to invert the spectrogram, there has been a significant effort to improve them. This has led to the development of autoregressive, flow, GAN, and diffusion-based vocoders (Kong et al., 2020; Oord et al., 2016; Prenger et al., 2019; Chen et al., 2020). \u201c\u2028\n\n\n> Q2 Sec 3.1: \"A notable limitation in these models is ... learn to replicate the voices of the training or reference speakers rather than creating new voices.\" Another false claim. It improper to state for such an \"limitation\" because that is not the goal of the task of vocoders.\n\nA2 We do not say the goal of the vocoder is to create the voice anywhere in our paper. We merely state a fact that the current GAN vocoder tries to replicate the voice of the training speaker instead of creating new voices.\n\n\n> Q3 Sec 3.1 \"consequently, a conditional GAN is employed to transform a Gaussian distribution, rather than mapping speech features to waveforms\". This is another improper comparison as what GAN does is to transfer the conditioning features (e.g. speech features) , rather than plain Gaussian noise, to waveforms.\n \nA3 The whole novelty of our paper is to transform a known distribution (a Gaussian distribution) into a waveform, which are speaker embeddings. We have a hard time finding what is the improper comparison here. \n\n> Q4  Sec 3.1 \"It is crucial, in the absence of Mel-spectrogram loss, for the discriminator to receive semantic tokens; otherwise, the generator could deceive the discriminator with intelligible speech.\" I don't see the connection.\n\nQ4 Mel-spectrum loss is very important to GAN-based vocoder, without it the network does not receive conditional information, in the absence of it, we provided conditional information in the form of semantic tokens.\n\n> Q5 Sec 4.2 says speaker similarity is evaluated in a 3-point scale from 0 to 1, but Table 3 shows speaker similarity as 4.x and 3.x values.\n\nA5 Table 3 shows speaker MOS instead of speaker similarity. We can not understand why the reviewer thinks it is speaker similarity. Table 3 explicitly writes speaker MOS. Both speaker similarity and Speaker MOS  were explicitly defined in our paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3241/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700447414821,
                "cdate": 1700447414821,
                "tmdate": 1700447892107,
                "mdate": 1700447892107,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PC7xrTGhz0",
                "forum": "gSB6IfUEGz",
                "replyto": "svFE9Aelt3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3241/Reviewer_dHpK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3241/Reviewer_dHpK"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses from the authors. After reading them, I decide to keep my review feedback and the rating scores.\n\nRe A5:\n\nMy understanding of the 3 subjective metrics described in Sec 4.2 are: speaker similarity is between 0-1; diversity score is between 0-5, naturalness range is not described. All 3 of them fall under the term \"MOS\", which stands for \"mean opinion score\". This section doesn't describe a metric named \"speaker MOS\".\n\nTable 3 reports two metrics \"Spk MOS\" and \"MOS\", which are not quite clear what they refers to as described in Sec 4.2. However, the text in Sec 5.2 says \"As indicated in Table 3, VoxGenesis achieves higher MOS scores in both speaker similarity and naturalness.\" So clearly to me that there is the discrepancy on the \"speaker similarity\" score ranges between the description in Sec 4.2 and the numbers reported in Table 3."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3241/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615482108,
                "cdate": 1700615482108,
                "tmdate": 1700615482108,
                "mdate": 1700615482108,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]