[
    {
        "title": "NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers"
    },
    {
        "review": {
            "id": "kTpU8lwTfD",
            "forum": "Rc7dAwVL3v",
            "replyto": "Rc7dAwVL3v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4886/Reviewer_8Wev"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4886/Reviewer_8Wev"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a latent diffusion-based speech synthesis framework for high-quality zero-shot speech synthesis. They utilize an audio codec as a latent representation and a conditional latent diffusion model could generate a latent representation. Then, the codec decoder generates a waveform audio. The zero-shot results show a better performance than the codec-based TTS model and YourTTS. Moreover, the audio quality is good."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "They propose the latent diffusion-based speech synthesis model. This work may be the first successful implementation of a latent diffusion model for speech synthesis. Although recently large-language model (LLM) -based speech synthesis models have been investigated, they have too many problems for speech synthesis resulting from the auto-regressive generative manner. However, this work adopts a parallel synthesis framework with latent diffusion, and successfully shows their generative performance by several speech tasks.\n\nRecent papers only compare their work with YourTTS but I do not think YourTTS is a good zero-shot TTS model. The audio quality of YourTTS is too bad. However, although recent models do not provide an official implementation, the authors tried to compare their model with many other works."
                },
                "weaknesses": {
                    "value": "1. They also conducted an ablation study well. However, it would be better if the authors could add the results according to the dataset and model size. The model size of NaturalSpeech 2 is too complex compared to VITS. In my personal experience, VITS with speaker prompt could achieve significantly better performance than YourTTS. \n \n2. For inference speed, NaturalSpeech 2 still has a low latency for its iterative generation. Although this discussion is included in the Appendix, it would be better if the authors could add the discussion of inference speed in the main text. This is just a limitation of diffusion models so I acknowledge the trade-off between quality and inference speed. Furthermore, I hope to know other metrics of NaturaSpeech 2 according to Diffusion Steps (WER or Similarity metric). Recently, Flow matching using optimal transport is utilized for fast speech synthesis. This could be adopted to this work. \n\n3. Some details are missing. Please see the questions."
                },
                "questions": {
                    "value": "1. This work utilizes a quantized latent vector for latent representation. In my experience, the quality of the model with the continuous latent representation before quantization showed a better performance in latent diffusion model for singing voice synthesis. Have you tried to train your model with the pre- or post-quantized representation for latent representation?\n\n2. The details of singing voice synthesis are missing. It would be better if you could add the details for pre-processing of musical scores. How do you extract the duration of phonemes in this work?\n\n3. How do you extract the pitch information? This significantly affects the performance so the details should be included. (about F0 min, F0 max, resolution, and pitch extraction algorithm).\n\n4. The authors may train the audio codec with their speech dataset. I think it is important to utilize a high-quality speech codec for high-quality speech synthesis. In this regard, I hope that the authors will mention about this part by comparing your model with the same model utilizing an official Soundstream codec as a latent representation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4886/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698649811393,
            "cdate": 1698649811393,
            "tmdate": 1699636473179,
            "mdate": 1699636473179,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wuwUxnT9Jr",
                "forum": "Rc7dAwVL3v",
                "replyto": "kTpU8lwTfD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4886/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4886/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8Wev"
                    },
                    "comment": {
                        "value": "First of all, we want to thank the reviewer for your careful reading and providing a lot of constructive comments! Below we address the concerns mentioned in the review. \n \n$\\textbf{Q1}$: It would be better if the authors could add the results according to the dataset and model size.\n\n$\\textbf{A1}$: This is a great suggestion. We conduct serial experiments by varying model sizes and data sizes. The results are shown as follows:\n| Model Size | Data Size | CMOS |\n| -------------- | ----- | ----- |\n| 400M | 44,000 h | 0.00 |\n| 400M | 10,000 h | -0.06 |\n| 400M | 1,000 h | -0.41 |\n| 60M | 1,000 h | -0.52 |\n\nWe have the following observations. \n* When we decrease the data size from 44k hours to 1k hours while keeping the model size unchanged, we can find the CMOS drops consistently. It indicates that the data size is important to generate high-quality speech. \n* When we further decrease the model size from 400M to 60M while keeping the data size unchanged, the CMOS drops 0.11, which indicates the model size is important as well.\n \n$\\textbf{Q2}$: Add discussion of inference speed in the main text. Furthermore, I hope to know other metrics of NaturaSpeech 2 according to Diffusion Steps (WER or Similarity metric).  And suggestions for flow matching.\n\n$\\textbf{A2}$: Thank you for your valuable suggestion.  Firstly, we incorporate this discussion in the revised version as suggested.\n\nSecondly, for additional metrics, we report SMOS and WER results in the table below. These results are in line with CMOS, further illustrating the trade-off between quality and inference speed.\n| Diffusion Step | RTF   | CMOS  | SMOS  | WER  |\n| -------------- | ----- | ----- | --------------- | ---- |\n| 150            | 0.366 | 0.00  | 4.06 | 2.01 |\n| 100            | 0.244 | -0.02 | 4.04 | 2.04 |\n| 50             | 0.124 | -0.08 | 3.98 | 2.09 |\n| 20             | 0.050 | -0.21 | 3.88 | 2.15 |\n\nFinally, as suggested, we would like to explore efficient strategies such as the consistency model[1] and flow matching model[2] to speed up the diffusion model.\n\n[1] Song, Yang, et al. Consistency models.\n \n[2] Lipman, Yaron, et al. Flow matching for generative modeling.\n\n$\\textbf{Q3}$: This work utilizes a quantized latent vector for latent representation. Have you tried to train your model with the pre- or post-quantized representation for latent representation?\n\n$\\textbf{A3}$: Thanks for your careful reading. In the preliminary experiment, we did not observe performance improvements when replacing the post-quantized representation with the pre-quantized representation. \n \nMoreover, using post-quantized representation is driven by engineering considerations. For example, rather than directly storing 256-dimensional FP32 (256 x 32 bits) latent representations, we only needed to store 16 INT16 quantized IDs (16 x 16 bits), leading to a 32x boost in storage efficiency.\n \n$\\textbf{Q4}$:  How do you extract the duration of phonemes in this work?\n\n$\\textbf{A4}$: Thanks for your query. We would like to clarify that our singing voice data comes in the form of recorded audio, not musical scores. We use the same alignment tool for extracting duration information from singing voice data. We have updated the paper and please refer to the revision in Appendix E. \n \n$\\textbf{Q5}$: How do you extract the pitch information?\n\n$\\textbf{A5}$: We appreciate your question. The pitch of each frame is extracted using the pyworld[1] with the dio + stonemask algorithm. The parameters are set to default.\n \n[1] https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder\n \n$\\textbf{Q6}$: The authors may train the audio codec with their speech dataset. I hope that the authors will mention about this part by comparing your model with the same model utilizing an official SoundStream codec as a latent representation.\n\n$\\textbf{A6}$: We appreciate your thorough understanding and insightful comments. Since we could not find the official pytorch implementation of SoundStream,  we attempted to reproduce it and the evaluation results are shown as follows (in the 12.8kbps setting):\n| Implementation | VISQOL | \n| --- | --- |\n| Ours                  |  4.38      |\n| SoundStream Paper | 4.26 | \n\nFrom the results, we can find that our implementation is competitive with the original performance."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700416825633,
                "cdate": 1700416825633,
                "tmdate": 1700417330924,
                "mdate": 1700417330924,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rvjS6bVcJW",
                "forum": "Rc7dAwVL3v",
                "replyto": "kTpU8lwTfD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4886/Reviewer_8Wev"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4886/Reviewer_8Wev"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for addressing my concern. I will keep my original score of 8. \n\nHowever, I have a question about the term of singing voice synthesis. I hope to confirm the definition of singing voice synthesis. The goal of singing voice synthesis is synthesizing the singing from musical note, not from the recorded singing voice isn't it right? If it is, the term of singing synthesis is overclaimed. The task in Section 4.4 is just singing voice conversion which is relatively easy task. The term may not be defined yet so I just hope to know your opinion."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700447344376,
                "cdate": 1700447344376,
                "tmdate": 1700447431276,
                "mdate": 1700447431276,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xiu7KFGrQn",
                "forum": "Rc7dAwVL3v",
                "replyto": "kTpU8lwTfD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4886/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4886/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8Wev"
                    },
                    "comment": {
                        "value": "Thanks for the question. We think there may be some misunderstanding. Firstly, we do not generate the singing from the recording, but from the lyric, ground truth duration, and ground truth pitch. Thus it is not singing voice conversion. Secondly, in practice, the ground truth pitch/duration can be determined by a separately-trained neural network conditioned on musical scores. Following [1,2], we use the ground truth pitch/duration during inference for simplification.\n \n[1] Liu, Jinglin, et al. Diffsinger: Singing voice synthesis via shallow diffusion mechanism.\n\n[2] Huang, Rongjie, et al. Make-A-Voice: Unified Voice Synthesis With Discrete Representation."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461249005,
                "cdate": 1700461249005,
                "tmdate": 1700461274138,
                "mdate": 1700461274138,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SCiuJgBdBX",
                "forum": "Rc7dAwVL3v",
                "replyto": "kTpU8lwTfD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4886/Reviewer_8Wev"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4886/Reviewer_8Wev"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for your response. \n\nI acknowledged that the authors utilized a duration and pitch to synthesize a singing voice, not from the recorded audio. However, they may be extracted from the recorded audio; therefore, they require the ground-truth singing voice to generate a singing voice. I know that this is a simple way to generate a singing voice by ground truth frame-level duration and pitch from the ground-truth audio, however what if I hope to generate the singing voice without ground-truth audio. In this regard, I stated it as a singing voice conversion which requires a ground-truth audio to extract a ground-truth duration and pitch as input representations. \n\nI just hope to discuss it. I'm not talking whether it is right or wrong! Feel free to response this :)"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462144745,
                "cdate": 1700462144745,
                "tmdate": 1700462179141,
                "mdate": 1700462179141,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DXcSVjUabo",
                "forum": "Rc7dAwVL3v",
                "replyto": "kTpU8lwTfD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4886/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4886/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to further express our sincere thanks for your insightful suggestion and engaging discussion! We truly value your time and efforts in the reviewing process."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665627770,
                "cdate": 1700665627770,
                "tmdate": 1700665627770,
                "mdate": 1700665627770,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kFgLdFRMxI",
            "forum": "Rc7dAwVL3v",
            "replyto": "Rc7dAwVL3v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4886/Reviewer_69Md"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4886/Reviewer_69Md"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new TTS model that is capable of generating speech with diverse speaker identities, prosody, and styles, in zero-shot scenarios and it can also sing. It outperforms the current SOTA methods in both objective and subjective metrics. The way it works is the following. First the neural audio codec that converts a speech waveform into a sequence of latent vectors with a codec encoder, and reconstructs the speech waveform from these latent vectors with a codec decoder. Then the codec encoder extracts the latent vectors from the speech and uses them as the target of the latent diffusion model which is conditioned on prior vectors. During inference it generates the latent vectors from text using the diffusion model and then generate the speech waveform using the codec decoder."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-Paper is very well written and provides good intuition and justification for all model choices that the authors have made. These choices are intuitive to make generated speech more natural and to overcome past bottlenecks in previous methods.\n-The new TTS algorithm has many capabilities such as generating diverse speech (different speakers, prosody, style) and in zero-shot scenarios. Singing is a bonus in this case.\n-NaturalSpeech2 beats current SOTA methods in both objective and subjective metrics.\n-Related work section is quite extensive.\n-In the end I believe that this work is a good contribution to the community."
                },
                "weaknesses": {
                    "value": "-One can hear in the more strenuous experiments that the audio samples have some kind of weird pitch or pace of speaking.\n-Paper might not be a very good fit in this venue. Although it has to do with learning representations, NaturalSpeech2 is more fit for a Speech venue such as InterSpeech or ICASSP."
                },
                "questions": {
                    "value": "-Why did the authors not include any experiments with single speaker data like LJSpeech.\n-It would be interesting to hear some audio samples with people that have an accent. This has not been explored in the community.\n-As an ablation what would be the shortest prompt in seconds that you can give for zero-speech synthesis?\n-After the phoneme Encoder you have a Duration and Pitch predictor. Why didn't you also include an Energy Predictor like the authors did in FastSpeech2 since the idea seems to be derived form there?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "They authors address this issue in the conclusions section. After all this is a speech synthesis work and it can be misused in the future. For this venue though I wouldn't want to see this work getting rejected because of this."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4886/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4886/Reviewer_69Md",
                        "ICLR.cc/2024/Conference/Submission4886/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4886/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698778307847,
            "cdate": 1698778307847,
            "tmdate": 1700420444203,
            "mdate": 1700420444203,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xvwikToAzA",
                "forum": "Rc7dAwVL3v",
                "replyto": "kFgLdFRMxI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4886/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4886/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 69Md"
                    },
                    "comment": {
                        "value": "First of all, we want to thank the reviewer for your careful reading and providing a lot of constructive comments! Below we address the concerns mentioned in the review. \n \n$\\textbf{Q1}$: Paper might not be a very good fit in this venue. Although it has to do with learning representations, NaturalSpeech2 is more fit for a Speech venue such as InterSpeech or ICASSP.\n\n$\\textbf{A1}$: We believe that our paper is fitting for ICLR.\n\nFirstly, we would like to highlight that NaturalSpeech 2 mitigates the representation issues inherent in large-scale TTS systems. Specifically, previous works utilize multiple discrete token sequences as speech representation, which introduces a dilemma between the codec and the acoustic model (please refer to Sec. 1 for more details). To this end, we employ a single continuous sequence as representation, which helps improve the performance.\n \nSecondly, we employ a new learning paradigm, which involves the use of latent diffusion and continuous representation, to effectively address the issue in the field of speech representation learning in large-scale TTS. This research aligns with the core interests of ICLR, especially in the primary area of 'representation learning for computer vision, audio, language, and other modalities.' \n \n$\\textbf{Q2}$: Why did the authors not include any experiments with single-speaker data like LJSpeech?\n\n$\\textbf{A2}$: Thank you for your question. We would like to highlight that our work focuses on diverse speech synthesis in terms of speaker identity, prosody, and style, especially the zero-shot text-to-speech synthesis (i.e., on unseen speakers).\n\nFor single-speaker datasets such as LJSpeech, firstly, they lack speech diversity since there is only one speaker. Thus, it is not aligned with our focus. Secondly, some previous works, such as [1], have already achieved human-level voice quality. This essentially narrows the scope for further enhancements using this dataset.\n\n[1] Tan, Xu, et al. \"Naturalspeech: End-to-end text to speech synthesis with human-level quality.\"\n \n$\\textbf{Q3}$: As an ablation what would be the shortest prompt in seconds that you can give for zero-speech synthesis? \n\n$\\textbf{A3}$:  This is a great suggestion to investigate the shortest prompt length. We vary the length of the prompt and report the corresponding SMOS on the LibriSpeech test set. The table below shows that even when reducing the prompt length from 3.0 seconds to 1.0 seconds, NaturalSpeech 2 is still capable of generating high-quality speech. It further demonstrates the robustness and stability of our system despite the reduction in prompt length.\n| Prompt Length| SMOS|\n| ------- | ---- | \n| 3.0 s  |  4.06  |\n| 2.0 s  |  3.98  |\n| 1.0 s  |  3.79  | \n \n$\\textbf{Q4}$: Why didn't you also include an Energy Predictor like the authors did in FastSpeech2 since the idea seems to be derived from there? \n\n$\\textbf{A4}$: Thank you for your insightful question. The energy predictor is indeed a versatile module. In practice, we have added the energy predictor and did not observe any improvement. For the sake of simplicity in our current model, we let the diffusion model implicitly predict energy information. However, we acknowledge that incorporating an energy predictor could further enhance controllability. We value this suggestion and will take it into consideration for our future research.\n \n$\\textbf{Q5}$: Concerns about the potential misuse in the future.\n \n$\\textbf{A5}$: Indeed, to protect users from misuse, we will implement a multi-faceted approach, including:\n* Usage Agreement: We will require users to agree not to misuse the technology, which includes not creating synthetic speech without approval.\n* Transparency Measures: We will ensure that any synthetic speech should clearly disclose that the speech is machine-generated to avoid misleading uses.\n* Auditing and Monitoring: There should be regular reviews to detect misuse and taking action against violators.\n* Reporting Mechanism:  We will establish a system for individuals to report any suspected misuse of the technology."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700416104265,
                "cdate": 1700416104265,
                "tmdate": 1700416104265,
                "mdate": 1700416104265,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7Lkur5Klb4",
                "forum": "Rc7dAwVL3v",
                "replyto": "xvwikToAzA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4886/Reviewer_69Md"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4886/Reviewer_69Md"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you authors for your details answer. I was in favor of accepting the paper initially, and now I am increasing my score since all of my comments have been addressed."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700420427371,
                "cdate": 1700420427371,
                "tmdate": 1700420427371,
                "mdate": 1700420427371,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RoNRaajMH1",
            "forum": "Rc7dAwVL3v",
            "replyto": "Rc7dAwVL3v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4886/Reviewer_9oT1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4886/Reviewer_9oT1"
            ],
            "content": {
                "summary": {
                    "value": "This paper describes a TTS model combining a number of modern components these include in-context learning (prompting) a diffusion model to connect conditioning information to latents, and latents defined by an autoencoder for waveform reconstruction.\n\nThe resulting model has many of the zero-shot capabilities of LM based TTS that have been presented in recent years, but by maintaining duration prediction for alignment, the model stays robust to a hallucination and dropping errors that impact other generative models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The model contains innovative structures in the in context learning for duration and pitch, and in the diffusion model.  Moreover the overall structuring of these components is novel.\n\nThe quality of the model is quite high and provides some important balancing between zero-shot capabilities and robustness compared to alternate models"
                },
                "weaknesses": {
                    "value": "The paper is sometimes unclear with regards to what the model components represent and how the components fit together.  For example, the use of SoundStream and wavenet is not obvious.  These are previously published approaches, that are used in novel ways here.  It took multiple readings to understand how they are being used in this paper, and even still i\u2019m not 100% sure that my understanding is correct.  Broadly, the paper relies too heavily on Figure 1.0 to describe how the model fits together. \n\nThe argumentation around continuous vs discrete tokens is very hard to follow.  It\u2019s not clear why the discrete token sequence must necessarily be longer than a continuous sequence (Introduction).  The first three pages spend a lot of effort describing why a continuous representation is a better fit for this task.  Then in Section 3.1 \u201cHowever, for regularization and efficiency purposes we use residual vector quantizers with a very large number of quantizers and codebook tokens to approximate the continuous vectors.  This provides two benefits\u2026\u201d This is a particularly surprising turn of the argument to then go on to describe why discrete tokens are useful here.\n\nThe diffusion formulation is too compact to be clearly followed.  Page 5. The following sentence includes a number of ambiguities.  \u201cThen we calculate the L2 distance between the residual vector with each codebook embedding in quantizer j and get a probability distribution with a softmax function, and then calculate the cross-entropy loss between the ID of the ground-truth quantized embedding ej and this probability distribution. Lce\u2212rvq is the mean of the cross-entropy loss in all R residual quantizers, and \u03bbce\u2212rvq is set to 0.1\u201d  I\u2019d recommend including an appendix entry or describing each clause separately in place."
                },
                "questions": {
                    "value": "Introduction \u201cthe zero-shot capability that is important to achieve diverse speech synthesis\u201d \u2013 why is zero-shot necessary for diverse speech synthesis?  Also, for what contexts, and use-cases is diverse speech synthesis necessary?\n\nIn the introduction \u2013 the checklist between NaturalSpeech 2 and \u201cprevious systems\u201d is somewhat strange.  Certainly there are previous systems that are non-autoregressive, or use discrete tokens.  I understand that this is not \u201call previous systems\u201d but those listed. But why compare only to those three related systems? The introduction and related work draw contrast with a variety of alternate TTS models.\n\nWhy use a diffusion model instead of any other NAR model?\n\nWhen presenting the \u201cprior model\u201d in section 3.2 is the phone encoder, duration predictor and pitch predictor pre-trained to some other target? or is there some other notion of a prior model here?\n\nWhat is the units used in the L_pitch loss? Hz? log Hz? something else?\n\nThe variable z is used in a number of different ways, could this be clarified (e.g. in Figure 2 between the prompt, input to diffusion model and output?)\n\nSection 4.1\nPage 6 \u201cBoth speakers in the two datasets\u201d are there only 2 speakers in the data sets?\nPage 6 what is value of sigma in the sigma-second audio segment as a prompt?\n\nHow much loss is incurred by filtering the output by a speech scoring model?  E.g.  are 99% of utterances accepted? or 1%?  \n\nNote: VCTK utterances are particularly noisy making is a poor comparison for CMOS, but the comparison to Librispeech is more representative.\n\nSection 4.2 \u201cWe apply the alignment tool\u201d \u2013 which alignment tool?\n\nWhat is the variance of the prosodic measures \u2013 it\u2019s hard to track whether the differences in Table 3 are significant or not.\n\n\u201cWhen we disable the speech prompt in diffusion, the model cannot converge\u201d \u2013 this seems remarkable.  Why does the model require a speech prompt to learn?\n\nBroader Impacts: What would such a protocol to protect users from misuse of this model look like? Presumably this model can generalize to unseen speakers already \u2013 so what protections are in place regarding the use of this model as of publication?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Zero-shot synthesizers have a strong potential for misuse."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4886/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786838685,
            "cdate": 1698786838685,
            "tmdate": 1699636472982,
            "mdate": 1699636472982,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E4j5eI59tx",
                "forum": "Rc7dAwVL3v",
                "replyto": "RoNRaajMH1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4886/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4886/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9oT1 [1/3]"
                    },
                    "comment": {
                        "value": "First of all, we want to thank the reviewer for your careful reading and providing a lot of constructive comments! Below we address the concerns mentioned in the review. \n \n$\\textbf{Q1}$: The paper is sometimes unclear with regard to what the model components represent and how the components fit together.\n$\\textbf{A1}$: Thanks for your careful reading. We have updated the paper and highlighted the updates as follows.\n\nFor neural audio codec, firstly, we adopt the SoundStream architecture because: 1) it provides good reconstruction quality and low bitrate, 2) it can convert waveforms into continuous vectors while retaining fine-grained details, and 3) based on the quantized token IDs, we can design the regularization loss, which further enhances the prediction precision. Secondly, we adapt the SoundStream by 1) we use the post-quantized latent $z$, i.e., the output from the RVQ, as the representation for waveform $x$, and as the training target for the diffusion model, 2) we develop the regularization loss $L_{ce-rvq}$.\n\nWe adopt WaveNet as the architecture of the diffusion model. Firstly, in our preliminary experiments, we compare various architectures including the vanilla transformer, conformer, UNet and WaveNet. Among these, WaveNet exhibits superior performance in terms of audio quality (CMOS), hence our choice for the diffusion model's architecture. Secondly, to facilitate in-context learning, we incorporate a Q-K-V attention and a FiLM layer in the block. Please refer to Appendix B for more details.\n \nRegarding the interrelation between the components, our system is composed of three components: a prior model, a diffusion model, and an audio codec.  Similar to [1], the prior model (a phoneme encoder and a duration/pitch predictor) encodes the text $y$ into a prior $c$. Then, the diffusion model predicts the speech representation $z$ using prior $c$ as a condition. Finally, the speech representation $z$ is input into the decoder of the audio codec to generate the speech signal $x$.\n \n[1]Ren, Yi, et al. Fastspeech 2: Fast and high-quality end-to-end text to speech.\n\n$\\textbf{Q2}$: As shown in Introduction, it\u2019s not clear why the discrete token sequence must necessarily be longer than a continuous sequence.\n\n$\\textbf{A2}$: Firstly, we would like to clarify that our paper doesn't assert that the discrete token sequences must necessarily be longer than a continuous sequence. What we claim is that previous works often use multiple RVQ discrete token sequences for speech representation. When these sequences are flattened, it results in a multiple-fold increase in length.\n\nSecondly, it's true that some attempts have been made to address this issue, but they still have their own problems. \nAudioLM[1] predicts the first two RVQ token sequences in the first stage, and predicts the remaining RVQ token sequences in the second stage. However, this approach has two drawbacks: 1) it requires two separate AM models and a two-stage generation process; 2) it still needs the flattening of the code sequences, which also extends the sequence length. VALL-E [2] first predicts the first RVQ token sequence in an AR manner, and then predicts the other RVQ token sequences in a NAR manner, which makes the modeling much more complex compared with one-stage generation. Moreover, MusicGen[3] claims that such parallel modeling may impact system performance.\n\nIn contrast, NaturalSpeech 2 uses a single continuous latent instead of multiple discrete tokens for each speech frame. It can prevent the increase in sequence length and support one-stage generation, which helps enhance overall performance.\n \n[1] Borsos, Zal\u00e1n, et al. Audiolm: a language modeling approach to audio generation.\n\n[2] Wang Chengyi, et al. Neural codec language models are zero-shot text to speech synthesizers.\n\n[3] Copet, Jade, et al. Simple and Controllable Music Generation."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700414092850,
                "cdate": 1700414092850,
                "tmdate": 1700415595363,
                "mdate": 1700415595363,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tZWw3C2gwC",
                "forum": "Rc7dAwVL3v",
                "replyto": "RoNRaajMH1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4886/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4886/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9oT1 [2/3]"
                    },
                    "comment": {
                        "value": "$\\textbf{Q3}$: The motivation of adopting RVQ for continuous representation. \n\n$\\textbf{A3}$: Thanks for the question.  Firstly, we would like to highlight that we claim predicting the continuous representation instead of the discrete tokens is beneficial to the high-quality TTS system, while the quantization of audio codec can be regarded as a regularization for the continuous space.\n\nSecondly, theoretically, the use of an infinite number of quantizers can approximate a continuous space. In our paper, we adopt more codebooks than [1,2] to achieve a better approximation (please refer to Appendix C). \n\nFinally, the quantization audio codec can bring the following benefits. 1) Data storage efficiency. By using RVQ codec, we do not need to directly store the latent representations, but store the code ID which can recover the latent by looking up the codebook during training. For example, instead of storing 256-dimensional FP32 (256 x 32 bits) latent representations directly, we only need to store 16-dimensional INT16 quantized IDs (16 x 16 bits), resulting in a $32\\times$ increase in storage efficiency. 2) Based on the quantized IDs, we designed a regularization loss, $L_{ce-rvq}$, which further enhances the prediction precision. \n \n[1]Borsos Zal\u00e1n, et al. Audiolm: a language modeling approach to audio generation.\n\n[2]Wang Chengyi, et al. Neural codec language models are zero-shot text to speech synthesizers.\n\n$\\textbf{Q4}$: The description of CE-RVQ loss is not clear.\n\n$\\textbf{A4}$: Thanks for your advice. We have provided more details in Appendix C.3 in the revised version. We also provide the detailed formulation of CE-RVQ loss as follows:\n \nMoreover, to clarify, for each residual quantizer $j \\in [1, R]$, we first get the residual vector: \n\\begin{equation} \nz_j = z_0 - \\sum^{j-1}_{m=1}\\hat{e}^{m},\n\\end{equation}\n \nwhere $\\hat{e}^m$ is the ground-truth quantized embedding in the $m$-th residual quantizer. Then we calculate the L2 distance between the residual vector with each codebook embedding in quantizer $j$ and get a probability distribution as follows: \n\\begin{equation} \nl_i = ||z_j - e^j_i||_{2}, \n\\end{equation}\n\n\\begin{equation} \ns_i = \\frac{e^{-l_i}}{\\sum^{N_j}_{k=1}{e^{-l_k}}},\n\\end{equation}\n\nwhere $N_j$ is the code number of residual quantizer $j$, and $s_i$ is the probability of code $i$ in codebook $j$. Finally, we can calculate the cross-entropy loss of residual quantizer $j$ given the ground-truth code index which is denoted as $L_{ce, j}$. The final CE-RVQ loss is shown as follows: \\begin{equation}\nL_{ce-rvq} = \\sum_{j=1}^{R}{L_{ce, j}} \n\\end{equation}\n\n$\\textbf{Q5}$: Why is zero-shot necessary for diverse speech synthesis?  For what contexts, and use-cases is diverse speech synthesis necessary?\n \n$\\textbf{A5}$:  Thanks for your careful reading.\n\nFirstly, diverse speech synthesis aims to generate natural and human-like speech with rich diversity in speaker identity (e.g., gender, accent, timbre), prosody, style, et.al. However, it is difficult and expensive to collect all kinds of speeches to capture such diversity. And for zero-shot synthesis, it is valuable for generating speech with a variety of elements like timbre, prosody, and emotion, without the need for explicit training for these attributes, which significantly eases the difficulty of data collection and broadens the potential diversity in speech synthesis.\n \nSecondly, diverse speech synthesis is necessary in various contexts and use-cases.  For instance,\n1) Voice Assistants: To make interactions more personalized and engaging, voice assistants need to be able to generate a variety of voices.\n\n2) Entertainment: In industries like animation or gaming, diverse speech synthesis can be used to create a wide array of unique character voices.\n\n3) Language Learning: Diverse speech synthesis can help language learners by providing a variety of accents and pronunciations to learn from.\n\n$\\textbf{Q6}$: The checklist between NaturalSpeech 2 and \u201cprevious system\u201d is not clear. This is not \u201call previous systems\u201d are included.\n\n$\\textbf{A6}$:  We would like to clarify that, as highlighted in the caption, the comparison in the checklist is specifically made with previous $\\textit{large-scale TTS systems}$, rather than all models.  We have noted the potential for misunderstanding and have revised the checklist accordingly."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415057524,
                "cdate": 1700415057524,
                "tmdate": 1700420075045,
                "mdate": 1700420075045,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r314LmgCOk",
                "forum": "Rc7dAwVL3v",
                "replyto": "RoNRaajMH1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4886/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4886/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9oT1 [3/3]"
                    },
                    "comment": {
                        "value": "$\\textbf{Q7}$: Why use a diffusion model instead of any other NAR model?\n \n$\\textbf{A7}$: Compared to NAR methods, the diffusion model stands out as it incorporates multiple iterations in inference. This allows for a progressive refinement over multiple iterations, which helps enhance the modeling capability, and makes it particularly effective high-quality generation.\n \nTo demonstrate this effectiveness, we employ FastSpeech2[1] as a NAR baseline in our paper (in Sec. 4.2). We adapt it by adding cross-attention on speech prompts for zero-shot synthesis. Furthermore, we also change the prediction target from the mel-spectrogram to the latent representation. For a fair comparison, we scale it to 400M parameters and trained it on the same MLS dataset. As shown in Table 2, the significant performance gap indicates limitations in its modeling capability.\n \n[1] Ren, Yi, et al. Fastspeech 2: Fast and high-quality end-to-end text to speech.\n\n$\\textbf{Q8}$: When presenting the \u201cprior model\u201d in Sec. 3.2 is the phone encoder, duration predictor and pitch predictor pre-trained to some other target? or is there some other notion of a prior model here?\n\n$\\textbf{A8}$: We refer to the phoneme encoder and duration/pitch predictor collectively as the \"prior model\". As mentioned in equation (6), the prior model is trained jointly.\n \n$\\textbf{Q9}$: What is the units used in the L_{pitch} loss? Hz? log Hz? something else?\n\n$\\textbf{A9}$: Thanks for your careful reading. We normalize the pitch to zero mean and unit variance.\n \n$\\textbf{Q10}$: The variable z is used in a number of different ways, could this be clarified (e.g. in Figure 2 between the prompt, input to diffusion model and output?)\n\n$\\textbf{A10}$: Thanks for your careful reading. We would like to clarify that the notation 'z' is used throughout the article to consistently represent the latent audio representation encoded by the codec. \n \n$\\textbf{Q11}$: Sec. 4.1 Page 6 \u201cBoth speakers in the two datasets\u201d are there only 2 speakers in the data sets? \n\n$\\textbf{A11}$: We sincerely apologize for the confusion. The phrasing was a typo and should be \"All speakers in the two datasets\", not \"Both speakers in the two datasets.\"  The LibriSpeech test-clean dataset indeed includes 40 distinct speakers, and the VCTK dataset includes 108. We have corrected this typo in the revised version. Thank you for pointing it out.\n \n$\\textbf{Q12}$: Page 6 what is value of sigma in the sigma-second audio segment as a prompt?\n\n$\\textbf{A12}$: Thanks for your careful reading. As mentioned in Sec. 4.2 'Generation Similarity', we set $\\sigma$ to $3$ in Sec.4.\n\n$\\textbf{Q13}$: How much loss is incurred by filtering the output by a speech scoring model?\n\n$\\textbf{A13}$: There may be some misunderstanding about the \u201closs incurred by filtering\u201d. We use a speech scoring model for reranking. We sample 10 candidates and select the best one.\n \n$\\textbf{Q14}$: Sec. 4.2 \u201cWe apply the alignment tool\u201d \u2013 which alignment tool?\n\n$\\textbf{A14}$: We use the internal alignment tool, which is similar to Montreal Forced Aligner[1].\n \n[1] McAuliffe, Michael et al. Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi.\n \n$\\textbf{Q15}$: What is the variance of the prosodic measures?\n\n$\\textbf{A15}$: We appreciate your insightful question. We provide the measures with a 95% confidence interval. For pitch, the measures (Mean, Std, Skew, Kurt) are $10.11_{\\pm 0.08}, 6.18_{\\pm 0.06}, 0.50_{\\pm0.01},1.01_{\\pm 0.03}$, respectively. For duration, the measures are $0.65_{\\pm 0.01}, 0.70_{\\pm 0.01}, 0.60_{\\pm 0.01}, 2.99_{\\pm 0.07}$, respectively. These results demonstrate that the improvements in Table 3 are statistically significant.\n \n$\\textbf{Q16}$: \u201cWhen we disable the speech prompt in diffusion, the model cannot converge\u201d \u2013 this seems remarkable. Why does the model require a speech prompt to learn?\n\n$\\textbf{A16}$: In practice, we find that the absence of a speech prompt, which supplies the speaker identity, may cause instability during the training phase. This may be attributed to the massive diversity of speech data given the large number of speakers in the training dataset.\n\n$\\textbf{Q17}$: Broader Impacts: What would such a protocol to protect users from misuse of this model look like?\n \n$\\textbf{A17}$: Thanks for your question. Indeed, to protect users from misuse, we will implement a multi-faceted approach, including:\n\n* Usage Agreement: We will require users to agree not to misuse the technology, which includes not creating synthetic speech without approval.\n\n* Transparency Measures: We will ensure that any synthetic speech should clearly disclose that the speech is machine-generated to avoid misleading uses.\n\n* Auditing and Monitoring: There should be regular reviews to detect misuse and taking action against violators.\n\n* Reporting Mechanism:  We will establish a system for individuals to report any suspected misuse of the technology."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415566498,
                "cdate": 1700415566498,
                "tmdate": 1700416168208,
                "mdate": 1700416168208,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6CZnATA9AV",
                "forum": "Rc7dAwVL3v",
                "replyto": "RoNRaajMH1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4886/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4886/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate your comprehensive feedback. As the discussion period nears its end, we would like to know if there are any additional questions. We are glad to answer them."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665670857,
                "cdate": 1700665670857,
                "tmdate": 1700701302921,
                "mdate": 1700701302921,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5TQYisWqJ0",
            "forum": "Rc7dAwVL3v",
            "replyto": "Rc7dAwVL3v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4886/Reviewer_c9Qd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4886/Reviewer_c9Qd"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents NaturalSpeech 2, a non-autoregressive TTS model that employs a diffusion mechanism to generate quantized latent vectors from neural audio codecs. It shows enhanced zero-shot TTS performance relative to the state-of-the-art large-scale neural codec language model. The proposed approach exhibits advancements in sample quality, intelligibility, robustness, speaker similarity, and generation speed when benchmarked against the baseline method. The authors further validate the superiority of their method over other alternatives via comprehensive qualitative and quantitative evaluations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper effectively tackles several major challenges inherent to non-autoregressive TTS modeling at scale. \n* The authors have carried out robust and wide-ranging experiments, yielding detailed results.\n* The reference list is both extensive and comprehensive."
                },
                "weaknesses": {
                    "value": "The proposed method's intricate modeling could hinder its extension to other applications. While the introduced model applies diffusion, it necessitates two additional losses and requires supplementary modules like a pitch predictor, prompt encoder and the second attention block. As an example, the recent state-of-the-art flow-matching based TTS method, VoiceBox [1] consists of rather simple model architecture; the flow-matching based duration predictor and audio model.\n\n[1] Le, Matthew, et al. \"Voicebox: Text-guided multilingual universal speech generation at scale.\" arXiv preprint arXiv:2306.15687 (2023)."
                },
                "questions": {
                    "value": "Given the concerns mentioned in the above weaknesses, it would be interesting to see if the proposed method could be adapted or refined to reduce its dependency on additional modules without increasing complexity or compromising sample quality."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4886/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4886/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4886/Reviewer_c9Qd"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4886/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698843124002,
            "cdate": 1698843124002,
            "tmdate": 1699636472909,
            "mdate": 1699636472909,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FGFlQI2csq",
                "forum": "Rc7dAwVL3v",
                "replyto": "5TQYisWqJ0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4886/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4886/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer c9Qd"
                    },
                    "comment": {
                        "value": "First of all, we want to thank the reviewer for your careful reading and providing a lot of constructive comments! Below we address the concerns mentioned in the review. \n \n$\\textbf{Q1}$: The proposed method necessitates two additional losses and requires supplementary modules like a pitch predictor, prompt encoder and the second attention block.  It would be interesting to see if the proposed method could be adapted or refined to reduce its dependency on additional modules.\n\n$\\textbf{A1}$: Thanks for your careful reading.\n1) For the prompt encoder, it is adopted to encode the prompt speech for high-quality zero-shot text-to-speech synthesis, which is also widely used in previous works, such as [1,2,3].\n \n2) For the second attention block, it is crucial to NaturalSpeech 2 for high-quality synthesis. Intuitively, the prompt needs to provide the model with aspects like timbre, prosody, energy, background, et.al. Thus, we adopt $m$ query tokens in the second attention block to model these important aspects. With experiments in Sec. 4.3 (Ablation Study\u2019s w/o. query attn), we can find this component is beneficial to the audio generation quality.\n \n3) For the CE-RVQ loss, it can significantly enhance the performance while incurring low computational cost. Intuitively, it is a regularization constraint inspired by the hierarchical structure of the audio codec. To ensure high-quality generation, we constrain the latent distribution and propose the CE Loss by matching the predicted $z_0$ with each quantizer. With experiments in Sec. 4.3 (Ablation Study\u2019s w/o. CE loss), we can find that both the CMOS and WER will degrade by removing it, which demonstrates the effectiveness of CE-RVQ loss.\n \n4) For the pitch predictor, it is a flexible module in NaturalSpeech 2. We have conducted the ablation experiment by removing the pitch predictor and observed no noticeable decline in performance. However, it significantly enhances the controllability of pitch, which is necessary for singing voice synthesis (please refer to Sec. 4.4 for more information). Thus, we still adopt the pitch predictor in NaturalSpeech 2.\n \n[1] Arik, Sercan, et al. Neural voice cloning with a few samples.\n\n[2] Jia, Ye, et al. Transfer learning from speaker verification to multispeaker text-to-speech synthesis.\n\n[3] Jiang, Ziyue, et al. Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413266766,
                "cdate": 1700413266766,
                "tmdate": 1700413266766,
                "mdate": 1700413266766,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JPUa2Iwedz",
                "forum": "Rc7dAwVL3v",
                "replyto": "FGFlQI2csq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4886/Reviewer_c9Qd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4886/Reviewer_c9Qd"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "I am grateful for the detailed response from the authors. They have provided rationales explaining the necessity of their proposed method's modeling. While I understand from their explanation and experimental results that the application of various modules significantly enhances performance, it appears to lean more towards engineering efforts. To strengthen the scientific contribution of this research, it could have been further validated by, for example, adding experimental results and analyses to demonstrate the effectiveness of these modules in applications beyond Text-to-Speech (TTS), such as in singing voice synthesis and various other tasks."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658438820,
                "cdate": 1700658438820,
                "tmdate": 1700658438820,
                "mdate": 1700658438820,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]