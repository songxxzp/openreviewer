[
    {
        "title": "Multi-Scale Representations by Varing Window Attention for Semantic Segmentation"
    },
    {
        "review": {
            "id": "3YVRoHucFO",
            "forum": "lAhWGOkpSR",
            "replyto": "lAhWGOkpSR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission104/Reviewer_L266"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission104/Reviewer_L266"
            ],
            "content": {
                "summary": {
                    "value": "The paper delves into the topic of multi-scale representations and identifies two primary challenges: \"scale inadequacy\" and \"field inactivation.\" The authors introduce the LWA model and, based on this, design the VWFormer as a decoder to enhance multi-scale representation capabilities. Through the effective LWA module, VWFormer outperforms most compute-friendly multi-scale decoders, such as FPN and MLP decoders."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Extensive experiments are conducted on Cityscapes, ADE20K, and COCOStuff-164K datasets. Despite reducing computational demands and the number of parameters, the proposed method demonstrates relative improvements compared to other approaches. \n\nThis work provides a thorough mathematical derivation of its method and showcases how to address the challenges of scale inadequacy and field inactivation.\n\nThis work presents a lot of ablation experiments to verify that the proposed methods outperform existing mechanisms.\n\nThe visualization results are interesting and provide many insights into multi-scale feature representations for semantic segmentation."
                },
                "weaknesses": {
                    "value": "1. The manuscript requires structural refinement. Although the paper proposes a new methodology, the empirical validation predominantly resides in the appendix. The core content appears to be overly enmeshed in granular theoretical derivations, leading to a somewhat convoluted narrative structure.\n2. The delineation between the figures, tables, and the main text is ambiguous. It is advisable to render the captions in a font slightly smaller than the primary text to enhance clarity. The layout of the illustrations also warrants improvement. For instance, in Figure 2, it would be beneficial to incorporate descriptions of specific letters either within the caption or designated sections of the image. The term \"PE\" is presented here, yet a precise elucidation is deferred until Section 3.3.\n3. The visualization results do not substantiate the issues you've raised. The quantified outcomes displayed in Figure 5 merely illustrate the discriminative results of the proposed network in comparison to Segformer for a specific image region. If the method you presented exhibits a substantial improvement over Segformer, I believe it would be acceptable. However, given that the data only indicates approximately a 1% enhancement relative to Segformer. There is a lack of targeted visualization results addressing the issues of scale inadequacy and field inactivation.\n4. The author introduced \"varying window attention,\" but neglected a comparative analysis with other similar methods, such as the 'Shifted Window' in the Swin-Transformer.\n5. Some state-of-the-art segmentation methods like HRViT, SegViT-v2, ViT-adapter, Lawin Transformer, etc. could be compared in the experiments to better verify the superiority of your proposed architecture.\n\nSincerely,"
                },
                "questions": {
                    "value": "1. Why not experiment with the same Encoder paired with different decoders (more than three)? This can help better show the consistent effectiveness of your approach. \n2. The computational load shown in Equation (2) appears highly reminiscent of the Swin Transformer. Could you possibly incorporate a comparison with the Swin Transformer?\n3. In Figure (3), you've employed an operation akin to that of Swin. However, after image concatenation in Swin, a mask is applied. As you concatenate features from different regions, how do you prevent interference or cross-effects between these diverse area features?\n4. SegFormer utilizes an MLP Decoder, which doesn't encompass an attention mechanism. Yet, in your VWA mechanism, you've incorporated an attention mechanism. How does this lead to a reduction in parameter count? This can be better analyzed.\n5. In the experimental section, could the results be organized based on the size of the modules in a systematic order?\n\nSincerely,"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission104/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission104/Reviewer_L266",
                        "ICLR.cc/2024/Conference/Submission104/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission104/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697681298948,
            "cdate": 1697681298948,
            "tmdate": 1700110308716,
            "mdate": 1700110308716,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fJVHeciv4I",
                "forum": "lAhWGOkpSR",
                "replyto": "3YVRoHucFO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments! Any new questions please let me know!"
                    },
                    "comment": {
                        "value": "### **For Weaknesses 1&2**: \n----\nThanks for the advice and careful review. We will refine the structure, delineation, and core content. We will check all captions, e.g. explaining PE in Fig. 2(b), and adjust the font size in all Figures to make readers clearly understand each of them. But with openness, I think it should be noted that two reviewers of all 4 think the presentation is good, one reviewer of all 4 thinks the presentation is fair, and two reviewers of all 4 think it is easy to follow. \n\n### **For Questions 1&5**: \n----\n> Experiment with the same Encoder paired with more than three decoders to show superiority consistently\n\n> Could the results be organized in an ascending order of module size ?\n\nGood idea to show our consistent superiority. The backbone choice depends on what the decoder's official paper used or what most papers paired it with. And the order in our paper is based on the performance of compared decoders.  Below is the result obtained by the same Encoder--Swin Transformer--paired with FPN, MLP-decoder, UPerNet, and our VW, respectively on ADE20K, and the order of other decoders is module-size ascending.\n\n| mIoU | Swin-Ti | Swin-S | Swin-B | Swin-L | \n| :----: | :----: | :----: | :----: | :----: | \n| FPN | 44.7 |  48.0 | 51.7 | 53.3 | \n| MLP | 46.2 | 48.4 | 52.5 | 53.9 | \n| Uper | 45.8 | 49.2 | 52.4 | 54.1 | \n| VW. | 46.9 | 50.4 | 53.6 | 55.4 | \n\nLikewise, we will rearrange the order of the compared modules in the paper according to their sizes.\n\n### **For Weakness 3 and Question 4**: \n----\nYour concern involves that Figure 5 can not substantiate the issues we have raised, i.e. scale inadequacy and field inactivation, and the improvement of about 1% is not substantial over segformer to show the effectiveness of addressing these issues. We will answer this concern from TWO perspectives.\n\n**(1)** I think visualization results showing the issues are not in Figure 5 but in Figure 1. In the beginning, the paper used EFR visualization to show the issue. It can be seen from Fig. 1(e) that the multi-scale feature of SegFormer only has the local and global scales(i.e. scale inadequacy) and some areas within the receptive field have almost zero values(i.e. field inactivation), as analyzed in Appendix A. The visualization of Figure 1 is averaged on many ADE20k val images, so I guess it might be too abstract to substantiate the proposed issue. THEREFORE, we sincerely request you to take a look at this [ANONYMOUS LINK](https://drive.google.com/file/d/1fKaeJBH8fFKDA4VM9X-ds4GTttfUdJ_E/view?usp=sharing) in which the specific ADE20K val image with EFRs of segformer and VWFormer are analyzed contrastively. We hope this new visualization will help to understand the receptive issue of SegFormer and show the strengths of VWFormer's multi-scale learning.\n\n**(2)** For comparison of VWFormer to segformer, meanIoU is not the only metric to be considered. From Table 1, you can see that besides meanIoU Table 1 reported params, flops, and memory usage. Why do we think the overall metrics comparison is meaningful? First, An improvement with no more computational budgets is a PARETO improvement, and especially considering that our motivation is to improve multi-scale representations of classical methods, the PARETO optimum is more favorable. Second, VWFormer is architecturally similar to adding VWA to the MLP-decoder so you naturally propose the **Question 4**. The answer can be very simple. Towards PARETO, We weakened the powerful multi-layer aggregation (MLA) module of VWFormer by reducing its original dimensionality (this is one step. Also along with LLE introduced in Sec.4.3 and a more efficient setting of VWFormer introduced in Appendix F) so our VW. can even be much more efficient than Seg. according to Table 1. At the beginning of this research, we ran VWFormer by keeping the original channel setting of MLA (768 or 2048), the improvements of VW. over Seg. become around 1.5% to 2.3%. \n|mIoU on ADE20K | B0 | B1 | B2 | B3 | B4 | B5 |\n| :- | :-: | :-: | :-: | :-: | :-: | :-: | \n|Seg. (original setting, shown in **Table 1**) | 38.0 | 43.1 | 47.5 | 50.0 | 51.1 | 51.8|\n|VW. (keeping original setting, **NOT** shown in Table 1) | 40.3 | 44.8 | 49.5 | 51.6 | 52.6 | 53.5|\n|VW. (with channel reduction and LLE, shown in **Table 1**) | 39.6 | 44.0 | 49.2 | 50.9 |  51.6 | 52.7|\n\n**To be continued.**"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068698786,
                "cdate": 1700068698786,
                "tmdate": 1700068698786,
                "mdate": 1700068698786,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gU3hH15edD",
                "forum": "lAhWGOkpSR",
                "replyto": "3YVRoHucFO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments! Any new questions please let me know! (Cont.)"
                    },
                    "comment": {
                        "value": "### **For Weakness 4 and Questions 2&3**: \n----\n> neglected comparative analysis to similar methods, e.g. Swin.\n\nThis paper just begins by comparing similar methods including the Swin. For Swin, the analysis reveals that the 'Shifted-Window' mechanism directly incurs the field inactivation issue (please see **Introduction, Figure 1(d)** showing the shift-pattern of Swin's receptive field, and its captions cited **Appendix A**). \n\n> an operation akin to that of Swin\n\nAfter reading your questions 2&3, I believe a clearer distinction between VWA and Swin attention may be helpful : ) **a**. Swin is of Self-attention. VWA is of Cross-attention. **b**. Swin shifts every window's location and does nothing about its size. VWA fixes every window's location and varies (enlarges) its context window's size (so VWA needs no mask). **c**. If Swin increases the receptive field for multi-scale learning, it can only stack itself upon itself and finally will meet the field inactivation issue. But if VWA increases the receptive field, it simply varies the context window's size, which should be computationally expensive but its incorporated DOPE and pre-scaling will handle this. \n\n>computational load of Eq.(2) reminiscent of Swin\n\nThe only similarity between Swin and VWA is they are both as computationally efficient as LWA. Question 2 also focuses on the computational load. To avoid misunderstandings because in Summary you said \"the authors introduced LWA\", it needs to be clarified that LWA computed by Eq. 2 is a preliminary idea, neither our contribution nor Swin's contribution. Whether it's the Swin or our proposed VWA, both are extensions of LWA. As explained in point **b** above, Swin only changes the location of the Local Window but does not change its size. Hence, Swin's computational load is the same as that of LWA, given by $4(HW)(C^2) + 2(HW)(P^2)C$ in Eq. 2. \n\nTip: In the paper, we presented the computation of LWA straightforwardly. Since you asked this question, if you want more details about how $4(HW)(C^2) + 2(HW)(P^2)C$ is obtained we will show you here. If you already know it, just skip to <For Weakness 5>. Assuming the feature map is of shape ($H, W, C$) and it can be split uniformly into ($H/P \\times W/P = \\frac{HW}{P^2}$) patches with shapes of ($P \\times P \\times C$), for the LWA implements self-attention on one patch, its computation consists of two parts, first is mapping query, key, value, and output, so the first part of computation is ($4 \\times P \\times P \\times C_{in} \\times C_{out} = 4{P^2}{C^2}$). And second is attention computation including calculating attention maps and doing weighted summation, both of which are ($P^2 \\times P^2 \\times C = {P^4}C$) indicating that every pixel's feature of the patch will multiply itself elementwise and every pixel does weighted summation over all pixels according to the attention maps, so the second part of the computation is ($2 {P^4}C$). Now that for one patch the computation cost is $4{P^2}{C^2}+2{P^4}C$, for the whole feature map the computation cost is $[\\frac{HW}{P^2} \\times (4{P^2}{C^2}+2{P^4}{C})] = 4{(HW)}{(C^2)}+2{(HW)}{(P^2)}C$\n\n### **For Weakness 5**: \n----\nExperiments in the paper were conducted mostly to show that VWFormer is the best MSD for semantic segmentation.\n\n\n>HRViT-b1,b2, and b3\n\nHRViT takes the MLP-decoder as the default MSD. If HRViT is paired with VWFormer, the improvements are reported as follows:\n| mIoU on ADE20K | HRViT-b1 | HRViT-b2 | HRViT-b3 |\n| :- | :-: | :-: | :-: | \n|HRViT-MLP | 45.6 | 48.8 | 50.2|\n| HRViT-VW. | 46.9 | 50.4 | 51.6|\n\n>SegViT-V2\n\nSegViT-V2 is a decoder designed for ViT. We evaluate VW. paired with BEiTv2.\n\n| mIoU on ADE20K | BEiT-V2-L  |\n| :- | :-: | \n| SegViT-V2 |  58.2 |\n| VW. | 59.0 |\n\n>ViT-Adapter \n\nAdapter is equipped to ViT paired with UperNet as MSD. The UperNet is replaced with VWFormer and we get the following improvements:\n| mIoU on ADE20K | ViT-B | ViT-L | \n| :- | :-: | :-: | \n|Adapter-Uper. | 52.5 | 54.4|\n|Adapter-VW. | 54.1 | 55.8 |\n\n\n> Lawin Transformer\n\nIndeed, VW. is Version 2 (or called thoroughly improved version) of Lawin."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069572521,
                "cdate": 1700069572521,
                "tmdate": 1700069572521,
                "mdate": 1700069572521,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SWznk52xly",
                "forum": "lAhWGOkpSR",
                "replyto": "gU3hH15edD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission104/Reviewer_L266"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission104/Reviewer_L266"
                ],
                "content": {
                    "title": {
                        "value": "Comment"
                    },
                    "comment": {
                        "value": "The reviewer would like to thank the authors for the responses and the added analyses. The added results show that the proposed method can help improve the segmentation performance of recent methods like HRViT.\n\nThe response helps to address many concerns and the reviewer would like to improve the rating. Please incorporate these analyses in the final version."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110516102,
                "cdate": 1700110516102,
                "tmdate": 1700110516102,
                "mdate": 1700110516102,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6ROtelCuKO",
            "forum": "lAhWGOkpSR",
            "replyto": "lAhWGOkpSR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission104/Reviewer_Vryj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission104/Reviewer_Vryj"
            ],
            "content": {
                "summary": {
                    "value": "This paper points out two problems existing in the previous multi-scale semantic segmentation representation: scale inadequacy and field inactivation. To solve these two problems, this paper presents its method - VWA (varying window attention). It begins by splitting local window attention (LWA) into query window and context window, which are variable so that queries learn representations on a specific scale. Then, to reduce memory consumption, the pre-scaling principle, densely overlapping patch embedding(DOPE), and copy-shift padding mode are proposed. Finally, VWFormer is devised for semantic segmentation, which shows its effectiveness in terms of performance and reducing cost."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ This paper makes full use of multi-scale features for fusion and proposes VMA to obtain multi-scale representation.\n+ The experiments and visual results show the problems that existed in the previous work, namely, scale inadequacy and field inactivation, which is insight.\n+ The VMFormer frame is more clearly drawn and is easy to follow."
                },
                "weaknesses": {
                    "value": "- Too much space is spent to explain the memory usage problem, resulting in some experimental results, such as Table 9 and some ablation experiments, which can only be placed in the appendix.\n- Lack of ablation experiments on the copy-shift padding mode. It is mentioned in the paper that filling with zero will cause attention collapse, but there are no experimental results to explain it.\n- There is no comparison between VWA and LWA.\n- The pre-scaling principle is to reduce memory usage, and the impact of pre-scaling on performance is not reflected.\n- The method part is not abundant, such as the short path branch. Overall, it seems that the original is not high, mainly VWA.\n- Inference time is not reflected in the experiments."
                },
                "questions": {
                    "value": "(1) For copy-shift padding mode(CSP), why not take the adjacent part to padding?\n(2) What is the short path branch?\n(3) What does pre. or post. in Table 7 refer to?  PE refers to padding with zero or CSP?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission104/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission104/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission104/Reviewer_Vryj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission104/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698847954190,
            "cdate": 1698847954190,
            "tmdate": 1699635935375,
            "mdate": 1699635935375,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nfnjulbdeS",
                "forum": "lAhWGOkpSR",
                "replyto": "6ROtelCuKO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments! Any new questions please let me know!"
                    },
                    "comment": {
                        "value": "### **For Weakness 1**: \n---\nThanks for the structure advice. We will move key ablation studies to the main text and some equations explaining memory usage to the appendix, re-balancing the math and experiments.\n\n### **For Weakness 2 and Question 1**: \n---\n> Lack of ablation experiments on the copy-shift padding mode\n\nThe ablation experiment on the copy-shift padding mode is **Table 8 Left** in the paper. It will be moved to the main text.\n\n> Why not take the adjacent part to padding\n\nI think you refer to adjacent part padding as the symmetric padding mode in Pytorch (If I mistake it please tell me). In this research, we have tried many techniques to tackle attention collapse, the symmetric padding mode has also been a candidate but it did not provide high improvements like CSP. This is because the symmetric mode only copies the already existing feature within the context window but CSP offers the window at the edge and corner a wider contextual receptivity.\n\nWhat does the \"wider contextual receptivity\" indicate? For example, given a feature map of $64\\times64$, the local window is $8\\times8$, and the context window is $32\\times32$. If the VWA is at the upper left corner, the context window's coverage on the feature map is the feature map's [-12:20, -12:20] (here minus means outside feature map), so $12\\times20+20\\times12 +12\\times12$ paddings are needed for the context window. That means, with symmetric padding mode, the padding elements are shifted from [0:12, 0:20], [0:20, 0:12], and [0:12, 0:12] of the feature map, all of which are already covered by the context window. But if using CSP, from Eq. 17-18, the padding elements are shifted from [20:32, 0:20], [0:20, 20:32], and [20:32, 20:32], and now the context window's coverage is changing and not initial [-12:20, -12:20] of the feature map but becomes [0:32, 0:32] of the feature map.\n\nBelow is our previous trial result on the adjacent padding. It can be seen the adjacent padding's improvement is not that high compared to copy&shift.\n| padding mode | mIoU(/MS) |\n| :----: | :----: | \n| zero padd. | 51.6 / 52.6 | \n| adjacent |  51.9 / 52.9 | \n|copy&shift |  52.3 / 53.6|\n\n### **For Weakness 3**: \n---\nSec 3.2 and Fig. 2 show how LWA becomes VWA and conclude that LWA is a special VWA with $R=1$. Table 6 analyzes the impact of LWA on VWFormer. If you want some particular comparison, please let me know and I will show them soon : )\n\n### **For Weakness 4 and Question 3**: \n---\n>  The impact of pre-scaling on performance is not reflected & What does pre. or post. in Table 7 refer to?\n\nThe impact of pre-scaling is reflected in Table 7 because pre. or post. refers to Pre-scaling and Post-scaling.\n\nSec. 3.2 shows that VWA with **no scaling** leads to unaffordable computation cost and memory usage. Sec. 3.3 shows that  **post-scaling** is a naive solution able to remove the extra computation but unable to handle the memory usage, and **pre-scaling** is proposed for handling both of computation cost and memory usage. Table 7 compares these three strategies. Then you question about:\n\n> PE refers to padding with zero or CSP?\n\nPE refers to patch embedding, just the same as 'patch embedding' in ViT[1]. Regardless of the post-scaling or pre-scaling, there must be specific scaling operators. PE is the operator doing post-scaling. For pre-scaling, we propose DOPE paired with PE, specifically DOPE--> PE, to implement it, as introduced in Sec 3.3.\n\n### **For Weakness 5 and Question 2**: \n---\n> What is a short path branch\n\nA short path (means with no heavy components along this path) branch refers to a residual connection[2] or an identity mapping i.e. 1x1 conv[3]. \n\n> The original is not high, mainly VMA\n\nThe basic idea of VMA is original. The architecture of VWFormer, even if it has no VWA, is original. The DOPE with pre-scaling strategy is original because it is a new way to embed patches for a Transformer-like vision framework. The copy&shift padding is original because it is a new padding mode different from off-the-shelf padding modes existing in Numpy or Pytorch. \n\n\n\nRef.\n\n-- [1] An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2022\n\n-- [2] Deep residual learning for image recognition. In CVPR, 2016.\n\n-- [3] Identity mappings in deep residual networks. In ECCV, 2016\n\n**To be continued.**"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700067263795,
                "cdate": 1700067263795,
                "tmdate": 1700549825748,
                "mdate": 1700549825748,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2ipf4Tegjx",
                "forum": "lAhWGOkpSR",
                "replyto": "6ROtelCuKO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments! Any new questions please let me know! (Cont.)"
                    },
                    "comment": {
                        "value": "### **For Weakness 6:**\n---\n> Inference time\n\nTo compare the inference time, we supplement Tables 1, 2, and 3 in the paper by assessing the frame per second (FPS), respectively. \n\nComparison of Ours to SegFormer\n| FPS | MiTB0 | MiTB1| MiTB2| MiTB3 | MiTB4| MiTB5| \n| :----: | :----: | :----: | :----: | :----: | :----: | :----: | \n| Seg. | 50.5 | 46.2 | 30.9 | 22.1 | 15.5 | 11.9 | \n| VW. | 53.4 | 49.8 | 35.3 | 26.8 | 19.2 | 14.0 |\n\nComparison of Ours to Swin-Uper.\n| FPS | Swin-Ti | Swin-S | Swin-B | Swin-L | \n| :----: | :----: | :----: | :----: | :----: | \n| Uper. | 18.5 | 15.2 | 8.7 | 6.2 | \n| VW. | 24.4 | 22.0 | 15.9 | 8.8 | \n\nComparison of Ours to Swin-MaskFormer\n| FPS | Swin-Ti | Swin-S | Swin-B | Swin-L | \n| :----: | :----: | :----: | :----: | :----: | \n| Mask.-FPN | 22.1 | 19.6 | 12.6 | 7.9 | \n| Mask.-VW. | 23.1 | 20.7 | 12.9 | 7.9 | \n\nIn summary, our inference time is faster than SegFormer and MaskFormer, much faster than UperNet, and slightly faster than FPN."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700067444923,
                "cdate": 1700067444923,
                "tmdate": 1700067444923,
                "mdate": 1700067444923,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cESbVMmSpU",
                "forum": "lAhWGOkpSR",
                "replyto": "6ROtelCuKO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The discussion period will end in two days\uff01"
                    },
                    "comment": {
                        "value": "Dear Reviewer Vryj,\n\nMay I inquire if you have received our responses to your comments on the paper (submission ID:104)?\n\nALSO, it might be worth considering that the most confident reviewer (L266) has improved the rating so currently there are two most confident reviewers (L266 and u8Vk) suggesting the paper is above the acceptance threshold.\n\nAt the moment the discussion period concludes, we will update a new version of the paper with additional analyses inspired by your comments.\n\nSincerely,\n\nAuthors of Paper (submission ID 104)"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550065017,
                "cdate": 1700550065017,
                "tmdate": 1700550065017,
                "mdate": 1700550065017,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VIqOdeph27",
                "forum": "lAhWGOkpSR",
                "replyto": "2ipf4Tegjx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission104/Reviewer_Vryj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission104/Reviewer_Vryj"
                ],
                "content": {
                    "comment": {
                        "value": "The authors have solved my concerns. I would like to improve the rating. I suggest that the authors had better present the new analyses and results in the final version."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671879565,
                "cdate": 1700671879565,
                "tmdate": 1700671879565,
                "mdate": 1700671879565,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fN3DIstpF0",
                "forum": "lAhWGOkpSR",
                "replyto": "6ROtelCuKO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the improvement! We look forward to your new rating!"
                    },
                    "comment": {
                        "value": "It is a pleasure to know that we can solve your concerns. We are incorporating new analyses and results into the paper."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727603314,
                "cdate": 1700727603314,
                "tmdate": 1700727728734,
                "mdate": 1700727728734,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Z492W6aaHo",
            "forum": "lAhWGOkpSR",
            "replyto": "lAhWGOkpSR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission104/Reviewer_u8Vk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission104/Reviewer_u8Vk"
            ],
            "content": {
                "summary": {
                    "value": "This paper observes the limitation of local window attention and proposes an efficient modification to enlarge the receptive field while keeping the computation efficiency. Specifically, it designs a pre-scaling strategy, which cleverly reduces the extra computation of enlarging window size. This paper presents detailed computation comparisons with existing methods and verifies the effectiveness of the proposed methods on several benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper starts from the visual observations, then goes into detailed computation analysis, and further proposes modifications to enhance. The logic between the whole paper is very smooth and easy to follow.\n\n2. The detailed complexity analysis substance the efficiency of the proposed method. Besides, the experimental results further verify computation efficiency compared to existing methods.\n\n3. Quantitative analysis verifies the effectiveness of the proposed methods, highlighting the significance of varying window sizes."
                },
                "weaknesses": {
                    "value": "1. The proposed copy-shift padding is a bit weird. It mimics the rolling operation in numpy and torch. I wonder why adopting this instead of symmetric or constant padding.\n\n2. Fig 2b and Sec 3.2 are not well-aligned. In Fig 2b there's a PE operation, which doesn't occur in Sec 3.2, which can confuse the readers."
                },
                "questions": {
                    "value": "1. Are there any ablation studies indicating the effectiveness of copy-shift padding?\n\n2. From Table 7 in Supp, it seems avg_pool can be a substitution of PE, then I wonder what the performance will be like if the rescaling method \"DOPE --> avg_pool --> 1x1 conv\" is taken."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission104/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698928710453,
            "cdate": 1698928710453,
            "tmdate": 1699635935310,
            "mdate": 1699635935310,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lTbb3k0dbB",
                "forum": "lAhWGOkpSR",
                "replyto": "Z492W6aaHo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments! Any new questions please let me know!"
                    },
                    "comment": {
                        "value": "### **For Weakness 1 and Question 1**: \n---\n> Copy&shift mimics the rolling operation\n\nVery insightful opinions. CSP seems like the roll function but very very vdifferent from it. If only seeing Figure 3, CSP is like \"copy&shift\" and the roll function is like \"cut&shift\". HOWEVER, more significantly, the roll function can only operate the elements of the feature map's edge parts, cutting an edge and shifting it to another edge. But from Eq. 17 and 18, CSP is proposed for copying the elements that are nearby outside the context window's inside-feature edges and then shifting these elements to padding so that the context window can cover them instead of zeros when the context window is at the edge or corner, which is the roll function in Numpy or Pytorch can not achieve. \n\n> Any ablation studies indicating the effectiveness\n\nAt the bottom of **page 15**, **Table 8 Left**, shows the effectiveness of CSP and that the zero padding mode impairs performance largely.\n\n> Symmetric or constant padding.\n\nIn this research, we have tried many techniques to tackle attention collapse, and symmetric padding mode has also been a candidate but it did not provide high improvements like CSP. We think this is because the symmetric mode only copies the already existing feature within the context window but CSP offers the window at the edge or corner a wider contextual receptivity [1]. For constant padding, zero padding is a kind of constant padding so we are not very clear on what constants may be useful for this problem and all candidates having been tried by us are dynamic methods.\n| padding mode | mIoU(/MS) |\n| :----: | :----: | \n| zero padd. | 51.6 / 52.6 | \n| symmetric |  51.9 / 53.0 | \n|copy&shift |  52.3 / 53.6|\n\n\n\n### **For Weakness 2**: \n---\nWe will revise the text-figure alignment issue in the paper to make readers clear. And we will polish every caption to let readers know what the used term in every Figure means : ) \n\n### **For Question 2**: \n---\nVery good question, you must have carefully read the paper. Although avg_pool can be a substitution for PE, the 2nd and 3rd rows in Table 7 show that avg_pool leads to an unignorable performance drop (53.7%-->52.8%). So if using avg_pool-->conv1x1 to substitute PE in the \"DOPE-->PE\" operation, there will be an unavoidable decrease in performance. From another view of efficiency, the 2nd and 3rd rows in Table 7 show that avg_pool--> conv1x1 is much more efficient than PE. But that works with the post-scaling strategy. With the pre-scaling strategy, according to the analysis in Sec. 3.3 and Eq. 13-15, it can be seen that PE is the same as a linear mapping. So PE in the \"DOPE-->PE\" operation with a pre-scaling strategy is efficient enough and it is redundant to replace it with avg_pool --> conv1x1 for more efficiency. Below is the comparison of \"DOPE-->PE\" with \"DOPE-->avg_pool-->conv1x1\", showing the performance drop caused by the substitution.\n| rescaling method | mIoU(/MS) |\n| :---- | :----: | \n| DOPE --> PE | 52.3 / 53.6 | \n| DOPE --> avg_pool-->conv1x1 |  51.8 / 52.8 | \n\nTip:\n\n[1] What does the \"wider contextual receptivity\" indicate? For example, given a feature map of $64\\times64$, the local window is $8\\times8$, and the context window is $32\\times32$. If the VWA is at the upper left corner, the context window's coverage on the feature map is the feature map's [-12:20, -12:20] (here minus means outside feature map), so $12\\times20+20\\times12 +12\\times12$ paddings are needed for the context window. That means, with symmetric padding mode, the padding elements are copied from [0:12, 0:20], [0:20, 0:12], and [0:12, 0:12] of the feature map, all of which are already covered by the context window. But if using CSP, from Eq. 17-18, the padding elements are copied from [20:32, 0:20], [0:20, 20:32], and [20:32, 20:32], and now the context window's coverage is not initial [-12:20, -12:20] of the feature map but becomes [0:32, 0:32] of the feature map."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700065031365,
                "cdate": 1700065031365,
                "tmdate": 1700065031365,
                "mdate": 1700065031365,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5yYQgTf4VC",
                "forum": "lAhWGOkpSR",
                "replyto": "Z492W6aaHo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The discussion period will end in less than two days!"
                    },
                    "comment": {
                        "value": "Dear Reviewer u8Vk,\n\nMay I inquire if you have received our responses to your comments on the paper (submission ID:104)?\n\nALSO, it might be worth considering that the most confident reviewer (L266) has improved the rating (rating 6 with confidence 5). Would you like to adjust your rating or confidence?\n\nSincerely,\n\nAuthors of Paper (submission ID 104)"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616218295,
                "cdate": 1700616218295,
                "tmdate": 1700616218295,
                "mdate": 1700616218295,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QltKnPJvzE",
                "forum": "lAhWGOkpSR",
                "replyto": "lTbb3k0dbB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission104/Reviewer_u8Vk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission104/Reviewer_u8Vk"
                ],
                "content": {
                    "title": {
                        "value": "Issues well solved"
                    },
                    "comment": {
                        "value": "All my issues have been solved and I will keep the score"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724385127,
                "cdate": 1700724385127,
                "tmdate": 1700724385127,
                "mdate": 1700724385127,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aSXuhTnBLa",
            "forum": "lAhWGOkpSR",
            "replyto": "lAhWGOkpSR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission104/Reviewer_1jxp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission104/Reviewer_1jxp"
            ],
            "content": {
                "summary": {
                    "value": "The submission introduces Varying Window Attention (VWA) to address scale inadequacy and field inactivation in semantic segmentation. VWA modifies the local window attention mechanism to dynamically adjust the receptive field, aiming to improve multi-scale representation learning. The paper also proposes a new multi-scale decoder, VWFormer, which incorporates VWA and demonstrates performance and efficiency gains on standard datasets. The work claims three main contributions: the VWA mechanism, the VWFormer decoder, and empirical improvements over state-of-the-art methods. Extensive experiments validate the effectiveness of the proposed methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ Introduction of Varying Window Attention (VWA) to dynamically adjust receptive fields, addressing scale inadequacy and field inactivation.\n+ Development of new principles such as pre-scaling, densely overlapping patch embedding (DOPE), and copy-shift padding mode (CSP) to enhance efficiency in receptive field variation.\n+ Empirical improvements in mean Intersection over Union (mIoU) and reductions in computational cost (FLOPs)."
                },
                "weaknesses": {
                    "value": "- The paper may not sufficiently differentiate VWA from existing local window attention mechanisms.\n- Potential concerns about the scalability and generalizability of the proposed method to different architectures or datasets."
                },
                "questions": {
                    "value": "1. How does VWA fundamentally differ from existing attention mechanisms in handling multi-scale representations?\n2. Include a breakdown of performance gains across different classes or segments within the datasets used."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission104/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699378949581,
            "cdate": 1699378949581,
            "tmdate": 1699635935241,
            "mdate": 1699635935241,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jqOpTzRvQD",
                "forum": "lAhWGOkpSR",
                "replyto": "aSXuhTnBLa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments! Any new questions please let me know!"
                    },
                    "comment": {
                        "value": "### **Above All**\n---\nWe noticed that you gave the score **Good** for all of Soundness, Presentation, and Contribution, yet the overall rating is marginally **below** the acceptance threshold. We are wondering if there might be a typo in the overall rating : )\n\n\n### **For Weakness 1 and Question 1**: \n---\n> How does VWA DIFFER from existing window attention mechanisms in handling multi-scale representation learning\n\nIn the paper we called them 'multi-scale attention' introduced in the last line of Sec. 2.1. Please see the second paragraph of **Sec. 2.1, Fig. 6, Appendix B, and Table 9** (both ISANet and ANN in Table 9 are 'multi-scale attention').\n\nI am glad to present a summary of these related methods here (If you have some particular methods for comparing with our VWA, please let me know and I will compare them. To my knowledge, the listed works in the paper are all existing ones). ISANet[1] and GG-Transformer[2] have the same core idea but apply it to different tasks, semantic segmentation and image recognition, respectively. This idea first splits the image into windows to learn local representations and then interlaces the pixels among these windows so (pixels in) every window can learn global representations. The biggest problem with the ISANet-like method is they did not provide a possibility to learn representations of other scales, as illustrated in Fig. 6 (a). Table 9 compares ISANet with ours, showing that Our VWA outperforms them by ISANet margins. \n\nANN[3], Focal Transformer[4], and Shunted Transformer[5] share the same core idea but apply it to different tasks. ANN is for semantic segmentation, and both Focal and Shunted are for image recognition. This idea depends on pooling filters to learn multi-scale representations (Appendix A analyzes the pooling's issue on multi-scale learning). One may question the proposed VWA also applies DOPE to downsample the image like pooling. The key point lies in that VWA changes the context window size so pixels in windows have varying receptive fields. However, the ANN-like method does not deal with the context window size so its receptive field is indeed fixed, as illustrated in Fig. 6(b). Table 9 also shows that VWA outperforms ANN by large margins.\n\nIn short word, existing multi-scale attention, whether based on ISANet or ANN, confines the representation learning within the window. Only VWA liberates the representation learning beyond the window, covering any region.\n\nRef.\n\n-- [1] Interlaced sparse self-attention for semantic segmentation. In IJCV, 2021\n\n-- [2] Glance-and-gaze\nvision transformer. In NIPS, 2022\n\n-- [3] Asymmetric non-local neural\nnetworks for semantic segmentation. In CVPR, 2019\n\n-- [4] Focal self-attention for local-global interactions in vision transformers. In NIPS, 2022\n\n-- [5] Shunted self-attention\nvia multi-scale token aggregation. In CVPR, 2022\n\n**To be continued.**"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700065839856,
                "cdate": 1700065839856,
                "tmdate": 1700069689241,
                "mdate": 1700069689241,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k4330dxf8P",
                "forum": "lAhWGOkpSR",
                "replyto": "aSXuhTnBLa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments! Any new questions please let me know! (Cont.)"
                    },
                    "comment": {
                        "value": "### **For Weakness 2 and Question 2**: \n---\n>Scalability or Generalizability\n\nGood points, we will conduct in-depth research on them.\n\nBUT now, some good indicators can be presented. Please see Appendix F. It introduces that our method has two settings of different model sizes to match backbones with different model capacities, including the efficient setting specially designed for lightweight backbones like SegFormer-B0 and B1 compared with ours in Table 1, and a normal setting for regular backbones. We have also experimented with scalability on the MLA module of VWFormer, for example, scaling the channel from 512 to 2048. With Mask2Former and Swin-L, our method can improve mIoU from 58.3% to 58.8%. To show generalizability, the proposed method has been evaluated with MiT (B0-->B5), Swin Transformer (Ti-->L), MaskFormer, Mask2Former, Deformable Attention, and ResNet(50-->101) in the paper. The benchmarks contain three challenging datasets as introduced in Appendix G. \n\n> A breakdown of performance gains\n\nI will show a breakdown of performance gains within Cityscapes which has 19-class segments. The upper results are obtained by MiT-B5 paired with SegFormer head (mIoU 82.26%) and the lower results are obtained by MiT-B5 paired with our VWFormer (mIoU 82.87%). \n\n|method | road | side. | build. | wall | fence | pole | light | sign | vege. | terrain | sky | man | rider | car | truck | bus | train | motor | bike|\n| :-| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | \n|Seg. | 98.5 | 87.3 | 93.7 | **68.6** | 65.7 | 69.5 | 75.6 | 81.8 | 93.2 | 66.2 | **95.7** | 85.3 | 69.6 | 95.6 | **85.7** | 91.7 | **84.7** | 73.8 | 80.7|\n|VW. |98.5 | 87.6 | 94.0 | 68.4 | 68.7 | 73.0 | 77.3 | 84.5 | 93.5 | 66.3 | 95.4 | 86.8 | 71.2 | 96.2 | 77.4 | 93.1 | 84.6 | 75.2 | 82.2|\n \nThe bold number is the class that the counterpart performs better than ours. Except for the \"truck\" class where SegFormer outperforms ours largely, which seems like a biased result, on the 'wall', 'sky', and 'train' SegFormer only slightly outperforms ours (by avg. 0.2%). And on the other 15 classes, Ours shows consistent superiority to SegFormer (by avg. 1.4%).\n\nBesides, we did a breakdown of performance gains within ADE20K which has 150-class segments. Since it has too many classes, I will list some important statistics. The comparable results are obtained from Swin-L-Mask2Former and Swin-L-Mask2Former with VWFormer in place of Deform-FPN. (1) The pure Mask2Former achieves 56.96% mIoU and ours achieves 58.27% mIoU. (2) In all 150 classes, their methods outperform ours in 58 classes, while ours outperform theirs in 92 classes. (3) In the 58 categories where their methods outperform ours, the average improvement is -4.7%. In the 92 categories where ours outperforms theirs, the average improvement is 5.2%."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700066548112,
                "cdate": 1700066548112,
                "tmdate": 1700725454861,
                "mdate": 1700725454861,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NiqR7zYcpS",
                "forum": "lAhWGOkpSR",
                "replyto": "aSXuhTnBLa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission104/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The discussion period will end in two days\uff01"
                    },
                    "comment": {
                        "value": "Dear Reviewer 1jxp,\n\n\nMay I inquire if you have received our responses to your comments on the paper (submission ID:104)? \n\nALSO, it might be worth considering that the most confident reviewer (L266) has improved the rating so currently there are two most confident reviewers (L266 and u8Vk) suggesting the paper is above the acceptance threshold. Again, reviewer Vryj also expressed an intention to improve the rating.\n\nAt the moment the discussion period concludes, we will update a new version of the paper with additional analyses inspired by your comments. \n\nSincerely,\n\nAuthors of Paper (submission ID 104)"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549239733,
                "cdate": 1700549239733,
                "tmdate": 1700725898224,
                "mdate": 1700725898224,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]