[
    {
        "title": "P-MapNet: Far-seeing Map Constructer Enhanced by both SDMap and HDMap Priors"
    },
    {
        "review": {
            "id": "ul7QJ7Wkk9",
            "forum": "lgDrVM9Rpx",
            "replyto": "lgDrVM9Rpx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3411/Reviewer_6mPa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3411/Reviewer_6mPa"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel online rasterized HDMapping algorithm P-MapNet and focuses on exploiting priors in both SDMap and HDMap to get rid of the current reliance on expensive HDMaps for autonomous vehicles. The authors propose two novel designs within P-MapNet: a multi-head cross-attention-based SDMap prior module to settle the problem of SDMap misalignment and a ViT-style HDMap prior refinement module pre-trained on the masked-autoencoder methodology. In the experiments, P-MapNet is evaluated on both the NuScenes, where it achieves a 13.4% improvement in mIOU at the range of 240m * 60m, and Argoverse2 dataset, where it increases by 9.36 mAP compared to the baseline method demonstrating the effectiveness of its far-seeing solution for online HDMap construction and localization challenges in autonomous driving scenarios via both SDMap and HDMap priors."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This work provides a detailed explanation of a two-phase OSM data-based rasterized SDMap generation method, contributing to the advancement of research on SDMap utilization.\n\n2. The main quantitative evaluations are performed on widely-used public datasets, namely NuScenes and Argoverse2, highlighting the salient performance improvement achieved by P-MapNet.\n\n3. The proposed design is thoroughly evaluated through a comprehensive set of ablations investigating the SDMap fusion methods. These ablations demonstrate the design merits of the proposed approach."
                },
                "weaknesses": {
                    "value": "1. The paper emphasizes the limitation of relying on HDMaps for autonomous vehicles to operate outside regions with this infrastructure, yet it relies heavily on HDMap priors to refine outputs and address issues such as broken and unnecessarily curved results. Additionally, the approach of generating HDMap with prior information from HDMap seems counterintuitive and unreasonable.\n\n2. The paper falls short of providing the results under the setting of camera-only modality and combining both SDMap prior and HDMap prior modules. This omission raises concerns about the true impact of HDMap priors on the overall performance.\n\n3. In terms of vectorization baseline results, the authors only reproduce HDMapNet under their new settings on the NuScenes dataset, without conducting a comparison with other state-of-the-art methods, both vectorized and rasterized-to-vectorized. This limits the thoroughness of the performance evaluation."
                },
                "questions": {
                    "value": "1. Could you please explain the reasons for selecting rasterized representation and employing post-processing for vectorized results instead of directly using a vectorized network? In Section 2.1, you mentioned the limitations of methods relying solely on onboard sensors, but it is not clear how this relates to the chosen representation. Additionally, you mentioned that your network is designed in a BEV dense prediction manner and the structured output space of BEV HDMap cannot be guaranteed. Given these factors, why not use the vectorized representation directly, which naturally addresses the problem and eliminates the need for additional MAE pretraining methodology?\n\n2. Can you provide a demonstration of why online generation of HDMap is necessary when given HDMap, and explain the relatively low increase in performance when HDMap priors are added?\n\n3. Can you report the results under the camera-only modality and when both SDMap prior and HDMap prior modules are combined, to accurately reflect the genuine influence of HDMap priors?\n\n4. Can you reproduce the results of more recent state-of-the-art methods in the new long-range settings to compare their performance with the vectorized results? This comparison should include, but not be limited to, VectorMapNet (mentioned but not compared with), MapTR, MapVR (which also perform rasterized-to-vectorized conversion), and PivotNet.\n\n5. In relation to Table 2, could you explain the significant decrease in frames per second (FPS) and how this might impact downstream or practical applications?\n\n6. Your SDMap Prior Module aligns misaligned SDMap with BEV features using multi-head cross-attention. However, what if there are localization errors present in the BEV features, which is a common occurrence in both camera-only and camera+lidar models?\n\nMinor issues:\nIn your summary of contributions, \"artefacts\" should be corrected to \"artifacts.\"\nRegarding your summary of contributions, I am confused about the example of \"P-MapNet is a far-seeing solution.\" Could you clarify what it is specifically used for?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3411/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3411/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3411/Reviewer_6mPa"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3411/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698673605881,
            "cdate": 1698673605881,
            "tmdate": 1699636292678,
            "mdate": 1699636292678,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Oe9X4LYLzY",
                "forum": "lgDrVM9Rpx",
                "replyto": "ul7QJ7Wkk9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3411/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3411/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, thank you for the professional feedbacks. We have updated the manuscript and marked the changes in red. Here are my answers to your related questions.\n## Questions.1 & Questions.4 & Weaknesses.3:\n\nWe further demonstrate the effectiveness of SD map prior by incorporating it into the state-of-the-art vectorized map constructor MapTR. The results are presented in **Appendix A.1** and **Fig.5**. It shows that the SD map prior can also improve the performance of MapTR on various ranges. Since the MAE HD map modelling method does not apply to MapTR, it is not evaluated.\n\nMeanwhile, we would like to point out that dense rasterized prediction on the BEV view still has advantages over direct vectorized map prediction methods. Vectorized representation is intrinsically troubled by control point selection ambiguity, which negatively impacts generalization in real-world autonomous driving applications. We believe in the future, rasterized map constructer and vectorized map constructor would co-exist in the academia and industry.\n\n| Range               | Method     | Divider | PedCrossing | Boundary | mAP                            |\n|---------------------|------------|---------|--------------|----------|--------------------------------|\n|    | MapTR      | 12.69   | 7.17         | 4.23     | 8.03                           |\n|  $240\\times 60m$   | MapTR-SDMap | **22.74** |  16.34    | 10.53    | 16.53 (+8.50)            |\n|                     | P-MapNet   | 14.51   | **25.63**        | **28.11** |  **22.75**                          |\n\n## Questions.2 & Weaknesses.1:\nWe actually didn't have the HDMap, we just pre-train the mask auto-encoding in the train split dataset and then evaluate in the val split dataset. Furthermore, to verify the generalizability of the HDMap Prior Module, we conducted cross-data experiments as detailed in **Appendix B.2**. In these experiments, we pre-trained the model on the train split of the Argoverse2 dataset and then evaluated it on the validation data of nuScenes.This approach also yielded an improvement in the mIoU.\nAdditionnally, a difference between our MAE and the conventional MAE may be the cause of this confusion. Specifically, a conventional MAE takes RGB image patches and directly regresses RGB image values of the whole image. Here, the input and output of our HDmap MAE are both rasterized segmentation masks. In our method, the output segmentation mask is supervised by a pixel-wise cross-entropy loss instead of the typical regression loss in a conventional MAE. As such our MAE can be naturally used for refinement during inference since the head is a segmentation head. To note, this segmentation mask MAE is exactly doing masked auto-encoding.\n## Questions.3 & Weaknesses.2:\nWe have updated the relevant experiments in **Tab.2**, and conducted relevant experiments in **Fig. 5** to obtain the visualization results and quantification results of the camera-only modality utilizing post-processing vectorization.\n## Questions.5\nRegarding the issue of model runtime, we conducted further experiments to break down the inference time and discovered that the refinement module consumes a significant amount of time. In fact, the HDMap Prior Module is an optional component and can be chosen based on specific requirements. The detailed experimental results are presented in **Table 13**.\n| Component     | Runtime (ms)     | Proportion | \n|---------------------|------------|---------|\n|  Image backbone  | 7.56      | 7.63%   | \n|  View transformation | 3.25 | 3.28% | \n|  Lidar backbone | 17.60  |17.76%  |\n|SDMap prior module|4.40|4.45%|\n|HDMap prior module|66.12|66.87%|\n|Total |98.87| 100%|\n## Questions.6\nHowever, localization errors\u00a0only influence the SDMap obtained from the global SDMAP, the BEV feature is online generated from the onboard sensors.\n## Minor issues:\nWe have changed the spelling of the word. Online long-distance HDMap generation can obtain good priors for downstream planning tasks, thereby improving downstream planning and control tasks to be safer and smoother[1].\n## Reference\n[1]Hao Dong, Xianjing Zhang, Xuan Jiang, Jun Zhang, Jintao Xu, Rui Ai, Weihao Gu, Huimin Lu, Juho Kannala, and Xieyuanli Chen. Superfusion: Multilevel lidar-camera fusion for long-range hd map generation and prediction. arXiv preprint arXiv:2211.15656, 2022"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3411/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700167278312,
                "cdate": 1700167278312,
                "tmdate": 1700475258653,
                "mdate": 1700475258653,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OdRr4vsgyM",
                "forum": "lgDrVM9Rpx",
                "replyto": "Oe9X4LYLzY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3411/Reviewer_6mPa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3411/Reviewer_6mPa"
                ],
                "content": {
                    "title": {
                        "value": "Feedback on the rebuttal materials"
                    },
                    "comment": {
                        "value": "Thank you for the additional results. I acknowledge the utilization of SDMap, which is a neat idea and a promising direction. The explanation on the MAE part nearly almost my concern except one thing that what's the motivation of using a MAE to encode HDMap prior here. It add tremendous runtime cost according to your runtime analysis, while the improvement is relatively moderate as shown in Table 2 in the main paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3411/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560505307,
                "cdate": 1700560505307,
                "tmdate": 1700560505307,
                "mdate": 1700560505307,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DjwyarcpdB",
            "forum": "lgDrVM9Rpx",
            "replyto": "lgDrVM9Rpx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3411/Reviewer_mZgt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3411/Reviewer_mZgt"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new approach called P-MapNet for far-seeing HD-Map generation. The proposed P-MapNet exploits the priors from SD-Map and HD-Map for long-distance HD-Map. This paper first generates SD-Map for nuScenes and Argoverse datasets and then presents the P-MapNet framework which is based on BEV and contains the SD-Map prior module and the HD-Map prior module. The HD maps are predicted by the segmentation head. The experiments can show the proposed method is effective, especially for long-range HD map prediction."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper presents a new HD map framework named P-MapNet aims for long-range HD map construction.\n2. This paper builds the SD map for two datasets based on OpenStreetMap.\n3. The proposed framework P-MapNet adopts the coarse SD-map prior and the fine-grained HD-map prior for far-seeing map construction.\n4. The proposed P-MapNet obtains significant results compared to HDMapNet."
                },
                "weaknesses": {
                    "value": "1. The experiments lack the comparisons with recent works, e.g., [1][2][3]. The baseline of HDMapNet is too old and weak.\n2. The idea of building long-distance HD maps has been explored in previous works [4,5], the authors should clearly state the difference and the superiority of the proposed framework.\n3. The proposed approach involves a large computation burden and  has lower inference speeds\n4. The authors evaluate the proposed framework on the benchmark with the max range of 240x60 while I'm concerned about the superiority of the proposed framework compared to the methods trained with the same range. In addition, I'm concerned about whether the proposed method has a limited range, and what the range is when SD maps do not work.\n5. The experiments about the downsampling factors of the SD map.\n6. Experimental results about P-MapNet(S+H) without lidar.\n7. In Sec.4.3, it's unclear how the initial maps are used in the mask image modelling and how the maps are refined during inference.\n\n[1] Liao et.al. MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction. ICLR 2023.\n[2] Liu et.al. VectorMapNet: End-to-end Vectorized HD Map Learning. ICML 2023.\n[3] Ding et.al. PivotNet: Vectorized Pivot Learning for End-to-end HD Map Construction. ICCV 2023.\n[4] Xiong et.al. Neural Map Prior for Autonomous Driving. CVPR 2023."
                },
                "questions": {
                    "value": "1. I'm concerned about whether the proposed framework can be applied to vectorized methods, such as MapTR[1], though this paper tries to vectorize the map through post-processing.\n\n[1] Liao et.al. MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction. ICLR 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3411/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698863017705,
            "cdate": 1698863017705,
            "tmdate": 1699636292598,
            "mdate": 1699636292598,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gNycppawOc",
                "forum": "lgDrVM9Rpx",
                "replyto": "DjwyarcpdB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3411/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3411/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, thank you for the professional feedbacks. We have updated the manuscript and marked the changes in red. Here are my answers to your related questions.\n## Weaknesses.1 & Weaknesses.4 & Questions.1:\n\nWe further demonstrate the effectiveness of SD map prior by incorporating it into the state-of-the-art vectorized map constructor MapTR. The results are presented in **Appendix A.1** and **Fig.5**. It shows that the SD map prior can also improve the performance of MapTR on various ranges. Since the MAE HD map modelling method does not apply to MapTR, it is not evaluated.\n\nMeanwhile, we would like to point out that dense rasterized prediction on the BEV view still has advantages over direct vectorized map prediction methods. Vectorized representation is intrinsically troubled by control point selection ambiguity, which negatively impacts generalization in real-world autonomous driving applications. We believe in the future, rasterized map constructer and vectorized map constructor would co-exist in the academia and industry.\n\nRegarding the effective distance of SDMap, due to GPU memory constraints, we only supported testing up to 240 meters. Theoretically, SDMap has priors at all distances, and in ranges beyond sensor perception, SDMap alone provides valuable information. The model could potentially guess the form of the HDMap based on SDMap, although the generated HDMap may not be very accurate due to misalignment issues.\n| Range               | Method     | Divider | PedCrossing | Boundary | mAP                            |\n|---------------------|------------|---------|--------------|----------|--------------------------------|\n|    | MapTR      | 12.69   | 7.17         | 4.23     | 8.03                           |\n|  $240\\times 60m$   | MapTR-SDMap | **22.74** |  16.34    | 10.53    | 16.53 (+8.50)            |\n|                     | P-MapNet   | 14.51   | **25.63**        | **28.11** |  **22.75**                          |\n\n## Weaknesses.2:\nOur approach fundamentally differs from the method used in Neural Map Prior[1]. In [1], a Neural Map must be pre-constructed to enhance the online generation of HDMaps, which is then updated as the vehicle repeatedly traverses the same area, aiming to obtain a more accurate HDMap. Additionally, the furthest distance covered in their paper is 160m by 100m.\nConversely, our method, based on the SDMap Prior Module, predicts HDMaps at ultra-long distances online, without the need for a prior traversal of the area. Furthermore, the maximum distance in our results extends to 240m by 60m.\n## Weaknesses.3\nRegarding the issue of model runtime, we conducted further experiments to break down the inference time and discovered that the refinement module consumes a significant amount of time. In fact, the HDMap Prior Module is an optional component and can be chosen based on specific requirements. The detailed experimental results are presented in **Table 13**.\n| Component     | Runtime (ms)     | Proportion | \n|---------------------|------------|---------|\n|  Image backbone  | 7.56      | 7.63%   | \n|  View transformation | 3.25 | 3.28% | \n|  Lidar backbone | 17.60  |17.76%  |\n|SDMap prior module|4.40|4.45%|\n|HDMap prior module|66.12|66.87%|\n|Total |98.87| 100%|\n## Weaknesses.5\nWe have completed relevant experiments, see Appendix A.4 for details. SDMap downsampling factors are the same as factor **d**.\n## Weaknesses.6\nWe have updated relevant experiments in **Tab.2**\n## Weaknesses.7\nA difference between our MAE and the conventional MAE may be the cause of this confusion. Specifically, a conventional MAE takes RGB image patches and directly regresses RGB image values of the whole image. Here, the input and output of our HDmap MAE are both rasterized segmentation masks. In our method, the output segmentation mask is supervised by a pixel-wise cross-entropy loss instead of the typical regression loss in a conventional MAE. As such our MAE can be naturally used for refinement during inference since the head is a segmentation head. To note, this segmentation mask MAE is exactly doing masked auto-encoding.\n\nWe have further refined the description and ablation experiments of the HDMap Prior Module, as detailed in **Section 4.3** and **Appendix B**. \n\n## Reference\n[1] Xiong et.al. Neural Map Prior for Autonomous Driving. CVPR 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3411/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700165782434,
                "cdate": 1700165782434,
                "tmdate": 1700292398032,
                "mdate": 1700292398032,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GzsvjooTj3",
            "forum": "lgDrVM9Rpx",
            "replyto": "lgDrVM9Rpx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3411/Reviewer_8h86"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3411/Reviewer_8h86"
            ],
            "content": {
                "summary": {
                    "value": "This draft improves the accuracy of the online map construction task by introducing prior information from SDMap and HDMap. It uses surround images and point cloud data as input to obtain BEV (Bird's Eye View) features, and then utilizes attention mechanism to extract corresponding features from SDMap to generate better bev feature. It further ensures the continuity of segmentation results by using a pre-trained HDMap model based on MAE. The utilization of these two priors significantly enhances the accuracy of map construction."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The introduction of prior information of SDMap significantly improves the map accuracy at both short range and long range.\n2. The highlight is the use of pretraining model based on MAE to ensure the continuity of segmentation result.\n3. The ablation experiments in this article are quite comprehensive."
                },
                "weaknesses": {
                    "value": "1. If a vectorization modeling approach is used, there might not be such discontinuities in results. This article should conduct further experiments to validate this matter.\n2. The metrics of vectorization results should be compared with vectorized modeling methods.\n3. The mask proportion of MAE should undergo some ablation experiments.\n4. The benefits brought by the prior information of HDMap are too small.\n5. There are some data inaccuracies in mIoU in Table 2."
                },
                "questions": {
                    "value": "What data should be used to train this MAE-based ViT?  And and would the model overfit if all the data is utilized?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3411/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698974638459,
            "cdate": 1698974638459,
            "tmdate": 1699636292522,
            "mdate": 1699636292522,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oV5hmIMcVB",
                "forum": "lgDrVM9Rpx",
                "replyto": "GzsvjooTj3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3411/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3411/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, thank you for the professional feedbacks. We have updated the manuscript and marked the changes in red. Here are my answers to your related questions.\n## Weaknesses.1 & Weaknesses.2\nWe further demonstrate the effectiveness of SD map prior by incorporating it into the state-of-the-art vectorized map constructor MapTR. The results are presented in **Appendix A.1** and **Fig.5**. It shows that the SD map prior can also improve the performance of MapTR on various ranges. Since the MAE HD map modelling method does not apply to MapTR, it is not evaluated.\n\nMeanwhile, we would like to point out that dense rasterized prediction on the BEV view still has advantages over direct vectorized map prediction methods. Vectorized representation is intrinsically troubled by control point selection ambiguity, which negatively impacts generalization in real-world autonomous driving applications. We believe in the future, rasterized map constructer and vectorized map constructor would co-exist in the academia and industry.\n| Range               | Method     | Divider | PedCrossing | Boundary | mAP                            |\n|---------------------|------------|---------|--------------|----------|--------------------------------|\n|    | MapTR      | 12.69   | 7.17         | 4.23     | 8.03                           |\n|  $240\\times 60m$   | MapTR-SDMap | **22.74** |  16.34    | 10.53    | 16.53 (+8.50)            |\n|                     | P-MapNet   | 14.51   | **25.63**        | **28.11** |  **22.75**                          |\n## Weaknesses.3\nWe have added relevant ablation experiments. Specifically, in **Appendix B.5**, we conducted 25%, 50%, and 75% experiments. Too high a mask ratio will lead to the lack of valid information and the actual refinement process of the input difference is large, too low a mask ratio can not force the network to capture the HDMap priors\u3002\n| Mask Proportion  | Divider | PedCrossing | Boundary | mAP                            |\n|--------------|---------|--------------|----------|--------------------------------|\n|    25%  | 64.8   | 51.4        |67.6     | 61.27                           |\n|    50% | **65.3** | 52.0    | **68.0**  | **61.77**             |\n|    75%   | 64.7   | **52.1**        | 67.7 | 61.50                         |\n## Weaknesses.4\nThis issue is also present in the work of MapPrior[1]. Essentially, our goal is to map the output of the SDMap Prior Module to the space of HDMaps. However, this process might not map onto the ground truth with high precision, hence the modest improvement in the mean Intersection over Union (mIoU) metric, which measures accuracy.In order to evaluate the realism after refining, we use a type of Perceptual Metrics, specifically LPIPS [2], to do so, and the results show that our HDMap Prior module has a significant improvement on predicting after refining than without refining. As detailed in **Appendix B.1**.\n| Method          | mIoU\u2191  | LPIPS\u2193 |  \n|-----------------|--------|--------|\n| Baseline        | 49.07  | 0.7872 |   \n| P-MapNet (S)    | 60.20  | 0.7607 |   \n| P-MapNet (S+H)  | **61.77**  | **0.7124**| \n## Weaknesses.5\nIt may be a problem with our table layout. You may have misread the results of only cam and fusion model. We have revised the table **Tab.2**.\n\n## Questions.1:\nInitially, we pre-trained our model using the train split of the nuScenes dataset, followed by evaluations on the validation split. This approach might have led to some overfitting. To verify the generalizability of the HDMap Prior Module, we conducted cross-data experiments as detailed in **Appendix B.2**. In these experiments, we pre-trained the model on the train split of the Argoverse2 dataset and then evaluated it on the validation data of nuScenes.This approach also yielded an improvement in the mIoU.\n\n## Reference\n[1]Xiyue Zhu, Vlas Zyrianov, Zhijian Liu, and Shenlong Wang. Mapprior: Bird\u2019s-eye view map layout estimation with generative models. arXiv preprint arXiv:2308.12963, 2023.\n[2]Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586\u2013595, 2018."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3411/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700165033383,
                "cdate": 1700165033383,
                "tmdate": 1700292267366,
                "mdate": 1700292267366,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cpRNHRiYOd",
            "forum": "lgDrVM9Rpx",
            "replyto": "lgDrVM9Rpx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3411/Reviewer_Je8q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3411/Reviewer_Je8q"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to incorporate map priors, including priors both in SDMap and HDMap, to improve the performance of HDMap generation. Weakly aligned SDMap priors are extracted and encoded as an alternative conditioning branch. A masked autoencoder pretraining on nuscenes is utilized to refine the HDMap. Extensive experiments demonstrate the effectiveness of propose method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper is well structured. The presentation is clear and easy to understand.\n2.\tThe novelty of the paper is good, The designs are motivated well and intuitive is good.\n3.\tMAE was used to improve the performance of map construction.\n4.\tThe experiments are extensive, although some necessary experiments are missing."
                },
                "weaknesses": {
                    "value": "1.\tThe benchmark is not compared reasonable. Comparison with recent advance works is needed.\n2.\tSome parts of the proposed method is not clarified clearly, such as the HDMap Refinement Module.\n3.\tThe performance of run time is not competitive.\n4.\tUtilizing pretrained MAE as second-stage refinement is interesting. However, its generalization as a pretrained model is more worthy of exploration."
                },
                "questions": {
                    "value": "1.\tIt is reasonable to compare with recent works with advance performance (e.g. [1], [2])\n2.\tSome parts of the proposed method is not clarified clearly, such as the HDMap Refinement Module. Please introduce more details about it. How can it refine the initial predictions with absent sidewalks and broken lane lines? Are there any insights?\n3.\tThe performance of run time seems not competitive. Please explain about that.\n4.\tUtilizing pretrained MAE as second-stage refinement is interesting. However, its generalization as a pretrained model is more worthy of exploration. The reviewer wonder how it works when it come to other dataset, e.g., pretrained on nuscenes while inferenced on Ago.\nPlease explain my concerns and modify the manuscript according to the negatives. If all my concerns are well addressed, I will raise my score.\n[1] Liao B, Chen S, Wang X, et al. Maptr: Structured modeling and learning for online vectorized hd map construction[J]. arXiv preprint arXiv:2208.14437, 2022.\n[2] Liao B, Chen S, Zhang Y, et al. Maptrv2: An end-to-end framework for online vectorized hd map construction[J]. arXiv preprint arXiv:2308.05736, 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3411/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699264593990,
            "cdate": 1699264593990,
            "tmdate": 1699636292385,
            "mdate": 1699636292385,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "knzWhXw5bg",
                "forum": "lgDrVM9Rpx",
                "replyto": "cpRNHRiYOd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3411/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3411/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, thank you for the professional feedbacks. We have updated the manuscript and marked the changes in red. Here are my answers to your related questions.\n## Weaknesses.1 & Questions.1:\n\nWe further demonstrate the effectiveness of SD map prior by incorporating it into the state-of-the-art vectorized map constructor MapTR. The results are presented in **Appendix A.1** and **Fig.5**. It shows that the SD map prior can also improve the performance of MapTR on various ranges. Since the MAE HD map modelling method does not apply to MapTR, it is not evaluated.\n\nMeanwhile, we would like to point out that dense rasterized prediction on the BEV view still has advantages over direct vectorized map prediction methods. Vectorized representation is intrinsically troubled by control point selection ambiguity, which negatively impacts generalization in real-world autonomous driving applications. We believe in the future, rasterized map constructer and vectorized map constructor would co-exist in the academia and industry.\n| Range               | Method     | Divider | PedCrossing | Boundary | mAP                            |\n|---------------------|------------|---------|--------------|----------|--------------------------------|\n|    | MapTR      | 12.69   | 7.17         | 4.23     | 8.03                           |\n|  $240\\times 60m$   | MapTR-SDMap | **22.74** |  16.34    | 10.53    | 16.53 (+8.50)            |\n|                     | P-MapNet   | 14.51   | **25.63**        | **28.11** |  **22.75**                          |\n\n## Weaknesses.2 & Questions.2:\nA difference between our MAE and the conventional MAE may be the cause of this confusion. Specifically, a conventional MAE takes RGB image patches and directly regresses RGB image values of the whole image. Here, the input and output of our HDmap MAE are both rasterized segmentation masks. In our method, the output segmentation mask is supervised by a pixel-wise cross-entropy loss instead of the typical regression loss in a conventional MAE. As such our MAE can be naturally used for refinement during inference since the head is a segmentation head. To note, this segmentation mask MAE is exactly doing masked auto-encoding.\n\nWe have further refined the description and ablation experiments of the HDMap Prior Module, as detailed in **Section 4.3** and **Appendix B**. \n\n## Weaknesses.3 & Questions.3:\nRegarding the issue of model runtime, we conducted further experiments to break down the inference time and discovered that the refinement module consumes a significant amount of time. In fact, the HDMap Prior Module is an optional component and can be chosen based on specific requirements. The detailed experimental results are presented in **Table 13**.\n| Component     | Runtime (ms)     | Proportion | \n|---------------------|------------|---------|\n|  Image backbone  | 7.56      | 7.63%   | \n|  View transformation | 3.25 | 3.28% | \n|  Lidar backbone | 17.60  |17.76%  |\n|SDMap prior module|4.40|4.45%|\n|HDMap prior module|66.12|66.87%|\n|Total |98.87| 100%|\n## Weaknesses.4 & Questions.4:\nIndeed, we have also considered this aspect and supplemented our method with a cross-dataset experiment, detailed in **Appendix B.2** and **Table 11**. We initially pre-trained the HDMap prior module on the train split of the Argoverse 2 dataset and then tested it on nuScenes val dataset. This approach also yielded an improvement in the mIoU."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3411/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700163484785,
                "cdate": 1700163484785,
                "tmdate": 1700292589691,
                "mdate": 1700292589691,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]