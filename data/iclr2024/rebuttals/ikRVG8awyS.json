[
    {
        "title": "RFold: RNA Secondary Structure Prediction with Decoupled Optimization"
    },
    {
        "review": {
            "id": "deJkqDfrr8",
            "forum": "ikRVG8awyS",
            "replyto": "ikRVG8awyS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4346/Reviewer_G4jZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4346/Reviewer_G4jZ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose RFold, a simple yet effective RNA secondary structure prediction in an end-to-end manner. Speci\ufb01cally, a decoupled optimization process that decomposes the vanilla constraint satisfaction problem into row-wise and column-wise optimization is introduced to simplify the solving process while guaranteeing the validity of the output. Besides, RFlod adopts attention maps as informative representations to automatically learn the pair-wise interactions of the nucleotide bases instead of using hand-crafted features to perform data pre-processing."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The article closely links RNA-related issues with the ICLR community."
                },
                "weaknesses": {
                    "value": "1. Presentation:\n    - Grammar issues, such as in the sentence: \"The general deep-learning-based RNA secondary structure prediction methods into three key parts,\" which is unclear.\n    - Incorrect citation format; the ICLR official template recommends the use of \\citep, but it seems the author used \\cite, causing the citation information to mix with the text, which hampers readability.\n    - Inconsistent abbreviation of equations; some use \"Eq.\" while others use \"Equ.\" (e.g., in the last line of page 5).\n\n2. The experimental results are not sufficiently detailed. The article emphasizes the validity of the proposed method's prediction results but lacks validity results for the baselines."
                },
                "questions": {
                    "value": "1. I gave this article a rejection recommendation, largely due to its poor presentation. In addition to the specific issues mentioned in the Weaknesses section, the explanation of the method is not very clear. For example, I'm not very clear about how Eq. 9 is derived. From my understanding, $\\mathbf{M}$ should represent the ground-truth, and $S_r, S_c$ constitute the decomposition of M. Then, is there any necessity to optimize $S_r$ and $S_c$?\n2. In Equation 13, there is no rigorous explanation provided for why the Row-Col Softmax function is used to approximate the Row-Col Argmax function for training.\n\nIf the author can effectively address my concerns, especially those related to presentation, I will reconsider this article and possibly revise my evaluation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698109802374,
            "cdate": 1698109802374,
            "tmdate": 1699636405614,
            "mdate": 1699636405614,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QX32PYVbW1",
                "forum": "ikRVG8awyS",
                "replyto": "deJkqDfrr8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer G4jZ,\n\nThank you for your professional comments. We apologize for the issues in the presentation. We have addressed these issues in the revised manuscript, highlighting the changes in blue font.\n\n**Q1** The validity results.\n \n**A1** Thank you for your kind suggestion. In the ablation study, we compared different post-processing strategies, as shown in Table 7. To eliminate the influence of the backbone model and preprocessing methods, we used RFold as an example. We compared RFold-E, which employs an unrolled algorithm, with RFold-S, which does not use any post-processing. We found that the validity of RFold's post-processing strategies is significantly better than that of others.\n\n**Q2** The explanation of the method.\n\n**A2** Yes, $\\boldsymbol{M}$ represents the ground-truth, and $S_r, S_c$ constitute the decomposition of $\\boldsymbol{M}$. However, directly optimizing for the final solution is infeasible. Our proposed decoupled optimization approach utilizes the symmetry of the matrix $\\widehat{H}$ to decouple the optimization into row-wise and column-wise components. Optimizing $S_r, S_c$\u200b separately allows for a more manageable and efficient computation process.\n\n**Q3** Why the Row-Col Softmax function is used to approximate the Row-Col Argmax function for training?\n\n**A3** Thank you for your insightful comment. The use of the Row-Col Softmax function to approximate the Row-Col Argmax function in the RFold model for RNA secondary structure prediction is primarily due to the differentiability requirements of the training process. \n\nThe Argmax function, which is used in the inference phase of RFold, is non-differentiable. On the other hand, the Softmax function is a differentiable approximation of the Argmax function. It provides a \"soft\" version of the maximum, assigning probabilities to each element in such a way that larger values get higher probabilities\u3002The Row-Col Softmax function serves as a differentiable proxy during training. It allows the model to learn the appropriate parameters by providing gradient information which is not possible with the Row-Col Argmax function.\n\nOnce the model is trained, during the inference phase, the Row-Col Softmax function can be replaced with the Row-Col Argmax function. This is because inference does not involve backpropagation or gradient computations, and the Argmax function provides a clear, decisive prediction that is necessary for the final RNA structure prediction."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699873504439,
                "cdate": 1699873504439,
                "tmdate": 1699873504439,
                "mdate": 1699873504439,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ys97RBnBN7",
            "forum": "ikRVG8awyS",
            "replyto": "ikRVG8awyS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4346/Reviewer_BXWa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4346/Reviewer_BXWa"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes RFold, a method for RNA secondary structure prediction. RFold introduces decoupled optimization that decomposes the constraint satisfaction problem into separate row-wise and column-wise optimizations. This simplifies optimization while guaranteeing valid outputs. RFold also employs Seq2map attention to automatically learn sequence representations, avoiding manual feature engineering. Experiments show RFold matches state-of-the-art approaches on benchmarks while having faster inference. Ablations validate the optimization and attention contributions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strength of RFold is its simple yet effective approach for RNA structure prediction. The post-processing step simplifies satisfying complex structural constraints. The pre-processing step automatically learns useful sequence representations without feature engineering. Together these enable high performance prediction with fast inference. Rigorous experiments demonstrate RFold matches or exceeds state-of-the-art methods across multiple benchmarks. The ablation studies clearly validate the benefits of the proposed techniques."
                },
                "weaknesses": {
                    "value": "A key weakness is the technical approaches seem incremental, lacking major deep learning innovations. The decoupled optimization and Seq2map attention offer straightforward extensions to UFold. This suggests the work may be better suited for a venue focused on the specific domain."
                },
                "questions": {
                    "value": "How is Equation 10 obtained with the proposed decoupled optimization?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4346/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4346/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4346/Reviewer_BXWa"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809905240,
            "cdate": 1698809905240,
            "tmdate": 1700717406823,
            "mdate": 1700717406823,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8iU1GP9uLx",
                "forum": "ikRVG8awyS",
                "replyto": "ys97RBnBN7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer BXWa,\n\nThank you for your constructive comments!\n\n**Q1** A key weakness is the technical approaches seem incremental, lacking major deep learning innovations. The decoupled optimization and Seq2map attention offer straightforward extensions to UFold. This suggests the work may be better suited for a venue focused on the specific domain.\n\n**A1** **The seminal work, E2Efold [1], was accepted by ICLR 2020 as one of the 48 oral presentations.** As shown in the methodology comparison table below, compared to SPOT-RNA, E2Efold modified the backbone model to a Transformer and introduced an unrolling algorithm for post-processing. Although promising, E2Efold did not perfectly address the problem of three constraints.\n\nBy employing novel decoupled optimization, **our proposed RFold is the first work to fully satisfy all constraints**. Furthermore, while UFold attempted to enhance performance with more complex pre-processing, our approach utilizes a straightforward seq2map attention to automatically capture pairwise features, thereby improving inference speed. We respect previous works, but believe RFold offers unique contributions.\n\n| Method    | SPOT-RNA      | E2Efold               | UFold                 | RFold                 |\n|----------------------------------|---------------|-----------------------|-----------------------|-----------------------|\n| pre-processing                   | pairwise concat | pairwise concat       | hand-crafted          | seq2map attention     |\n| backbone model                   | ResNet + LSTM | Transformer           | U-Net                 | U-Net                 |\n| post-processing                  | \u00d7             | unrolled algorithm    | unrolled algorithm    | decoupled optimization|\n| constraint (a)                   | \u00d7             | \u2714                     | \u2714                     | \u2714                     |\n| constraint (b)                   | \u00d7             | \u2714                     | \u2714                     | \u2714                     |\n| constraint (c)                   | \u00d7             | \u00d7                     | \u00d7                     | \u2714                     |\n| RNAStralign F1 score | 0.711         | 0.821                 | 0.915                 | **0.977**             |\n| Inference time                   | 77.80 s       | 0.40 s                | 0.16 s                | **0.02** s            |\n\n\n**Q2** How is Equation 10 obtained with the proposed decoupled optimization?\n\n**A2** Our proposed decoupled optimization approach in RFold simplifies the complex task of RNA secondary structure prediction by dividing it into separate row-wise and column-wise optimizations. This method effectively manages the complexity by handling these aspects independently and then integrating them to form a comprehensive solution. The process relies on the symmetry of the matrix $\\widehat{H}$. We utilize Argmax functions for each sub-problem due to their suitability in selecting maximal values while maintaining structure constraints, though they are not the sole method available for such optimizations.\n\n[1] RNA Secondary Structure Prediction By Learning Unrolled Algorithms, ICLR 2020."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699873466248,
                "cdate": 1699873466248,
                "tmdate": 1699873466248,
                "mdate": 1699873466248,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wBNl82vbWh",
            "forum": "ikRVG8awyS",
            "replyto": "ikRVG8awyS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4346/Reviewer_Y2Rp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4346/Reviewer_Y2Rp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a decoupled optimization approach for the RNA folding problem. Through decoupled optimization formulation of the mapping matrix learning, the RNA folding prediction becomes much faster. Comprehensive experiments demonstrate the superiority of the proposed method compared to bunch of baseline methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The writing is well-organized and the core idea is clearly presented. In general, although this reviewer has not read too much RNA paper before, the problem formulation and key algorithm design can be easily captured;\n\n(2) This paper conducts very comprehensive experiments to demonstrate the effectiveness of the proposed approach. This reviewer firmly believes the proposed method can bring some improvements to this domain of research."
                },
                "weaknesses": {
                    "value": "It seems that the major point of the proposed method is mainly about the optimization formulation of the assignment matrix learning while the seq2map network architecture and the mapping matrix formulation are mainly inherited from previous works. So probably the novelty of the proposed method is a little bit limited. However, this reviewer is not very familiar with RNA folding frontier research, thus it is not very fair for this reviewer to evaluate its novelty and significance."
                },
                "questions": {
                    "value": "Does the Rol-Column-Softmax operation have a training instability issue? Since this operation is very similar to the Sinkhorn operation and the Sinkhorn algorithm has training stability issues. This reviewer afraid that the proposed method also has the same drawback."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4346/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4346/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4346/Reviewer_Y2Rp"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836157456,
            "cdate": 1698836157456,
            "tmdate": 1699636405116,
            "mdate": 1699636405116,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TgmksTLVTg",
                "forum": "ikRVG8awyS",
                "replyto": "wBNl82vbWh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer Y2Rp,\n\nThank you for your thoughtful and inspiring comment!\n\n**Q1** The proposed method is mainly about the optimization formulation of the assignment matrix learning while the seq2map network architecture and the mapping matrix formulation are mainly inherited from previous works.\n\n**A1** Thank you for your meticulous review! We would like to claim that both the optimization formulation and the seq2map attention are our main contributions. As stated in the introduction, we decompose deep learning-based RNA secondary structure prediction methods into three key components: preprocessing, the backbone model, and post-processing.\n* The pre-processing step means projecting the 1D sequence into 2D matrix. (**1D -> discrete 2D**)\n* The backbone model learns from the 2D matrix and then outputs a hidden matrix of continuous values. (**discrete 2D -> continuous 2D**)\n* The post-processing step converts the hidden matrix into a contact map, which is a matrix of discrete 0/1 values. (**continuous 2D -> discrete 2D**) \n\nThe methodology comparison is shown in the table below:\n\n| Method                           | SPOT-RNA      | E2Efold               | UFold                 | RFold                 |\n|----------------------------------|---------------|-----------------------|-----------------------|-----------------------|\n| pre-processing                   | pairwise concat | pairwise concat       | hand-crafted          | seq2map attention     |\n| backbone model                   | ResNet + LSTM | Transformer           | U-Net                 | U-Net                 |\n| post-processing                  | \u00d7             | unrolled algorithm    | unrolled algorithm    | decoupled optimization|\n| constraint (a)                   | \u00d7             | \u2714                     | \u2714                     | \u2714                     |\n| constraint (b)                   | \u00d7             | \u2714                     | \u2714                     | \u2714                     |\n| constraint (c)                   | \u00d7             | \u00d7                     | \u00d7                     | \u2714                     |\n| RNAStralign F1 score | 0.711         | 0.821                 | 0.915                 | **0.977**             |\n| Inference time                   | 77.80 s       | 0.40 s                | 0.16 s                | **0.02** s            |\n\nIt can be observed that the only similarity between RFold and previous methods is the use of U-Net as the backbone model, similar to UFold. Furthermore, the most significant difference of RFold compared to earlier methods is that its predictions for secondary structures are guaranteed to satisfy all three constraints.\n\n\n**Q2** Does the Rol-Column-Softmax operation have a training instability issue?\n\n**A2** Thank you for your insightful question! We have added the loss curve plot in **Appendix G of the revised manuscript**. It can be observed that the training loss decreased steadily across the training epochs. The possible reasons for this may include: (i) The Softmax function, used in the Row-Column-Softmax, is generally numerically stable; (ii) The objective of the Row-Column-Softmax is not to make the matrix doubly stochastic (as in the Sinkhorn algorithm) but to approximate the Row-Col-Argmax operation in a differentiable manner for training purposes."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699873431045,
                "cdate": 1699873431045,
                "tmdate": 1699873431045,
                "mdate": 1699873431045,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DgiVUI5rmr",
            "forum": "ikRVG8awyS",
            "replyto": "ikRVG8awyS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4346/Reviewer_fGRe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4346/Reviewer_fGRe"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an end-to-end deep learning pipeline for RNA secondary structure prediction from the input 1D sequence. Compared to previous deep learning pipelines, the proposed one simplifies the manually-crafted data pre-processing step with the learned representation and the result post-processing step through reformulating and approximating the loss function. On standard benchmarks, the proposed method significantly outperforms current SOTA methods in both accuracy and speed. The ablation studies clearly demonstrate the effectiveness of the proposed design choices."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The paper is well-written. The \"preliminary\" section clearly summarizing existing works and the figures are helpful for understanding.\n\n- The proposed method has a simple yet effective design.\n\n- The proposed method significantly outperforms the previous SOTA on the standard benchmarks.\n\n- The ablation studies are helpful to verify the design choices for the data pre-processing and result post-processing."
                },
                "weaknesses": {
                    "value": "On the high-level, the proposed method employs a simple trick to decode M from H: compute the row-argmax and col-argmax, and then Hadamard product them, where the output is guaranteed to be a symmetric 0-1 matrix satisfying the constraints. Overall, the paper does a good job to justify this simple approach by reformulating the optimization objective and approximating it to the train deep learning model. However, there are some details that may need modification.\n\n- The solution to Eq (6) is not Eq (10). Here, H is the unconstrained output and \\hat H can have negative values. (1) If \\hat H is all non-positive (0 on the diagonal), the optimal solution to Eq (6) is M=0. (2) Even if \\hat H is non-negative, consider \\hat H = [5,4;4,1]. The row-col-argmax(\\hat H) = [1,0;0,0] while the optimal for Eq (6) is [1,0;0,1].\n\n- Suppose Eq (10) is correct, it is unclear how good is the approximation in Eq (14). (1) It'll be great to compare with the approximation (row-softmax \\odot col-softmax). (2) argmax can been seen as softmax with temperature T->0. Current approximation uses T=1. It'll be great to add an ablation study to examine how good is the softmax approximation.\n\n--------------\nMinor\n- Eq (1): the order of composition should be swapped. F(x) = G[H(x)]. => F = G\\circ H"
                },
                "questions": {
                    "value": "- Under Eq (9), what does \\otimes refer to? Are S_r and S_c sets of vectors? Maybe better to define S_r and S_c as matrices and use Hadamard product.\n- I really like the paper, but the soundness of the formulation and approximation slightly make me concern as pointed out in the weakness section. I'd like to hear authors' explanation on them. One simple change is to tune down the claim \"optimal solution\" to \"a greedy algorithm solution\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699132822683,
            "cdate": 1699132822683,
            "tmdate": 1699636405043,
            "mdate": 1699636405043,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YNi0o5Ee1K",
                "forum": "ikRVG8awyS",
                "replyto": "DgiVUI5rmr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer fGRe,\n\nWe sincerely appreciate your careful and insightful comments! \n\n**Q1** $\\boldsymbol{\\widehat{H}}$ can have negative values?\n\n**A1** Thank you again for your careful review! We have defined $\\boldsymbol{\\widehat{H}} = (\\boldsymbol{H} \\odot \\boldsymbol{H}^T) \\odot \\boldsymbol{\\bar{M}}$ below Eq.6. The element-wise multiplication $\\boldsymbol{H} \\odot \\boldsymbol{H}^T$ results in a matrix where each element is the square of the corresponding element in $\\boldsymbol{H}$. Since squaring any real number results in a non-negative value, this operation yields a matrix with non-negative elements. $\\boldsymbol{\\bar{M}} \\in \\{0,1\\}$ contains only non-negative values, typical for a constraint matrix, often containing 0s and 1s to indicate the presence or absence of constraints. Thus, the final element-wise multiplication in $\\boldsymbol{\\widehat{H}}$ does not introduce any negative values.\n\n**Q2** The row-col-argmax($\\boldsymbol{\\widehat{H}}$) may be not optimal.\n\n**A2** The example is interesting. However, considering $\\boldsymbol{\\widehat{H}} = (\\boldsymbol{H} \\odot \\boldsymbol{H}^T) \\odot \\boldsymbol{\\bar{M}}$ and $\\boldsymbol{\\bar{M}}$ is defined as $\\boldsymbol{\\bar{M}}_{ij}:=1$ if $x_i x_j \\in \\mathcal{B}$ and $|i-j| \\geq 4$. This example is not suitable for our case because it excludes the diagonal.\n\n**Q3** How good is the approximation in Eq (14)?\n\n**A3** Your understanding that row-col softmax is an approximation of row-col argmax is definitely correct. We should use row-column argmax because the predicted contact matrix is meant to be discrete. However, row-column argmax is not differentiable, which is why we use row-column softmax in the training phase for differentiable optimization.\n\n**Q4** Eq (1): the order of composition should be swapped. F(x) = G[H(x)]. => F = G\\circ H.\n\n**A4** Thanks for your careful review. We have fixed this issue in the revised manuscript.\n\n**Q5** Under Eq (9), what does \\otimes refer to? Are $S_r$ and $S_c$ sets of vectors? Maybe better to define $S_r$ and $S_c$ as matrices and use Hadamard product.\n\n**A5** We are sorry for the confusion. In our context, $S_r$ and $S_c$ represent vectors, and the symbol $\\otimes$ denotes the Hadamard product. The formulation has been refined in the revised manuscript for greater clarity.\n\n**Q6** Tune down the claim \"optimal solution\" to \"a greedy algorithm solution\".\n\n**A6** Thank you for your insightful suggestion. We agree that 'a greedy algorithm solution' is suitable because it allows us to decompose the problem into row-wise and column-wise sub-problems. We have refined the description in the revised manuscript."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699873372198,
                "cdate": 1699873372198,
                "tmdate": 1699873372198,
                "mdate": 1699873372198,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]