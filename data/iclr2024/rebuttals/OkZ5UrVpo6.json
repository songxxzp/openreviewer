[
    {
        "title": "Traceable Federated Continual Learning"
    },
    {
        "review": {
            "id": "6yxSV5Kti8",
            "forum": "OkZ5UrVpo6",
            "replyto": "OkZ5UrVpo6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4507/Reviewer_drQ8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4507/Reviewer_drQ8"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript introduces a specific sub-category of federated continual learning where, tasks are repeatable and evoke the necessity to handle such conditions in practical scenarios.\nWith this setup in mind, the paper introduces TagFed, a framework that actively traces tasks and ensures the usage and optimization of dedicated sub-models for each task.\nThe authors use a novel benchmark to evaluate the new paradigm in federated continual learning and TagFed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The authors contextualize their work extremely well, highlighting important issues and assumption made in federated continual learning.\n- Given the contributions, the experimental validation is very thorough.\n- Detailed descriptions of the setup and highlights of important takeaways from the experiments offer a good reading experience to the user."
                },
                "weaknesses": {
                    "value": "- TagFed uses weight-based pruning to obtain task-specific sub-models and then uses distillation to ensure that appropriate server and client models can distill and learn from one another. However, the specific type of clustering/aggregation performed and at which point in the server and client they are performed remain slightly obscure.\n- Since TagFed maintains a copy of weights for each task, with an increase in the number of tasks there would be a large memory overhead within each client, especially if we consider the worst case scenario of every client observing all the available set of tasks.\n- The notion of cross-entropy loss at the server level is confusing since there was no clear definition of a public dataset at the server level.\n- Since traceability is argued to be critical to the functioning of TagFed under the proposed FCL setting, explicit evaluation of the task detection mechanism against GLFC's \"Task Transition Detection\" measure would add a more straightforward comparison and deepen the discussion on the importance of this measure."
                },
                "questions": {
                    "value": "- Could the authors detail the exact (a) nature of clustering/identification of sub-tasks, and (b) whether are they performed at both the server and client side?\n- Given the repetitive pruning mechanism used to obtain task-specific sub-models, could the authors discuss whether they observe a drop in effective capacity with an increase in the number of classes/tasks?\n- Could the authors confirm whether the intuition of memory overhead, as stated in the weaknesses, matches the expectation from TagFed with an increase in the number of tasks? If so, do you have a solution to avoid storing multiple such copies?\n- Does TagFed use a public dataset at the server level? If not, how is ground-truth obtained for the cross-entropy loss? If there is a public dataset, could you describe the specifics of which dataset was used and how?\n- Could the author's compare their approach against GLFC's \"Task Transition Detection\" measure, purely from a task detection capability standpoint?\n- From an ablation standpoint, could the authors provide more in-depth descriptions of how w/o TTL and w/o GKA experiments are set up?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4507/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698689561325,
            "cdate": 1698689561325,
            "tmdate": 1699636427144,
            "mdate": 1699636427144,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "KwniTGfF7i",
            "forum": "OkZ5UrVpo6",
            "replyto": "OkZ5UrVpo6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4507/Reviewer_oWSe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4507/Reviewer_oWSe"
            ],
            "content": {
                "summary": {
                    "value": "The authors developed an approach called traceable federated continual learning (TFCL) for federated learning settings where the model sees a few tasks sequentially with repetition. The TFCL approach is composed of pruning-based sub-model generation and server-client knowledge distillation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper studies an interesting setting: continual learning with repetitive tasks."
                },
                "weaknesses": {
                    "value": "1. If I understand Section 4.2 correctly, the traceable augmentation simply duplicates the model a few times and lets each copy deal with a task. This seems to be a baseline implementation.\n2. If we decide to use separate models for each task, the pruning-based sub-model generation does not seem necessary. Although pruning may reduce computation and storage costs, that is not closely related to the problem in this paper.\n3. If the pruning-based sub-model generation is not necessary, there is no need to use a knowledge distillation framework to aggregate models. The knowledge distillation framework enables the aggregation of heterogeneous models. However, the model heterogeneity is introduced by an unnecessary sub-model generation operation."
                },
                "questions": {
                    "value": "What does \"$p_q = p_i = ... = p_j$\" in Section 4.2 imply?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4507/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791720606,
            "cdate": 1698791720606,
            "tmdate": 1699636426989,
            "mdate": 1699636426989,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "f5sljej0lw",
            "forum": "OkZ5UrVpo6",
            "replyto": "OkZ5UrVpo6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4507/Reviewer_1NM3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4507/Reviewer_1NM3"
            ],
            "content": {
                "summary": {
                    "value": "This paper focused on federated continual learning, and proposed Tractable Federated Continual Learning (TFCL), where repetitive tasks can be accurately and effectively traced and processed to boost the performance. In particular, the proposed TagFed framework has been implemented to achieve TFCL through feature tracing, submodel augmentation and group-level knowledge federation. Experimental results were provided to validate the performance of TagFed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposed TagFed for Tractable Federated Continual Learning (TFCL). Specifically, TagFed includes feature tracing, submodel augmentation and group-level knowledge federation. Experimental results were provided to validate the performance of TagFed."
                },
                "weaknesses": {
                    "value": "1. In order to implement TagFed, it needs to check whether or not the current task is a repetitive task. This is done through tracing and connecting to previous tasks. As a result, there is a natural tradeoff between performance and costs in terms of memory. If TagFed stores the previous tasks over a very long period, it will benefit TagFed which tasks advantage of repetitive tasks. However, this will need a lot of memory as well. Though there are some discussions in this paper. It is not clear to the reviewer how TagFed fully addresses this issue. Do you leverage a \u201ctime-window\u201d to determine \u201cthe length of the time periods\u201d for the previous tasks to be stored? Is it a sliding window? How to determine the length of such a window? A fixed or adaptive value of the window size?\n2. Following the above question, in Table 3, the authors present some numerical results on the complexity of TagFed and baselines. There is no clear statement about the experimental settings (neither in the appendix), and hence it is not easy to justify how \u201cgood\u201d or \u201cbad\u201d these values. For instance, 11.7MB can be a very large or small cost depending on the system configuration. No memory cost is provided. \n3. In Section 3.1, the authors claimed that recent works did not consider the data repeatability in a task sequence. However, cannot they still be applied in the presence of data repeatability? These recent works are designed for general task sequences which include the repeatable ones. \n4. This paper leverages several state-of-the-art datasets for image classification tasks, which are widely used in the FL setting. However, in general, these datasets do not this data repeatability property. How do you generate the data repeatability cases from these datasets?\n5. In Section 3.2, \u201cFor a certain time\u201d, this is a quite myopic description. How frequent is the repeatability? what\u2019s its impact on the performance? \n6. To achieve the incremental training for non-repetitive new tasks, a threshold is needed. However, the reviewer did not find information on how to determine this threshold, what\u2019s impact on the performance, do you need to tune this value? No ablation study was provided. \n7. A minor question: for the traceable augmentation for repetitive new tasks, it seems that it requires the tasks to repeat at least twice (therefore the value of j can be meaningful). The implementation indeed requires the information of task p_j. However, what will happen if there is no such j? i.e., only repeat once? \n8. The proposed TagFed is most heuristic based method without strong theoretical performance guarantee (E.g., convergence analysis)."
                },
                "questions": {
                    "value": "See the comments above in weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4507/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4507/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4507/Reviewer_1NM3"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4507/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799566979,
            "cdate": 1698799566979,
            "tmdate": 1699636426900,
            "mdate": 1699636426900,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "8B1NUj9EFz",
            "forum": "OkZ5UrVpo6",
            "replyto": "OkZ5UrVpo6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4507/Reviewer_MVcU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4507/Reviewer_MVcU"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose a new paradigm, namely Traceable Federated Continual Learning (TFCL), aiming to cope with repetitive tasks by tracing and augmenting them. Following the new paradigm, they further develop TagFed, a framework that enables accurate and effective Tracing, augmentation, and Federation for TFCL."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper proposes a new paradigm: Traceable Federated Continual Learning (TFCL), which copes with repetitive tasks by tracing and augmenting them. \n\n2.  Following the new paradigm, this paper develops a framework that enables accurate and effective tracing, tracing, augmentation, and federation for TFCL.\n\n3. Results are promising"
                },
                "weaknesses": {
                    "value": "I wonder the motivation of this setting. Leave federated learning alone, let us focus on the continual learning part, I wonder if there are any previous works in the continual learning research community that study the case proposed in this paper? (i.e., the upcoming task data are NOT completely different from previous tasks). From my understanding, this setting makes the modification from the perspective of continual learning.\n\nIf there are, have you combined them into your setting to setup a series of simple baselines? For instance, combining federated algorithms like FedAVG with the continual learning algorithms.\n\nAlso, have you tried to compare with recent related works on FCL? For insrance [1,2], which do not required task identifiers. Can you clarify and discuss about them?\n\nMissing related works:\n[1] S Babakniya et al., Don't Memorize; Mimic The Past: Federated Class Incremental Learning Without Episodic Memory\n[2] D Qi et al., Better generative replay for continual federated learning"
                },
                "questions": {
                    "value": "Please refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4507/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810398051,
            "cdate": 1698810398051,
            "tmdate": 1699636426793,
            "mdate": 1699636426793,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]