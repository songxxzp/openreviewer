[
    {
        "title": "NOISY MULTI-VIEW CONTRASTIVE LEARNING FRAMEWORK FOR ENHANCING TOP-K RECOMMENDATION"
    },
    {
        "review": {
            "id": "HKwZ8IhUCY",
            "forum": "er7VhmqZEA",
            "replyto": "er7VhmqZEA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7168/Reviewer_g3vX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7168/Reviewer_g3vX"
            ],
            "content": {
                "summary": {
                    "value": "To improve the data sparsity issue, this paper proposes a multi-view contrastive learning framework for knowledge-aware recommendation. The proposed model generates multiple modeling views including a collaborative view, a semantic view, and a structural view. The three views are constructed by utilizing part of the heterogeneous data combining the user-item interaction information and the item-entity knowledge information. Then two contrastive learning losses, and an alignment and uniformity loss is applied for supervision enhancement. Experiments on ML-100K and ML-1M validate the effectiveness of the proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Clear presentation. The paper is well-writen with good illustration figures. The introduction clearly highlights the major research motivation and key contributions of this paper. And it has a clear structure to introduce the view generation part and the contrastive learning part, respectively. I find it easy to follow.\n- Important research topic. The paper targets an important research topic, namely self-supervised learning for knowledge-aware recommender systems.\n- Technical design. The contrastive learning method adopted in this paper is conducted from multiple dimensions, including the cross-view contrastive learning, and the alignment and uniformity constraints."
                },
                "weaknesses": {
                    "value": "- Limited novelty. There have already been some self-supervised learning approaches proposed for knowledge-aware recommendation (e.g. KGCL [1], KGIC [2], KACL [3]). In terms of view generation and contrastive constraints, this paper does not make sufficiently innovative contribution to this topic compared to the existing works.\n- Insufficient experiments. i) The empirical study is conducted on two small-size datasets that are not aligned with the recent studies on knowledge-aware recommendation. ii) The paper does not involve the existing contrastive KG recommendation methods as baselines. iii) There is only the overall performance comparison. There lack other experiments such as ablation study, hyperparameter study, anti-noise investigation, for a comprehensive empirical study.\n- Important part of methodology is not clearly explained. I cannot find the specific definitions for some of the self-supervised learning loss terms, such as $\\mathcal{L}_u^g$, $\\mathcal{L}_i^g$, $\\mathcal{L}_u^l$, $\\mathcal{L}_i^l$, and $L_{local}$. Please correct me if I am wrong.\n\nMinor mistakes: \n- In the task formulation section, there should be braces in the definitions for user/item sets, the knowledge graph, and so on.\n- LightGCN is the original model name. Adding a hyphen in the name (Light-GCN) may cause confusion. If the used GCN architecture is not exactly LightGCN, using other expressions like light-weight GCN would be better.\n- The paper utilizes $\\mathcal{L}$ for loss terms in most cases, but sometimes $L$ is used. The notations could be better if unified.\n- Typo: \"This results ...\" in page 5\n\n[1] Knowledge Graph Contrastive Learning for Recommendation\n\n[2] Improving Knowledge-aware Recommendation with Multi-level Interactive Contrastive Learning\n\n[3] Knowledge-Adaptive Contrastive Learning for Recommendation"
                },
                "questions": {
                    "value": "- What are the major improvements brought by the proposed method, in comparison to the existing CL methods for KG recommendation listed above?\n- How to calculate the CL loss terms in detail?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7168/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7168/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7168/Reviewer_g3vX"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7168/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698481375461,
            "cdate": 1698481375461,
            "tmdate": 1699636849849,
            "mdate": 1699636849849,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "wGUuagplmP",
            "forum": "er7VhmqZEA",
            "replyto": "er7VhmqZEA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7168/Reviewer_1tBK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7168/Reviewer_1tBK"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript proposes to use contrastive learning for enhancing top-k recommendation. Alignment and uniformity constraint module are introduced to both the global and local parts. Experimental results on two datasets show that the proposed NMCLK outperforms previous methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper is written clearly and easily understandable.\nThe experimental results seem to be good on two datasets."
                },
                "weaknesses": {
                    "value": "The main contribution of this manuscript is introducing self-supervised learning methods which are commonly used in CV and NLP to the recommendation field. I think the novelty is not enough for an ICLR paper.\nThe contrastive learning is to make features from the same field similar and in contrast, with large distances for disparate ones. Is that always true in recommendation?\nThe ablation studies are not convincing, the authors should conduct experiments with and without each proposed component."
                },
                "questions": {
                    "value": "See the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7168/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7168/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7168/Reviewer_1tBK"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7168/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761934711,
            "cdate": 1698761934711,
            "tmdate": 1699636849730,
            "mdate": 1699636849730,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "pUzhd8jhea",
            "forum": "er7VhmqZEA",
            "replyto": "er7VhmqZEA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7168/Reviewer_8SVs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7168/Reviewer_8SVs"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel Noisy Multi-view Contrastive Learning framework for Knowledge-aware recommender systems (NMCLK). NMCLK generates three different views over user-item interactions and knowledge graphs, and further introduces a noise addition module to improve model robustness. The three views include a global-level structural view, a local-level user-item view, and an item-item semantic view. NMCLK also utilizes representation loss and uniformity loss to enhance the quality of the learned user-item representations. Experimental results on two movie recommendation datasets demonstrate that the proposed method outperforms state-of-the-art approaches."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- NMCLK is able to outperform a number of baseline approaches on two movie recommendation datasets."
                },
                "weaknesses": {
                    "value": "- The novelty of the paper is limited. The authors should derive more insights in the alignment and uniformity constraints in contrastive learning, and maybe design a better contrastive module.\n- The design choice of the contrastive module is not explained. In addition, the readers cannot know how and why the multi-view framework works, and cannot see the performance improvement after using the multi-view framework.\n- The model is only evaluated on NMCLK on two movie recommendation datasets, and it is questionable whether the performance of NMCLK will generalize to other recommendation domains.\n- No ablation study is conducted to verify the effectiveness of the introduced modules, e.g., the noise module and the contrastive learning module."
                },
                "questions": {
                    "value": "- wu2022noisytune and simgcl are not properly cited.\n- Section 5.1.1 says \"Table 1 displays the statistics of the three datasets mentioned above\", while only two datasets are used."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7168/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788069292,
            "cdate": 1698788069292,
            "tmdate": 1699636849618,
            "mdate": 1699636849618,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "ySOJRAnI34",
            "forum": "er7VhmqZEA",
            "replyto": "er7VhmqZEA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7168/Reviewer_X5Ei"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7168/Reviewer_X5Ei"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to utilize multi-view contrastive learning to enhance the recommender system. The proposed model based on self-supervised learning can aggregate the information from item knowledge graph, item similarity and historic records. Extensive experiments verify the effectiveness of the proposed model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe writing quality of this paper is high.\n2.\tThis paper introduces a novel contrastive learning model to enhance the CTR recommendation task."
                },
                "weaknesses": {
                    "value": "1.\tThis paper lacks theoretical analysis to clarify how each contrastive learning module benefits the final CTR task.\n2.\tIt would be beneficial to test the proposed model in various datasets across different domains, not just in the movie domain.\n3.\tA comprehensive ablation study is necessary to clarify the effectiveness of each proposed module, including the contrastive module and the feature alignment module.\n4.\tThere is a citation format error below Equation (2): \"Inspired by SimGCL (simgcl) and Noisytune wu2022noisytune papers' additions.\""
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7168/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699437288350,
            "cdate": 1699437288350,
            "tmdate": 1699636849501,
            "mdate": 1699636849501,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]