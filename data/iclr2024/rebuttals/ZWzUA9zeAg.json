[
    {
        "title": "Effective Data Augmentation With Diffusion Models"
    },
    {
        "review": {
            "id": "qCWlPODEya",
            "forum": "ZWzUA9zeAg",
            "replyto": "ZWzUA9zeAg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7589/Reviewer_5pTF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7589/Reviewer_5pTF"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed DA-Fusion, which uses an image-to-image diffusion model to generate new synthetic images to assist classification tasks. DA-Fusion utilizes Textual Inversion to learn the word embeddings of unseen concepts and employs the data balancing and random intensity trick to improve the augmentation results further. The empirical shows the effectiveness of DA-Fusion in the low-shot setting on common concepts, fine-grained concepts, and rare concepts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper shows positive empirical results on several classification tasks covering common, fine-grained, and rare concepts.\n\n- The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "(1) The technical contribution is limited. DA-Fusion combines several existing methods, like the image-to-image diffusion model, the Textual Inversion, and the data balancing technique.\n\n(2) DA-Fusion outperforms the much simpler RandAugment method mostly on extremely low-shot settings (less than 16 images per class). The improvements of DA-Fusion seem to be marginal when there are more than 16 images per class. Also, it is unfair to compare with RandAugment, which only uses the default hyperparameters, while DA-Fusion is \u201cfine-tuned\u201d on the target data (i.e., textual inversion, selecting $M$, and other parameters). The authors should search for the optimal number of operations and augmentation magnitude for RandAugment for a fairer comparison.\n\n(3) The authors tested their method on COCO and PASCAl VOC datasets for pure classification but not object detection. Being a general image-to-image diffusion model, the generation step in DA-Fusion may alter the position of the objects in input images. It seems that DA-Fusion can only be applied to classification tasks."
                },
                "questions": {
                    "value": "- The authors used a different number of augmented images per real image, $M$ ($M$=50 for spurge and $M$=10 for other data). Are there any guidelines for selecting $M$?\n\n- As pointed out in the weakness, the technical contribution of this work is slightly limited. It is recommended to compare DA-Fusion with more and stronger data augmentation baselines (instead of just RandAugment and Real Guidance) to claim a more significant empirical contribution."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7589/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7589/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7589/Reviewer_5pTF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7589/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697691485791,
            "cdate": 1697691485791,
            "tmdate": 1700539503404,
            "mdate": 1700539503404,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zXvbAHHeLX",
                "forum": "ZWzUA9zeAg",
                "replyto": "qCWlPODEya",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response To Reviewer 5pTF (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our paper and for your feedback. The points discussed in this review center around the strength of the technical contribution, the fairness of the evaluation, the selection of hyperparameters, and the applicability of the method beyond classification tasks. We address these and provide clarifications in the following rebuttal.\n\n# DA-Fusion\u2019s Technical Contribution\n\n__\u201cThe technical contribution is limited. DA-Fusion combines several existing methods, like the image-to-image diffusion model, the Textual Inversion, and the data balancing technique\u201d__\n\nDA-Fusion is a performant method for augmenting classification datasets. On seven tasks, we outperform RandAugment (Cubuk et al. 2019, https://arxiv.org/abs/1909.13719), a popular data augmentation technique, and Real Guidance (He et al., 2023, https://arxiv.org/abs/2210.07574), a state-of-the-art method for generative data augmentation presented at ICLR 2023. Our subsequent analysis in Figure 6 highlights a failure mode of the state-of-the-art method: it performs no better than RandAugment for fine-grain and unseen visual concepts. Addressing this failure mode is crucial for generative data augmentation so it performs well for all kinds of images, not just images with common concepts Stable Diffusion has seen in its training data.\n\nWe are contributing the first generative data-augmentation technique with strong performance on fine-grain and unseen visual concepts (see Figure 6 in the paper). The simplicity of the method can be viewed as a strength: it does not require specialized components for strong performance. Simplicity has the advantage of (1) fewer hyperparameters, (2) straightforward reproducibility, and (3) modularity. These are desirable traits for a data-augmentation technique."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700333841498,
                "cdate": 1700333841498,
                "tmdate": 1700333841498,
                "mdate": 1700333841498,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eJLgipiNT4",
                "forum": "ZWzUA9zeAg",
                "replyto": "qCWlPODEya",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response To Reviewer 5pTF (2/2)"
                    },
                    "comment": {
                        "value": "# Evaluation & Hyperparameter Selection\n\n__\u201cit is unfair to compare with RandAugment, which only uses the default hyperparameters, while DA-Fusion is fine-tuned\u201d__\n\nDA-Fusion and RandAugment use a fixed set of hyperparameters across all tasks in the paper (excluding Leafy Spurge, which we are updating in the manuscript per the additional experiment noted in a later response). We did not fine-tune the hyperparameters of DA-Fusion per task, and doing so for either method would risk biasing the evaluation. The hyperparameters of RandAugment were shown in Cubuk et al. 2019 (https://arxiv.org/abs/1909.13719) to work well across several standard datasets.\n\n__\u201cThe authors should search for the optimal number of operations and augmentation magnitude for RandAugment for a fairer comparison.\u201d__\n\nThis would risk unfairly favoring RandAugment in our evaluation. We use a uniform set of hyperparameters with DA-Fusion and do the same with RandAugment and Real Guidance. Note that standard data augmentation can be applied in parallel with DA-Fusion (see Figure 3 in the paper), and improvements in performance due to fine-tuning RandAugment per task would also improve the performance of DA-Fusion by a corresponding amount.\n\n__\u201cAre there any guidelines for selecting M?\u201d__\n\nWe are happy to include a discussion of the role and selection of M in the next revision of the manuscript. Introduced in Section 4.2, this hyperparameter controls the number of synthetic images generated per real image before training starts. The larger M, the higher the computational cost of DA-Fusion, but the higher performance can be attained. We ablate M in Figure 11 and find that our method is not sensitive to its value, observing that larger values of M perform marginally better than smaller values. M should be chosen based on the computational constraints of the user, but we find M=10 is an effective value across standard datasets.\n\n__\u201ccompare DA-Fusion with more and stronger data augmentation baselines (instead of just RandAugment and Real Guidance)\u201d__\n\nAt time of submission, the most recent and performant generative data augmentation is Real Guidance, adapted from He et al. 2023 (https://arxiv.org/abs/2210.07574). Being the state-of-the-art method, Real Guidance is the strongest and most relevant baseline to include. RandAugment is chosen for inclusion due to being recent, popular, and performant. We include an additional comparison against CutMix (Figure 12), another recent, popular, and performant data augmentation, and results are consistent with the existing comparison to RandAugment.\n\nIf you have any specific additional data augmentation baselines to compare to DA-Fusion, we are happy to include them in the next revision of the manuscript.\n\n__\u201cThe authors used a different number of augmented images per real image, M (M=50 for spurge and M=10 for other data).\u201d__\n\nWe have evaluated DA-Fusion on the new Leafy Spurge task with the same value of M as the rest of the datasets in Figure 5 and will add this in the manuscript. The new experiment is consistent with the existing results in the paper: that DA-Fusion continues to outperform RandAugment and Real Guidance. Results can be viewed at this anonymous link:\nhttps://drive.google.com/drive/folders/1cSPeNTOmZK-vTTrxF5xvtEtBtzrYRMXd?usp=sharing\n\n# Applicability For Object Detection\n\n__\u201cDA-Fusion may alter the position of the objects in input images \u2026 It seems that DA-Fusion can only be applied to classification tasks.\u201d__\n\nOur focus in this work is classification, but our public code supports masks for objects, allowing DA-Fusion to independently transform objects in the image, and the background. (This is the relevant section)[https://github.com/anonymous-da-fusion/da-fusion/blob/master/semantic_aug/augmentations/textual_inversion.py#L152 ] of the anonymous code. By using masks, DA-Fusion can preserve the location of objects in the image. Examples using DA-Fusion with masks to preserve the locations of objects are in the following anonymous google drive link. This functionality allows our method to be applied to datasets that require object locations to be preserved during augmentation, such as object detection and segmentation tasks. We are adding a section discussing this application of DA-Fusion in the appendix of the manuscript.\nhttps://drive.google.com/drive/folders/15O0HWkQsEskmzmI8F14ISf2xNdwOlKBG?usp=sharing"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700333906906,
                "cdate": 1700333906906,
                "tmdate": 1700334274765,
                "mdate": 1700334274765,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0NzDVVkzqp",
                "forum": "ZWzUA9zeAg",
                "replyto": "qCWlPODEya",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Following Up On The Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our manuscript, we would like to follow up on our rebuttal.  If there are any remaining points that you would like us to address, please let us know.  Otherwise, thank you for your review and we look forward to your response."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529777788,
                "cdate": 1700529777788,
                "tmdate": 1700529777788,
                "mdate": 1700529777788,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3SqCbNmY8V",
                "forum": "ZWzUA9zeAg",
                "replyto": "qCWlPODEya",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7589/Reviewer_5pTF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7589/Reviewer_5pTF"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. The authors addressed my concerns about the technical contribution and experiment settings. It is good that DA-Fusion can be applied to object detection tasks. Based on the additional studies, I am increasing my score from 5 to 6."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700539482331,
                "cdate": 1700539482331,
                "tmdate": 1700539482331,
                "mdate": 1700539482331,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jebeeNk7qD",
            "forum": "ZWzUA9zeAg",
            "replyto": "ZWzUA9zeAg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7589/Reviewer_hrLX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7589/Reviewer_hrLX"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose a diffusion model-based data augmentation technique for image classification. The method is based on a pretrained diffusion model. Textual inversion is applied to learn word embeddings for the classes in the target dataset for which the pretrained diffusion model may not learn before. During the generative process, real image is inserted at some time step to guide the generation. When training the target model, real image and synthetic image are mixed by probabilistic sampling. The paper further discuss some design choices for the proposed method, including the time step at which the real image is inserted during generation, strategies to prevent leakage of internet data, mixing ratio of real and synthetic images and the number of augmentations generated for each image. The proposed method is benchmarked on seven classification datasets and shown to outperform RandAugment and Real Guidance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method that combines several existing techniques for data augmentation is interesting.\n2. The paper provide insightful analysis on several design choices, including the time step at which the real image is inserted during generation, strategies to prevent leakage of internet data, mixing ratio of real and synthetic images and the number of augmentations generated for each image.\n3. The paper will release code and an aerial imagery dataset of leafy spurge, which will facilitate future study in this direction."
                },
                "weaknesses": {
                    "value": "1. The proposed technique is only applicable for classification tasks. It is not clear how it can be applied for object detection and segmentation tasks. My thinking is that one of the major drawbacks of such generative model-based augmentation method vs. traditional method may be that it can not simultaneously generate the segmentation mask and bounding box annotation for the augmented images."
                },
                "questions": {
                    "value": "1. I am not clear how the data-centric leakage prevention is performed. The paper mention that \"switching from a prompt ... is sufficient\" and \"Section 4.1 goes into detail\". If data-centric leakage refers to using prompt like \"a photo of a Class3\", what is the setting difference between Fig.5 and Fig.9?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7589/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7589/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7589/Reviewer_hrLX"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7589/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698489834301,
            "cdate": 1698489834301,
            "tmdate": 1699636919410,
            "mdate": 1699636919410,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rtWc9jCyEF",
                "forum": "ZWzUA9zeAg",
                "replyto": "jebeeNk7qD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response To Reviewer hrLX"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our paper, and for your valuable feedback! We appreciate your questions and positive impression of the paper. In this rebuttal, we hope to answer your questions, and discuss the points in your review to address the weakness you listed. The main weakness discussed in this review is the applicability of DA-Fusion beyond classification tasks to object detection and semantic segmentation. We agree with the reviewer that data augmentation has been crucial for object detection and semantic segmentation models, and we discuss in this rebuttal how DA-Fusion can be applied out-of-the-box to such tasks.\n\n# Detection & Segmentation\n\n__\u201cone of the major drawbacks of such generative model-based augmentation method vs. traditional [data augmentation] may be that it can not simultaneously generate the segmentation mask and bounding box annotation for the augmented images\u201d__\n\nApplying DA-Fusion to images with segmentation mask and bounding box annotations requires either (1) generating the annotations with the image, or (2) taking existing annotations and augmenting the corresponding input images in a manner that preserves the annotations. Our anonymous public code __supports capability (2) out-of-the-box__, and (here is the relevant section)[https://github.com/anonymous-da-fusion/da-fusion/blob/master/semantic_aug/augmentations/textual_inversion.py#L152] of the anonymous code. By using masks, DA-Fusion can preserve the location of objects in the input images. Examples using DA-Fusion with masks to preserve the locations of objects are in the following anonymous google drive link. We are adding a section discussing this application of DA-Fusion in the appendix of the manuscript.\nhttps://drive.google.com/drive/folders/15O0HWkQsEskmzmI8F14ISf2xNdwOlKBG?usp=sharing\n\n# Answers To Questions\n\n__\u201cIf data-centric leakage refers to using prompt like \u2018a photo of a Class3\u2019, what is the setting difference between Fig.5 and Fig.9?\u201d__\n\nData-centric leakage prevention modifies the prompt used with Stable Diffusion to omit the name of the class, and we discuss the technical details in the paragraph titled \u201cData-Centric Leakage Prevention\u201d in Section 6.1 of the manuscript. The high-level difference between Figure 5 and Figure 9 is whether Stable Diffusion knows the name of the class of the image. If the class name is known as in Figure 5, we prompt Real Guidance with \u201ca photo of a <class-name>\u201d and we initialize the learned pseudo-prompt for DA-Fusion with the class name.\n\nIf we don\u2019t know the class name as in Figure 9, we prompt Real Guidance with \u201ca photo\u201d and we initialize the learned pseudo-prompt for DA-Fusion with the word \u201cthe\u201d. These values are discussed in Table 1 of Appendix G - Hyperparameters. The ablation in Figure 9 shows that DA-Fusion works well for concepts that Stable Diffusion may not have learned the name of."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503308071,
                "cdate": 1700503308071,
                "tmdate": 1700503308071,
                "mdate": 1700503308071,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BpsmIVdNVc",
            "forum": "ZWzUA9zeAg",
            "replyto": "ZWzUA9zeAg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7589/Reviewer_KxYv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7589/Reviewer_KxYv"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces DA-Fusion that generates additional training data using Stable Diffusion. It involves learning a word embedding that represents each class in the dataset and using SDEdit for generation. Experimental results are shown for few-shot classification on different types of datasets e.g., common concepts, rare concepts, etc, with supportive results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper was well written and easy to follow.\n- Performing textual inversion for novel concepts seems like a promising idea."
                },
                "weaknesses": {
                    "value": "1. Generalizability of the method.\n    - It seems like the learned tokens are helpful for learning dataset specific biases, as it can add additional information on the general \u201cstyle\u201d of the dataset and of its classes. However, it seems to make certain assumptions about the datasets, e.g., that the images are object centric - only the target class is present, they have standard poses/viewpoints. It is not clear if there is substantial variations within the images of that class, e.g. in iwildcam, where the images from each class can be from different camera traps, thus, different backgrounds, camera parameters, the animals can have very different poses, or can be highly occluded. In this case, what would the single word embedding learn?\n\n2. Desideratas (sec 4)\n    - Controllable (content)\n        - Is textual inversion (as it is used in the paper) controllable? I.e. Is it easy to control for whether the token learns the style or object? Or if certain objects tends to co-occur with another, e.g. train and rails, it may not be necessarily learning the token for trains but also the objects that co-occur with it.\n    - \u201cperformant: gains in accuracy justify the additional computational cost of generating images from Stable Diffusion\u201d\n        - There does not seem to be any comparisons with compute cost relative to the baseline Rand Aug. DA-Fusion requires performing textual inversion, which may be expensive.\n        - It would also be interesting to compare the performance with standard augs like Rand Aug for different M. E.g., [1] showed that training on synthetic data outperforms when the generated dataset size much larger than the original dataset size. It is possible that at smaller M, standard augmentations outperform synthetic data, with the additional benefit that it is also fast to compute.\n\n3. Experiments\n    - Randaug, which includes several color based augmentations does not seem to be a good baseline for fine grained datasets like Flowers102, that may require the original color for classification.\n\n\n[1] Sariyildiz et al. Fake it till you make it: Learning transferable representations from synthetic ImageNet clones. CVPR\u201923"
                },
                "questions": {
                    "value": "In addition to the questions in weakness, some clarifications about the experiments:\n- How does the method perform zero shot? If the learned tokens did capture the class characteristics, and there is no distribution shift between training and test, it seems like DA-Fusion should do well.\n- Is the leafy spurge task a binary classification problem i.e., detecting if there is leafy spurge in the image or not? If that is the case, how often do other plants occur in the data? I was wondering how much fine-grained details can textual inversion capture.\n- How much variance is there in the class word embedding? If textual inversion is performed again with a different seed, how different would the generations be? If they are, it can be another way to introduce diversity."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7589/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7589/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7589/Reviewer_KxYv"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7589/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698713408303,
            "cdate": 1698713408303,
            "tmdate": 1700735455999,
            "mdate": 1700735455999,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x8yIKdXlJB",
                "forum": "ZWzUA9zeAg",
                "replyto": "BpsmIVdNVc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response To Reviewer KxYv (1 / 3)"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our paper and for your feedback. The discussion points shared by this reviewer center around (1) the generalizability of the method to challenging datasets, (2) controlling what visual concepts our method learns when images have co-occurring objects, and (3) the cost of generating synthetic data compared to standard data augmentations.\n\nWe address these points in our rebuttal.\n\n# Generalizing To Challenging Datasets:\n\n\n__\u201cit seems to make certain assumptions about the datasets, e.g., that the images are object centric - only the target class is present, they have standard poses/viewpoints\u201d__\n\n\nGenerative data augmentations should generalize to a variety of challenging datasets, including those with atypical viewpoints, poses, and objects. We test this in our paper by adapting the COCO and PASCAL VOC object detection tasks into classification problems. While the remaining standard datasets we use for evaluation tend to make certain assumptions that simplify benchmarking, COCO and PASCAL VOC have images with __many co-occurring objects__ in a __variety of poses__, with __occlusions__, and in __atypical viewpoints__. DA-Fusion outperforms prior work on these tasks by up to +10 percentage points.\n\nHere are images from the COCO dataset used in our evaluation that satisfy the properties discussed by this reviewer (the following links are anonymous): \n\n(__first image__): https://farm4.staticflickr.com/3664/3300381750_6bca02bce3_z.jpg\n\nhttp://farm1.staticflickr.com/89/263427144_db4970197f_z.jpg\n\nhttp://farm9.staticflickr.com/8102/8453554475_6a130a1c3b_z.jpg\n\nThe first image linked above includes (1) a close-up view of a fire-hydrant, (2) multiple people in a variety of poses, (3) one of whom is occluded by the fire-hydrant, (4) and three partially-occluded cars. DA-Fusion has strong performance on COCO and PASCAL VOC tasks (see Figure 5 in the paper), despite these challenging properties of the images.\n\n\n__\u201cdifferent backgrounds, camera parameters, the animals can have very different poses, or can be highly occluded. In this case, what would the single word embedding learn?\u201d__\n\n\nTextual Inversion is a powerful technique for learning visual concepts. Gal et al. 2022 (https://arxiv.org/abs/2208.01618) show Textual Inversions for objects with different backgrounds, viewpoints, poses, and partial occlusions. See Figures 3-5 in Gal et al. 2022. Textual Inversion is effective in these conditions and generates objects and styles well.\n\nThe embeddings found by Textual Inversion capture a distribution. They represent an object in a variety of poses, with a variety of occlusions, and on a variety of backgrounds."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700262918931,
                "cdate": 1700262918931,
                "tmdate": 1700262918931,
                "mdate": 1700262918931,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hfqI3zUu2G",
                "forum": "ZWzUA9zeAg",
                "replyto": "BpsmIVdNVc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response To Reviewer KxYv (2 / 3)"
                    },
                    "comment": {
                        "value": "# Controlling What DA-Fusion Generates: \n\n__\u201cIs it easy to control for whether the token learns the style or object? \u2026 it may not be necessarily learning the token for trains but also the objects that co-occur with it.\u201d__\n\nGenerative models are uniquely promising for data augmentation because they allow the user to control the content and style of images in training datasets. DA-Fusion implements three simple interfaces that control the content and style of what is generated by our method.\n\n1) guiding what\u2019s generated with prompts\n\n2) guiding what Textual Inversion learns with prompts\n\n3) erasing the ability to generate specific concepts written in prompts\n\nEach control method has a simple natural language interface. \n\n__Control Method 1__: The word embeddings learned by DA-Fusion are flexible, and the user can modify the content and style of generations after learning the word embeddings. For example, if the prompt \u201ca photo of a <class>\u201d leads to the images named \u201coriginal generation\u201d at the following anonymous link, changing the prompt to \u201ca photo of a <class> wearing a bow-tie\u201d, leads to the images named \u201cgeneration with bow-tie\u201d at the following anonymous link:\nhttps://drive.google.com/drive/folders/1gnCa0SBd9tadL9o2SK0jmNNYoNvC76Sc?usp=sharing\nWe are updating the manuscript to discuss __Control Method 1__ in the appendix.\n\n__Control Method 2__: Suppose we show DA-Fusion the image named \u201coriginal image\u201d at the following anonymous link and learn a \u201c<class>\u201d word embedding using this image. The image has a cat and a dog. If we want the \u201c<class>\u201d word embedding to focus on the cat and ignore the dog, we can prompt Stable Diffusion with \u201ca photo of a <class> sitting on the right of a dog\u201d when learning the \u201c<class>\u201d word embedding. If we generate images using an inference-time prompt \u201ca photo of a <class>\u201d the generations only have cats, showing the \u201c<class>\u201d word embedding has ignored the dog and focused on the cat in the original image.\nhttps://drive.google.com/drive/folders/1lmr7e7qBZHwyGJyIkxt-0bUAxcn0934M?usp=sharing\nWe are updating the manuscript to discuss __Control Method 2__ in the appendix.\n\nControlling the style of what tokens learn can be done with Textual Inversion. We build on the Textual Inversion script released by HuggingFace, and by using `--learnable_property=style` the user can switch modes and cause tokens to learn style rather than object-based concepts.\n\n__Control Method 3__: Finally, we can erase Stable Diffusion\u2019s generation ability for specific concepts. Suppose we generate images of roads using the prompt \u201ca photo of a <class>\u201d and we realize that cars often show up in the generations, but our real data never has cars. We can erase the \u201ccar\u201d concept and prevent Stable Diffusion from generating it, resulting in the following images. Additional examples for erasing concepts are in Figure 7 of the manuscript.\nhttps://drive.google.com/drive/folders/1eDkXFKhpN94ZEhQtih97PdXsJtjzqKuz?usp=sharing\nWe discuss __Control Method 3__ in Section 6.1 of the manuscript.\n\n# Computational Cost Analysis:\n\n__\u201cThere does not seem to be any comparisons with compute cost relative to the baseline Rand Aug. DA-Fusion requires performing textual inversion, which may be expensive.\u201d__\n\nThe trade-off of performance and computational cost is important for users considering the viability of generative data augmentations. We agree the cost of DA-Fusion is higher than traditional data augmentation, but our method is parallelizable, and the cost is once upfront, whereas the cost of traditional data augmentation is at every training iteration.\n\nWe provide statistics for DA-Fusion in the following table, assuming a dataset with 100 classes, 4 images per class, a training batch size of 32, trained for 10,000 steps, with M=5. The machine has 4 Nvidia RTX 6000 ada GPUs, and image generation is parallelized across all the GPUs.\n\n| Method | Total Time (minutes) |\n| ----------- | ----------- |\n| DA-Fusion (25-step diffusion sampler) | 21:19 |\n| DA-Fusion (12-step diffusion sampler) | 14:47 |\n| RandAugment | 12:18 |\n| No Augmentation | 08:15 |\n\nDA-Fusion is only 20.2% more expensive than RandAugment with the 12-step diffusion sampler according to wall-clock time in this configuration, and performs up to 24.2% better for fine-grain concepts, which is discussed in the paper in Section 6, Figure 6 in the manuscript. We are adding this cost analysis to the paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263055828,
                "cdate": 1700263055828,
                "tmdate": 1700286048692,
                "mdate": 1700286048692,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UMo3xl8a7E",
                "forum": "ZWzUA9zeAg",
                "replyto": "BpsmIVdNVc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Following Up On The Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our manuscript, we would like to follow up on our rebuttal.  If there are any remaining points that you would like us to address, please let us know.  Otherwise, thank you for your review and we look forward to your response."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529749040,
                "cdate": 1700529749040,
                "tmdate": 1700529749040,
                "mdate": 1700529749040,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U2GvfnC5dK",
                "forum": "ZWzUA9zeAg",
                "replyto": "UMo3xl8a7E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7589/Reviewer_KxYv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7589/Reviewer_KxYv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the additional analysis and discussion!\n\nOne clarification, DA-fusion is said to have \"no prior knowledge about the image content\", but for Control Method 2 (the case where there are co-occuring objects) DA-fusion seems to requires knowledge about the image content to distinguish the cat and dog. Did I understand this correctly? For the datasets that are less object-centric (with co-occuring objects) is the prompt used to perform textual inversion still \"a photo of $w_i$\" or does it depend on the class/image etc? Thanks!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604965549,
                "cdate": 1700604965549,
                "tmdate": 1700604965549,
                "mdate": 1700604965549,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CVl9WmJaNd",
                "forum": "ZWzUA9zeAg",
                "replyto": "BpsmIVdNVc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional Clarifications"
                    },
                    "comment": {
                        "value": "Thank you for your questions and discussion!\n\n__\"for Control Method 2 (the case where there are co-occuring objects) DA-fusion seems to requires knowledge about the image content to distinguish the cat and dog. Did I understand this correctly?\"__\n\nIf __Control Method 2__ is used, it benefits from a prompt that describes which object to focus on, but note this is an *additional way to control DA-Fusion* and not a core part of the method. DA-Fusion does not require knowledge about the image content to generate performant augmentations. In Figure 5-11 of the manuscript, we do not provide image-level knowledge to DA-Fusion, and we see strong performance improvements between 12% and 24% depending on how fine-grain or rare concepts are.\n\n__\"For the datasets that are less object-centric (with co-occuring objects) is the prompt used to perform textual inversion still \"a photo of $w_i$ \" or does it depend on the class/image etc?\"__\n\nThe prompt format in textual inversion is the same for all datasets and does not depend on the class/image. We use the prompt format \"a photo of a <class>\" where \"<class>\" is a learned pseudo-prompt. This choice of prompt (part of enforcing uniform hyperparameters across all datasets) ensures that DA-Fusion is not fine-tuned to a particular kind of dataset.\n\nThank you for the discussion, please let us know if you have additional questions. We look forward to your response."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606085970,
                "cdate": 1700606085970,
                "tmdate": 1700721367178,
                "mdate": 1700721367178,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vLYyXH3HLs",
                "forum": "ZWzUA9zeAg",
                "replyto": "CVl9WmJaNd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7589/Reviewer_KxYv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7589/Reviewer_KxYv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification. My main concern would then still be on the generalizability of the method, that e.g., Control Method 2 requires image level information to learn the correct object. Thus, I am inclined to keep my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699418023,
                "cdate": 1700699418023,
                "tmdate": 1700699418023,
                "mdate": 1700699418023,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U77ypKj8rC",
                "forum": "ZWzUA9zeAg",
                "replyto": "BpsmIVdNVc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarifications For Generalizability"
                    },
                    "comment": {
                        "value": "Thanks for following up on the generalizability of the method, we appreciate your clarification to your earlier concern. In this response, we will show that DA-Fusion does not use image level information to learn the correct object. We appreciate the back-and-forth discussion you had with us during the peer review process, and hope our new points on generalizability can help resolve your concern.\n\n# Learning The Correct Object Without Image-Level Information\n\n__\u201cMy main concern would then still be on the generalizability of the method, that e.g., Control Method 2 requires image level information to learn the correct object.\u201d__\n\nDA-Fusion infers which object is the correct one purely from shared statistics in the images, and does not use image-level information to identify the correct object. __Control Method 2__ is *an additional mechanism* that we don\u2019t use in our evaluation in Figures 5-11, but can be used by practitioners who want additional hands-on control over what DA-Fusion generates. The primary way to instruct DA-Fusion what to generate is by choosing example images for learning word embeddings that contain the desired visual concept. We show this procedure identifying the correct object when the example images have co-occurring objects, and partial occlusions.\n\nExamples of images from COCO with co-occurring objects, and partial occlusions shown to DA-Fusion are uploaded at the following anonymous link. In this example, we show DA-Fusion partially occluded cats in different poses with co-occurring objects, like the stuffed elephant blocking the cat in example image four. DA-Fusion correctly focuses on the cats, shown by the generations uploaded at the anonymous link, despite co-occurring objects, and occlusions.\nhttps://drive.google.com/drive/folders/1wSc4D-XXXJ-AzjO6SbOFy_YBng9YD64F?usp=sharing\n\n---\n\nIn summary, DA-Fusion does not use image level information (e.g no image-specific prompt) to learn the correct object, and only requires example images that contain the desired concept. The generations at the link above show DA-Fusion is robust to poorly-chosen examples with co-occurring objects, and occlusions."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712723787,
                "cdate": 1700712723787,
                "tmdate": 1700713368124,
                "mdate": 1700713368124,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kYqozG8B2M",
                "forum": "ZWzUA9zeAg",
                "replyto": "U77ypKj8rC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7589/Reviewer_KxYv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7589/Reviewer_KxYv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks again for the clarification, I misunderstood the purpose of the control methods. I will raise my score from 5 to 6. Also given that there are many follow up works on textual inversion that e.g., amortizes the optimization, or extracts several fine-grained concepts etc., which may be orthorgonal to the method, but it would be interesting to see how much of an improvement it makes."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735435475,
                "cdate": 1700735435475,
                "tmdate": 1700735435475,
                "mdate": 1700735435475,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JbsLQb9RfC",
            "forum": "ZWzUA9zeAg",
            "replyto": "ZWzUA9zeAg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7589/Reviewer_rXxt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7589/Reviewer_rXxt"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel approach to data augmentation in deep learning, addressing the issue of limited diversity in traditional augmentation methods. The authors propose a method called DA-Fusion, which leverages large pretrained generative models to generate variations of real images while respecting their semantic attributes. They fine-tune pseudo-prompts to instruct the diffusion model on what to augment, and their approach is tested on few-shot image classification tasks across various domains, including a real-world weed recognition task.\n\nThe authors provide a comprehensive review of related work, highlighting the advantages of diffusion models in image generation and editing. They also discuss the challenges of preventing leakage of internet data when using large pretrained generative models for synthetic data generation.\n\nThe experimental results show that DA-Fusion consistently improves accuracy in various domains, outperforming traditional data augmentation methods. The paper includes a detailed analysis of the results, including a breakdown by the presence of common, fine-grain, or completely new concepts in the datasets.\n\nThe paper concludes by suggesting future directions for improving the flexibility and performance of their method, such as better control over image augmentation, maintaining temporal consistency in decision-making settings, and enhancing the photo-realism of the diffusion model backbone.\n\nOverall, the paper presents a promising approach to data augmentation that addresses the limitations of traditional methods and demonstrates its effectiveness in various domains. The discussion of potential future improvements adds value to the work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The work on DA-Fusion offers several strengths and introduces significant novelty in the context of data augmentation:\n* Novel Data Augmentation Technique: DA-Fusion introduces a unique approach to data augmentation by leveraging large pretrained generative models. It goes beyond traditional data augmentation methods that mainly involve geometric transformations. This novelty lies in utilizing generative models to create diverse and semantically meaningful variations of real images.\n* Semantic Preservation: Unlike traditional data augmentation techniques that focus on basic geometric transformations, DA-Fusion aims to respect the semantic attributes of images. It modifies images in a manner that retains their underlying semantics, such as the design of objects or specific visual details, making it more relevant for real-world recognition tasks.\n* Few-shot Learning Improvement: The paper demonstrates the effectiveness of DA-Fusion in enhancing few-shot image classification tasks. It is particularly valuable in scenarios where only a limited number of real images per class are available. This is a significant advantage as few-shot learning is a challenging problem with practical applications.\n* Fine-Grained Concepts: The authors show that DA-Fusion is especially beneficial for fine-grained concepts, where traditional data augmentation methods may not provide sufficient diversity. This highlights the method's potential in improving the recognition of subtle visual differences in image classification tasks.\n* Leakage Mitigation: The paper addresses the important issue of preventing leakage of internet data, which is often a concern when utilizing large pretrained generative models. The proposed defenses to mitigate leakage during evaluation contribute to the robustness and reliability of the method.\n* Generalization to New Concepts: DA-Fusion's ability to generalize to new concepts not seen during the diffusion model's training is a noteworthy aspect. This is a crucial capability, as it allows the method to adapt to a wide range of recognition tasks without the need for extensive additional data.\n* Clear Experimental Validation: The work provides a thorough experimental validation, including results on a variety of datasets spanning common, fine-grained, and completely new concepts. The consistent improvement in accuracy across different domains demonstrates the practical applicability and versatility of the approach."
                },
                "weaknesses": {
                    "value": "Here are some of the weaknesses of this work:\n* Complexity and Computational Cost: The proposed method involves fine-tuning pseudo-prompts for each concept, which can be computationally expensive and time-consuming. This approach might not be as practical as traditional data augmentation techniques that are computationally efficient and easy to implement.\n* Lack of Control Over Augmentations: While DA-Fusion introduces the concept of modifying images while respecting their semantic attributes, it does not explicitly provide fine-grained control over how the augmentations are performed. This lack of control may limit its applicability to specific use cases where precise image modifications are required.\n\nSome nitpicks: Citations starting from second page are not in brackets like that in the first page. Would be nice to have consistency and keep them in brackets."
                },
                "questions": {
                    "value": "I believe the idea is nice and clear. Paper is well written. Some might be of the opinion that the idea is not novel. However, I do like the effective use of pre-trained diffusion models and textual inversion for data augmentation. I like the detailed analysis that the authors have done for their approach.\n\nOne thing I would like to see is how this method can be used to generate positive and negative samples for contrastive learning based approaches. I understand this can be a herculean task for the authors to do now so not required as of now. But curious to know about the effect of this data aug strategy in SSL."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7589/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728661205,
            "cdate": 1698728661205,
            "tmdate": 1699636919113,
            "mdate": 1699636919113,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Nx27NqVCeP",
                "forum": "ZWzUA9zeAg",
                "replyto": "JbsLQb9RfC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response To Reviewer rXxt (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our paper and for your valuable feedback! We appreciate the thorough review, your enthusiastic points, and positive impression of the manuscript. We hope to answer your questions and discuss the weaknesses you listed in our rebuttal. The main points listed by this reviewer center around the complexity and computational cost of DA-Fusion, controlling the augmentations generated by DA-Fusion, and extensions to self-supervised learning tasks.\n\nWe address these points, organized by category, in the following sections.\n\n# Complexity and Computational Cost\n\n__\u201cThis approach might not be as practical as traditional data augmentation techniques that are computationally efficient and easy to implement.\u201d__\n\nThe trade-off of performance and computational cost is important for users considering the viability of generative data augmentations. We agree the cost of DA-Fusion is higher than traditional data augmentation, but our method is parallelizable, and the cost is once upfront, whereas the cost of traditional data augmentation is at every training iteration.\n\nWe provide statistics for training a classifier with DA-Fusion in the following table, assuming a dataset with 100 classes, 4 images per class, a training batch size of 32, trained for 10,000 steps, with M=5. The machine has 4 Nvidia RTX 6000 ada GPUs, and image generation is parallelized across all the GPUs.\n\n| Method | Total Time (minutes) |\n| ----------- | ----------- |\n| DA-Fusion (25-step diffusion sampler) | 21:19 |\n| DA-Fusion (12-step diffusion sampler) | 14:47 |\n| RandAugment | 12:18 |\n| No Augmentation | 08:15 |\n\nTraining a classifier with DA-Fusion is only 20.2% more expensive than RandAugment with the 12-step diffusion sampler according to wall-clock time in this configuration, and performs up to 24.2% better for fine-grain concepts, which is discussed in the paper in Section 6, Figure 6 in the manuscript. We are adding this cost analysis to the paper.\n\n__\u201cThe proposed method involves fine-tuning pseudo-prompts for each concept, which can be computationally expensive and time-consuming.\u201d__\n\nWe agree that fine-tuning pseudo-prompts is time-consuming. Each concept requires 10 minutes of fine-tuning on our machine, and this can be parallelized. One fine-tuning experiment requires 11GB of RAM, and we are able to fit 4 of these in parallel on an NVIDIA RTX 6000 ada GPU, resulting in 16 parallel fine-tuning experiments with 4 GPUs. With our 100 class dataset, this requires 10 * 100 / 16 = 62.5 minutes of time spent fine-tuning.\n\nFaster methods for learning pseudo-prompts are being developed. Recent work by Voronov et al. 2023 (https://arxiv.org/abs/2302.04841) could reduce this cost by a factor of 8, resulting in a total cost of 7.8 minutes of time spent fine-tuning. Additionally, HyperNetworks are being developed (Ruiz et al. 2023 https://arxiv.org/abs/2307.06949) that may eliminate the fine-tuning step by directly predicting the pseudo-prompt embeddings with a negligible cost."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469032399,
                "cdate": 1700469032399,
                "tmdate": 1700501975323,
                "mdate": 1700501975323,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Spft3932L9",
                "forum": "ZWzUA9zeAg",
                "replyto": "JbsLQb9RfC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7589/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Controlling The Augmentation\n\n__\u201cThis lack of [fine-grained control over how the augmentations are performed] may limit its applicability to specific use cases where precise image modifications are required.\u201d__\n\nGenerative models are uniquely promising for data augmentation because they allow the user to control the content and style of images in training datasets. DA-Fusion implements three simple interfaces that control the content and style of what is generated by our method.\n\n1) guiding what\u2019s generated with prompts\n2) guiding what Textual Inversion learns with prompts\n3) erasing the ability to generate specific concepts written in prompts\n\nEach control method has a simple natural language interface. \n\n__Control Method 1__: The word embeddings learned by DA-Fusion are flexible, and the user can modify the content and style of generations after learning the word embeddings. For example, if the prompt \u201ca photo of a <class>\u201d leads to the images named \u201coriginal generation\u201d at the following anonymous link, changing the prompt to \u201ca photo of a <class> wearing a bow-tie\u201d, leads to the images named \u201cgeneration with bow-tie\u201d at the following anonymous link:\nhttps://drive.google.com/drive/folders/1gnCa0SBd9tadL9o2SK0jmNNYoNvC76Sc?usp=sharing\nWe are updating the manuscript to discuss __Control Method 1__ in the appendix.\n\n__Control Method 2__: Suppose we show DA-Fusion the image named \u201coriginal image\u201d at the following anonymous link and learn a \u201c<class>\u201d word embedding using this image. The image has a cat and a dog. If we want the \u201c<class>\u201d word embedding to focus on the cat and ignore the dog, we can prompt Stable Diffusion with \u201ca photo of a <class> sitting on the right of a dog\u201d when learning the \u201c<class>\u201d word embedding. If we generate images using an inference-time prompt \u201ca photo of a <class>\u201d the generations only have cats, showing the \u201c<class>\u201d word embedding has ignored the dog and focused on the cat in the original image.\nhttps://drive.google.com/drive/folders/1lmr7e7qBZHwyGJyIkxt-0bUAxcn0934M?usp=sharing\nWe are updating the manuscript to discuss __Control Method 2__ in the appendix.\n\nControlling the style of what tokens learn can be done with Textual Inversion. We build on the Textual Inversion script released by HuggingFace, and by using `--learnable_property=style` the user can switch modes and cause tokens to learn style rather than object-based concepts.\n\n__Control Method 3__: Finally, we can erase Stable Diffusion\u2019s generation ability for specific concepts. Suppose we generate images of roads using the prompt \u201ca photo of a <class>\u201d and we realize that cars often show up in the generations, but our real data never has cars. We can erase the \u201ccar\u201d concept and prevent Stable Diffusion from generating it, resulting in the following images. Additional examples for erasing concepts are in Figure 7 of the manuscript.\nhttps://drive.google.com/drive/folders/1eDkXFKhpN94ZEhQtih97PdXsJtjzqKuz?usp=sharing\nWe discuss __Control Method 3__ in Section 6.1 of the manuscript.\n\n# Questions & Miscellaneous\n\n__\u201cOne thing I would like to see is how this method can be used to generate positive and negative samples for contrastive learning based approaches.\u201d__\n\nWe think this is a promising next direction as well, and agree with the reviewer that it would require a significant amount of work to include a proper empirical analysis. The inclusion is beyond the scope of this paper, but we are happy to discuss what this extension would entail. \n\nGenerative data augmentations like DA-Fusion can edit images to interpolate between different classes. With this in mind, to generate highly informative positive and negative augmentations that \u201cstraddle\u201d the decision boundary of two classes, we can start from images of class A, and transform them into images of class B that resemble class A. These samples are positive for class B, and negative for class A. This may be helpful for classes that are often confused.\n\n__\u201cSome nitpicks: Citations starting from second page are not in brackets like that in the first page. Would be nice to have consistency and keep them in brackets.\u201d__\n\nThanks for catching this! We are updating the citation format in the manuscript to keep citations consistent by changing citations to brackets from the second page and after."
                    },
                    "title": {
                        "value": "Response To Reviewer rXxt (2/2)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469157539,
                "cdate": 1700469157539,
                "tmdate": 1700501783413,
                "mdate": 1700501783413,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tpnZIz7h7t",
                "forum": "ZWzUA9zeAg",
                "replyto": "Nx27NqVCeP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7589/Reviewer_rXxt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7589/Reviewer_rXxt"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. The authors addressed my concerns and I would like to keep my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633418811,
                "cdate": 1700633418811,
                "tmdate": 1700633418811,
                "mdate": 1700633418811,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]