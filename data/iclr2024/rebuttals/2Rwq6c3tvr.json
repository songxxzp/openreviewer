[
    {
        "title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models"
    },
    {
        "review": {
            "id": "itGFll8nSX",
            "forum": "2Rwq6c3tvr",
            "replyto": "2Rwq6c3tvr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission992/Reviewer_NUVj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission992/Reviewer_NUVj"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method for assessing data contamination in models by prompting the model to complete instances of a given dataset. The generated responses are evaluated using either overlap (via ROUGE-L / BLEURT) differences between prompts with/without the dataset specification, or a GPT-4 based few-shot classifier. The instance level information is then used to decide on the partition (train/test/val) / dataset level."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method can be applied to black-box models hidden behind an API with little model interaction.\n- The study of data contamination, particularly with respect to commonly used evaluation datasets, is an important area.\n- An attempt is made to validate the method by fine-tuning on a given downstream dataset"
                },
                "weaknesses": {
                    "value": "- The aggregation from instance to partition level seems to be rather ad hoc (contaminated if at least 1 or 2 instances are contaminated); and a proper ablation regarding these hyperparameters is missing.\n- Experiments are performed only with black-box models; using open models with known training details would have supported a more reliable evaluation, since (more of) their training details are known.\n- The comparison of Alg. 2 (GPT-4 ICL) with human judgments seems to be rather biased, since the same human annotators created the ICL examples."
                },
                "questions": {
                    "value": "- The first paragraph of Section 3.1.1 is quite confusing: the text makes it difficult to understand which components it refers to.\n- Given your observations in Section 5, (3) that the `ChatGPT-Cheat` method fails due to GPT-4's safeguards being triggered when trying to retrieve examples from these datasets, I wonder how these safeguards would also affect the results you get with your prompt templates.\n- For unpaired instances, a random-length prefix is displayed in the prompt; how is this random length sampled? And what is its effect?\n- (minor comment): typo: page 2, first paragraph, last sentence: \"obtains\"  -> \"obtained\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission992/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission992/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission992/Reviewer_NUVj"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission992/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697982706170,
            "cdate": 1697982706170,
            "tmdate": 1699636025461,
            "mdate": 1699636025461,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "klvxpm7nAy",
                "forum": "2Rwq6c3tvr",
                "replyto": "itGFll8nSX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission992/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission992/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response by Authors"
                    },
                    "comment": {
                        "value": "We appreciate your comments highlighting the importance of the study of data contamination, especially in the context of black-box models, and are grateful for your constructive feedback.\n\nWe address your concerns in the following in the same order they are listed.\n\n> Aggregation from instance- to partition-level contamination.\n\nIn our validation study, we empirically validated the number of exact/near-exact matches that can signal partition-level contamination by performing controlled contamination on the base model of GPT-3.5 (Section 3.3). Then, based on these findings, we studied a set of numbers for both exact and near-exact matches as our **default setting** to generalize to the partition-level contamination.\n\nWe would like to emphasize that the aforementioned numbers we studied as our default setting for determining partition-level contamination are **not fixed**. As mentioned in our paper, these criteria, i.e., the number of exact or near-exact matches, can be adjusted depending on a specific context and the perceived severity of contamination in a given setting. In other words, our replication-based strategy is completely separate from the way that a dataset partition is flagged as contaminated based on the number of exact/near-exact matches.\n\n> The unavailability of training data for black-box models.\n\nWe understand your concern about the unavailability of the pretraining data for black-box models. In this regard, one of the main purposes of our validation study (Section 3.3) is to address this concern by introducing contamination through **known data**  to the GPT-3.5 base model and replicating this known data using our approach to validate our strategy. We also experiment with the GSM8k dataset on GPT-4, as a known data included in the pretraining data of GPT-4 based on the technical report provided by OpenAI. These experiments showed that exact/near-exact matches, replicated through guided instruction, are indeed indicative of prior exposure to the data, thereby revealing data contamination. We have also provided several examples of exact/near-exact replicas from this experiment in Figure 2 and Table 11 in Appendix G in the updated version of our submission.\n\nIn addition to the aforementioned validation study in our paper, we would like to refer to the study by Carlini et al. (2023) where they assessed the memorization ability of LLMs by replicating their training data. Inferring from their results and observations, replication of certain data confirms its presence in the training data of the LLMs. This implies that even **without direct access to the actual training data**, the presence of specific data can be inferred and confirmed through replication of this data by disclosing the LLM\u2019s internalized information. From this perspective, as our method fundamentally detects contamination by confirming the presence of dataset instances through replication, experimenting with both closed-source and open-source LLMs would yield the same insights.\n\n> The comparison of Alg. 2 (GPT-4 ICL) with human judgments seems to be rather biased, since the same human annotators created the ICL examples.\n\nIn almost all scenarios where Machine Learning models/algorithms are applied, the underlying purpose is to design a system that can follow human judgments, e.g., labels in classification tasks, summaries in summarization tasks, etc. In our case, GPT-4 ICL aims to follow human judgments for detecting data contamination.\n\n> The first paragraph of Section 3.1.1 is quite confusing.\n\nThank you for bringing this to our attention. We rephrased this paragraph (in blue) in the current revised version of the paper to address the clarity.\n\n> I wonder how the safeguards would affect the results you get with your prompt templates.\n\nUsing our prompt templates, we did not observe any cases in which the safeguards were triggered, so we always got completion for the provided partial reference instance in the input prompt.\n\n> For unpaired instances, a random-length prefix is displayed in the prompt; how is this random length sampled? And what is its effect?\n\nThe selection of both the instance itself and the length of the prefix used in the prompt was performed at random. As we also mentioned in our paper, for unpaired instances with multiple sentences, the prefixes are created by arbitrarily cutting the instances at the end of a complete sentence, whereas for instances containing a single (long) sentence, a random sentence fragment is removed.\nIn terms of the effect of length, we would like to refer to the well-studied research by Carlini et al. (2023) where they found that memorization in LLMs significantly grows as we increase the number of tokens of context used to prompt the model.\n\n> Typo: \"obtains\" -> \"obtained\"\n\nThank you! We fixed the typo in the current revised version of our paper.\n\nReferences: \\\n[1] Quantifying Memorization Across Neural Language Models (Carlini et al., ICLR 2023)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission992/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700374085598,
                "cdate": 1700374085598,
                "tmdate": 1700374085598,
                "mdate": 1700374085598,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gVPppV5XKg",
            "forum": "2Rwq6c3tvr",
            "replyto": "2Rwq6c3tvr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission992/Reviewer_KgsM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission992/Reviewer_KgsM"
            ],
            "content": {
                "summary": {
                    "value": "The paper offers a fresh perspective on assessing the capabilities of LLMs in terms of potential dataset contamination. The authors introduce two novel methodologies to measure these aspects. The first method uses BLEURT and ROUGE-L evaluation metrics, while the second leverages GPT-4's few-shot in-context learning prompt. A significant part of the evaluation revolves around identifying potential data contamination issues, and the results are compared against a baseline method, ChatGPT-Cheat. The findings underscore the nuances and intricacies in effectively evaluating LLMs and the paper serves as a guide to understand their limitations and potential."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: The paper offers a fresh perspective on assessing the capabilities of LLMs in terms of potential dataset contamination. The methodologies introduced, especially the use of GPT-4's few-shot in-context learning, is innovative.\nQuality: The research appears thorough with detailed evaluations using two different algorithms. The results are well-tabulated, and the comparison with ChatGPT-Cheat offers a clearer understanding of the proposed methods' effectiveness.\nClarity: The paper is structured coherently, and the methodologies, evaluations, and results are presented in a clear and organized manner, making it easier for the reader to follow.\nSignificance: Given the increasing utilization and reliance on LLMs in various applications, understanding their limitations and behaviors is crucial. This paper addresses this need, making it a significant contribution to the field."
                },
                "weaknesses": {
                    "value": "Scope: The paper focuses primarily on GPT-3.5 and GPT-4. A broader range of LLMs could provide more generalizable insights."
                },
                "questions": {
                    "value": "How do the proposed methods scale when evaluating even larger LLMs or when considering different architectures beyond GPT?\nCould the authors provide insights into the trade-offs between the two algorithms, especially in terms of computational cost and reliability?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission992/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813977349,
            "cdate": 1698813977349,
            "tmdate": 1699636025389,
            "mdate": 1699636025389,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4CSsguGAzu",
                "forum": "2Rwq6c3tvr",
                "replyto": "gVPppV5XKg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission992/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission992/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response by Authors"
                    },
                    "comment": {
                        "value": "Thank you for acknowledging the originality and significance of our study, and for your detailed review of our work.\n\nWe address your comments and questions in the following in the same order they are listed.\n\n> The scope of LLMs.\n\nWe appreciate your suggestion about including a broader range of LLMs. Our focus on GPT-3.5 and GPT-4 was informed by their prevalence in the field and the unique challenges of data contamination in closed-source LLMs, which are less transparent than open-source models both in terms of the model weights and pre-training data. While this focus allows for an in-depth exploration of these specific models, we acknowledge the potential benefits of expanding our study to include a more diverse set of LLMs for broader insights.\n\n> Detecting data contamination for larger LLMs using our method.\n\nFor larger LLMs, based on the well-studied research by Carlini et al. (2023), as the memorization ability of LLMs significantly grows with their capacity, we believe for larger LLMs, it becomes even easier to replicate instances using our template prompts.\n\n> Detecting data contamination for architectures beyond GPT.\n\nSince our method hinges on replicating instances to validate their presence by emitting the inner learned knowledge, our method works best with decoder-only components (e.g., GPTs), although for other components like encoder-only (e.g., BERT), Magar & Schwartz (2022) investigated other methods.\n\n> Computational cost trade-offs between the two algorithms.\n\nIn terms of the computational cost trade-offs between the two algorithms, Algorithm 1 with ROUGE-L is much friendlier than Algorithm 1 with BLEURT and Algorithm 2 (GPT-4 ICL), as these two need GPUs to run. On the other hand, the latter two methods use dense representations to evaluate the candidate text against the original instance, thereby performing better in terms of semantic match.\n\n> Reliability comparison between the two algorithms.\n\nIn terms of reliability, since by using GPT-4 ICL, it is possible to provide examples of exact/near-exact matches from human evaluation for downstream judgments, it is much more aligned with human evaluations compared to Algorithm 1 with BLEURT and/or ROUGE-L.\n\nReferences: \\\n[1] Quantifying Memorization Across Neural Language Models (Carlini et al., ICLR 2023) \\\n[2] Data Contamination: From Memorization to Exploitation (Magar & Schwartz, ACL 2022)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission992/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700371384326,
                "cdate": 1700371384326,
                "tmdate": 1700371384326,
                "mdate": 1700371384326,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ahw9C9Ww4y",
            "forum": "2Rwq6c3tvr",
            "replyto": "2Rwq6c3tvr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission992/Reviewer_6REU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission992/Reviewer_6REU"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method to detect dataset leakage/contamination in LLMs first at the instance level before they bootstrap to the partition (test, valid, train). At the instance level, they do so via a guided prompt that was crafted to bias the model towards outputting data in a format that is likely to overlap with the dataset example. Once a candidate dataset and partition is flagged, the authors mark it as leaked if either the overlap between reference instances is statistically significantly higher when using guided peompts compared to general prompts or is determined to be an exact or near match using a GPT-4 based classifier. The best classification models match with 92-100% accuracy the labels provided by human experts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Intuitive guided and general prompts to detect instance level contamination.\n- Approximating human expert classification for exact and approximate match using GPT-4 as a classifier, i.e. approximating semantic match.\n- Validation on a known contaminated LLM."
                },
                "weaknesses": {
                    "value": "- The authors rely on the algorithm to begin with when deciding what partitions were not leaked and should be added during fine-tuning. This has a circular dependence/assumption. (This point was addressed during discussion with the authors as a writing/explanation issue rather than a true circular dependence).\n- Different levels of data leakage is not considered. For example, would GPT-4 be detected as having seen paritions of datasets that follow well-known formats seen from other datasets if it sees only a metadata description of a new dataset? (This limitation is now acknowledged in a footnote with additional details on metrics as well as below in the discussion with the authors)."
                },
                "questions": {
                    "value": "My main question/concern is on the reliability of the instance level contamination detection. Specifically, if a dataset follows a well-known and observed in other dataset format, if a model such as GPT-4 sees the dataset description and meta-data, would it generate sufficiently many near matches to appear as contaminated with a new dataset, despite observing only metadata?\nI understand that this work relaxes the prefix match from Sainz et al., but I wonder if this is likely to generate false signal in models that show an ability to generalise from few examplea and/or metadata."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission992/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission992/Reviewer_6REU",
                        "ICLR.cc/2024/Conference/Submission992/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission992/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818351021,
            "cdate": 1698818351021,
            "tmdate": 1700699171117,
            "mdate": 1700699171117,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V7MHbLCNXn",
                "forum": "2Rwq6c3tvr",
                "replyto": "ahw9C9Ww4y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission992/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission992/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response by Authors"
                    },
                    "comment": {
                        "value": "Thank you for recognizing the intuitive and innovative design of our approach, and for your valuable feedback.\n\nWe address your comments in the following in the same order they are listed.\n\n> Circular dependence/assumption.\n\nTo address the concern about circular dependence, we would like to mention that we were able to introduce contamination in our validation study (Section 3.3) using a set of synthetic instances from a fake dataset to break this circular dependency that seems to be present in the assumption. However, we found this a little bit confusing for the readers. Therefore, the main idea of contaminating using the partitions that were not leaked was only to maintain consistency throughout the paper for readers. Otherwise, there is no circular dependency.\n\n> Possible contamination through the metadata of another dataset.\n\nRegarding the possible contamination arising from metadata in another dataset\u2014a point raised in both the Weaknesses and Questions sections\u2014we would like to state that this is an interesting issue that we leave for future study. Our current work in this paper focuses on the complex issue of *detecting the presence* of contamination and not on *where* the contamination comes from. Nevertheless, we acknowledge the importance of identifying the sources of contamination."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission992/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700370440399,
                "cdate": 1700370440399,
                "tmdate": 1700370440399,
                "mdate": 1700370440399,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dyVy3OhwKe",
                "forum": "2Rwq6c3tvr",
                "replyto": "ahw9C9Ww4y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission992/Reviewer_6REU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission992/Reviewer_6REU"
                ],
                "content": {
                    "comment": {
                        "value": "> Circular dependence/assumption.\n\nIt was indeed confusing between bootstrap and circular dependency, thank you for the clarification.\n\n> Metadata contamination\n\nI appreciate that this problem is beyond the scope of the paper, but still feel this should be mentioned as a potential limitation. I am still unconvinced if the paper can distinguish between contamination or just metadata contamination for a sufficiently accurate model. I do not feel this devalues the result, as for certain tasks, both can/should count as contamination regardless of the mechanism."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission992/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700444293640,
                "cdate": 1700444293640,
                "tmdate": 1700444310872,
                "mdate": 1700444310872,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WlN2BaElqG",
                "forum": "2Rwq6c3tvr",
                "replyto": "82boqsgRhq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission992/Reviewer_6REU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission992/Reviewer_6REU"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response and the addition of a limitations section (even as a footnote to point out and encourage further research). That was the main point I wanted to raise. \n\nFurther, thank you for the insightful point-by-point discussion on each metric and how they (fail) to distinguish the source/type of contamination. As I agree this is beyond the scope of the paper, this explanation is beyond what I expected.\n\nI have adjusted my score to reflect my new confidence in the paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission992/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699076793,
                "cdate": 1700699076793,
                "tmdate": 1700699076793,
                "mdate": 1700699076793,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tt6KtrCPtQ",
            "forum": "2Rwq6c3tvr",
            "replyto": "2Rwq6c3tvr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission992/Reviewer_HqCE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission992/Reviewer_HqCE"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates data contamination in Large Language Models (LLMs). To address this issue, the paper suggests a novel approach where a random-length initial segment of the data instance is used as a prompt, along with information about the dataset name and partition type. The paper then assesses data contamination based on the LLM's output. This evaluation can be done by measuring the surface-level overlap with the reference instance or by leveraging GPT-4's few-shot prediction capabilities.\nBased on the results, the paper suggests that GPT-4 is contaminated with AG News, WNLI, and XSum datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method is straightforward and adaptable to a wide range of datasets."
                },
                "weaknesses": {
                    "value": "1. I have concerns regarding the soundness of the paper's evaluation methodology.\nThe proposed method hinges on the assumption that a data instance is contaminated in an LLM if the LLM can complete the instance based on its prefix. The paper's evaluation primarily revolves around how well the proposed methods are compared to human experts under this assumption However, these concerns raise doubts about whether the underlying assumption holds for several reasons.\n(1) The inability of an LLM to complete a reference does not necessarily imply that the instance was not used during training. It could be attributed to model forgetting or the model's failure to memorize surface-level features while still having learned the semantic-level features of the data instance. This could lead to a high false negative rate in the evaluation.\n(2) An LLM may have encountered the input of a data instance without having seen the actual data instance itself. For instance, in sentiment classification tasks, text can be included in the LLM's training set as long as its label is not provided alongside the text. The ability to complete the input text does not necessarily indicate data contamination in LLMs, potentially resulting in a high false positive rate.\n(3) The unavailability of training data details for ChatGPT and GPT-4 due to their proprietary nature prevents a comprehensive evaluation of the proposed method. The current evaluation primarily focuses on how closely the proposed model aligns with human guess about data contamination, in cases where the actual training data is undisclosed. It seems essential to assess the method on models with publicly accessible training data, such as OpenLlama.\n2. There's a potential fairness issue in using GPT-4 to evaluate the prediction from itself."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission992/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission992/Reviewer_HqCE",
                        "ICLR.cc/2024/Conference/Submission992/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission992/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698898212454,
            "cdate": 1698898212454,
            "tmdate": 1700516753283,
            "mdate": 1700516753283,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AFwvBYWPSu",
                "forum": "2Rwq6c3tvr",
                "replyto": "tt6KtrCPtQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission992/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission992/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response by Authors (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments on our paper, particularly regarding the soundness of our evaluation methodology. We appreciate the opportunity to address your concerns and provide further clarification on the underlying assumptions of our proposed method.\n\nRegarding your points 1.1 and 1.3, we would like to clarify that these concerns have already been considered and addressed in Section 3.3 of the paper. For point 1.2, we provide a detailed explanation below, complemented by an example for clarity. Moreover, to address your concern more thoroughly, we have included additional details in Appendix B in the updated version of our paper to show how infusing the labels in the input prompt accounts for false positives when generating downstream completions.\n\nWe address your concerns in the following in the same order they are listed.\n\n> 1.1. The inability of an LLM to complete a reference instance.\n\nWe understand that an LLM\u2019s inability to complete a partial reference instance could be due to various factors. However, the memorization ability of LLMs is a well-studied topic. Carlini et al. (2023) suggest that memorization in LLMs significantly grows as we increase (1) the capacity of the model; (2) the number of tokens of context used to prompt the model; and (3) the number of times an example has been duplicated. The first two are the ones we **directly** make use of in our study. Therefore, consistent failure/inability to complete partial reference instances more plausibly indicates the lack of prior exposure to the data, rather than forgetting surface-level features or failure in memorization, especially in the context of LLMs such as GPT-4 and GPT-3.5.\n\nFurther, in our study, we carefully examine **near-exact matches** to account for the exact scenarios you highlighted. To control the false negative rate in these cases, we empirically validated the number of near-exact matches that can signal partition-level contamination by performing controlled contamination on the base model of GPT-3.5 (Section 3.3). Then, based on these findings, we studied a set of numbers for both exact and near-exact matches as our **default setting** to generalize to the partition-level contamination. \n\nWe would like to emphasize that the aforementioned numbers we studied as our default setting for determining partition-level contamination are **not fixed**. As mentioned in our paper, these criteria, i.e., the number of exact or near-exact matches, can be adjusted depending on a specific context and the perceived severity of contamination in a given setting. This flexibility in our approach allows for the effective addressing of potential false negative concerns. In other words, our replication-based strategy is completely separate from the way that a dataset partition is flagged as contaminated based on the number of exact or near-exact matches, adding the benefit of proactively handling potential false negative concerns.\n\n> 1.2. The importance of label inclusion in the input prompt.\n\nThe pivotal idea behind incorporating the **exact label** in the input prompt is to exactly account for the false positive rates you are referring to. To address your concern, we intuitively show that when an LLM is instructed to complete a partial reference instance, the **exact label** corresponding to the reference instance is taken into account by the LLM for the downstream completion.\n\nTo exemplify, in the WNLI dataset, sentence 1 entries are not unique, meaning that the same sentence 1 can be used with a different sentence 2, resulting in a different label. As a result, this is a suitable case to show how including a label can impact downstream completion. Please consider the following examples and completions:\n\n*Example 1 (from the **validation** split of the WNLI dataset, used in our paper in Figure 1):* \\\nsentence 1: The dog chased the cat, which ran up a tree. It waited at the top. \\\nlabel: 1 (entailment) \\\nCompletion for sentence 2 by GPT-4: `The *cat* waited at the top.` (exact match)\n\n*Example 2 (from the **train** split of the WNLI dataset, taken to support our claim here):* \\\nsentence 1: The dog chased the cat, which ran up a tree. It waited at the top. \\\nlabel: 0 (not entailment) \\\nCompletion for sentence 2 by GPT-4: `The *dog* waited at the top.` (this is an **exact match** with respect to the instance from the **train** split of the WNLI dataset, but an **inexact match** with respect to the instance from the **validation** split of the WNLI dataset (Example 1))\n\n*These completions can be replicated using the same settings provided in the paper.*\n\nAs you can see, different labels lead to tailored completions, addressing your concern about false positives. We included additional details with more examples in Appendix B in the updated version of our paper to further emphasize the importance of label inclusion during the replication process."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission992/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368963710,
                "cdate": 1700368963710,
                "tmdate": 1700368963710,
                "mdate": 1700368963710,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5w43szFuAX",
                "forum": "2Rwq6c3tvr",
                "replyto": "14lCDkzILl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission992/Reviewer_HqCE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission992/Reviewer_HqCE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response. I find that the table 6 and 11 along with the explanation in rebuttal addressed my concerns in general. I've adjusted the scores to reflect this."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission992/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516718698,
                "cdate": 1700516718698,
                "tmdate": 1700516718698,
                "mdate": 1700516718698,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]