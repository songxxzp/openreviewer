[
    {
        "title": "RTFS-Net: Recurrent time-frequency modelling for efficient audio-visual speech separation"
    },
    {
        "review": {
            "id": "EbDNEhvCRP",
            "forum": "PEuDO2EiDr",
            "replyto": "PEuDO2EiDr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission891/Reviewer_A4oN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission891/Reviewer_A4oN"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel Recurrent Time-Frequency Separation Network architecture that performs audio-visual source separation tasks effectively and efficiently. The model is characterized by three parts. First, each modality goes through its own processing module, and then the cross-dimensional attention fusion (CAF) consolidates information from both modalities. The spectral source separation block performs masking-based separation. The separation results show promising improvement given the compact size and computational efficiency the new model architecture introduces."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The paper presents solid improvement compared to the existing baseline systems. Considering the amount of model compression the proposed model introduced, the improvement is significant.\n\n- All the procedures and modules are well-defined with enough details.\n\n- The choice of the model architectures makes sense, including the dual-path structure, attention-based consolidation, and complex masks.\n\n- Ablation studies are thorough."
                },
                "weaknesses": {
                    "value": "While the paper is packed with useful information, there are still some parts that need elaboration.\n\n- As the authors mention, the dual-path RNN idea is not new to this problem. I understand that the authors chose SRU for their complexity-related considerations, but I also wonder if the audio processing module could benefit from its own self-attention mechanism, such as in the SepFormer model. \n\n- The spectral source separation module might be the weakest contribution, because complex masks have been extensively studied in the audio-only source separation literature. \n\n- I wish the paper provides more details on the TDANet block for video processing, which is relegated to the reference in the current version."
                },
                "questions": {
                    "value": "- The authors chose to \"add\" f_1 and f_2 (eq 11) after the CAF processing. I think it's a little abrupt in the sense that there might be other choices that preserve the unique information that each vector learns, such as concatenation. Have the authors considered other ways to combine the two vectors?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission891/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785372938,
            "cdate": 1698785372938,
            "tmdate": 1699636015633,
            "mdate": 1699636015633,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uv6VMrzCcy",
                "forum": "PEuDO2EiDr",
                "replyto": "EbDNEhvCRP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission891/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer A4oN"
                    },
                    "comment": {
                        "value": "We appreciate the effort you've invested in reviewing our manuscript and providing detailed feedback. Your insightful suggestions have been incredibly valuable, and we're confident that by addressing the points you've raised, we can enhance the quality of our paper. Below, we've addressed each of the concerns you've highlighted in your review:\n\n***Q1: As the authors mention, the dual-path RNN idea is not new to this problem. I understand that the authors chose SRU for their complexity-related considerations, but I also wonder if the audio processing module could benefit from its own self-attention mechanism, such as in the SepFormer model.***\n\n**A1:** We experimented with different sequence processing architectures in Appendix D, which has now been updated to include several transformer designs. The self-attention mechanism leads to a decrease of 1 dB SI-SNRi, while significantly increasing the memory requirements of the model. Other transformer approaches resulted in extremely high parameter counts, which contradicts our aim of constructing a lightweight AVSS model.\n\n***Q2: The spectral source separation module might be the weakest contribution, because complex masks have been extensively studied in the audio-only source separation literature.***\n\n**A2:** Some research has been conducted in the AOSS field using complex masks, but such studies are relatively scarce in the context of audio-visual speech separation. Additionally, to our knowledge, current state-of-the-art audio-only source separation methods, such as Sepformer, still employ real-valued masks. Indeed, TF-GridNet, the current AOSS SOTA method, does not use masks at all and directly computes the separated sources. We provided a study on all these different methods in Table 4. It is important to note that results in AOSS do not necessarily translate to the AVSS field.\n\n***Q3: I wish the paper provides more details on the TDANet block for video processing, which is relegated to the reference in the current version.***\n\n**A3:** We have included an explanation of the video preprocessing block (VP BLOCK) in Appendix A.\n\n***Q4: The authors chose to \"add\" $\\mathbf{f}_1$ and $\\mathbf{f}_2$ (eq 11) after the CAF processing. I think it's a little abrupt in the sense that there might be other choices that preserve the unique information that each vector learns, such as concatenation. Have the authors considered other ways to combine the two vectors?***\n\n**A4:** In Table 2 of our paper we compare the CAF block with pure concatenation (CTCNet Fusion (adapted)) of the audio and visual features. We observe that despite our CAF Block using only 3.6% ofthe parameters and 1.3% of the MACs, it outperformed pure concatenation by a large margin.\n\nThe purpose for the CAF Block is to produce a low-parameter and MAC alternative to CTCNet\u2019s fusion approach. If we directly concatenated $\\mathbf{f}_1$ and $\\mathbf{f}_2$, and then downscaled to the correct channel dimension using a convolution, this would be the same parameter and MAC count as the pure concatenation discussed above and in Table 2 of our paper. However, in order to fully answer your question, we ran an experiment where we concatenated $\\mathbf{f}_1$ and $\\mathbf{f}_2$ and then downscaled using a group convolution. Using a group convolution does not avoid the memory increase that comes from the concatenation, but it does avoid increasing the parameter and MAC counts significantly. The results show that this is an ineffective operation, and that a simple summation should be preferred.\n\n|  Method       |   Params  (K)  |  MACs  (M)  |  LRS2-2Mix SI-SNRi      |  LRS2-2Mix SDRi       |\n| --------------- | ------------- | --------------- | ------------- | ------------- |\n|  Add  (ours)  |  14.1       |  14.3  |  739  |  21,896  |\n|  Concat       |  13.7       |  13.9  |  741  |  21,913  |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700375021975,
                "cdate": 1700375021975,
                "tmdate": 1700375021975,
                "mdate": 1700375021975,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GEHeUMhb7A",
                "forum": "PEuDO2EiDr",
                "replyto": "uv6VMrzCcy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission891/Reviewer_A4oN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission891/Reviewer_A4oN"
                ],
                "content": {
                    "title": {
                        "value": "Confirm"
                    },
                    "comment": {
                        "value": "I confirm the author response was taken into account."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661574331,
                "cdate": 1700661574331,
                "tmdate": 1700661574331,
                "mdate": 1700661574331,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YiPLEJMydw",
            "forum": "PEuDO2EiDr",
            "replyto": "PEuDO2EiDr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission891/Reviewer_ZTD4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission891/Reviewer_ZTD4"
            ],
            "content": {
                "summary": {
                    "value": "The authors build upon previous research in audio-only and audio-visual speech recognition by focusing on improving efficiency and fidelity of separated speech. They draw a lot of inspiration from the CTCNet paper and extend it to the TF domain to improve the efficiency. The solution has been evaluated on standard benchmark datasets and compared to previous state-of-the-art methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Audio samples of separation are available and source to be made available when the paper is published.\n2. The writing is easy to follow.\n3. Clear modeling details are provided."
                },
                "weaknesses": {
                    "value": "1. The baseline methods listed in table 1 should include their references.\n2. RTFS-Net-12 is only about 10% more efficient that CTCNet. How much difference does that make in practical applications?\n3. Some of the comparison examples are not distinguishable to this reviewer. This makes me wonder how to interpret the relative SNR gains."
                },
                "questions": {
                    "value": "1. Since the best performance is achieved with R=12, why not explore a higher R?\n2. Have the authors considered conducting studies with human listeners? If the target application is ASR, would it be helpful to measure WER in a recognition task?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission891/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808187174,
            "cdate": 1698808187174,
            "tmdate": 1699636015554,
            "mdate": 1699636015554,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BKdzDjcbB8",
                "forum": "PEuDO2EiDr",
                "replyto": "YiPLEJMydw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission891/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer ZTD4"
                    },
                    "comment": {
                        "value": "Thank you very much for your valuable feedback. We have revised the manuscript and added some experiments to demonstrate the superiority of our model. We would also like to make some comments below to address your specific questions:\n\n***Q1: The baseline methods listed in table 1 should include their references.***\n\nA1: Thank you for spotting that, we have updated the table.\n\n***Q2: RTFS-Net-12 is only about 10% more efficient that CTCNet. How much difference does that make in practical applications?***\n\nA2: From the 10% figure we assume that the efficiency you mention is RTFS-Net's inference time of 109.9ms vs CTCNet's inference time of 122.7ms. The inference time is the time taken to process 2 seconds of audio. Scaling up to 1000 hours of audio, it would take 54.95 hours for RTFS-Net-12 and 61.35 hours for CTCNet. Since the separation performance of RTFS-Net is higher, and the time taken is shorter, the logical solution would be to use RTFS-Net as it would save around 6 and a half hours of GPU processing, and the associated electricity cost. Further, the performance drop from switching to RTFS-Net-4 is pretty minimal. Using this model, 1000 hours of audio would only take 28.9 hours.\n\nThe total parameters and MACs are also important factors to be considered in practical applications. Compared to CTCNet, RTFS-Net-12 has reduced computational costs by three times, while the number of parameters utilized is only one-tenth.\n\n***Q3: Some of the comparison examples are not distinguishable to this reviewer. This makes me wonder how to interpret the relative SNR gains.***\n\nA3: You are correct. The purpose of the demo, and indeed this paper, is to show that SOTA performance can be replicated using a fraction of the parameters and a greatly reduced model complexity. For example, in Appendix C we showed that if we do not compress the frequency and time resolutions of the audio features (i.e. q=1), we can outperform the SOTA method by a significant margin using only R=4 repetitions. Using 16 or more layers with q=1 would greatly increase the SNR further. However, the large memory requirement and computational complexity associated with this approach defeats the purpose of our paper.\n\n**Q4: Since the best performance is achieved with R=12, why not explore a higher R?**\n\nA4: A higher value of R leads to increased computational demands, which contradicts our intention to propose a lightweight model. Nonetheless, we still present a result as shown below. The results indicate that a higher R achieves better separation quality, but it comes with a more significant computational cost. Note that because we share parameters, increasing R only increases the MACs and not the parameter count.\n\nRTFS-Net with 16 and 20 layers on LRS2 dataset\n\n|  Model        |  SI-SNRi  |  SDRi  |  Params (K)  |  MACs (G)  |\n| --------------- | ----------- | -------- | -------------- | ------------ |\n|  RTFS-Net-12  |  14.9     |  15.1  |  739         |  56.4      |\n|  RTFS-Net-16  |  15.2     |  15.5  |  739         |  73.7      |\n|  RTFS-Net-20  |  15.4     |  15.6  |  739         |  90.9      |\n\nRTFS-Net-20 outperforms CTCNet by over 1dB in both SDRi and SI-SNRi, while still utilizing only 91G MACs \u2013 just over half the MACs utilized by CTCNet. However, 20 RTFS Blocks requires a much larger GPU memory for training. As a result, we had to use NVIDIA 3090/4090s in order to obtain these results in a short period of time.\n\n***Q5: Have the authors considered conducting studies with human listeners? If the target application is ASR, would it be helpful to measure WER in a recognition task?***\n\nA5: We tested the Automatic Speech Recognition (ASR) results on the LRS2 test set. The results show that RTFS-Net generalizes well to downstream tasks, achieving almost identical performance to CTCNet \u2013 the previous SOTA method. For details, please refer to our response to the third question from Reviewer #1FXL.\n\nWe have shared a demo in the supplementary material that can be listened to at your leisure, and we have shared our results with other members of our lab. However, conducting a formal study with large quantities of human participants or professional stenographers lies beyond the scope of our current research and is not feasible within the limited timeframe given for responding to reviews and questions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700374603759,
                "cdate": 1700374603759,
                "tmdate": 1700374603759,
                "mdate": 1700374603759,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4UwCmTptE7",
                "forum": "PEuDO2EiDr",
                "replyto": "BKdzDjcbB8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission891/Reviewer_ZTD4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission891/Reviewer_ZTD4"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledging authors' responses"
                    },
                    "comment": {
                        "value": "Thank you for the thoughtful responses. Adding the new experimental results on ASR and the variation of R to the manuscript will be necessary to drive clarity. Additionally, the qualification on the subjective improvement is also important for the reader.\n\nSome of the tradeoffs have to be driven home more clearly.  If the biggest contribution is that the system achieves SOTA WER and human listening perception results at a tiny fraction of the computational cost, that has to be primary narrative with a clear focus on the importance of 10-13% savings in practical applications."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673263384,
                "cdate": 1700673263384,
                "tmdate": 1700673263384,
                "mdate": 1700673263384,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hNmXJUkMcS",
            "forum": "PEuDO2EiDr",
            "replyto": "PEuDO2EiDr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission891/Reviewer_siqV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission891/Reviewer_siqV"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes RTFS-Net, a new time-frequency (TF) domain audio-visual speech separation method. It introduces three main innovations:\n\n* RTFS Blocks independently model time and frequency dimensions of audio\n* Cross-dimensional Attention Fusion Block efficiently fuses audio and visual data\n* Spectral Source Separation Block preserves phase/amplitude information\n\nExperiments show RTFS-Net matches or beats prior time domain methods on LRS2, LRS3, and VoxCeleb2 datasets, while using 10x fewer parameters and 3-6x fewer computations.\n\nRTFS-Net is the first TF model to surpass most contemporary time domain methods for audio-visual speech separation. It demonstrates TF domain methods can achieve good performance at lower computational cost through novel modeling of time-frequency spectrograms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Achieves near state-of-the-art performance for audio-visual speech separation while being very parameter and computationally efficient\n* Outperforms all compared time domain methods, proving time-frequency domain modeling can achieve better performance if done effectively\n* Innovative modeling of time and frequency dimensions independently in RTFS Blocks\n* Attention-based fusion mechanism in CAF Block is very lightweight but fuses audio and visual data very effectively\n* Spectral Source Separation Block properly handles phase/amplitude to avoid losing audio information\n* Model code and weights will be released for full reproducibility"
                },
                "weaknesses": {
                    "value": "* Testing is limited to only 2 speaker mixtures. Performance with more speakers is uncertain.\n* Missing PESQ evaluation in results which most other target speech extraction papers provide\n* Doesn't include latest SOTA model comparison: Dual-Path Cross-Modal Attention for Better Audio-Visual Speech Extraction, ICASSP 2023.\nhttps://arxiv.org/pdf/2207.04213.pdf. This gives superior performance for SI-SNRi and provides PESQ results as well. It does not provide MACs analysis."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission891/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission891/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission891/Reviewer_siqV"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission891/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698994464164,
            "cdate": 1698994464164,
            "tmdate": 1700434630030,
            "mdate": 1700434630030,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0FfriNSLNg",
                "forum": "PEuDO2EiDr",
                "replyto": "hNmXJUkMcS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission891/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer siqV"
                    },
                    "comment": {
                        "value": "We would like to thank you for your very positive feedback. We are honored that you have found potential in our work. Please find below our answers to the main questions that you have raised:\n\n***Q1: Testing is limited to only 2 speaker mixtures. Performance with more speakers is uncertain.***\n\n**A1:**   We tested our model on two-speaker mixtures because most existing methods are verified in this manner. Furthermore, it is important to note that our model incorporates visual information from a single speaker to isolate and extract the audio of this target speaker. The integration of visual cues plays a crucial role in enhancing the accuracy of our model, as well as other AVSS models. Hence, we are not limited by the number of speakers.\n\nWe randomly sampled speakers from the LRS2-2Mix test set to obtain a dataset with three speakers. The results indicate that the presence of multiple speakers somewhat impacts the model's performance. However, it is important to note that these models were not trained on the dataset with three speakers, only tested on the dataset with three speakers. In addition, RTFS-Net-12 outperformed CTCNet by a significant margin, scaling much better to the harder task.\n\n|  Model          |  SI-SNRi  |  SDRi  |\n| ----------------- | ----------- | -------- |\n|  AV-ConvTasNet  |  10.0     |  10.3  |\n|  AVLIT-8        |  10.4     |  10.8  |\n|  CTCNet         |  12.5     |  12.8  |\n|  RTFS-Net-12    |  13.7     |  13.9  |\n\n***Q2: Missing PESQ evaluation in results which most other target speech extraction papers provide.***\n\n**A2:** We have added the PESQ evaluation metrics to Table 1 in the updated paper. As with SI-SNRi and SDRi, the PESQ scores show that RTFS-Net obtains comparable accuracy to the previous SOTA method (CTCNet) across all three datasets, while utilizing fewer parameters and MACs.\n\n***Q3: Doesn't include latest SOTA model comparison: Dual-Path Cross-Modal Attention for Better Audio-Visual Speech Extraction, ICASSP 2023. https://arxiv.org/pdf/2207.04213.pdf. This gives superior performance for SI-SNRi and provides PESQ results as well. It does not provide MACs analysis.***\n\n**A3:** The model code for this ICASSP publication is not open source. Its results on the LRS3 dataset for the model \u201cAV-TASNET\u201d are different from ours, indicating that either their mixing method is different, or that their train/eval/test splits might be different. For similar reasons, we also did not include AV-Sepformer ([https://arxiv.org/abs/2306.14170](https://arxiv.org/abs/2306.14170)) in our paper. Both of these methods are Sepformer-like, meaning the parameter count for each model is at least 20 million. However, by comparing our paper with AV-Sepformer\u2019s PESQ values, this method also does not outperform CTCNet or RTFS-Net (AV-Sepformer: 2.31, CTCNet: 3.00, RTFS-Net: 3.00).\n\nWe would also like to emphasize that the purpose of RTFS-Net is not to obtain SOTA performance. As can now be seen in the updated Appendix C, we can easily outperform CTCNet by directly applying TF-GridNet to the audio-visual setting with only 4 RTFS-Net block layers. The focus of RTFS-Net is to obtain comparable performance to the SOTA method using a very small model footprint. Our contribution is a fast, efficient and quick-to-train model."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700374293568,
                "cdate": 1700374293568,
                "tmdate": 1700374293568,
                "mdate": 1700374293568,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0rNdAm3Siu",
            "forum": "PEuDO2EiDr",
            "replyto": "PEuDO2EiDr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission891/Reviewer_1FXL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission891/Reviewer_1FXL"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors present a novel time-frequency domain audio-visual speech separation method (Recurrent Time-Frequency Separation Network), a unique attention-based fusion technique for the efficient integration of audio and visual information, and a new mask separation approach. Results show that the proposed approach outperforms the previous SOTA method using only 10% of the parameters and 18% of the MACs. The authors claim that this is the first time-frequency domain audio-visual speech separation method to outperform all contemporary time-domain SOTA ones."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "One main strength of this paper is the proposed Recurrent Time-Frequency Separation Network that processes the data in the frequency dimension, the time dimension, and the joint time-frequency dimension."
                },
                "weaknesses": {
                    "value": "It is mentioned that the RTFS blocks share parameters (including the AP Block), leading to reduced model size and increased performance. Therefore, more description/explanation for this would be helpful."
                },
                "questions": {
                    "value": "Are there any overlapping speech in the train/test data?\nDo the authors perform any downstream task like speech recognition on the reconstructed speech?\nOne possible downstream task for speech separation is speech-to-speech dubbing. In this case, both the speech and background/nonspeech sound are needed. Have the authors looked into reconstructing background/nonspeech sound?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission891/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission891/Reviewer_1FXL",
                        "ICLR.cc/2024/Conference/Submission891/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission891/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699482439551,
            "cdate": 1699482439551,
            "tmdate": 1700632278710,
            "mdate": 1700632278710,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7uxwSni1kj",
                "forum": "PEuDO2EiDr",
                "replyto": "0rNdAm3Siu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission891/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer 1FXL"
                    },
                    "comment": {
                        "value": "We would like to thank you for the time taken to review our paper and for your extensive comments. Your suggestions are very helpful, and we believe we can significantly improve the paper by making the respective adjustments. We have compiled the issues that you have pointed out and answered them as follows:\n\n***Q1: Explain the benefits of shared parameters.***\n\n**A1:** The purpose of sharing parameters between the RTFS Block layers is to create an overall network architecture that uses recurrent connections to continuously refine the clarity of the model\u2019s output through the R \u201ctime steps\u201d, just like an RNN. Additionally, this approach also reduces the quantity of model parameters, creating a more lightweight design. A more detailed discussion can be found in Appendix B on the updated paper. From the results in Appendix B (copied below), we can see that sharing the parameters reduces the model size from 2 million parameters to 739,000 parameters, while also delivering a slight performance improvement.\n\n| Shared Parameters | LRS2-2Mix SI-SNRi | LRS2-2Mix SDRi | Params (K) | MACs (G) |\n|-------------------|-------------------|----------------|------------|----------|\n| Not shared        | 13.6              | 13.9           | 2,183      | 21.9     |\n| Shared            | 13.7              | 14.0           | 739        | 21.9     |\n\n***Q2: Is there any overlapping speech in the train/test data?***\n\n**A2:** In our approach, we did not impose any restrictions on the extent of overlap in the speech samples within the training and test sets. As a result, the data in both sets encompasses a diverse range of overlaps, including heavily overlapped, moderately overlapped, and non-overlapping speech. This variety ensures a comprehensive evaluation of the model's performance across different degrees of speech overlap.\n\n***Q3:Downstream task.***\n\n**A3:** The speech separation task is a front-end task generally used to enhance speech recognition accuracy. To address your question, we tested the speech recognition accuracy on the LRS2-2Mix test dataset. We utilized the publicly available Google Speech-to-Text API to obtain the recognition results. Our focus was on measuring the Word Error Rate (WER), where a lower rate is indicative of superior performance. The experimental results demonstrate that our model can achieve competitive speech recognition accuracy, validating the performance of our model in downstream tasks. However, the task of separating background noise, also referred to as universal voice separation, was not investigated in this paper as it falls outside the scope of our current research. Our work, and all previous works that we compare with in our paper, are specifically designed for target speaker extraction \u2013 using the lip movements of a target speaker to extract that speaker\u2019s audio signal. As there are no lip movements associated with background noise, implementing new modules, or modifying the architecture would be necessary in order to address the task of speech-to-speech dubbing. However, it represents an intriguing task for future research.\n\n|  Models         |  SDRi  |  WER (%)  |\n| ----------------- | -------- | ----------- |\n|  Mixture        |  -     |  84.91    |\n|  Ground-truth   |  -     |  17.74    |\n|  CaffNet-C      |  12.5  |  32.96    |\n|  Visualvoice    |  11.8  |  34.45    |\n|  AV-ConvTasNet  |  12.8  |  31.43    |\n|  AVLIT-8        |  13.1  |  31.85    |\n|  CTCNet         |  14.6  |  24.82    |\n|  RTFS-Net-4     |  14.3  |  29.66    |\n|  RTFS-Net-8     |  14.8  |  27.42    |\n|  RTFS-Net-12    |  15.1  |  24.93    |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700373970481,
                "cdate": 1700373970481,
                "tmdate": 1700373970481,
                "mdate": 1700373970481,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NuPUSx8rD4",
                "forum": "PEuDO2EiDr",
                "replyto": "7uxwSni1kj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission891/Reviewer_1FXL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission891/Reviewer_1FXL"
                ],
                "content": {
                    "comment": {
                        "value": "It is great to see that the proposed approach works well with the downstream ASR task with reduced model size."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672331237,
                "cdate": 1700672331237,
                "tmdate": 1700672331237,
                "mdate": 1700672331237,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]