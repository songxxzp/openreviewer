[
    {
        "title": "CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets"
    },
    {
        "review": {
            "id": "tiCV4QnXcv",
            "forum": "G0vdDSt9XM",
            "replyto": "G0vdDSt9XM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3233/Reviewer_3JvW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3233/Reviewer_3JvW"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents CRAFT, a framework for tool creation and retrieval to customize large language models (LLMs) for various tasks and domains. CRAFT creates a specialized toolset by prompting LLMs to generate and abstract code solutions for problems, and validates and deduplicates the tools. CRAFT retrieves relevant tools from the toolset by multi-view matching, and adds them to the prompt of LLMs for code generation. CRAFT improves performance on vision-language, tabular processing, and mathematical reasoning tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper proposes a tool generation and tool-using framework for LLMs which is a good attempt to enhance LLMs' capability to solve reasoning tasks by generating programs.\n\n2. Personally I think the idea of a \"pseudocode library\" proposed in future work is cool and meaningful.\n\n3. The experiments on baselines and different LLMs are comprehensive and the result is promising. Basically, I agree with the authors that tool generation puts a high demand on the LLMs' coding ability.\n\n4. The created toolsets are a particularly important contribution to the LLM community."
                },
                "weaknesses": {
                    "value": "1. The authors mentioned that alternative backbone models like CodeLlama demonstrate near-random performance. Can the authors provide such results (the performance of different LLMs in creating and using tools) in the experiment?\n\n2. I suggest that the author should make the distinction between more specific methods more prominently in the main text (though the difference has been discussed in the experimental setting), such as by creating a table to compare various tool-augmented language model methods, and so on. The current Figure 1 appears to be similar to previous work like LATM, making it difficult to showcase the uniqueness of this article."
                },
                "questions": {
                    "value": "1. What does \"bug-free\" mean and how do the authors ensure that the generated tools are \"bug-free\"?\n\n2. What is the result of tool generation with GPT-3.5-turbo and other LLMs?\n\n3. Can CRAFT be adapted to programming tasks like HumanEval since it generates \"APIs\"?\n\n4. Can the authors discuss more on the \"pseudocode library\" like can we use a natural language library and how is it different from in-context learning?\n\n5. Can the authors analyze more on the created toolsets like where they might it can be applied/generalized?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3233/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3233/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3233/Reviewer_3JvW"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3233/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698668438607,
            "cdate": 1698668438607,
            "tmdate": 1699636271660,
            "mdate": 1699636271660,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qF7WMx4r67",
                "forum": "G0vdDSt9XM",
                "replyto": "tiCV4QnXcv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3233/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3233/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for thinking our work makes an important contribution and also acknowledge the potential in our future work proposal. We are terribly sorry for giving late author responses because we are running some additional experiments, which consume a lot of time for autoregressive generation. The following is our response. \n\nWeakness\n1. Can the authors provide such results (the performance of different LLMs in creating and using tools) in the experiment?\n\n| Model         | GQA   |       | OK-VQA |       | A-OKVQA |      |\n|---------------|-------|-------|--------|-------|---------|------|\n|               | Acc   | F1    | Acc    | F1    | Acc     | F1   |\n| CodeLLaMA-13B | 4.78  | 6.82  | 2.31   | 3     | 5.32    | 8.47 |\n\n\nIn our experiments, we try CodeLLaMA-13B and observe that they can only achieve near-random performance on the visual tasks, as demonstrated in the table.\n\n2. I suggest that the author should make the distinction between more specific methods more prominently in the main text.\n\nThanks for your advice.  We summarize the distinctions between our work and previous work regarding tool creation in the following table: \n| Tool-Creation Method | Dataset for Create Tools | Reuse Tools? | Tool Base Size            | Retrieval-enhanced? |\n|----------------------|--------------------------|--------------|---------------------------|---------------------|\n| CREATOR              | Test Set                 | No           | 0                         | No                  |\n| LATM                 | Train Set                | Yes          | 1                         | No                  |\n| CRAFT                | Instruction Dataset or Train Set | Yes | >100; Theoretically Unlimited | Yes              |\n\nThe major difference between CRAFT and LATM is that LATM uses three training samples to create only one tool for one dataset, and directly apply the tool to all downstream test samples, which is neither scalable nor generalizable. Hence, LATM cannot tackle tasks that contain diverse patterns. In contrast, with the help of our retrieval method, we are able to scale up the tool set and address various problems. Experiments in Table 1 have demonstrated that CRAFT achieves better performance."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3233/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708611274,
                "cdate": 1700708611274,
                "tmdate": 1700708611274,
                "mdate": 1700708611274,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "efa9X7HXdB",
            "forum": "G0vdDSt9XM",
            "replyto": "G0vdDSt9XM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3233/Reviewer_nWn7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3233/Reviewer_nWn7"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces CRAFT, a novel framework designed to enhance large language models (LLMs) by creating and retrieving task-specific tools. CRAFT creates toolsets first and equips LLMs with a component that retrieves these tools to solve complex tasks. Experiments on vision-language, tabular processing, and mathematical reasoning tasks demonstrate the superiority of this approach over strong baselines. The analysis reveals that performance improvement is consistent when scaling the number of tools and the capability of backbone models, and that the created tools exhibit low complexity and atomicity."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tTraditional approaches to augment LLMs with tools lack flexibility, as they rely on general-purpose APIs. CRAFT addresses this problem by reusing task-related tools, which is more flexible.\n2.\tThis method can adapt off-the-shelf LLMs to new domains and modalities without finetuning.\n3.\tExperiments show that the proposed framework can improve a lot compared to previous approaches."
                },
                "weaknesses": {
                    "value": "1.\tThe setting of the experiments is a little bit limited. There are many agent benchmarks like MINT, AgentBench, and so on, which focus on the problem-solving capacity of LLMs as agents. The reviewer thinks the work needs to be further verified on broader benchmarks for agents.\n2.\tThe comparison with LATM is a little bit unfair. The toolset created by CRAFT is the output of GPT-4, while the tool used by LATM is created by an inferior model if there is no misunderstood.\n3.\tThe transferability of the toolset should be discussed as I noticed that the toolset for the VQA task and the toolset for the reasoning task are not the same. Maybe the authors can experiment to create a general tool set for all tasks and see what will happen."
                },
                "questions": {
                    "value": "Please see the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3233/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698717545417,
            "cdate": 1698717545417,
            "tmdate": 1699636271583,
            "mdate": 1699636271583,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7SH4soGmgt",
                "forum": "G0vdDSt9XM",
                "replyto": "efa9X7HXdB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3233/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3233/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank reviewers for thinking that our approach is flexible to augment LLMs compared to traditional ones. The following is our response. \n\nWeaknesses\n1. The setting of the experiments is a little bit limited. \n\nResponse: Thanks for the advice. We did not include recent agent benchmarks, such as MINT and AgentBench, in our paper since they came out concurrently with CRAFT. We are also interested to see how CRAFT can benefit agents, but due to the time limits in the author response period, we are unable to adapt CRAFT to those benchmarks and finish the evaluation in time. However, since there is some overlap between our task and MINT/AgentBench, we expect the trends in our submission will translate to those in these two benchmarks.\n\n2. The comparison with LATM is a little bit unfair.\n\nResponse: In fact, for fair comparisons, CRAFT follows exactly the same setting as LATM, and creates tools using GPT-4 and uses tools with GPT-3.5-Turbo. We will highlight this in the revision. \n\n3. The transferability of the toolset should be discussed.\n\nResponse: \nThanks for the suggestion. We have tried to mix the tools of different tasks into a single set and observe that this has little impact on the downstream task performance. We think this  can be attributed to the retrieval approach in CRAFT, which will only select tools that are most useful for the problem. We will include this in the revision."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3233/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708259175,
                "cdate": 1700708259175,
                "tmdate": 1700708259175,
                "mdate": 1700708259175,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "e08cIfMo0P",
            "forum": "G0vdDSt9XM",
            "replyto": "G0vdDSt9XM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3233/Reviewer_A2gB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3233/Reviewer_A2gB"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces CRAFT, a novel framework for augmenting Large Language Models (LLMs) with specialized tools to tackle complex tasks. CRAFT generates task-specific toolsets and provides a retrieval component, enabling LLMs to offload functions to external modules through code snippets and APIs. This approach overcomes the limitations of general-purpose APIs, offering tailored solutions and improved flexibility. The framework ensures the quality and reusability of tools through validation, abstraction, and deduplication processes. Experiments across various domains, including vision-language, tabular processing, and mathematical reasoning, demonstrate substantial improvements over strong baselines. The paper's in-depth analysis confirms the scalability of CRAFT, the significance of each component, and the reliability and simplicity of the created tools. Ultimately, CRAFT presents a plug-and-play solution, enhancing the adaptability and problem-solving capabilities of off-the-shelf LLMs without requiring finetuning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. CRAFT showcases originality by combining tool learning, code generation, and retrieval to enhance LLMs' capabilities, applying this novel approach across various tasks and domains.\n\n2. The framework is rigorously validated across different tasks, demonstrating substantial improvements and ensuring tool correctness and reusability, reflecting the high quality of the work.\n\n3. The paper is well-structured and clearly written, providing a comprehensive presentation of the CRAFT framework, its applications, and experimental results.\n\n4. CRAFT addresses crucial challenges in augmenting LLMs, offering a significant advancement in the field and demonstrating practical applicability and effectiveness across diverse domains."
                },
                "weaknesses": {
                    "value": "1. The paper could benefit from a more detailed exploration of scenarios where CRAFT might not perform as expected. Understanding the limitations and potential failure cases of the framework would provide a more balanced view and help guide future improvements.\n\n2. While the paper compares CRAFT to several strong baselines, expanding this comparison to include a wider range of existing tools and frameworks (e.g., SOTA methods in VQA) would strengthen the validity of the claimed improvements. This would also help in positioning CRAFT more clearly in the landscape of existing solutions.\n\n3. The paper could provide a more in-depth analysis of the tool creation and retrieval components of CRAFT. Understanding how different types of tools contribute to performance improvements and how the retrieval mechanism interacts with various tasks would offer valuable insights.\n\n4. While the paper mentions the scalability of CRAFT, providing empirical evidence and a more thorough discussion on how the framework scales with the number of tools and the complexity of tasks would be beneficial.\n\n5. The paper could explore and address potential biases in the tool creation process, especially considering the reliance on GPT-4 for generating code solutions. Ensuring fairness and mitigating biases is crucial for the applicability of CRAFT across diverse scenarios.\n\n6. Including a user study or examples of real-world applications of CRAFT could provide additional validation of the framework's practicality and effectiveness, offering a more comprehensive evaluation."
                },
                "questions": {
                    "value": "1. Failure Cases: Can the authors provide specific examples or scenarios where CRAFT may not perform optimally? Insight into challenges or limitations faced by the framework would be valuable for a comprehensive understanding.\n\n2. Baseline Comparison: Could the authors expand on the choice of baselines used for comparison? Including a broader range of existing tools and frameworks (existing SOTA methods) might help in better positioning CRAFT\u2019s contributions.\n\n3. Tool Creation and Retrieval Analysis: How do different types of tools contribute to the performance improvements observed with CRAFT? Additionally, how does the tool retrieval mechanism interact with various tasks?\n\n4. Real-World Application: Are there examples of real-world applications where CRAFT has been applied? Including such examples or results from a user study could provide additional validation for the framework.\n\n5. Tool Abstraction and Deduplication: Could the authors elaborate on the process of abstracting code solutions into reusable snippets and the criteria used for deduplication? Understanding this process in detail would provide clarity on the quality assurance of tools.\n\n6. Handling of Descriptive Responses: The paper addresses potential issues with underestimated performance due to descriptive responses from LLMs. Could the authors provide more details on how this issue is handled or mitigated in CRAFT?\n\n7. Scalability: The paper mentions the scalability of CRAFT. Could the authors provide empirical evidence or a more detailed discussion on how the framework scales with the number of tools and the complexity of tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3233/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3233/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3233/Reviewer_A2gB"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3233/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817655729,
            "cdate": 1698817655729,
            "tmdate": 1699636271485,
            "mdate": 1699636271485,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b1j9zMcHlc",
                "forum": "G0vdDSt9XM",
                "replyto": "e08cIfMo0P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3233/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3233/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are glad the reviewer thinks CRAFT addresses crucial challenges in augmenting LLMs. The following are our authors' responses.\n\nWeaknesses\n1. The paper could benefit from a more detailed exploration of scenarios where CRAFT might not perform as expected.\n\nResponse: Thanks for the advice. We believe that such a systematic analysis is interesting, but during this response period, we failed to find an automatic method to identify and categorize them. Therefore, it may be beyond our capacity to finish the analysis in such a short time, but we will try to figure it out in the revision when time is available.\n\nWe identify one specific failure mode in the CRAFT framework. In the abstraction process, there are some cases of successfully renaming the function names and writing a general docstring, and thus the tools look generalizable outside. However, inside the function, the specific variable names or textual inputs may fail to be converted into generic ones (expected results are shown in fig.2, replace all specific variable names with general ones, e.g., cat\u2192animal, desk\u2192object) and wrap textual inputs of internal function calls as arguments of the tool (e.g., date = df[\"date\"]\u2192date = df[column name], where the value of column name is passed in by tool users). Therefore, there are still some question-specific input-output formats. That said, the tool functions can only tackle a specific problem, instead of all problems of the same type. Such a mismatch between tool description and its implementation of the functionality could lead to errors when LLMs tend to call the tool to solve other problems. A potential solution from the tool side is to improve LLMs\u2019 instruction-following ability so that they can abstract tools better, or introduce an extra verification process to filter out tools that \u201coverclaim\u201d their original functionality. In this case, we can further improve the quality of the created toolsets.\n\n2.  Expanding the comparison to include a wider range of existing tools and frameworks.\n\nResponse: \n| Method               | TabMWP | MATH |\n|----------------------|--------|------|\n| GPT-3.5-Turbo Vanilla| 68.2   | 25.7 |\n| POT+COT              | 80.0   | 54.0 |\n| POT+Rectify          | 81.2   | 63.8 |\n| POT+Rectify+COT      | 87.3   | 61.4 |\n| CRAFT                | 88.4   | 68.1 |\n\n\nWe thank the reviewer for their constructive feedback. We have added four baselines on TabMWP and MATH. VQA, a third task we consider, takes more engineering efforts. As a result, we were not able to complete this experiment on VQA given limited time. Concretely, \u2018GPT-3.5-Turbo Vanilla\u2019 directly asks GPT-3.5-Turbo model to solve the problems; Program-Of-Thought (POT)+ Chain-Of-Thought (COT), on top of POT, adds COT prompting at the beginning; POT+Rectify introduces a rectification process when error occurs in POT programs; POT+Rectify+COT combines the above two techniques.\n\nDetails on rectification: If an error occurs, then the LLM is prompted with demonstrations to correct the error. Applying a similar prompt format as before, the format of demonstrations \u201c[EXAMPLE x]\u201d now changes to \u201c### Question [QST]\\n ### Original [ORI]\\n ### Error [ERR]\\n ### Rectification [REC]\u201d, where we provide the original tool implementation and calling decision in [ORI], offer the error tracebacks [ERR], and concatenate natural language reasoning on the error with the rectified code in [REC]. \nIn summary, both COT and the Rectify process can enhance POT on these two tasks, and their combination may further boost POT performance on TabMWP. However, CRAFT still outperforms all of them on both tasks.\n\n3. The paper could provide a more in-depth analysis of the tool creation and retrieval components of CRAFT.\n\nResponse: Thanks for the suggestion. For tool creation, how different types of tools contribute to performance improvements depends on what types of patterns exist in downstream tasks. As shown in our analysis section, there are tens of hundreds of types of tools in the toolset, so ablating different types of tools one by one would be extremely time-consuming and costly. Hence, we conduct a simpler experiment by removing the five most frequently retrieved tools from the MATH toolset and evaluating model performance on the reduced tool set. We found that the accuracy on MATH drops from 68.1 to 58.9, indicating that these tools are very useful to the task.\n\nFor retrieval, the high-level strategy is task-agnostic: We prompt Turbo to \u201cdescribe what it needs\u201d by inferring the name and docstring of tool functions that might be helpful for each problem, and then retrieve tools that are most similar to the expected ones from our tool sets to solve the problem. However, to improve the precision of retrieval, we design task-specific prompts as shown in Appendix B.5, mainly characterized by providing a list of \u201cmost frequently used words\u201d in the toolset so that Tuobo-inferred names can better match the tools in our sets."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3233/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707757883,
                "cdate": 1700707757883,
                "tmdate": 1700707757883,
                "mdate": 1700707757883,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8bW1qiDV7p",
                "forum": "G0vdDSt9XM",
                "replyto": "0fWQkJpyCL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3233/Reviewer_A2gB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3233/Reviewer_A2gB"
                ],
                "content": {
                    "comment": {
                        "value": "The authors respond to most of my questions/concerns. After reviewing the authors' responses and other reviews, I am keeping my initial score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3233/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721869299,
                "cdate": 1700721869299,
                "tmdate": 1700721869299,
                "mdate": 1700721869299,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]