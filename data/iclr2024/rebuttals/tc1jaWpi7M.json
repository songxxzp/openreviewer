[
    {
        "title": "Completing Visual Objects via Bridging Generation and Segmentation"
    },
    {
        "review": {
            "id": "Ew7EAzwXij",
            "forum": "tc1jaWpi7M",
            "replyto": "tc1jaWpi7M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission739/Reviewer_t7dr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission739/Reviewer_t7dr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to boost the object completion via integrating object segmentation into the denoising process of diffusion. The listed visual results look good."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Introducing mask segmentation to facilitate object completion is reasonable, since the completed objects shouldn't reflect strange shapes."
                },
                "weaknesses": {
                    "value": "1. Overall, the paper is easy to follow, but need to be further improved. For example, some symbols are not well explained like the condition $E$ in Line 156. The subscript '_t' is misleading to denote the denoising step of DDPM/DDIM and the proposed IMD. The figure 3 is also confusing. The diffusion model should denoise $x_t$ to $x_0$, but the authors give the start noise as $x_0$. Besides, the time-step of IMD is not illustrated in this figure.\n\n2. It's not rigorous to name the progressive completion process as 'mask denoising'. The object's mask generated from SAM is taken as  a condition to diffusion instead of the denoising uint as $x_t$.   \n\n\n3. It is not clear that whether the other comparison method are retrained in the evaluation data. If yes, the training details and the incomplete mask's interaction in their networks should be claimed, otherwise the authors should explain how to obtain these completion results with the off-the-shelf generation models.\n\n4. It would be better if the authors can give some analysis on the sampling speed and more qualitative results of the proposed method."
                },
                "questions": {
                    "value": "Overall, the proposed idea of leveraging object segmentation to boost object completion is interesting, while the authors should further improve the manuscript to make the contribution more convincing. I am glad to upgrade the rate depending on the rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission739/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission739/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission739/Reviewer_t7dr"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission739/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698485384656,
            "cdate": 1698485384656,
            "tmdate": 1700561924650,
            "mdate": 1700561924650,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ADq9tD3wMn",
                "forum": "tc1jaWpi7M",
                "replyto": "Ew7EAzwXij",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission739/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer t7dr"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the time and effort to review our paper. Our answers to the questions are as follows.\n\n---\n\n**1. Some symbols in the paper need to be further improved.**\n\nWe would like to thank the reviewer for the detailed inspection. We have incorporated the suggested changes to the revised manuscript.\n\n---\n\n**2. It's not rigorous to name the progressive completion process as 'mask denoising'. The object's mask generated from SAM is taken as a condition to diffusion instead of the denoising unit as $x_t$.**\n\nThanks for the comments. We agree that the mask denoising is very different from the image denoising diffusion while we hope the word \"mask denoising\" can help the reader understand our iterative mask completion process. Given a partially visible object $I_p$ and its corresponding partial mask $M_p$, the conventional object completion task aims to find a generative model $\\mathcal{G}$ such that $I_c\\leftarrow\\mathcal{G}(I_p)$, where $I_c$ is the complete object. Here, we additionally add the partial mask $M_p$ to the condition  $I_c\\leftarrow\\mathcal{G}(I_p, M_p)$, where $M_p$ can be assumed as an addition of the complete mask and a noise $M_p=M_c+\\Delta$. By introducing a segmentation model $\\mathcal{S}$, we can find a mask denoiser $\\mathcal{S}\\circ\\mathcal{G}$ from the mask completion process: $M_c\\leftarrow\\mathcal{S}\\circ\\mathcal{G}(I_p, M_c+\\Delta)$. If we consider the occlusion $\\Delta$ as a noise, $\\mathcal{S}\\circ\\mathcal{G}$ can be assumed as conducting mask denoising for each IMD step. We will further clarify the usage of \"mask denoising\" in the revision.\n\n---\n\n**3. It is not clear whether the other comparison methods are retrained in the evaluation data. If yes, the training details and the incomplete mask's interaction in their networks should be claimed, otherwise the authors should explain how to obtain these completion results with the off-the-shelf generation models.**\n\nWe would like to clarify that the baseline methods are not retrained in the evaluation dataset. We consider the baselines can be directly utilized due to (1) the object completion can be treated as a subtask of the in/outpainting objectives that the baseline methods are trained on (2) the object categories in the evaluation set are common which should be within the large-scale training data of the baselines. In addition, to evaluate on a more general object setting, our model is also not trained on the evaluation set (DYCE) but on OpenImage (Line 231). In this way, we consider our comparison can fairly reflect the effectiveness of the proposed method.\n\n---\n\n**4. It would be better if the authors can give some analysis on the sampling speed and more qualitative results of the proposed method.**\n\n|Reduce SD step|Generation stage|Segmentation Stage|Total|FID|\n| :----:| :----: | :----: | :----: | :----: |\n|| 14.3s | 1.2s |15.5s |16.9|\n|$\\checkmark$| 8.6s | 1.2s|9.8s| 17.4|\n\nTable D: Inference time for each stage of IMD on a single V100 GPU. \n\nThanks for your suggestion. We provide the sampling time of each component as shown in Table D. We notice that the most time-consuming part is the diffusion process. To improve the inference speed, we notice that decreasing the diffusion step number in the first several IMD steps will not severely degrade the performance. By incorporating this idea into MaskComp, the average running time was reduced to 2/3 original time. We also provide more qualitative results and results in a potential application (layered image generation) as shown in Figure A ([github.com/iclr23anonymous739/more_vis.pdf](https://github.com/iclr23anonymous739/rebuttal/blob/main/more_vis.pdf)) and Figure B ([github.com/iclr23anonymous739/application.pdf](https://github.com/iclr23anonymous739/rebuttal/blob/main/application.pdf)). We believe the idea of using segmentation to boost generation and the strong capability of MaskComp can be of interest to the community. We will incorporate the suggested results in the revision."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700070041219,
                "cdate": 1700070041219,
                "tmdate": 1700072527337,
                "mdate": 1700072527337,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1uFlme3H5Z",
                "forum": "tc1jaWpi7M",
                "replyto": "ADq9tD3wMn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission739/Reviewer_t7dr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission739/Reviewer_t7dr"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I am glad to see the authors' feedback. \nAfter reading the comments from other reviewers, some new concerns arose.\n\n1. The current analysis of limitations is insufficient. Is it possible that the $N$ generated candidates are with error generations? The object masks by SAM cannot tell the semantic correctness of the candidates. The authors should give a more sufficient analysis of the failure cases. \n\n2. The reasons for comparing the finetuned diffusion with the other off-the-shelf competitors for ''fair comparison'' are not convincing. If the finetuning process is not necessary for other methods due to their original powerful generalization ability, then what is the reason for the proposed method to conduct the retraining process? Although not trained on DYCE,  the used training datasets including AHP and OpenImage usually have diverse samples. The authors should demonstrate that the superiority of the proposed method is from the help of segmentation instead of the retraining process."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466814683,
                "cdate": 1700466814683,
                "tmdate": 1700466814683,
                "mdate": 1700466814683,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3Hmqul6eBl",
                "forum": "tc1jaWpi7M",
                "replyto": "3kvhXgcSMC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission739/Reviewer_t7dr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission739/Reviewer_t7dr"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thanks for your response. \nI advise the authors to give a sufficient analysis on the failure cases in the next version. \nAs for the comparison with other diffusion models, two questions still remain.\n\n1. How to process the occluded mask as a condition to generate the completed result?\n\n2. Whether the proposed MaskComp is only effective at the retrained controlnet or can be directly used in the pretrained version."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532277831,
                "cdate": 1700532277831,
                "tmdate": 1700532277831,
                "mdate": 1700532277831,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c5sZkmCzeV",
                "forum": "tc1jaWpi7M",
                "replyto": "Ew7EAzwXij",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission739/Reviewer_t7dr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission739/Reviewer_t7dr"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thanks again for the authors feedback. My concerns have been resolved. I am glad to update my rate."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561801641,
                "cdate": 1700561801641,
                "tmdate": 1700561834072,
                "mdate": 1700561834072,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I26Ookncta",
            "forum": "tc1jaWpi7M",
            "replyto": "tc1jaWpi7M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission739/Reviewer_2Pq9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission739/Reviewer_2Pq9"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an iterative generation strategy for object completion, which alternates between a mask-conditioned image generation stage and a segmentation stage. Specifically, given a partial object and partial mask, the generation stage trains a conditional diffusion U-Net to generate a complete object; the segmentation stage re-segments the generated object image, and the result, which hopefully is more complete,  will be used as the conditional mask for the subsequent generation stage.  The proposed method is evaluated on AHP and DYCE datasets with comparisons to diffusion-based baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed joint object image and mask completion strategy is well-motivated and the overall method seems novel for the target task. \n- The paper is mostly easy to follow. \n- The experiments include both automatic and human-based metrics for evaluation, and the results are better than baselines."
                },
                "weaknesses": {
                    "value": "- The justification for the entire iterative procedure is lacking. While it is ideal to achieve improvements as shown in Figure 5, there is no guarantee that such improvement can be realized in a realistic setting. In particular, the mask-denoising controlnet is trained in a local manner, which may generate a worse image, and the segmentation stage is largely dependent on the segmentation model S, which may produce noisy segmentation output. Therefore, it is unclear whether this design would work in general cases.    \n\n- The proposed method lacks mode diversity for object completion. The segmentation stage uses an averaging operator, which seems problematic since it would lead to mode average and is unable to capture potential different modes in object completion. This is important to generate different candidates for image editing since there are multiple possibilities for the occluded regions. \n \n- Some technical aspects of the method are unclear. For example:\n   + In the generation stage, how does the interpolation is implemented? How does the method generate the interpolated masks and its time embedding? \n   + What are the notations M, E in the paragraph of \"Diffusion model\" ? The details of the adopted diffusion model are missing in the main text. \n \n- Experimental evaluation is a bit lacking in several aspects and the results are not fully convincing: \n   + It is unclear how the baselines and this method are compared. It is worth noting that the proposed method only generates the foreground object while the other methods also produce the background. Are the background removed before comparison, for the FID computation and user study? It seems unfair if the background is treated differently.  \n   + A reasonable baseline is to combine amodal segmentation with condition control image generation, which is missing in the comparison. \n   + It is unclear how the method is sensitive to the segmentation quality. What if there are different degrees of segmentation error?\n   + As mentioned above, it would be more convincing if the method is evaluated on the cases with diverse modes of object shape in the occluded regions."
                },
                "questions": {
                    "value": "See above for detailed questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission739/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739417076,
            "cdate": 1698739417076,
            "tmdate": 1699636000940,
            "mdate": 1699636000940,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j9vFD7A2Tg",
                "forum": "tc1jaWpi7M",
                "replyto": "I26Ookncta",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission739/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2Pq9 - Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the time and effort to review our paper. Our answers to the questions are as follows.\n\n---\n\n**1. The justification for the entire iterative procedure is lacking. It is possible to have noisy output (worse image/inaccurate mask) in both the generation and segmentation stages.**\n\nWe would like to provide an analysis of the noise-tolerant capability of the proposed MaskComp. We agree that worse images and inaccurate masks can appear in the generation and segmentation stages respectively. Since all the generation processes are conditioned on the original partial image (as shown in Figure 1), both worse images and inaccurate masks will just influence the conditioned mask for the next iteration. We have two mechanisms to mitigate the influence of the noisy conditioned masks: \n\n- *Error Suppression through Mask Voting*: We vote among multiple masks to form the input of the next iteration. In this way, if only a few masks are degraded, the impact of them can be recovered by the voting operation. \n\n- *Error Tolerance in IMD Iteration*: We train the mask-denoising ControlNet to handle a wide range of occluded masks. Consequently, if the conditioned mask undergoes minimal improvement or degradation due to the noises in a given iteration, it can still be improved in the subsequent iteration. While this may slightly extend the convergence time, it is not anticipated to have a significant impact on the ultimate image quality.\n\nIn this way, we consider the IMD process can work in a robust manner. We will show a quantitative analysis in the later response to question 7. \n\n---\n\n**2. The proposed method lacks mode diversity for object completion due to the averaging operator in the IMD process.**\n\nWe agree that MaskComp lacks shape diversity due to the averaging operator. However, we consider that object completion tasks typically only prioritize the estimation closest to the ground truth object. For example, in the closely related amodal segmentation task, the evaluation is conducted by calculating the IoU between the predicted mask and the GT mask. We consider retaining only the most plausible estimation to be reasonable since the rough shape of objects can be deduced by the visible parts in most cases.\n\nIn addition, we consider the lack of shape diversity will not hinder the usage of MaskComp in practice. We discuss the potential application of MaskComp in Figure 10. Since the current image editing models struggle to modify partial images, MaskComp can serve as the first step to complete objects and then feed to an image editing model to further process the image. We show another cool application, layered image generation, of MaskComp as shown in Figure A ([github.com/iclr23anonymous739/application.pdf](https://github.com/iclr23anonymous739/rebuttal/blob/main/application.pdf)).\n\n---\n\n**3. In the generation stage, how does the interpolation is implemented? How does the method generate the interpolated masks and its time embedding?**\n\nWe designed several occlusion strategies to create diverse occlusion (the details can be found in Line 417-424). During training, we conduct the random occlusion process twice for each complete mask $M_c$. The partial mask $M_p$ is achieved by considering the occluded areas in both of the occlusion processes. And the interpolated mask $M$ is generated by using one of the occlusions. The time embedding used for the gating operation is shared with the time embedding for encoding the diffusion step in the stable diffusion.\n\n---\n\n**4. What are the notations M, E in the paragraph of \"Diffusion model\"? The details of the adopted diffusion model are missing in the main text.**\n\nThe $M$ denotes the interpolated mask (Line 146) with an occlusion rate between the complete mask $M_c$ and partial mask $M_p$ and $E$ denotes the text prompt of the object (please note that the text prompt is necessary to fine-tune stable diffusion while optional during inference). We leverage frozen stable diffusion (Line 242) as our underlining diffusion model. We further clarified the notations in the revised version.\n\n---\n\n**5. It is unclear how the baselines and this method are compared. Are the background removed before comparison, for the FID computation and user study?**\n\nYes, we aim to ignore the background during all the evaluations. For the quantitative comparison, the computing of FID only considers the ground-truth object area to eliminate the influence of the generated background (Caption of Table 1). For the user study, we give detailed instructions to participants to focus only on the foreground object region and ignore the difference presented in the background (Line 457)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069759668,
                "cdate": 1700069759668,
                "tmdate": 1700637172608,
                "mdate": 1700637172608,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xx3eETSjH7",
                "forum": "tc1jaWpi7M",
                "replyto": "I26Ookncta",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission739/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2Pq9 - Part 2"
                    },
                    "comment": {
                        "value": "**6. Comparison with a combination of amodal segmentation and condition image generation as a baseline.**\n\n|Method|FID|\n| ----| :----: | \n|AISFormer+ControlNet| 29.4 |\n|MaskComp| 16.9 |\n\nTable A: Performance comparison with amodal segmentation baseline.\n\nThanks for the suggestion. We conducted an additional experiment with the masks from the SOTA open-sourced amodal segmentation method ([A] AISFormer) and leveraged ControlNet to generate images based on the amodal masks. We report the performance as shown in Table A. We notice that our method shows obvious superior performance compared to the baseline setting.\n\n[A] AISFormer: Amodal Instance Segmentation with Transformer\n\n---\n\n**7. What if there are different degrees of segmentation error?**\n\n|Noise degree|Iter 1|Iter 3|Iter 5|Iter 7|Iter 9|\n| ----| :----: | :----: | :----: | :----: | :----: | \n|15\\% area| 28.4 | 22.7| 18.9 | 17.2 |16.5|\n|10\\% area| 26.4 | 21.4 | 18.1 | 17.0| 16.4|\n|5\\% area| 24.9 | 19.6 | 17.0 | 16.2 | 16.0|\n|No noise| 24.7 | 19.4 | 16.9 | 16.1 | 15.9|\n\nTable B: Ablation with noisy conditioned mask. We leverage FID to evaluate the performance. The error is added based on the size of the foreground object.\n\nWe conducted an experiment to manually add random errors to the masks. As shown in Table B, we ablate on the iteration number and the degree of segmentation error. We notice that the segmentation error will just increase the converge iteration number while the final performance will not be severely influenced. In addition, since MaskComp predicts a black background, it is easy for segmentation models to segment the foreground objects. Therefore, a large error in segmentation is not expected.\n\n---\n\n**8. It would be more convincing if the method is evaluated on the cases with diverse modes of object shape in the occluded regions.**\n\n|Occ.|20\\%|40\\%|60\\%|80\\%|\n| ----| :----: | :----: | :----: | :----: | \n|FID| 13.4 | 15.7 | 17.2 | 29.9 |\n\nTable 2 (b): Performance with different occlusion rates.\n\n|Occ.|Rectangle|Oval|Object|\n| ----| :----: | :----: | :----: | \n|FID| 15.3 | 15.1| 16.9 |\n\nTable C: Performance with different occlusion types. Object denotes the occlusion is created with a random object shape.\n\nWe reported the performance with different occlusion rates as shown in Table 2 (b). We additionally report the performance with different occlusion types as shown in Table C. Since the object occlusion has a more complex boundary, the results with rectangle and oval occlusions show a lower FID than that with more complex object occlusions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069819915,
                "cdate": 1700069819915,
                "tmdate": 1700077106969,
                "mdate": 1700077106969,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HQjnobUsin",
                "forum": "tc1jaWpi7M",
                "replyto": "I26Ookncta",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission739/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Followup Response to Reviewer 2Pq9"
                    },
                    "comment": {
                        "value": "Thanks for your time and effort in reviewing our work. We hope our earlier response can resolve your concern. As the deadline for the discussion period is approaching, if any aspect of our response or the method itself remains unclear, please do not hesitate to reach out. We are more than willing to provide further clarification.\n\nHere is a summary about the suggested changes to the manuscript:\n- Further discussion about the errors and robustness of the IMD process (Line 317-337).\n- Further explanation about the interpolated mask and time embedding (Line 253-257).\n- Clarification of the $M$, $E$ and the underlined diffusion model (Line 153-158).\n- Comparison with a combination of amodal segmentation and condition image generation (Table 4 (c) & Line 354-359).\n- Performance with different occlusion types (Table 4 (d) & Line 359-360).\n\nWe look forward to your continued feedback and sincerely hope that you can improve the rating based on the responses."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634938999,
                "cdate": 1700634938999,
                "tmdate": 1700636773223,
                "mdate": 1700636773223,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LR6WaZFQc7",
                "forum": "tc1jaWpi7M",
                "replyto": "I26Ookncta",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission739/Reviewer_2Pq9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission739/Reviewer_2Pq9"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. \n\n- 1&7. The argument on the robustness does not fully convince me. The voting does help but it would hurt diversity, which is a dilemma. While enlarging the range of masks may help, there may also be cases which the training does not cover. Thanks for the empirical study, which does show the method is stable on this benchmark. \n\n- 2. The groundtruth may not be unique, especially for articulated objects. \n\n- 3-5 Thanks for the clarification. It would be better to apply a GT mask before sending the results to any evaluation process.\n\n- 6. Thanks for the additional results, which addressed my previous concern. \n\n- 8. Thanks for the results, but I would like to clarify that \"diverse modes of object shape\" refer to potential multiple GT masks due to different poses of complex objects, such as human/animal classes."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707661663,
                "cdate": 1700707661663,
                "tmdate": 1700707726887,
                "mdate": 1700707726887,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uE9ChxO94H",
                "forum": "tc1jaWpi7M",
                "replyto": "OdMoBUMNLs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission739/Reviewer_2Pq9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission739/Reviewer_2Pq9"
                ],
                "content": {
                    "title": {
                        "value": "Further comments"
                    },
                    "comment": {
                        "value": "Thanks for the summary. In my opinion, it is important to cope with uncertainty in such completion problems, and this challange needs to be addressed properly in the first place."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719666964,
                "cdate": 1700719666964,
                "tmdate": 1700719666964,
                "mdate": 1700719666964,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4aZozWEHLC",
                "forum": "tc1jaWpi7M",
                "replyto": "I26Ookncta",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission739/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback. However, we do not agree with the reviewer due to the lack of concrete reasons to support his point. We consider we have explained the unnecessity of shape diversity in the completion task from the (1) conventional (amodal evaluation),  (2) practical (application) and (3) empirical (analysis of human annotator) perspectives and have provided convincing evidence to support our claim that sacrificing shape diversity to gain reliability and image quality is deserving.\n\nWe would encourage the reviewer to discuss the specific concern with other reviewers and AC during the discussion phase. Thank you."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720649514,
                "cdate": 1700720649514,
                "tmdate": 1700740301973,
                "mdate": 1700740301973,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "me4V4LOya7",
            "forum": "tc1jaWpi7M",
            "replyto": "tc1jaWpi7M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission739/Reviewer_PKfR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission739/Reviewer_PKfR"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to generate a complete object given partial observation. Instead of designing an end-to-end pipeline, this paper introduces object masks as an intermediate representation to complete the object interactively. Specifically, in each iteration, the proposed method generates a set of completions using stable diffusion and extracts the corresponding masks with a segmentation model. These masks are fused and fed into the generation process for the next iteration. Experiments and analysis demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method (MaskComp) introduces a novel interactive approach to complete an object by generating object masks as guidance.\n\n1. Section 3.3 tries to give some theoretical analysis of MaskComp, which is interesting and helps to understand the benefit of introducing masks in the generation approach.\n\n1. From the visual results, I find MaskComp completes the input partial object and achieves higher perceptual quality compared to other methods. Quantitatively, it also achieves higher FID and user study scores.\n\n1. The paper's presentation is good, making it easy to follow and understand."
                },
                "weaknesses": {
                    "value": "1. The technical contributions of the proposed method could be further improved. For now, the proposed method is mainly a mask-guided stable diffusion model with an off-the-shelf segmentation model to produce the mask condition. Using SAM to generate masks is straightforward and the mask voting process gives no surprises.\n\n1. If generating a mask is the key to generating high-quality images, why not directly use an encoder-decoder model like U-Net or an SD to predict the target complete mask in one step? As generating the mask is relatively a simple task, I believe a U-Net might be enough to obtain an accurate object mask (this can be regarded as an outpainting task for binary images). It would be interesting to have conducted such an experiment.\n\n1. More ablations study of the proposed method should be given. E.g., different segmentation models, different mask voting strategies, \n\n1. One of the main drawbacks of the proposed method is its slow inference speed. Each iteration in the generation process involves generating multiple image candidates and their segmentations (segment anything model is quite slow), let alone running for multiple iterations. The authors did not report the running time statistics of different methods and omitted this limitation in the limitations section. I encourage the authors to have more comparisons and discussions of the running time.\n\n1. The FID metric shown in Table 1 is problematic. The FID only considers the ground-truth object area; however as shown in Figure 6, different methods generate different foreground areas that may not match the ground-truth object mask. Thus, this metric will yield a higher FID score if the generated object has a large discrepancy \bwith the ground truth even if it is realistic.\n\n1. I cannot find the qualitative comparisons with ControlNet and Kandinsky.\n\n1. I would like to see the results of using the ground-truth mask as input to see how different methods perform with such an oracle. This also tells the generation upper bound of mask-guided methods."
                },
                "questions": {
                    "value": "The authors should provide more experiments and discussions in the rebuttal to address the weaknesses raised above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission739/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831749465,
            "cdate": 1698831749465,
            "tmdate": 1699636000870,
            "mdate": 1699636000870,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bhqF05SqL4",
                "forum": "tc1jaWpi7M",
                "replyto": "me4V4LOya7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission739/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PKfR - Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the time and effort to review our paper. Our answers to the questions are as follows.\n\n---\n\n**1 & 2. Using SAM to generate masks is straightforward and the mask voting process gives no surprises. If generating a mask is the key to generating high-quality images, why not directly use an encoder-decoder model like U-Net or an SD to predict the target complete mask in one step?**\n\n|Method|SD|UNet |IMD (Ours)|\n| ----| :----: |:----: |:----: |\n|IoU| 82.4 | 75.4 |88.5 | \n\nTable A: IoU of predicted masks on AHP dataset.\n\nIn this paper, we investigate the relation between object generation and segmentation. We found that even if the segmentation model is not trained, the segmentation can still benefit the generation. It is a good idea to conduct experiments to investigate other segmentation models.\nWe report the results of using a ControlNet (frozen SD) and UNet with the partial object as the condition to predict the complete mask in Table A. We notice that the complete mask predicted from the IMD process shows obviously higher IoU to the ground truth which indicates the IMD process is an effective mask prediction approach.\n\nIn addition, we would like to clarify that the mask prediction is not simply conducted by the SAM but by the entire IMD process with both generation and segmentation stages. We treat the segmentation as the shape property of the generated object. Therefore, with an object generation model, we can directly obtain the shape using SAM without training another mask-specific model.\n\nWe also want to further clarify the usage of the voting operation. Given a partial object $I_p$, the generation model samples from a distribution that contains both realistic and unrealistic images. As shown in Figure A ([github.com/iclr23anonymous739/dist.pdf](https://github.com/iclr23anonymous739/rebuttal/blob/main/dist.pdf)), since the image distribution is complex, the expectation $E[I_c]$ cannot represent a realistic image. However, when we consider the shape of the generated images, we are excited to find that the expectation $E[\\mathcal{S}(I_c)]$ leads to a more realistic shape. We consider this observation interesting as, for most of the other generation tasks (non-conditioned), averaging the object shape will just yield an unrealistic random shape. This observation (Figure 4) serves as one of our core observations to build the IMD process. Here, SAM serves as the tool to extract the object shape and voting is a way to binarize the expectation of object shapes to a binary mask. \n\n---\n\n**3. More ablation studies of the proposed method should be given. E.g., different segmentation models, and different mask voting strategies.**\n\n|Method|Mask2Former|ClipSeg|SAM|\n| :----| :----: | :----: | :----: |\n|FID| 22.5| 19.9 |16.9|\n\nTable B: Performance with different segmentation models.\n\n|Method|Voting with logits|Mean with logits|Voting with mask| Mean with mask|\n| :----| :----: | :----: | :----: | :----: |\n|FID| 16.9| 17.2 |17.6| 17.0 |\n\nTable C: Performance with different voting strategies. Logits: mask logits before binarzing. \n\nWe conducted ablation experiments to determine the design choice in the segmentation stage. We report the ablation studies about segmentation models and voting strategies in Table B and Table C. The current design choice of using SAM and voting with logits is based on the ablation results. \n\n---\n\n**4. Comparisons and discussions of the running time.**\n\n|Reduce SD step|Generation stage|Segmentation Stage|Total|FID|\n| :----:| :----: | :----: | :----: | :----: |\n|| 14.3s | 1.2s |15.5s |16.9|\n|$\\checkmark$| 8.6s | 1.2s|9.8s| 17.4|\n\nTable D: Inference time for each stage of IMD on a single V100 GPU. \n\nThank you for your suggestion. We demonstrate the running time of each component in IMD as shown in Table D. We acknowledge that the inference speed of MaskComp is slow. To improve the inference speed, we notice that decreasing the diffusion step number in the first several IMD steps will not severely degrade the performance. By incorporating this idea into MaskComp, the average running time was reduced to 2/3 original time. We will add the discussion to our limitation section.\n\n---\n\n**5. The FID only considers the ground-truth object area. Thus, this metric will yield a higher FID score if the generated object has a large discrepancy with the ground truth even if it is realistic.**\n\n|Method|ControlNet|Kandinsky 2.1|SD 1.5|SD 2.1|MaskComp (Ours)|\n| ----| :----: | :----: | :----: | :----: | :----: |\n|AHP|45.4|43.9|41.4|39.9|21.3|\n|DYCE|49.4|47.7|43.4|41.1|25.4|\n\nTable E: Performance comparison with FID scores calculated with SAM masks. \n\nThanks for the comment. We report the FID scores evaluated by cropping the object with SAM in Table E. We notice that, for the FID scores calculated with both GT mask and SAM mask, our method outperforms baseline methods with an obvious marginal which indicates the effectiveness of the proposed method."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068977087,
                "cdate": 1700068977087,
                "tmdate": 1700635986652,
                "mdate": 1700635986652,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2b70aCH4oQ",
                "forum": "tc1jaWpi7M",
                "replyto": "me4V4LOya7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission739/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PKfR - Part 2"
                    },
                    "comment": {
                        "value": "**6. Qualitative comparisons with ControlNet and Kandinsky.**\n\nThanks for pointing this out. We add additional comparisons with ControlNet and Kandinsky in Figure 6 ([github.com/iclr23anonymous739/comparison.pdf](https://github.com/iclr23anonymous739/rebuttal/blob/main/comparison.pdf)). We notice that MaskComp achieves superior performance compared to baseline methods.\n\n---\n\n**7. The results of using the ground-truth mask as input to see how different methods perform with such an oracle (This tells the generation upper bound of mask-guided methods).**\n\n|Mask|Visible|Noisy|Complete|\n| ----| :----: | :----: | :----: |\n|FID| 16.9| 15.3 |12.7|\n\nTable 2 (a): Performance with different conditioned masks. Occlusion rate: partial mask > noisy mask > complete mask.\n\nWe reported the results of using the ground-truth mask as the condition in Table 2 (a) and there is also a visual comparison among different conditioned masks in Figure 2. With the ground-truth mask, the generation model shows a promising performance to complete the objects. This also serves as the core motivation for our iterative mask denoising process (IMD) which starts from a partial mask to gradually generate a complete mask to boost the generation process."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069474297,
                "cdate": 1700069474297,
                "tmdate": 1700636002576,
                "mdate": 1700636002576,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a1pJZFVtV1",
                "forum": "tc1jaWpi7M",
                "replyto": "me4V4LOya7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission739/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Followup Response to Reviewer PKfR"
                    },
                    "comment": {
                        "value": "We deeply appreciate the time and effort invested in reviewing our work and providing valuable feedback. Since the deadline for the discussion period is approaching and we haven't heard back from you, here's a summary of our earlier response to the concerns raised:\n- *Weakness 1 & 2*: We discuss the motivation for using of SAM and voting with our observations and conduct additional experiments to compare the IMD process with baselines. Moreover, we clarify that the mask prediction is not simply conducted by the SAM but by the entire IMD process with both generation and segmentation stages.\n\n- *Weakness 3*: We provide the suggested ablation results and incorporate the discussion in Table 4 & Line 349-353 in the revised version.\n\n- *Weakness 4*: We discuss the inference speed and incorporate the analysis in Table 3 (c) and Line 295-300 in the revised version.\n\n- *Weakness 5*: We report the FID score calculated with masks predicted from the generated images and update the Table 1 to better illustrate the performance comparison.\n\n- *Weakness 6*: We update Figure 6 to additionally compare with ControlNet and Kandinsky.\n\n- *Weakness 7*: We further discuss the results in Table 3 (a) about the performance with GT mask as condition.\n\nWe are open to further discussions if there are any aspects that remain unclear. We look forward to your continued feedback and sincerely hope that you can improve the rating based on the responses."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636160836,
                "cdate": 1700636160836,
                "tmdate": 1700636668648,
                "mdate": 1700636668648,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Qy9zmHkvzJ",
            "forum": "tc1jaWpi7M",
            "replyto": "tc1jaWpi7M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission739/Reviewer_xAwN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission739/Reviewer_xAwN"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method to complete an object which undergoes occlusion. The algorithm alternats generation and segmentation stages to infer the shape and texture of the original object without occlusion. The segmentation and generation helps each other to refine the  result as the iteration goes on. Diffusion model generates the image utilizing the mask info as a condition. Segmentation is basically derived from the generation result; for better result multiple instance of images are generated and their segments are averaged to yield segmentation mask. The experimental result shows IFD comparision with some recent researches, and human assessment is also performed to compare the results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Clever idea improved the result of occluded object completion effectively. The recent progress of image generation models are actively analyzed and the authors found useful problem task.\nIn the quantitative evaluation the FID metric shows significant performance compared other method, and the numbers are convinced by showing qualitative results.\nGreatly overcame the random unstable results which occurs frequently from the image generation model by averaging the results of multiple runs."
                },
                "weaknesses": {
                    "value": "While this paper has attractive strengths, this research is rather applicational research that exploits good features of prior researches. Considering the overall direction of the papers presented in this conference (ICLR), readers may expect more theoretical idea or fundamental thas can be transferrable to of stimulate other research. This paper is heavely dependent on Zhang 2023 paper."
                },
                "questions": {
                    "value": "The user study is included in this research to evaluate the quality of object completion.\nIf more details of the user study are provided, it will be more convincing. In many other research areas user study is conducted; and to dispel any latent bias or mistakes they generally offer user study method and protocol, such as: how many persons participate? how to select the subjects? what exactly were the sentences for questions? how was the user interface or the testing environment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission739/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698892264950,
            "cdate": 1698892264950,
            "tmdate": 1699636000792,
            "mdate": 1699636000792,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o57goVBcqI",
                "forum": "tc1jaWpi7M",
                "replyto": "Qy9zmHkvzJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission739/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xAwN"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the time and effort to review our paper. Our answers to the questions are as follows.\n\n---\n\n**1. Readers of ICLR may expect more theoretical ideas or fundamentals that can be transferrable to stimulate other research. This paper is heavily dependent on Zhang 2023 paper.**\n\nThanks for the important comment. We would like to further clarify that the idea behind MaskComp is to investigate the relation between image generation and segmentation. Previous work PaintSeg [A] has proved that image generation can benefit the segmentation and lead to training-free object segmentation capability. In this work, our results indicate that segmentation can also be beneficial to image generation with the proposed IMD process. To better understand the process, we also provide a theoretical analysis of the process in Section 3.3. We consider that the idea of leveraging segmentation to boost generation can be of interest to the community. \n\n[A] PaintSeg: Training-free Segmentation via Painting, NeurIPS 2023\n\n---\n\n**2. If more details of the user study are provided, it will be more convincing. How many people participate? How to select the subjects? What exactly were the sentences for questions? How was the user interface or the testing environment?**\n\nWe provide information about the user study in the supplementary materials (Line 450-464). There are 16 people participated in the user study. All participants were selected from graduate students who have relevant knowledge to understand the task. We illustrate the instructions for the participants in Line 456-564. We demonstrate the user interface in Figure A ([github.com/iclr23anonymous739/platform.png](https://github.com/iclr23anonymous739/rebuttal/blob/main/platform.png))."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068482243,
                "cdate": 1700068482243,
                "tmdate": 1700637032981,
                "mdate": 1700637032981,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FIoWOGGGhj",
                "forum": "tc1jaWpi7M",
                "replyto": "o57goVBcqI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission739/Reviewer_xAwN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission739/Reviewer_xAwN"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors. Although my final overall rate will not be changed, the user study parts are clarified by the comment and spplementary material."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688344682,
                "cdate": 1700688344682,
                "tmdate": 1700688344682,
                "mdate": 1700688344682,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]