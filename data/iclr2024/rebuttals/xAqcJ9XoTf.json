[
    {
        "title": "On the Stability of Expressive Positional Encodings for Graph Neural Networks"
    },
    {
        "review": {
            "id": "L2j7GKiKUL",
            "forum": "xAqcJ9XoTf",
            "replyto": "xAqcJ9XoTf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6928/Reviewer_mePj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6928/Reviewer_mePj"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the stability of eigenvector-based positional encodings while previous methods mainly focus on the sign- and basis-invariant properties. The authors claim that the instability of the previous method is caused by the hard partition of eigenvectors and the ignorance of eigenvalues. To address this challenge, this paper proposes SPE, which leverages the eigenvalues to re-weight the eigenvectors in a soft partition way. SPE is provably stable and shows great expressive power. Experiments on various tasks validate the superiority of the proposed method over baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed SPE is provably stable, which means that it can generalize to unseen graphs. I think this strong inductive learning ability is crucial for graph representation learning. The theoretical contribution is great.\n\n2. The proposed SPE shows great expressive power, which not only universally approximates previous basis invariant functions but also can distinguish the cycles in graphs. Experimental results validate the effectiveness of the proposed method.\n\n3. In addition to previous methods that conduct experiments in the basic molecular property prediction tasks, this paper also considers a more challenging out-of-distribution (OOD) task for evaluation."
                },
                "weaknesses": {
                    "value": "1. The complexity of the proposed SPE is much larger than previous positional encoding methods because it needs to reconstruct graph structures, i.e., $\\boldsymbol{V} \\operatorname{diag}\\left(\\phi(\\boldsymbol{\\lambda})\\right) \\boldsymbol{V}^{\\top}$, whose complexity is $\\mathcal{O}(KN^{2})$. In contrast, the Transformer-based methods, e.g., BasisNet, only have the complexity of $\\mathcal{O}(NK^{2})$, where $K \\ll N$.\n\n2. In the molecular property prediction task, SPE has more parameters than baselines. It would be better if the authors could align the number of parameters across different methods. Additionally, in the OOD tasks, the improvement of SPE over baselines is marginal."
                },
                "questions": {
                    "value": "Here are some concepts that I am not sure I fully understand. Please correct me if there are any misunderstandings.\n\n1. In equation (1), what does $\\mathbb{R}^{n \\times d} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^{n \\times p}$ mean? I understand that $\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times p}$ represents the function applied to the position features of each node. What does $\\mathbb{R}^d$ indicate? Operation on eigenvalues?\n\n2. What is the difference between a hard partition and a soft partition?  I do not see a clear definition. Does hard partition indicate a fixed number of eigenvectors and does soft partition mean it can handle a variable number of eigenvectors?\n\n3. Is it possible to replace the element-wise MLPs of $\\phi$ with polynomial functions? In this situation, I think the complexity can be significantly reduced and the expressiveness can be preserved since polynomials are also non-linear."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6928/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698655765315,
            "cdate": 1698655765315,
            "tmdate": 1699636807491,
            "mdate": 1699636807491,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XVn3aF4Bat",
                "forum": "xAqcJ9XoTf",
                "replyto": "L2j7GKiKUL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6928/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6928/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer mePj"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments. \n\n> **W1**: The complexity of the proposed SPE is much larger than previous positional encoding methods because it needs to reconstruct graph structures, i.e., $V\\phi V^{\\top}$, whose complexity is $N\\times N\\times K$. In contrast, the Transformer-based methods, e.g., BasisNet, only have the complexity of $NK^2$, where $K<<N$.\n\nNote that the complexity of BasisNet is also $N\\times N\\times K$, because it also needs to compute the matrix $V_kV^{\\top}_k\\in\\mathbb{R}^{N\\times N\\times d_k}$ in $k$-th eigensubspace for $k=1, 2,...$. Also in general a transformer-based method should have complexity $O(N^2)$ because it requires to compute pair-wise correlation. \n\nEmpirically, we also find SPE's running time is much faster than BasisNet. Kindly check our unified response ``*running time evaluation*'' above.\n\n> **W2**: In the molecular property prediction task, SPE has more parameters than baselines. It would be better if the authors could align the number of parameters across different methods. Additionally, in the OOD tasks, the improvement of SPE over baselines is marginal.\n\nThank you for the suggestion. We will align the number of parameters in the revised manuscript. \n\nFor OOD task, the core idea is not to achieve state-of-the-art performance but to show how previous unstable methods suffer from the **risk of instability** on OOD data. In this sense, standard GNN model is actually a strong baseline because it is very stable. \n\n> **Q1**: In equation (1), what does $\\mathbb{R}^{n\\times d}\\times\\mathbb{R}^d\\to\\mathbb{R}^{n\\times p}$ mean? \n\nYes, it means a general positional encoding method can take both eigenvalues and eigenvectors as input.\n\n> **Q2**: What is the difference between a hard partition and a soft partition? I do not see a clear definition. Does hard partition indicate a fixed number of eigenvectors and does soft partition mean it can handle a variable number of eigenvectors?\n\nHard partition literally needs to do partition of eigenspace, i.e., splitting eigenvectors $V$ into different eigensubspace $(V_1, V_2,....)$ and computing $(V_1V_1^{\\top}, V_2V_2^{\\top}, ....)$. While ``soft partition'' actually **does not do partition** but instead is just a terminology to describe the smooth combination over all eigenvectors, $V\\text{diag}(\\phi(\\lambda))V^{\\top}$. \n\n> **Q3**: Is it possible to replace the element-wise MLPs of  with polynomial functions? In this situation, I think the complexity can be significantly reduced and the expressiveness can be preserved since polynomials are also non-linear.\n\nWe think MLPs are not be the bottleneck of computation complexity. This is because the input size is simply the number of eigenvectors to use, which is usually fixed and does not scale with graph size. On the other hand, MLPs are usually believed to be more expressive than polynomials. So overall MLPs are preferable for us."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292931478,
                "cdate": 1700292931478,
                "tmdate": 1700296226749,
                "mdate": 1700296226749,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TqPgLT1PDX",
                "forum": "xAqcJ9XoTf",
                "replyto": "XVn3aF4Bat",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6928/Reviewer_mePj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6928/Reviewer_mePj"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the clarifications. Now I have no doubt about the efficiency of SPE."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538805859,
                "cdate": 1700538805859,
                "tmdate": 1700538805859,
                "mdate": 1700538805859,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HB5HEzVcb8",
            "forum": "xAqcJ9XoTf",
            "replyto": "xAqcJ9XoTf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6928/Reviewer_qQbF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6928/Reviewer_qQbF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new approach for generating positional encodings which are stable and can universally approximate basis invariant functions. To compute those encodings, the method first decomposes the Laplacian matrix, it then applies different permutation equivariant functions to the eigenvalues, and uses the output of those layers to produce matrices of dimension $n \\times n$ which are then fed to another permutation equivariant network (e.g., a GNN). The proposed method is evaluated on molecular property prediction datasets and also on a dataset with domain shifts where it outperforms other positional encoding methods in most cases."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The stability of graph learning algorithms is a topic that has not been explored that much yet and deserves more attention from the community. The results presented in this paper contribute to this direction.\n\n- In my view, the paper has some value since several individuals from the graph learning community would be interested in knowing its findings. Practitioners would also be interested in utilizing the proposed encodings since in many settings, existing GNN models fail to generalize to unseen domains.\n\n- The proposed model achieves low values of MAE on the ZINC and Alchemy datasets and outperforms the baselines. This might be related to the model's ability to identify and count cycles of different lengths."
                },
                "weaknesses": {
                    "value": "- I feel that the paper lacks some explanations. It is not clear which modules of the proposed method contribute to it being stable. If no $\\phi_\\ell$ layers are added, wouldn't $K_\\ell$ be equal to 1? In my understanding, this wouldn't hurt stability. Also, it seems to me that as $m$ increases the bound becomes looser and looser. If that's the case, why do we need multiple such permutation equivariant layers?\n\n- One of the main weaknesses of this paper is the proposed model's complexity. Function $\\rho$ takes a tensor of dimension $n \\times n \\times m$ as input. This might not be problematic in case the model is trained on molecules since molecules are small graphs. But in case of other types of graphs such as those extracted from social networks which are significantly larger, this can lead to memory issues.\n\n- The proposed approach is much more complex that a standard GNN model, but in most cases it provides minor improvements over a model that does not use positional encodings. For instance, the improvement on Alchemy is minor, and also on DrugOOD, SPE provides minor improvements in the Assay and Scaffold domains and no improvements in the Size domain.\n\n- No running times of the different models are reported in the paper. \n\n- The proposed model seems to advance the state of the art in the field of positional encodings for graphs, however, it is not clear whether it also advances the state of the art in the graph learning community. I would suggest the authors compare the proposed approach against some recently proposed GNN models, and not only against methods that produce positional encodings.\n\nTypos:\\\np.6: \"hold and Let\" -> \"hold and let\"\\\np.7: \"we take to $\\rho$ to be\" -> \"we take $\\rho$ to be\"\\\np.8: \"which hypothesizes is because\" -> \"which is because\""
                },
                "questions": {
                    "value": "In Figure 2, how did you compute the Lipschitz constant of MLPs? We can compute the Lipschitz constant for models that consist of a single layer, but exact computation of the Lipschitz constant of MLPs\nis NP-hard [1].\n\n[1] Virmaux, Aladin; Scaman, Kevin. Lipschitz regularity of deep neural networks: analysis and efficient estimation. Advances in Neural Information Processing Systems, 2018."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6928/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672327362,
            "cdate": 1698672327362,
            "tmdate": 1699636807378,
            "mdate": 1699636807378,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aEX8Qc4ZTf",
                "forum": "xAqcJ9XoTf",
                "replyto": "HB5HEzVcb8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6928/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6928/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer qQbF"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thoughful review. The followings are our response. \n\n> **W1**: I feel that the paper lacks some explanations. It is not clear which modules of the proposed method contribute to it being stable. If no $\\phi_i$ layers are added, wouldn't  $K_i$ be equal to 1? In my understanding, this wouldn't hurt stability.\n\nThe stability stems from the operation $V\\text{diag}(\\phi(\\lambda)) V^{\\top}$. The key intuition is to combine multiple eigensubspaces with smooth composition, avoiding the splitting of individual eigensubspaces (which is what previous methods did).\n\nIf you simply let $\\phi(\\lambda)$ be $\\lambda$, it is true that this model is also stable. However, its **expressive power** will be damaged. For instance, if we let $\\phi(\\lambda)=\\lambda$ and use full eigenvectors, then $V\\text{diag}(\\phi(\\lambda))V^{\\top}$ simply becomes Laplacian $L$ (and equivalently adjacency matrix $A$). In this case SPE is just acting like a normal graph neural networks on $A$, bringing no aditional expressive power. Similarly, larger $m$ (number of $\\phi$) may damage stability but could bring more expressivity. So in practice we let $m$ be a hyperparameter for the stability-expressivity trade-off.\n\n> **W2**: One of the main weaknesses of this paper is the proposed model's complexity.\n\nWe agree with the reviewer that complexity of current implementation of SPE could be a problem for large graphs. This problem also appears in BasisNet, whose complexity is also $O(N^2)$. Empirically, our unified response ``*running time evaluation*'' above shows the running time of SPE and other methods. We can see that SPE's running time is comparable to SignNet and is much faster than BasisNet. \n\nOne alternative way for reducing complexity could be to sparse the $V\\text{diag}(\\phi) V^{\\top}$ by only computing and storing its $(i,j)$-entry where $(i,j)$ is an edge. Then these positional encodings are treated as edge features for downstream tasks. The complexity will reduce from $O(n^2)$ to $O(|E|)$ where $|E|$ is the number of edges. In practice, the sparsing of $V\\text{diag}(\\phi) V^{\\top}$ can be efficiently done by firstly assigning $V_{i, :}$ to $i$-th node features and using an one-layer message passing neural network to update and store edge features $[V\\text{diag}(\\phi) V^{\\top}]\\_{i,j}=\\sum_{k}[\\phi(\\lambda)]\\_k V\\_{i,k}V\\_{j,k}$. \n\n> **W3**: `The proposed approach is much more complex that a standard GNN model, but in most cases it provides minor improvements over a model that does not use positional encodings.'\n\nIt is true that on Alchemy all PE methods provide minor improvements. But still SPE is the best. We conjecture that Laplacian encodings may not be a very good inductive bias on this dataset. For DrugOOD, the core idea is not to achieve state-of-the-artperformance but to show how previous unstable methods suffer from the **risk of instability** on OOD data. Standard GNN model is actually a strong baseline because it is very stable. \n\n> **W4**: No running times of the different models are reported in the paper.\n\nThank you for raising this point. We add running time of all methods on ZINC and DrugOOD in unified response ``*running time evaluation*'' above. Overall we can see that for our current implementation SPE has comparable running time to SignNet and much faster than BasisNet.\n\n> **W5**: The proposed model seems to advance the state of the art in the field of positional encodings for graphs, however, it is not clear whether it also advances the state of the art in the graph learning community. I would suggest the authors compare the proposed approach against some recently proposed GNN models, and not only against methods that produce positional encodings.\n\nThank you for the valuable sugeestion. We plan to add a few recently proposed GNN models for comparison during the rebuttal period. Again, graph positional encodings are important for graph learning community since (1) it plays a fundamental role for constructing powerful graph transformers [1, 2]; (2) it can be plugged into any GNN models as feature augmentation [3] and improve their expressive power.\n\n\n[1] Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. AAAI Workshop on Deep Learning on Graphs: Methods and Applications, 2021.\n\n[2] Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent L\u00e9 etourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. Advances in Neural Information Processing Systems, 34, 2021.\n\n[3] Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Graph neural networks with learnable structural and positional representations. In International Conference on Learning Representations, 2022."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292262424,
                "cdate": 1700292262424,
                "tmdate": 1700296061213,
                "mdate": 1700296061213,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hv0szzSsmx",
                "forum": "xAqcJ9XoTf",
                "replyto": "HB5HEzVcb8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6928/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6928/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer qQbF (2)"
                    },
                    "comment": {
                        "value": "> **Q1**: In Figure 2, how did you compute the Lipschitz constant of MLPs? We can compute the Lipschitz constant for models that consist of a single layer, but exact computation of the Lipschitz constant of MLPs is NP-hard\n\nThank you for pointing this out. Technically, the paper mentioned refers to the best (or minimal) Lipschitz constant. Here by Lipschitz constant we actually mean an upper bound for best Lipschitz constant. Note that we can upper-bound the best Lipschitz constant of MLPs by multiplying the operator norm of weight matrices. We clarify this in the Appendix Section B.4 in the revised manuscript."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292290304,
                "cdate": 1700292290304,
                "tmdate": 1700296124406,
                "mdate": 1700296124406,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "effdL7JK9H",
                "forum": "xAqcJ9XoTf",
                "replyto": "hv0szzSsmx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6928/Reviewer_qQbF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6928/Reviewer_qQbF"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their response. I appreciate the authors' comments and additional experiments. Actually, SPE's running time seems not to be prohibitive. However, the considered datasets contain only small graphs. I would like to see how the running time grows as a function of the size of the input graphs. The authors could create some synthetic dataset for this purpose.\n\nI agree with the authors that positional encodings are really important mainly because they are very general and can be plugged into any model. But do we actually need all these positional encodings? As I mentioned in the review, SPE provides minor improvements on the considered datasets. Could the authors construct some synthetic dataset where the SPE (because of its stability properties) significantly outperforms the models that use different encodings?\n\nI am not fully convinced by the authors' response to my first comment. The authors claim that we might need a larger $m$ (number of $\\phi$) such that the model is more expressive, but this may damage stability. The main selling point of the paper is the stability of the produced representations, but by adding several layers, the model might become stable. I think this deserves further discussion and some experimental results on that would strengthen the paper."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600334824,
                "cdate": 1700600334824,
                "tmdate": 1700600334824,
                "mdate": 1700600334824,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VlDFUgeYOt",
                "forum": "xAqcJ9XoTf",
                "replyto": "HB5HEzVcb8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6928/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6928/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up response to reviewer qQbF"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the new response. \n\nAt the beginning, we would like to address your concern about stability property, which is the most important contribution of our work.\n>  The authors claim that we might need a larger $m$ (number of $\\phi$) such that the model is more expressive, but this may damage stability. The main selling point of the paper is the stability of the produced representations, but by adding several layers, the model might become stable.\n\nNote that the one of main message we want to convey is **``stability guarantee is important''**, instead of ``more stability is better''. This is because a model class with stability guarantee has generalization guarantee as well. Its stability level (which relates to generalization) and expressive power naturally forms a trade-off. The Lipschitz constant of $\\phi$, number of $\\phi$, etc., serve as factors to control the trade-off and can be tuned to achieve empirical good results in practice. So it **does not hurt** to let $m$ be a hyper-parameter and it does not conflict with our stability argument.\n\n> I think this deserves further discussion and some experimental results on that would strengthen the paper.\n\nWe agree on this point. Actually what we showed in Figure 2 is exactly the effect of stability level on model's expressive power and generalization, although we control the stability of $\\phi$ by its Lipschitz constant instead of the number of $\\phi$ to use. \n\n> I would like to see how the running time grows as a function of the size of the input graphs.\n\nWe think this is a good suggestion and we are working on it right now. \n\n> Could the authors construct some synthetic dataset where the SPE (because of its stability properties) significantly outperforms the models that use different encodings?\n\nFrom the theory (see lemma 3.4 in ``*why previous methods are unstable'*'above), we know that by properly constructing graphs with small eigengap, previous unstable positional encoding methods should suffer from huge stability issue. Unfortunately, we may not be able to finish this experiment due to limited time of rebuttal period."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636872715,
                "cdate": 1700636872715,
                "tmdate": 1700636958104,
                "mdate": 1700636958104,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y9BdEyAR47",
                "forum": "xAqcJ9XoTf",
                "replyto": "HB5HEzVcb8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6928/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6928/Authors"
                ],
                "content": {
                    "title": {
                        "value": "More running time results"
                    },
                    "comment": {
                        "value": "Dear reviwer qQbF,\n\nPlease kindly check Table 5 and 6 in Appendix B.6 for new running time results on synthetic datasets with different graph size. Basically, we can see that SPE has a similar running time as SignNet when graph size is less than 160. In contrast, BasisNet causes out-of-memory (OOM) problem even we use batch size 5 for graph size 160. When graph size increases to 320, both SPE and BasisNet will lead to OOM due to square complexity, and SignNet's running time is very high as well."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727777243,
                "cdate": 1700727777243,
                "tmdate": 1700727795555,
                "mdate": 1700727795555,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wKNS3L4lkD",
            "forum": "xAqcJ9XoTf",
            "replyto": "xAqcJ9XoTf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6928/Reviewer_TVwL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6928/Reviewer_TVwL"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Stable and Expressive Positional Encodings (SPE), an architecture that mainly addresses the challenges of instability in using Laplacian eigenvectors as positional encodings for graph neural networks. The key insight to overcome instability is to avoid `hard partitions' of eigen-subspaces, and instead, use soft partitions via Lipshitz continuous functions over the spectrum. The stability of SPE is proved and validated via out-of-distribution generalization experiments.  Universal expressiveness is also proved, mainly based on another work, i.e., BasisNet."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- S1. I like the design of the experiment in that the authors validate the stability of SPEs from an aspect of out-of-distribution generalization.\n\n- S2. The authors target at robustness/instability and generalization of PEs, which is novel in the literature.\n\n- S3. The paper is overall well written."
                },
                "weaknesses": {
                    "value": ">  W1. The instability of prior method (i.e., the so-called hard partition method) is not proved. \n\n The authors point out under Eq.2 that **hard partition** is induced when $\\[\\phi_{\\ell}(\\boldsymbol{\\lambda})\\]_j=\\mathbb{1}$(other places in $\\phi(\\cdot)$ are zeros), \n\nand then $\\boldsymbol{V}\\text{diag}(\\phi_{\\ell}(\\boldsymbol{\\lambda}))\\boldsymbol{V}^{T}$ is the $\\ell$-th subspace. \n\nThe problem is that, if we set $\\\\{ \\phi_i \\\\}_{i=1}^{m}$ that induce the hard partitions, \n\nthen they are **constant functions** and meet the $K_{\\ell}$-Lipschitz continuous assumption in Assumption 3.1, which is then used to prove the stability of SPE. \n\nTherefore, the question is,  **is the prior method (i.e., the counterpart that uses hard partitions) really unstable?** It seems that hard-partitioned SPE can be proved to be stable via exactly the same proof of Theorem 3.1.\n\n> W2. Equivalence for $\\\\{\\phi_i\\\\}_{i=1}^{m}$ .\n\nThe authors restrict $\\\\{\\phi_i\\\\}_{i=1}^{m}$ to be permutation equivariant, whose input is the Laplacian spectrum. Here, the authors are asking for equivalence under the reordering of eigenmaps/eigenvalues, instead of the reordering of graph nodes. \n\n> W3. On the universal expressiveness. \n\nThe proof of this SPE's universality relies on being reduced to BasisNet. Therefore, two problems arise: \n\n- The experiment regarding expressiveness, i.e., the graph substructure counting, does not include BasisNet.\n- According to Lim et al. (2023), the instance of BasisNet, Unconstrained-BasisNet, universally approximates any continuous basis invariant function. In Unconstrained-BasisNet,  IGN-2 (Maron et al., 2018) is the core part to achieve such expressiveness. However, in implementation, the authors set $\\rho$ to be one identical GIN (Xu et al., 2019),  which would surely limit the expressiveness. \n\n> W4. Lack of description of baseline models.\n\nFor the same reason as in W3, in the experimental part, specific choices of baseline instances, i.e.,  $\\rho$ and $\\phi$ of BasisNet, should be described more clearly."
                },
                "questions": {
                    "value": "Please check W1, W2, and W3. Below is an additional question: \n\nQ1: Would the learned  $\\\\{\\phi_i\\\\}_{i=1}^{m}$ be close to each other? This would lead to similar position encodings."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6928/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6928/Reviewer_TVwL",
                        "ICLR.cc/2024/Conference/Submission6928/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6928/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833804741,
            "cdate": 1698833804741,
            "tmdate": 1700473286561,
            "mdate": 1700473286561,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7TqIsplVSG",
                "forum": "xAqcJ9XoTf",
                "replyto": "wKNS3L4lkD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6928/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6928/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TVwL"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comprehensive and constructive review.\n\n> **W1**: The instability of prior method (i.e., the so-called hard partition method) is not proved.\n\nThank you for pointing this out. Kindly check our unified response ``*Why previous methods are unstable?*'' above for detailed proof. \n\nRegarding your argument that hard paritition is stable by Theorem 3.1, it is not valid because $\\phi$ is actually **discontinuous**. To see this, let us consider basis-invariant methods using hard partition of eigensubspaces (e.g., BasisNet). In this case, the $k$-th entry of the hard partition $\\phi_{\\ell}(\\lambda)$ is $$[\\phi\\_{\\ell}(\\lambda)]\\_k=1,  \\text{if $\\lambda_k$ is $\\ell$-th smallest eigenvalue},\\quad [\\phi\\_{\\ell}(\\lambda)]\\_k =0, \\text{otherwise}$$ \nThis $\\phi$ function  is more like a **step function** instead of a constant function. It is **discontinuous**, and thus is not Lipschitz continuous. So Assumption 3.1 does not hold and Theorem 3.1 cannot be applied for such hard partition functions. You may kindly check Appendix C in the revised manuscript for details and examples.\n\n> **W2**: The authors restrict $\\phi$ to be permutation equivariant, whose input is the Laplacian spectrum. Here, the authors are asking for equivalence under the reordering of eigenmaps/eigenvalues, instead of the reordering of graph nodes.\n\nThe overall model is indeed permutation **equivariant to node reordering**. This is because the permutation equivariance of $\\phi$ (w.r.t eigenvalues) gurantees the permutation equivariance of the $V\\phi(\\lambda)V^{\\top}$ to node indices reordering. To formally see this, suppose we have eigendecomposition for $L=V\\text{diag}(\\lambda)V^{\\top}$. Applying permutation matrix $P$ to $L$ may produce an new decomposition in form of $PLP^{\\top}=PVQ\\text{diag}(\\lambda)Q^{\\top}V^{\\top}P^{\\top}$ with certain $Q$, where $Q\\in O(\\lambda)$ is an block-diagonal matrix where each block is a rotation matrix (i.e., $O(\\lambda)=\\\\{\\oplus_iQ_i: Q_i\\in O(d_i), d_i\\text{ is the dim of $i$-th eigensubspace}\\\\}$). That means the new eigenvectors after permutation become $PVQ$. If $\\phi$ is permutation equivaraint, then it means $Q\\text{diag}(\\phi(\\lambda))Q^{\\top}=QQ^{\\top}\\text{diag}(\\phi(\\lambda))=\\text{diag}(\\phi(\\lambda))$. Here the first equality holds since permutation equivariant $\\phi$ produces same values for entries of the same eigensubspace and thus we can switch the order of multiplication with block-diagonal $Q$.  Therefore the new positional encoding $PVQ\\text{diag}(\\phi(\\lambda))Q^{\\top}V^{\\top}P^{\\top}=PV\\text{diag}(\\phi(\\lambda))V^{\\top}P^{\\top}$, that is, the permutation version of the old positional encoding.\n\n> **W3**: The proof of this SPE's universality relies on being reduced to BasisNet. Therefore, two problems arise: 1. The experiment regarding expressiveness, i.e., the graph substructure counting, does not include BasisNet. 2. According to Lim et al. (2023), the instance of BasisNet, Unconstrained-BasisNet, universally approximates any continuous basis invariant function. In Unconstrained-BasisNet, IGN-2 (Maron et al., 2018) is the core part to achieve such expressiveness. However, in implementation, the authors set to be one identical GIN (Xu et al., 2019), which would surely limit the expressiveness.\n\n1. Please kindly check the new counting results of BasisNet in Figure 3 in the refined manuscript. \n2. We agree that $\\rho$ has to be sufficiently powerful to achieve universality. But in practice we find GIN can already get promising results. This may be because the proposed architecture $V\\text{diag}(\\phi(\\lambda))V^{\\top}$ is already expressive and thus a complicated $\\rho$ may not be that necessary for the current tasks.\n\n> **W4**: For the same reason as in W3, in the experimental part, specific choices of baseline instances, i.e., $\\phi$ and  $\\rho$ of BasisNet, should be described more clearly.\n\nThank you for pointing this out. We follow the same setting in BasisNet paper and use 2-IGNs for $\\rho$ and MLPs for $\\phi$. These details are made more clear in the Appendix B.2 in our revised manuscript.\n\n> **Q1**: Would the learned $\\phi_i$ be close to each other? This would lead to similar position encodings.\n\nWe don't observe such phenomenon in practice, but if it is the case it just means one channel for such $\\phi$ is sufficient for positional encoding on this specific task."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700290956423,
                "cdate": 1700290956423,
                "tmdate": 1700295652292,
                "mdate": 1700295652292,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2hATHDwKXn",
                "forum": "xAqcJ9XoTf",
                "replyto": "7TqIsplVSG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6928/Reviewer_TVwL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6928/Reviewer_TVwL"
                ],
                "content": {
                    "title": {
                        "value": "Still on the \"discontinuity\" of hard-partioning \\phi  (W1)"
                    },
                    "comment": {
                        "value": "Thanks to the author for the reply. However, I don't think I articulated **W1** clearly enough that **you completely missed my point**. Given the limited time remaining for author-reviewer interaction, I'd like to restate **W1** first (before I can go through ``Why previous methods are unstable?'' carefully).\n\nYour response is:\n> **RW1** ... **This function is more like a step function instead of a constant function**. It is discontinuous, and thus is not Lipschitz continuous. So Assumption 3.1 does not hold and Theorem 3.1 cannot be applied for such hard partition functions. You may kindly check Appendix C in the revised manuscript for details and examples.\n\nI would like to draw the author's attention that   $\\phi_{\\ell}$, **as you have written in P4, is multivariate**, i.e. the input and output are both $d$-dimensional. Therefore, I can set $\\phi_{\\ell}$ to be **constant**, which is in the form of $[0,0,\\cdots, 1, 0, 0]$. Such an **constant function** is of course continuous and satisfies Assumption 3.1, which is then used to prove the stability of SPE."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700401535068,
                "cdate": 1700401535068,
                "tmdate": 1700401535068,
                "mdate": 1700401535068,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Cc4a4lOjkI",
                "forum": "xAqcJ9XoTf",
                "replyto": "wKNS3L4lkD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6928/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6928/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Phi also needs to be permutation equivariant as stated in the page 5"
                    },
                    "comment": {
                        "value": "We thank the reviewer for raising this really good question and we are sorry for making this confusion. The reviewer says setting $\\phi_{\\ell}(\\lambda)$ to be a **constant vector** also satisfies Assumption 3.1. This is definitely **true**. However, this does not necessarily make it stable, because we also stated that ``**$\\phi_{\\ell}$ are always permutation equivariant neural networks**\"\" in Page 5, after equation (2). Theorem 3.1 is based the equivariant condition of $\\phi_{\\ell}$. In constrat, a constant vector of form $[0, 0, ..., 1, 0, 0]$  is clearly NOT permutation equivariant, so Theorem 3.1 cannot apply. The only equivariant constant vector is $c\\cdot [1, 1, 1, ..., 1, 1]$ (c is some constant) and it kind of reduces to PEG and is stable indeed.\n\nIn fact, if $\\phi_{\\ell}$ is NOT permutation equivariant, then the overall model is NOT permutation equivariant as well. The overall permutation equivariance is a necessary condition for stability (Remark 3.1), this is why we always constrain $\\phi_{\\ell}$ to be equivariant when discussing SPE. For this, please see our response above to your question on W2. \n\nSpecifically, to understand why setting $[0, 0, ..., 1, 0, 0]$ (say, $k$-th entry is 1) is unstable, we can consider there are multiple eigenvalues, say $\\lambda_k=\\lambda_{k+1}$, and the corresponding eigenspace $S=\\text{Span}(v_{k},v_{k+1})$ where $v_k, v_{k+1}\\in \\mathbb{R}^n$ give the basis the eigenspace. When $\\phi=[0,0,...,1,0,0]$, even if the graph does not change, the positional encoding $V\\phi V^T$ becomes $v v^T$ with any $v\\in S$ and $||v||_2=1$, which is non-unique and thus causes instability. \n\nAgain, thank you for let us realize this is not clear to the readers. We will make this equivariance condition more clear in the refined manuscript. Also, you can kindly check Appendix C in the refined manuscript, which explains why stability Theorem 3.1 cannot be applied to all of the previous methods."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700422547266,
                "cdate": 1700422547266,
                "tmdate": 1700437692926,
                "mdate": 1700437692926,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "upSdZUWbGX",
                "forum": "xAqcJ9XoTf",
                "replyto": "wKNS3L4lkD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6928/Reviewer_TVwL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6928/Reviewer_TVwL"
                ],
                "content": {
                    "title": {
                        "value": "Response: on Phi"
                    },
                    "comment": {
                        "value": "Thanks for the quick follow-up. Now I can accept the property and role of $\\phi$ : \n\n- $\\phi_{\\ell}$ are permutation equivariant, such that they yield the same values on indices corresponding to the same eigenvalues, which then leads to the equivalence relation $Q\\text{diag}(\\phi(\\lambda))Q^{\\top}=QQ^{\\top}\\text{diag}(\\phi(\\lambda))=\\text{diag}(\\phi(\\lambda))$. I think it is better to say that the permutation equivariance of  $\\phi_{\\ell}$ contributes to basis invariance. The equivariance of $\\phi$ digests $Q$, which is brought by the uncertainty of basis $V$ -- In BasisNet, $Q$ is ``digested\" by $V V^{\\top}$, instead.\n\n-  $\\phi_{\\ell}$ are continuous, combing equivariance, this makes SPE stable.\n\nI will raise my score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470599350,
                "cdate": 1700470599350,
                "tmdate": 1700470661434,
                "mdate": 1700470661434,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Zm9uhiPIz3",
                "forum": "xAqcJ9XoTf",
                "replyto": "wKNS3L4lkD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6928/Reviewer_TVwL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6928/Reviewer_TVwL"
                ],
                "content": {
                    "title": {
                        "value": "Response: on W3&W4"
                    },
                    "comment": {
                        "value": "In the initial stages of the rebuttal, my primary focus centered around two key points:\n\n- W1&W2: Addressing the role of $\\phi$.\n\n- W3&W4: Discussing the relationship between this study and BasisNet.\n\nWhile progress has been made on the first point, my attention remains directed toward remaining issues on the second aspect.\n\nThe utilization of a single identical GIN repeated $n$ times for setting $\\rho$ raises concerns among reviewers. This concern isn't solely due to the potential increase in time complexity, but also stems from the fact that such a model structure is notably uncommon. Could the use of stacked GINs (although limits expressiveness theoretically) be a contributing factor enabling your model to outperform BasisNet?"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700473221975,
                "cdate": 1700473221975,
                "tmdate": 1700473221975,
                "mdate": 1700473221975,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bdWYLwARw6",
            "forum": "xAqcJ9XoTf",
            "replyto": "xAqcJ9XoTf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6928/Reviewer_fM7Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6928/Reviewer_fM7Q"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes stable and expressive Laplacian positional encodings (SPE) by performing a soft and learnable partition of eigensubspaces. The encoding guarantees that small perturbations to the input Laplacian induce a small change to the final positional encodings. The empirical results suggest a trade-off between stability (correlated with better generalization) and expressive power."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea to address the stability of LapPE is novel, with SPE being a universal basis invariant architecture. \n- The motivation is well established, the difference to other related works is concise, the propositions are described well and the strength of SPE as a universal basis invariant architecture is presented thoroughly. \n- The experiments show the improvement in generalisation for SPE and its improved capabilities in recognising substructures, which are interesting outcomes of the architecture."
                },
                "weaknesses": {
                    "value": "- The novelty of the method itself is partially limited as the idea to use a weighted correlation over the eigenvectors closely resembles the correlation used in BasisNet. \n- The experiments are limited. The performance of SPE in Table 1 is sub-par, and details of the experimental results in Figure 2 are unclear and the hyperparameters seem not to be reported, which makes it hard to reproduce the experiments. \n- The point regarding the trade-off between expressivity and generalisation is unclear. Is there a formal explanation which we can quantify? \n- Perhaps additional experiments could be useful, e.g.,:\n  - An experiment comparing the generalisation gap of LapPE/BasisNet/SPE.\n  - Evaluating the performance of LapPE/BasisNet/SPE on LRGB or TUDatasets."
                },
                "questions": {
                    "value": "Please refer to my review."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6928/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698959323520,
            "cdate": 1698959323520,
            "tmdate": 1699636807133,
            "mdate": 1699636807133,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KJzy6FnVZL",
                "forum": "xAqcJ9XoTf",
                "replyto": "bdWYLwARw6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6928/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6928/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fM7Q"
                    },
                    "comment": {
                        "value": "We first thank the reviewer for the valuable comments. Here is our response. \n\n> **W1**: The novelty of the method itself is partially limited as the idea to use a weighted correlation over the eigenvectors closely resembles the correlation used in BasisNet.\n\nNote that the key difference between our method and BasisNet is not just adding a weight: BasisNet computes the correlation $V_kV_k^{\\top}$ between eigenvectors $V_k$ **within** the **$k$-th eigensubspace** for different $k$; in contrast, SPE computes the correlation $V\\phi V^{\\top}$ between **all the eigenvectors $V$ across different eigensubspaces**. This allows us to combine multiple eigensubspaces with smooth composition. The main argument is that this operation equipped with a continuous and equivariant $\\phi$ is the key to achieve **stability**.\n\n> **W2**: The experiments are limited. The performance of SPE in Table 1 is sub-par.\n\nThe SPE's result on ZINC is 0.0693, which greatly improves the baseline such as SignNet (0.0853) and BasisNet (1.5555). In fact, this result also outperforms many other recently proposed positional encodings methods [1, 2, 3] and even highly expressive GNN models [4, 5, 6]. \n\nFor Alchemy dataset, SPE is still the best, though all positional encodings bring a little improvement. We conjecture that Laplacian encodings may not be a good inductive bias for this dataset.\n\nAlso thanks for pointing the hyperparameter things out. Some of the hyperameters were reported in the Appendix, Section B and we add more details of them in the revised manuscript.\n\n> **W3**: The point regarding the trade-off between expressivity and generalisation is unclear. Is there a formal explanation which we can quantify?\n\nA classic interpretation is the **variance bias trade-off**. Let $\\hat{\\epsilon}_n$ be the training loss, $\\epsilon$ be the test loss, $\\mathcal{H}$ be the hypothesis class and $\\hat{h}_n=\\arg\\min\\_{h\\in \\mathcal{H}}\\hat{\\epsilon}\\_n(h)$ be the trained model. If we increase the size of the hypothesis class (i.e., make the model more complicated and expressive, which corresponds to enlarge the Lipschitz constants of $\\phi$ in Figure 2), intuitively\n- training loss $\\hat{\\epsilon}_n(\\hat{h}_n)$ is going to decrease, because we are optimizing over a larger hypothesis class. Empirically this corresponds to the overall decreasing trend of training loss as shown in the left-most plots in Figure 2.\n- generalization gap $\\epsilon(\\hat{h}_n)-\\hat{\\epsilon}_n(\\hat{h}_n)$ is going to increase, because from Proposition 3.1 this term is upper bounded by the complexity of the model and the divergence between training and test data distribution. Empirically this corresponds to the overall increasing trend of generalization gap shown in the right-most plots in Figure 2.\n\n> **W4**: Perhaps additional experiments could be useful, e.g.,: 1. An experiment comparing the generalisation gap of LapPE/BasisNet/SPE. 2. Evaluating the performance of LapPE/BasisNet/SPE on LRGB or TUDatasets.\n\nThank you for the good suggestions. \n\n1. Now we add the training loss and the generalization gap (test loss - training loss) on ZINC dataset as shown in Table 3, Appendix B.5 in the refined manuscript. We can see that though SignNet and BasisNet achieve a pretty low training MAE (high expressive power), their generalization gap is larger than other baselines (poor stability) and thus the final test MAE is not the best. For baseline GNN and PEG, they are pretty stable with small generalization gap, but the poor expressive power make them hard to fit the dataset well (training loss is high). In contrast, SPE has not only a lowest training MAE (high expressive power) but also a small generalization gap (good stability). That is why it can obtain the best test performance among all the models.\n\n2. We are working on the TUDatasets you suggest and the results will be released before the end of rebuttal. LRGB consists of more and larger molecular graphs and is hard to be covered due to limited time of rebuttal period. \n\n[1] Dwivedi, Vijay Prakash, et al. \"Graph neural networks with learnable structural and positional representations.\" arXiv preprint arXiv:2110.07875 (2021).\n\n[2] Eliasof, Moshe, et al. \"Graph Positional Encoding via Random Feature Propagation.\" arXiv preprint arXiv:2303.02918 (2023).\n\n[3] Ramp\u00e1\u0161ek, Ladislav, et al. \"Recipe for a general, powerful, scalable graph transformer.\" Advances in Neural Information Processing Systems 35 (2022): 14501-14515.\n\n[4] Bodnar, Cristian, et al. \"Weisfeiler and lehman go cellular: Cw networks.\" Advances in Neural Information Processing Systems 34 (2021): 2625-2640.\n\n[5] Frasca, Fabrizio, et al. \"Understanding and extending subgraph gnns by rethinking their symmetries.\" Advances in Neural Information Processing Systems 35 (2022): 31376-31390.\n\n[6] Zhang, Bohang, et al. \"A complete expressiveness hierarchy for subgraph gnns via subgraph weisfeiler-lehman tests.\" arXiv preprint arXiv:2302.07090 (2023)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289708269,
                "cdate": 1700289708269,
                "tmdate": 1700295465212,
                "mdate": 1700295465212,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wKGTBbOHdK",
            "forum": "xAqcJ9XoTf",
            "replyto": "xAqcJ9XoTf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6928/Reviewer_Fdup"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6928/Reviewer_Fdup"
            ],
            "content": {
                "summary": {
                    "value": "This works further deepens results on basis-invariant GNNs building on work of e.g. Wang et al (ICLR 2022) and Lim&Robinson et al (ICLR 2023). The authors propose a simple generalization of basis-net, with strong theoretical guarantees (H\u00f6lder-smoothness / stability and basis-invariance). They achieve strong performance in standard molecular benchmarks, out-of-distribution benchmarks and cycle count tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Extremely well written and easy to follow. Figure 1 is nice!\n\nThe proposed model SPE (Eq. 2) is a simple generalization extension of basis-net. It is very interesting to see that such a straightforward modification yields such strong (and non-trivial) theoretical results (Theorem 1, etc.).\n\nEmpirical performance is very convincing, e.g. for Zinc and the OOD tests.\n\n------ during rebuttal ------\nas reviewers addressed my concerns and in fact added clarifications on runtime and more importantly on the unstability of previous methods I raised my score and now clearly vote for acceptance."
                },
                "weaknesses": {
                    "value": "The achieved theoretical and empirical results, while very interesting, seem somewhat incremental compared to Wang et al (ICLR 2022) and Lim&Robinson et al (ICLR 2023). The authors should discuss the differences more clearly. In particular:\n* The reason for H\u00f6lder continuity ($c\\neq1$) is not fully clear. E.g. does PEG / standard basis-net already already satisfy your stability criterion Def 3.1 and / or Assumption 3.1.? If yes, what is the conceptual / theoretical benefit of your proposed architecture. If not, could you please provide an argument why the assumptions fail for PEG / basis-net.\n* Is $c\\neq 1$ crucial for any proof / guarantee / assumption? \n* Are there cases where SPE satisfies the stability assumption of Wang et al (ICLR 2022), i.e., with $c=1$?\n\nPlease see also the questions below.\n\nMinor:\n* Remark 3.1 Should probably be attributed to Wang et al (ICLR 2022), as they have the same statement for $c=1$.\n\nI am happy to raise my score, if the authors properly address my concerns and questions."
                },
                "questions": {
                    "value": "Please provide runtimes for your proposed method. Preferably for pre-processing and overall runtime. This would help to put the achieved results into context with basis-net, etc.\n\nWhile the OOD generalization bound is very interesting, can you also state a standard PAC-style generalization bound (same distribution for train and test)?\n\nDo you have a counter-example where SPE cannot count $k$-cycles for $k\\leq 6$?\n\nThe $n\\times n\\times m$ might be somewhat excessive for certain datasets. Would it be possible to exchange $V\\\\phi(\\cdot) V^T$ to $V^T\\phi(\\cdot) V^T$ to get the much smaller $d\\times d\\times m$ instead? If not what would this more compact model correspond to?\n\nIf I am notmistaken SPE and basis-net should have the same expressivity and thus at least theoretically be both equally capable of counting cycles etc. Can you provide some intuition why SPE performs significantly better than basis-net in this task (Figure 3)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6928/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6928/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6928/Reviewer_Fdup"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6928/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699034864527,
            "cdate": 1699034864527,
            "tmdate": 1700391490826,
            "mdate": 1700391490826,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XIgILBK1FK",
                "forum": "xAqcJ9XoTf",
                "replyto": "wKGTBbOHdK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6928/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6928/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Fdup"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful question. Below is our response. \n>  **W1**: does PEG / standard basis-net already satisfy your stability criterion Def 3.1 and / or Assumption 3.1.?\n\nBasisNet does not have stability guarantee Def 3.1 because it does not satisfy Assumption 3.1. In short that is because we can see it as using discontinuous $\\phi$ functions, which violates Assumption 3.1. Kindly check our unified response  ``*Why previous methods are unstable?* '' above. \n\nPEG is indeed stable, but its expressive power is extremely limited. This is becasue PEG cannot distinguish eigenvectors $V\\in\\mathbb{R}^{n\\times d}$ and eigenvectors $VQ\\in\\mathbb{R}^{n\\times d}$, where $Q\\in O(d)$ is an arbitrary $d$-dim rotation matrix. This point is also summarized in our Introduction section `` But, to achieve stability, it (PEG) completely ignores the distinctness of each eigensubspaces\nand processes the merged eigenspaces homogeneously. Consequently, it loses expressive power and\nhas, e.g., a subpar performance on molecular graph regression tasks''. And this also got verified by our experiments on ZINC, Table 1, which shows that the performance of PEG is limited. Additionally, we show that it is due to the high training loss of PEG in Table 3, Appendix B.5 in the revised manuscript. This verifies the poor expressive power of PEG.\n\n> **W2&3**: Is $c\\neq 1$ crucial for any proof / guarantee / assumption? Are there cases where SPE satisfies the stability assumption of Wang et al (ICLR 2022), i.e., with $c=1$?\n\n$c\\neq 1$ serves for describing the stability property of SPE, which is the general Holder continuity instead of Lipschitz continuity. For the second question, we think it is an open question. At this point we are not sure whether SPE can have a Lipschitz bound instead of a Holder bound. But fortunately even $c\\neq 1$ is sufficient to provide a OOD generalization guarantee.\n\n> **Q1**: Please provide runtimes for your proposed method. Preferably for pre-processing and overall runtime. This would help to put the achieved results into context with basis-net, etc.\n\nThank you for pointing this out. We add the average running time of training and inference, which can be found in our unified response ``*running time evaluation*'' above. The pre-processing is the standard Laplacian decomposition same as others.\n\n> **Q2**: While the OOD generalization bound is very interesting, can you also state a standard PAC-style generalization bound (same distribution for train and test)?\n\nThe PAC generalization bound is quite different from OOD bound. OOD bound states how the population risk may change for the same prediction model but two different population distributions, while the PAC theory describes the difference of population risk between empirical risk minimizer and the population risk minimizer. \n\n> **Q3**: Do you have a counter-example where SPE cannot count $k$-cycles for $k\\le 6$?\n\nAs implied by Proposition 3.4, SPE can count all $k$-cycles with $k\\le 6$. So there is no such counter-example.\n\n> **Q4**: The  $n\\times n\\times m$ might be somewhat excessive for certain datasets. Would it be possible to exchange $V\\phi(\\cdot)V^{\\top}$to $V^{\\top}\\phi(\\cdot)V^{\\top}$ to get the much smaller $d\\times d\\times m$ instead? If not what would this more compact model correspond to?\n\nWe cannot naively do $V^{\\top}\\phi V$ as the dimension does not match. One alternative way could be to sparse the $V\\phi V^{\\top}$ by only computing and storing its $(i,j)$-entry where $(i,j)$ is an edge. Then these positional encodings are treated as edge features for downstream tasks. The complexity will reduce from $O(n^2)$ to $O(|E|)$ where $|E|$ is the number of edges. In practice, the sparsing of $V\\phi V^{\\top}$ can be efficiently done by firstly assigning $V_{i, :}$ to $i$-th node features and using an one-layer message passing neural network to update and store edge features $[V\\phi V^{\\top}]\\_{i,j}=\\sum_{k}[\\phi(\\lambda)]_{k} V\\_{i,k}V\\_{j,k}$.\n\n> **Q5**: Can you provide some intuition why SPE performs significantly better than basis-net in this task (Figure 3)?\n\nThis is because BasisNet is not a stable method and thus generalizes poorly. Actually both SPE and BasisNet can fit pretty well on training dataset (training loss < 0.05). However, BasisNet suffers from overfitting with a large generalization gap (test error - training error). In contrast, the generalization gap of SPE is almost zero. This phenomenon is also observed on ZINC, as shown in Table 3, Appendix B.5 in the refined manuscript."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288335265,
                "cdate": 1700288335265,
                "tmdate": 1700295303860,
                "mdate": 1700295303860,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nYeiBuI9i3",
                "forum": "xAqcJ9XoTf",
                "replyto": "XIgILBK1FK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6928/Reviewer_Fdup"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6928/Reviewer_Fdup"
                ],
                "content": {
                    "title": {
                        "value": "Quick follow up"
                    },
                    "comment": {
                        "value": "Thank you for your reply. Two quick follow-up comments.\n\n* The $\\leq 6$ was a typo, I've meant to ask whether you know counter-examples where SPE cannot count cycles of size $\\geq 7$.\n* Of course PAC and OOD are different. I was just curious whether your assumptions (stability, H\u00f6lder-smooth, etc.) can also be used to derive PAC bounds."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700290358294,
                "cdate": 1700290358294,
                "tmdate": 1700290358294,
                "mdate": 1700290358294,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "advzFgeEOC",
                "forum": "xAqcJ9XoTf",
                "replyto": "lZI1T0wuaq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6928/Reviewer_Fdup"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6928/Reviewer_Fdup"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my concerns. In particular, thanks for the clarifications about the unstability of previous methods and the runtime aspects. I raise my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700391418630,
                "cdate": 1700391418630,
                "tmdate": 1700391418630,
                "mdate": 1700391418630,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]