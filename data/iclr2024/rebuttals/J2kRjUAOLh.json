[
    {
        "title": "Contrastive Predict-and-Search for Mixed Integer Linear Programs"
    },
    {
        "review": {
            "id": "Uipxj4Qg21",
            "forum": "J2kRjUAOLh",
            "replyto": "J2kRjUAOLh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2/Reviewer_KTmj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2/Reviewer_KTmj"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose to integrate contrastive learning with the pipeline of solving mixed integer linear programming. They manage to generate positive samples and negative samples during the training. The positive samples are optimal or near-optimal solutions of MILP, while the negative samples are infeasible or low-quality solutions. The model is then trained by these samples via supervised contrastive learning to predict better solutions. After this, the predicted solutions are improved by the PaS framework to make them become valid optimal solutions. Experiments on multiple datasets show the performance of the proposed ConPas framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n2. The idea of utilizing contrastive learning in MILP looks interesting to me.\n3. The experiments contain various MILP datasets."
                },
                "weaknesses": {
                    "value": "1. I find one work in related work[1] very similar to this paper. Both of these two papers propose to utilize contrastive learning in solving MILP and share the core idea of generating positive and negative samples. The only difference is the operation after the contrastive learning part, the ICML paper[1] uses large neighborhood search (LNS) and this ICLR paper uses Predict and Search (PaS). Actually, I think this paper is covered by the ICML paper, as PaS could be regarded as a variant of LNS. Though the authors do mention this ICML paper in the related work, they do not discuss the difference between their work and the ICML paper, nor compare it as a baseline. \n2. Though the idea of utilizing contrastive learning in MILP looks interesting, I consider the current usage of contrastive learning to be more like an incremental part. In this work, solving MILP basically relies on the performance of PaS. I am not sure if this contribution is good enough for ICLR. To me, this work is more like using contrastive learning to find a better initialization for PaS, of which the application is limited. \n3. The results of experiments look good, but I think more datasets with hard cases are required. In my own experience of using SCIP,  I think MVS and MIS are relatively easy for SCIP. In contrast, the datasets from NeurIPS 2021 ML4CO are difficult for SCIP, but it looks like the authors did not select the whole datasets of ML4CO, as they said: \"IP instances are taken from the NeurIPS 2021 ML4CO competition Gasse et al. (2022).\" I wonder how the data is selected. In fact, there are 3 benchmarks in NeurIPS 2021 ML4CO[2], I wonder why the authors neglect them. Besides, a common dataset MIPLIB is also missing in the paper.\n\n\n[1] Huang, T., Ferber, A.M., Tian, Y., Dilkina, B. &amp; Steiner, B.. (2023). Searching Large Neighborhoods for Integer Linear Programs with Contrastive Learning. <i>Proceedings of the 40th International Conference on Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 202:13869-13890 Available from https://proceedings.mlr.press/v202/huang23g.html.\n\n[2] https://www.ecole.ai/2021/ml4co-competition/"
                },
                "questions": {
                    "value": "1. Please discuss your paper with the ICML paper I mentioned in the weakness. In my view, these two papers are very similar and the ICML paper seems to cover your work to some extent. A comparison in experiments is also suggested if possible.\n2. As I mentioned before, this work is more like using contrastive learning to find a better initialization for PaS. I wonder can this work be applied to methods other than PaS? e.g. Neural Diving mentioned in the paper. \n3. The datasets in the experiments require more improvement."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697774654807,
            "cdate": 1697774654807,
            "tmdate": 1699635924059,
            "mdate": 1699635924059,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0kNBOJMhsI",
                "forum": "J2kRjUAOLh",
                "replyto": "Uipxj4Qg21",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the feedback and suggestions. Regarding the weaknesses and questions concerning the novelties and choices of MIP benchmark, please kindly refer to the general responses.\n\nIn addition, we would like to further discuss how this work could be applied beyond PaS to answer your 2nd question: ConPaS is more versatile since the prediction coming out of its ML model can be useful in different ways. An example is to warm start LNS as mentioned earlier. In addition, one could leverage the ML prediction from ConPaS to assign variable branching priorities and/or generate cuts in tree searches such as branch-and-bound (or branch-and-cut) search. We defer the deployment of ConPaS in different algorithms to future work.\n\nWe also want to clarify that Neural Diving is a more restricted variant of ConPaS and PaS, where it corresponds to setting \\Delta = 0 in PaS that allows no change of the assigned values once they\u2019re fixed in the search."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367634501,
                "cdate": 1700367634501,
                "tmdate": 1700367634501,
                "mdate": 1700367634501,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2W4cMY9YtS",
                "forum": "J2kRjUAOLh",
                "replyto": "Uipxj4Qg21",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2/Reviewer_KTmj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2/Reviewer_KTmj"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the detailed response and the improved quality of the paper. However, I think the main concern of this paper is still the difference between ConPas and CL-LNS. I understand that they shall be complementary to each other, but I still believe that the approaches of these two works are similar or at least with strong correlation, as mentioned by other reviewers. Therefore, I think the authors should include the discussion between ConPas and CL-LNS in the **main paper**, instead of just mentioning it as related works, otherwise, it will be suspected of deliberately avoiding. As the authors use a lot of space in the general response to describe the difference in their general response, you can not suppose the readers of your paper understand the difference just by mentioning and citing it. Due to the similarity of ConPas and CL-LNS, It's not an exaggeration to open a separate subsection, which could include a discussion of differences or add a table of comparison. Only in this way, the readers can fully understand the novelty of this work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552524218,
                "cdate": 1700552524218,
                "tmdate": 1700552548566,
                "mdate": 1700552548566,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PMdcjp79U4",
            "forum": "J2kRjUAOLh",
            "replyto": "J2kRjUAOLh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2/Reviewer_nvLV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2/Reviewer_nvLV"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a method for finding primal solutions to mixed-integer programs using a graph neural network-based approach. The training and performance of the approach is improved through the use of constrastive learning, which has been gaining popularity in a variety of deep reinforcement learning applications due to the fact that it does not require expensive labeling of data to \"pre-train\" networks. The approach is based on the \"predict and search\" method from a previous ICLR paper. The approach is evaluated experimentally on several relatively easy MIP problems and a dataset of integer programs from the NeurIPS 2021 ML4CO competition."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Contrastive learning shows great promise in the space of combinatorial optimization; we see again and again that it is an effective mechanism for reducing training time and creating great models.\n\n- The empirical performance of the method on the datasets tested is quite strong.\n\n- (Updated) The novelty of the paper, while not huge, is sufficient for ICLR. The authors have indicated how it differs from CL-LNS, and the bi-level model is an interesting contribution that other groups solving MIPs will want to consider."
                },
                "weaknesses": {
                    "value": "- The instance dataset is not so great, but I admit there are not so many good public MIP problems out there. Simply put, claiming that you can solve the CA dataset to a MIP person is just really not that interesting. Since all the other MIP papers at ICLR/NeurIPS seem to have the same problem, I'll let it pass.\n\n- Using SCIP as a direct point of comparison is not really fair. SCIP is trying to prove optimality, while the method proposed in this work is just a primal heuristic. I appreciate, however, that the authors do not make big claims about beating SCIP the way some papers in this area do. They do seem to understand that beating SCIP is relatively meaningless.\n\n- I am a little surprised to not see an abalation study on the modified loss function. (Update: the authors have provided one, and the modified loss works and is not the only reason it is outperforming previous work)\n\n- The introduction's description of Gurobi and CPLEX is not complete. They are really branch and cut algorithms with (what CPLEX calls) \"dynamic search\" (and a whole bunch of other stuff, who knows what half of it is...) (Update: this seems to be fixed)\n\n- (Update) I still feel like there could be more experimentation regarding the negative examples (e.g., versus the strategy in the CL-LNS paper??). Since this is the main contribution, I wish it was actually more in focus throughout the paper."
                },
                "questions": {
                    "value": "All questions have been answered."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2/Reviewer_nvLV",
                        "ICLR.cc/2024/Conference/Submission2/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698406646855,
            "cdate": 1698406646855,
            "tmdate": 1700665090489,
            "mdate": 1700665090489,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8Yg7Up2BrI",
                "forum": "J2kRjUAOLh",
                "replyto": "PMdcjp79U4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the feedback and suggestions. Regarding the weaknesses concerning the novelties, choices of MIP benchmark and using SCIP as a baseline (weaknesses 1,2 and 3), please kindly refer to the general responses to all reviewers posted at the top.\n\nRegarding weaknesses 4 and 5:\n\n(4) We conduct an additional ablation study on ConPaS-LQ on the MVC and CA problems. (Due to limited computation resources, we are still in the process of getting results for ConPaS-inf and other problems.)\nThe initial results are shown in the table below, where ConPaS-LQ (unweighted) refers to training using the original InfoNCE function without considering different qualities of the samples and ConPaS-LQ (weighted) refers to training using the modified loss. When we use the original loss function, ConPaS is still able to outperform PaS. Its performance further improves when the modified loss function is used.\n\n|                        | MVC        |                 | CA         |                 |\n|------------------------|------------|-----------------|------------|-----------------|\n|                        | Primal Gap | Primal Integral | Primal Gap | Primal Integral |\n| PaS                    | 0.17%      | 13.9            | 1.16%      | 28.9            |\n| ConPaS-LQ (unweighted) | 0.12%      | 3.3             | 0.57%      | 24.3            |\n| ConPaS-LQ (weighted)   | 0.10%      | 2.8             | 0.16%      | 19.7            |\n\n\n\n(5) We thank the reviewer for the suggestions for a more accurate description for solvers like Gurobi and CPLEX. We have updated the text accordingly in the new draft."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367555183,
                "cdate": 1700367555183,
                "tmdate": 1700367716368,
                "mdate": 1700367716368,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qBOwsxbPuW",
                "forum": "J2kRjUAOLh",
                "replyto": "8Yg7Up2BrI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2/Reviewer_nvLV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2/Reviewer_nvLV"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you for these results. These results are very good to know. \n\nLet me emphasize that the statement in bold at the end of your general response does not interest me, and it shouldn't interest the other reviewers either.\n\nI now understand the novelty of the paper to be the way you compute the negative examples for the contrastive learning and that there are key differences to CL-LNS. This puts the paper in somewhat of a different light. I don't usually raise scores by this much, but actually the bilevel model for computing negative examples is rather clever and really works well. I encourage the other reviewers to take this into account. I will adjust my review."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664835850,
                "cdate": 1700664835850,
                "tmdate": 1700664835850,
                "mdate": 1700664835850,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LMXXBnRdjZ",
            "forum": "J2kRjUAOLh",
            "replyto": "J2kRjUAOLh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2/Reviewer_GFiJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2/Reviewer_GFiJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a construction approach of positive and negative samples based on the quality of milp problem\u2019s feasible solutions. With the constructed samples, one can train a GNN model to predict good assignments for integer variables using contrastive learning mechanism, which helps search optimal solution more quickly. Superior experimental results demonstrate the effectiveness and generalizability of the proposed approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The research topic is valuable and the paper is well written. Moreover, the designed method is presented with succinctly and clearly as well as its motivation. The performance of trained GNN is also impressive, which indicates the superiority of the proposed method."
                },
                "weaknesses": {
                    "value": "There are still some issues needed to be addressed to make this paper meet the requirement of ICLR: \n1.\tThe contribution and novelty is not summarized clearly and relatively weak. The main contributions of this paper are applying contrastive learning to predict and search optimal solution.\n\n2.\tResults of empirical evaluation can be more solid and convincing. The experiments are just conducted on two generated dataset and one competition dataset, without the recognized authoritative dataset MIPLIB2017 benchmark. Furthermore, only an open-source MILP solver, which is not well configured, is involved in baselines. Considering that different configuration can significantly affect the solver\u2019s performance, I would expect some further comparative experiments conducted on SCIP configured with tuned parameters or some more powerful commercial solvers (like GUROBI and CPLEX)."
                },
                "questions": {
                    "value": "I noticed that the effect of hyperparameter k0 and k1 is evaluated. Of course, this hyperparameter is important, because it controls the tradeoff between the feasibility and quality of predicted solutions. However, considering that MILP instances generally have different scales of integer variables, a specific number of integer variables may not be a good choice. I was wondering that would it be better if we use the coverage rate (i.e., the ratio of fixed variables in the entire set of integer variables when using predict methods like Neural Diving) to control the fixed number of integer variables.  \n\nIn addition, some studies indicate that each instance has an unique optimal coverage rate (https://arxiv.org/abs/2308.00327), so I think that evaluating the effect of k0 by just computing an average number on one dataset (CA) may not help readers configure their own prediction model properly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2/Reviewer_GFiJ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698413312588,
            "cdate": 1698413312588,
            "tmdate": 1699635923859,
            "mdate": 1699635923859,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "50D8TvJWne",
                "forum": "J2kRjUAOLh",
                "replyto": "LMXXBnRdjZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the feedback and suggestions.\nRegarding the weaknesses, please refer to the discussions on the novelties of the work and choices of benchmark in the general response to all reviewers. \n\nBelow are our responses to answer the question regarding hyperparameters and coverage rates:\n\nWe agree that using coverage rates as alternatives to model k0 and k1 would be more helpful when the instances are diverse in size. In our paper, we described a systematic way in Section 5.1 \u201cHyperparameters\u201d to tune both k0 and k1 based on a percentage of the number of variables (10%-50%). We believe that this hyperparameter tuning method is easy to follow. We report the results of different k0 for CA to demonstrate how tuning could be done.\n\nRegarding the optimal coverage rate studied in [Yoon et al., 2023], it is important for methods like Neural Diving (ND) since it requires training a separate model for each coverage rate. With an optimal coverage rate identified, it helps overcome the training inefficiency of ND. However in ConPaS, instead of fixing all variables according to the prediction, we let the MIP solver explore regions around the prediction that allows more flexibility and room for inaccuracy in prediction, therefore removing the need for an accurate coverage threshold. \n\n\n[Yoon et al., 2023] Threshold-aware Learning to Generate Feasible Solutions for Mixed Integer Programs. Arxiv 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367506949,
                "cdate": 1700367506949,
                "tmdate": 1700367506949,
                "mdate": 1700367506949,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YDnAiTaavU",
            "forum": "J2kRjUAOLh",
            "replyto": "J2kRjUAOLh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2/Reviewer_fyvF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2/Reviewer_fyvF"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a predict-and-search approach for solving mixed integer programming(MIP), according to a GNN-guided approach from [Han2022]. The algorithm collects high-quality solutions as positive samples and low-quality solutions as negative samples, and then trains the prediction model by contrastive learning. The authors demonstrate that the proposed method outperforms the baseline on four commonly used mixed-integer linear programming datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The effect of improving the prediction model through contrastive learning training method is intuitive and effective.\n2. The author's experiments show that the proposed method has a significant improvement over the baseline.\n3. The paper is mostly well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The technical novelty is limited. First, it is a somewhat straightforward application of contrastive learning to predict-and-search. Second, the proposed method is essentially the same as the ICML 2023 paper [Huang2023] (Figure 1 of this paper almost coincides with Figure 1 in [Huang2023]), if we consider the procedure as a one-step LNS.\n\n2. Since the proposed approach is based on predict-and-search, it cannot guarantee the optimality or feasibility. This limitation is not discussed or analyzed properly in this paper. For example, there is no empirical study on the feasibility ratio on the test instances. The authors should also conduct experiments on more constrained problems. Furthermore, it is somewhat unfair to compare the anytime performance with SCIP, since the proposed method (as well as predict-and-search) essentially solves a much simpler problem than SCIP since some variables are fixed.\n\n3. The authors collected training data using Gurobi, but only compared the test performance with SCIP. I cannot see any reason why not compare with Gurobi at test time.\n\n4. The authors used two ways to collect negative samples, but only report their empirical performance, without a deep analysis on which way is more reasonable.\n\n5. The authors did not show the results of how accurate the solution prediction is."
                },
                "questions": {
                    "value": "Please see the above weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2/Reviewer_fyvF"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824385414,
            "cdate": 1698824385414,
            "tmdate": 1699635923726,
            "mdate": 1699635923726,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ui2FqW5xoH",
                "forum": "J2kRjUAOLh",
                "replyto": "YDnAiTaavU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the feedback and suggestions.\nRegarding the weaknesses in the novelties and comparison to SCIP and Gurobi (weaknesses 1,2 and 3), please kindly refer to the general responses to all reviewers.\n\nTo address the other weaknesses:\n\n2. ConPaS is a solution construction heuristic and we acknowledge that our approach doesn\u2019t guarantee optimality or feasibility. Similarly, this drawback also applies to Neural Diving (ND) [Nair et al, 2020] and PaS [Han et al, 2023]. However, in a distributional setting where one needs to solve similar MIP instances over and over again, approaches like ND and ConPaS can be particularly helpful if it is able to predict solutions that are empirically feasible and of high quality. This is indeed true according to our experiments - on the five MIP benchmarks (including one in the Appendix), we achieve a 100% feasibility rate using a consistent set of hyperparameters on each benchmark, confirming the applicability of these approaches. However, we also acknowledge that ConPaS (or ND and PaS) is not universally applicable to all MIP solving, especially on more constrained problems. For example, using MIP for scientific discoveries when the solutions are sparse could be extra challenging [Deza et al., 2023] and often we need to design other approaches tailored to them. We have added the discussion in the conclusion section.\n\n3. Thank you for this comment. We use Gurobi to collect data since Gurobi typically runs a lot faster than SCIP. For data collection, we set the time limit to an hour for Gurobi. We could easily replace Gurobi with SCIP for data collection and get the same-quality training data but this comes at the cost of 4-8 times (4-8 hours per instance)  longer runtime on average. Due to our limited computational resources, using Gurobi for data collection is more practical for us.\n\nWe have included results on Gurobi in Appendix Section D.2 in the updated draft. We show that ConPaS still outperforms Gurobi and PaS significantly in terms of both primal integral and primal gap. \n\n\n4. The main motivation to design negative samples this way is that we want them to be close to positive samples in the input space but actually with very different quality (i.e., near miss). From a theoretical point of view, the InfoNCE loss we use has the property that it will automatically focus on hard negative pairs (i.e., samples with similar representation but of very different qualities) and learn representations to separate them apart (See e.g., [Tian 2022]). While our approach is built upon a theoretical understanding of contrastive learning, we acknowledge that our work designs the negative samples heuristically and does not aim for theoretical impacts. On the other hand, we believe that our work contributes a new principled method that demonstrates strong empirical performance in challenging domains.\n\n\n5. Regarding the accuracy of the predicted solutions, we would like to point out that the prediction accuracy doesn\u2019t strongly correlate with the performance of the downstream task where the predictions are used (in this paper, the downstream task is the search phase). The ML model is trained on multiple solution samples and when deployed in the search, we use only a part of the predictions controlled by the hyperparameters. Therefore, there is no standard way to quantify the accuracy of the ML predictions in this setting that captures the downstream performance.\n\n[Nair et al., 2020] Solving mixed integer programs using neural networks, Arxiv 2020.\n\n[Han et al., 2023] A GNN-guided predict-and-search framework for mixed-integer linear programming. ICLR 2023\n\n[Deza et al., 2023] Fast Matrix Multiplication Without Tears: A Constraint Programming Approach. CP 2023\n\n[Tian 2022] Understanding Deep Contrastive Learning via Coordinate-wise Optimization. NeurIPS 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367461894,
                "cdate": 1700367461894,
                "tmdate": 1700367461894,
                "mdate": 1700367461894,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IquAq42HFZ",
                "forum": "J2kRjUAOLh",
                "replyto": "Ui2FqW5xoH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2/Reviewer_fyvF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2/Reviewer_fyvF"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for giving the detailed response and new experimental results. However, some of my concerns remains. \n\n1. Regarding the novelty over CL-LNS, I think \"complementary\" is not very sufficient to justify the differences or novelty. Moreover, the authors claimed two novelties, the negative data collection method and new loss function, but they do not provide any ablation study to support their advantages.\n\n2. Regarding the prediction accuracy, I am not satisfied with the response. If the prediction has low impact on the downstream tasks, then why you need a prediction after all? If a poor prediction can also lead to a good final performance, then I suspect the meaning and usefulness of the ML part, and the performance improvement may come from tuning other hyperparameters. So accuracy is important, because it justifies your core contribution which is an ML component. Also, I do not agree with the last statement in response 5. Prediction accuracy is very easy to quantify, and we do not need to involve the downstream tasks here.\n\nI will increase my score, but still believe that this paper needs further improvement."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667810729,
                "cdate": 1700667810729,
                "tmdate": 1700667810729,
                "mdate": 1700667810729,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]