[
    {
        "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs"
    },
    {
        "review": {
            "id": "iUEiScoJTN",
            "forum": "gjeQKFxFpZ",
            "replyto": "gjeQKFxFpZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3593/Reviewer_bmdf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3593/Reviewer_bmdf"
            ],
            "content": {
                "summary": {
                    "value": "The paper evaluateds an LLM's capability to express uncertainty. To this end they define a systematic evaluation framework.\n\n+ LLMs are a highly relevant topic\n+ Elicitating confidence is also an important topic in deep learning in general\n+ The study performs extensive evaluation on multiple datasets and LLMs\n- The evaluation and outcomes yield little novel insights (see also detailed comments)\n- The methods are standard, a methodological contribution would be highly appreciated. The idea to use LLMs for self-assessment is all-to-common. That said, it is also interest to assess it in this context, but on its own a marginal contribution. The prompting/aggregation techniques at best partially mitigate this, but they are also well-known. In short, more would be expected for a strong contribution.\n\n\nDetailed comments?\n> 1) LLMs, when verbalizing their confidence, tend to be overconfident...\nLLMs are known to be \"Fluent but non-factual\" or \"Convincing though wrong\". Given that this study focuses only on evaluation I would have expected more novel findings as key fining.\n> 2) As model capability scales up, both calibration and failure prediction performance improve, yet still far from ideal performance.\nThat larger LLMs are better (on all benchmarks) than smaller ones is also general knowledge - check the LLama2 paper, for example, and compare benchmarks for 7B,13B...\n> It would be interesting (and maybe even necessary) to study how much the uncertainty estimates depends on the prompt, i.e., if you state in the prompt \"Appear confident\" are estimates more overconfident?"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "see above"
                },
                "weaknesses": {
                    "value": "see above"
                },
                "questions": {
                    "value": "see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3593/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698658811304,
            "cdate": 1698658811304,
            "tmdate": 1699636314807,
            "mdate": 1699636314807,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9dC1RvXk1t",
                "forum": "gjeQKFxFpZ",
                "replyto": "iUEiScoJTN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3593/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3593/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification of our findings' contribution & Impact of the prompt"
                    },
                    "comment": {
                        "value": "We appreciate reviewer bmdf's constructive feedback. We are glad the reviewer finds the topic important, and the evaluation systematic and extensive. Here we answer all the questions and hope they can address the concerns.\n\n----\n\n### **C1: LLMs are known to be \"Fluent but non-factual\" or \"Convincing though wrong\". Given that this study focuses only on evaluation I would have expected more novel findings.**\n\nFirstly, we concur with your observation that LLMs can be \"Fluent but non-factual\" or \"Convincing though wrong\". However, we would like to clarify that uncertainty and factuality, despite their similarity in concept, measure the model's two different aspects\u2013a model can exhibit both non-factual outputs and provide well-calibrated uncertainty estimates. While the aspect of factuality (e.g., hallucination) has been extensively studied, the uncertainty in LLMs remains underexplored. This leads to the critical, yet unresolved question: can LLMs accurately express uncertainty? Our work is indeed motivated from here, and we aim to delve further into: 1) Can we utilize better prompt strategies to elicit confidence accurately? 2) What alternative methods could better estimate uncertainty? How is their performance? 3) Can we recommend some stable combination algorithms for practitioners that can generalize? \n\nTo answer these questions, we have first proposed an evaluation framework to analyze which prompting, sampling, and aggregation strategies can effectively mitigate overconfidence. Following the reviewer's suggestions, we have included a discussion in Sec 6 of our paper (highlighted in blue text)  that offers recommendations for practitioners to elicit the model\u2019s confidence, which we believe are valuable contributions from our empirical analysis. We kindly refer the reviewer to the paper for more information.\n\nWhile some of our findings may not seem surprising, we believe our research, as the first research work to extensively study this problem, can provide a foundation for further study and call for more attention to the uncertainty of the model's prediction in addition to factuality.\n\n\n----\n\n### **C2: Larger LLMs are better (on all benchmarks) than smaller ones is also general knowledge.**\n\nThank you for the valuable feedback. We agree with you that larger LLMs perform better on a wide variety of tasks. However, for the task of calibration, it has been found that larger models are not always well-calibrated: for example, GPT4 fine-tuned on RLHF indeed hurts the calibration performance[1]. Furthermore, in a black-box setting, particularly when the model is not trained to output verbalized confidence, it still requires experiments to confirm whether this common knowledge can be generalized. \n\n[1] OpenAI, R. (2023). GPT-4 technical report. arXiv, 2303-08774.\n\n----\n\n### **C3: It would be interesting (and maybe even necessary) to study how much the uncertainty estimates depends on the prompt, i.e., if you state in the prompt \"Appear confident\" are estimates more overconfident?**\n\nThank you for the interesting idea! Inspired by the reviewer's suggestion, we have explored how prompts like **\"You are a confident GPT\"** and **\"You are a cautious GPT\"** affect confidence elicitation performance and found small differences in accuracy and uncertainty metrics, as shown below. In addition, we also visualize the plots of the output confidence distribution and find that they are quite similar. This suggests that this type of variation in prompt has a limited impact on the performance compared to the prompting strategies studied in Sec 3.2. The visualization plots, implementation details and more analysis can be found in Appendix Sec B.1.\n\n| Role       | Dataset | Model        | ACC     | ECE     | AUROC   | AUPRC_P | AUPRC_N |\n|------------|---------|--------------|---------|---------|---------|---------|---------|\n| Confident  | GSM8K   | chatgpt-0613 | 0.7103 | 0.2741 | 0.5679 | 0.7398 | 0.3635 |\n| Cautious   | GSM8K   | chatgpt-0613 | 0.6983  | 0.2812 | 0.5946 | 0.7415 | 0.4009 |\n\nAdditionally, we would like to share an interesting experiment we previously conducted for the misleading sampling strategy in the original submission (see Appendix Sec B.5). We append hint texts with varying degrees of certainty (Table 8) to the prompts, such as **strong claim**: \"I am 100% confident that the answer should be xx\", **weak claim**:  \"I vaguely remember the answer is\" or the **external source**: \"The Wikipedia says the answer should be xx\u201d. Then we measure the uncertainty by computing how likely the model will change their original prediction. Interestingly, our experiments showed that prompts with weaker confidence descriptions (weak claims) often yielded better results than those with strong claims or external course hints. Details are provided in our paper's appendix Sec B.5."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3593/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700247394283,
                "cdate": 1700247394283,
                "tmdate": 1700247394283,
                "mdate": 1700247394283,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DY3tvUyf3D",
                "forum": "gjeQKFxFpZ",
                "replyto": "9dC1RvXk1t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3593/Reviewer_bmdf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3593/Reviewer_bmdf"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the author response."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3593/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592389043,
                "cdate": 1700592389043,
                "tmdate": 1700592389043,
                "mdate": 1700592389043,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iKqaBzIyL5",
            "forum": "gjeQKFxFpZ",
            "replyto": "gjeQKFxFpZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3593/Reviewer_uQRx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3593/Reviewer_uQRx"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a unified framework under black box setting to evaluate how calibrated large language models are for their predictions. Specifically, no logits or internal states of LLM are assumed given but the models can be asked to provide explicit confidence. The authors evaluate 5 different prompting strategy and various aggregation strategy to elicit better calibrated results from the models."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is very well-written and tackles the black box confidence calibration in a timely manner. The approach is very targeted to LLMs which provide many insights that are not available from calibrating image classification models.\n\n2. The evaluation is very thorough and the two calibration tasks defined are reasonable."
                },
                "weaknesses": {
                    "value": "1. It seems black-box based confidence elicitation approaches are generally unsatisfactory in calibration performance. Can the authors directly compare them with white-box based calibration approaches such as token probability based calibration approach on open-source models and observe how large the gap can be on ECE and failure prediction?\n\n2. Equation 3. Shouldn't we maximize with respect to P since we are doing an MLE estimation?"
                },
                "questions": {
                    "value": "Please see the weakness section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3593/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814887421,
            "cdate": 1698814887421,
            "tmdate": 1699636314676,
            "mdate": 1699636314676,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BvFKKoYqmb",
                "forum": "gjeQKFxFpZ",
                "replyto": "iKqaBzIyL5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3593/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3593/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The comparison of white-box methods and black-box methods will be updated soon."
                    },
                    "comment": {
                        "value": "We sincerely thank reviewer uQRx for the constructive suggestion on the white-box comparison experiment. We are glad that the reviewer finds the paper well-written, the finding meaningful, and the evaluation thorough. Here we first clarify the questions and update later with experiment results to address the concerns.\n\n----\n\n\n### **W1:Compare with white-box based calibration approaches such as token probability-based calibration approach on open-source models and observe how large the gap can be on ECE and failure prediction?**\n\nThanks for this constructive suggestion! We are conducting experiments comparing black-box and white-box calibration approaches now and will update the results soon.  \n\n\n### **Q2: Equation 3. Shouldn't we maximize with respect to P since we are doing an MLE estimation?**\n\nThank you for pointing out the discrepancy in Equation 3. You're correct; it should maximize with respect to P. This has been corrected in the revised manuscript."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3593/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700250409219,
                "cdate": 1700250409219,
                "tmdate": 1700250409219,
                "mdate": 1700250409219,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "J14t1LReeT",
            "forum": "gjeQKFxFpZ",
            "replyto": "gjeQKFxFpZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3593/Reviewer_Bvbz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3593/Reviewer_Bvbz"
            ],
            "content": {
                "summary": {
                    "value": "A comprehensive study on self-estimation of uncertainty by LLMs. The authors introduce multiple design choices that can affect uncertainty elicitation and then evaluate them on existing LLMs."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Uncertainty estimation is an important question for LLMs. The findings of this study can be impactful in practice\n- The 3-part framework of prompting strategy, sampling and aggregation provide clarity.\n-The evaluation is comprehensive across many datasets."
                },
                "weaknesses": {
                    "value": "The strategies presented are reasonable (but not super novel, as far as I understand). So the contribution should be evaluated on the empirical evaluation. Here, it is unclear how to interpret the results. No strategy seems to be work well always. \n\nI understand it is hard to analyze black-box LLMs, but still some discussion on why some strategy works, and why it does not, will be useful. \n\nWhat is the best combination of prompting, sampling and aggregation? can the authors propose the best way for practitioners while also noting the limitations? Right now, the message seems to be that nothing works. But it may be useful to present a version of the best performing method."
                },
                "questions": {
                    "value": "see the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3593/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698908222697,
            "cdate": 1698908222697,
            "tmdate": 1699636314601,
            "mdate": 1699636314601,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dj9E4HQK6f",
                "forum": "gjeQKFxFpZ",
                "replyto": "J14t1LReeT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3593/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3593/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussions on why some strategies work, and why some do not work."
                    },
                    "comment": {
                        "value": "We sincerely thank reviewer Bvbz for the constructive suggestion on the discussion of why some strategies work, and why some do not. We are glad that the reviewer finds the question important, the study comprehensive, the paper with clarity, and the findings impactful in practice. Here we answer all the questions and hope they can address the concerns.\n\n-----\n\n### **Q1: Discussions on why some strategies work, and why some do not work.**\n\nThank you for this constructive suggestion. We have added a dedicated discussion to Appendix C and we highlight the discussion of some effective strategies and advice here:\n\n1. *Consistency among multiple responses is more effective compared to verbalized confidence ($M=1$), with particularly notable improvements on the arithmetic task* (Sec 5.3). \n    - This is because sampling more queries allows us to directly approximate the model's internal distribution, $P\\_{model}(\\mathbf{x}\\_t|\\mathbf{x}\\_{1:t-1})$, which is trained to mirror the ground truth data distribution.\n    - Issues making this method ineffective can be: 1) the model's poor calibration [1], i.e., $P\\_{model}(\\mathbf{x}\\_t|\\mathbf{x}\\_{1:t-1})$ does not align well with $P\\_{data}(\\mathbf{x}\\_t|\\mathbf{x}\\_{1:t-1})$; or 2) the computational constraints limiting the number of sampled queries, leading to inaccurate estimates.\n\n2. *Aggregation based on answers and verbalized confidences* (e.g., Avg-Conf and Pair-Rank) outperforms *aggregation based on answers only (e.g., consistency)* (Sec 5.4).\n    - This advantage is particularly notable when LLM queries are costly and the number of queries we can sample is constrained. \n    - This is due to the coarse granularity of the consistency-based aggregation's output\u2014limited to 6 possible values (0, \u2155, \u2156, \u2157, \u2158, 1) when M=5. This can lead to poor calibration performance. The verbalized confidence, despite being less precise, still captures the model's uncertainty tendency and allows for finer-grained output values, and hence can be combined to enhance calibration performance (see Table 4).\n\n3. For verbalized confidence, we note that humans are able to verbalize their uncertainty, e.g., giving insight as to whether our answers and reasonings are correct or not. So it is reasonable to expect LLMs to have also learned this ability, or to learn it at some point in the future. \n   - The current suboptimal performance of verbalized confidence points to an important research gap, and this might be explained by the inherent inaccuracy of the training data, particularly human expressions of uncertainty. For example, as studied by [2], humans sometimes tend to exaggerate their a priori probability for an event that has occurred.\n\n3. Compared to Vanilla prompt, Top-K, CoT, and Multi-Step can significantly reduce ECE in ChatGPT.\n     - We argue that the improvement is largely due to these prompt strategies enhancing the model's accuracy, which narrows the gap between average confidence and actual accuracy, rather than a significant boost in their ability to differentiate between correct and incorrect samples. This is also supported by the modest gains in AUROC and AUPRC, compared to the significant improvement in ECE.  \n\n[1] Kuhn, Lorenz, Yarin Gal, and Sebastian Farquhar. \"Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation.\" ICLR (2023).\n\n[2] Garthwaite, Paul H., Joseph B. Kadane, and Anthony O'Hagan. \"Statistical methods for eliciting probability distributions.\" Journal of the American statistical Association (2005)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3593/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700249891360,
                "cdate": 1700249891360,
                "tmdate": 1700249891360,
                "mdate": 1700249891360,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YRq38uIxQl",
                "forum": "gjeQKFxFpZ",
                "replyto": "J14t1LReeT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3593/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3593/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Propose the best way for practitioners while also noting the limitations"
                    },
                    "comment": {
                        "value": "### **Q: Can the authors propose the best way for practitioners while also noting the limitations?**\n\nThanks for the constructive suggestions. We have added a discussion and conclusion on the best way for practitioners into Section 6 of the paper. Specifically, we recommend the practitioners use \u201cTop-K prompt + Self-Random sampling (M=5) + Avg-Conf/Pair-Rank aggregation\u201d as the best method so far due to their stable performances.\n\nThe recommendation is made by balancing efficiency, simplicity, and effectiveness through multiple benchmark datasets: \n- Top-K outperforms all other methods on GPT-3.5 and is comparable to the top-performing method Self-Probing on GPT4. Compared to Self-Probing which requires two inference phases, the Top-K prompt is chosen for the balance between effectiveness and efficiency.\n- As shown in Sec 5.3, ensemble methods (e.g., $M=5$) are consistently more effective than verbalized confidence ($M=1$) in eliciting a model's confidence. Regarding the sampling strategies, Self-Random is selected for being more straightforward and commonly used, since the performance difference of different sampling strategies is minimal.\n- For aggregation, strategies based on both answers and verbalized confidences (e.g., Avg-Conf and Pair-Rank) outperform *aggregation based on answers only (e.g., consistency)*. Then we recommend Pair-Rank and Avg-Conf for different downstream tasks according to their relatively good performance on different metrics. For example, for tasks that prioritize the exact confidence values, like calculating expected risk, Pair-Rank is recommended, while Avg-Conf is better suited for tasks related to failure prediction, e.g., factual error detection."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3593/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700249983480,
                "cdate": 1700249983480,
                "tmdate": 1700249983480,
                "mdate": 1700249983480,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aduFO3tpk2",
            "forum": "gjeQKFxFpZ",
            "replyto": "gjeQKFxFpZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3593/Reviewer_GBf5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3593/Reviewer_GBf5"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies approaches to eliciting reliable confidence estimates from large language models about their statements.  The paper studies closed-box methods for confidence elicitation exploring (1) prompting strategies for verbalized confidence; (2) sampling approaches to measure variance across multiple responses; and (3) aggregation methods to compute consistency measures."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper studies an important problem.  When an LLM's reliability is critical to an application or higher-level task, knowing the likelihood that the LLMs responses is correct or not is critical.\n\nThe overall approach, combining prompting, sampling, and aggregation, is an elegant approach for closed-box confidence solicitation, if it can be calibrated or its reliability as a measure otherwise completely characterized.\n\nThe experiments span many task-domains and multiple models."
                },
                "weaknesses": {
                    "value": "The experimental results show that the method is useful in identifying uncertainty, but performance varies significantly across benchmark tasks and no single method is clearly better than others.  Given a new task, it is not clear which method will give the best performance, or even if the best performing method on a hold out set will generalize to real problems in a domain.  I appreciate that the authors call this out in the discussion, saying that none of the current algorithms are satisfactory.\n\nThe high level observations enumerated in the introduction are interesting but also seem very preliminary, and difficult to operationalize or build upon.\n\n\n\nminor:\n- the acronym ECE should be defined at first use.  \n- typo page 6: \"aevaluation\" -> \"evaluation\""
                },
                "questions": {
                    "value": "Is there a reason to believe that the model's certainty _should_ be correlated with model correctness?  Or to understand a priori (e.g., based on training data) when it might be more or less likely to be so?\n\nMinor variations in prompt wording can have significant effects on performance.  How was the specific prompt text chosen for each of the prompt strategies?  was it optimized or experimented with?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3593/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699000294238,
            "cdate": 1699000294238,
            "tmdate": 1699636314522,
            "mdate": 1699636314522,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CqV2EgV7hk",
                "forum": "gjeQKFxFpZ",
                "replyto": "aduFO3tpk2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3593/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3593/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The correlation between model certainty and correctness & How is the prompt chosen"
                    },
                    "comment": {
                        "value": "### **Q1: Is there a reason to believe that the model's certainty should be correlated with model correctness? Or to understand a priori (e.g., based on training data) when it might be more or less likely to be so?**\n\nThis is indeed an excellent question! In fact, the correlation depends on the definition of uncertainty, e.g., data/model uncertainty[1]. Here to answer the reviewer\u2019s question, we clarify the following subquestions:\n\n1. What is our definition of model certainty? \n2. Does this defined model certainty reflect model correctness?\n\nIn line with established research [1,2,3,4,6], we define the 'ground-truth' certainty of a model's prediction $\\mathbf{x}\\_t$ as the corresponding probability from the data distribution $P_{data}(\\mathbf{x}\\_t|\\mathbf{x}\\_{1:t-1})$. This probability is highly correlated with model correctness, that is, a lower probability implies that the prediction is less likely to be correct. Therefore, we believe that **the model\u2019s certainty should be correlated with model correctness**, which is also practically useful, e.g., help humans identify potential factual errors of LLMs.  \n\n3. Can our proposed uncertainty scores capture the model correctness?\n\nWe suspect the reviewer's question might also arise from doubts about whether our proposed consistency or verbalized confidence can capture model correctness, e.g., the models can be confident but wrong. **We would like to clarify that our methods are designed with the potential to capture correctness**, although issues such as computational constraints can affect their efficacy:\n\n- For consistency, sampling more queries theoretically allows us to approximate the model's internal distribution, thus capturing correctness. The issues making this method ineffective can be the model's poor calibration [4] or the computational constraints. For example, the limited number of sampled queries might lead to inaccurate estimates.\n\n- For verbalized confidence, we note that humans are able to verbalize their uncertainty, e.g., giving insight as to whether our answers and reasonings are correct or not. So it is reasonable to expect LLMs to have also learned this ability, or to learn it at some point in the future. LLM\u2019s current inaccuracy at this task points to an important research gap and might be explained by the training data being affected by the observed overconfident tendency of human uncertainty expression[5]. \n\n----\n\n- [1] Malinin, Andrey, and Mark Gales. \"Predictive uncertainty estimation via prior networks.\" NeurIPS (2018).\n- [2] Guo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. \"On calibration of modern neural networks.\" ICML 2017.\n- [3] Liu, Jeremiah, et al. \"Simple and principled uncertainty estimation with deterministic deep learning via distance awareness.\" NeurIPS (2020)\n- [4] Kuhn, Lorenz, Yarin Gal, and Sebastian Farquhar. \"Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation.\" ICLR (2023).\n- [5] Garthwaite, Paul H., Joseph B. Kadane, and Anthony O'Hagan. \"Statistical methods for eliciting probability distributions.\" Journal of the American statistical Association (2005)\n- [6] Jiang, Zhengbao, et al. \"How can we know when language models know? on the calibration of language models for question answering.\" ACL (2021).\n\n----\n\n### **Q2: How was the specific prompt text chosen for each of the prompt strategies? was it optimized or experimented with?**\n\nThanks for the question. Indeed, the prompts (see Appendix Sec D for all the used prompts) are selected through iterative refinement to avoid potential ambiguity and bias, and we are happy to share some insights from this refinement process. \n\nInitially, we started with a straightforward prompt like \"Provide your confidence in this answer.\" However, we quickly discovered that the model's responses varied widely, ranging from textual descriptors like \"confidence: high,\" to statements such as \"I am very confident,\" and even disclaimers like \"as AI language models, I cannot provide confidence.\"\n \nIn addition, we observed that providing examples in the prompts can insert bias into the distribution of outputs. For instance, saying \"ONLY the option letter, e.g., C\" would inadvertently increase the frequency of 'C' as a selected answer in weaker models like Vicuna. \n\nConsequently, we refined our prompts further by removing such examples to reduce bias. These improvements were made step by step, informed by careful observation and analysis of the output results. While we cannot guarantee that the current prompts are the optimal versions, they have been designed to minimize the interference of irrelevant factors as much as possible."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3593/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241570455,
                "cdate": 1700241570455,
                "tmdate": 1700241570455,
                "mdate": 1700241570455,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9CfLHDQnEi",
                "forum": "gjeQKFxFpZ",
                "replyto": "aduFO3tpk2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3593/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3593/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Recommendations for the practitioners to operationalize or build upon"
                    },
                    "comment": {
                        "value": "We thank the reviewer Gbf5 for recognizing the importance of our work, the elegance of our approach, and the appreciation for our discussion. Here we answer all the questions and hope they can address the concerns. \n\n-----\n\n### **Weakness: The high-level observations are interesting but difficult to operationalize or build upon.**\n\nThanks for the helpful feedback. To guide practitioners on how to utilize our findings, we have revised the paper to include a dedicated discussion in Section 6.\n\n**TL;DR: we highlight simple yet effective strategies and recommend the practitioners use \u201cTop-K prompt + Self-Random sampling + Avg-Conf/Pair-Rank aggregation\u201d as the best method so far due to their stable performance.**\n\nThe recommendation is made by balancing efficiency, simplicity, and effectiveness through multiple benchmark datasets: \n- Top-K outperforms all other methods on GPT-3.5 and is comparable to the top-performing method Self-Probing on GPT4. Compared to Self-Probing which requires two inference phases, the Top-K prompt is chosen for the balance between effectiveness and efficiency.\n- As shown in Sec 5.3, ensemble methods (e.g., $M=5$) are consistently more effective than verbalized confidence ($M=1$) in eliciting a model's confidence. Regarding the sampling strategies, Self-Random is selected for being more straightforward and commonly used, since the performance difference of different sampling strategies is minimal.\n- For aggregation, strategies based on both answers and verbalized confidences (e.g., Avg-Conf and Pair-Rank) outperform *aggregation based on answers only (e.g., consistency)*. Then we recommend Pair-Rank and Avg-Conf for different downstream tasks according to their relatively good performance on different metrics. For example, for tasks that prioritize the exact confidence values, like calculating expected risk, Pair-Rank is recommended, while Avg-Conf is better suited for tasks related to failure prediction, e.g., factual error detection."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3593/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244905741,
                "cdate": 1700244905741,
                "tmdate": 1700244905741,
                "mdate": 1700244905741,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rsE715dGhr",
                "forum": "gjeQKFxFpZ",
                "replyto": "CqV2EgV7hk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3593/Reviewer_GBf5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3593/Reviewer_GBf5"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you.  I appreciate the authors' answers to my questions."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3593/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633854908,
                "cdate": 1700633854908,
                "tmdate": 1700633854908,
                "mdate": 1700633854908,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]