[
    {
        "title": "Prometheus: Inducing Evaluation Capability in Language Models"
    },
    {
        "review": {
            "id": "OSSEF7fGgI",
            "forum": "8euJaTveKw",
            "replyto": "8euJaTveKw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7178/Reviewer_WpqT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7178/Reviewer_WpqT"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces PROMETHEUS, an open-source large language model (LLM) that aims to provide evaluation capabilities on par with the proprietary GPT-4. To achieve this, the authors create a new dataset called FEEDBACK COLLECTION, containing diverse and fine-grained user assessment criteria. PROMETHEUS is trained using this dataset and demonstrates a strong correlation with GPT-4's evaluation capabilities, as well as human evaluators.\n\nThis paper addresses the limitations of using proprietary LLMs like GPT-4 for evaluation, such as closed-source nature, uncontrolled versioning, and prohibitive costs. The PROMETHEUS aims to offer an alternative that is open-source, reproducible, and cost-effective. The FEEDBACK COLLECTION dataset allows the model to generalize to various evaluation preferences and real-world scenarios. In tests, PROMETHEUS outperforms other baselines and shows potential as a universal reward model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The organization of this paper is well-structured, making it easy to read and comprehend.\n2. This work presents the creation of a FEEDBACK COLLECTION dataset, which encompasses a diverse range of scoring criteria, reference answers, and feedback. Based on this, an evaluation LLM is trained for assessing the text generated by large language models.\n3. The analysis in this work is thorough, discussing the selection of base models, data construction, and demonstrating the importance of reference answers. This provides valuable insights for the evaluation of large models in the field."
                },
                "weaknesses": {
                    "value": "1. The main contribution of this paper is the FEEDBACK COLLECTION dataset. However, the dataset has not been made publicly available, and the construction details are unclear. For instance, the content of Step 2 is incomplete, and Step 3 is overly simplistic. Furthermore, the prompts used during construction have not been disclosed.\n2. Assessing the consistency of scores alone is insufficient; it is also necessary to evaluate the feedback corresponding to these scores. On one hand, it is important to determine whether the feedback aligns with the scoring criteria. On the other hand, it should be examined if the feedback can be appropriately matched with the given scores. In fact, humans are not solely interested in obtaining a score; they are more concerned with the feedback associated with that score, which can further guide the large language model to generate desired answers.\n3. It is unclear whether the test data in Table 1 is manually constructed or generated by GPT4-0613. If it is generated by GPT4-0613, why are the Pearson/Kendall/Spearman evaluation metrics not equal to 1?\n4. For the Unseen FEEDBACK COLLECTION Testset, should all unseen instances be extracted? If the evaluation is conducted by combining the 50 Unseen samples with the 1000 samples similar to the training distribution, would this overshadow the true performance when facing unseen distribution during training?"
                },
                "questions": {
                    "value": "see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7178/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835981372,
            "cdate": 1698835981372,
            "tmdate": 1699636851479,
            "mdate": 1699636851479,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uIuHr4xx5N",
                "forum": "8euJaTveKw",
                "replyto": "OSSEF7fGgI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Dataset Publicity Issues, Construction Details of the Feedback Collection and Detailed Analysis of the Language Feedback"
                    },
                    "comment": {
                        "value": "Dear reviewer WpqT,\n\nWe appreciate your comments and review of the paper. \n\n(W1) - (W4) is our response to the \u201cWeaknesses\u201d. Please note that we have updated our draft of the paper, so please refer to the index of Tables and Figures in the updated version.\n\n---\n\n### **(W1) Dataset Publicity Issues and Construction Details of the Feedback Collection**\n\n#### [**Dataset Publicity Issues**]\nRegarding our dataset, we couldn\u2019t upload our file due to its large size (OpenReview has a 100MB limit) and we didn\u2019t include the link due to the anonymous policy (it is currently open-source at a public website). We will include the link in our camera-ready version. Also, we have uploaded our anonymized version of the code as supplementary material. \n\n#### [**Details for Dataset Construction Process**]\n\nHere is a detailed explanation of the dataset construction process in the updated draft (Section 3.1) :\n\nThe collection process consists of (1) the curation of 50 initial seed rubrics, (2) the expansion of 1K new score rubrics through GPT-4, (3) the augmentation of realistic instructions, and (4) the augmentation of the remaining components in the training instances (i.e. responses including the reference answers, feedback, and scores). Figure 6 shows the overall augmentation process.\n\n* Step 1: Creation of the Seed Rubrics We begin with the creation of a foundational seed dataset\nof scoring rubrics. Each author curates a detailed and fine-grained scoring rubric that each personnel considers pivotal in evaluating outputs from LLMs. This results in an initial batch of 50 seed rubrics.\n\n* Step 2: Augmenting the Seed Rubrics with GPT-4 Using GPT-4 and our initial seed rubrics, we\nexpand the score rubrics from the initial 50 to a more robust and diverse set of 1000 score rubrics. Specifically, by sampling 4 random score rubrics from the initial seed, we use them as demonstrations for in-context learning (ICL), and prompt GPT-4 to brainstorm a new novel score rubric. Also, we prompt GPT-4 to paraphrase the newly generated rubrics in order to ensure PROMETHEUS could generalize to the similar score rubric that uses different words. We iterate the brainstorming \u2192 paraphrasing process for 10 rounds. The detailed prompt used for this procedure is in Appendix J.\n\n* Step 3: Crafting Novel Instructions related to the Score Rubrics With a comprehensive dataset of 1000 rubrics at our disposal, the subsequent challenge was to craft pertinent training instances. For example, a score rubric asking \u201cIs it formal enough to send to my boss\u201d is not related to a math problem. Considering the need for a set of instructions closely related to the score rubrics, we prompt GPT-4 to generate 20K unique instructions that are highly relevant to the given score rubric.\n\n* Step 4: Crafting Training Instances Lastly, we sequentially generate a response to evaluate and corresponding feedback by prompting GPT-4 to generate each component that will get a score of i (1 \u2264 i \u2264 5). This leads to 20 instructions for each score rubric, and 5 responses & feedback for each instruction. To eliminate the effect of decision bias when fine-tuning our evaluator LM, we generate an equal number of 20K responses for each score. Note that for the response with a score of 5, we generated two distinctive responses so we could use one of them as an input (reference answer).\n\nFor the prompts used for each step, please refer to Appendix J in the updated draft. We hope the updated version will waive your concern regarding the lack of details.\n\n---\n\n### **(W2) Detailed Analysis of the Feedback**\n\nWe strongly agree that language feedback often conveys richer information compared to just looking at the scoring decision for evaluation. We have included the results regarding the examination of the feedback quality from Prometheus and GPT-4, which is shown in Figure 5.\n\nWhen human annotators assessed the failure cases of Prometheus and GPT-4, `the **semantic inconsistencies** between the **\u201cscore decision & feedback\u201d** (GPT-4: 2.00%, Prometheus: 2.86%) or the **\u201cscore rubric & feedback\u201d** (GPT-4: 8.00%, Prometheus: 5.71%) wasn\u2019t the main reason.\n\nWe are thrilled towards the direction of extracting meaningful insights from the language feedback which could be explored in future work. In this aspect, we found that existing open-source models such as Llama-2-Chat (70B) often generate abstract feedback that is meaningless and GPT-4 might be too expensive to use as it took $8,000 to construct the Feedback Collection. \n\nWe hope Prometheus could be used as a reliable and inexpensive source for further research in this direction."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699715314915,
                "cdate": 1699715314915,
                "tmdate": 1699940085856,
                "mdate": 1699940085856,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0U8KiCwQwC",
            "forum": "8euJaTveKw",
            "replyto": "8euJaTveKw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7178/Reviewer_LTFo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7178/Reviewer_LTFo"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents Prometheus, an open-source language model that provides fine-grained evaluation capabilities comparable to GPT-4. The authors aim to overcome the challenges of using GPT-4 as an evaluator, such as its closed-source nature, uncontrolled versioning, and high cost. Prometheus is trained on a new dataset, the Feedback Collection, which includes a wide range of user-based evaluation criteria. The model shows strong correlation with GPT-4 evaluation on seven benchmarks and outperforms ChatGPT in human evaluation. Remarkably, Prometheus demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Prometheus can assess responses based on novel and unseen score rubrics and reference materials provided by the user. This flexibility makes it applicable to a variety of real-world criteria.\n2. Prometheus can be freely used and further enhanced by the academic community, facilitating transparency and reproducibility.\n3. Prometheus shows remarkable performance in comparison with GPT-4 in terms of evaluation capabilities and the quality of generated feedback.\n4. The creation of the Feedback Collection, a dataset designed specifically for the task of teaching fine-grained evaluation to language models, is a significant contribution."
                },
                "weaknesses": {
                    "value": "1. One of my concerns about this work is whether can Prometheus be generalized to other fields since the downstream benchmarks are close the the training data. More results on unseen data and more specific domains can better improve this work. \n\n2. Potential bias of Prometheus. Can Prometheus be attacked by some adversarial attack methods? Does it have stronger biases like length bias compared with GPT-4?\n\n3. Dependency on GPT-4 Feedback: The training of Prometheus relies heavily on feedback generated by GPT-4. The model's ability to generalize beyond the feedback patterns of GPT-4 is unclear."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7178/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7178/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7178/Reviewer_LTFo"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7178/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698842866815,
            "cdate": 1698842866815,
            "tmdate": 1699636851369,
            "mdate": 1699636851369,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BHWpc9eXUt",
                "forum": "8euJaTveKw",
                "replyto": "0U8KiCwQwC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Generalization to Other Fields"
                    },
                    "comment": {
                        "value": "Dear reviewer LTFo,\n\nWe appreciate your comments and review for the paper.\n\n(W1) - (W3) is our response to the \u201cWeaknesses\u201d. Please note that we have made significant updates to the organization of the paper (content is same), so please refer to the index of Tables and Figures from the updated version.\n\n---\n\n### **(W1) Generalization to Other Fields**\n\n#### [**Difference Between the Feedback Collection and the Datasets used for Evaluation**]\nWe would first like to emphasize that the datasets used for experiments in Table 3 (Vicuna Bench, MT Bench, Flask) contains instructions that have different characteristics with the training dataset we used (Feedback Collection).\n\nSpecifically, the dataset in Table 4 have the following characteristics:\n\n* **Vicuna Bench**:Relatively short instructions that are mostly consisted of How/What type of questions in addition to some coding and math instructions. (e.g., \"How can governments utilize fiscal and monetary policies to combat economic recessions?)\n\n* **MT Bench**: The 2nd instruction we used in this multi-turn dataset mostly asks for revision or further explanation of the previous response. (e.g., \u201cNow, do the same task again but only use four-word sentences.\u201d)\n\n* **Flask Eval**: Contains academic NLP tasks such as classification datasets (e.g., MMLU, FEVER) or the train split of instruction datasets (e.g., Self-Instruct, WizardLM) (e.g., \"Paraphrase the given text in an academic style. Input: Lots of papers have been published on this topic.\u201d)\n\nIn contrast, the **Feedback Collection** was constructed with the main consideration of a very detailed, realistic situation where a user is interacting with an AI. (e.g., \u201cSuppose there is a friend who is feeling low due to a poor performance on a recent exam. The friend is now seeking advice, encouragement, and a bit of humor to lighten the mood. How would one approach this situation, incorporating wit and a playful tone to uplift the friend's spirit?\u201d)\n\nThe point we would like to emphasize is that Prometheus shows a higher correlation with both human evaluators (Figure 3) and GPT-4 (Table 3) in all of these different datasets. This supports that Prometheus could generalize to other instruction evaluation settings.\n\n#### [**Prometheus also works in Coarse-grained Relative Evaluation Setting although it was trained for Fine-grained Aboslute Evaluation**]\n\nAlso, we would like to emphasize that the human preference dataset experiments in Table 4 (HHH Alignment, MT Bench Human Judgment) is evidence indicating the generalizability of Prometheus.\n\nSpecifically, we would like to emphasize the following two points:\n\n* Results show that Prometheus could **generalize to coarse-grained criteria** (Helpfulness, Harmlessness, Honesty) although it was trained with detailed and fine-grained criteria (Cultural Sensitivity, Humorous, Considering regulation and compliance requirements).\n\n* Results show that while Prometheus was trained in an absolute scoring setting, its evaluation capabilities could also be **transferred to a relative scoring setting**. Prometheus outperforms open-source reward models that were specifically trained with human preference datasets (StanfordNLP Reward Model, ALMOST) and even GPT-3.5-Turbo.  Although it doesn\u2019t get access to both responses, it could manage to make a score decision that gives a higher score to human-preferred responses. Without any generalization, this would be extremely hard.\n\n---"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699713235964,
                "cdate": 1699713235964,
                "tmdate": 1699713980860,
                "mdate": 1699713980860,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YoOts4ycFO",
                "forum": "8euJaTveKw",
                "replyto": "0U8KiCwQwC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Potential Bias of Prometheus and Dependency on GPT-4 Feedback"
                    },
                    "comment": {
                        "value": "---\n\n### **(W2) Potential bias of Prometheus**\n\n#### [**Absolute Scoring is less vulnerable to Length Bias**]\nWe strongly agree that your concern regarding various biases that could occur during evaluation is very crucial in the field. As you specifically mentioned about **length bias**, we would like to discuss more about it in detail.\n\nWe would like to highlight that in this aspect, evaluation based on **Absolute Scoring holds a strong advantage over Relative Scoring** since the evaluator LM doesn\u2019t get vulnerable to the length differences or other surface patterns between the two responses during evaluation. \n\nIn Figures 13,14 and 15, we have measured whether Prometheus or GPT-4 gave a higher score to longer responses in an absolute evaluation setting and found that it is not the case. The responses that got a score of 1 - 5 all have similar score distributions, supporting the idea that Absolute Scoring might be a good way to mitigate length biases.\n\n#### [**Additional Experiment on Adversarial Dataset**]\n\nTo further analyze other datasets, we have conducted experiments with the recently proposed LLMBar dataset [1]. This dataset has an adversarial subset where the better response is shorter and the worse response is longer. With the same setting as in our human preference datasets in Table 4, we have conducted additional experiments and obtained the following results:\n\n|          | Random Guess | Llama-2-Chat (70B) | GPT-3.5-Turbo-0613 | Prometheus-13B | GPT-4-0613 |\n|----------|--------------|--------------------|--------------------|----------------|------------|\n| Accuracy | 48.91        | 35.87              | 44.57              | 63.04          | 75.00      |\n\nThis result supports that Prometheus is relatively robust to adversarial patterns such as length bias compared to other baselines such as Llama-2-Chat (70B) and GPT-3.5-Turbo-0613.\n\n---\n\n### **(W3) Dependency on GPT-4 feedback**\n\n#### [**Justification on the reliance towards GPT-4**]\n\nWe strongly agree that the concern you have raised is a very important point not only for our work but also for the overall community in general since a lot of recent work heavily relies on data augmented from GPT-4.\n\nNonetheless, augmentation based on GPT-4 has the following advantages relevant to the strengths you have mentioned:\n\n* GPT-4 shows remarkable performance in terms of Evaluation, and previous work [2,3,4] has also highlighted that GPT-4 could closely emulate human evaluators.\n\n* For performing fine-grained evaluation with detailed criteria, the only source we could rely on was either GPT-4 or human evaluators since other LLMs could not flexibly ground on the given score rubric. While we approximately spent $8,000 to create the Feedback Collection, collecting the same amount with only humans will cost at least 10x, especially because the level of expertise required for such annotations is very high.\n\n* While our construction process heavily relies on GPT-4, it also opens the door for other researchers to build upon our work and catch up since they could fully utilize the open-source dataset and model instead of paying another $8,000 through OpenAI API access.\n\nOn the other hand, we think that exploring different strategies such as mixing with human feedback or other types of model feedback (when it exists) is a worthwhile future work to explore.\n\n#### [**Difference of the Language Feedback Pattern between Prometheus and GPT-4**]\n\nLastly, we would like to highlight that although Prometheus was trained on data from GPT-4, it shows quite a different feedback pattern compared to that of GPT-4.\n\nIn Figure 5, when inspecting the quality of the feedback, we observed that human annotators determined that GPT-4 generated relatively abstract and generic feedback, while Prometheus generated either too critical or too optimistic feedback.\n\nHence, even though the training data was generated by GPT-4, the results show that they can behave differently. This may not be a conclusive answer to your concern, but we hope it clears some of it.\n\n---\n\n### **References**\n\n[1] Zeng, Z., Yu, J., Gao, T., Meng, Y., Goyal, T. and Chen, D., 2023. Evaluating large language models at evaluating instruction following. arXiv preprint arXiv:2310.07641.\n\n[2] Zheng, L., Chiang, W.L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. and Zhang, H., 2023. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. arXiv preprint arXiv:2306.05685.\n\n[3] Dubois, Y., Li, X., Taori, R., Zhang, T., Gulrajani, I., Ba, J., Guestrin, C., Liang, P. and Hashimoto, T.B., 2023. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387.\n\n[4] Ye, S., Kim, D., Kim, S., Hwang, H., Kim, S., Jo, Y., Thorne, J., Kim, J. and Seo, M., 2023. Flask: Fine-grained language model evaluation based on alignment skill sets. arXiv preprint arXiv:2307.10928."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699713737388,
                "cdate": 1699713737388,
                "tmdate": 1699713875700,
                "mdate": 1699713875700,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nlrcKZ32Mv",
                "forum": "8euJaTveKw",
                "replyto": "ZJhpYcyfzw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7178/Reviewer_LTFo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7178/Reviewer_LTFo"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for your response and responding to the question.\n\nGood luck !!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631348613,
                "cdate": 1700631348613,
                "tmdate": 1700631348613,
                "mdate": 1700631348613,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U4HwynR7s1",
            "forum": "8euJaTveKw",
            "replyto": "8euJaTveKw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7178/Reviewer_B7Vr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7178/Reviewer_B7Vr"
            ],
            "content": {
                "summary": {
                    "value": "Paper presents a new benchmark for building evaluation systems with LLMs. Although the paper contribution is promising, there are some serious problems in the paper. Many of the figures are missing and unvisible. The paper contribution, whether this is a novel LLM, or a data set generated by gpt-4 is unclear. The model is advertised as open-source but how the data will be shared is unstated. If an LLM is built on this data, which is described as a 100K synthesized data set, how is it an 13B LM is unclear. Paper cannot be published in such state with so much missing information."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Proposes open-source LLM for evaluation"
                },
                "weaknesses": {
                    "value": "Model implementation is not described.\nExperimental methodology not clear or supported.\nMost figures missing.\nContribution too small (not any new data, model or any advertised contribution is clearly described).\nData is synthetic and not corrected by humans for any potential errors."
                },
                "questions": {
                    "value": "Where is Figure 2?\nWhere is Figure 4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7178/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698888977685,
            "cdate": 1698888977685,
            "tmdate": 1699636851267,
            "mdate": 1699636851267,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CeUcXO6424",
                "forum": "8euJaTveKw",
                "replyto": "U4HwynR7s1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Issues with the Figures"
                    },
                    "comment": {
                        "value": "Dear reviewer B7Vr,\nWe appreciate the review and comments on the paper.\n\nBefore addressing the other issues, I think the problem of the missing figure might be due to your browser.\nI had the same problem before when using Safari. The draft size is quite large and Safari doesn't seem to render it very well.\n\nCould you try another browser or environment to check if the problem holds?\nCurrently, Figure 2 and Figure 4 are included in the draft."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699684055050,
                "cdate": 1699684055050,
                "tmdate": 1699688419466,
                "mdate": 1699688419466,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6fIykQNAtA",
                "forum": "8euJaTveKw",
                "replyto": "U4HwynR7s1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification of the Implementation, Experimental Methodology and Contribution"
                    },
                    "comment": {
                        "value": "Dear reviewer B7Vr,\n\nWe appreciate your comments and review for the paper.\n\n(W1) - (W4) are our responses to the weaknesses you have mentioned. Please note that we have made significant updates to the organization of the paper (content is same), so please refer to the index of Tables and Figures from the updated version.\n\n---\n\n### **(W1) Details of Model Implementation**\n\nOur explanation of the model implementation is included in Section 3.2 and Appendix F. Using the llama-recipes repository, we have fine-tuned Llama-2-Chat (7B & 13B) to obtain Prometheus using 8 A100 (80GB) GPUs with FSDP. The training is similar to Chain-of-Thought Fine-tuning, where we fine-tune to sequentially generate the feedback and the the score.\n\nThe hyperparameters we used during training and inference are included in Table 8 and 9.\n\n---\n\n### **(W2) Explanation of the Experimental Methodology**\n\nSince our main objective is to obtain a open-source evaluator language model that could closely emulate GPT-4 and human evaluators, we mainly divide our experiments into two different phases. Further details are included in Appendix B.\n\n* We first check how the score is similar with either human evaluators (as shown in Figure 3) and GPT-4 (as shown in Table 2 and 3). Specifically, after parsing the score decision, we compare with Pearson, Spearman, and Kendall-Tau correlation with the opponent.\n* Next we check how the other component that Prometheus generated, which is the language feedback has good quality. This is shown in Figure 4 along with an analysis in Figure 5. Specifically, we ask human evaluators to compare the language feedback and choose among Prometheus and GPT-4. \n\n---\n\n### **(W3) Explanation of our Contribution**\n\nThe main contributions of our work are as follows:\n\n* We introduce the FEEDBACK COLLECTION dataset specifically designed to train an evaluator LM. Compared to previous feedback datasets, it includes customized scoring rubrics and reference answers in addition to the instructions, responses, and feedback.\n* We train PROMETHEUS, the first open-source LLM specialized for fine-grained evaluation that can generalize to diverse, real-world scoring rubrics beyond a single-dimensional preference such as helpfulness and harmlessness.\n* We conduct extensive experiments showing that by appending reference materials (reference answers, fine-grained score rubrics) and fine-tuning on feedback, we can induce evaluation capability into language models. PROMETHEUS shows high correlation with human evaluation, GPT-4 evaluation in absolute scoring settings, and also shows high accuracy in\nranking scoring settings.\n\n---"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699720064325,
                "cdate": 1699720064325,
                "tmdate": 1699840187709,
                "mdate": 1699840187709,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]