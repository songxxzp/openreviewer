[
    {
        "title": "Compositional Interfaces for Compositional Generalization"
    },
    {
        "review": {
            "id": "Ri6JLItXln",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7644/Reviewer_B95a"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7644/Reviewer_B95a"
            ],
            "forum": "D1w3huGGpu",
            "replyto": "D1w3huGGpu",
            "content": {
                "summary": {
                    "value": "This paper targets on developing agents for compositional generalization to novel combinations of observation and action spaces by using COIN architecture."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The target of this paper on generalization (specifically compositional generalization) is a key area in machine learning.\n2. A modular approach to compositional combinations of observation and action spaces seems a good fit and appropriate approach.\n3. Experiments show improvement over various COIN baselines."
                },
                "weaknesses": {
                    "value": "1. This paper seems to be solely an application of COIN architecture in the setting of compositional combinations of observation and action spaces. It is not clear what novelty or technical contribution it has.\n2. The experiments are mainly on COIN architecture. No comparison with other approaches. It is hard to assess the effectiveness of this method."
                },
                "questions": {
                    "value": "1. Why is there no comparison with non-COIN approaches?\n2. What is the novelty of this paper? It seems to be simply an application of COIN architecture on compositional combinations of observation and action spaces."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7644/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697150322744,
            "cdate": 1697150322744,
            "tmdate": 1699636929520,
            "mdate": 1699636929520,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FytYlA8Fu3",
                "forum": "D1w3huGGpu",
                "replyto": "Ri6JLItXln",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7644/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the feedback, we are glad they found the problem relevant and our approach a good fit for the problem! In our response, we hope to clarify the key contributions of the paper as well as the choice of baselines.\n\n> This paper seems to be solely an application of COIN architecture in the setting of compositional combinations of observation and action spaces. It is not clear what novelty or technical contribution it has.\n\nThe primary contribution of the paper is the study of compositional generalization of multi-task multi-modal agents, specifically with respect to observation, action and instruction spaces. To the best of our knowledge, our work is the only such study to date. It is _not merely an application of a modular architecture, but rather a systematic evaluation of how much a modular architecture improves compositional generalization_ for such agents. We provided detailed experiments and analysis with respect to the number of training combinations, data size, and hold-out set difficulty. This form of compositional generalization is an important property of generalist agents: many real-world tasks  will involve training agents with a variety of sensors and actuators to accomplish a wide variety of objectives.\n\nThe remaining novel contributions of our work are as follows:\n- An environment that enables such study by: (a) allowing easy composition of arbitrary observation, action and instruction spaces, (b) supporting a large number of observation, action and instruction spaces, (c) providing an automated way to generate training data\n- An architecture that can handle learning in this setting through modular design, as well as incorporating new perception modalities without impacting performance on the tasks the agent was previously trained on.\n\n\n> The experiments are mainly on COIN architecture. No comparison with other approaches. It is hard to assess the effectiveness of this method.\n\nWith respect to the choice of baselines, the literature has no standard off-the-shelf approaches applicable to our setting beyond our comparisons to single-task models.  The closest possible approaches are tokenized transformers such as those used in GATO (which is not open-sourced). With such model, however, \"how to tokenize\" would become a non-trivial question beyond the scope of our work, even if GATO was open-sourced.\n\nHence the most fair comparison is with respect to agents trained in a single-task setting or the expert data (in our case, the experts always complete the task). Note that for the same reasons, GATO also does not compare their model to alternative multi-task multi-modal architectures.\n\n-----\n\nWe hope our response clarified the novelty of our contribution and the choice of baselines. If there are any remaining concerns, we are happy to discuss them here."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529457597,
                "cdate": 1700529457597,
                "tmdate": 1700529551014,
                "mdate": 1700529551014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eREfw2Tjwq",
                "forum": "D1w3huGGpu",
                "replyto": "FytYlA8Fu3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7644/Reviewer_B95a"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7644/Reviewer_B95a"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the authors' response."
                    },
                    "comment": {
                        "value": "Based on the authors' response, my rating remains the same."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678170304,
                "cdate": 1700678170304,
                "tmdate": 1700678170304,
                "mdate": 1700678170304,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GFgabLHSn0",
            "forum": "D1w3huGGpu",
            "replyto": "D1w3huGGpu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7644/Reviewer_Pm6p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7644/Reviewer_Pm6p"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to use end-to-end modular architectures to achieve compositional generalization to unseen combinations of observation\nand action spaces. It requires individual encoding and action modules for each observation and action space, with a single controller connecting them shared across different spaces. The paper constructs an synthetic environment with compositional structure and show through extensive experiments that the proposed method enables generalization to unseen combinations of observation and action spaces."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ Originality:\n\nThe paper proposes an effective design for compositional generalization of the agent's observation and action spaces. While modular architectures are well known in previous works, the presented work is novel in a sense that it is a successful demonstration that modular design can be effectively used for compositional generalization.\n\n+ Quality & Clarity:\n\nThe paper is well-written. Experiment designs are clean and comprehensive in general. The presentation is well-organized and easy to follow.\n\n+ Significance:\n\nThe presented work can be potentially beneficial for generalist agent design."
                },
                "weaknesses": {
                    "value": "- Significance:\n\nIt would be great if noisy real-world data or more complex visual data is included in the testing scenario. It is unclear for now whether this design is robust enough to deal with the not-that-clean data regime."
                },
                "questions": {
                    "value": "i) Could the authors provide insights or discussions about why the proposed method performs worse in the instruction space of `Sort by Property`? Is there a class of tasks that the proposed method fail to handle?\n\nii) For the `HARD HOLDOUTS` set-up, will the performance further improve if there are more easy combinations available for training? Are there any other possible solutions to address the easy-to-hard transfer problem?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed.",
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7644/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7644/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7644/Reviewer_Pm6p"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7644/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823931794,
            "cdate": 1698823931794,
            "tmdate": 1699636929415,
            "mdate": 1699636929415,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "clB011EjEQ",
                "forum": "D1w3huGGpu",
                "replyto": "GFgabLHSn0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7644/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are thankful to the reviewer for the positive feedback, we are glad they found the paper well-written and recognized the novelty of our work! In the following, we hope to address the reviewer's concern about complexity of the environment and answer their questions. \n\n> It would be great if noisy real-world data or more complex visual data is included in the testing scenario. It is unclear for now whether this design is robust enough to deal with the not-that-clean data regime. \n\nWhile we agree that results on noisier environments would be interesting, there were no such environments available that would allow us to study compositional generalization wrt observation, action and instruction spaces specifically. Moreover, in order to draw strong conclusions, it is often more useful to study the phenomena in a controlled, synthetic environment. By focusing on a simpler environment design, we were able to: \n(a) generate a very large number of environment combinations and individual spaces, \n(b) have a much better control of the factors of variation \u2013 in our case, we can only vary the observation modality, action space and instruction space \u2013 without introducing other confounding sources of variation that would be present in more complex environments.\n\n## Questions\n\n> i) Could the authors provide insights or discussions about why the proposed method performs worse in the instruction space of Sort by Property? Is there a class of tasks that the proposed method fail to handle?\n\nWhile we can not tell with certainty, we suspect the performance on \"Sort by Property\" instructions are lower because of how different optimal policies are for this task compared to others. This is the only instruction in which the goal state is defined by distance between particular items \u2013 a feature that may be harder to learn when there are multiple other objectives competing for representational space. \n\n> ii) For the HARD HOLDOUTS set-up, will the performance further improve if there are more easy combinations available for training? Are there any other possible solutions to address the easy-to-hard transfer problem?\n\nIn Figure 6, we already vary the number of easier combinations used in training and find that by increasing those leads to improvement in performance. Note that we report only zero-shot performance on hard combinations, and expect that fine-tuning on some data from hard holdouts would further improve the performance.  \n\n------\n\nWe thank you for the interesting questions! We hope our response has addressed the reviewer's concerns about the choice of environment.  If there are any further questions or concerns, we are happy to answer them here."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528985232,
                "cdate": 1700528985232,
                "tmdate": 1700528985232,
                "mdate": 1700528985232,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DmnpdVfeeB",
                "forum": "D1w3huGGpu",
                "replyto": "clB011EjEQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7644/Reviewer_Pm6p"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7644/Reviewer_Pm6p"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' point-to-point and detailed responses. I appreciate the efforts they made to address my concerns. For the HARD HOLDOUTS set-up, I was actually expecting more results other than those in Figure 6. Given the author responses and comments from other reviewers, I will keep my rating as it is. However, I will not feel uncomfortable if other reviewers think the contribution is limited due to lack of novelty and/or insufficient experiment evaluation."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681161574,
                "cdate": 1700681161574,
                "tmdate": 1700681161574,
                "mdate": 1700681161574,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VRog1Rkn4l",
            "forum": "D1w3huGGpu",
            "replyto": "D1w3huGGpu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7644/Reviewer_h2vj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7644/Reviewer_h2vj"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a dataset and a system for compositional generalization to novel observation spaces, action spaces, and tasks using end-to-end modular architectures. These architectures divide the task into specialized differentiable modules for encoding observations and predicting actions; and they are connected. The authors create a controlled environment for testing. Experiments show that the modular approach allows agents to generalize to unseen combinations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper presented an modular approach towards compositional generalization, which is an important field and is definitely relevant to the theme of the conference. The sections defining the model and describing the experiments are well-structured, effectively conveying the core concepts and findings of the paper."
                },
                "weaknesses": {
                    "value": "The main weakness of the paper is its limited contribution.\n\nOn the dataset aspect, it's a bit unclear what's new in this dataset. So the proposed dataset is based on a 2D grid world, with some 3D rendering. I do not see significant differences between this setting and Minigrid (https://github.com/Farama-Foundation/Minigrid), and many works have been built on Minigrid. For example, Minigrid also contains different observation spaces (symbolic vs. image), and there are also works that use text descriptions too. It also contains instruction-level compositionality. Another example in robotics is CompoSuite: A Compositional Reinforcement Learning Benchmark https://arxiv.org/pdf/2207.04136.pdf\n\nOn the model aspect, I don't see significant differences between this paper and other papers that use (most times pretrained) LLMs for decision-making. For example, https://arxiv.org/pdf/2202.01771.pdf they have studied different encodings of the input too. Its also similar to GATO, except that GATO does not use \"modules for encoding observations\". Another example in robotics is Modular Lifelong Reinforcement Learning via Neural Composition https://openreview.net/pdf?id=5XmLzdslFNN\n\nFindings. I think the findings of the paper are not completely new. Many aspects of it have been demonstrated in many works, mostly in more realistic settings such as robot manipulation. For example, GATO also demonstrated how it could learn new tasks faster based on other pretrained tasks. And again, the results here are only demonstrated in a very synthetic setting, it is very unclear how these findings can be generalized to realistic learning settings (e.g., multitask learning for robotics)."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7644/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7644/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7644/Reviewer_h2vj"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7644/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698890530216,
            "cdate": 1698890530216,
            "tmdate": 1701064543552,
            "mdate": 1701064543552,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4bJth7yBEo",
                "forum": "D1w3huGGpu",
                "replyto": "VRog1Rkn4l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7644/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are thankful to the reviewer for the positive comments, we are glad they found the topic relevant and the presentation clear! We however strongly disagree with the reviewer's claim about the lack of novelty in our work. As noted by the reviewer Pm6p, our work is novel in demonstrating that a modular architecture is an effective way to achieve compositional generalization wrt observation, action and instruction spaces; as well as for transfer from easy to hard combinations and to novel perceptual modalities. We also developed an environment that enables such study. In our response, we will clarify those contributions and key differences from prior work.\n\n## Environment\n\nNone of the existing environments, including the two the reviewer proposed (MiniGrid, CompoSuite), were suitable for studying the form of compositional generalization our work is primarily interested in, i.e. generalization to combinations of perception modalities, action and instruction spaces. \n\nThe primary component lacking in Minigrid is a variety of action spaces \u2013 by limiting ourselves to only one action space, we would be missing a key factor of variation required for our study. Furthermore, MiniGrid supports one type of image observation (2D image), whereas our environment can generate an arbitrary number of visual observations by projecting the 3D space differently (see Figure 2). An important practical consideration is that MiniGrid codebase is _not constructed to support easy mixing and matching_ of the different types of observations, actions and instructions. With the extensive changes required to support the type of work we were interested in, construction of an entirely new codebase was more feasible than an extension of the existing MiniGrid codebase.\n\nCompoSuite supports easy combining of the following 4 elements: robotic arm, object id, obstacle id and instruction (with 4 options for each). However, different modalities of observation spaces are notably missing in CompoSuite. This is an important consideration when the aim is to train agents that can ingest different modalities (such as text, images, actuator positions) that warrant the use of specialized architectures for representation. Moreover, the four elements in CompoSuite do not map clearly to variations in observation, action and instruction distinction, which are the focus of our paper. \n\nIn conclusion, the contributions in our environment are:\n1. easy specification of each env instance as an explicit combination of perception, action and instruction space,\n2. large number of perception modalities, action and instruction spaces,\n3. easy addition of new spaces,\n4. automated generation of expert data via A* solver. \n\n## Model \n\nWhile, as the reviewer notes, each of the elements of our architecture is not unique, the specific combination is uniquely well suited for the settings we are interested in. The architectures used in GATO and LID, while also relying on transformer architectures, do not use composable modules to capture different observation and action spaces. The GATO architecture (also used in multi-modal multi-task settings), relies on tokenization and serialization of observation inputs, which will have variable length and would not allow easy incorporation of new observation modalities without affecting the performance on the pretraining tasks.\n\nThe modules in (Mendez et al., 2022) specialize in _different subtasks_, instead of different observation and action modalities. Furthermore, in their work, the modules are searched over and composed in sequence to capture the subproblems in the overall task. \n\n## Findings\n\nTo the best of our knowledge, there is no other work that studies compositional generalization with respect to the three common and general factors of variation: observation, action and instruction spaces. While GATO does demonstrate transfer to new tasks, there are no experiments that would shed light on the compositional generalization wrt perception and action modalities. There are also no experiments demonstrating transfer specifically to new perception modalities. \n\nFurthermore, none of the experiments in GATO demonstrate that fine-tuning on the downstream tasks can be done without affecting performance on the remaining tasks, which is one of the demonstrated properties of our model. Finally, note that while large-scale experiments in realistic settings are often desirable, for the sake of drawing out clear empirical patterns (such as compositional generalization with respect to specific factors of variation), controlled experiments in synthetic domains are more useful.  \n\n-----\n\nIn conclusion, we argue that overall, the contribution and the insights gained are sufficiently different from prior work to warrant a significant contribution. These clarifications will be added to the updated version of the paper. We hope our response addressed your concerns, if there are any remaining concerns, we are happy to discuss them here."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528681180,
                "cdate": 1700528681180,
                "tmdate": 1700528744679,
                "mdate": 1700528744679,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qs88y1Xaef",
            "forum": "D1w3huGGpu",
            "replyto": "D1w3huGGpu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7644/Reviewer_n2i7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7644/Reviewer_n2i7"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Compositional Interfaces (COIN) architectures, the end-to-end modular architectures for compositional generalization to unseen combinations of observation and action spaces in embodied agent settings.\nDifferentiable modules in COIN handle observation encoding and action prediction.\nEach observation or action space has a module, but the controller is shared.\nThe environment with compositional structure is developed to investigate the architecture.\nAn environment instance is generated by combining observation, action, and instruction space.\nThe experiments show COIN enables compositional generalization and transfer learning. It generalizes to unseen combinations, and novel observation modalities can be quickly integrated."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The problem of compositional generalization for embodied agent settings is important.\n\n- The paper also developed a flexible compositional environment.\n\n- It uses end-to-end training and does not require a special training scheme.\n\n- The experiments support the ability of COIN for compositional generalization and transfer learning."
                },
                "weaknesses": {
                    "value": "There are concerns that the task and the architecture do not cover general cases of compositional generalization in embodied agent tasks.\n\n(1) **Disentangled inputs:**\nIt handles disentangled inputs since the observation, action, and instruction space are separately provided, but it does not address more general entangled inputs.\n\n(2) **Types of combinations:**\nThere can be more possible compositional generalization problems, such as novel combinations of shape and color in observation.\n\n(3) **Given component IDs:**\nThe IDs of observation and action spaces are provided for each sample to select modules. They can be hidden in more general cases.\n\nHere are some other concerns.\n\n(4) The agent is trained with imitation learning, while another widely used algorithm is reinforcement learning.\n\n(5) In the experiment, the environment is small and synthetic. It also lacks strong baselines of existing embodied agent algorithms."
                },
                "questions": {
                    "value": "Please refer to the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7644/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699278842707,
            "cdate": 1699278842707,
            "tmdate": 1699636929172,
            "mdate": 1699636929172,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aj0OpckPo9",
                "forum": "D1w3huGGpu",
                "replyto": "qs88y1Xaef",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7644/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback! \nWe'll address the reviewer's concern about the limited generality in the following.\n\n> (1) Disentangled inputs: It handles disentangled inputs since the observation, action, and instruction space are separately provided, but it does not address more general entangled inputs.\n\n> (3) Given component IDs: The IDs of observation and action spaces are provided for each sample to select modules. They can be hidden in more general cases.\n\nWe do only consider the case where the instruction and observations are provided separately, and the observation and action IDs are known. However, we do not think this is a significant limitation of our work in terms of its impact and contribution. Instruction and observation are typically provided together (in fact, this is a default assumption in the [instruction following](https://arxiv.org/abs/1906.03926) literature and goal-conditioned sequential decision making). Similarly, the observation and action IDs are often known \u2013 the action space typically needs to be known in order to enact the action in the environment, while the observation space can be determined from the format of the input. \n\n> (2) Types of combinations: There can be more possible compositional generalization problems, such as novel combinations of shape and color in observation.\n\nIn this work, we chose to specifically focus on compositional generalization with respect to combinations of observations, actions and instructions, as those are particularly relevant for generalist (multi-task, multi-modal) agents. As the reviewer suggests, there is lots of prior work on other kinds of compositional generalization -- that is good too!  We of course neither attempt nor claim to cover every possible instance of \"compositionality\"; similarly most other works on compositionality do not attempt all instances.  Even _discussing_ (much less algorithmically treating) all possible generalization problems is beyond the scope of a monograph, much less a single ICLR submission.\n\n> (4) The agent is trained with imitation learning, while another widely used algorithm is reinforcement learning.\n\nWhile adding comparison with other kinds of objectives would further strengthen our results, we argue that using only imitation learning is not a significant limitation of our work. Since the question of transfer and generalization is mostly related to the architecture and training regime, the results are also informative for the agents trained with reinforcement learning. The most related published works, such as [multi-game decision transformers](https://arxiv.org/abs/2205.15241) and [GATO](https://arxiv.org/abs/2205.06175), also train their agents with expert data only (either via offline RL or imitation learning). \n\n> (5) In the experiment, the environment is small and synthetic. \n\nWhile the larger scale and more realistic environments are often desirable, we argue that for the sake of drawing out clear and consistent empirical patterns, controlled environments such as the one developed in this paper are more useful. Such environment enabled us to: (a) generate a very large number of environment combinations and individual spaces, with which we can extract more signal from the noise, (b) have a much better control of the factors of variation \u2013 in our case, we can only vary the observation modality, action space and instruction space \u2013 without introducing other confounding sources of variation that would be present in more complex environments\n\n> It also lacks strong baselines of existing embodied agent algorithms.\n\nWith respect to the choice of baselines, the literature has no standard off-the-shelf approaches applicable to our setting beyond our comparisons to single-task models.  The closest possible approaches are tokenized transformers such as used in GATO (which is not open-sourced).  In particular, \"how to tokenize\" would become a non-trivial question beyond the scope of this work, even if GATO was open-sourced. \n\nHence the most fair comparison is with respect to agents trained in a single-task setting or to the performance in the expert data (in our case, the experts always complete the task). Note that for the same reasons, GATO also does not compare their model to alternative multi-task multi-modal architectures.\n\n-------\n\nWe hope our response addressed your concerns, if there are any remaining concerns, we are happy to discuss them here!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700522633050,
                "cdate": 1700522633050,
                "tmdate": 1700522633050,
                "mdate": 1700522633050,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]