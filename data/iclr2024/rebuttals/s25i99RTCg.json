[
    {
        "title": "Multi-modal Latent Diffusion"
    },
    {
        "review": {
            "id": "Odsu7QsPA5",
            "forum": "s25i99RTCg",
            "replyto": "s25i99RTCg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5378/Reviewer_SqcE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5378/Reviewer_SqcE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new approach to handle the issue, called the coherence-quality tradeoff, of the Multimodal VAE. Specifically, the authors use a set of independently trained, uni-modal, deterministic autoencoders. And, they introduce a multi-time training method to learn the conditional score network in the diffusion model, which enables multi-modal generative modeling."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper is clear and well-organized. \n2.\tThe appendix significantly enhances the paper by thoroughly supplementing the technical and experimental details."
                },
                "weaknesses": {
                    "value": "1.\tIt is a nice try to facilitate multi-modal generative modeling by concatenating latent variables from different modalities and employing a diffusion model. However, the methodology of the paper presents numerous issues and lacks in-depth discussion. Furthermore, the method proposed principally depends on intuitive reasoning, with a noticeable absence of solid theoretical underpinnings. \n    \n    - The paper employs diffusion on latent variables concatenated from different modalities. One concern is whether the data distributions corresponding to vectors from these various modalities significantly diverge. If so, does utilizing the same noise schedule could result in a lack of synchronization between the denoising and noise-adding processes across the different modalities? \n    - Additionally, the optimization speeds of different modalities may inherently vary, leading to discrepancies in performance. In other words, the proposed method might achieve satisfactory results with simplistic datasets, but training becomes substantially more challenging when scaled to extensive, real-world data scenarios.\n    - In the context of conditional generation, the authors employ a masking technique to generate the desired modality based on the known one. However, the question arises: how is the intensity of the conditions controlled? This aspect is crucial for ensuring the effectiveness of the generative process.\n    - Within the model's framework, a discrepancy arises between the training and inference stages in terms of the number of modalities. For instance, during training, the model might handle three modalities: A, B, and C. However, in a scenario where inference is desired based solely on modality A to predict B, would a masked C still be necessitated? This raises questions about the model's flexibility and its adaptability to accommodate various generative scenarios with different modalities. The capacity to dynamically adjust to these conditions without compromising the integrity of the generative process is pivotal.\n    \n2. Some recent work, such as MMVAE+(Palumbo et al., 2023), should be included as a baseline. Work parallel to this paper, score-based multimodal autoencoders (Wesego et al., 2023), should be discussed in Section 2.\n\n3. The experiments require further refinement. \n\n   - The quantitative comparisons on the CUB dataset should be integrated into the main text. Moreover, the coherence metric for image->caption has not improved, necessitating a comprehensive comparative analysis and case demonstrations of caption generation. Additionally, the visual representations in this section of the paper are quite unclear, making them difficult to interpret. \n   - Considering that the differences in certain metrics on the CUB dataset are not particularly pronounced, it is recommended to augment the study with a comparative analysis on the Bimodal CelebA dataset.\n   - The authors have not made their code available, which makes it hard to reproduce the experimental results.\n\n\n**Reference:**\n\n[1] Palumbo, Emanuele, Imant Daunhawer, and Julia E. Vogt. \"MMVAE+: Enhancing the generative quality of multimodal VAEs without compromises.\" Fifth Symposium on Advances in Approximate Bayesian Inference-Fast Track. 2023.\n\n[2] Wesego, Daniel, and Amirmohammad Rooshenas. \"Score-Based Multimodal Autoencoders.\" arXiv preprint arXiv:2305.15708 (2023)."
                },
                "questions": {
                    "value": "Please refer to the specifics outlined in the \u201cWeaknesses\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5378/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5378/Reviewer_SqcE",
                        "ICLR.cc/2024/Conference/Submission5378/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5378/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698387445011,
            "cdate": 1698387445011,
            "tmdate": 1700637090934,
            "mdate": 1700637090934,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bddHj6JHj1",
                "forum": "s25i99RTCg",
                "replyto": "Odsu7QsPA5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5378/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5378/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough review. We hope to resolve all your concerns with additional clarification and experimental results.\n\n> **It is a nice try to facilitate multi-modal generative modeling by concatenating latent variables from different modalities and employing a diffusion model. However, the methodology of the paper presents numerous issues and lacks in-depth discussion. Furthermore, the method proposed principally depends on intuitive reasoning, with a noticeable absence of solid theoretical underpinnings**\n\nWe respectfully disagree with the critique regarding the lack of rigor in the methodology of our paper. Our approach is grounded in more than just intuitive reasoning, please refer to Sections 3, 4 and Appendix A.2. In fact, feedback from other reviewers has pointed out that our work leans heavily towards mathematical rigor, potentially at the expense of intuitive understanding.\n\n\n**The paper employs diffusion on latent variables concatenated from different modalities. One concern is whether the data distributions corresponding to vectors from these various modalities significantly diverge. If so, does utilizing the same noise schedule could result in a lack of synchronization between the denoising and noise-adding processes across the different modalities?**\n\nWe can analyze the problem separating two different time regimes: large diffusion times, and small ones. \n\nLarge diffusion times **cannot** be the source of synchronization problems. Indeed it is widely known that under loose assumptions [Collet2008], the discrepancy (in KL sense) between time varying probability densities and their final (Gaussian in this case) steady state distributions decreases exponentially fast. In the language of our work: $KL(q(r^{A_1},t) | \\rho(r^{A_1})\\leq C\\exp(-\\lambda t)$, where the parameters $C,\\lambda>0$ are independent of time. This result holds for any starting distribution $q(r^{A_1},0)$ [Collet2008]. \n\nFor small diffusion times, the question of whether different modalities respond differently to noise injection is important. In Appendix A.2, \u201cLatent space robustness against diffusion perturbation\u201d of the submitted paper, we discuss in length about this matter (see also Figure 5). \n\nIt is important to notice that it is this very question, together with the information theoretic analysis presented in Section 4.2, that motivates the multi-time diffusion process as an alternative to the inpainting approach (which is based on a single diffusion time).\n\n* [Collet2008] J.F. Collet and F. Malrieu. Logarithmic sobolev inequalities for inhomogeneous markov semigroups. ESAIM: Probability and Statistics, 12:492\u2013504, 2008\n\n>**Additionally, the optimization speeds of different modalities may inherently vary, leading to discrepancies in performance. In other words, the proposed method might achieve satisfactory results with simplistic datasets, but training becomes substantially more challenging when scaled to extensive, real-world data scenarios.**\n\nWe respectfully disagree, this concern is speculative. In our experiments, we have not observed any issues related to the optimization speed raised by different modalities. The performance of our method has remained robust and consistent across various datasets.\n\nAs explained in the submitted version of the paper, we \u201c avoid any form of interference in the back-propagated gradients corresponding to the uni-modal reconstruction losses. Consequently, gradient conflict issues [Javaloy et al., 2022], where stronger modalities pollute weaker ones, are avoided.\u201d (see Section 3, below Equation 2 in our paper).\n\nThe only impact of scaling to larger, higher-resolution datasets is on the computational cost of the (single, amortized) score network, which scales with the dimensions of the concatenated latent sizes. However, this does not inherently make training more challenging from a methodological standpoint.\n\n**[Note: this reply continues in a separate comment.]**"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5378/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573293788,
                "cdate": 1700573293788,
                "tmdate": 1700576837651,
                "mdate": 1700576837651,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HcuLGTWBLA",
                "forum": "s25i99RTCg",
                "replyto": "Odsu7QsPA5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5378/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5378/Authors"
                ],
                "content": {
                    "title": {
                        "value": "."
                    },
                    "comment": {
                        "value": "> **In the context of conditional generation, the authors employ a masking technique to generate the desired modality based on the known one. However, the question arises: how is the intensity of the conditions controlled? This aspect is crucial for ensuring the effectiveness of the generative process.**\n\nIn our MLD method, conditional generation is indeed obtained through a masking process. This allows, assuming for example to consider as conditioning modality $A=a$, to have access to the *conditional score* of the diffused modality $B$ for any given diffusion time $t\\in[0,T]$. Consequently, it is possible to generate samples from $p(B|A=a)$, by simulating the reverse diffusion. \nIn the our implementation, there is not such a thing such as the *intensity*  of the conditions, as such strength assumes only binary values: either we generate conditionally, or unconditionally. This however, can be easily relaxed using the same simple tricks described in [Ho2021], where the introduction of an extra guidance parameter allows modulation of the importance of the conditioning modality. While interesting, we consider such a simple case an interesting venue for future works. \n\n\n[Ho2021] J. Ho, T. Saliman. Classifier-free Diffusion Guidance, NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications.\n\n>**Within the model's framework, a discrepancy arises between the training and inference stages in terms of the number of modalities. For instance, during training, the model might handle three modalities: A, B, and C. However, in a scenario where inference is desired based solely on modality A to predict B, would a masked C still be necessitated? This raises questions about the model's flexibility and its adaptability to accommodate various generative scenarios with different modalities. The capacity to dynamically adjust to these conditions without compromising the integrity of the generative process is pivotal.**\n\n\nIn the scenario you describe, where the model is trained on three modalities (A, B, and C) but inference is desired based on modality A to \u201cpredict\u201d B, it is not necessary to involve modality C. If the goal is to ensure that modality C does not influence the generation from A to B, our method simply considers C as a missing modality during the inference stage. This flexibility is an integral part of our model's design, ensuring that it can dynamically adjust to different conditions without compromising the integrity of the generative process.\n\n>**Some recent work, such as MMVAE+(Palumbo et al., 2023), should be included as a baseline. Work parallel to this paper, score-based multimodal autoencoders (Wesego et al., 2023), should be discussed in Section 2.**\n\n\nWe thank the reviewer for the suggestions. We included in the new version of the full experimental campaign results for all the datasets using MMVAE+ [Palumbo et al., 2023]. Our findings suggest that MLD outperforms this additional competitor, corroborating the claim that our method has excellent performance compared to the current SoA. \n\nWe also now cite the work from Wesego et al., 2023, but prefer to refer to it upfront in Section 1, rather than discussing it as part of the limitations of multimodal VAEs. Note that the work from Wesego et al. 2023, is an interesting approach that only loosely resembles our method. To the best of our understanding: 1) it uses Variational autoencoders, to produce latent variables of the same size for all modalities; 2) conditional generation is achieved through an auxiliary energy based model in the spirit of a \u201cclassifier-free guidance\u201d mechanism. We have performed additional experiments using the very same dataset in Wesego et al., 2023 (despite it being a concurrent submission) and included results in our new version of the paper: we did not have time to re-implement their method from scratch, so we used the results reported in their tables and compared them to our results. Our findings indicate that our method overall achieves the best results in terms of generative quality and coherence metric.\n\n[Wesego2023] Daniel Wesego and Amirmohammad Rooshenas, Score-Based Multimodal Autoencoders. https://arxiv.org/abs/2305.15708\n\n**[Note: this reply continues in a separate comment.]**"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5378/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573493741,
                "cdate": 1700573493741,
                "tmdate": 1700577096652,
                "mdate": 1700577096652,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wdj7pIBODh",
                "forum": "s25i99RTCg",
                "replyto": "Wcgc6HU8x4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5378/Reviewer_SqcE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5378/Reviewer_SqcE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. Upon thorough review of the revised submission, I have chosen to increase my rating."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5378/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637003065,
                "cdate": 1700637003065,
                "tmdate": 1700637003065,
                "mdate": 1700637003065,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cYk6WzcAg9",
            "forum": "s25i99RTCg",
            "replyto": "s25i99RTCg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5378/Reviewer_865d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5378/Reviewer_865d"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a method for conditionally generating multiple modalities of data which allows for certain modalities to be generated while conditioned on other existing modalities (e.g. generating images and audio from text). In contrast with VAE-based approaches, which can suffer from information loss due to the explicit separation of distinct modality subsets, the authors propose their method MLD, which avoids the problem by training separate unimodal autoencoders _deterministically_, and allowing a diffusion model to learn conditional generation of each modality\u2019s latent space. The diffusion model is trained to be conditioned on random subsets of modalities so that it remains robust to conditioning on any subset. The authors then compare MLD on several datasets against multi-modal VAE approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "### Good comparisons to multi-model VAEs with encouraging results\n\nThe authors do a good job of comparing their method MLD to other VAE-based works, and their results on their datasets are encouraging.\n\n### Good comparison in-painting and explanation for why it works less well\n\nThe authors also preemptively address a very natural question of why in-painting and cold diffusion might work less well compared to their approach of robustly training the diffusion model to be conditioned on different subsets of modalities. Their ablation study empirically justifies this claim, or at least it shows that in-painting is not significantly better than their proposed method."
                },
                "weaknesses": {
                    "value": "### No comparison to multi-modal diffusion models\n\nAlthough the authors have done a good job comparing to previous VAE-related works for multi-modal generation, there is no comparison with purely diffusion-based works. For example, the rather popular Any-to-Any Composable Diffusion (CoDi) (Tang, et. al., 2023) work is very closely related to MLD. CoDi also attempts to solve the multi-modal generation problem. Like the proposed method, CoDi performs diffusion in latent space and allows for conditioning on arbitrary subsets of modalities. Conditioning is done through \u201clatent alignments\u201d, where the latent space of some modalities are attended to by generated modalities. To tackle the problem of coherence, CoDi performs \u201cbridge alignments\u201d to pre-align the latent representations of each modality. Since this work is so similar in methodology to the proposed work here, it should be benchmarked against, as well.\n\n### Datasets benchmarked against are somewhat limited\n\nThe datasets in this work are fairly small (the MNIST datasets). The CUB dataset is larger, but only has two modalities. Since one of the core claims of this paper is successful multi-modal generation for arbitrary _subsets_ of modalities, this paper would be much stronger if it could also show MLD working well on another large dataset with more than two modalities (e.g. videos with audio and text).\n\n### More background on multi-modal VAEs would be nice\n\nDiffusion models are fairly common and well known at this point, but multi-modal VAEs are less well known (in my opinion). It would have been nice to have more background on how multi-modal VAEs work before describing their limitations.\n\nA main figure which illustrates the structure of these multi-modal VAEs in comparison to the proposed method would also be very helpful.\n\n### Some of the equations and math could be clearer\n\nOftentimes, there are equations presented which are presented without much explanation of each component (e.g. Equation 4, Equation 6, Equation 7). These can be a bit confusing to go through when there are simple English descriptions that can be offered instead (or certainly in conjunction) (e.g. \u201ckeeping the modalities in $A_1$ static throughout the forward and reverse diffusion process\u201d). These equations should be explained in more straightforward English or even replaced with English descriptions, because the equations do not aid in additional understanding of the paper\u2019s contributions.  Other equations like Equation 5 are certainly not needed for the understanding of this paper, since the modification onto diffusion-model training is very minor."
                },
                "questions": {
                    "value": "Minor grammatical suggestion: there should be an en-dash in \u201ccoherence\u2013quality tradeoff\u201d, not a hyphen."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5378/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685822662,
            "cdate": 1698685822662,
            "tmdate": 1699636543425,
            "mdate": 1699636543425,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rV74IbbEXA",
                "forum": "s25i99RTCg",
                "replyto": "cYk6WzcAg9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5378/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5378/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **Good comparisons to multi-model VAEs with encouraging results, Good comparison in-painting and explanation for why it works less well**\n\nWe thank the reviewer  for acknowledging the two main strengths of our work: a comprehensive experimental validation, which shows superior performance of MLD compared to competitors, and the need to modify classical diffusion processes to include the concept of multi-time, being the naive approach based on in-painting limited, as argued using information theoretic principles.\n\n> **Although the authors have done a good job comparing to previous VAE-related works for multi-modal generation, there is no comparison with purely diffusion-based works.**\n\nThank you for the suggestion, we know the work of Tang et al. very well! Let us start by re-stating the objective of our work: study the limitations of a large literature on multimodal generative modeling through latent fusion, and propose a radically new approach to latent mixing to overcome the limitations exemplified by the generation quality/coherence traderoff. To do that, we need to be able to compute quality (easy) and coherence (not so easy).\nNow, we indeed considered using CoDi in our experimental campaign. We see two options:\n\n1) Use the datasets from CoDi. For MLD, this requires training four autoencoders and one diffusion model on data that is several orders of magnitude larger than what our computational infrastructure allows (in CoDi, they use pre-trained models, we can\u2019t do the same, because our method is conceptually different). Even if we were up to the challenge, we would face a greater problem: computing the coherence between video modality and another set of modalities is not well understood (e.g., see Table 10 of CoDi, where coherence between a video input and any subset of output is not available).\n\n2) Use the datasets we adopt in our work, which would allow comparing CoDi not only to our MLD, but also to all the other methods we study. In this case, pre-trained CoDi diffusion models cannot be used, and we would have to re-train all components of CoDI from scratch. Even if we were up to the challenge, we would face a greater problem: CoDi **requires** a \u201cmaster modality\u201d, which is text. Many of the datasets we use (see also our answer to Reviewer yi7k on real-world applications) do not have the text modality, and it is not clear how to select a \u201cmaster modality\u201d to align to.\n\nTo conclude, while we recognize CoDi's impressive engineering and its contributions to the field (and we properly cite this work), its primary focus on text-driven-multimodal applications is not compatible with our goal of studying \"latent fusion\" and the quality/coherence tradeoff.\n\n>**Datasets benchmarked against are somewhat limited his paper would be much stronger if it could also show MLD working well on another large dataset with more than two modalities (e.g. videos with audio and text).**\n\nWe understand that the video modality is attractive, especially from the aesthetic point of view. Pushing the limits by striving for higher and higher resolutions is key for application-oriented methods that aim at surpassing SOTA results.\n\nHowever, our work focuses on a different research question: is it possible to break the limits of multimodal latent fusion with a radically new approach to achieve a sweet spot in the generative quality/coherence tradeoff? The answer is affirmative, and our claims are fully supported by the results obtained with our experimental protocol.\n\nIn light of a similar suggestion from Reviewer SqcE, we performed additional experiments (see Appendix E.5) with the CelebAMask-HQ dataset [Lee2020] as done in [Wesego2023], which consists of face images, each having a segmentation mask, and attributes, so 3 modalities. Our results are clear: the method we propose outperforms competitors from the state of the art, even with this new dataset.\nOther kinds of datasets, for example those including videos, are not appropriate for our goal because computing the coherence metric is not currently possible.\n\n* [Lee2020] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive facial image manipulation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n* [Wesego2023] Daniel Wesego and Amirmohammad Rooshenas, Score-Based Multimodal Autoencoders. https://arxiv.org/abs/2305.15708\n\n\n**[Note: this reply continues in a separate comment.]**"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5378/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572104188,
                "cdate": 1700572104188,
                "tmdate": 1700572104188,
                "mdate": 1700572104188,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6MFQyp4PvC",
                "forum": "s25i99RTCg",
                "replyto": "bMDRkbEX0p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5378/Reviewer_865d"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5378/Reviewer_865d"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for providing additional details.\n\n**Comparison with CoDi**\n\nThe explanation for why it would be prohibitive to apply MLD to the exact datasets from CoDi makes sense. It is unfortunate that the method does not scale to such larger datasets, and that is likely a drawback of MLD which should be made clear (which is okay).\n\nHowever, comparing with CoDi as a method would still be very informative to understand how MLD works compared to another diffusion-based method. The two methods are fundamentally similar, and especially given how well-know CoDi is, it is important to show some comparison with CoDi (even if MLD underperforms in some areas compared to CoDi). The selection of master modality can be done fairly easily, too. For example, for MHD, using the image modality as the master would be a reasonable selection just for benchmarking purposes.\n\n**Additional datasets**\n\nThe new results on the additional datasets are quite nice and have helped alleviate this particular concern for me. It would still be very informative, I think, to see how MLD performs on datasets with more modalities, as MLD is somewhat unique due to how it effectively trains (and samples from) the diffusion model on arbitrary partitions of the modalities. That said, it would understandable if there simply aren't any real-world applications which have many modalities.\n\nHowever, something that has come up in this discussion (and was touched upon in my initial review) is the question of scalability. Multi-modal models will generally always be more difficult and expensive to train, but the method would still need to be able to scale relatively well to be applied to real-world situations (even if it is infeasible to apply it to the CoDi datasets in such a short time frame, which is understandable). Can the authors provide some understanding of the training time (in number of epochs and wall time), comparing the VAE and diffusion-model training time for MLD (I see the number of epochs in Tables 8 and 9) versus the training time of a simple multi-modal diffusion model (which very well may underperform in both quality and coherence, and can only generate all modalities of an object unconditionally)?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5378/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671010909,
                "cdate": 1700671010909,
                "tmdate": 1700671010909,
                "mdate": 1700671010909,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U24V2XnuL6",
            "forum": "s25i99RTCg",
            "replyto": "s25i99RTCg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5378/Reviewer_yi7k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5378/Reviewer_yi7k"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the challenges associated with multi-modal generative modeling, a domain that focuses on generating data across multiple modalities such as images, text, and audio. The primary concern is the coherence-quality tradeoff observed in existing Multi-modal Variational Autoencoders (VAEs), where improving generative coherence across modalities might compromise the generation quality and vice versa.\nTo tackle these challenges, the authors introduce a novel approach named Multimodal Latent Diffusion (MLD). Unlike traditional multi-modal VAEs that often suffer from latent collapse and information loss, MLD employs independently trained, deterministic uni-modal autoencoders. Each modality is encoded into a specific latent variable, and these variables are then concatenated. The joint data generation is facilitated by a score-based diffusion model in the latent space, which reverses a stochastic noising process starting from a Gaussian distribution.\nThe experimental results are promising, outperforming the baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The introduction of the Multimodal Latent Diffusion (MLD) method offers a new perspective on multi-modal generative modeling.\n- The paper effectively tackles the coherence-quality tradeoff observed in existing multi-modal VAEs.\n- The inclusion of experimental results provides concrete evidence for the paper's claims, underscoring its quality and relevance."
                },
                "weaknesses": {
                    "value": "- Clarity in Presentation: While the paper is comprehensive, certain sections, especially those with plenty of mathematical formulations, might benefit from further simplification or more intuitive explanations for a broader audience.\n- Dataset Diversity in Experiments: The current experiments focus primarily on simple or low-resolution datasets. Would the MLD approach's efficacy be consistent when tested on more popular, high-resolution datasets? Expanding the experimental evaluation to include such datasets might enhance the paper's applicability and appeal to the broader research community.\n- Real-world Applications: The paper could be enriched by providing more real-world applications or use-cases to showcase the practical significance of MLD."
                },
                "questions": {
                    "value": "From what I've gathered, the MLD approach involves concatenating latent vectors derived from several uni-modal autoencoders. Following this, a diffusion model is trained within this combined latent space. Subsequently, a mechanism is introduced to facilitate conditional generation. Based on my understanding:\n1. How is the architecture of the autoencoders, such as the image AE and text AE, designed? In the context of stable diffusion, pure convolutional networks are utilized for both encoding and decoding. Does the MLD approach adopt a similar design for images?\n2. How have you determined the latent dimensions for each modality, and what criteria influenced your decision on their dimensionality?\n3. How scalable is the MLD approach when dealing with a large number of modalities or high-dimensional data within each modality?\n4. Given the independent training of uni-modal autoencoders, how do you ensure that the concatenated latent space is cohesive and meaningful? Are there any challenges in ensuring convergence during training?\n5. You mentioned you used 4 A100 GPUs for a total of roughly 4 months of experiments. Could you give more details about the training?\n6. Does your implementation yield results that are in line with those presented in the original baseline studies?\n7. The authors appear to have deviated from the official Style files and Templates as provided by the ICLR 2024 Call for Papers (https://iclr.cc/Conferences/2024/CallForPapers). Notably, there are discrepancies in the citation format."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5378/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698694932515,
            "cdate": 1698694932515,
            "tmdate": 1699636543326,
            "mdate": 1699636543326,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ow2HvkpuZS",
                "forum": "s25i99RTCg",
                "replyto": "U24V2XnuL6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5378/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5378/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the main strengths of our work: a new angle to overcome the generative quality-coherence tradeoff, and the extensive experimental validation of our idea. We hope our thorough rebuttal will clarify the reviewers' doubts.\n\n> **Clarity in Presentation: While the paper is comprehensive, certain sections, especially those with plenty of mathematical formulations, might benefit from further simplification or more intuitive explanations for a broader audience**\n\nWe do agree that the formulation might hinder accessibility of our work to a broader audience. However, we think that the mathematical rigor we adopt is a requirement for a venue like ICLR. To align with the reviewer\u2019s request, we can commit to writing a new section in the Appendix called \u201cIntuitive summary\u201d which we hope will help the readers grasp at an intuitive level the key ideas of our method, without using the language of mathematics. Please refer to Appendix A.5 in the new version of the paper for a preliminary draft of such a discussion.\n\n> **Dataset Diversity in Experiments: The current experiments focus primarily on simple or low-resolution datasets. Would the MLD approach's efficacy be consistent when tested on more popular, high-resolution datasets? Expanding the experimental evaluation to include such datasets might enhance the paper's applicability and appeal to the broader research community.**\n\nIn our research, we aim to demonstrate that the widely recognized concept of \"latent fusion\" through the product of experts, mixture of experts or combination thereof from the literature is not sufficient, particularly in terms of balancing generative quality and coherence. Our approach, which uses joint latent diffusion processes, effectively enables a new form of \"latent fusion\", whereby latent modalities jointly coevolve in the reverse, generative process. The datasets we selected for this study are not just adequate but crucial for drawing a meaningful comparison with the current state-of-the-art methods in \"latent fusion\". \n\nIn light of a similar suggestion from Reviewer SqcE, we performed additional experiments (**see Appendix E.5**) with the **CelebAMask-HQ dataset** [Lee2020] as done in [Wesego2023], which consists of face images, each having a segmentation mask, and attributes, so 3 modalities. Our results are clear: the method we propose outperforms competitors from the state of the art, even with this new dataset.\nOther kinds of datasets, for example those including videos, are not appropriate for our goal because computing the coherence metric is not currently possible.\n\n* [Lee2020] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive facial image manipulation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n* [Wesego2023] Daniel Wesego and Amirmohammad Rooshenas, Score-Based Multimodal Autoencoders. https://arxiv.org/abs/2305.15708\n\n> **Real-world Applications: The paper could be enriched by providing more real-world applications or use-cases to showcase the practical significance of MLD.**\n\nIn our exploration of the industrial applications of MLD, we have directed our focus toward the automotive industry, recognizing the potential impact of our research in this field. Specifically, we have conducted extensive experiments of MLD on automotive-related datasets. However, due to the private nature of these datasets, we are unable to share them publicly and have consequently omitted these specific tests from our paper.\n\nDespite this limitation, we can share insights into the practical use-cases we investigated. One key application of MLD in automotive technology is in mitigating the issues caused by faulty car sensors. In these applications, the text modality is absent, which hinders the use of many available pre-trained generative models (see also a comment by Reviewer 865d). Our model has shown promising results for in-car applications such as parking assistance, especially in situations where sensors are either faulty or their vision is occluded. These tests showcase the robustness and adaptability of MLD in handling real-world challenges in the automotive sector.\n\n**[Note: this reply continues in a separate comment.]**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5378/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570930943,
                "cdate": 1700570930943,
                "tmdate": 1700571228417,
                "mdate": 1700571228417,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9YGfIOIoyf",
                "forum": "s25i99RTCg",
                "replyto": "U24V2XnuL6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5378/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5378/Authors"
                ],
                "content": {
                    "title": {
                        "value": "."
                    },
                    "comment": {
                        "value": ">**How is the architecture of the autoencoders, such as the image AE and text AE, designed? In the context of stable diffusion, pure convolutional networks are utilized for both encoding and decoding. Does the MLD approach adopt a similar design for images?**\n\nIn our MLD approach, we have emphasized the importance of tailoring each autoencoder to its specific input modality to achieve best performance. This principle is reflected in our design choices for image, audio, and text autoencoders. We apologize if references to supplementary material were not clear, but all details concerning the architectures are already reported in Appendix D. For the case of images, we consider the same convolutional architectures of the competitors, which combine convolutional and fully connected blocks. Additional details concerning other modalities are available in Appendix D.\n\nNote that our method is conceptually different from the Stable Diffusion (SD) approach, where pure convolutional networks are adopted. In that work, a \u201clatent space\u201d is only motivated by the need for reduced computational complexity of the diffusion model. In SD, the \u201clatent\u201d input to the diffusion model is a downsized image with 4 channels (details may vary depending on the SD version). In the VAE literature we are considering, latent space has a different meaning, and usually corresponds to a flattened vector which belongs to a real space of a given dimension. This difference has important practical implications: with our approach, it is much simpler to **mix** latent spaces of different modalities, whereas with SD such mixing would require careful engineering effort. Finally, recall that SD is unidirectional: text-to-image, but not the other way around.\n\n>**How have you determined the latent dimensions for each modality, and what criteria influenced your decision on their dimensionality?**\n\nThis is a good question which is a research line per se! The manifold hypothesis [Gorban2018] tells us that natural data resides on a lower-dimensional space, and we tend to use small latent sizes, in accordance with what has been done in the literature. We have experiments with larger latent sizes and in general performance improves, up to a point. The interesting tradeoff in our case is that the smaller the latent size, the more computationally efficient is latent diffusion.\n\n[Gorban2018] Gorban AN, Tyukin IY. Blessing of dimensionality: mathematical foundations of the statistical physics of data. Philos Trans A Math Phys Eng Sci. 2018;376(2118):20170237. doi:10.1098/rsta.2017.0237\n\n>**How scalable is the MLD approach when dealing with a large number of modalities or high-dimensional data within each modality?**\n\nOur method MLD scales well in terms of the number of modalities, as well as in terms of the resolution of an individual modality in its input space. Concerning the number of modalities, this is visible for example in Fig. 18, Section E.3, where we vary the number of modalities used in the PolyMNIST dataset, and show that MLD performs extremely well, especially in terms of generation quality, while maintaining the highest coherence. Concerning resolution, this is a direct consequence of projecting the input in a lower-dimensional latent space.\n\nThe primary bottleneck in MLD is the latent dimension selected for each modality. Larger latent sizes are necessary for complex modalities, and this affects the computational cost of our multi-time masked diffusion model. In reference to the previous remark, we select appropriate latent dimensions for each modality because we use well-studied datasets from the literature, for which our model does not require extremely high costs. For an application of MLD \u201cin the wild\u201d, a good engineering practice of studying the data first, for example through the lenses of our deterministic autoencoders, should be applied.\nNote that MLD provides an important advantage when compared to multimodal VAE-based approaches: there is no requirement for all latent spaces to have exactly the same dimension. This is evidently not true for e.g. Mixture based approaches, where the different modalities are required to have the same latent size (otherwise sum of latent means is not possible).\n\n**[Note: this reply continues in a separate comment.]**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5378/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571417724,
                "cdate": 1700571417724,
                "tmdate": 1700571577402,
                "mdate": 1700571577402,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rEoAsPfSdc",
            "forum": "s25i99RTCg",
            "replyto": "s25i99RTCg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5378/Reviewer_mRTE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5378/Reviewer_mRTE"
            ],
            "content": {
                "summary": {
                    "value": "this paper focuses on multi-modal image generation. the definite of multi-modal is multiple dataset distributions including text and image.  the proposed method is based on latent diffusion and the authors proposed som modification to make it work on multi-modal datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* This paper focuses on classical problem in machine learning: multimodal dataset.\n* the proposed method works better than other competing methods"
                },
                "weaknesses": {
                    "value": "* the results dont look very good. e.g., MLD painted birds in Fig 22. \n* the paper lacks a overall diagram showing the whole model design. its' a bit difficult to understand the model design"
                },
                "questions": {
                    "value": "the other exciting methods seem quite weak. e.g., in Fig 20, MVAE cannot even generate digits very well, and in page 22, MVAE and MOPOE can't generate legible birds at all. are theses meaningful benchmark methods in 2023?\nare there strong methods the author can use?\n\nhow is text generation done in the proposed method? I assume in figure 22, the models generated both images and text."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5378/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816340169,
            "cdate": 1698816340169,
            "tmdate": 1699636543206,
            "mdate": 1699636543206,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "52xUyjaiuL",
                "forum": "s25i99RTCg",
                "replyto": "rEoAsPfSdc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5378/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5378/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**Strengths:\nThis paper focuses on classical problem in machine learning: multimodal dataset.\nthe proposed method works better than other competing methods**\n\nWe thank the reviewer for acknowledging that our work, which focuses on a classical and important problem in machine learning, i.e., multimodal generation, defined a new method that greatly outperforms all the considered alternatives from the state of the art.\n \nWe have included additional experiments with the CelebAMask-HQ dataset [Lee2020] (see Appendix E.5 ), which consists of 3 modalities : face images, each having a segmentation mask, and attributes. Our MLD method outperforms competitors from the state of the art on both coherence and quality metrics.\n\n> **the other exciting methods seem quite weak. e.g., in Fig 20, MVAE cannot even generate digits very well, and in page 22, MVAE and MOPOE can't generate legible birds at all. are theses meaningful benchmark methods in 2023? are there strong methods the author can use?**\n\nThe literature we target focuses on the problem of multimodal generative modeling, with a vast array of models that combine several input modalities in a joint, mixed latent space, which is then used to generate multimodal data. At the risk of being simplistic, we could state that the key differentiating factor in all such prior work is the method used to mix latent representations of different modalities into a unique latent space: broadly speaking, either the mixing happens according to a \u201cproduct of experts\u201d, a \u201cmixture of experts\u201d, or a combination thereof.\n\nThe reviewer correctly acknowledges that the considered competitors suffer from limitations, especially from the perspective of generative quality. This is not a flaw of our comparative analysis, but a widely known literature result that pinpoints an open question: how can we achieve a better tradeoff in generative quality vs. coherence? This is *the current state of the art in 2022/2023*, as testified by works published in ICLR:\n\n* \u201cMMVAE+: Enhancing the Generative Quality of Multimodal VAEs Without Compromises\u201d, ICLR 2023, https://openreview.net/pdf?id=BYHy9WwxFU **(see fig. 7)**\n* \u201cOn the Limitations of Multimodal VAEs\u201d, ICLR 2022, https://arxiv.org/pdf/2110.04121.pdf **(see Fig. 4)**\n\nThe poor results we obtain for competitors is far from surprising, as we spent particular effort into replicating the experimental setup of our competitors, including architectures and hyper-parameters. Consequently, we claim that the answer to \u201care these meaningful benchmark methods in 2023?\u201d is a sound yes.\n\nImportantly, the problems of our competitors are not related to architecture capacity, but to intrinsic limitations of the mixing method, which induces the generative quality/coherence tradeoff. This is exactly the conclusion reached in:\n\u201cOn the Limitations of Multimodal VAEs\u201d, ICLR 2022, https://arxiv.org/pdf/2110.04121.pdf .\n\n> **the results dont look very good. e.g., MLD painted birds in Fig 22.**\n\nWe respectfully disagree with this comment. It is clear that MLD outperforms competitors from an image quality generation point of view (see also the FID scores Table 16 Appendix E.4). This is a result of how our multi-time diffusion model achieves latent modality mixing, since the auto-encoder architectures we use are the same as for all competitors. \n\n> **how is text generation done in the proposed method? I assume in figure 22, the models generated both images and text.**\n\nIndeed, the methods we consider in this work can perform joint generation of all modalities. Text generation is carried out exactly like for all other modalities: a sampled latent variable is fed to a text-specific decoder. \n\nNote that our goal is not to engineer SOTA encoder-decoder architectures for the text modality, which would likely require using a LLM and several orders of magnitude more compute and training data. Instead, as clarified in the Appendix of the submitted paper, we adhere to related literature practice:  \u201cFor CUB, we use the same autoencoders architecture and implementation settings as in Daunhawer et al. [2022]. Laplace and one-hot categorical distributions are used to estimate likelihoods of the image and caption modalities respectively.\u201d\n\n> **The paper lacks a overall diagram showing the whole model design. its' a bit difficult to understand the model design**\n\nWe think the reviewer might have missed Fig. 4 in the Appendix of the submitted paper, where we clearly display a diagram of the model design."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5378/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571159385,
                "cdate": 1700571159385,
                "tmdate": 1700575147084,
                "mdate": 1700575147084,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]