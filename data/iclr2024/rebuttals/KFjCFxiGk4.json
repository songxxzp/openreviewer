[
    {
        "title": "Certified Deductive Reasoning with Language Models"
    },
    {
        "review": {
            "id": "sdbFghGBqS",
            "forum": "KFjCFxiGk4",
            "replyto": "KFjCFxiGk4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4276/Reviewer_GDuS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4276/Reviewer_GDuS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a way to utilize a theorem prover with a large language model to produce answers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The high level idea seems good (but the details I'm no so clear about). The results are very good."
                },
                "weaknesses": {
                    "value": "The main problem with this that the details of the architecture isn't clear. Here is what I understand: The LLM gets language (The \"Context\" in the figures). The LLM generates a \"formalized context\" that can be used as the input to Peano. Peano implements a guide function, and outputs a set of valid one-step conclusions. This is input back into the LLM by biasing the logits (whatever that means), then presumably the LLM does sometime else to generate the next formalized contexts to do the next steps and so on. At some stage this halts and one of them produces an answer. (Does the LLM also outputs natural language?)\n[Alternatively: Using figure 2 as an example, The LLM takes the contact and produces the formalized context and the formalized goal. Peano takes these and outputs a proof (Is this the \"reasoning\" in that figure?). That would seem to make the most sense. But that can't be correct as the external tool only answers \"what inferences can be made next?\".]"
                },
                "questions": {
                    "value": "What is the interface between the LLM and Peano? (What is the input of each and what is the output? Does Peano have any knowledge built-in (e.g., axioms for deontic logic)?\n\nWhat is an example application beyond artificial logic puzzles? (The legal reasoning is a good example, but it only used the theorem prover for bootstrapping.)\n\nWhat does \"bias the logits\" mean? How is it done? How does the theorem prover determine how to bias them?\n\n(My rating assumes there is a satisfactory answer to these questions. I will downgrade my rating if I still cannot understand the interface after the rebuttal period.)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4276/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4276/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4276/Reviewer_GDuS"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4276/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697671495265,
            "cdate": 1697671495265,
            "tmdate": 1700857310272,
            "mdate": 1700857310272,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U3KRrwKBev",
                "forum": "KFjCFxiGk4",
                "replyto": "sdbFghGBqS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4276/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4276/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "Thank you for the review and the encouraging comments! We provide clarifications on the approach and other questions below, and are happy to discuss any of these further.\n\n> What is the interface between the LLM and Peano? (What is the input of each and what is the output?\n\nWe're happy to expand on the explanation in Section 3.1 (we'll update the Appendix with pseudocode based on the description below and the discussion here). Essentially, three components that are tightly integrated: the LLM decoding algorithm, LogicGuide, and Peano. The LLM decoding algorithm will output token by token while constraining the output for the next token to come from a subset of the vocabulary determined by LogicGuide. Outside of a guided block (delimited by \"[[\" and \"]]\"), there are no constraints on which token can be emitted. When the model opens a guided block with \"[[\", LogicGuide first constrains it to output one of the LogicGuide actions followed by a colon. In the cases where the model outputs the \"infer\" action, LogicGuide calls Peano to determine which inferences are valid at this point. Peano will take as input all the formalized premises and previous inferences, which are extracted from the LLM's output so far, and output a list of valid one-step inferences. LogicGuide will take this list and compute token constraints for the LLM decoding algorithm so that its output, even over multiple tokens, will come from this list. Thus, looking at Figure 2, all three components are tightly connected to produce the output. The input here for the model is just the Context and Question. The model then starts to write its \"Formalized context\", then the goal, and finally the reasoning and answer. Every time the model opens a block with \"[[\", LogicGuide is called to determine constraints. In particular, in the [[infer]] blocks, LogicGuide calls Peano to determine valid inferences.\n\nIn general, applying such constraints is non-trivial because LLM tokens can break up these sequences (such as \"[[\", where the brackets might be split up in separate BPE tokens depending on the surrounding context). These complications are handled by CSD, which we use for decoding with constraints.\n\n> Does Peano have any knowledge built-in (e.g., axioms for deontic logic)?\n\nIn our experiments, we initialized Peano with an empty theory in PrOntoQA, ProofWriter and DeontiQA -- the relevant axioms are formalized by the LLM in-context. However, it might also be interesting for future work to explore scenarios where some background knowledge is given to the language model implicitly through the theory used to initialize Peano.\n\n> What is an example application beyond artificial logic puzzles? (The legal reasoning is a good example, but it only used the theorem prover for bootstrapping.)\n\nThe framework of guides can generally serve to explore connecting formal systems with LLM reasoning, besides the examples we provided. Many other important fragments of reasoning have been formalized into different logics - temporal logic (e.g., for reasoning about events), modal logic (e.g., connected to theory of mind reasoning), deontic logic (e.g., permissions, obligations), and so on. We provide a starting point for these, since these logics can be formalized in Peano and thus used to guide LLMs.\n\n> What does \"bias the logits\" mean? How is it done? How does the theorem prover determine how to bias them?\n\nThe output layer of the Transformer will predict unnormalized weights (logits) for each token in its vocabulary at each iteration. These logits are passed through a softmax to then become a probability distribution for sampling. When doing constrained decoding, such as in CSD, we want to \"select\" a subset of the tokens that will be allowed for sampling the next token. In some APIs like the OpenAI API, there is no option for directly selecting vocabulary tokens, but there is a way to add a \"logit bias\" - a weight that will be added before softmax to the output logit of the given vocabulary tokens. A sufficiently large negative bias effectively sets the probability for that token to zero. This trick, which was used in the CSD paper [1], is how we effectively constrain the OpenAI LLMs to valid completions. We use the same trick with LLaMA offline.\n\n> (My rating assumes there is a satisfactory answer to these questions. I will downgrade my rating if I still cannot understand the interface after the rebuttal period.)\n\nThat is fair \u2013 we will be happy to answer further questions or try to explain in different ways! Let us know what would be most helpful.\n\n[1] Poesia, G., Polozov, O., Le, V., Tiwari, A., Soares, G., Meek, C., & Gulwani, S. Synchromesh: Reliable code generation from pre-trained language models. ICLR 2022"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4276/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700031755048,
                "cdate": 1700031755048,
                "tmdate": 1700031755048,
                "mdate": 1700031755048,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xvofdni60q",
                "forum": "KFjCFxiGk4",
                "replyto": "U3KRrwKBev",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4276/Reviewer_GDuS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4276/Reviewer_GDuS"
                ],
                "content": {
                    "comment": {
                        "value": "What I still don't understand is: after the ]], LogicGuide calls Peano, which creates an output (list of one-step inferences). The LLM just continues its generation;  the only control you have over it is to bias the tokens being generated. Presumably the one step inferences are statements in logic and are not tokens that are being biased. How do you get from these statements to a way to bias the logits; presumably there are many ways the LLM could generate the one step conclusions. Or do you actually force the LLM to generate exactly the text of one of  the one-step inferences, in which case where does that text come from?  I could imaging you add a large constant to the logits of first word of each of the one step conclusions and then to subsequent words for the matching conclusions. In this way you are forcing the LLM to state exactly one of the one-step conclusions as its next few  tokens? (Most of your description in the paper seems to be about how to get the input to LogicGuide/Peano, which I'm guessing is the challenging part to implement.) This seems to be very sensitive to which tokens the LLM will generate (e.g., it can't do greedy decoding), but has to fairly sample from its predicted distribution of the next token.\n\nWhen you say \"The model then starts to write..\" I guess you mean the LLM writes (because you could also output the results of Peano)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4276/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540077734,
                "cdate": 1700540077734,
                "tmdate": 1700540077734,
                "mdate": 1700540077734,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e3AT0o56lW",
                "forum": "KFjCFxiGk4",
                "replyto": "X9xREXsMab",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4276/Reviewer_GDuS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4276/Reviewer_GDuS"
                ],
                "content": {
                    "comment": {
                        "value": "In your comment, I am assuming that \"the model\" is the LLM. If so, it makes sense.\n\nNow I just don't understand how you get the LLM to output [[infer:  or even [[ in the first place. How does it know that it should ask for advice."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4276/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611767043,
                "cdate": 1700611767043,
                "tmdate": 1700611767043,
                "mdate": 1700611767043,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NTbGapkQqG",
                "forum": "KFjCFxiGk4",
                "replyto": "RBqWGDkl6O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4276/Reviewer_GDuS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4276/Reviewer_GDuS"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks. That's what I had guessed (but I'm finding my guesses are often wrong ;^)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4276/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701583920,
                "cdate": 1700701583920,
                "tmdate": 1700701583920,
                "mdate": 1700701583920,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gAcUfM7Fcm",
            "forum": "KFjCFxiGk4",
            "replyto": "KFjCFxiGk4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4276/Reviewer_cFZp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4276/Reviewer_cFZp"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies logical reasoning in natural language with LLMs. Whereas a number of existing approaches may arrive at the correct answer with a wrong reasoning chain, this work proposes an approach to guide the LLM generations using a logic solver that constrains the space of possible generations to those that are logically valid. With this approach, while there can still be errors in the translation stage (i.e. the stage where the LLM translates from natural language to logical form), the logical conclusions made on those translations are valid. Experimental results are shown on multiple datasets including ProofWriter, PrOntoQA, Syllogism Validity, LegalBench, and ReClor."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Logical reasoning (or more generally multi-hop reaosning) in natural language with LLMs is an important area of research.\n- Showing results both for prompting and finetuning.\n- The writing was mostly clear and easy to follow.\n- The reported improvements for ReClor could be quite encouraging."
                },
                "weaknesses": {
                    "value": "- Most of the experiments are done on the ProofWriter and the PrOntoQA datasets. Both these datasets have been constructed by turning logical theories into natural language using very simple templates. This is especially true for the PrOntoQA dataset where each sentence is of the format \"X is Y\" which is simply equivalent to (X, is, Y) in the triple notation. For this reason, while these datasets are appropriate benchmarks for measuring the general reasoning capacity of off-the-shelve LLMs, I do not think they are good benchmarks for the model proposed in this paper (translating these datasets back into their logical form is just too easy for nowadays LLMs). For this reason, while those results could be good sanity checks, I don't think they truly represent the merit of the proposed approach. They highly overestimate the performance we can expect on real tasks but highly underestimating how difficult it is to translate an actual natural language passage into logical form. \n- The failure example highlighted in Page 6 (translating to (sees A B) in one place and (see A B) in another) makes me worry about the applicability of the proposed approach to reasoning problems beyond synthetic tasks such as ProofWriter and PrOntoQA. It also makes me  think that BoardgameQA might have been a slightly better dataset to use. While it has also been generated synthetically by converting logical theories into textual format, the missing knowledge piece of it makes it better resemble real-world problems, and makes for a good test to see the extent of the \"see\" vs\" sees\" problem in the proposed approach. \n- While the results on the ReClor dataset are quite encouraging, I find them quite surprising as well for multiple reasons. 1- Given that the model is finetuned only on 120 samples, and considering the size of the models used, I would expect that the models should just overfit to those examples without any task transfer. 2- If I understand correctly, the finetuning is not on a mixture of the original data and the 120 data points, so I would expect that the model's general task solving ability should go down. 3- The ProofWriter and PrOntoQA datasets only require deductively applying the modus ponens rule, whereas the ReClor dataset requires more complicated rules and reasoning. For these reasons, I found the improvements a bit surprising and the provided explanation does not give much insights."
                },
                "questions": {
                    "value": "- On which categories from Table 2 of the BoardgameQA paper do you expect your approach to fail/succeed? And why?\n- Given that the results in Table 6 are tested in a zero-shot setting, how do you extract the final answer? Is it possible that after finetuning on the 120 examples, the model mainly just learns to produce outputs in the specified format making it easier to extract the final answer (and hence higher predictive accuracy)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4276/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698700157106,
            "cdate": 1698700157106,
            "tmdate": 1699636395101,
            "mdate": 1699636395101,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tegalRwZlq",
                "forum": "KFjCFxiGk4",
                "replyto": "gAcUfM7Fcm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4276/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4276/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "Thank you for the thoughtful comments on our work! We address the questions below, and are happy to engage further.\n\n> While the results on the ReClor dataset are quite encouraging, I find them quite surprising as well for multiple reasons. 1- Given that the model is finetuned only on 120 samples, and considering the size of the models used, I would expect that the models should just overfit to those examples without any task transfer. 2- If I understand correctly, the finetuning is not on a mixture of the original data and the 120 data points, so I would expect that the model's general task solving ability should go down. 3- The ProofWriter and PrOntoQA datasets only require deductively applying the modus ponens rule, whereas the ReClor dataset requires more complicated rules and reasoning. For these reasons, I found the improvements a bit surprising and the provided explanation does not give much insights.\n\nThank you for the question -- we will update the paper with a better discussion of the transfer results, which is indeed needed. Although we have few public technical details on the GPT-3.5 fine-tuning internals, the fine-tuning method is most likely a low-rank adaptation (like LoRA [1], which we used for our experiments on LLama), so the parameters of the base model are likely unchanged. Instead, a small number of parameters is learned on top of the model to adjust its behavior to the new examples. In general, parameter-efficient fine-tuning methods like LoRA tend to not overfit as easily or degenerate the model's behavior because they operate with relatively low capacity. This is also why we don't need many examples for fine-tuning (the OpenAI documentation recommends starting with an even smaller number, like 50, and iterating from there, which corroborates this). Our understanding is that low-rank fine-tuning is not necessarily \"teaching the model completely new skills\" (such as inference patterns unseen during pre-training), but rather reinforcing skills that were already present in the training data, at the expense of less relevant ones for the task. In turn, fitting higher-quality examples yields better results.\n\n> On which categories from Table 2 of the BoardgameQA paper do you expect your approach to fail/succeed? And why?\n\nLogicGuide can in principle be applied to all of those types of rules and comparisons, except Ortography (since Peano does not currently have a representation for strings as sequences of characters). For problems where there are missing assumptions, we can ask the model to formally state those for use in inference. We previously ran a preliminary experiment with this using PrOntoQA False (where the rules violate world knowledge), where instead of giving the model the premises and asking it to reason with them, we gave it the reasoning trace, without any premises, and asked it to first formalize assumptions that would enable the argument to work, and only then proceed with the formal argument. We found few-shot prompting for completing with the assumptions to be effective, and we believe a version of this could also work for the kinds of gaps that happen in BoardgameQA, where the model also needs to identify and complete missing assumptions in the problem (e.g., that \"every dog is a mammal\" to connect \"sam is a dog\" and \"every mammal \u2026\"). We are running a scaled up version of this experiment and will update the paper with results on it soon (and let you know here as well).\n\n> Given that the results in Table 6 are tested in a zero-shot setting, how do you extract the final answer? Is it possible that after finetuning on the 120 examples, the model mainly just learns to produce outputs in the specified format making it easier to extract the final answer (and hence higher predictive accuracy)?\n\nIn the zero-shot setting, we prompted GPT-3.5 Turbo with an explanation of both the task and the desired output format, and we did allow for some deviation by looking at the outputs of the base model and matching extra patterns. We observed that the original model before our fine tuning did follow the desired format, and only deviated from it in a few predictable ways (e.g., instead of saying just \"Answer: A\" as we asked, it would often say \"Answer: A - <<option text here>>\"). We note that the fine-tuned model was still subject to these variations because the format of ReClor was not exactly the one during training (e.g., here there were several options for the answers, while the training examples were just true/false).\n\nThank you again for the attentive review! Please let us know if we can clarify further. We'll update you here once we have the results on the experiments with missing assumptions.\n\n[1] HU, Edward J. et al. LoRA: Low-Rank Adaptation of Large Language Models. ICLR 2022"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4276/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700030990232,
                "cdate": 1700030990232,
                "tmdate": 1700031035954,
                "mdate": 1700031035954,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aHWXWTMbEK",
            "forum": "KFjCFxiGk4",
            "replyto": "KFjCFxiGk4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4276/Reviewer_HHEy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4276/Reviewer_HHEy"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel tool, termed \"guide,\" designed to ensure that language models engage in sound step-by-step reasoning. As a primary illustration, LogicGuide utilizes general logical reasoning systems to guide models towards producing logically consistent explanations. Experimental results indicate that LogicGuide enhances the performance of language models, in terms of reasoning accuracy, reducing content effects, self-learning and generalization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper introduces a novel logical guidance framework designed to aid LLMs in performing logical inference. The method employs the most general form of deductive reasoning, making it versatile across a range of reasoning scenarios.\n2. Experiments across multiple datasets validate that LogicGuide enhances the performance of language models. The paper also provides specific examples demonstrating its efficacy in mitigating the impact of unwarranted prior assumptions and performing self-learning."
                },
                "weaknesses": {
                    "value": "1. The proposed method necessitates a reliance on a complex formalization process during training and inference.\n\n2. The scenarios considered in the paper seem a bit limited. Despite experimenting on diverse datasets, the nature of problems within them appear quite similar. In more generalized contexts, it might be challenging to formalize and identify corresponding actions, such as `objects`, `relations`, etc.\n\n3. The paper's primary contribution, namely, how to harness logic to ensure output consistency, seems to overlap with prior work on the Peano theorem and the constrained Semantic Decoding algorithm, which weakens the novelty of the current research.\n\n4. It seems the proposed idea is similar to the idea in Logic-LM. The authors did not discuss their differences.\n\n   Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning https://arxiv.org/abs/2305.12295"
                },
                "questions": {
                    "value": "1. How likely that encountering a formalization failure may happen, and are there strategies in place to minimize formalization errors?\n2. To what extent does using constrained generation reduce the reasoning space, so as to mitigate the issue of \"logical inferences made next can have a potentially large set of answers\"? Is it possible that still there may be a considerably large set of answers, if so, how does your method decide on the the most appropriate content to generate next?\n3. Discussions on generalization involve models bootstrapped from other formalizable tasks. In scenarios challenging to formalize, what amount of preparatory work, such as the number of samples of formalizable tasks, is essential to ensure the model with strengthened generalization inference capabilities? If in the absence of abundant corresponding simpler tasks, how to generalize \"guide\" in broader scenarios?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4276/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4276/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4276/Reviewer_HHEy"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4276/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824043986,
            "cdate": 1698824043986,
            "tmdate": 1699636395028,
            "mdate": 1699636395028,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4XDvKu5Ob8",
                "forum": "KFjCFxiGk4",
                "replyto": "aHWXWTMbEK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4276/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4276/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thoughtful comments, and the encouraging remark that the approach is \"versatile across a range of reasoning scenarios\"! We respond to the questions and concerns below, and are happy to discuss these further.\n\n> The proposed method necessitates a reliance on a complex formalization process during training and inference.\n\nWhile this is true for problems that can be adequately formalized, our results (Section 4.4) show that bootstrapping GPT-3.5 Turbo on such problems, where formalization helps during training, can also improve reasoning when not formalizing at inference time.\n\n> In more generalized contexts, it might be challenging to formalize and identify corresponding actions, such as objects, relations, etc.\n\nWe agree that this can be hard in more naturalistic problems. Our experiments in Section 4.4 show that bootstrapping on simpler problems can still help even when not formalizing at inference time.\n\n> Overlap with prior work on the Peano theorem and the constrained Semantic Decoding algorithm\n\nOur framework of guides is a novel interface between these components. Peano itself is simply a formal theorem proving environment, with no natural language component. Moreover, while Constrained Semantic Decoding was originally used in cases where the entire output is constrained, our framework of guides generalizes this to allow the model to selectively turn the guide tool (and therefore the constraints) on and off during generation. Thus, we build on these prior works but ultimately implement new capabilities.\n\n> It seems the proposed idea is similar to the idea in Logic-LM.\n\nThank you for the reference. We'll add it to our Related Work section. In summary, works like Logic-LM use the LLM to parse the problem, and then offload reasoning entirely to an external engine. Thus, the model does not produce a reasoning trace in natural language (a \"chain-of-thought\"), since reasoning is carried out by the external tool (note that none of the tools used in Logic-LM produce such a trace either). In our work, Peano is only used to limit what inferences the model itself can do, but ultimately the LLM is still the reasoner, and still produces a chain-of-thought. We need these reasoning traces to fine-tune the model itself in a self-improvement fashion (which we show in Sections 4.3 and 4.4). \n\n> How likely that encountering a formalization failure may happen, and are there strategies in place to minimize formalization errors?\n\nIt varies from model to model - the OpenAI models were fairly good at formalization in our experiments, whereas LLaMA lagged behind. For the formalization blocks, LogicGuide still imposes constraints on well-formedness during generation to minimize errors. Namely, the constraints guarantee that the model will output expressions that are syntactically valid and coherent with its own proposition and object annotations, thus avoiding errors due to such inconsistencies. These constraints were triggered in around 5-10% of the cases depending on the dataset for text-davinci-003 and gpt-3.5-turbo, where otherwise there would have been formalization errors. We are running a more in-depth analysis on this and will add it to the paper soon.\n\n> To what extent does using constrained generation reduce the reasoning space, so as to mitigate the issue of \"logical inferences made next can have a potentially large set of answers\"? Is it possible that still there may be a considerably large set of answers, if so, how does your method decide on the the most appropriate content to generate next?\n\nThe number of possible valid inferences for the datasets we used were in the order of dozens in the worst case. In these cases, the language model is constrained to generate from that set when it opens an [[infer]] block, and uses its own log-probabilities to decide what is most appropriate in the current context / solution. Thus, we allow the model to choose its logical inferences while constraining it to logically sound ones. Allowing the language model to decide amongst logically valid continuations is unique to our guide approach.\n\n> In scenarios challenging to formalize, what amount of preparatory work, such as the number of samples of formalizable tasks, is essential to ensure the model with strengthened generalization inference capabilities?\n\nThis is a good question. In general, we cannot strictly guarantee that positive transfer will happen between the training and inference tasks. However, using low-rank fine-tuning methods [1], we observe that even a few hundred high-quality training examples can improve reasoning performance. In turn, the guided models are able to produce higher quality training examples for this purpose, as our results show.\n\nPlease let us know if we have clarified your concerns. We're happy to discuss these further or provide more information!\n\n[1] HU, Edward J. et al. LoRA: Low-Rank Adaptation of Large Language Models. ICLR 2022"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4276/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700030093674,
                "cdate": 1700030093674,
                "tmdate": 1700030093674,
                "mdate": 1700030093674,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZV7WqUu35q",
                "forum": "KFjCFxiGk4",
                "replyto": "XGff1ytZ5E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4276/Reviewer_HHEy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4276/Reviewer_HHEy"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the author responses."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4276/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741973997,
                "cdate": 1700741973997,
                "tmdate": 1700741973997,
                "mdate": 1700741973997,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]