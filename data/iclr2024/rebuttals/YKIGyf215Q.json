[
    {
        "title": "Structured Fine-Tuning Enables Data-Efficient Adaptation of Code Language Models"
    },
    {
        "review": {
            "id": "zReDKGbZeY",
            "forum": "YKIGyf215Q",
            "replyto": "YKIGyf215Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6558/Reviewer_t7DZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6558/Reviewer_t7DZ"
            ],
            "content": {
                "summary": {
                    "value": "Most language models trained on code treat their inputs as plain-text tokens, which disregards the structured nature of programs. This work proposes to train LMs with a fairly low-level tree representation, serialized to text. Adding this formulation to both encoder-decoder and decoder-only models (in slightly different forms) increases the training speed on a range of tasks and offers benefits in some tasks after 1K training examples."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall, this work offers promising initial results. Compared to the (large body) of recent work on using the tree and graph representations of code, this paper chooses a fairly low-level and easily serializable tree representation. That makes the approach relatively flexible and may contribute to its positive performance. The tasks defined for CodeT5 are reasonable and the methodology was easy to follow.\n\nThe results generally paint a positive picture; focusing just on exact match (for reasons discussed below), the most substantial gains are seen by CodeT5 on a few benchmarks, while on others the performance is about even with vs. without structural information.\n\nThe paper is quite well written."
                },
                "weaknesses": {
                    "value": "While the BLEU score results are very strong, the exact match rate is much less so. The structured models are often no better than their plain counterparts on the latter. This raises serious concerns around the correctness of the BLEU score implementation. One possibility is that it was measured without removing the non-terminal tokens from the serialized representations, which would make it relatively easier to predict matching tokens but not more useful for the downstream task. It is hard to tell from the implementation if this happened -- I found a flag that appears to toggle this behavior, but it is not clear if it was used. In any case, the gap is large enough to both raise concerns around the methodology and, if it is sound, create the impression that BLEU score is quite a poor proxy for performance here, since the exact match rate is virtually unchanged.\n\nThis aside, a strong downside of using tree-like representations of code is that they make the models less (or not at all) helpful in partial code contexts, where a syntax tree cannot be unambiguously parsed. The proposed format  is no exception. This rules out its usage for much of the most common use-case of LLMs for code (code completion), among others. The chosen representation also roughly doubles the length of a given program when encoded as tokens. This comes at a substantial cost given the quadratic complexity of attention involved in Transformers, and the limitations commonly imposed on context windows.\n\nFinally, the work experiments with rather small models. While this is understandable from a resource usage perspective, it is not clear that the results will generalize to billion-scale LMs. A series of scaling results, even on sub-B parameter models, would be helpful here.\n\nMinor notes:\n- Fig. 1 does not include whitespace tokens, which might be helpful to illustrate the point made about formatting on page 4.\n- P4: the choice of x, y, z to refer to important elements is somewhat surprising; I would suggest finding more distinctive tokens for these."
                },
                "questions": {
                    "value": "Please consider the weaknesses above, and in particular, answer:\n\n- Is the BLEU score implemented and evaluated only on the original code tokens? If so, what would cause there to be such a large gap in BLEU score but none in exact match? And given that, is BLEU score relevant? Perhaps provide examples where a snippet generated by a structured model has a much higher BLEU score and is also more useful, despite being inexact.\n- How do you assess the cost/benefit trade-off of (a) requiring parseable code and (b) increasing the length of the program's representation in terms of tokens given that the most common use-case of LLMs for code is code generation, often in incomplete contexts."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6558/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698449502474,
            "cdate": 1698449502474,
            "tmdate": 1699636740895,
            "mdate": 1699636740895,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "KwmaV6eIwG",
            "forum": "YKIGyf215Q",
            "replyto": "YKIGyf215Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6558/Reviewer_28Uu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6558/Reviewer_28Uu"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses LLMs used as code generators. The main objective is to introduce structured knowledge in LLMs. This is done by linearizing trees representing codes and proposing many loss functions that take into consideration linearized trees."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper shows that including prior structured knowledge is important\n- Experiments are solid and show what is the real impact of adding structured knowledge of code"
                },
                "weaknesses": {
                    "value": "- The way of including structured knowledge is by using sequences of tokens that are treated as sequences of tokens. Indeed, trees are serialized\n- No other ways of encoding trees are explored"
                },
                "questions": {
                    "value": "There is no comparison with other ways to include structures. Can you comment on this?\nMoreover, structures have been largely used in neural networks for NLP, e.g., TreeLSTM https://aclanthology.org/N16-1035/, KERMIT https://aclanthology.org/2020.emnlp-main.18/. How do the current work compares with those models?\n\n\n\nMinor: page 4, just after equation 1. k should be t"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6558/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698776088342,
            "cdate": 1698776088342,
            "tmdate": 1699636740730,
            "mdate": 1699636740730,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "mvRmCRMRQz",
            "forum": "YKIGyf215Q",
            "replyto": "YKIGyf215Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6558/Reviewer_cViY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6558/Reviewer_cViY"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the code language models. The authors explore a data-efficient adaptation of pre-trained code language models by further training and fine-tuning them with program structures. They represent programs as parse trees, refine a model with serialized parse trees, and fine-tune with structures. The experiments were conducted on several established datasets. The results show the effectiveness of the proposed approach."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors focus on an important area.\n2. To incorporate code structure in pretraining tasks is interesting."
                },
                "weaknesses": {
                    "value": "1. The experimental results may have some issues. \n2. The authors missed some related work."
                },
                "questions": {
                    "value": "The authors focus on an important area of code-related tasks. Existing pretraining models fail to consider the well-defined and unambiguous structures inherent in programming languages. To address this problem, the authors proposed their approach. The idea is overall interesting. However, I find some issues in this paper. \n\n1. The experimental results may have some issues. \n\nThe main issue is in the experiments. I find that in the experiments, the CodeT5 only achieves 27.5+ BLEU, 30+ CodeBLEU, and 16+ EM in the Concode dataset. However, in their paper[1], they achieved 41.48 BLEU, 44.10 CodeBLEU, and 22.70 EM, respectively. This phenomenon also exists in other tasks in this paper. The performance in this paper even performs worse than non-pretrained structure models [7]. I doubt the reliability of the experiments, which makes the entire paper unconvincing. Further, I find the CodeT5 (Structured) even performs worse than the original CodeT5 in EM of the Concode dataset. This cannot be strong evidence to show the effectiveness of the proposed approach.\n\n2. The authors missed some related work. \n\nThe authors aim to incorporate code structure in neural networks. However, there are some related work that is missed (e.g., [2-8]). \n\n\n\n[1] Wang, Y., Wang, W., Joty, S., & Hoi, S. C. (2021). Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859.\n\n[2] Rabinovich, M., Stern, M., & Klein, D. (2017, July). Abstract Syntax Networks for Code Generation and Semantic Parsing. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 1139-1149).\n\n[3] Yin, P., & Neubig, G. (2017, July). A Syntactic Neural Model for General-Purpose Code Generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 440-450).\n\n[4] Dong, L., & Lapata, M. (2016, August). Language to Logical Form with Neural Attention. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 33-43).\n\n[5] Sun, Z., Zhu, Q., Mou, L., Xiong, Y., Li, G., & Zhang, L. (2019, July). A grammar-based structural cnn decoder for code generation. In Proceedings of the AAAI conference on artificial intelligence (Vol. 33, No. 01, pp. 7055-7062).\n\n[6] Yin, P., & Neubig, G. (2018, January). TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (Demo Track).\n\n[7] Sun, Z., Zhu, Q., Xiong, Y., Sun, Y., Mou, L., & Zhang, L. (2020, April). Treegen: A tree-based transformer architecture for code generation. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 05, pp. 8984-8991).\n\n[8] Zhu, Q., Sun, Z., Zhang, W., Xiong, Y., & Zhang, L. (2022). Grape: Grammar-Preserving Rule Embedding. In IJCAI (pp. 4545-4551)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6558/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819833776,
            "cdate": 1698819833776,
            "tmdate": 1699636740619,
            "mdate": 1699636740619,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "lBqU9Jav6j",
            "forum": "YKIGyf215Q",
            "replyto": "YKIGyf215Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6558/Reviewer_adVs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6558/Reviewer_adVs"
            ],
            "content": {
                "summary": {
                    "value": "This paper first proposes a data-efficient adaptation method for the pre-trained code language models. Specifically, it \nrepresents the code programs as parse trees, called CSTs, and refines a model with serialized CSTs. Then it encourages the model to learn not only the associations of code text in different languages but also the mappings of their structures and grammars, with a small number of samples. The designed framework is applicable to both the encoder-decoder and decoder-only architectures. Comprehensive comparable experiments show the effectiveness of the designed model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This model framework is flexible and can be applied to both encoder-decoder and decoder-only models.\n2. The paper is well-organized and well-written with clear motivations, detailed discussion, nice figures, and sufficient comparison experiments, making it easy to follow and understand.\n3. This work performs comprehensive experiments over benchmark data to show the effectiveness and efficiency."
                },
                "weaknesses": {
                    "value": "1. The proposed data-efficient adaption method for the pre-trained code language models is not novel. The main idea of the model design is to convert the source code to the serialized form of CSTM, which I believe is not solid and sound enough.\n2. This work converts the source code into the serialized form of CST and further feeds these CST into LLMs. I am curious about the motivation for the design of CST. I know some existing methods convert source code into some formats. But what are the differences between CST and these existing methods?  What are the advantages of CST?\n3. This work claims that the model is much more efficient than existing methods. I would suggest that this work discusses the complexity of these methods to prove their efficiency."
                },
                "questions": {
                    "value": "1. What is the novelty of the model design? I do not think the serialized form of CSTM can be considered as the main novelty of this work.\n2. What are the differences between CST and these existing methods?  \n3. What are the advantages of CST?\n4. Discuss the complexity of these methods to prove their efficiency."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6558/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823358696,
            "cdate": 1698823358696,
            "tmdate": 1699636740472,
            "mdate": 1699636740472,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "x14D5UGYbh",
            "forum": "YKIGyf215Q",
            "replyto": "YKIGyf215Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6558/Reviewer_kKKV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6558/Reviewer_kKKV"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents structured finetuning for coding language models. Specifically, they represent programs as concrete syntax trees (CSTs), and perform continual pretraining and finetuning with the serialized CSTs. For encoder-decoder architectures, they also present several pretraining objectives using the tree structures, including masked subtree prediction and masked node prediction. They evaluate on several benchmarks including those for code translation, code generation and code summarization, and demonstrate that finetuning with the CST format improves the performance over the text-only baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Leveraging the code structure is a promising direction for improving coding language models.\n\n2. The authors conduct experiments on multiple important domains for coding applications."
                },
                "weaknesses": {
                    "value": "1. The novelty of the approach is very limited, with several missing references. First, the idea of representing programs as trees is not new. In particular, the paper misses the discussion of [1], which proposes to encode the input code in its syntax tree format, and also decode the syntax tree for code translation. On the other hand, [2] and other works cited in the paper (e.g., UniXCoder) also designed pretraining objectives that utilize the tree structure.\n\n2. This work argues that using CST is better than other formats such as AST; however, there is no empirical evidence to justify this claim. Adding this comparison is helpful, especially since AST has been used by several prior works.\n\n3. For code translation, it is better to use those benchmarks with unit tests, such as Transcoder [3] and MBXP [4], as other metrics without execution are less reliable.\n\n4. For MBPP results, the CodeGen paper reports a pass@1 of 7.46, which is much higher than the baseline reported in Figure 3. What causes the discrepancy? Also, it is helpful to report results with more samples, e.g., pass@10 and pass@100 as in the CodeGen paper.\n\n[1] Xinyun Chen, Chang Liu, Dawn Song, \"Tree-to-tree Neural Networks for Program Translation\", Advances in Neural Information Processing Systems, 2018.\n[2] Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao Liu, Li Li, Hao Wu, Jin Liu, Xin Jiang, \"SynCoBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code Representation\".\n[3] Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, Guillaume Lample, \"Unsupervised Translation of Programming Languages\n\", NeurIPS 2020.\n[4] Ben Athiwaratkun et al., Multi-lingual Evaluation of Code Generation Models, ICLR 2023."
                },
                "questions": {
                    "value": "1. Please clarify the novelty of this work and add missing references [1] [2].\n\n2. Please provide empirical comparison to justify the claim that using CST is better than other formats such as AST.\n\n3. For code translation, please use those benchmarks with unit tests, such as Transcoder [3] and MBXP [4].\n\n4. Please clarify the result discrepancy on MBPP, and report results with more samples, e.g., pass@10 and pass@100.\n\n[1] Xinyun Chen, Chang Liu, Dawn Song, \"Tree-to-tree Neural Networks for Program Translation\", Advances in Neural Information Processing Systems, 2018.\n[2] Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao Liu, Li Li, Hao Wu, Jin Liu, Xin Jiang, \"SynCoBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code Representation\".\n[3] Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, Guillaume Lample, \"Unsupervised Translation of Programming Languages\n\", NeurIPS 2020.\n[4] Ben Athiwaratkun et al., Multi-lingual Evaluation of Code Generation Models, ICLR 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6558/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699152442781,
            "cdate": 1699152442781,
            "tmdate": 1699636740337,
            "mdate": 1699636740337,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]