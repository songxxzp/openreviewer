[
    {
        "title": "Backdoor Contrastive Learning via Bi-level Trigger Optimization"
    },
    {
        "review": {
            "id": "cWMIQSweAY",
            "forum": "oxjeePpgSP",
            "replyto": "oxjeePpgSP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7033/Reviewer_Tcds"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7033/Reviewer_Tcds"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a poisoning-based backdoor attack for Contrastive Learning (CL), targeted at the feature extractor. Through the bi-level optimization that simulates a backdoored CL pre-trained model in the inner loop, it trains a trigger generator to produce poisoned samples with *robust* triggers that survive the data augmentation of CL. These triggers also exhibit transferability across serveral victim CL training strategies and backbone architectures. Experimental validation confirms the effectiveness, transferability and robustness of the attack."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The motivation behind the attack is well-described by incorporating the alignment and uniformity inherent in CL. The method for training the trigger generator is succinctly presented in an 11-line pseudocode.\n- The experiments are comprehensive, including experiments of CL backdoor defense and the transferability in three aspects (i.e., CL training strategies, model architectures and datasets).\n- The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "- **Lack of details.** The implementation of the proposed attack seems to lack some details:\n    - The setting of K and J are not included in the submission, and their relationship with N remains unclear. Is K large enough to ensure the convergence of the surrogate backdoored model?\n  - Additionally, does the x-axis in Figure 4 refer to N? If it includes the inner loop updates (N*J), does it make the comparison of loss curves somewhat unfair?\n  - What does the expression *regularly re-initialize the surrogate feature extractor* (in section 4) mean? Does it imply that, after the initialization (line2 in Algorithm1), there is a subsequent re-initialization at some point?\n- What determines superior transferable ability of the chosen surrogate CL framework (SimSiam)? The fundamental factors may need further analysis and clarification in ablation experiments, such as the choices of different data augmentations.\n- The BLTO procedure contains both a backdoor generator and a backdoored surrogate model \u03b8. Does the co-training surrogate backdoored model perform as well as an backdoored model actually trained on the poisoned data?"
                },
                "questions": {
                    "value": "- In the evaluation on transferability, the adopted backbone encoders are all of CNN architecture (e.g., ResNet, MobileNet, ShuffleNet, SqueezeNet), and the datasets are just CIFAR-10 and CIFAR-100. More diverse choices of backbone architecture and dataset may be necessary, such as the architurecture of ViT and more challenging datasets like ImageNet.\n- Besides, though the proposed attack targets at the feature extractor, the victim settings in the experiments are limited to the classification task. I think it could be extended to more tasks to demonstrate the effectiveness of the proposed attack."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7033/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698111491086,
            "cdate": 1698111491086,
            "tmdate": 1699636825869,
            "mdate": 1699636825869,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n3OUgtUQNh",
                "forum": "oxjeePpgSP",
                "replyto": "cWMIQSweAY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Tcds \u2014 Part 1/2"
                    },
                    "comment": {
                        "value": "> Q1. Lack of details: K, J and their relationship with N, Figure 4, re-initialization\n\nThank you for your sugguestion. We added all these details in the revised paper for better clarity:\n\n* In Algorithm 1, K and J depend only on the batch size and the total number of data points in the involved dataset. K is calculated via (len of backdoored dataset)/(batch size), and J is calculated via (len of clean dataset)/(batch size). On the other hand, N is independent from K and J (in our default settings, N is 400). We also provide a typical plot (inner and outer loss objectives with N steps) during the bi-level trigger optimization in Appendix C.7, and we can observe that such a design already leads to a converging loss curve.  \n\n* All data in Figure 4 reflects the victim's training stage: the victim is using the backdoored dataset to pre-train encoder via CL. The x-axis in Figure 4 refer to the **victim's training epoch**. At this stage, **the attacker has finished the trigger optimization and cannot intervene the victim's training process**.  Therefore, it is **fair** to make the comparison between our attack and others, as the difference only lies in different poisoning data injected in the victim's training set.\n\n\n* For \"regularly re-initialization\", we meant to re-initialize the surrogate feature extractor every N bi-level optimization steps and repeat the procedure for M times. In our experiment, we repeat for M=2 times. The reason to perform re-initialization is to prevent the trigger generator from overfitting to one particular surrogate models (so we reset it and retrain another surrogate model after certain time).\n\n\n> Q2. What determines superior transferable ability of the chosen surrogate CL framework (SimSiam)? \n\nNote that although we pick SimSiam as our default surrogate CL framework, our method could also work with other CL frameworks. Here we provide the attack performance (on CIFAR-10) with another two CL frameworks (BYOL and SimCLR) as surrogate models.\n\n| Surrogate Strategy |        |        | SSL Method |        |         |        |\n|:------------------:|:------:|:------:|:----------:|:------:|:-------:|:------:|\n|                    | SimCLR |        |    BYOL    |        | Simsiam |        |\n|                    |  ACC   |  ASR   |    ACC     |  ASR   |   ACC   |  ASR   |\n|      SimSiam       | 90.10% | 91.27% |   91.21%   | 94.78% | 90.18%  | 84.63% |\n|        BYOL        | 89.83% | 71.95% |   90.70%   | 76.40% | 90.06%  | 74.08% |\n|       SimCLR       | 89.27% | 91.45% |   90.13%   | 78.58% | 88.36%  | 87.62% |\n\nWe can observe that the attack transferability is not limited to a specific surrogate CL method. We have include these experimental results in Appendix C. 6 (Table 10). We believe the key reason is that though CL frameworks differ with each other in work flows (negative pairs, loss function designs, etc), their core mechanisms are similar (i.e., alignment and uniformity), as discussed in Section 3. This allows our trigger generator fits other CL frameworks easily once it succeed in backdooring one particular CL framework (i.e., the surrogate CL framework in our algorithm). Such a transferable ability across different CL frameworks is also reported by existing work [2].  \n\n> Q3. Does the co-training surrogate backdoored model perform as well as an backdoored model actually trained on the poisoned data?\n\nThanks for the question. We provide the surrogate model's performance in the following (framework: SimSiam, dataset: CIFAR-10, backbone: resnet18). For comprison, we also showed victim model's performance after the same training epoch as the surrogate model.\n\n\n|  | Surrogate Model | Actual Model| \n|:---:|:---:|:---:| \n| BA | 86.98% | 87.31% | \n| ASR | 96.72% | 92.74% | \n\nHere are our observation and analysis based on our above experimental records.\n\n* **For ASR,** the surrogate backdoored model performs better than an backdoored model actually trained on the poisoned data. This is easy to understand since the trigger generator is directly optimized from the surrogate backdoored model, making it fit the surrogate model better.\n\n* **For backdoor accuracy (BA),** the surrogate backdoored model performs similar with an backdoored model actually trained on the poisoned data, as they both learn knowledge from those clean data points."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700418632820,
                "cdate": 1700418632820,
                "tmdate": 1700418632820,
                "mdate": 1700418632820,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T2vwyf73sG",
                "forum": "oxjeePpgSP",
                "replyto": "cWMIQSweAY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Tcds \u2014 Part 2/2"
                    },
                    "comment": {
                        "value": "> Q4&Q5. More diverse choices of backbone architecture and dataset. Extended to more tasks beyond classification.\n\nThanks for your suggestion. We have added additional experiments on more model architectures: we evaluate our attack when the victim adopts **ViT** (e.g., ViT small/16 on ImageNet-100 via SimCLR), **RegNetX** and **RegNetY** (e.g., regnety_200mf, regnety_400mf on CIFAR-10 via SimCLR). The surrogate models use SimSiam (the default setting in our Table 1). Results on ViT can be found in Appendix C.4 (Table 8), and results on RegNetX, RegNetY is attached in Table 2. We also provide the results as follows:\n\n| Dataset: CIFAR-10 | RegNetX_200mf | RegNetY_400mf |\n|:---:|:---:|:---:|\n| BA | 87.50% | 89.31% |\n| ASR | 93.42% | 86.75% |\n\n| Dataset: ImageNet-100 | ViT-small/16 |\n|:---:|:---:|\n| BA | 63.39% |\n| ASR | 82.66% |\n\nAbove experimental results indicates that our BLTO attack presents remarkable effectiveness across more modern DNN backbones. Given time constraints, it is not quite possible for us to conduct even larger experiments on full ImageNet data. In terms of target learning task, we mainly focus on the classification task following the common practice in this area [3, 4, 5] and we would be happy to extend it into other tasks in our future work.  \n\n**References**\n\n[1] Tongzhou Wang and Phillip Isola. \"Understanding contrastive representation learning through alignment and uniformity on the hypersphere.\" PMLR 2020.\n\n[2] He, Hao et al. \"Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning.\" (ICLR 2023)\n\n[3] Li, Changjiang et al. \"An Embarrassingly Simple Backdoor Attack on Self-supervised Learning.\" (arXiv 2023, this is the official edited version of CTRL)\n\n[4] Saha et al. \"Backdoor Attacks on Self-Supervised Learning.\" CVPR 2022.\n\n[5] \"CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning.\" (arXiv 2023)"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700418738806,
                "cdate": 1700418738806,
                "tmdate": 1700418738806,
                "mdate": 1700418738806,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hdrFsOXS2H",
                "forum": "oxjeePpgSP",
                "replyto": "cWMIQSweAY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion Reminder"
                    },
                    "comment": {
                        "value": "We really appreciate the constructive comments from reviewer Tcds, which significantly enhance the quality of our work. As we approach the end of discussion stage, we would like to ask if there are any additional comments regarding our response, and we are more than happy to address them. Additionally, we would appreciate if you could consider updating your scores if our rebuttal has satisfactorily addressed your concerns. Thanks again for your time and effort in providing thoughtful feedback!"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713451474,
                "cdate": 1700713451474,
                "tmdate": 1700713451474,
                "mdate": 1700713451474,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PA3PM0lay1",
            "forum": "oxjeePpgSP",
            "replyto": "oxjeePpgSP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7033/Reviewer_KWn8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7033/Reviewer_KWn8"
            ],
            "content": {
                "summary": {
                    "value": "This paper identifies that the current data poisoning-based backdoor attacks on contrastive learning adopt a fixed trigger design and have a limited attack success rate. To overcome this limitation, a novel bi-level optimization approach is proposed. In this framework, the inner optimization simulates the contrastive learning (CL) dynamics of a surrogate victim, while the outer optimization optimizes the trigger generator and ensures that the backdoor trigger remains close to the target throughout the surrogate CL procedure. Extensive experiments are conducted to compare the proposed methods with state-of-the-art (SOTA) attacks, such as SSL backdoor and CTRL, demonstrating superior attack effectiveness. Furthermore, the proposed methods can effectively evade existing SOTA defenses."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed attack method is novel and shows superior effectiveness in comparison with the SOTA.\n\n2. The experiments are comprehensive; the authors compare the proposed method with different attack methods, evaluate it against backdoor defenses, and also discuss the effect of various data augmentations.\n\n3. The overall writing is good. The methodology and experimental results are not difficult to comprehend."
                },
                "weaknesses": {
                    "value": "1. The motivation could be better articulated. The authors claim that the fixed trigger design leads to limited ASR. However, in the methodology, not only is the trigger generator adopted, but a bi-level optimization strategy is also used to optimize the trigger generator. This raises the question: Is the trigger generator alone sufficient for the success of the proposed attack? If not, it suggests that the fixed trigger is not the primary cause of the current limitation. I recommend that the authors conduct an ablation study on this matter and be cautious with their claims.\n\n2. In the experiments, only SimSiam is adopted as the surrogate Contrastive Learning (CL)  method. The experimental results demonstrate that selecting this framework indeed achieves good performance, but it does not provide a direct rationale for choosing SimSiam. It is possible that using SimCLR or BYOL could yield better results, and it is recommended to supplement this part with additional experiments for verification.\n\n3. The comparison with other recently developed works could enhance the contribution:\n(1) PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning.\n(2) CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning.\nNotably, in \"PoisonedEncoder,\" a bi-level optimization strategy is also employed to formulate the attack. How does this work differ from theirs?"
                },
                "questions": {
                    "value": "1. Is the trigger generator alone sufficient for the success of the proposed attack?\n\n2. How does this work differ from the \"PoisonedEncoder\"?\n\n3. If the reference data $x_r$ is randomly sampled from the target class? does it need to be included in the downstream dataset to ensure the success of the attack?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7033/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698304601070,
            "cdate": 1698304601070,
            "tmdate": 1699636825762,
            "mdate": 1699636825762,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2bV7IGuySL",
                "forum": "oxjeePpgSP",
                "replyto": "PA3PM0lay1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KWn8 \u2014 Part 1/2"
                    },
                    "comment": {
                        "value": "Thank you for your constructive suggestions to strengthen our work. We hope our following response addresses your concerns, and we are more than willing to answer any follow-up questions.\n\n> Q1. Claim on \"fixed trigger design\" and ablation study using trigger generator alone.\n\nThanks for the insight. In fact, **our ablation study in Appendix C.3 (Table 7) was designed to answer this question**. Specifically, in Table 7, the case w/o inner optimization exactly corresponds to your desired case: the generator is used but the bi-level formulation is disabled (i.e., the generator is only optimized on a pretrained clean encoder). We copy the results as follows:\n\n|  |  | SSL Method |  |\n|:---:|:---:|:---:|:---:|\n|  | SimCLR | BYOL | Simsiam |\n|  | ASR | ASR | ASR |\n| bi-level optimization | 91.27% | 97.17% | 84.63% |\n| w/o inner optimization | 71.98% | 60.13% | 17.44% |\n\nObserve that without bi-level optimization, the generator still presented a certain level of backdoor effectiveness. This partially suggests the ineffectiveness of \"fixed trigger design\". However, simply adopting the generator is still not satisfactory and we found that utilizing bi-level optimization can further boost the performance.\n\nWe have also adjusted expressions about \"fixed trigger design\" in the paper to avoid possible misunderstanding. In our context, \"fixed trigger design\" indicates those triggers that are not optimized, such as fixed patch in SSL backdoor [1]. \n\n> Q2. Additional experiments using SimCLR or BYOL as the surrogate CL method.\n\nThanks for the insight. In fact, our bi-level trigger optimization is not restrictive to a specific CL framework, and all unsupervised CL methods can serve as surrogate methods. To verify this, our supplementary ablation study (**with SimCLR and BYOL as the sorrogate methods**) on CIFAR-10 is shown as follows (along with **SimSiam already presented in our Table 1**):\n\n| Surrogate Strategy |        |        | SSL Method |        |         |        |\n|:------------------:|:------:|:------:|:----------:|:------:|:-------:|:------:|\n|                    | SimCLR |        |    BYOL    |        | Simsiam |        |\n|                    |  ACC   |  ASR   |    ACC     |  ASR   |   ACC   |  ASR   |\n|      SimSiam       | 90.10% | 91.27% |   91.21%   | 94.78% | 90.18%  | 84.63% |\n|        BYOL        | 89.83% | 71.95% |   90.70%   | 76.40% | 90.06%  | 74.08% |\n|       SimCLR       | 89.27% | 91.45% |   90.13%   | 78.58% | 89.36%  | 87.62% |\n\nWe can observe that our BLTO attacks can be effective on various surrogate CL methods. We have include these experimental results in Appendix C. 6 (Table 10)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700418372923,
                "cdate": 1700418372923,
                "tmdate": 1700418372923,
                "mdate": 1700418372923,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vafDpJYri7",
                "forum": "oxjeePpgSP",
                "replyto": "PA3PM0lay1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KWn8 \u2014 Part 2/2"
                    },
                    "comment": {
                        "value": "> Q3. Comparison with PoisonedEncoder and CorruptEncoder. \n\nThanks for pointing out these works. We have added corresponding discussions of them in related works, preliminaries and methdologies.\n\n* **PoisonedEncoder[2]**: We notice that this work's threat model is different from ours: it intends to fool the victim's encoder to misclassify **attacker's select inputs** (not all inputs attached with trigger) into target categories. Therefore, it is not a backdoor attack method and thus the objectives in loss functions are actually different. Indeed, PoisonedEncoder also proposed a bi-level optimization fomulation. However, it finally resort to non-iterative heuristic solutions, potentially due to the challenges in solving their corresponding bi-level problem. We have included this discussion in related works section.\n\n* **CorruptEncoder[3]**: This is indeed a revelent work and we have **already discussed it in our related works**. We did not compare with CorruptEncoder mainly due to their unique triggered image generation design: they need to combine background images with objects in various ways. Such a design make it difficult to perform on small size image datasets, such as CIFAR-10 and CIFAR-100. Also they have not yet published the code or processed object/background images and thus it is nontrivial to reproduce the method at the current stage. \n\n> Q4. If the reference data is randomly sampled from the target class? Does it need to be included in the downstream dataset to ensure the success of the attack?\n\nThe reference data is randomly sampled from the target class. For instance, suppose \"truck\" is the target class, we randomly sample 500 \"truck\" images (from CIFAR-10) as reference data. We added more classifications to the procedure in **Appendix A.2**. \n\nThe reference data is not required to be included in the victim's downstream dataset to ensure the success of the attack. To verify this, we conduct a new experiment where we changed the downstream dataset to STL-10 dataset while the reference data is from CIFAR-10. The victim could use SimCLR, BYOL or SimSiam to train their ResNet-18 backbone. The downstream classifier is a linear DNN, similar to BadEncoder [4]. We added this evaluation in **Appendix C.6 (Table 15)** and list the results as follows:\n\n| victim's CL methods | SimCLR | BYOL | SimSiam|\n|:---:|:---:|:---:|:---:|\n| ACC/BA | 76.73% | 79.04% |80.47%|\n| ASR | 94.74% | 96.19% |87.55%|\n\nThe results indicate that our attack doesn't require the overlapping between reference data and downstream dataset to obtain a good attack performance.\n\n**References**\n\n[1] Saha et al. Backdoor Attacks on Self-Supervised Learning. CVPR 2022.\n\n[2] Hongbin Liu et al, \"PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning\" (Usenix Security 2022)\n\n[3] CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning.\n\n[4] Jia et al., BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning. IEEE S&P 2022."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700418416094,
                "cdate": 1700418416094,
                "tmdate": 1700418416094,
                "mdate": 1700418416094,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G7YU6vXwZl",
                "forum": "oxjeePpgSP",
                "replyto": "PA3PM0lay1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion Reminder"
                    },
                    "comment": {
                        "value": "We really appreciate the constructive comments from reviewer KWn8, which significantly enhance the quality of our work. As we approach the end of discussion stage, we would like to ask if there are any additional comments regarding our response, and we are more than happy to address them. Additionally, we would appreciate if you could consider updating your scores if our rebuttal has satisfactorily addressed your concerns. Thanks again for your time and effort in providing thoughtful feedback!"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713418356,
                "cdate": 1700713418356,
                "tmdate": 1700713418356,
                "mdate": 1700713418356,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Fm9kIBjqx4",
            "forum": "oxjeePpgSP",
            "replyto": "oxjeePpgSP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7033/Reviewer_wKXh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7033/Reviewer_wKXh"
            ],
            "content": {
                "summary": {
                    "value": "This paper formulates backdooring contrastive learning (CL) as a bi-level trigger encoder optimization problem. They claim that existing attacks using fixed triggers fail to maintain similarity between triggered data and target class in CL's embedding space due to data augmentation and uniformity effect, limiting their success. The proposed method formulates a bi-level optimization that simulates the victim's CL training dynamics in the inner level and optimizes a trigger generator in the outer level to keep triggered data close to the target throughout the inner CL training. This results in resilient triggers that can survive CL mechanisms. Experiments show the attack achieves success under varying victim settings and defenses. Analyses are provided on how CL mechanisms affect attack performance. The optimized triggers capture semantics related to the original input, explaining the attack's effectiveness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This work provides a formulation of the backdoor problem in contrastive learning as a bi-level optimization to identify a backdoor generator that is able to generate triggered images.\n\n- The authors provided an approximated solution to the formulated bi-level optimization.\n\n- The authors evaluated three datasets and compared them with two existing attacks. The attack is further evaluated with existing defenses from two lines of work (model-based backdoor trigger detection and model-based backdoor mitigation).\n\n- The writing is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "- The paper lacks analysis or discuss on the impact of using more accurate Hessian approximations to solve the proposed bi-level optimization, relying only on a discrete solution.\n\n- The baseline implementations and results seem questionable based on inconsistencies with original papers and recent related work- the attack success rates for SSL backdoor and CTRL differ notably from prior reported values.\n\n- The related work review and experiment scope is too narrow:\n   1. Recent attacks using similar target-class-based trigger synthesis are not compared to.\n   2. A relevant defense for detecting backdoor samples in contrastive learning is not discussed."
                },
                "questions": {
                    "value": "The proposed bi-level optimization formulation is solved using a discrete approximation without much discussion on the impact of using more efficient yet accurate Hessian approximations or evaluates the convergence of the proposed solution. Bi-level optimization often benefits from analyzing such approximations rather than directly providing a discretized solution.\n\nThe evaluation results comparing against baselines may contain erroneous implementations. In particular, the reported attack success rate (ASR) for SSL backdoor differs notably from values in the original paper, which because the low efficacy of their work, they even used a different metric based on number of misclassified samples. Also, the ASR for CTRL is much lower than results from recent works (e.g., the original paper and [1]) that show CTRL can achieve above 80% ASR with SimCLR on CIFAR-10, contrasting the authors' significantly lower values. Additional to the original papers, another separated work [1] evaluated these two attacks also confirms the potential erroneous implementations in this work.\n\nThe related work could be expanded and compared more thoroughly. Some recent attacks [2] also leverage synthesized triggers using solely the target class, similar to the proposed approach, which is worth to be incorporated and compare. Additionally, a recent backdoor sample detection method [1] demonstrates effectiveness in detecting poisoned samples in contrastive learning unlabeled datasets, which is relevant but not discussed or evaluated.\n\n[1] Pan, Minzhou, et al. \"ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms.\" Usenix Security (2023).\n\n[2] Zeng, Yi, et al. \"Narcissus: A practical clean-label backdoor attack with limited information.\" ACM CCS (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The results of implementation of related work (SSL backdoor and the CTRL) seems largely different than what has been reported in the original papers and recent work [1]. Not quiet sure if this requires a flag, just to make sure.\n\n[1] Pan, Minzhou, et al. \"ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms.\" Usenix Security (2023)."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7033/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7033/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7033/Reviewer_wKXh"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7033/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698709389369,
            "cdate": 1698709389369,
            "tmdate": 1700864786625,
            "mdate": 1700864786625,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XPUZn961VL",
                "forum": "oxjeePpgSP",
                "replyto": "Fm9kIBjqx4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wKXh \u2014 Part 1/2"
                    },
                    "comment": {
                        "value": "Thank you for the comments to help clarify our work. We hope our following response addresses your concerns, and we are more than willing to answer further questions.\n\n> Q1. The baseline implementations and results seem questionable based on inconsistencies attack success rates for SSL backdoor and CTRL from prior works.\n\nWe are sorry for the confusion and we believe it is just a misunderstanding. We have provided detailed discussion and make it clear in **Appendix C.8**. \n\nIn fact, **the ASR difference is due to the contrastive learning (CL) implementation adopted by the victim**, which can be seen from the model accuracy on clean data: using our CL implementation, the victim encoder can achieve almost $90\\%$ BA (i.e., ACC) on CIFAR-10; while with the original CTRL's CL implementation, the encoder achieves $80\\%$ ACC on CIFAR-10 (i.e., Table 1 in CTRL [1]). \n\nIn the following, we provide detailed evidences (e.g., CL implementation difference, related works and codes) to demonstrate the fidelity of our experiment on SSL backdoor and CTRL. We hope this can clarify your doubts about our implementation and result. \n\n1. **Difference in CL implementation that results in different ASRs and ACCs**: our CL implementation for victim follows standard contrastive learning practices (i.e., SimCLR [4]), which differs from CTRL's CL implementation in 1) **data normalization**, and 2) **batch-wise data augmentation strategy**. Specifically, in CTRL's official implementation, it did not use instance-specific data augmentation within the batch during training and data normalization during testing, which leads to an encoder with a **lower ACC/BA and a higher ASR**.\n\n\n2. **Reason of not using CTRL's CL implementation**: note that in our threat model, the attacker cannot control how the victim train the actual CL model. In practice, victims would prefer adopting CL that produces a more accurate encoder with higher ACC. In fact, our CL implementation follows standard practices with matching ACC: SimCLR (Table B.5 in [4]) reports **90.6% ACC** for ResNet on CIFAR-10, which is close to our reported **90% ACC / BA** in Table 1. However, the SimCLR implemented in CTRL only achieves **80.5% ACC** (Table 1 in CTRL [1]). Besides, CTRL's CL on ImageNet-100 has **42.2% ACC**, which is even lower than ACC on the whole ImageNet (around **69% ACC** in Table 6 of [4]). Therefore, we believe that we actually considered a more practical scenario for the victim's CL implementation.  \n\n3. **Existing work [3] reports similar ASR as ours (on CTRL and SSL backdoor):** our reported ASR of CTRL and SSL backdoor on ImageNet-100 is consistent with those in [3]. To be specific, Table 1 and Table 2 in [3] report **CTRL** on ImageNet100 with **28.8% ASR** and **SSL backdoor** with **14.3% ASR**, and their **ACCs / BAs** are about **70%**. These results in general match ours on ImageNet-100, e.g., CTRL with **35.62% ASR**, SSL backdoor with **13.10% ASR**, and their **70% ACC / BA**). Note that  **70% ACC / BA** in our works and [3] matches common CL practices, higher than **40% ACC / BA** in CTRL (Table 1 of CTRL [1]).\n\n4. **An anonymous GitHub repository is provided to reproduce our results and CTRL's results**, please check ***https://anonymous.4open.science/r/CTRL_correctness-9D51***. Here in the victim contrastive learning procedure, the victim uses SimCLR to train a ResNet18 backbone on **backdoored** CIFAR-10 (as in CTRL [1] Table 1 and ASSET [2] Table 5). Directly excuting the following Python files in the repository can reproduce CTRL's and our results:\n    - `main_train.py`: CTRL's contrastive learning implementation following their GitHub repository \"https://github.com/meet-cjli/CTRL/tree/master,\" which can report a **similar ASR (80+%)** and **similar ACC (80+%)** in CTRL [1]. \n    - `main_train_my.py`: our contrastive learning implementation, which reports CTRL's performance with similar **ASR (30+%)** and **ACC (90+%)** in our paper. The major differences in our CL implementation are detailed above and in this anonymous repository."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700417560115,
                "cdate": 1700417560115,
                "tmdate": 1700417560115,
                "mdate": 1700417560115,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sRkKaaHBgt",
                "forum": "oxjeePpgSP",
                "replyto": "Fm9kIBjqx4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wKXh \u2014 Part 2/2"
                    },
                    "comment": {
                        "value": "> Q2. Discuss on using more accurate Hessian approximations to solve the proposed bi-level optimization.\n\n\nWe sincerely thank you for the suggestion. We added a **convergence plot Figure 11** for solving this bi-level optimization problem, and we can observe that our adopted interleaving update strategy is already effective and leads to convergent training loss. Note that such a design is natural and widely adopted in many different problems (such as [5][6]). In fact, [7] actually showed that omitting the second derivatives does not lead to major performance change on the bi-level solution. While Hessian approximations could potentially provide a more accurate solution, it could also bring a higher computation complexity and time cost due to the calculation on the second derivatives, which limits the attack practicability on large-scale datasets. We add this discussion in **Appendix C.7**.\n\n\n> Q3. Related work discussion on backdoor attack [8] and backdoor sample detection [2].\n\nThank you for the suggestions. We have added discussion to these works [2, 8] in **Section 2**.\n\n**Comparison with Narcissus [8]**: Narcissus is primarily designed for clean-label backdoor attack in **supervised learning**, which is different from our contrastive learning setting. Specifically, **this attack synthesizes triggers using not only the target class, but also a supervised surrogate model (obtained from labeled datasets)**, while our threat model assumes no access to such a supervised surrogate model by default but trains a surrogate encoder via contrastive learning on an unlabeled dataset. Due to this difference in attack knowledge, this work is not directly comparable of our work but we have discussed it in the updated paper.\n\n**Evaluation under ASSET [2]**: while we have provided an evaluation against multiple backdoor defenses in Section 5.3, we are happy to further evaluate under this suggested one and include it in Section 5.3 and Appendix C.5. Specifically, on our backdoored CIFAR-10 data (with poisoning rate 1%), we used ASSET to sift backdoored data points, and caluclated True Positive Rate **TPR = 5.39%** and False Positive Rate **FPR = 32.17%**. This low TPR and high FPR indicate that our backdoor can still effectively resist ASSET's detection. \n\n**References**\n\n[1] Li, Changjiang et al. An Embarrassingly Simple Backdoor Attack on Self-supervised Learning. (arXiv 2023, this is the official edited verison of CTRL)\n\n[2] Pan, Minzhou, et al. \"ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms.\" Usenix Security (2023).\n\n[3] Zhang, Jinghuai, et al. \"CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning.\" arXiv preprint arXiv:2211.08229 (2022).\n\n[4] Ting Chen, et al. \"A Simple Framework for Contrastive Learning of Visual Representations\" ICML 2020.\n\n[5] Hanxun Huang, et al. \"Unlearnable Examples: Making Personal Data Unexploitable\" ICLR 2021.\n\n[6] Aleksander Madry et al. \"Towards Deep Learning Models Resistant to Adversarial Attacks\" ICLR 2018.\n\n[7] Chelsea Finn, et al. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" ICML 2017.\n\n[8] Zeng, Yi, et al. \"Narcissus: A practical clean-label backdoor attack with limited information.\" ACM CCS (2023)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700418117671,
                "cdate": 1700418117671,
                "tmdate": 1700418117671,
                "mdate": 1700418117671,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wWJ7lneYbN",
                "forum": "oxjeePpgSP",
                "replyto": "XPUZn961VL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7033/Reviewer_wKXh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7033/Reviewer_wKXh"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Q1"
                    },
                    "comment": {
                        "value": "Thank you to the authors for the detailed explanation and for sharing the code anonymously. Despite this, I still have some unresolved concerns. My primary issue pertains to the significant disparity in the Attack Success Rate (ASR) on the Cifar-10 dataset, as compared to other studies. Specifically, paper [1] reports results of ACC: 80.5%, ASR: 85.3%, and paper [2] reports ACC: 85.3%, ASR: 81.4%. In contrast, the results in your paper show ACC: 89.84% and ASR: 32.18%, which is particularly notable since all three studies utilize the same SimCLR and ResNet-18 as the target model.\n\nTo address this, it might be beneficial if you could provide data from a slightly earlier stage in the model's training, for example, when the ACC is around 85% or 80%. Reporting the associated ASR at these points could offer additional insights and might help in identifying what could be missing in the current analysis. This could be particularly revealing for the CTRL attack and might provide a deeper understanding of its effects. While I appreciate your current response, these additional details could be instrumental in fully addressing my concerns."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600515002,
                "cdate": 1700600515002,
                "tmdate": 1700600515002,
                "mdate": 1700600515002,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8I3NMLkVPB",
                "forum": "oxjeePpgSP",
                "replyto": "sRkKaaHBgt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7033/Reviewer_wKXh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7033/Reviewer_wKXh"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Q2"
                    },
                    "comment": {
                        "value": "Thank you to the authors for sharing the convergence graph. While it illustrates the effectiveness of your attack, my query regarding the Hessian was not to seek empirical evidence of convergence. Instead, I suggest that the authors include a discussion about the decision to avoid explicit methods in solving the bi-level optimization.\n\nFrom my perspective, it would be beneficial to discuss both the advantages (such as points a, b, c) and potential drawbacks (like points d, e, f) of your discrete computational approach, and how these were addressed through empirical analysis. A theoretical exploration of convergence would be highly valuable, though I recognize the challenges it presents given the limited time.\n\nAdditionally, I recommend that the paper should at least include a discussion on how similar works in the field have approached bi-level optimization problems, particularly those that have also employed discrete methods. If possible, an ablation study comparing these approaches would further strengthen your work. Essentially, this would serve to justify the technical choices made in your algorithm, rather than just validating its convergence empirically."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601081165,
                "cdate": 1700601081165,
                "tmdate": 1700601081165,
                "mdate": 1700601081165,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u0llJ4ICiD",
                "forum": "oxjeePpgSP",
                "replyto": "8I3NMLkVPB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7033/Reviewer_wKXh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7033/Reviewer_wKXh"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Q3"
                    },
                    "comment": {
                        "value": "Thank the authors for the additional discussions on additional attacks and detection methods. I'd like to highlight why a comparison with the Narcissus attack could enhance the paper's contribution and emphasize its novelty:\n\n1. Both this work and Narcissus use a surrogate feature extractor trained with target-class data for the outer optimization phase.\n2.  Each method employs discrete inner and outer optimization processes.\n3. The key difference lies in the inner optimization: Narcissus aims to universally minimize supervised learning loss towards the target class, **making backdoored samples more akin to the target class.** In contrast, this paper employs the **contrastive learning structures** to achieve the goal of generating backdoored samples that are embedding-space similar to the target class, albeit through different technical means.\n\nWhile this difference stems from varying threat models, a side-by-side comparison with Narcissus would be beneficial. It would not only highlight the technical novelty of your approach, particularly in the use of contrastive learning elements like augmentations and contrastive loss, but also enhance the overall understanding of your paper's contribution.\n\nRegarding the ASSET results, I appreciate you providing them. Including these in the paper could greatly benefit its depth and relevance, as ASSET is a solid baseline in contrastive learning, which many current baselines (such as Neural Cleanse, CLP, I-BAU, and Distillation) do not specifically evaluate over contrastive learning."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604353360,
                "cdate": 1700604353360,
                "tmdate": 1700604353360,
                "mdate": 1700604353360,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "H20Zdv0PYW",
                "forum": "oxjeePpgSP",
                "replyto": "u0llJ4ICiD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7033/Reviewer_wKXh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7033/Reviewer_wKXh"
                ],
                "content": {
                    "title": {
                        "value": "The Space of Improvements"
                    },
                    "comment": {
                        "value": "I find the results of this work technically solid and interesting. However, there are concerns that need to be addressed to enhance the paper's technical contribution and experimental validity. These include:\n\n1. A more thorough comparison and discussion with the Narcissus attack.\n2. A justification for choosing a discrete approach to resolve bi-level optimization, especially if it deviates from explicit or implicit methods of approaching hypergradient. The current naming of the paper as related to bi-level optimization may be misleading in this context.\n3. Unresolved issues in the reimplementation of SSL backdoor and the CTRL experiments.\n\nWhile I appreciate the authors' effort in responding to my comments, the current state of the paper requires further refinement in several areas. This includes improving the presentation, better highlighting the technical novelties, and delving deeper into related work for more insightful analysis.\n\nBased on these considerations, I am inclined to maintain my current rejection rating, providing space for the authors to improve the work. I believe the proposed attack has solid results, and with the suggested enhancements, the paper has the potential to significantly improve."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604913460,
                "cdate": 1700604913460,
                "tmdate": 1700604913460,
                "mdate": 1700604913460,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cRIefile1Q",
            "forum": "oxjeePpgSP",
            "replyto": "oxjeePpgSP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7033/Reviewer_Saos"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7033/Reviewer_Saos"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method to perform poisoning-based\nbackdoor attacks on contrastive learning. It searches\noptimal triggers by using a designed bi-level optimization\napproach on the surrogate models. Experiments on three\ndatasets (CIFAR-10, CIFAR-100, and ImageNet-100) validate\nthe effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Backdoor attacks on contrastive learning is an important direction.\n\n* Analysis of the reason that the proposed attack works\nwell is discussed in section 5.4."
                },
                "weaknesses": {
                    "value": "* The proposed method requires a surrogate to perform\nbi-level optimization. This paper mentions it uses SimSiam\nwith ResNet18 as the surrogate model. The success of the\nproposed method is based on the transferability of the\ntriggers optimized on the surrogate model. While Table 1 and\n2 demonstrate the proposed method has good transferability,\nthe evaluation is not comprehensive. It is suggested to\ninclude more self-supervised learning methods such as Jigsaw\n[1], MoCoV2 [2], and DINO [3]. For different model\narchitectures, the results under modern architectures such\nas ViT and RegNetY are missing (Note that ViT and RegNetY\nare commonly used in self-supervised learning related\nresearches like\nhttps://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md).\nSince the transferability is very important to the\nperformance of the method, it is suggested to conduct more\nextensive experiments. For example, adding the results under\ndifferent architectures to Table 1 or adding the results\nunder different CL methods to Table 2 (So that it will have\na Table including the results under different combinations\nof architectures and CL methods). \n\n* The hyper-parameters such as the learning rates of the\nmodels might also influence the performance of the proposed\nmethod. For example, if the learning rates and the batch\nsizes of the surrogate model are significantly different from\nthose used by the victim models, then the attack success\nrates might also reduced. It is suggested to add the\ndiscussion about this.\n\n* In the experiments, this paper assumes that the downstream\ndataset and the pre-training dataset used for self-supervised\nlearning is the same. Typically, the downstream users will\nuse different datasets to conduct the downstream training.\nMany existing works such as BadEncoder [4] also mainly\ninvestigate this practical scenario. The results under this\npractical scenario are missing in this paper.\n\n* Carlini et al. [5] is an important existing work in the\nfield of poisoning-based backdoor attacks on contrastive\nlearning. Although it mainly focuses on the vision-language\ncontrastive learning, the comparisons, and the discussion\nabout it is still important.\n\n* The robustness under Feng et al. [6] is not discussed. Is the\nbackdoor samples in the proposed method have high cosine\nsimilarity between each other?\n\n* The usages of the surrogate models and the bi-level\noptimization is not new in the field of poisoning attacks\nand backdoor attacks [7,8], which somehow weaken the\ncontributions of this paper.\n\n\n\n[1] Noroozi et al., Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles. ECCV 2016.\n\n[2] Chen et al., Improved Baselines with Momentum Contrastive Learning. arXiv 2020.\n\n[3] Caron et al., Emerging Properties in Self-Supervised Vision Transformers. ICCV 2021.\n\n[4] Jia et al., BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning. IEEE S&P 2022.\n\n[5] Carlini et al., Poisoning and Backdooring Contrastive Learning. ICLR 2022.\n\n[6] Feng et al., Detecting Backdoors in Pre-trained Encoders. CVPR 2023.\n\n[7] Souri et al., Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch. NeurIPS 2022.\n\n[8] Zhu et al., Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy. arXiv 2023."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7033/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7033/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7033/Reviewer_Saos"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7033/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698986191713,
            "cdate": 1698986191713,
            "tmdate": 1700732649343,
            "mdate": 1700732649343,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DbInqYFOWd",
                "forum": "oxjeePpgSP",
                "replyto": "cRIefile1Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Saos \u2014 Part 1/2"
                    },
                    "comment": {
                        "value": "Thank you for your constructive suggestions to strengthen our work. We hope our following response addresses your concerns, and we are more than willing to answer any follow-up questions.\n\n> Q1. Include more self-supervised learning methods such as Jigsaw, MoCoV2, and DINO, and more model architectures such as ViT and RegNetY in Table 1 and Table 2.\n\nWe appreciate your suggestion and have included more results in **Table 2, Appendix C.4 (Table 8) and Appendix C.6 (Table 14)**.\n\n**More self-supervised learning methods**: we further evaluate the effectiveness of our attack when the victim uses **MoCo** [1] on CIFAR-10, and the surrogate model uses SimSiam (the same setting with our Table 1). Results are reported in C.6 (Table 14) and as follows:\n\n|  | Non-backdoor | SSL backdoor | CTRL | Ours |\n|:---:|:---:|:---:|:---:|:---:|\n| ACC/BA | 90.38% | 89.97% | 90.07% | **90.14%** |\n| ASR | 10.18% | 12.22% | 26.81% | **81.92%** |\n\nAccording to above results, our attack maintains effective in MoCo and can still outperform existing CL attacks prominently.\n\n\n**More model architectures**: we evaluate our attack when the victim adopts **ViT** [2] (e.g., ViT small/16 on ImageNet-100 via SimCLR), **RegNetX** and **RegNetY** [3] (e.g., regnety_200mf, regnety_400mf on CIFAR-10 via SimCLR). The surrogate models uses SimSiam (the same setting with our Table 1). Results on ViT can be found in Appendix C.4 (Table 8), and results on RegNetX, RegNetY is attached in Table 2. We also provide the results as follows:\n\n| Dataset: CIFAR-10 | RegNetX_200mf | RegNetY_400mf |\n|:---:|:---:|:---:|\n| BA | 87.50% | 89.31% |\n| ASR | 93.42% | 86.75% |\n\n| Dataset: ImageNet-100 | ViT-small/16 |\n|:---:|:---:|\n| BA | 63.39% |\n| ASR | 82.66% |\n\nAbove experimental results indicates that our BLTO attack presents remarkable effectiveness across more modern DNN backbones.\n\n> Q2. Discussion on varying hyper-parameters used in surrogate model and victim model. \n\nThanks for pointing out this. We have added a suggested analysis in **Appendix C.6** and list the results as follows.\n\n**Surrogate model**: when optimizing the trigger, by default, we train a surrogate ResNet18 model on CIFAR-10 via Simsiam. We set batch size=512, and use a dynamic learning rate (with a cosine scheduler, base lr=0.03, final lr=0).\n\n**Victim model**: suppose the victim trains a ResNet18 backbone on CIFAR-10 via SimCLR. To evaluate whether different hyper-parameters used by victim will influence the attack effectiveness, we vary the batch size of victim in {256, 512, 1024} and scale surrogate model's learning rate by {$\\times 0.5$, $\\times 1$, $\\times 2$}:\n\n| Batch size | 256 | 512 |1024|\n|:---:|:---:|:---:|:---:|\n| ACC/BA | 86.05% | 90.10% |90.85%|\n| ASR | 91.48% | 91.27% |89.39%|\n\n| learning rate | $\\times$ 0.5 | $\\times$ 1 | $\\times$ 2|\n|:---:|:---:|:---:|:---:|\n| ACC/BA | 88.05% | 90.10% |88.76%|\n| ASR | 91.80% | 91.27% |92.54%|\n\nFrom above results, we find that our attack can retain effectiveness even though the victim's CL methods, hyper-parameters differ from the attacker's.\n\n\n> Q3. Discussion on using different pre-training dataset and downstream dataset. \n\nThanks for this important insight. In fact, **our evaluation in Table 3 was designed for such practical scenario**. For example in Table 3, the column 1% indicates that 1% of the pre-training dataset contains CIFAR-10 data (i.e., 500 CIFAR-10 data points) and the rest 99% are CIFAR-100 data (i.e., 49500 CIFAR-100 data points), while the downstream dataset is 100% CIFAR-10 data (i.e., 50000 CIFAR-10 data points). In this case, **the downstream dataset** and **the pre-training dataset are not the same**, while our attack success rate is still remarkable.\n\nWe now further consider a more challenging case when the victim uses backdoored **CIFAR-10** for pre-training and uses **STL-10** for downstream evaluation. The victim could use SimCLR, BYOL or SimSiam to train their ResNet-18 backbone. The downstream classifier is a linear DNN, similar to BadEncoder [4]. We added this evaluation in **Appendix C.6 (Table 15)** and list the results as follows:\n\n| Victim's CL methods | SimCLR | BYOL | SimSiam|\n|:---:|:---:|:---:|:---:|\n| ACC/BA | 76.73% | 79.04% |80.47%|\n| ASR | 94.74% | 96.19% |87.55%|\n\nThe results indicate that our attack doesn't require the overlapping between pre-trained dataset and downstream dataset to obtain a good attack performance."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700418183968,
                "cdate": 1700418183968,
                "tmdate": 1700418183968,
                "mdate": 1700418183968,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7sTvCxGsXQ",
                "forum": "oxjeePpgSP",
                "replyto": "cRIefile1Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Saos \u2014 Part 2/2"
                    },
                    "comment": {
                        "value": "> Q4. Discussion on existing work Carlini et al. [5] in the field of poisoning-based backdoor attacks on contrastive learning.\n\nThanks for your pointing out this interesting work. We have discussed it in **Section 2**, and here we provide our understanding on this work.\n\n\n[5] backdoors the CLIP model which targets on image-text pairs $(a, b)$. It works by inserting backdoor trigger in image $a$ and adjusting the text in $b$ that corresponds to a downstream label of interest, such that the pre-trained encoder can associate the trigger with the target text. In this sense, the backdoor mechanism in [5] is still similar to supervised learning setting where the text can be regarded as the \"label\" for image. \n\nOur backdoor attack focuses on a different contrastive learning (CL) scenario with a single image modality $a$, and thus there is no \"label\" information at all. Therefore, the difficulty lies in how to enforce the encoder to associate the trigger with a target category, which is not relevent to [5]'s strategy. \n\n> Q5. The robustness under Feng et al. [6] is not discussed. Is the backdoor samples in the proposed method have high cosine similarity between each other?\n\nThanks for poiting out this possible defense. We are happy to further evaluate our attack under this defense method via trigger inversion in **Section 5.3 and Appendix C.5**. Specifically, we examined our backdoored ResNet18 encoder (pretrained on the backdoored CIFAR-10 via SimCLR using the same configuration in our Table 4 and Table 5) and report the results for L1-norm and P1-norm here. \n\n|  | clean encoder (SimCLR) | backdoored encoder (SimCLR) | clean encoder (BYOL) | backdoored encoder (BYOL) |\n|:---:|:---:|:---:|:---:|:---:|\n| L1-norm | 582.56 | 719.58 | 638.37 |  604.38|\n| P1-norm (backdoored when < 0.1) | 0.19 | 0.23 | 0.20 | 0.20 |\n\nAccording to the default threshold in [6], if the P1-norm $<0.1$, it's likely that the encoder is \"backdoored\". The results show that P1-norm can not really differentiate clean and backdoored encoder. We argue that this is because our backdoor trigger is image-specific (generator-based), which somehow provide the robustness over these trigger inversion defenses (including [7, 8] covered in our paper).\n\n\n> Q6. The usages of the surrogate models and the bi-level optimization is not new [9, 10], which somehow weaken the contributions of this paper.\n\nWe agree that these techniques are indeed widely used, even in a broader field than ML security. However, we believe **our main contribution is identifying the weakness of existing CL attacks** (e.g., the failure of compromising CL's uniformity mechanism discussed in Section 3 and Section 5.4), **which naturally motivates us to formulate our trigger optimization problem**. The surrogate model and bi-level optimization techniques are just tools for us to realize our novel motivation. Therefore, in our viewpoints, our contribution doesn't lie in the adoption of \"surrogate models and bi-level optimization\" themselves, but in how we formulate our desired goals and use these tools to solve them.\n\n**References**\n\n[1] Chen et al., \"Improved Baselines with Momentum Contrastive Learning.\" arXiv 2020.\n\n[2] Alexey Dosovitskiy, et al., \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" ICLR 2021.\n\n[3] Xu, Jing, et al., \"RegNet: Self-Regulated Network for Image Classification\" TNNLS 2023.\n\n[4] Jia et al., \"BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning.\" IEEE S&P 2022.\n\n[5] Carlini et al., \"Poisoning and Backdooring Contrastive Learning.\" ICLR 2022.\n\n[6] Feng et al., \"Detecting Backdoors in Pre-trained Encoders.\" CVPR 2023.\n\n[7] Bolun Wang et al., \"Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks.\" IEEE S&P 2019\n\n[8] Mengxin Zheng et al., \"SSl-cleanse: Trojan detection and mitigation\nin self-supervised learning.\" CVPR 2023\n\n[9] Souri et al., \"Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch.\" NeurIPS 2022.\n\n[10] Zhu et al., \"Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy.\" arXiv 2023."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700418215791,
                "cdate": 1700418215791,
                "tmdate": 1700418215791,
                "mdate": 1700418215791,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jepbe1DprB",
                "forum": "oxjeePpgSP",
                "replyto": "cRIefile1Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion Reminder"
                    },
                    "comment": {
                        "value": "We really appreciate the constructive comments from reviewer Saos, which significantly enhance the quality of our work. As we approach the end of discussion stage, we would like to ask if there are any additional comments regarding our response, and we are more than happy to address them. Additionally, we would appreciate if you could consider updating your scores if our rebuttal has satisfactorily addressed your concerns. Thanks again for your time and effort in providing thoughtful feedback!"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713363995,
                "cdate": 1700713363995,
                "tmdate": 1700713363995,
                "mdate": 1700713363995,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KE44XOH7Eo",
                "forum": "oxjeePpgSP",
                "replyto": "7sTvCxGsXQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7033/Reviewer_Saos"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7033/Reviewer_Saos"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed response. Although I am not satisfied with the response to Q3 and Q4, I increased my score to 6 since other concerns are addressed, especially the experiments demonstrate the proposed method has strong transferability to different victim models. For Q3, it is suggested to include the results for more (upstream dataset, downstream dataset) pairs. For Q4, my question about the similarity between the features of the backdoor samples is not answered. I know the proposed attack uses the image-specific trigger and DECREE focuses on the static trigger, but the backdoored models might still be detected if the formalization of the trigger can be optimized (such as using the trigger formalization proposed in UNICORN [1]) as long as the backdoor samples have high feature space cosine similarity between each other.\n\n[1] Wang et al., UNICORN: A Unified Backdoor Trigger Inversion Framework. ICLR 2023."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732635921,
                "cdate": 1700732635921,
                "tmdate": 1700732635921,
                "mdate": 1700732635921,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]