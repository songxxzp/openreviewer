[
    {
        "title": "Local Expert Diffusion Models for Efficient Training in Denoising Diffusion Probabilistic Models"
    },
    {
        "review": {
            "id": "oJYMQCBnsN",
            "forum": "cdIyf38mRw",
            "replyto": "cdIyf38mRw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1057/Reviewer_arQF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1057/Reviewer_arQF"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to improve the training efficiency of current diffusion models. The authors propose a MDM (Multi-expert Diffusion Model), based on the observation that DMs exhibit different convergence rates and training patterns at different time steps. Experiments show improvements in the training efficiency compared to three baseline models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The training efficiency of the diffusion model is an important research question worth exploring.\n\n- The proposed method shows improvements on CIFAR-10 and ImageNet-32 over baselines in terms of training time and generative results."
                },
                "weaknesses": {
                    "value": "I have several concerns/questions about the current manuscript, which makes me believe that it is not yet ready for publication.\n\n- In the introduction, the authors claim that \u201ctraining each time step $x_t$ is conducted independently (Song et al., 2020)\u201d. This statement looks very confusing and unclear to me. DDPMs are still trained in a sequential manner, my sense is that the authors may want to refer to the fact that the starting time step $t$ is essentially uniformed sampled from [1,T] (corresponding to line 3 of DDPM training algorithm)?  However, this does not mean that each diffusion time step is independent in training, which also contradicts the nature of a Markovian process.\n\n- Following my previous point, if each interval of diffusion steps is trained in an independent manner, then how can we perform inference, especially when the denoising steps reach the intersection of those independent trained intervals? Essentially, we need to condition on the previous $x_t$ to estimate the $x_{t-1}$, but I am confused how this could be done if $t$ happens to be the end step of the proposed MDm method.\n\n- Still in the intro, \u201cWe interpret that fast convergence can be achieved by minimizing negative interactions across different time intervals?\u201d What are those negative interactions? These are not explained in the manuscript.\n\n- In the captions of Fig.2, \u201cwe average five adjacent points to filter \u2026\u201d what are those five points? Also, the eight $\\tau$ notations come with no explanations, making the figure difficult to read.\n\n- In Table 1, why precisions and recalls are used in image generations?\n\n- In the training dynamics analysis, are there any particular reasons for which the process is divided into 8 intervals, and the methodological design adopts three-interval designs?\n\n- In terms of experiments, the current experiments are only performed with CIFAR-10 and ImageNet-32, which are both low in image resolutions, making the experimental results less convincing.\n\n- Also, can authors provide insights on why the proposed MDM can improve the generative quality?"
                },
                "questions": {
                    "value": "Please see the Weaknesses for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1057/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1057/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1057/Reviewer_arQF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1057/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697489401121,
            "cdate": 1697489401121,
            "tmdate": 1699636031987,
            "mdate": 1699636031987,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "QxvLPgBGaZ",
            "forum": "cdIyf38mRw",
            "replyto": "cdIyf38mRw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1057/Reviewer_qZGE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1057/Reviewer_qZGE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a Multi-expert Denoising Diffusion Model (MDM) to improve training efficiency for diffusion model. The key idea is to partition the diffusion process into separate time intervals and train an expert model independently on each interval. This allows exploiting more parallel resources and faster convergence of experts on their specialized time steps."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper targets a highly relevant and challenging practical problem, the training efficiency of diffusion models.\n- The multi-expert approach is well-motivated through detailed analysis of diffusion model training dynamics across time steps.\n- The method demonstrably provides substantial gains in training time and compute requirements."
                },
                "weaknesses": {
                    "value": "- More analysis could be provided on how sensitive performance is to the time step divisions between experts.\n- Only evaluated on CIFAR and downsampled ImageNet datasets. Scaling up to higher resolutions not shown.\n- While the paper is generally well-organized, the layout could be refined to enhance reader engagement, particularly in terms of the placement of figures and tables relative to the corresponding text."
                },
                "questions": {
                    "value": "n/a"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1057/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1057/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1057/Reviewer_qZGE"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1057/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698589131366,
            "cdate": 1698589131366,
            "tmdate": 1699636031892,
            "mdate": 1699636031892,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "DqeUqo0eH0",
            "forum": "cdIyf38mRw",
            "replyto": "cdIyf38mRw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1057/Reviewer_8QH1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1057/Reviewer_8QH1"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to divide the time interval of diffusion models into three parts and train an expert model within each sub-interval separately. The paper carried out analysis of convergence speed and attention map statistics to determine the best way of splitting the time interval. Experimental results indicate that the MoE approach converges faster than the baseline in terms of both TC and WTC."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is clearly written and easy to follow. Before getting to the designed division of time interval, detailed analysis of training dynamics and attention map statistics have been carried out, making the motivation clearer."
                },
                "weaknesses": {
                    "value": "- My main concern is that this mix of expert (MoE) idea for diffusion models has already been proposed by many existing works, such as eDiff-I, RAPHAEL and SDXL, with similar conclusions and observations to the ones in this paper. The novelty seems very marginal. \n\n- The result in Table 1 seems a bit weird. As been mentioned in \"variational diffusion model\", noise schedule shouldn't affect the training objective and therefore shouldn't affect the final sample quality, except for the endpoints of the schedule. Maybe that's an indication that runs in table 1 are not full converged yet. \n\n- At the first glance of figure 3(a) it seems that t8 is challenging with very slow convergence. However, it doesn't necessarily mean that t8 is difficult, perhaps it is just that the training objective is not weighted properly across difference time steps.\n\n- The paper mentioned that the first and third time intervals require more training iterations to converge. Have you investigated that just make these two intervals narrower such that all three intervals can converge in about the same pace? A general and automatic recipe on choosing time intervals for MoE can be more interesting.\n\n- In Section 4.2 they proposed to use std of attention softmax logits as a measurement of concentration. But why not calculating entropy, which exactly represents such characteristic?"
                },
                "questions": {
                    "value": "Please see comments above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1057/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817239696,
            "cdate": 1698817239696,
            "tmdate": 1699636031794,
            "mdate": 1699636031794,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "eEyp5IrD3H",
            "forum": "cdIyf38mRw",
            "replyto": "cdIyf38mRw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1057/Reviewer_Rh6m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1057/Reviewer_Rh6m"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces dedicated expert models for different diffusion model time steps. Motivated by the fact that training independent models can maximize parallel efficiency, the authors present several observations that motivate the grouping of the diffusion time steps and training them independently for both improved efficiency and performance. The empirical effectiveness is supported by two small scale experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. Although the idea of using dedicated models for different diffusion time steps is by no means new or surprising, the authors reveal some very interesting observations that can be useful for future research. For example, the authors report different convergence speeds for time steps, where it's quite counter-intuitive that t1 demonstrates slower convergence speed compared to middle steps.\n\n2. The paper is presented in very good writing quality."
                },
                "weaknesses": {
                    "value": "My main concern with the paper is the contribution. \n\nWe use a shared model for all diffusion time steps mainly for the sake of overall model size (parameters). And it is very natural that using more dedicated parameters can improve the results of the diffusion model. \n\nAs also discussed by the authors in Section 2, using expert models for different diffusion steps is not a new idea and has been used before. And I respectfully disagree with the authors' discussions regarding the methods such as eDiff-I (last paragraph on page 2).\nIn this paragraph, the authors discussed two main differences/advantages of the proposed method compared to the previous methods like eDiff-I, but neither is solid. \n\n1. Model pretraining\n\nAs discussed in eDiff-I, they do pretraining mainly to reduce the training complexity. They do not have to rely on pretraining to maintain the same final results. I don't find the pretraining of the existing work indispensable. And this can hardly be considered a disadvantage.\n\n2. Conditional image generation v.s. unconditional image generation\n\nThe authors argue that they do unconditional image generation which 'affects various applications.' However, there is no evidence that expert-model methods for conditional image generation cannot work for unconditional image generation. And methods like eDiff-I directly target the most challenging text-to-image generation while the efficacy of the proposed method is only evaluated on two 32x32 small-scale experiments. Therefore, the contribution of this paper is further weakened. \n\nEmpirical evaluations on two 32x32 image generation tasks are not sufficient to support the idea. At least some higher-resolution unconditional experiments with 255x256 LSUN datasets are the minimum. Given the A6000 GPU used for the current results, the additional high-resolution experiments should be absolutely affordable."
                },
                "questions": {
                    "value": "Please discuss the clear advantages of the proposed method compared to the existing diffusion model with expert models."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1057/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699061200890,
            "cdate": 1699061200890,
            "tmdate": 1699636031737,
            "mdate": 1699636031737,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]