[
    {
        "title": "Can AI-Generated Text be Reliably Detected?"
    },
    {
        "review": {
            "id": "ejcYLdwjGU",
            "forum": "NvSwR4IvLO",
            "replyto": "NvSwR4IvLO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6773/Reviewer_reDY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6773/Reviewer_reDY"
            ],
            "content": {
                "summary": {
                    "value": "The authors highlight the potential weaknesses of AI-generated text detectors such as neural-network based detectors, zero-shot AI text detection, watermarking, and information retrieval-based detectors. Specifically, the authors propose a recursive paraphrasing attack that recursively paraphrases an AI generated text until the text is likely to be classified as non AI-generated. Experimental results using 2000 text passages with each roughly 300 tokens in length from the XSum dataset and 2 different models used for paraphrasing suggest the attack is significantly effective against a wide range range of AI-generated text detectors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The authors address the important problem of reliable AI-generated text detection. This problem is likely to become increasingly important with the rapid rise in popularity of large language models in society.\n\n* The proposed approach is simple yet effective in undermining the reliability of current AI-generated text detectors. The simplicity of the attack may also make it more likely to generalize to different models and domains.\n\n* The authors perform a human evaluation using Amazon's Mechanical Turk (MTurk) to evaluate the resulting paraphrased text passages. Their results suggest the original content of the text passage is preserved in addition to the grammar and overall text quality.\n\n* The authors also discuss the overall hardness of AI text detection, providing a formal upper bound of detection performance based on the total variation (TV) between AI-generated and human-generated text distributions.\n\n* The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "* The experiments only use text passages from a single dataset, and tend to only evaluate a single model for each detector type. Evaluations on a wider range of datasets and detectors would greatly strengthen any generalizable claims regarding the proposed paraphrasing attack.\n\n* Lack of baseline methods. In many of the experiments, the proposed attack is the only method being evaluated, have the authors compared their attack with similar attacks?\n\n* I appreciate the human evaluation in the \"Watermarked AI Text\" section, however, this type of evaluation is missing in the other experimental sections. For example, Figure 5 claims a significant drop in detector accuracy with minimal degradation in text quality; however, it's unclear to me how significantly text quality degrades based on a perplexity score increase from 6.15 to 13.55.\n\n* The insight that smaller TV between AI-generated and human-generated text distributions leads to more difficult detection problems seems rather obvious. Although the authors show that more complex models can lead to smaller TV distances, the authors do not provide any empirical evidence that smaller TV distances actually lead to more difficult AI text detection.\n\n* There is no empirical runtime evaluation of the proposed attack.\n\n* There are several grammatical errors throughout the paper, consider using a service like Grammarly to fix these issues.\n\n* Figure 9 is not colorblind friendly."
                },
                "questions": {
                    "value": "* How is TV distance defined, and why is it difficult to compute for larger datasets?\n\n* How did the authors determine 5 rounds of paraphrasing to be sufficient?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6773/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697860703926,
            "cdate": 1697860703926,
            "tmdate": 1699636781063,
            "mdate": 1699636781063,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iR9Db9PcHR",
                "forum": "NvSwR4IvLO",
                "replyto": "ejcYLdwjGU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6773/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6773/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reDY Part 1/2"
                    },
                    "comment": {
                        "value": "We thank you for your review and for noting the problem that we address as \u201cincreasingly important\u201d. We are delighted to know that you find our approach to be \u201csimple yet effective\u201d and our paper to be \u201cwell-written\u201d. We address your comments below. We also invite you to read our global response, where we discuss common comments and new experiments in detail.\n\n---\n\n> Evaluations on a wider range of datasets and detectors would greatly strengthen any generalizable claims regarding the proposed paraphrasing attack.\n\nThank you for your comment. In our revised Appendix A, we add more results of our attacks with *all* the detectors on different domains \u2013 PubMedQA (a medical text dataset) and Kafkai (Deepfake text detection by Pu et al., based on reviewer JvAi\u2019s suggestion) \u2013 and models (OPT-1.3B and GPT-2-Medium). Consistent with our previous results in the main paper, we are able to break all the detectors in all the new experimental settings that we consider. \n\n---\n\n> Lack of baseline methods. In many of the experiments, the proposed attack is the only method being evaluated, have the authors compared their attack with similar attacks?\n\nOur work is the first to show the limitations of 4 different classes of detectors such as watermarking, retrieval-based, zero-shot, and trained detectors. To the best of our knowledge, we are not aware of other baseline attacks that break these detectors. As we discussed in our paper, previous works proposed weaker attacks via span replacements. However, Kitchenbauer et al. (2023) show that watermarking is robust to such attacks. Please find more details regarding our contributions in our Global response.\n\n---\n\n> I appreciate the human evaluation in the \"Watermarked AI Text\" section, however, this type of evaluation is missing in the other experimental sections. For example, Figure 5 claims a significant drop in detector accuracy with minimal degradation in text quality; however, it's unclear to me how significantly text quality degrades based on a perplexity score increase from 6.15 to 13.55.\n\nThank you for this comment. As reviewer Ce9G notes, the results in Figure 5 were run on a shorter and smaller dataset when compared to the ones in the \u201cWatermarked AI Text\u201d section. Hence, it had a few very short text samples, which resulted in high perplexity scores. To be consistent with the \u201cWatermarked AI Text\u201d section (and obtain improved tradeoffs between our attack performance and text quality), we perform experiments on *all* the detectors with longer and larger datasets in the revision. \n\nWe also modified Figure 5 in the revision based on the new dataset. As shown in the Figure, the perplexity only drops from 7.6 to 9.3 (measured using a larger OPT-13B model), while the detection rate degrades from 100% to below 60%, showing a clearer tradeoff between our attack performance and text quality. We agree with the reviewer that having a complimentary human evaluation can be insightful for all detectors. Unfortunately, performing such large-scale human studies with all the datasets and detectors would be expensive for us. However, we have provided several examples of paraphrased texts in Appendix B.2.  More experimental results are also provided in Appendix A.\n\n---\n\n> Although the authors show that more complex models can lead to smaller TV distances, the authors do not provide any empirical evidence that smaller TV distances actually lead to more difficult AI text detection.\n\nOur theoretical result (Theorem 1 and Figure 7) shows that as the total variation between human and AI-generated text decreases, the performance of even the best possible detector also decreases. This implies that smaller TV distances will lead to more difficult AI text detection.\n\nAnother way of understanding this phenomenon could be by considering the true and false positive rates (TPR and FPR). For a given FPR (say, 1%), the goal of a detector would be to maximize the TPR. However, the difference between TPR and FPR is bounded by the total variation. Thus, if the total variation decreases, the best-possible TPR also decreases, making detection harder. With this in mind, it is sufficient to empirically show that the TV decreases as AI models become more sophisticated.\n\nNote that showing detection is difficult with respect to a particular detector does not imply the nonexistence of a better detector capable of distinguishing human and AI-generated texts. Our objective is to show a fundamental difficulty in detection. Thus, we first show that detection performance decreases with a decreasing TV (Theorem 1). Then, we provide empirical evidence that TV decreases as AI models become more sophisticated.\n\n---\n\n> There is no empirical runtime evaluation of the proposed attack.\n\nThank you for the comment. We evaluate the time taken for our recursive paraphrase attacks (for 5 rounds) for AI passages of 300 tokens in length to be 36 seconds per passage. We add this to our revised Appendix A.1."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6773/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508328539,
                "cdate": 1700508328539,
                "tmdate": 1700508328539,
                "mdate": 1700508328539,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QTNfFKKcoV",
                "forum": "NvSwR4IvLO",
                "replyto": "ejcYLdwjGU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6773/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6773/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reDY Part 2/2"
                    },
                    "comment": {
                        "value": "> Figure 9 is not colorblind friendly\n\nWe have updated Figure 9 with a colorblind-friendly color palette.\n\n---\n\n> How is TV distance defined, and why is it difficult to compute for larger datasets?\n\nWe use the definition of TV that measures the distance as half of the difference between the probability density functions, i.e., the TV between two distributions P and Q is given by:\n\nTV(P, Q) = $\\frac{1}{2}\\sum_x |P(x) - Q(x)|$\n\nThe difficulty of computing this distance for text distributions does not come from the size of the datasets. Instead, it is due to the size of the sample space. The distribution of text sequences of n tokens from the token set $T$ has the sample space $T \\times T \\times T \\times$ \u2026 (n times). The size of this space is $|T|^n$, which is exponential in the sequence length n. In order to compute TV, we need to estimate the probability density for each element in the sample space with a high degree of confidence. This significantly increases the number of samples needed to estimate the TV for larger sequence lengths.\n\n---\n\n> How did the authors determine 5 rounds of paraphrasing to be sufficient?\n\nThe number of recursion rounds is a hyperparameter, and we chose it to be 5 without any specific reason. More rounds of recursion might further deteriorate detection rates with further tradeoffs in text quality. Note that just 2 rounds of recursions are enough to degrade the watermark detection rates to below 50% in all the settings (revised Figure 10, Appendix A.1)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6773/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508563655,
                "cdate": 1700508563655,
                "tmdate": 1700508585970,
                "mdate": 1700508585970,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C8RTEPcr96",
                "forum": "NvSwR4IvLO",
                "replyto": "ejcYLdwjGU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6773/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6773/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Any remaining concerns?"
                    },
                    "comment": {
                        "value": "Dear reviewer reDY,\n\nWe thank you for your thoughtful comments and feedback. Since we are nearing the end of the discussion phase, we would like to know if you have any remaining concerns regarding our revised paper or additional experiments. We\u2019d be happy to address them."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6773/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652896282,
                "cdate": 1700652896282,
                "tmdate": 1700652896282,
                "mdate": 1700652896282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DwSSE6zLMt",
                "forum": "NvSwR4IvLO",
                "replyto": "C8RTEPcr96",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6773/Reviewer_reDY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6773/Reviewer_reDY"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their detailed response, however after reading the response and the concerns brought up by other reviewers, I am inclined to retain my original score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6773/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668846285,
                "cdate": 1700668846285,
                "tmdate": 1700668846285,
                "mdate": 1700668846285,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "L4d3VQbLGR",
            "forum": "NvSwR4IvLO",
            "replyto": "NvSwR4IvLO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6773/Reviewer_Ce9G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6773/Reviewer_Ce9G"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a study to showcase whether the AI-generated text can be reliably detected. For that, the authors have performed several experiments by transforming the text through recursive para-phrasing and showcasing the vulnerability of existing detection/defense algorithms. Further, the authors also showcase the spoofing attack which aims to label the genuine text into the AI-generated text by the defense algorithm."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It is highly important to understand the limitations of current AI-generated text detection algorithms and to accurately identify the generated text to protect privacy and ethics."
                },
                "weaknesses": {
                    "value": "* The biggest concern is with the para-phrasing task. Should be called the para-phrased text generated by any LLMs? Does it not destroy the inherent characteristics of the generated model? Have the original texts also been paraphrased by the paraphrases? What's the impact of para-phrasing genuine texts?\n* Ablation study of different paraphrasers? What is the contribution here? Do the authors have directly used the existing algorithms for phrasing? With the existence of several studies, the current paper leaves a low contribution concerning the sensitivity of LLM detectors.\n\n[1] Krishna K, Song Y, Karpinska M, Wieting J, Iyyer M. Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense. NeurIPS 2023\n[2] Kumarage T, Sheth P, Moraffah R, Garland J, Liu H. How Reliable Are AI-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts. EMNLP. 2023.\n\n* The experimental setting is weak. It is not clear how many samples have been used (2000 or 1000).\n* Are these existing detectors trained using multiple augmentation strategies such as data of multiple LLMs and/or para-phrased samples?\n* For retrieval-based detectors only 100 samples have been used reflecting inconsistency in the experimental setup. Figure 5 also shows the drastic drop in PPI value.\n* A detailed experimental study is needed concerning experiments in sections 4 (i) and 4 (ii). Only 3-length tokens are used along with a single-layer LSTM network. Further, the decrement shown in Figure 9, is statistically significant?"
                },
                "questions": {
                    "value": "Please check the weakness section.\n\n--------------- Post Rebuttal --------------\n\nThanks for responding. However, in light of serious concerns such as the destruction of inherent characteristics of LLMs through paraphrasing, existing works on similar themes, and limited evaluation, I would like to retain my original rating.\n\nThe authors can conduct the analysis when human texts are also paraphrased and while augmenting the data, these paraphrased texts can also be used. If we used original (human and AI) and augmented (paraphrased human and AI), will it still increase type-1 (or 2) but decrease the other one?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6773/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6773/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6773/Reviewer_Ce9G"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6773/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698652307390,
            "cdate": 1698652307390,
            "tmdate": 1700658338391,
            "mdate": 1700658338391,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6hoIgE7wlD",
                "forum": "NvSwR4IvLO",
                "replyto": "L4d3VQbLGR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6773/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6773/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Ce9G Part 1/2"
                    },
                    "comment": {
                        "value": "We thank you for your review and for noting our research problem of analyzing the limitations of AI text detectors to be \u201chighly important\u201d. We address your comments below. We also invite you to read our global response, where we discuss common comments and new experiments in detail.\n\n---\n\n> Have the original texts also been paraphrased by the paraphrases? What's the impact of para-phrasing genuine texts?\n\nWe only paraphrase the LLM generations and **not** the original human text. Paraphrases of original human text would be considered AI text since it is generated by a paraphrasing language model. Hence, we do not paraphrase them using a language model to retain them as genuine human texts.\n\n---\n\n> The biggest concern is with the para-phrasing task. Should be called the para-phrased text generated by any LLMs?\n\nThe paraphrased text should be considered as AI text by definition since it is output by a language model. For example, if a user queries a watermarked ChatGPT to paraphrase their essay, the paraphrased output will be watermarked and hence detected as AI-generated.\n\n---\n\n > Does it not destroy the inherent characteristics of the generated model?\n\nYes, it might destroy the inherent characteristics of the generated model. Intuitively, this might be the reason why our attack works. For example, paraphrasing a watermarked text might remove the watermark patterns, destroying the inherent watermarked LLM text characteristics. This is what an attacker desires.\n\n---\n\n> What is the contribution here? Do the authors have directly used the existing algorithms for phrasing? With the existence of several studies, the current paper leaves a low contribution concerning the sensitivity of LLM detectors.\n\nAfter receiving advice from the program chairs, we can note that a draft of our work had appeared on a public platform before the related papers were made public. To keep anonymity, we cannot reveal the name of the platform or give a link to it. Nevertheless, ours is the first work to comprehensively show the limitations of 4 different classes of text detectors \u2013 watermark-based, retrieval-based, zero-shot, and trained detectors. Though we use existing paraphraser models, we are the first to propose *recursive paraphrasing attacks* to effectively break the stronger retrieval-based detector by Krishna et al. (2023) and watermark-based detectors. We are the first to present *spoofing attacks* on text detectors that can potentially affect the reputation of LLM developers. We also present novel theoretical results that indicate that reliable AI text detection could get increasingly difficult as LLMs evolve. Please find our contribution details on page 3 (last paragraph).\n\n---\n\n> Ablation study of different paraphrasers? \n\nTo our knowledge, DIPPER by Krishna et al. (2023) is the strongest open-source paraphraser that exists. We perform recursive paraphrase attacks only using DIPPER since weaker paraphrasers will generate texts of lower quality with recursion. We would like to highlight again that our empirical contributions here are the algorithms that we designed for the attacks (recursive paraphrasing and spoofing) and not the paraphraser model.\n\n---\n\n> The experimental setting is weak. It is not clear how many samples have been used (2000 or 1000).\n\nThank you for the comment, we have revised the paper to make this clearer. We use a total of 2000 samples, 1000 per human and AI text classes.\n\nWe further perform more experiments on all the detectors with larger and longer samples in the revised Appendix A. We consider different domains \u2013 PubMedQA (a medical text dataset) and Kafkai (Deepfake text detection by Pu et al., based on reviewer JvAi) \u2013 and multiple target LLMs (OPT-1.3B and GPT-2-Medium). We consider 1000 to 2000 samples per experiment in all these settings. Consistent with our previous results, we are able to break all the detectors with a slight tradeoff in text quality.\n\n---\n\n> Are these existing detectors trained using multiple augmentation strategies such as data of multiple LLMs and/or para-phrased samples?\n\nTo the best of our knowledge, the existing open-sourced trained detectors do not use augmented/paraphrased data for training. However, our Corollary 2 presents a fundamental tradeoff between type-1 and type-2 errors. It indicates that as detectors become more robust to paraphrases (by training on paraphrased samples), potentially more human passages will be wrongly flagged as AI text. Thus, reducing type-2 errors might lead to an increase in type-1 errors, which is not desirable.\n\nCorollary 2 also indicates that as AI-paraphrasers become more human-like, this tradeoff will become more significant. Hence, the fundamentally hard task of text detection will become increasingly difficult as AI-paraphrasers evolve."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6773/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509050266,
                "cdate": 1700509050266,
                "tmdate": 1700509050266,
                "mdate": 1700509050266,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vsQaIhFtfO",
                "forum": "NvSwR4IvLO",
                "replyto": "L4d3VQbLGR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6773/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6773/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Any remaining concerns?"
                    },
                    "comment": {
                        "value": "Dear reviewer Ce9G,\n\nWe thank you for your thoughtful comments and feedback. Since we are nearing the end of the discussion phase, we would like to know if you have any remaining concerns regarding our revised paper or additional experiments. We\u2019d be happy to address them."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6773/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652847182,
                "cdate": 1700652847182,
                "tmdate": 1700652847182,
                "mdate": 1700652847182,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JeXojqH3RI",
                "forum": "NvSwR4IvLO",
                "replyto": "L4d3VQbLGR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6773/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6773/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to additional post-rebuttal concerns by Reviewer Ce9G"
                    },
                    "comment": {
                        "value": "We thank you for engaging in the discussion. We have tried to address your concerns below. We hope our responses clarify your concerns. \n\n---\n\n> serious concerns such as the destruction of inherent characteristics of LLMs through paraphrasing\n\nAs we discuss in our rebuttal response, we do not believe that destroying inherent characteristics of the target LLM outputs is a concern; rather, this is what an attacker desires. We want to highlight that the paraphrasing attacks we perform **remove these inherent LLM signatures while maintaining the context, meaning, and quality of the text** (please see our human study in Table 1 and Appendix B.1). Hence, an attacker can effectively evade detection via automated paraphrasing. \n\nFor instance, suppose an attacker uses a watermarked LLM to generate propaganda. They can paraphrase the watermarked propaganda to remove the watermark patterns (which is the inherent watermarked LLM characteristic) while maintaining the content and quality of the text. In this manner, the attacker can increase their chances of evading the watermark detector. \n\nPlease let us know if this explanation addresses your concern.\n\n---\n\n> existing works on similar themes\n\nOur work is the first to analyze the vulnerabilities of four different classes of existing detectors. As we mentioned previously, based on the advice we received from the Program Chair, **we can reveal that a draft of our work was released on a public platform even before the related works** (including Krishna et al. and Kumarage et al. mentioned by the reviewer) were made public. However, in order to preserve anonymity, we can not give more details or link to the draft.\n\nIn spite of this, ours is the first work to comprehensively analyze the limitations of 4 different categories of text detectors. We are the first to **break and provide state-of-the-art attack results** against the stronger watermarking (Kirchenbauer et al. 2023) and retrieval-based (Krishna et al. 2023) detectors. We are the first to reveal a new vulnerability of these text detectors to **spoofing attacks** where an adversarial human can write a text that is detected to be AI-generated. We are also the first to theoretically show results that indicate the **hardness of AI text detection**.\n\n---\n\n> limited evaluation\n\nPlease note that based on your previous review, we **added more experimental settings to Appendix A of our revised draft**. Here, we analyze all the detectors with large and long datasets (datasets varying from 1000 to 2000 passages, with each passage 200 to 300 tokens in length). We also evaluate the text quality using perplexity and MTurk human studies. One can always add more datasets and models to a paper, but we feel that our experimental results are comprehensive enough to reveal the vulnerabilities of existing detectors.\n\n---\n\n> The authors can conduct the analysis when human texts are also paraphrased, and while augmenting the data, these paraphrased texts can also be used.\n\nIf we understand the comment correctly, you are suggesting the use of data augmentations via paraphrased passages to train detectors. We believe this is an interesting idea for future work. But please note that our focus in this paper is to reveal vulnerabilities of existing detectors, and training new (and perhaps more robust) detectors is not in the scope of this paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6773/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670907288,
                "cdate": 1700670907288,
                "tmdate": 1700670907288,
                "mdate": 1700670907288,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sUpHCBGFhP",
            "forum": "NvSwR4IvLO",
            "replyto": "NvSwR4IvLO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6773/Reviewer_JvAi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6773/Reviewer_JvAi"
            ],
            "content": {
                "summary": {
                    "value": "The authors claim current methods for detecting AI-generated text from LLMs are ineffective, and their proposed recursive paraphrasing attack can bypass detectors. Watermarking techniques are also vulnerable and can be fooled by their proposed method for misidentifying human text as AI-generated. The authors claim, the challenge of distinguishing AI from human text is fundamentally difficult, as evidenced by a proposed theoretical model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Well-Written and Structured Content also the research tackles an increasingly important topic in the AI community. \n- The paper's focus on recursive paraphrasing attacks represents an innovative and practical contribution to the field of AI security by showing that these attacks can effectively remove watermarks from AI text. \n- The paper supports its practical experiments with theoretical proofs, providing a deep understanding of the problem space."
                },
                "weaknesses": {
                    "value": "The paper does not include testing on a diverse array of datasets like the M4 or Deepfake text detection, which encompasses Multi-generator, Multi-domain, and Multi-lingual data. Incorporating these datasets could provide a more comprehensive evaluation of the paraphrasing model's effectiveness across different text generation sources, domains, and languages.\n\nPu, J., Sarwar, Z., Abdullah, S. M., Rehman, A., Kim, Y., Bhattacharya, P., ... & Viswanath, B. (2023, May). Deepfake text detection: Limitations and opportunities. In 2023 IEEE Symposium on Security and Privacy (SP) (pp. 1613-1630). IEEE.\n\nWang, Yuxia, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Chenxi Whitehouse et al. \"M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection.\" arXiv preprint arXiv:2305.14902 (2023).\n\nRecursively paraphrased text could potentially suffer from semantic drift, where the meaning changes or degrades with each paraphrase iteration. How do you address the concern of maintaining semantic integrity and coherence with only perplexity metrics in text after multiple rounds of paraphrasing?"
                },
                "questions": {
                    "value": "Question 1: Recursively paraphrased text could potentially suffer from semantic drift, where the meaning changes or degrades with each paraphrase iteration. How do you address the concern of maintaining semantic integrity and coherence in text after multiple rounds of paraphrasing? \n\nQuestion 2: Could you elaborate on how perplexity (without semantic understanding) and other quality metrics have been validated to accurately reflect the readability and coherence of the paraphrased text?\n\nQuestion 3: Given that detection methods are constantly evolving, how might adaptive detectors, which are designed to learn and counteract paraphrasing patterns over time, impact the effectiveness of the DIPPER paraphrasing model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6773/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699139973857,
            "cdate": 1699139973857,
            "tmdate": 1699636780816,
            "mdate": 1699636780816,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D7wAKjgGAq",
                "forum": "NvSwR4IvLO",
                "replyto": "sUpHCBGFhP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6773/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6773/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to JvAi"
                    },
                    "comment": {
                        "value": "We thank you for your review and for noting our paper to be \u201cwell-written and structured\u201d. We are happy to find that the reviewer believes that we tackle an \u201cincreasingly important topic in the AI community\u201d, and our attacks and theory are \u201cinnovative and practical\u201d. We address your reviews below. We also invite you to read our global response, where we discuss common comments and new experiments in detail.\n\n---\n\n> The paper does not include testing on a diverse array of datasets... Incorporating these datasets could provide a more comprehensive evaluation of the paraphrasing model's effectiveness across different text generation sources, domains, and languages.\n\nThank you for your comment. In our revised Appendix A, we add more results of our attacks on different domains \u2013 PubMedQA (a medical text dataset) and Kafkai (Deepfake text detection by Pu et al., based on your review). We also evaluate the attacks on two target LLMs \u2013 OPT-1.3B and GPT-2-Medium. Consistent with our previous results in the main paper, we are able to break all the detectors in all the new experimental settings that we consider. \n\n---\n\n> How do you address the concern of maintaining semantic integrity and coherence with only perplexity metrics in text after multiple rounds of paraphrasing?  Could you elaborate on how perplexity (without semantic understanding) and other quality metrics have been validated to accurately reflect the readability and coherence of the paraphrased text?\n\nAs you rightly note, metrics in NLP, such as perplexity, have their limitations. To evaluate the semantic quality of our recursive paraphrasing framework, we perform MTurk human evaluations. As shown in Section 2.2 (Table 1) and Appendix B.1, the human evaluators scored 70% of our recursive paraphrases to have high-quality content preservation. 89% of the recursive paraphrases were scored to have high text quality or grammar.\n\n---\n\n> Given that detection methods are constantly evolving, how might adaptive detectors, which are designed to learn and counteract paraphrasing patterns over time, impact the effectiveness of the DIPPER paraphrasing model?\n\nThis is an interesting question that can be answered using Corollary 2 (Appendix C.2) in our paper. Here we discuss a fundamental tradeoff of AI text detection in the presence of paraphrasing. Corollary 2 indicates that if a detector becomes more robust to AI paraphrasing, type-I errors will increase, and more human passages will be wrongly flagged as AI text by the detector. This shows a tradeoff between type-1 and type-2 errors of AI text detectors.\n\nCorollary 2 also indicates that as AI-paraphrasers become more human-like, this tradeoff will become more significant. Hence, the fundamentally hard task of text detection will become increasingly difficult as AI-paraphrasers evolve."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6773/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507982656,
                "cdate": 1700507982656,
                "tmdate": 1700507982656,
                "mdate": 1700507982656,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Th9MtEcIxK",
                "forum": "NvSwR4IvLO",
                "replyto": "sUpHCBGFhP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6773/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6773/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Any remaining concerns?"
                    },
                    "comment": {
                        "value": "Dear reviewer JvAi,\n\nWe thank you for your thoughtful comments and feedback. \nSince we are nearing the end of the discussion phase, we would like to know if you have any remaining concerns regarding our revised paper or additional experiments. We\u2019d be happy to address them."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6773/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652789823,
                "cdate": 1700652789823,
                "tmdate": 1700652810988,
                "mdate": 1700652810988,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]