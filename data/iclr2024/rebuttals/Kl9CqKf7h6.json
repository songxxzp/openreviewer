[
    {
        "title": "FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent"
    },
    {
        "review": {
            "id": "M9PothyLjX",
            "forum": "Kl9CqKf7h6",
            "replyto": "Kl9CqKf7h6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2230/Reviewer_xGw7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2230/Reviewer_xGw7"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes FedHyper, which adopts Hypergradients to dynamically adjust learning rate for global learning rate (LR), server-side local LR, and client-side local LR. The authors demonstrate the effectiveness of FedHyper on various datasets and models, including image classification and language modeling tasks. FedHyper is shown to outperform other FL learning rate scheduling algorithms in terms of convergence rate and final accuracy on various datasets and models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written and easy to understand, with clear explanations and well-designed experiments. FEDHYPER is a versatile algorithm that can seamlessly integrate with and augment the performance of existing optimization algorithms. This versatility makes it a valuable tool for researchers and practitioners working in the field of FL."
                },
                "weaknesses": {
                    "value": "The proposed FedHyper highly relies on the existing method, i.e., hypergradient. It's unclear why the learning rate can be updated following Eq. (4-5). It would be better to give the intuitive explanation for this. FedHyper does not discuss the issue of training cost in FL."
                },
                "questions": {
                    "value": "- Why are the baseline methods different for the three settings in Figure 3?\n- Could you please provide a more detailed and intuitive explanation of hypergradients? I\u2019m looking for a thorough understanding, and I\u2019m willing to give a higher score for a comprehensive explanation.\n- Is the learning rate fixed for baseline methods like FedAvg?\n- Since FedHyper requires updating the learning rate using current and previous time gradients, does it incur significantly higher computational and resource costs compared to other baseline methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "null"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2230/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2230/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2230/Reviewer_xGw7"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697773705695,
            "cdate": 1697773705695,
            "tmdate": 1699636156569,
            "mdate": 1699636156569,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "suZv3AVlZw",
                "forum": "Kl9CqKf7h6",
                "replyto": "M9PothyLjX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your insightful feedback and valuable questions regarding our manuscript. We appreciate the opportunity to clarify and expand on the points you've raised."
                    },
                    "comment": {
                        "value": "We acknowledge the concern about FedHyper\u2019s reliance on the hypergradient method and the need for a clearer explanation of the learning rate update process as per Equations (4-5). We have thoroughly explained how hypergradient works with simple examples. We respectfully urge the reviewer to consider these explanations in the final assessment. Below, we address each of your concerns and questions in detail:\n\n>W1: The proposed FedHyper highly relies on the existing method, i.e., hypergradient. It's unclear why the learning rate can be updated following Eq. (4-5). It would be better to give an intuitive explanation for this.\n\nEq. (4-5) is the hypergradient descent process on the learning rate $\\eta$ in centralized training. In the $t$th epoch of standard learning process of SGD algorithm, we first compute the model gradient $\\nabla F(w^{(t)})$ by gradient descent. Then, we use the product of current learning rate $\\eta^{(t)}$ and the model gradient to update the model parameters. Here in FedHyper, we update $\\eta^{(t)}$ each epoch. The updated learning rate $\\eta^{(t)}$ at $t$th epoch is computed as the sum of the previous learning rate $\\eta^{(t-1)}$ and the product of the model gradients at epoch $t$ and $(t-1)$. This is mathematically represented as: \n\n$\\eta^{(t)} = \\eta^{(t-1)} + \\nabla F(w^{(t)}) \\cdot \\nabla F(w^{(t-1)})$\n\nSince $\\nabla F(w^{(t)})$ and $\\nabla F(w^{(t-1)})$ are matrices of the same dimension because they are all gradients on the same model, their element-wise product results in a scalar. This scalar can be directly added to the original learning rate for updating the learning rate. \n\nTo further clarify, we provide an illustrative example here to show how hypergradient works. We assume that there is a simple optimization problem with initial learning rate $\\eta^{(0)} = 0.1$, the model parameters have size 1x2.\nWe assume that the gradients on the model in the first three epochs are:\n\n$\\nabla F(w^{(0)}) = [0.2, 0.1]$\n\n$\\nabla F(w^{(1)}) = [0.1, 0.1]$\n\n$\\nabla F(w^{(2)}) = [-0.1, -0.1]$\n\nThus, we start to schedule the learning rate in 1st epoch, we use the model gradient in the 0th and 1st epochs to update $\\eta^{0}$ to $\\eta^{1}$:\n\n$\\eta^{(1)} = \\eta^{(0)} + \\nabla F(w^{(0)}) \\cdot \\nabla F(w^{(1)})$\n\n$=0.1 + [0.2, 0.1] \u00b7 [0.1, 0.1]$\n\n$= 0.13$\n\n$\\eta$ increases from 0.1 to 0.13 because the model gradients in the first two epochs have similar directions-i.e., all the items are positive.\n\nThen the learning rate in 2nd epoch will be updated by:\n\n$\\eta^{(2)} = \\eta^{(1)} + \\nabla F(w^{(1)}) \\cdot \\nabla F(w^{(2)})$\n\n$=0.13 + [0.1, 0.1] \u00b7 [-0.1, -0.1]$\n\n$= 0.11$\n\n$\\eta$ decreases from 0.13 to 0.11 because the 1st and 2nd epochs have opposite gradients.\n\nSpecifically, in FL, we adapt this update rule to global and local learning rates, and use global model updates to replace the gradient in FedHyper-G and FedHyper-SL, use local gradients to replace the gradient in FedHyper-CL.\n\n>W2: FedHyper does not discuss the issue of training costs in FL. \n\nWe appreciate the emphasis on the importance of training costs in FL. To address this, we have included a detailed analysis of the time cost of our three schedulers and baselines. We run the experiments on time cost of our three schedulers and baselines. The following results are the time cost (in seconds) of different global optimization algorithms when training CIFAR10 for 200 communication rounds:\n\n=======================================\n\nFedHyper-G $\\quad$ FedAvg $\\quad$ FedAdam $\\quad$ FedExp\n\n...................................................................................\n\n  $\\quad$ 44060 $\\quad$ $\\quad$ 44031 $\\quad$ $\\quad$ 44045 $\\quad$$\\quad$ 45193\n\n=======================================\n\nWe can see that the different global schedulers have similar running time largely due to the dominant time consumption of the local training process, while global updates and optimization constitute only a small fraction of the overall overhead. However, consider that FedHyper-G can reach convergence faster than baselines, we will have less cost than our baselines ultimately.\n\nThen, here is the time cost (in seconds) of different local learning rate scheduling algorithms in the same setting:\n\n=======================================\n\nFedHyper-CL  $\\quad$    FedAvg(SGD)   $\\quad$  FedAvg(Adam)\n\n...................................................................................\n\n$\\quad$ 45780$\\quad$$\\quad$ $\\quad$ $\\quad$ 44031 $\\quad$ $\\quad$ $\\quad$ 32369\n\n=======================================\n\nSurprisingly, Adam requires much less training time than SGD and FedHyper-CL although it requires more computing overhead. It may be because CUDA or PyTorch has specifically designed optimization algorithms for Adam. However, we can find that FedHyper-CL only increase less than 5% computation cost compared with FedAvg(SGD) but gets convergence up to 3 times faster than SGD. Therefore, FedHyper's faster convergence offsets this slight increase in computation time."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700081634712,
                "cdate": 1700081634712,
                "tmdate": 1700081634712,
                "mdate": 1700081634712,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7BPSzAGIhJ",
                "forum": "Kl9CqKf7h6",
                "replyto": "M9PothyLjX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2230/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">Q1: Why are the baseline methods different for the three settings in Figure 3?\n\nThis is because the baselines are not universal schedulers, some of them are only global schedulers-i.e., FedExp, FedAdam, and FedAdagrad, and some are only client local schedulers-i.e., SGD and Adam. Thus, to ensure fairness, we only compare the FedHyper-G with the global scheduler baseline, FedHyper-CL with client's local scheduler baselines\u2026 etc.\n\n>Q2: Could you please provide a more detailed and intuitive explanation of hypergradients? I\u2019m looking for a thorough understanding, and I\u2019m willing to give a higher score for a comprehensive explanation.\n\nWe direct the reviewer to the improved explanation in W1, where we have elaborated on the concept and practical application of hypergradients, supplemented by a simple, illustrative example.\n\n>Q3: Is the learning rate fixed for baseline methods like FedAvg?\n\nFedAvg has fixed global and local learning rates. Decay-G, Decay-SL, and Decay-CL are the FedAvg with learning rate decay on global, server-side local, and client-side local learning rates respectively. FedExp calculates a global learning rate for each round, thus its global learning rate is not fixed. FedAdam and FedAdagrad directly manipulate gradient data, hence, we no longer manipulate the learning rate. Instead, we adopt the optimal initial learning rates summarized through experiments in [1] for our three datasets.\n\n>Q4: Since FedHyper requires updating the learning rate using current and previous time gradients, does it incur significantly higher computational and resource costs compared to other baseline methods?\n\n While it's true that FedHyper introduces additional computational steps in each epoch or communication round compared to the baseline FedAvg method, the key advantage lies in its ability to significantly accelerate convergence. This faster convergence means that FedHyper can achieve the desired level of accuracy with fewer training rounds and epochs, ultimately leading to a net saving in computational resources.To quantify this, we consider the total number of training epochs required for the global model to reach a specific accuracy as an intuitive metric of computational cost for FedHyper and the baseline methods. Here is the number of communication rounds needed to get 56% accuracy on CIFAR10 dataset:\n\n==================================================================================\n\n FedHyper-G $\\quad$ FedHyper-SL $\\quad$ FedHyper-CL $\\quad$ FedAvg $\\quad$ FedAvg(Adam) $\\quad$ FedAdagrad $\\quad$ FedExp\n\n..............................................................................................................................................................................\n\n$\\quad$ 121 $\\quad$ $\\quad$ $\\quad$ $\\quad$ 128 $\\quad$ $\\quad$ $\\quad$  $\\quad$ 104 $\\quad$ $\\quad$$\\quad$ 156$\\quad$$\\quad$$\\quad$$\\quad$155 $\\quad$ $\\quad$$\\quad$ $\\quad$ $\\quad$181 $\\quad$ $\\quad$ $\\quad$ 125\n\n==================================================================================\n\nThen, here is the number of communication rounds needed to get 50% accuracy on the Shakespeare dataset:\n\n==================================================================================\n\n FedHyper-G $\\quad$ FedHyper-SL $\\quad$ FedHyper-CL $\\quad$ FedAvg $\\quad$ FedAvg(Adam) $\\quad$ FedAdagrad $\\quad$ FedExp\n\n..............................................................................................................................................................................\n\n $\\quad$ 249 $\\quad$ $\\quad$ $\\quad$ $\\quad$ 177 $\\quad$ $\\quad$ $\\quad$  $\\quad$ 51 $\\quad$ $\\quad$ \n $\\quad$ 292$\\quad$$\\quad$$\\quad$ $\\quad$48 $\\quad$ $\\quad$$\\quad$ $\\quad$ $\\quad$ 101 $\\quad$ $\\quad$ $\\quad$ 336\n\n==================================================================================\n\nWe can get the result that FedHyper gets convergence with less rounds than baselines. From W2, we can find that FedHyper adds require less than 5% computing time than FedAvg, thus it will cost less time to get convergence, which means less computational cost and time cost.\n\nReference:\n\n[1]Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecn\u02c7 y,` Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700082299112,
                "cdate": 1700082299112,
                "tmdate": 1700082436019,
                "mdate": 1700082436019,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BUwp2CdtfZ",
            "forum": "Kl9CqKf7h6",
            "replyto": "Kl9CqKf7h6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2230/Reviewer_EDET"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2230/Reviewer_EDET"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces FedHyper, a novel learning rate scheduler adept at managing both global and local learning rates. FedHyper distinguishes itself by accelerating convergence and automating learning rate adjustments. To validate FedHyper's efficacy, the authors perform thorough experiments on benchmark datasets, offering a comprehensive evaluation of its performance."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1) The paper addresses a significant issue in Federated Learning (FL) by emphasizing the pivotal role of learning rate scheduling. It successfully argues the necessity for meticulous attention in this area and proposes pragmatic solutions, thereby contributing tangibly to advancements in FL.\n2) The adoption of the hypergradient method within this context is noteworthy. Its proven effectiveness in FL not only reinforces the method's utility but also broadens its appeal, suggesting it could be beneficial in a variety of other areas. This aspect of the paper stands out as particularly insightful.\n3) The experimental framework of the paper is commendable for its solidity. By utilizing benchmark datasets, the research provides ample evidence to support the stated contributions of FedHyper. This rigorous approach to experimentation substantiates the claims made, enhancing the paper's credibility."
                },
                "weaknesses": {
                    "value": "1) One concern is the lack of empirical evidence supporting the client-side scheduler's strategy of employing a global model update to limit the growth of the local learning rate. The paper would greatly benefit from an ablation study to confirm the validity of this approach, ensuring that the strategy is both necessary and effective.\n2) The analysis seems incomplete when it comes to comparing FedHyper with FedAdam. The latter, considered a baseline, is conspicuously missing from the \"performance of FedHyper\" section after being included in the cooperation analysis. The reason for this omission is unclear, and it restricts a full understanding of how FedHyper stands against established methods.\n3) The paper falls short in explaining the underlying reasons behind FedHyper's superior ability to fine-tune learning rates compared to its contemporaries, such as FedExp. The missing intuitive rationale or clear justification leaves the reader questioning why FedHyper is ostensibly more efficient. Addressing this would make the advantages of FedHyper more transparent and convincing."
                },
                "questions": {
                    "value": "Please refer to Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698639986598,
            "cdate": 1698639986598,
            "tmdate": 1699636156466,
            "mdate": 1699636156466,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7VeF52mB62",
                "forum": "Kl9CqKf7h6",
                "replyto": "BUwp2CdtfZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2230/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">W1: One concern is the lack of empirical evidence supporting the client-side scheduler's strategy of employing a global model update to limit the growth of the local learning rate. The paper would greatly benefit from an ablation study to confirm the validity of this approach, ensuring that the strategy is both necessary and effective.\n\nTo show that the added item with global model update can improve the performance of FedHyper-CL, we add an ablation study on the item in different $\\alpha$ in non-iid data distribution. Here are the results:\n\n=====================================\n\n$\\quad$ $\\quad$ $\\quad$ $\\quad$ $\\quad$ $\\quad$ 0.2 $\\quad$ $\\quad$ 0.5 $\\quad$ $\\quad$ 0.75\n\n...............................................................................\n\nWith item  $\\quad$$\\quad$  55.10% $\\quad$58.42% $\\quad$57.18%\n\nWithout item$\\quad$53.62% $\\quad$57.79% $\\quad$57.12%\n\n=====================================\n\nWe observed that the added term primarily improves model training accuracy when the non-iid degree is high-i.e., low $\\alpha$ value. This aligns with our intuition for incorporating this term - to enhance the effectiveness of heterogeneous client scenarios.\n\n>W2: The analysis seems incomplete when it comes to comparing FedHyper with FedAdam. The latter, considered a baseline, is conspicuously missing from the \"performance of FedHyper\" section after being included in the cooperation analysis. The reason for this omission is unclear, and it restricts a full understanding of how FedHyper stands against established methods.\n\nWe thank the reviewer for pointing out an oversight in our experiments. According to FedExp, the FedAdam should be compared with algorithms incorporating server momentum methods. Thus, here we provide the comparison between FedHyper-G+Server Momentum and FedAdam and the result is as follows:\n\nCIFAR10 200epochs:\n\n========================\n\nFedHyper-GM $\\quad$ FedAdam\n\n..................................................\n\n $\\quad$ 58.66  $\\quad$$\\quad$$\\quad$$\\quad$ 47.61\n\n========================\n\nShakespeare 600epochs:\n\n========================\n\nFedHyper-GM $\\quad$ FedAdam\n\n..................................................\n\n $\\quad$ 53.44 $\\quad$ $\\quad$$\\quad$$\\quad$ 50.98\n\n========================\n\nFedHyper-GM outperforms FedAdam in accuracy.\n\n>W3: The paper falls short in explaining the underlying reasons behind FedHyper's superior ability to fine-tune learning rates compared to its contemporaries, such as FedExp. The missing intuitive rationale or clear justification leaves the reader questioning why FedHyper is ostensibly more efficient. Addressing this would make the advantages of FedHyper more transparent and convincing.\n\nWe thank the reviewer for suggestions on adding intuitive explanations. Here we provide an intuitive explanation on why FedHyper can outperform FedExp in some situations. FedExp tends to use a larger global learning rate when the local model updates have higher variance. However, due to the fact that the local model update exhibits a smaller variance in the early stages of training and a larger variance as it approaches convergence, FedExp thus may has a higher global learning rate in the late stage than in the beginning of the training process, which can be observed in Figure 6 in our appendix. This contradict with our intuition that the learning rate should be large at the beginning and be low in the late stage. Thus, FedExp cannot work consistently stable across different tasks. It may only work well on tasks that need high global learning rates."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700253444554,
                "cdate": 1700253444554,
                "tmdate": 1700253444554,
                "mdate": 1700253444554,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3RtIVCHBxp",
            "forum": "Kl9CqKf7h6",
            "replyto": "Kl9CqKf7h6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2230/Reviewer_aiMM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2230/Reviewer_aiMM"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to introduce a structured approach to learning rate scheduling within the realm of Federated Learning, a critical step that enhances efficiency, particularly in the initialization phase. The authors present FedHyper, a framework that leverages hypergradient methods to adjust learning rates based on the inner product of gradients. FedHyper encompasses three distinct schedulers for comprehensive application: a global scheduler for the server-side learning rate, a server-side local scheduler, and a client-side local scheduler. Through rigorous testing on three datasets, FedHyper demonstrates superior performance compared to state-of-the-art baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- By targeting a pivotal issue in Federated Learning, the study positions itself within a crucial niche. The focus on optimizing learning rate scheduling addresses a substantive bottleneck in the field, underlining the paper's relevance.\n- The hypergradient method is interesting and effective, and the proposed method could be applied to other hyperparameters as well. Simplicity and practicality are the hallmarks of the proposed scheduler, making it an attractive tool for real-world application. Its ease of use could significantly benefit practitioners in the field.\n- The convergence of FedHyper is theoretically proofed, making this work more sound.\n- The results from extensive experiments demonstrate the significant performance improvement in both convergence rate and final accuracy.\n- The paper excels in clarity and accessibility, effectively illustrating the mechanics of hypergradient descent in learning rate scheduling through well-conceptualized figures (e.g., Figures 1 and 2)."
                },
                "weaknesses": {
                    "value": "- The paper's primary methodology involves utilizing the inner product of gradients to modify the learning rate, a technique that potentially incurs additional computational expenses compared to the more basic FedAvg. The study falls short by not evaluating or discussing these potential overheads, leaving the reader uncertain about the practical trade-offs.\n- In FedHyper-CL, the author\u2019s statement \u201cdirectly applying Eq. (19) to local learning rates can lead to an imbalance in learning rates across clients\u201d is not well supported by analysis or reference. So, the motivation of adding an item in FedHyper-CL is not clear.\n- The paper does not provide the direct comparison with FedAdam."
                },
                "questions": {
                    "value": "See the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722766231,
            "cdate": 1698722766231,
            "tmdate": 1699636156353,
            "mdate": 1699636156353,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "01QgnkzUx1",
                "forum": "Kl9CqKf7h6",
                "replyto": "3RtIVCHBxp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2230/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "The authors would like to thank the reviewers for their time and especially for all of the thoughtful questions and constructive suggestions. We would like to share our responses below.\n\n>W1: The paper's primary methodology involves utilizing the inner product of gradients to modify the learning rate, a technique that potentially incurs additional computational expenses compared to the more basic FedAvg. The study falls short by not evaluating or discussing these potential overheads, leaving the reader uncertain about the practical trade-offs.\n\nWe appreciate the emphasis on the importance of training costs in FL. To address this, we have included a detailed analysis of the time cost of our three schedulers and baselines. We ran the experiments on time cost of our three schedulers and baselines. The following results are the time cost (in seconds) of different global optimization algorithms when training CIFAR10 for 200 communication rounds:\n\n=======================================\n\nFedHyper-G $\\quad$ FedAvg $\\quad$ FedAdam $\\quad$ FedExp\n\n...................................................................................\n\n  $\\quad$ 44060 $\\quad$ $\\quad$ 44031 $\\quad$ $\\quad$ 44045 $\\quad$$\\quad$ 45193\n\n=======================================\n\nDifferent global schedulers have similar running times largely due to the dominant time consumption of the local training process, while global updates and optimization constitute only a small fraction of the overall overhead. However, our FedHyper-G can reach convergence faster than baselines, so we will have less cost than our baselines ultimately.\n\nThen, here is the time cost (in seconds) of different local learning rate scheduling algorithms in the same setting:\n\n=======================================\n\nFedHyper-CL  $\\quad$    FedAvg(SGD)   $\\quad$  FedAvg(Adam)\n\n...................................................................................\n\n$\\quad$ 45780$\\quad$$\\quad$ $\\quad$ $\\quad$ 44031 $\\quad$ $\\quad$ $\\quad$ 32369\n\n=======================================\n\nAdam requires much less training time than SGD and FedHyper-CL although it requires more computing overhead may be because CUDA or PyTorch has specifically designed optimization algorithms for Adam. FedHyper-CL only increases less than 5% computation cost compared with FedAvg(SGD) but gets convergence up to 3 times faster than SGD. Therefore, FedHyper's faster convergence offsets this slight increase in computation time.\n\n>W2: In FedHyper-CL, the author\u2019s statement \u201cdirectly applying Eq. (19) to local learning rates can lead to an imbalance in learning rates across clients\u201d is not well supported by analysis or reference. So, the motivation for adding an item in FedHyper-CL is not clear.\n\nIn heterogeneous federated learning, different clients have datasets in different distributions. Some of the local datasets are easy to train and some are hard. If we do not add the item, the local learning rate will quickly increase in the clients with easy datasets and decrease in the clients with hard datasets. This is the reason of adding the item. The impact of heterogeneous clients can be found in [1] and [2].\n\n>W3: The paper does not provide a direct comparison with FedAdam.\n\nWe thank the reviewer for pointing out an oversight in our experiments. According to FedExp, the FedAdam should be compared with algorithms incorporating server momentum methods. Thus, here we provide the comparison between FedHyper-G+Server Momentum and FedAdam and the result is as follows:\n\nCIFAR10 200epochs:\n\n========================\n\nFedHyper-GM $\\quad$ FedAdam\n\n..................................................\n\n $\\quad$ 58.66  $\\quad$$\\quad$$\\quad$$\\quad$ 47.61\n\n========================\n\nShakespeare 600epochs:\n\n========================\n\nFedHyper-GM $\\quad$ FedAdam\n\n..................................................\n\n $\\quad$ 53.44 $\\quad$ $\\quad$$\\quad$$\\quad$ 50.98\n\n========================\n\nFedHyper-GM outperforms FedAdam in accuracy.\n\nReferences:\n\n[1]Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. arXiv preprint arXiv:1907.02189, 2019.\n\n[2]Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. Advances in neural information processing systems, 33:7611\u20137623, 2020."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700246406779,
                "cdate": 1700246406779,
                "tmdate": 1700246406779,
                "mdate": 1700246406779,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nflBAOLWQN",
            "forum": "Kl9CqKf7h6",
            "replyto": "Kl9CqKf7h6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2230/Reviewer_MWyj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2230/Reviewer_MWyj"
            ],
            "content": {
                "summary": {
                    "value": "This work extends the traditional hypergradient learning rate scheduler to federated learning scenarios. Specifically, the authors propose a novel theoretical framework that jointly considers the global and local learning rates with respect to global and local updates. The authors have conducted sufficient experiments to validate the performance."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The paper is well written. The authors thoroughly reviewed the previously related work, hypergradient, and discussed the limitations of the work in detail. Then the authors naturally extended the method into federated learning scenarios.\n\n2. The proposed method is intuitive, and the theoretical guarantee is solid.\n\n3. The authors have conducted sufficient experiments to validate the proposed methods and discussed the suitability of several variants under different hardware constraints."
                },
                "weaknesses": {
                    "value": "The proposed method is only evaluated on simple datasets and tasks, such as CIFAR-10 and FMNIST, and tested with small models. The hyperparameter choices for these simple scenarios would be relatively straightforward, particularly in the case of standard centralized training. It would be beneficial if the authors could test the performance improvements on more challenging cases, such as unbalanced or large datasets, or in fine-tuning settings."
                },
                "questions": {
                    "value": "FedHyper-SL and FedHyper-G appear to update different sets of hyperparameters using the same gradients. It would be beneficial if the authors could provide a more detailed comparison between these two variants, considering that FedHyper-G seems to be a special case of FedHyper-SL."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2230/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2230/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2230/Reviewer_MWyj"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698895807548,
            "cdate": 1698895807548,
            "tmdate": 1700593136880,
            "mdate": 1700593136880,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IzeGKrxBFk",
                "forum": "Kl9CqKf7h6",
                "replyto": "nflBAOLWQN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Summary: We appreciate the reviewer's thoughtful feedback on our work. We address all the concerns in detail below."
                    },
                    "comment": {
                        "value": ">W1: The proposed method is only evaluated on simple datasets and tasks, such as CIFAR-10 and FMNIST, and tested with small models. The hyperparameter choices for these simple scenarios would be relatively straightforward, particularly in the case of standard centralized training. It would be beneficial if the authors could test the performance improvements on more challenging cases, such as unbalanced or large datasets, or in fine-tuning settings.\n\nTo test the performance improvements of FedHyper on more challenging cases, we adopt FedHyper-G on federated fine-tuning large language models. We use a Llama-7b base model to fine-tune the Databricks-dolly-15k dataset and test it on the MMLU test set. We use LoRA to fine-tune the local models for 5 communication rounds and 2 local epochs each round. We get the following results:\n\n======================\n\n$\\quad$FedAvg $\\quad$ FedHyper-G\n\n..............................................\n\n $\\quad$ 32.74$\\quad$$\\quad$$\\quad$ 33.18\n\n======================\n\nThis result shows that FedHyper can also bring benefits to the fine-tuning of large language models. Considering that the local optimizer of fine-tuning llms is a more complicit problem, we leave adopting FedHyper-CL to llm fine-tuning a future topic.\n\n>Q1: FedHyper-SL and FedHyper-G appear to update different sets of hyperparameters using the same gradients. It would be beneficial if the authors could provide a more detailed comparison between these two variants, considering that FedHyper-G seems to be a special case of FedHyper-SL.\n\nWe highly appreciate the reviewer's perspective that FedHyper-G is a special case of FedHyper-SL. According to our observation, FedHyper-SL outperforms FedHyper-G in most of the task because the local learning rate takes effect in every local epoch while the global learning rate only works once per round. This is the comparison of FedHyper-G and FedHyper-SL and FedHyper-G+FedHyper-SL in our three dataset:\n\nFMNIST:\n\n=====================================\n\n$\\quad$FedHyper-G FedHyper-SL FedHyper-G+SL\n\n..............................................................................\n\n $\\quad$ $\\quad$ 97.04  $\\quad$ $\\quad$ 97.01 $\\quad$ $\\quad$ $\\quad$ 96.90\n\n=====================================\n\nCIFAR10:\n\n=====================================\n\n$\\quad$FedHyper-G FedHyper-SL FedHyper-G+SL\n\n..............................................................................\n\n $\\quad$ $\\quad$ 57.44 $\\quad$ $\\quad$ 58.35 $\\quad$ $\\quad$ $\\quad$ 58.37\n\n=====================================\n\nShakespeare:\n\n=====================================\n\n$\\quad$FedHyper-G FedHyper-SL FedHyper-G+SL\n\n..............................................................................\n\n $\\quad$ $\\quad$ 51.42  $\\quad$ $\\quad$ 51.89 $\\quad$ $\\quad$ $\\quad$ 52.56\n\n=====================================\n\nWe can find that the performance of FedHyper-G and FedHyper-SL vary for different tasks. In FMNIST and CIFAR10, the performance of FedHyper-G and FedHyper-SL are similar to each other, there is also no performance improvement in FedHyper-G+SL. In the Shakespeare dataset, which is more sensitive to local learning rate, FedHyper-SL and FedHyper-G+SL outperform FedHyper-G. This indicates that for most of the simple tasks, FedHyper-G is enough and has the least computation and communication overhead. For local learning rate sensitive tasks, we recommend using FedHyper-SL."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700537234943,
                "cdate": 1700537234943,
                "tmdate": 1700537234943,
                "mdate": 1700537234943,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xGXK5qZOUC",
                "forum": "Kl9CqKf7h6",
                "replyto": "WSfttvqrlz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2230/Reviewer_MWyj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2230/Reviewer_MWyj"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed answers! Most of my concerns were addressed, I increase my score to 8."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593206617,
                "cdate": 1700593206617,
                "tmdate": 1700593206617,
                "mdate": 1700593206617,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]