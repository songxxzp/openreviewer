[
    {
        "title": "Imitation Learning from Observation with Automatic Discount Scheduling"
    },
    {
        "review": {
            "id": "12GU1ZpfLn",
            "forum": "pPJTQYOpNI",
            "replyto": "pPJTQYOpNI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2299/Reviewer_nyXG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2299/Reviewer_nyXG"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces \"Automatic Discount Scheduling\", a mechanism to alter the discount factor during RL training. It is argued that it is helpful to alter the discount factor in order to incentivize an agent to learn early behaviors before later ones, in cases where the reward signal is a proxy reward computed based on the agent's observations in comparison to expert demonstrations. An example of such a proxy reward is optimal transport (OT) between the expert's and agent's observations. Positive results are shown on 9 Meta-World environments (e.g., comparing OT to OT + Automatic Discount Scheduling)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper presents a heuristic method for discount scheduling that helps overcome issues when doing imitation learning through proxy rewards on tasks that have \"progress dependencies\". While simple, the method seems to be novel and effective.\n- Results are presented on 9 tasks of various complexity in the Meta-World benchmark.\n- Ablations show comparisons of ADS to fixed discount factors and exponential discount scheduling, motivating the desire for adaptive scheduling.\n- The paper is well-written and presented clearly."
                },
                "weaknesses": {
                    "value": "- I think the paper could state more precisely what the problem being addressed is. I appreciate the motivating example of the Basketball task in Meta-World, wherein the agent learns to grasp the ball successfully but then sweeps it away before moving towards the hoop. Does the problem lie in (1) using any \"traditional proxy-reward-based method,\" (2) using optimal transport specifically as the reward function, (3) using optimal transport with a visual encoder that does not capture task details well, and/or (4) using optimal transport over partial observations (where the partial observations are not sufficient to deduce task progress)? My feeling is that (3) is the main reason for the described behavior in the Basketball task, but the paper seems to imply that the problem is with (1), i.e., proxy reward methods in general. I think some additional clarification on this point would be valuable.\n- Related to to the above point, is the motivating example mitigated if one uses a visual encoder that is more specific to the task instead of a frozen pre-trained ResNet -- so that the similarity function induced by the visual encoder better captures task progress? It appears that (part of) the underlying problem is that there is high visual similarity in the end frames (e.g. the visual embedding is focusing on the robot in the frame and not the basketball). Would fine-tuning your visual encoder to the demonstration (as in [1]) help address this problem?\n- What is the motivation for using longest increasing subsequence a heuristic for progress alignment? As mentioned in the paper, this seems to correspond to \"macroscopic progress.\" What are the advantages to LIS over OT for the progress recognizer; and if LIS is good at measuring task progress, can we just use it for the reward function instead of OT?\n\n[1] Haldar, Siddhant, et al. \"Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations.\"  2023."
                },
                "questions": {
                    "value": "Please see Weaknesses section above. I have also included some minor additional questions here:\n\n- Have the authors experimented with using a simple curriculum learning approach? For example, e.g. Maximize OT with the first 25% of the demonstration, then the first 50%, then the first 75%, then 100% of the demonstration. How well would this perform compared to the proposed approach?\n- Have the authors experimented with other cost functions in the OT formulation (e.g. a different visual encoder), and do the positive effects of ADS still hold?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2299/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2299/Reviewer_nyXG",
                        "ICLR.cc/2024/Conference/Submission2299/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2299/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698741353764,
            "cdate": 1698741353764,
            "tmdate": 1700504643539,
            "mdate": 1700504643539,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VNHQSgzids",
                "forum": "pPJTQYOpNI",
                "replyto": "12GU1ZpfLn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2299/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nyXG  (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments. We conduct further experiments, and provide clarification to your questions and concerns as below.\n\n> **Q1: Does the problem lie in (1) using any \"traditional proxy-reward-based method,\" (2) using optimal transport specifically as the reward function, (3) using optimal transport with a visual encoder that does not capture task details well, and/or (4) using optimal transport over partial observations (where the partial observations are not sufficient to deduce task progress)? My feeling is that (3) is the main reason for the described behavior in the Basketball task, but the paper seems to imply that the problem is with (1), i.e., proxy reward methods in general. I think some additional clarification on this point would be valuable.**\n\n- We appreciate the reviewer's meticulous consideration. We believe the problem lies in (1), i.e., proxy reward methods in general.\n\n    -  Appendix C.1 indicates that apart from OT, ADS can also improve the performance of GAIfO, another proxy-reward-based approach that uses adversarial imitation learning and alters between training of a generator (the policy) and the discriminator (the reward function). We observe that in the initial stage of training, the discriminator is still undertrained, making GAIfO easy to fall into local optimal solutions like the one in Figure 2(a). Since the min-max adversarial optimization is very complex in practice, the algorithm requires extensive interaction to escape this local optimum. Applying ADS to GAIfO can avoid this case, as ADS weakens the negative impact of the later proxy rewards.\n\n    - In Q2.1 and Q2.2, we will further show that the positive effects of ADS still hold for different visual encoders used in reward calculation.\n\n\n> **Q2.1: Related to the above point, is the motivating example mitigated if one uses a visual encoder that is more specific to the task instead of a frozen pre-trained ResNet -- so that the similarity function induced by the visual encoder better captures task progress? It appears that (part of) the underlying problem is that there is high visual similarity in the end frames (e.g. the visual embedding is focusing on the robot in the frame and not the basketball). Would fine-tuning your visual encoder to the demonstration (as in [1]) help address this problem?**\n\n- We agree that fine-tuning the universally pretrained vision model on in-domain data has the potential to improve the feature quality. However, the fine-tuning method in the paper [1] mentioned by the reviewer needs expert action information, which is lacking in ILfO settings. Instead, we try BYOL [2], a self-supervised representation learning method to finetune the visual encoder.\nThe results of the experiment have been included in Appendix C.7 of the revised version.\nThe results show that fine-tuning the visual encoder consistently enhances OT+ADS in 3 tasks, but the impact on pure OT may vary when testing different tasks. More importantly, OT+ADS still significantly outperforms pure OT when both algorithms are equipped with this fine-tuned encoder. \n\n\n> **Q2.2: Have the authors experimented with other cost functions in the OT formulation (e.g. a different visual encoder), and do the positive effects of ADS still hold?**\n\n- Thank the reviewer for the suggestion. We have conducted experiments with a different visual encoder (ResNet-50 pretrained with R3M [3]) and have added the results to Appendix C.6 in the revised version. The experimental results demonstrate that the positive impact of ADS still holds for the R3M encoder.\n\n\n[2] Grill, Jean-Bastien, et al. \"Bootstrap your own latent-a new approach to self-supervised learning.\" Advances in neural information processing systems 33 (2020): 21271-21284.\n\n[3] Nair, Suraj, et al. \"R3m: A universal visual representation for robot manipulation.\" arXiv preprint arXiv:2203.12601 (2022)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408362063,
                "cdate": 1700408362063,
                "tmdate": 1700408362063,
                "mdate": 1700408362063,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uHmKBeGKEJ",
                "forum": "pPJTQYOpNI",
                "replyto": "UdfHLQ87Kk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2299/Reviewer_nyXG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2299/Reviewer_nyXG"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer"
                    },
                    "comment": {
                        "value": "Thank you to the authors for the response and running the additional experiments. I think that the inclusion of simple curriculum approaches, the experiments with different visual encoders, and the demonstration of ADS with another proxy-based reward method strengthen the paper.\n\nOne minor nitpick - In Section C.1, I would soften the phrase \"ADS consistently enhances the performance of GAIfO over the nine tasks.\" This appears to be the case for some but not all of the 9 tasks."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504593814,
                "cdate": 1700504593814,
                "tmdate": 1700504593814,
                "mdate": 1700504593814,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "V4VUIVUoem",
            "forum": "pPJTQYOpNI",
            "replyto": "pPJTQYOpNI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2299/Reviewer_pLG5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2299/Reviewer_pLG5"
            ],
            "content": {
                "summary": {
                    "value": "The paper delves into the challenge of Imitation Learning from Observations (ILfO) for robotic agents, where they must learn from unlabeled video demonstrations without knowing the expert's actions. While many convert ILfO problems into Inverse Reinforcement Learning (RL) issues using proxy rewards, the paper identifies a limitation: tasks with a \"progress dependency\" property. In such tasks, agents must first grasp the expert's earlier behaviors before mastering subsequent ones. The study finds that reward signals for later steps impede learning initial behaviors. To overcome this, the authors introduce a new ILfO framework with an Automatic Discount Scheduling (ADS) mechanism. This mechanism adaptively adjusts the RL discount factor during training, emphasizing early rewards and gradually incorporating later rewards once initial behaviors are learned. Tests on nine Meta-World tasks show this method surpasses existing techniques, even solving tasks previously deemed unsolvable."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The research stands out in its originality by identifying a previously unaddressed challenge in conventional ILfO algorithms, specifically their limitations in handling tasks with progress dependency. Moreover, the introduction of the Automatic Discount Scheduling (ADS) mechanism within the ILfO framework is a novel contribution, showcasing a creative combination of existing ideas to address a new problem.\n\nThe quality of the research is evident in its thorough approach to problem-solving. The authors not only diagnose the issue with current ILfO algorithms but also provide a robust solution in the form of the ADS mechanism. Their method's ability to outperform state-of-the-art ILfO methods in all nine Meta-World tasks further attests to its quality.\n\nThe paper clearly articulates the challenges faced by conventional ILfO algorithms, the intricacies of tasks characterized by progress dependency, and the proposed solution. The introduction of the ADS mechanism and its role in prioritizing earlier behaviors for agents is presented with lucidity.\n\nThe significance of the paper is twofold. First, it sheds light on a critical limitation in existing ILfO algorithms, broadening the understanding of the domain. Second, by introducing a solution that not only addresses this limitation but also excels in tasks previously deemed unsolvable, the research holds substantial importance for the advancement of robotic imitation learning."
                },
                "weaknesses": {
                    "value": "At its heart, the paper's key proposition seems intuitive. Given that the objective is to imitate a sequence of actions, it's somewhat expected that there should be a dependency between actions. The current approach might be seen as a direct response to an oversight in the original problem formulation. Exploring more sophisticated reward designs or distance measurements could potentially offer a more nuanced solution to the challenge."
                },
                "questions": {
                    "value": "Although the experiments show significant improvement over the selected models, I'm interested in the following comparisons.\n1. A straightforward strategy to address the challenge of imitating sequences would be to divide the sequence into temporal slices and then imitate each slice in order. The absence of this seemingly obvious method in the comparative analysis is a missed opportunity. Including this approach in the experiments would provide a more comprehensive evaluation of the proposed ADS mechanism, especially when benchmarked against such a basic strategy.\n2. How does the model compare with RL learning with a goal-based reward?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2299/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799843696,
            "cdate": 1698799843696,
            "tmdate": 1699636162598,
            "mdate": 1699636162598,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xwpMqCkJyp",
                "forum": "pPJTQYOpNI",
                "replyto": "V4VUIVUoem",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2299/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pLG5"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments. We conduct further experiments, and provide clarification to your questions and concerns as below.\n\n> **Q1: Given that the objective is to imitate a sequence of actions, it's somewhat expected that there should be a dependency between actions. The current approach might be seen as a direct response to an oversight in the original problem formulation. Exploring more sophisticated reward designs or distance measurements could potentially offer a more nuanced solution to the challenge.**\n\n-  We would like to express our gratitude for this insightful comment. We agree that ADS provides a direct solution to the challenge we presented in our paper, and it has proven to be effective. Additionally, we believe that ADS aligns with the way humans learn, where we gradually master previous skills before moving on to more advanced ones.\nWe also agree that exploring more sophisticated reward and distance measurement designs shows promise. However, we believe we should first propose straightforward solutions to tackle the problem at hand, and we plan to delve deeper into the more sophisticated designs suggested by the reviewer in our future work.\n\n> **Q2: A straightforward strategy to address the challenge of imitating sequences would be to divide the sequence into temporal slices and then imitate each slice in order.**\n\n-  Is the reviewer suggesting that we can truncate the expert demonstrations and increase the truncation length with a series of tiered curricula?\nIf so, we acknowledge that this can also be a reasonable solution to address the presented challenge, and we thank the reviewer for the suggestion. \n    - We have tried two ways to schedule the curricula: (1) switching the curriculum at regular intervals or (2) adaptively switching the curriculum based on the progress recognizer. We have updated additional experiment results to Appendix C.5 of the revised version. Both implementations show improvements over pure OT, which again validates the core argument of our paper. However, they still fall short when compared to OT+ADS. In these curriculum-based methods, when the curriculum assigns an underestimated truncation length, the agent can not receive feedback for imitating later behaviors and the policy learning will temporarily get stuck. To achieve satisfactory performance, it is necessary to have a high-quality curriculum switcher that will seldom provide underestimated truncation length, which requires significant efforts on hyperparameter tuning. On the other hand, ADS is a softer instantiation of our high-level idea, as the later rewards are not entirely excluded. Ablation studies in Section 5.4 also support that ADS is not sensitive to the hyperparameters of the discount scheduler. Therefore, we prefer discount factor scheduling to setting truncation length curricula.\n\n- If we misunderstand the question, please post additional comments and we will be happy to have further discussions.\n\n\n> **Q3: How does the model compare with RL learning with a goal-based reward?**\n\n- Thank you for pointing out this baseline. We implement such a reward by the negative distance between the agent's current frame and the demonstration's last frame. The experimental results are shown in Appendix C.2. As this algorithm does not utilize the information given by the first T-1 frames of the demonstrations, its performance is poor compared to other proxy-reward-based methods."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408592016,
                "cdate": 1700408592016,
                "tmdate": 1700408592016,
                "mdate": 1700408592016,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F1ik0BdevF",
                "forum": "pPJTQYOpNI",
                "replyto": "V4VUIVUoem",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2299/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To reviewer pLG5: Sincerely looking forward to further feedback"
                    },
                    "comment": {
                        "value": "Dear reviewer:\n\nWe would like to express our gratitude for your time and efforts in reviewing our work. We have made clarifications and conducted additional experiments to address the issues raised in your review. If our response has addressed your concerns, we would be grateful if you could re-evaluate our work.\n\nWe are always open to further questions and comments, and we would be happy to discuss and resolve the issues you may still have.\n\nOnce again, thank you for your valuable input.\n\nBR,\n\nThe authors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585124317,
                "cdate": 1700585124317,
                "tmdate": 1700585124317,
                "mdate": 1700585124317,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iHik02p7mF",
                "forum": "pPJTQYOpNI",
                "replyto": "xwpMqCkJyp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2299/Reviewer_pLG5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2299/Reviewer_pLG5"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for their detailed explanation. After reading the reviews from other reviewers and the responses from the authors, I'd like to keep my current score unchanged."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632258796,
                "cdate": 1700632258796,
                "tmdate": 1700632258796,
                "mdate": 1700632258796,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GkPfKPFsgT",
                "forum": "pPJTQYOpNI",
                "replyto": "V4VUIVUoem",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2299/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To reviewer pLG5: Do you have remaining concerns?"
                    },
                    "comment": {
                        "value": "Dear reviewer:\n\nThank you for your response. We would like to know if you have any remaining concerns and look forward to further discussions with you.\n\nBR,\n\nThe authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635586172,
                "cdate": 1700635586172,
                "tmdate": 1700635675245,
                "mdate": 1700635675245,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PTL9c9sjJQ",
            "forum": "pPJTQYOpNI",
            "replyto": "pPJTQYOpNI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2299/Reviewer_K5dw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2299/Reviewer_K5dw"
            ],
            "content": {
                "summary": {
                    "value": "This paper works towards the common difficulty of learning earlier behaviours in ILfO imitation learning tasks, which is due to the property of progress dependencies of ILfO. To encourage the agent to master earlier parts of demonstration before proceeding to subsequent ones, the authors propose a mechanism called Automatic Discount Scheduling (ADS). Experiments prove the idea works and brings great gain compared with SOTA approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. As demonstrated by the paper, the problem of progress dependencies is a critical obstacle for effective ILfO learning. Several persuasive examples provided by paper illustrates this point. The proposed solution seizes a key part of the cause of this issue and posit a well-designed learning technique - ADS to avoid it. The demonstration is quite clear and algorithm design is intuitive and reasonable.\n2. Experiments are comprehensive with sufficient performance gain. Ablation study is abundant. Details are provided for possible reproduction of the results."
                },
                "weaknesses": {
                    "value": "1. I'm quite curious about the motivation of this paper: it is clear by reading the introduction part to know that proxy reward based ILfO is susceptible to such progress dependency issue. However, the problem seems to be similar to a common issue for reinforcement learning which is called the catastrophic forgetting problem. Also classic methods like Q-learning already involves a replay buffer to avoid the possibility of being stuck by a local optimality, or the so-called instability problem of RL training. It would be more convincing to discuss the relationship between these issues and the one solved by this work.\n2. If a model-based planning is employed, will it also alleviate ILfO's problem? How does it compare with the ADS as proposed?"
                },
                "questions": {
                    "value": "1. How's the progress dependeny issue related to RL difficulty like instability or catastrophic forgetting?\n2. If a model-based planning is employed, will it also alleviate ILfO's problem? How does it compare with the ADS as proposed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2299/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833615904,
            "cdate": 1698833615904,
            "tmdate": 1699636162510,
            "mdate": 1699636162510,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "88zSIcBmRt",
                "forum": "pPJTQYOpNI",
                "replyto": "PTL9c9sjJQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2299/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer K5dw"
                    },
                    "comment": {
                        "value": "Thank you for your inspiring comments. We provide clarification to your questions and concerns as below.\n\n> **Q1: The problem seems to be similar to a common issue for reinforcement learning which is called the catastrophic forgetting problem. Also classic methods like Q-learning already involves a replay buffer to avoid the possibility of being stuck by a local optimality, or the so-called instability problem of RL training. It would be more convincing to discuss the relationship between these issues and the one solved by this work.**\n\n- We appreciate the reviewer for the comment, but we are not sure whether we correctly understand the reviewer's question.\nDoes the reviewer think that the agent can initially learn the early behaviors, but later forgets those learned behaviors when proceeding to subsequent parts? If so, we clarify the phenomenon we want to show is not forgetting previously learned behaviors but rather failing to master the expert\u2019s early behavior.\n\n    - The problem solved by our work is related to proxy-reward-based ILfO methods, which follow the nature of maximizing cumulative proxy rewards. The problem arises when the agent does not learn previous behaviors and focuses on maximizing rewards in later stages. It turns out the agent gets stuck in local optima, making it unable to learn the correct early behaviors.\n    - For example, in the basketball task in Section 3, even if the agent's explorative policy (i.e., adding Gaussian noises to the actions proposed by the actor) occasionally samples trajectories that can pick the ball up, the agent still focuses on optimizing rewards in later stages and can not substantially learn this correct early behavior. Therefore, the problem is not forgetting previous learned behaviors.\n    - To address this issue, we propose to restrict the impact of later rewards until the agent has successfully mastered the early behaviors.\n     \n- If we misunderstand the question, please post additional comments and we will be happy to have further discussions.\n\n> **Q2: If a model-based planning is employed, will it also alleviate ILfO's problem? How does it compare with the ADS as proposed?**\n\n- Good question. We would like to remark on two points:\n\n    - In principle, employing a model-based planning component can not directly address the challenge mentioned in Q1. Even with model-based planning, the agent still optimizes the sum of future proxy rewards when choosing an early action, so the negative impact of proxy rewards assigned to later steps is not eliminated. By weakening the negative impact of the later proxy rewards, ADS can address this challenge at the root.\n\n    - Notably, a model-based planning method is tangential to ADS. We agree that replacing the RL agent in our algorithm with a model-based planning method can probably further enhance the performance, and we leave it as future work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408682643,
                "cdate": 1700408682643,
                "tmdate": 1700408682643,
                "mdate": 1700408682643,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8xG2A6n2HN",
            "forum": "pPJTQYOpNI",
            "replyto": "pPJTQYOpNI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2299/Reviewer_NwHk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2299/Reviewer_NwHk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes ADS, an imitation-learning-from-observation method that equips proxy-based reward with automatic discount scheduling. The core idea is to put a scheduler on the discount factor of the environment as the policy progresses to follow the expert demonstrations. Experiments show that the proposed method beat the selected baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The presented idea is simple and well motivated.\n+ Strong empirical performance compared to selected baselines."
                },
                "weaknesses": {
                    "value": "- While the presented idea is simple and interesting, it demands further analysis:\n  - If the goal is to first learn to follow earlier parts of trajectories first, and then move forward once policy learns, why not simply put a scheduler on truncating the expert trajectories, instead of on the discount factor? Changing the discount factor seems unnatural, especially considering that it is used together with an off-policy RL algorithm. As soon as one changes the discount factor, the target Q value for all data stored in the replay buffer changes even if one does not update the target Q network.\n- The main comparison in figure 3 does not seem fair: the baselines should be other curriculum learning approaches instead of vanilla proxy-reward approaches.\n- Scheduling the discount factor is not unique to ILfO but is generic to all RL problems. Can the authors provide more analysis on its implications in the generic RL setting? For example, how should we expect the convergence properties to change when we perform a discount factor scheduling."
                },
                "questions": {
                    "value": "My main question:\n\n- Why schedule the discount factor instead of expert demonstration (truncate) length\n- Implications on the RL setting when changing the discount factor\n\nAlso, I am curious to know the exact formulations of the cost functions used in the OT methods in the paper.\n\nPlease see above in the weaknesses section for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2299/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699282253206,
            "cdate": 1699282253206,
            "tmdate": 1699636162440,
            "mdate": 1699636162440,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lmgLTSAhsX",
                "forum": "pPJTQYOpNI",
                "replyto": "8xG2A6n2HN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2299/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NwHk"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments. We conduct further experiments, and provide clarification to your questions and concerns as below.\n\n> **Q1.1: If the goal is to first learn to follow earlier parts of trajectories first, and then move forward once policy learns, why not simply put a scheduler on truncating the expert trajectories, instead of on the discount factor?**\n\n- We thank the reviewer for proposing a new baseline. \nWe fully agree that implementing a scheduler to truncate expert trajectories is also a reasonable solution to address the challenge outlined in our paper. \nIn response to the reviewer's suggestion, we have conducted additional experiments and updated the results in Appendix C.4 of the revised version. \nWe try two implementations of this idea, using a linear scheduler or an adaptive scheduler (based on the progress recognizer) to truncate expert trajectories, respectively. \nBoth implementations demonstrate improvements over pure OT, which again validates the core argument presented in our paper.\nHowever, they still fall short when compared to OT+ADS. \nIn the truncating method, when the scheduler assigns an underestimated truncation length, the agent can not receive feedback for imitating later behaviors, and the policy learning will temporarily get stuck. To achieve satisfactory performance, it is necessary to have a high-quality scheduler that will seldom underestimate the truncation length, which requires significant efforts on hyperparameter tuning.\nOn the other hand, ADS is a softer instantiation of our high-level idea, as the later rewards are not entirely excluded. Ablation studies in Section 5.4 also support that ADS is not sensitive to the hyperparameters of the scheduler.\nTherefore, we prefer discount factor scheduling to truncation length scheduling.\n\n> **Q1.2: As soon as one changes the discount factor, the target Q value for all data stored in the replay buffer changes even if one does not update the target Q network.**\n\n- A changing value target is common in modern RL algorithms. For instance, the practical implementation of Soft Actor-Critic [1] uses a dynamically updated temperature parameter, which controls the scale of an entropy term in the value target. Various techniques (e.g., computing the exponential moving average of the learned networks to obtain slowly changing target networks [2]) can help to stabilize learning in this case. Therefore, smoothly updating the discount factor will not present extra challenges in practice.\n\n> **Q2: The main comparison in Figure 3 does not seem fair: the baselines should be other curriculum learning approaches instead of vanilla proxy-reward approaches.**\n\n- Figure 3 is intended to demonstrate the effectiveness of our method in addressing challenges encountered by the SOTA proxy-reward-based approaches, which is the core motivation of our paper. To the best of our knowledge, none of the existing ILfO approaches deploy a curriculum learning mechanism. As discussed in Q1.1, we compared our method to a simple curriculum learning approach the reviewer proposed, which demonstrates the superior performance of our method. If we miss some potentially related work, please let us know and we will be happy to have further discussions.\n\n> **Q3: Can the authors provide more analysis on its (scheduling the discount factor) implications in the generic RL setting?**\n\n- We highlight a critical difference between the ILfO setting and the generic RL setting: in ILfO, the expert demonstrations can serve as a reference for monitoring the agent's learning progress and enable the design of our automatic discount scheduling mechanism. However, in the generic RL setting, we seem to lack information to decide when to raise the discount factor. This problem makes it difficult to apply discount scheduling to the generic RL setting, as a predefined discount schedule is highly inflexible. Therefore, our paper does not concern much about the implication in the generic RL setting. Nevertheless, we agree that studying discount scheduling in other specific RL settings is a promising direction and leave it as future work.\n\n> **Q4: I am curious to know the exact formulations of the cost functions used in the OT methods in the paper.**\n\n- As described in Section 5.1, we utilize cosine distance over the features extracted by a frozen ResNet-50 network (pretrained on the ImageNet dataset) to construct the cost function.\n\n[1] Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint arXiv:1812.05905 (2018).\n\n[2] Lillicrap, Timothy P., et al. \"Continuous control with deep reinforcement learning.\" arXiv preprint arXiv:1509.02971 (2015)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408753790,
                "cdate": 1700408753790,
                "tmdate": 1700408753790,
                "mdate": 1700408753790,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ANRGVNGgBq",
                "forum": "pPJTQYOpNI",
                "replyto": "8xG2A6n2HN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2299/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer NwHk: Sincerely looking forward to further feedback"
                    },
                    "comment": {
                        "value": "Dear reviewer:\n\nWe appreciate your time and efforts in reviewing our work. We have provided clarification and conducted additional experiments to address the issues raised in your comments. If our response has addressed your concerns, we would be grateful if you could re-evaluate our work. \n\nIf you have any further questions or comments, please let us know. We would be happy to discuss them with you.\n\nThank you once again for your valuable input.\n\n\nBR,\n\nThe authors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584472131,
                "cdate": 1700584472131,
                "tmdate": 1700584472131,
                "mdate": 1700584472131,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]