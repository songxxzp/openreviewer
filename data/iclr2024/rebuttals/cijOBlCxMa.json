[
    {
        "title": "CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models"
    },
    {
        "review": {
            "id": "6RgIYIUd3p",
            "forum": "cijOBlCxMa",
            "replyto": "cijOBlCxMa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3663/Reviewer_h7FN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3663/Reviewer_h7FN"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a CustomNet, an innovative approach to customizing objects in image generation based on SDs. It overcomes the limitations of existing methods, including slow optimization, identity preservation issues, and copy-pasting effects. It incorporates 3D novel view synthesis, allowing for diverse and identity-preserving outputs. It provides precise location and background control, surpassing existing techniques. This method enables zero-shot object customization without test-time optimization, offering better control and enhanced visual results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Incorporating 3D novel view synthesis differentiates the proposed CustomNet from prior approaches, improving identity preservation and varied output generation.\n\n2. This method's intricate designs address the limitations of existing techniques, allowing for precise object location and flexible background control.\n\n3. The proposed pipeline handles real-world objects and complex backgrounds effectively.\n\n4. The proposed zero-shot object customization is achieved without test-time optimization, allowing for control over the location, viewpoints, and background."
                },
                "weaknesses": {
                    "value": "1. The proposed CustomNet focuses on the limitation of 256 \u00d7 256 resolution, which may affect the quality of generated images.\n\n2. Non-rigid transformations and object style changes are not supported, limiting flexibility."
                },
                "questions": {
                    "value": "1. How is CustomNet different from past methods for object customization? How does it achieve better identity preservation and varied output generation?\n\n2. Can you explain the detailed designs mentioned in the text that allow for location and background control in CustomNet?\n\n3. How does CustomNet handle real-world objects and complex backgrounds in its dataset construction pipeline, and what's the motivation?\n\n4. Can you explain zero-shot object customization without test-time optimization and its multi-aspect control?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3663/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698041785953,
            "cdate": 1698041785953,
            "tmdate": 1699636322533,
            "mdate": 1699636322533,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "33M2RtR1e0",
                "forum": "cijOBlCxMa",
                "replyto": "6RgIYIUd3p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3663/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3663/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response by Authors to Reviewer h7FN-- Part 1/2"
                    },
                    "comment": {
                        "value": "## **W1. Limitation of 256$\\times$256 Resolution.**\n\n\n\n- **Limitation of Resolution**\n\n    - The constraint of the resolution of $256\\times256$ can be addressed in two ways. For the first one, we can use an improved Zero1-to-3, *e.g.*, The Zero1-to-3 ++ [8], which has extended the resolution to $512\\times512$. The second one is to train our CustomNet on large-resolution data from the current zero-1-to-3 checkpoint. \n\n    \n    -  We update more real-world object customization cases in the update paper, shown in **Figures 12, 13, 14**.\n\n***\n\n## **W2. Non-rigid transformations and style changes.**\n\n- It can be solved by training our model on a larger web-scale dataset with our dataset construction pipeline. The Non-rigid transformations and style changes can be controlled by text prompts, which has been proven in the SOTA Text-to-Image Diffusion model. Current Text-to-image diffusion models present promising results by training with large web-scale (text, image) pair datasets. With our dataset construction pipeline, we can convert the web-scale text-image pair datasets into (text, image, and novel-view image) pair datasets, then train our model to achieve non-rigid transformations and style changes.\n\n***\n\n## **Q1. Difference between the proposed CustomNet and existing methods. How does CustomNet achieve better performance?**\n\n\n- **Difference from other works.**\n\n    - We should kindly remind you that we novelty incorporate the 3D novel view synthesis capability into the task of object customization, obtaining more flexible customization results with various backgrounds. Note that previous object customization methods (optimization-based Dreambooth[4], Textual Inversion[5], and encoder-based Paint-by-example[2], Anydoor[3]) do not consider the 3D properties of the inserted object but learn a compressed token embedding to represent the object visual information. Therefore, they struggle to balance identity preservation and output diversity.  For example, Paint-by-example[2] loses identity, while Anydoor [3] produces copy-paste results, lacking diversity. other optimization-based methods Dreambooth[4], and Textual Inversion[5] suffer long test-time optimization times and are prone to over-fitting.\n\n    - In 3D novel-view synthesis fields, following Zero-1-to-3, most works focus on how to improve it to generate better geometry and quality, e.g. Magic123 [6], One-2-3-45 [7]. They still have the limitation in Zero-1-to-3 that can not be applied in real-world customization applications directly. However, CustomNet provides such a possibility to bridge the 3D novel-view synthesis and object customization with flexible multiple controls. \n\n- **CustomNet achieves better identity preservation and varied output generation in the following three aspects**:\n\n    - **Model architecture.**  Other methods mainly extract object identity visual information into a compressed embedding vector (whose dimension is normally 1 $\\times$ 768), Which makes it hard to preserve the identities of different objects. CustomNet preserves objects' identity by contacting objects corresponding latent with the UNet input latent, and sending them into UNet together. the object latent is the output of the VAE encoder, which can reconstruct the object input image by the VAE decoder. Therefore, the object latent contains the most information as the total model (VAE + Unet) can do.\n\n    - **Explicit control with 3D prior.** Other methods change the object generation by text control. The text prompt may be ambiguous in semantics, and the words that are not used be describe the object could also affect the object's identity. We design a separate viewpoints control branch that learns to warp the object into different view synthesis. 3D viewpoint warping is a rigid process that has less uncertainty than text description. Therefore, with our explicit control, we can preserve object identity better when generating various results.\n\n    - **Dataset construction.**  Other methods mainly use the text-image pair dataset for customization, while we further generate a multi-view for the object image which provides more information for the model to learn the object visual concepts. \n\n***"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3663/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462436495,
                "cdate": 1700462436495,
                "tmdate": 1700463322963,
                "mdate": 1700463322963,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fOTbwwE5ja",
                "forum": "cijOBlCxMa",
                "replyto": "6RgIYIUd3p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3663/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3663/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely Look Forward to Your Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer h7FN:\n\nThanks again for all of your constructive comments and suggestions, which have helped us improve the quality and clarity of this paper\n\nWe sincerely hope that our added experiments and analyses could address your concerns.\n\nSince the deadline for discussion is approaching, please feel free to let us know if there are any additional clarifications or experiments that we can offer. Your suggestions are highly appreciated.\n\nBest wishes,\n\nAuthors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3663/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577767559,
                "cdate": 1700577767559,
                "tmdate": 1700577767559,
                "mdate": 1700577767559,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qGYJnJOzkz",
            "forum": "cijOBlCxMa",
            "replyto": "cijOBlCxMa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3663/Reviewer_vmn1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3663/Reviewer_vmn1"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an method for inserting objects into scenes. The main contribution is a unified framework that allows users to specify the background, the object, and its location. The background can be specified by text description (generation) or actual image pixels (composition), while the object is specified by its background-subtracted image, the desired relative camera view, and its bounding box within the final image."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "There are two main strengths of the paper:\n\n1. The presented method is an end-to-end solution for the task of placing objects into scenes, which produces harmonious outputs easier than a multi-stage pipeline.\n\n2. The presented results look convincing in both preserving the object's identity a and harmonizing the final composition."
                },
                "weaknesses": {
                    "value": "The novelty of the approach is somewhat limited, as it essentially plugs background generation/compositing into the Zero-1-2 architecture. Placing the object into a localized bounding box as a guide instead of keeping it centralized would be straightforward to do in Zero-1-2.\n\nAn interesting component of the system is the synthetic dataset of objects composed onto backgrounds. Publishing that dataset for future research would add to the contribution."
                },
                "questions": {
                    "value": "How come you need to specify both the camera translation and image location? Wouldn't it be enough to only encode camera rotation and the bounding box to place the object anywhere in the image?\n\nYou mention that Zero-1-to-3 method cannot produce non-centered objects. But they allow users to specify full 3D camera rotation and translation, which is a component that this paper inherits. Why wouldn't translation enable users to place objects off-center?\n\nIf I understood the section 3.2 correctly, to make Figure 2 more clear, you could show 2 branches as the input to the text encoder: \"sitting on the beach\" for the Generation branch and NULL for the Composition branch.  Although I may be wrong, since the model could possibly take in both  the background image and the textual description. What happens in that case in terms of the output?\n\nPhrases like \"delicate designs\" and \"intricate designs\" seem out of place in this paper. The network design, if that's what this refers to, seems reasonable and straightforward.\n\nIt may be helpful to specify that R is a 3x3 matrix and T is a 3-vector. If that really is the case (as it is in Zero-1-2), then the rotation is not just a rotation, but also non-uniform scaling and sheer and possibly even reflection. Is that too many degrees of freedom to specify as input for a human user? Why not just specify the viewing direction with something like azimuth+altitude?\n\nYou did not define d in equation 2.\n\nA figure visualizing some of the synthetic dataset images would be helpful.\n\nPaper should discuss some remaining artifacts in the limitation section, such as:\n  * Minor change of identity in Figure 1 Row 3 (look at the number 1 on the toy car)\n  * Bad aspect ratio of the dog in Figure 1 Row 4 (it's too squished)\n  * The white pixels around the character in Figure 4 Row 1 Last Column\nWhere do these come from and how would one go about improving them?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3663/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3663/Reviewer_vmn1",
                        "ICLR.cc/2024/Conference/Submission3663/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3663/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698593128018,
            "cdate": 1698593128018,
            "tmdate": 1700495645090,
            "mdate": 1700495645090,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WnIrsrgROu",
                "forum": "cijOBlCxMa",
                "replyto": "qGYJnJOzkz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3663/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3663/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response by Authors to Reviewer vmn1-- Part 1/3"
                    },
                    "comment": {
                        "value": "## **W1. Novelty and object placement.**\n\n\n\n### **About Novelty**\nWe would like to kindly remind you that CustomNet is designed for **the task of object customization** that can place the object into desired scenes harmoniously with the explicit viewpoint, location, and background controls. We are not a simple application of zero-1-to-3 in the object customization field, but we introduce it to image customization for the first time, by addressing the inherent limitations of zero-1-to-3. \nCompared to previous image customization methods, CustomNet is the *first to investigate the novel view synthesis in the customization field*. \nIt can preserve the object's identity while controlling the viewpoint and increasing the output diversity.\n\n- **Improvements of zero-1-to-3.** \n\n    - Our module designs, together with the training dataset construction, are important improvements for the final outcome of object customization. They cannot be viewed as a simple addition to zero-1-to-3. They are specialized designs for object customization. Without those designs for object customization in the zero-1-to-3 model, the synthesized image lacks the background, and the object is centrally localized, which is far from our goal of object customization. The comparison to the Zero-1-to-3 limitation in our updated Figure 10 shows that our designs are necessary and effective. \n    Besides, the concurrent paper TOSS [1] also mentions in their paper that the extra text input could improve the generation quality of novel views over zero-1-to-3.\n    In our CustomNet, incorporating real-world data with text could also benefit the generation results, which is insightful to improve the novel-view synthesis model.  \n\n        \n    - Specifically, as described in the main paper, we demonstrate that Zero-1-to-3 has the following limitations when applied to object customization, and we make specific designs to address those limitations and adapt them to the task of object customization.\n    \n        - I. Zero-1-to-3 cannot be controlled by text for background. **v.s.** We design a dual-attention mechanism to simultaneously control viewpoint and text. \n        - II. Zero-1-to-3 cannot generate/composite backgrounds with a given background image. **v.s.** We designed a unified pipeline to combine it.\n        - III. Zero-1-to-3 can only process images located in the image's center. **v.s.** We design the concatenation module to place the object in arbitrary locations flexibly. \n        - IV. Zero-1-to-3 usually generates unreal effects. **v.s.** We design a real dataset construction pipeline to extend its capability on real data. \n\n- **Difference from other works.**\n\n    - We should kindly remind you that we novelty incorporate the 3D novel view synthesis capability into the task of object customization, obtaining more flexible customization results with various backgrounds. Note that previous object customization methods (optimization-based Dreambooth[4], Textual Inversion[5], and encoder-based Paint-by-example[2], Anydoor[3]) do not consider the 3D properties of the inserted object but learn a compressed token embedding to represent the object visual information. Therefore, they struggle to balance identity preservation and output diversity.  For example, Paint-by-example[2] loses identity, while Anydoor [3] produces copy-paste results, lacking diversity. other optimization-based methods Dreambooth[4], and Textual Inversion[5] suffer long test-time optimization times and are prone to over-fitting.\n\n    - In 3D novel-view synthesis fields, following Zero-1-to-3, most works focus on how to improve it to generate better geometry and quality, e.g. Magic123 [6], One-2-3-45 [7]. They still have the limitation in Zero-1-to-3 that can not be applied in real-world customization applications directly. However, CustomNet provides such a possibility to bridge the 3D novel-view synthesis and object customization with flexible multiple controls. \n\n\n- **Object placement.** The original Zero-1-to-3 model struggles to synthesize novel views of an object at an arbitrary location, in their camera settings, they cannot control translation but only azimuth angle, polar angle, and size. Therefore, we introduce a localized bounding box as guidance. \nIt is a simple but effective design for solving this problem. \nWhen applying the same location control strategy to zero-1-to-3 directly, we can see that significant artifacts would occur, while our CustomNet has reasonable results in the updated **Figure 10**.\n\n\n***\n\n\n## **W2. Dataset construction pipeline.**\n\n- Thank you for your advice. Our dataset can help CustomNet obtain more harmonious results since the real images serve as the model target. We will release the\ndataset, together with the full pipeline for constructing the dataset to contribute to the development of future research.\n\n\n***"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3663/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462220562,
                "cdate": 1700462220562,
                "tmdate": 1700463263815,
                "mdate": 1700463263815,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3ygUfFmICA",
                "forum": "cijOBlCxMa",
                "replyto": "qGYJnJOzkz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3663/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3663/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response by Authors to Reviewer vmn1-- Part 2/3"
                    },
                    "comment": {
                        "value": "## **Q1. Camera translation and object location.**\n\n- We should clarify that the definition and design of **RT** is consistent with the setting in Zero-1-to-3. More specifically, Zero-1-to-3 renders 3D objects into images with a predefined camera pose that always points at the center of the object. \nThis means the viewpoint is actually controlled by polar angle $\\theta$, azimuth angle $\\phi$, and radius $r$ (distance away from the center), not the full rotation and translation matrices, resulting all objects to be central of the rendered image. Thus, the Zero-1-to-3 can only synthesize objects at the image center, lacking the capacity to place the object at the **arbitrary spatial location** in the image.\n\n- Note that zero-1-to-3 cannot perform translation and can only generate objects localized centrally. We tackle this problem by specifying the bounding box at the desired location to the latent. This design is actually consistent with your comments.\n\n\n***\n\n\n## **Q2. Translation in Zero-1-to-3.**\n\n- Note that the zero-1-to-3 model cannot synthesize objects with arbitrary spatial translations, and can only control the size of the objects. Please refer to Q1 for the specified reasons. We address this limitation by placing objects in a specified location bounding box.\n\n\n***\n\n\n## **Q3. Generation and composition branches.**\n\n- You are right. We provide two ways to control the background: textual description and given background image, referred to as generation and composition branches, respectively. \nFor the generation branch, We added a new text branch to enable generating the background complying with the given text description with the Dual-Attention module. \n\n- As for the composition branch, when both given text and background image, the model generation will be dominated by the background images, as it provided more information than text when training. \n\n\n***\n\n## **Q4. Delicate or intricate.**\n\n- We will modify our phrase more precisely. Our goal that customizing an object with identity preservation and diverse control is straightforward. However, to achieve this goal, we have several careful designs from dataset construction (both synthetic and real datasets) to model architecture (DualAttn for text and viewpoints, and location control in initial latent concat).\n\n***\n\n\n## **Q5. Definition of RT.**\n\n- The **RT** is controled by the polar angle $\\theta$, azimuth angle $\\phi$, and radius $r$ (distance away from the center). Following your advice, We added the detailed statements in the paper in **Appendix A.2**, which is the same as the following sentence. \"The definition and design of RT is consistent with the setting in Zero-1-to-3. In Zero-1-to-3, they render 3D objects into images with a predefined camera pose that always points at the center of the object. This means the viewpoint is actually controlled by polar angle $\\theta$, azimuth angle $\\phi$, and radius $r$ (distance away from the center),  resulting in all objects being central to the rendered image. Therefore, the rotation matrix R can only control polar angle $\\theta$, azimuth angle $\\phi$, and the translation vector T can only control the radius $r$ between the object and camera. Thus the Zero-1-to-3 can only synthesize objects at the image center, lacking the capacity to place the object at the arbitrary location in the image. We tackle this problem by specifying the bounding box at the desired location to the latent.\"\n\n***\n\n\n## **Q6. Definition of d.**\n\n- Thank you for pointing out the mistakes. We have updated the definition on it in the **Sec. 3.2**."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3663/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462307100,
                "cdate": 1700462307100,
                "tmdate": 1700462333679,
                "mdate": 1700462333679,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "z0pGGAh3K8",
                "forum": "cijOBlCxMa",
                "replyto": "qGYJnJOzkz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3663/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3663/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response by Authors to Reviewer vmn1-- Part 3/3"
                    },
                    "comment": {
                        "value": "## **Q7. Visualization samples of the dataset.**\n\n- We have updated more dataset visualization samples in the paper, please see the updated **Figure 9**.\n\n***\n\n## **Q8. Artifacts in the limitation section.**\n\n- First, this is a hard case in which other models usually fail to preserve their identity, see the updated **Figure 11**. Some minor changes would indeed occur when customizing objects with highly detailed textures. Compared to previous methods, our identity preservation is highly competitive. \nWe will mention it in the limitation, and continuously improve the identity in future work.\n\n- The white square in the background causes the artifacts. When testing another method, we place a white square to mask the original object.\nWhen testing our CustomNet, we mistakenly keep the white square and the model interprets the white pixel as the background.\nOur CustomNet will generate a good image without artifacts when provided with a pure background image. Please refer to the revised **Figure 4** for further clarification.\n\n- The model allows for modifications to be made to the location bounding box and viewpoints as specified by the user. We have set an overhead view of the dog in the figure, resulting in it appearing squished.\nSuch an artifact can be addressed by modifying the location bounding box and viewpoints.\n\n\n***\n\n[1] Yukai Shi, et al. \"TOSS: High-quality Text-guided Novel View Synthesis from a Single Image.\" ArXiv preprint arXiv:2310.10644.\n\n[2] Binxin Yang, et al. \"Paint by Example: Exemplar-based Image Editing with Diffusion Models.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n[3] Xi Chen, et al. \"AnyDoor: Zero-shot Object-level Image Customization.\" ArXiv preprint arXiv:2307.09481.\n\n[4] Nataniel Ruiz, et al. \"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n[5] Rinon Gal, et al. \"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion.\" In International Conference on Learning Representations, 2023. \n\n[6] Guocheng Qian, et al. \"Magic123: One Image to High-Quality 3D Object\nGeneration Using Both 2D and 3D Diffusion Priors.\" ArXiv preprint arXiv:2306.17843.\n\n\n[7] Minghua Liu, et al. \"One-2-3-45: Any Single Image to 3D Mesh in 45\nSeconds without Per-Shape Optimization.\" ArXiv preprint arXiv:2306.16928."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3663/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462357804,
                "cdate": 1700462357804,
                "tmdate": 1700462686495,
                "mdate": 1700462686495,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pnnkV0ksbp",
                "forum": "cijOBlCxMa",
                "replyto": "3ygUfFmICA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3663/Reviewer_vmn1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3663/Reviewer_vmn1"
                ],
                "content": {
                    "title": {
                        "value": "Re: Q1, Q5"
                    },
                    "comment": {
                        "value": "Thank you for the explanation. I have a follow-up question:\n\nIs the radius parameter needed now that you can specify a whole bounding box? What does it achieve? Does it further control the scale of the object within the bounding box? Does it control the camera field-of-view (a larger radius may yield an almost orthographic projection)? Is it largely ignored and the bounding box overpowers the radius parameter?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3663/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494721404,
                "cdate": 1700494721404,
                "tmdate": 1700494721404,
                "mdate": 1700494721404,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Jhwbbsy86T",
                "forum": "cijOBlCxMa",
                "replyto": "qGYJnJOzkz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3663/Reviewer_vmn1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3663/Reviewer_vmn1"
                ],
                "content": {
                    "title": {
                        "value": "Raising the rating"
                    },
                    "comment": {
                        "value": "Thanks to the authors for the rebuttal. In light of all the clarifications, additional figures, and open-sourcing the synthetic dataset+pipeline, I am raising my rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3663/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495564276,
                "cdate": 1700495564276,
                "tmdate": 1700495564276,
                "mdate": 1700495564276,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PF35SjjGWV",
                "forum": "cijOBlCxMa",
                "replyto": "z1LiAyja5i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3663/Reviewer_vmn1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3663/Reviewer_vmn1"
                ],
                "content": {
                    "title": {
                        "value": "Radius parameter"
                    },
                    "comment": {
                        "value": "You may want to mention in the paper that you set the radius to some default value (is it 1.0?) and use only the rotation parameters of the camera view and the bounding box.\n\nThis raises another follow up question: what happens if the bounding box is squished (e.g. very wide and short)? Does the object become non-uniformly scaled (appears too wide)? Or does it just scale down to the smaller dimension and still retain the expected aspect ratio?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3663/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527692654,
                "cdate": 1700527692654,
                "tmdate": 1700527692654,
                "mdate": 1700527692654,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QE4PFRi5kP",
            "forum": "cijOBlCxMa",
            "replyto": "cijOBlCxMa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3663/Reviewer_96bJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3663/Reviewer_96bJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel approach to text-to-image generation that integrates 3D novel view synthesis for enhanced object customization. Unlike traditional methods that have challenges with identity preservation and limited customization, CustomNet offers explicit viewpoint control, location adjustments, and flexible background generation. It leverages a new dataset construction pipeline using both synthetic multiview data and natural images. The result is a model that achieves zero-shot object customization with improved identity preservation, diverse viewpoints, and harmonious outputs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n2. The authors present a comprehensive solution that seamlessly integrates background inpainting into the Zero-1-to-3 pipeline, addressing its potential limitation in altering the background or location of the object.\n3. The paper utilizes the latest vision foundation models, such as SAM and BLIP, in conjunction with \"Zero-1-to-3,\" to construct a dataset that supports the training of the proposed unified framework. The inverse data generation pipeline which decompose the netural image into the training component is interesting."
                },
                "weaknesses": {
                    "value": "1. A significant concern with this paper is its limited novelty. It leans heavily on the \"Zero-1-to-3\" model as its foundation. While the addition of background inpainting offers an enhancement, the core mechanism\u2014explicit 3D novel view synthesis\u2014remains unchanged. Moreover, it inherits constraints from \"Zero-1-to-3\", particularly the resolution limitation of 256 \u00d7 256, which might not be practical for real-world scenarios.\n2. The approach to decompose foreground objects using SAM and augmenting them using \"Zero-1-to-3\" is intriguing. However, there are lingering questions about the methodology. For instance, considering that SAM segments every element in a scene, how does the paper specifically determine the foreground object? Furthermore, if the selected foreground object falls outside the \"Zero-1-to-3\" training domain, is the efficacy of the method compromised?\n3. The paper mostly compares CustomNet with \"Zero-1-to-3\" and a few other models. A broader discussion with state-of-the-art models in the domain would have given a better quality assessment of CustomNet's performance. For example, [1] also employs stable diffusion for background inpainting while preserving the appearance of foreground objects through a dual-branch composition.\n\n[1] Li, Siyuan, et al. \"OVTrack: Open-Vocabulary Multiple Object Tracking.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                },
                "questions": {
                    "value": "Please see above weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3663/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698800559901,
            "cdate": 1698800559901,
            "tmdate": 1699636322359,
            "mdate": 1699636322359,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pW9hoXox72",
                "forum": "cijOBlCxMa",
                "replyto": "QE4PFRi5kP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3663/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3663/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## **W1. Novelty and Resolution Constraint.** \n\n\n### **About Novelty**\nWe would like to kindly remind that CustomNet is designed for **the task of object customization** that can place the object into desired scenes harmoniously with the explicit viewpoint, location, and background controls. We are not a simple application of zero-1-to-3 in the object customization field, but we introduce it to image customization for the first time, by addressing the inherent limitations of zero-1-to-3. \nCompared to previous image customization methods, CustomNet is the *first to investigate the novel view synthesis in the customization field*. \nIt can preserve the object's identity while controlling the viewpoint and increasing the output diversity.\n\n- **Improvements of zero-1-to-3.** \n\n    - Our module designs, together with the training dataset construction, are important improvements for the final outcome of object customization. They cannot be viewed as a simple addition to zero-1-to-3. They are specialized designs for object customization. Without those designs for object customization in the zero-1-to-3 model, the synthesized image lacks the background, and the object is centrally localized, which is far from our goal of object customization. The comparison to the Zero-1-to-3 limitation in our updated Figure 10 shows that our designs are necessary and effective. \n    Besides, the concurrent paper TOSS [1] also mentions in their paper that the extra text input could improve the generation quality of novel views over zero-1-to-3.\n    In our CustomNet, incorporating real-world data with text could also benefit the generation results, which is insightful to improve the novel-view synthesis model.  \n\n        \n    - Specifically, as described in the main paper, we demonstrate that Zero-1-to-3 has the following limitations when applied in object customization, and we make specific designs to address those limitations and adapt them to the task of object customization.\n    \n        - I. Zero-1-to-3 cannot be controlled by text for background. **v.s.** We design a dual-attention mechanism to simultaneously control viewpoint and text. \n        - II. Zero-1-to-3 cannot generate/composite backgrounds with a given background image. **v.s.** We designed a unified pipeline to combine it.\n        - III. Zero-1-to-3 can only process images located in the image's center. **v.s.** We design the concatenation module to place the object in arbitrary locations flexibly. \n        - IV. Zero-1-to-3 usually generates unreal effects. **v.s.** We design a real dataset construction pipeline to extend its capability on real data. \n\n- **Difference from other works.**\n    \n    - We should kindly remind that we novelty incorporate the 3D novel view synthesis capability into the task of object customization, obtaining more flexible customization results with various backgrounds. Note that previous object customization methods (optimization-based Dreambooth[4], Textual Inversion[5], and encoder-based Paint-by-example[2], Anydoor[3]) do not consider the 3D properties of the inserted object but learn a compressed token embedding to represent the object visual information. Therefore, they struggle to balance identity preservation and output diversity.  For example, Paint-by-example[2] loses identity, while Anydoor [3] produces copy-paste results, lacking diversity. Other optimization-based methods Dreambooth[4], and Textual Inversion[5] suffer long test-time optimization times and are prone to over-fitting.\n\n    - In 3D novel-view synthesis fields, following Zero-1-to-3, most works focus on how to improve it to generate better geometry and quality, e.g. Magic123 [6], One-2-3-45 [7]. They still have the limitation in Zero-1-to-3 that can not be applied in real-world customization applications directly. However, CustomNet provides such a possibility to bridge the 3D novel-view synthesis and object customization with flexible multiple controls. \n\n### **Limitation of Resolution**\n\n- The constraint of the resolution of $256\\times256$ can be addressed by in two ways. For the first one, we can use an improved Zero1-to-3, *e.g.*, The Zero1-to-3 ++ [8], which has extended the resolution to $512\\times512$. The second one is to train our CustomNet on large-resolution data from the current zero-1-to-3 checkpoint. \n\n-  We update more real-world object customization cases in the update paper, shown in **Figure 12, 13, 14**."
                    },
                    "title": {
                        "value": "Official Response by Authors to Reviewer 96bJ -- Part 1/2"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3663/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461979830,
                "cdate": 1700461979830,
                "tmdate": 1700462712075,
                "mdate": 1700462712075,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "H0L9yYBbyJ",
                "forum": "cijOBlCxMa",
                "replyto": "QE4PFRi5kP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3663/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3663/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response by Authors to Reviewer 96bJ -- Part 2/2"
                    },
                    "comment": {
                        "value": "## **W2. Details of the Dataset construction pipeline.**\n\nWe provide more details about our dataset construction pipeline. \n\n- How to specifically determine the foreground object:  Given an image, We use BLIP2 (Github repo: https://github.com/salesforce/LAVIS/blob/main/examples/blip2_instructed_generation.ipynb) to extract the foreground object with the following instruction:\n'''\n{\"image\": image, \"prompt\": \"Question: What foreground objects are in the image? find them and separate them using commas. Answer:\"}\n'''\nThen we feed the queried object and its corresponding text to SAM, SAM can receive text as input and output the segmentation mask of the corresponding object.\n\n- When the selected foreground object falls outside the Zero-1-to-3 training domain, we may get unsatisfying results. We have tried to alleviate this issue in our CustomNet\n    1. We jointly train with both synthetic and real datasets, which will complement each other and narrow the gap between the synthetic and real datasets. \n    2. During training, though the input object image may be unsatisfying, we adopt the original natural image with the object as the target. It is helpful for model training as the target image provides information about how to place an object into a result image harmoniously and naturally, which helps in real-world scenarios. On the other hand, the synthetic dataset endows the model with the warping ability among multiple views. Combined with the two datasets, CustomNet demonstrates natural and high-fidelity customization. \n    3. In our CustomeNet, except for the image input, CustomNet also receives text as input. The text helps to improve the viewpoints control ability that Zero-1-to-3 is hard to manage, as shown in the updated **Figure 10**.\n\n\n***\n\n\n## **W3. Broad Discussion with other methods.**\n\n- To achieve customization, We can use a one-stage model like the optimization-based method Dreambooth[4] or Encoder-based Paint-by-example [2]. The optimization-based method suffers long test-time optimization times and is prone to over-fitting, while the encoder-based method may lose identity. We can also use two-stage methods that first get the foreground object image, then paste it to a new background directly or with diffusion inpainting. The OVTrack [9] uses stable diffusion background inpainting while preserving the appearance of foreground objects for a data hallucination strategy tailored to appearance modeling in multi-object tracking. Our Customnet can generate different views of the foreground object, and use text to generate background for it.\n\n- We have conducted quantitative and qualitative comparisons to the state-of-the-art encoder-based and optimization-based image customization methods in Sec. 4 of the main paper. We update further discussion with the OVTrack[9] and other models, such as Blended latent diffusion[10], etc. in **the Discussion in Sec. 3 in the updated paper**.\n\n\n\n***\n\n\n[1] Yukai Shi, et al. \"TOSS: High-quality Text-guided Novel View Synthesis from a Single Image.\" ArXiv preprint arXiv:2310.10644.\n\n[2] Binxin Yang, et al. \"Paint by Example: Exemplar-based Image Editing with Diffusion Models.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n[3] Xi Chen, et al. \"AnyDoor: Zero-shot Object-level Image Customization.\" ArXiv preprint arXiv:2307.09481.\n\n[4] Nataniel Ruiz, et al. \"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n[5] Rinon Gal, et al. \"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion.\" In International Conference on Learning Representations, 2023. \n\n[6] Guocheng Qian, et al. \"Magic123: One Image to High-Quality 3D Object\nGeneration Using Both 2D and 3D Diffusion Priors.\" ArXiv preprint arXiv:2306.17843.\n\n\n[7] Minghua Liu, et al. \"One-2-3-45: Any Single Image to 3D Mesh in 45\nSeconds without Per-Shape Optimization.\" ArXiv preprint arXiv:2306.16928.\n\n\n[8] Ruoxi Shi, et al. \"Zero123++: a Single Image to Consistent Multi-view Diffusion Base Model.\" ArXiv preprint arXiv:2310.15110.\n\n\n[9] Li, Siyuan, et al. \"OVTrack: Open-Vocabulary Multiple Object Tracking.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[10] Omri Avrahami, et al. \"Blended Latent Diffusion.\" In SIGGRAPH, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3663/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462103626,
                "cdate": 1700462103626,
                "tmdate": 1700463222944,
                "mdate": 1700463222944,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "af85RHWZmX",
                "forum": "cijOBlCxMa",
                "replyto": "QE4PFRi5kP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3663/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3663/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely Look Forward to Your Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer 96bJ:\n\nThanks again for all of your constructive comments and suggestions, which have helped us improve the quality and clarity of this paper\n\nWe sincerely hope that our added experiments and analyses could address your concerns.\n\nSince the deadline for discussion is approaching, please feel free to let us know if there are any additional clarifications or experiments that we can offer. Your suggestions are highly appreciated.\n\nBest wishes,\n\nAuthors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3663/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577702991,
                "cdate": 1700577702991,
                "tmdate": 1700577702991,
                "mdate": 1700577702991,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]