[
    {
        "title": "Looped Transformers are Better at Learning Learning Algorithms"
    },
    {
        "review": {
            "id": "KOYZjo7FEV",
            "forum": "HHbRxoDTxE",
            "replyto": "HHbRxoDTxE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7006/Reviewer_QsDA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7006/Reviewer_QsDA"
            ],
            "content": {
                "summary": {
                    "value": "In the paper the authors examine the applicability of looped transformers to the task of learning linear regression in context (giving sampled examples). They compare the performance of a looped transformer with a standard 12 layer transformer and a least squares solver. They show that for some settings the looped transformer matches or even outperforms the performance of a standard transformer on the task of linear regression, while incorporating significantly less parameters."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- It is an interesting approach to use looped transformers for the task of linear regression in-context. It seems to work well with significantly less parameters.\n- The authors examine different settings, like using input injection, choosing the number of iterations, altering the number of layers, heads and the dimension of the embeddings.\n- The experimental evaluation is convincing."
                },
                "weaknesses": {
                    "value": "- The application of the looped transformer (in order to match the performance of a standard transformer) requires an extensive hyperparameter search for b and T. The authors make suggestions how this could be avoided, which should maybe be subject to further research.\n- The structure of the paper can be improved, e.g., by ending the paper with a short conclusion instead of the related works.\n\nMinor details:\n- \"It is worth noting\" instead of \"it's worth noting\"\n- \"use the scheduling does not significantly impact the outcome\" instead of \"use the scheduling doesn\u2019t significantly impact the outcome\"\n- The Figures are too small\n- Fig. 2: The curve for the least squares solver is difficult to see\n- Fig. 2 description: (left) and (right) should not be written after the punctuation in the sentence but before."
                },
                "questions": {
                    "value": "- What is the experimental setting for the plots shown in Figure 2? \n- Why are the curves for the looped transformer and the standard Transformer in Figure 2 exactly overlapping?\n- How much time and resources does it take to find optimal values for b and T?\n- If you consider the hyperparameter search is it still more useful to apply looped transformers instead of the standard transformers?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7006/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7006/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7006/Reviewer_QsDA"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7006/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740733786,
            "cdate": 1698740733786,
            "tmdate": 1700820939669,
            "mdate": 1700820939669,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Pu8wtPBq2O",
                "forum": "HHbRxoDTxE",
                "replyto": "KOYZjo7FEV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7006/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7006/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QsDA"
                    },
                    "comment": {
                        "value": "Thank you for your feedback on our paper. We appreciate your comments and would like to address your concerns.\n\n\n> `an extensive hyperparameter search for b and T`: The authors make suggestions how this could be avoided, which should maybe be subject to further research.\n\nThank you for pointing this out. An extensive hyperparameter search for $b$ and $T$ is subject to the goal of finding the appropriate values for $b$ and $T$ such that the looped transformer:\n- requires shortest training and inference time (related to the value of $b$),\n- occupies least training memory (related to the value of $T$),\n- is able to find a fixed-point solution beyond trained iterations.\n\n\nFrom our understanding of looped transformer training presented in the paper, the larger $b$ is, the better the looped transformer is at finding a fixed-point solution beyond trained iterations. Consequently, if the main objective of employing a looped transformer is to achieve this fixed-point solution, the strategy would involve progressively increasing $b$ until the solution is attained. This is akin to determining the optimal number of layers in a standard transformer.\n\nThe search for $T$ mainly relates to balancing between achieving a fixed-point solution and avoiding training instability. Including regularization strategies can ease this instability, thus reducing the intensity of tuning for $T$.\n\n> How much time and resources does it take to find optimal values for b and T? If you consider the hyperparameter search is it still more useful to apply looped transformers instead of the standard transformers?\n\nAs discussed in the above section, we believe if the goal is to find the value of $b$ and $T$ such that the looped TF can find a fixed-point solution, the burden of hyper-paremter search will be reduced by including the regularization strategies. So we would think there is no harm applying the looped transformer over the standard transformer.\n\n> `the structure of the paper can be improved, and minor details in the paper`: ending the paper with a short conclusion instead of the related works.\n\nThanks for your suggestions. We have modified the draft accordingly.\n\n> What is the experimental setting for the plots shown in Figure 2? Why are the curves overlapping?\n\nIn figure 2, we train the vanilla TF and looped TF on linear regression problem, with problem dimension $d=20$. In the left figure, we demonstrate the performance of looped transformer on in-context samples $k=40$. For the overlapping curve, we test the looped TF and standard TF on the same set of test prompts, therefore they have very similar performance.\n\nThank you once again for your valuable feedback. We hope that our response effectively addresses your concerns."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7006/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552165205,
                "cdate": 1700552165205,
                "tmdate": 1700552165205,
                "mdate": 1700552165205,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "n6QINWM5G3",
            "forum": "HHbRxoDTxE",
            "replyto": "HHbRxoDTxE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7006/Reviewer_xBYm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7006/Reviewer_xBYm"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a training methodology for looped transformers to effectively emulate iterative algorithms and provides empirical evidence that demonstrate the advantages of looped transformer on in-context learning. However, since all experiments are made on simulated datasets, whether the proposed method is effective in dealing with real-world data remains to be validated."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) The paper uses the looped transformer to emulate iterative learning algorithms and presents a novel methodology to train the looped transformer under reasonable assumptions.\n(2) The paper provides a wide range of evaluation and detailed ablation studies of the proposed method on simulated datasets and demonstrates its superior performance compared to standard, non-recursive transformers."
                },
                "weaknesses": {
                    "value": "(1) Since all experiments are made on simulated datasets, whether the proposed method is effective in dealing with real-world data remains to be validated.\n(2) The classes of functions studied in the paper (including linear regression, decision tree, 2-layer ReLU NN, etc.) are ideal and relatively simple compared to the functions emerged in practical applications."
                },
                "questions": {
                    "value": "1. Can you provide more details about the probability distribution over the used classes of functions, especially for decision trees and 2-layer ReLU NN?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7006/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698849822104,
            "cdate": 1698849822104,
            "tmdate": 1699636820890,
            "mdate": 1699636820890,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kHXWqCFE3G",
                "forum": "HHbRxoDTxE",
                "replyto": "n6QINWM5G3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7006/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7006/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xBYm (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thoughtful feedback and appreciate the opportunity to address your concerns.\n\n> `experiments are only on simluated datasets`: whether the proposed method is effective in dealing with real-world data remains to be validated. The classes of functions studied in the paper (including linear regression, decision tree, 2-layer ReLU NN, etc.) are ideal and relatively simple compared to the functions emerged in practical applications.\n\nIndeed, we acknowledge that the datasets in our study are comparatively simplistic. This is due to the focus of this paper is on training a transformer to solve data-fitting problem with simple function classes. \n\nTo establish a connection with real-world applications and further validate our approach, we have conducted additional experiments using the following 10 datasets from [OpenML](https://www.openml.org/), which we believe better represent practical scenarios:\n\n\n| dataset id | Name | # numerical features | # instances |\n| -------- | -------- | -------- | -------- |\n| 720     | abalone     | 7     | 4177 |\n| 725     | bank8FM     | 8     | 8192 |\n| 737     | space_ga     | 8     | 3107 |\n| 803     | delta_ailerons     | 5     | 7129 |\n| 807     | kin8nm     | 8     | 8192 |\n| 816     | puma8NH     | 8     | 8192 |\n| 819     | delta_elevators     | 6     | 9517 |\n| 847     | wind     | 14     | 6574 |\n| 871     | pollen     | 5     | 3848 |\n| 42192     | compas-two-years     | 7     | 3848 |\n\nLet $S =$ {$720, 725, 737, 803, 807, 816, 819, 847, 871, 42192$} be the set of all datasets. The datasets we examined have 0/1 binary labels. We trained the transformer and the looped transformer on 9 datasets and evaluated its in-context learning ability on the unseen test dataset. Both transformer and looped transformer have 256 embedding size, 8 heads. The tranformer has 12 layers, and looped transformer has 1 layer, trained to maximum loop iteration $b=30$, and $T=15$.\n\nWe uniformly sampled prompts from 9 datasets, where for each prompt, we first randomly selected a training set, then randomly selected $k+1$ samples from this training set, with $k$ being the number of in-context samples. During testing, we applied a similar approach for each test sample, selecting $k$ in-context samples from the test dataset, with care taken to exclude the test sample itself from these in-context pairs. The test accuracies are presented below: \n\n| test dataset id | train dataset ids | vanilla TF | looped TF |\n| -------- | -------- | -------- | -------- |\n| 720 | S \\ {720} | 0.626 $\\pm$ 0.008  | **0.662 $\\pm$ 0.008** |\n| 725 | S \\ {725} | 0.511 $\\pm$ 0.007    | 0.504 $\\pm$ 0.008 |\n| 737 | S \\ {737} | 0.656 $\\pm$ 0.006   | **0.72 $\\pm$ 0.01** |\n| 803 | S \\ {803} | 0.394 $\\pm$ 0.01    | 0.40 $\\pm$ 0.01 |\n| 807 | S \\ {807} | 0.405 $\\pm$ 0.004   | 0.416 $\\pm$ 0.005 |\n| 816 | S \\ {816} | 0.463 $\\pm$ 0.004   | 0.462 $\\pm$ 0.004 |\n| 819 | S \\ {819} | 0.483 $\\pm$ 0.005   | **0.568 $\\pm$ 0.01** |\n| 847 | S \\ {847} | 0.668 $\\pm$ 0.007   | **0.757 $\\pm$ 0.006**|\n| 871 | S \\ {871} | **0.532 $\\pm$ 0.004**   | 0.51 $\\pm$ 0.005 |\n| 42192 | S \\ {42192} | 0.65 $\\pm$ 0.005  | 0.65 $\\pm$ 0.008 |\n\nAs the result indicates, the looped transformer demonstrates comparable, and in some cases, better performance to the standard transformer in solving these OpenML datasets. We have updated the draft with this experiment results, and we believe these experiments offer a more realistic representation of practical applications.\n\nIn terms of further real-world applications, consider predicting user / customer behavior based on their historical data. Each customer represents a different context, and the optimal model form (linear, nonlinear, neural network, etc.) is often unknown. A transformer could learn an effective model for user data and then apply this learned model to new users. This is an area of ongoing investigation in our work, and we look forward to reporting our findings in the future."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7006/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551886454,
                "cdate": 1700551886454,
                "tmdate": 1700551886454,
                "mdate": 1700551886454,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rhZiAs0utI",
            "forum": "HHbRxoDTxE",
            "replyto": "HHbRxoDTxE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7006/Reviewer_gzpZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7006/Reviewer_gzpZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use looped transformers to solve in-context learning, which achieves comparable performance to the standard transformer, but utilizes less than 10% of the parameters."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It is interesting to see looped transformers work well for in-context learning.\n\nThe paper provides a thorough evaluation and ablations of looped transformers for in-context learning. \n\nThe paper is well written."
                },
                "weaknesses": {
                    "value": "My primary concern lies in the relevance of looped transformers to in-context learning. It appears that their main advantage is in reducing the number of parameters. However, it's not entirely clear why this reduction in parameters is crucial for in-context learning, especially in cases involving linear functions, sparse linear functions, random decision trees, and 2-layer ReLU networks. I find that the paper lacks in-depth mathematical insights or a thorough exploration of the practical implications that would help address this concern."
                },
                "questions": {
                    "value": "What is the mathematical insight of looping transformers for in-context learning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7006/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699000011080,
            "cdate": 1699000011080,
            "tmdate": 1699636820773,
            "mdate": 1699636820773,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TahN6pArWB",
                "forum": "HHbRxoDTxE",
                "replyto": "rhZiAs0utI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7006/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7006/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gzpZ"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback.\n\n> `relevance of looped transformers to in-context learning`: It\u2019s not entirely clear why this reduction in parameters is crucial for in-context learning, especially in cases involving linear functions, sparse linear functions, random decision trees, and 2-layer ReLU networks\n\nWe do not claim that a *reduction in parameters* is key to success in in-context learning. Instead, we aim to introduce a promising training method for a looped transformer, specifically tailored for in-context data fitting problems. The advantages of applying a looped transformer are twofold: 1) it reinforces the iterative nature commonly employed in solving data-fitting problems, and 2) it offers the benefit of saving parameters.\n\n> `lacks in-depth mathematical insights`: What is the mathematical insight of looping transformers for in-context learning?\n\nFrom the perspective of the expressive power of looped transformer: it has been extensively explored. [Bai et al., Theorem 3](https://arxiv.org/pdf/1909.01377.pdf) demonstrated that any traditional $L$-layer neural network can be represented by a weight-tied, input-injected network, highlighting the expressiveness of such networks. Furthermore, [Giannou et al.](https://arxiv.org/pdf/2301.13196.pdf) investigated how an encoder looped transformer can function as a universal computer. These studies provide solid theoretical foundations and constructions to demonstrate the significant expressiveness of transformers with looped architectures. Our work, instead, focuses on the empirical validation of this architecture, and presents a practical training method to effectively train a looped transformer architecture for in-context learning. \n\nFrom the perspective of applying a looped transformer to in-context data fitting problems: the most common and effective training algorithms are iterative and recursive in nature, such as gradient descent. Our insight is that equipping transformer architectures with a recursive (looped) structure enables them to learn policies for in-context learning that leverage the advantages of recursion.\n\nWe hope our response address your concerns. We have updated the draft to include the discussion. Thanks again for your valuable insights and comments to our paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7006/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551623106,
                "cdate": 1700551623106,
                "tmdate": 1700551623106,
                "mdate": 1700551623106,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]