[
    {
        "title": "Training and inference of large language models using 8-bit floating point"
    },
    {
        "review": {
            "id": "ai1U0R8GVC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5929/Reviewer_ViM8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5929/Reviewer_ViM8"
            ],
            "forum": "AIbQ3HDDHU",
            "replyto": "AIbQ3HDDHU",
            "content": {
                "summary": {
                    "value": "This paper contains thorough details inference/fine-tuning with FP8 quantized linear layers in the context of language models. I believe it will be useful to the community. The main part of the method is how to choose the correct per-tensor scaling bias."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is a key piece missing from the large scale FP8 literature. Figure 1 in particular is presented clearly and contains important details for successful FP8 inference/fine-tuning."
                },
                "weaknesses": {
                    "value": "The largest weakness in this paper is in transparency -- it is claimed throughout (e.g., the title) that FP8 training will be demonstrated. However, results are only provided for fine-tuning. I would suggest changing the title/intro to be more clear."
                },
                "questions": {
                    "value": "- Do the authors think that their results will hold for FP8 training from scratch?\n- Do the authors believe it's possible to use any of their methodology for other layers in the network such as attention?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5929/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697151847023,
            "cdate": 1697151847023,
            "tmdate": 1699636630718,
            "mdate": 1699636630718,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0wnN3dGRce",
                "forum": "AIbQ3HDDHU",
                "replyto": "ai1U0R8GVC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5929/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5929/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for remarking that our paper \u201cis a key piece missing from the large scale FP8 literature\u201d and how Figure 1 \"contains important details for successful FP8 inference/fine-tuning\".\n\nConcerning the weakness about transparency and the first question, we understand how the term \u201ctraining\u201d can be confusing and have modified the title and introduction to better reflect the scope of the paper. Let us clarify that we don\u2019t claim to have experiments for FP8 pre-training from scratch. However, from a conceptual point of view, our methodology can be applied to both pre-training and fine-tuning. That\u2019s why we encapsulate both under the term \u201ctraining\u201d. In the experiments, we clarify that we are only doing fine-tuning (see the title of subsection 4.5 for instance). We would love to have pre-training results, but they are really expensive and only affordable for a handful of AI labs worldwide, especially for the models we tested of up to 13 billion parameters. As a result, we don\u2019t include them here and leave them for future work. \n\nConcerning the second question about extending the FP8 methodology to other layers, we think so but need more time to have publishable results about it. As a result, we leave this question for future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5929/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415223551,
                "cdate": 1700415223551,
                "tmdate": 1700417696655,
                "mdate": 1700417696655,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AvaTV6agiO",
                "forum": "AIbQ3HDDHU",
                "replyto": "0wnN3dGRce",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5929/Reviewer_ViM8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5929/Reviewer_ViM8"
                ],
                "content": {
                    "comment": {
                        "value": "> However, from a conceptual point of view, our methodology can be applied to both pre-training and fine-tuning. That\u2019s why we encapsulate both under the term \u201ctraining\u201d.\n\nMy concern is that while it could be applied conceptually, it might not actually work in practice. Thank you for changing the title to be more clear, but there are still many places where I would recommend this to be changed (e.g., first sentence of abstract). I will leave my score as is, which is to still recommend accept."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5929/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609390541,
                "cdate": 1700609390541,
                "tmdate": 1700609390541,
                "mdate": 1700609390541,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fTsIIDpUjN",
            "forum": "AIbQ3HDDHU",
            "replyto": "AIbQ3HDDHU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5929/Reviewer_LXwv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5929/Reviewer_LXwv"
            ],
            "content": {
                "summary": {
                    "value": "* The paper conducts experiments for FP8 inference and finetuning in the context of LLMs.\n* It also provides various implementation details such as scaling factor calculations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The paper goes into great detail on how exactly quantization is carried out and how scaling factors are computed.\n* Additional general discussion and statistics studies are presented in the Appendix."
                },
                "weaknesses": {
                    "value": "* The paper repeatedly emphasizes \"training\" in FP8 (e.g., in the title, in the abstract, etc.), yet I could not find any actual training experiments, that is training a large LLM from scratch, in the paper. The paper only performs some finetuning on GLUE tasks, which is significantly less interesting given that it is comparatively cheap and FP8 speedups thus not so crucial while, in many cases, even more affordable finetuning techniques like QLoRA also work well. I think significant presentation changes are required to clarify that the paper focuses on inference and finetuning.\n* Most of the methodology appears to me like standard low quantized training techniques, e.g., using additional scales that are determined dynamically, adapted directly to FP8. Could you explain more precisely what exactly is new? I also did not find a Related Work section discussing this in more detail.\n* FP8 inference has been studied extensively by e.g. [3, 4] and also in the context of LLMs [1, 2]. Further, [5] finetunes (and even trains from scratch) large Transformers in FP8. Hence, the overall novelty of the work appears very low.\n\nUnfortunately, as the paper overall does not seem to contain significant novelty, neither in methodology nor in results, I cannot recommend acceptance at this point.\n\n[1] ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats, Wu et al.\n\n[2] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models, Zhang et al.\n\n[3] FP8 Quantization: The Power of the Exponent, Kuzmin et al.\n\n[4] FP8 versus INT8 for efficient deep learning inference, Baalen etl a.\n\n[5] FP8 Formats for Deep Learning, Micikevicius et al."
                },
                "questions": {
                    "value": "* What real-world inference speedups do you observe when finetuning and inferencing in FP8 using your setup?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5929/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5929/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5929/Reviewer_LXwv"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5929/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784207786,
            "cdate": 1698784207786,
            "tmdate": 1699636630604,
            "mdate": 1699636630604,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UbtHfZjpCw",
                "forum": "AIbQ3HDDHU",
                "replyto": "fTsIIDpUjN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5929/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5929/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for remarking that our paper \u201cgoes into great detail on how exactly quantization is carried out\u201d and \u201chow scaling factors are computed\u201d. We also appreciate their interest in the additional explanations and plots in the appendix.\n\nConcerning the weakness about \u201ccould not find any actual training experiments\u201d, we understand how the term \u201ctraining\u201d can be confusing and have modified the title and introduction to better reflect the scope of the paper.  Let us clarify that we don\u2019t claim to have experiments for FP8 pre-training from scratch. However, from a conceptual point of view, our methodology can be applied to both pre-training and fine-tuning. That\u2019s why we encapsulate both under the term \u201ctraining\u201d. In the experiments, we clarify that we are only doing fine-tuning (see the title of subsection 4.5 for instance). We would love to have pre-training results, but they are really expensive and only affordable for a handful of AI labs worldwide, especially for the models we tested of up to 13 billion parameters. As a result, we don\u2019t include them here and leave them for future work. \n\nConcerning the other two weaknesses about the novelty, the reviewer rightly points out that we take ideas from standard quantization techniques. For example, our max approach to choose the scales builds upon quantisation ideas from INT formats (see section 2.3). However, our paper is novel because we adapt the max approach to FP8 and detail its application to quantize weights, activations and gradients for both FP8 training and inference. Moreover, we shed light upon scaling decisions taken in popular FP8 software implementations (e.g. Transformer Engine).  Some of the fine-grained details and justifications of this implementation are not made explicit. We hope that by opening up our methodology and testing it in the experiments in Section 4, other FP8 researchers can build on top of it.The paper [5] that the reviewer mentions, claims to train large transformers in FP8 from scratch, but leaves many details of the implementation missing, hindering reproducibility. In contrast, we provide a pseudocode of the full forward and backward pass (see figure 1), explain the nuances between quantizing to FP8 the weights, activations and gradients, and show the mathematical formulas to compute the scaling biases (see subsection 2.3). \n\nLastly, concerning the question about real-world speedups, the hardware that we employ doesn\u2019t have native FP8 native. This is mentioned in the hardware section H.2, in the appendix. As a result, we don\u2019t claim to have such speedup numbers and leave it for future work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5929/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415150357,
                "cdate": 1700415150357,
                "tmdate": 1700417670305,
                "mdate": 1700417670305,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BZVCrokdQj",
                "forum": "AIbQ3HDDHU",
                "replyto": "UbtHfZjpCw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5929/Reviewer_LXwv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5929/Reviewer_LXwv"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thank you for making adjustments regarding the use of the term \"training\". I understand that pretraining is expensive, but that is also why reducing costs via FP8 is so crucial there. There are many methods that could be applied \"from a conceptual point of view\" but fail in practice in many interesting setups. Similarly, I understand that the authors may not have access to expensive Hopper GPUs to implement and demonstrate actual speedups. However, these aspects could have been strengths of the paper if they were present.\n\nI appreciate that the authors describe technical details to aid reproducability but I do not think that this constitutes sufficient novelty to recommend acceptance, as most of the paper's key results have been established before, using similar techniques (see references in my review). Additionally, I think a lot of the pseudocode, diagrams and formulas are essentially standard practice in low precision literature and are thus hard to count as significant novelty. Finally, there are no comparisons demonstrating the impact of those precise details that are actually different, relative to existing work.\n\nHence, I maintain my initial score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5929/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662948539,
                "cdate": 1700662948539,
                "tmdate": 1700662948539,
                "mdate": 1700662948539,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pOYj4I8JUT",
            "forum": "AIbQ3HDDHU",
            "replyto": "AIbQ3HDDHU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5929/Reviewer_PWj7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5929/Reviewer_PWj7"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new methodology for selecting a scaling factor value (Exponent Bias) when representing numbers with an 8-bit floating point in deep learning training and inference. This methodology is based on roughly matching the dynamic range between the parameters and the 8-bit floating point numerical format. In particular, the exponent bias is either selected dynamically for each parameter ( FP8-AMAX ) or selected uniformly for all parameters (FP8-CSCALE). The paper explores the training of two types of large language models, namely GPT and LIama 2, using FP8 representation for model sizes ranging from 111M to 70B. The results indicate that the performance is on par with the FP16 representation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1- The paper is well-written and organized.\n2- The new methodology for FP8 has been evaluated on various large language models, demonstrating that this approach is generalizable."
                },
                "weaknesses": {
                    "value": "1- The paper's contributions and novelty are not immediately clear. The methodology for calculating the Exponent Bias resembles the asymmetric quantization process for INT8, where the scaling factor is determined using a max operation. Furthermore, even within the scope of 8-bit floating point representation, determining the exponent bias based on the max operation has been explored in prior research. The author is recommended to clarify the paper's unique contributions, especially in comparison to the following studies:\n\n[1] Tambe, Thierry, et al. \"Algorithm-hardware co-design of adaptive floating-point encodings for resilient deep learning inference.\" 2020 57th ACM/IEEE Design Automation Conference (DAC). IEEE, 2020.\n\n[2] Sun, Xiao, et al. \"Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks.\" Advances in Neural Information Processing Systems 32 (2019).\n\n[3] Kuzmin, Andrey, et al. \"Fp8 quantization: The power of the exponent.\" Advances in Neural Information Processing Systems 35 (2022): 14651-14662.\n\n[4] Lee, Janghwan, and Jungwook Choi. \"Optimizing Exponent Bias for Sub-8bit Floating-Point Inference of Fine-tuned Transformers.\" 2022 IEEE 4th International Conference on Artificial Intelligence Circuits and Systems (AICAS). IEEE, 2022.\n\n2- Comparisons with other numerical formats, such as INT8, block floating point, logarithmic number systems, and posit, are not discussed. For instance, the results in [5,6] indicate that INT8 performance is superior for inference, even for models like the Transformer.\n\n[5] van Baalen, Mart, et al. \"FP8 versus INT8 for efficient deep learning inference.\" arXiv preprint arXiv:2303.17951 (2023).\n[6] Zhang, Yijia, et al. \"Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models.\" arXiv preprint arXiv:2305.12356 (2023)."
                },
                "questions": {
                    "value": "What is the reason for using the max operation to compute the exponent bias? Why didn't the author consider other statistical metrics such as mean or mode? The max operation typically works best when the distribution is symmetric. However, the distribution in deep learning is often asymmetric."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5929/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5929/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5929/Reviewer_PWj7"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5929/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793841617,
            "cdate": 1698793841617,
            "tmdate": 1700711896482,
            "mdate": 1700711896482,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o1Sd53G75c",
                "forum": "AIbQ3HDDHU",
                "replyto": "pOYj4I8JUT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5929/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5929/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for remarking that our paper is \u201cwell-written and organized\u201d and that our FP8 approach is \u201cgeneralizable\u201d to other large language models in addition to the GPT and Llama models of the paper.\n\nConcerning the weaknesses, the reviewer has caveats about our novelty and contribution. We acknowledge in the paper that the max approach to choose the scales builds upon quantisation ideas from INT formats (see section 2.3). However, our paper is novel because we adapt the max approach to FP8 and detail its application to quantize weights, activations and gradients for both FP8 training and inference. Moreover, we shed light upon scaling decisions taken in popular FP8 software implementations (e.g. Transformer Engine). The papers suggested by the reviewer have been important contributions for FP8, but in our view don\u2019t cover: 1) details for practitioners to reproduce the FP8 methodology that has gained traction in the community, illustrated by the Transformer Engine library, Noune et al. [2022] and Micikevicius et al. [2022]; 2) training and inference validation for large language models of more than 1 billion parameters; 3) evolution of the scales during training and inference.\n\nConcerning the second weakness about lacking \u201ccomparisons with other numerical formats\u201d, we refer the reader to our Appendix C, where we discuss them. It\u2019s true that we don\u2019t run experiments with those other formats, but we believe that our paper is self-contained with the focus on FP8. Other papers cited in our manuscript already cover those comparisons, see for instance Kuzmin et al. [2022] and van Baalen et al. [2023].\n\nRegarding the question about why using the max operation and not others, we focus on max since it is simpler than computing the mode or mean while being sufficient to match FP8 training and inference validation compared to higher precision (see section 3). Moreover, max is the methodology gaining traction in FP8 libraries like Transformer Engine. Other papers that we cite already cover those comparisons, see Micikevicius et al. [2022] and van Baalen et al. [2023] which employ other methods like MSE or percentile."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5929/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415079539,
                "cdate": 1700415079539,
                "tmdate": 1700415079539,
                "mdate": 1700415079539,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HFM0OIvfNP",
                "forum": "AIbQ3HDDHU",
                "replyto": "o1Sd53G75c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5929/Reviewer_PWj7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5929/Reviewer_PWj7"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "I appreciated the author's response to my comments. The study itself is valuable to communities, and I have increased my score to 5. However, as most of the approaches used in this paper are well-established in previous studies, I still believe that the paper's novelty is not sufficient for acceptance."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5929/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712312924,
                "cdate": 1700712312924,
                "tmdate": 1700712312924,
                "mdate": 1700712312924,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]