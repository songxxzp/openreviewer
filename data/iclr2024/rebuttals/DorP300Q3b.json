[
    {
        "title": "Learning Pseudo 3D Representation for Ego-centric 2D Multiple Object Tracking"
    },
    {
        "review": {
            "id": "YvDPUsYHhS",
            "forum": "DorP300Q3b",
            "replyto": "DorP300Q3b",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission672/Reviewer_yTKp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission672/Reviewer_yTKp"
            ],
            "content": {
                "summary": {
                    "value": "This work addresses the difficult problem of data mapping in 2D multiple object tracking (MOT), particularly in the context of object occlusions. While this problem is complicated in 2D, it is much easier to handle in 3D space using a 3D Kalman filter. The authors propose a new approach that uses 3D object representations to improve data mapping in 2D MOT.\n\nIn their method, referred to as P3DTrack, they use 3D object representations learned from monocular video data and monitored by 2D tracking labels, eliminating the need for manual annotation from LiDAR or pre-trained depth estimators. This approach differs from existing depth-based MOT methods in that it learns the 3D object representation along with the object association module.\n\nThe authors conduct extensive experiments and demonstrate the effectiveness of their approach by achieving the best performance on popular egocentric datasets such as KITTI and Waymo Open Dataset (WOD). They also commit to publishing the code for their method to make it accessible for further research and practical implementation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+The study presents a novel approach to the task\n+The proposed method is based only on RGB 2D input, which has the advantage that the hardware required is very simple and basic.  \n+Methods are evaluated on two public datasets. Choice of dataset is motivated and explained \n+The paper is well structured and written. Study is well motivated\n+Clear description of implementation and methods allows reproduction of experiments\n+Authors perform ablation study with various experiments showing advantages of proposed architecture over other SOTA methods."
                },
                "weaknesses": {
                    "value": "-Not clear in which dataset the ablation was performed? Is it for both or just one? It should be done for both datasets\n-The paper lacks qualitative results. Instead, there is a figure in the supplementary material that clearly explains the problem and the improvement. The paper would benefit from more qualitative results like this. This could be done as a teaser figure on the first page, giving the reader a good overview of the topic and the contribution. \n-It is not stated on which device the inference times are measured. Is it on the same GPU on which it was trained? One GPU or more? How far is it from working in real time? It is not clear and SLAM related applications require working in real time."
                },
                "questions": {
                    "value": "1. Why do authors use the specified object detection method instead of something newer like DETR or YOLOv8?\n2. How does the system work at night when visibility is reduced? Does the ability of 3D reconstruction remain the same or does it decrease?\nThe last \"-\" could be placed as question too."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission672/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698318207962,
            "cdate": 1698318207962,
            "tmdate": 1699635994537,
            "mdate": 1699635994537,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tOYwg2LXk2",
                "forum": "DorP300Q3b",
                "replyto": "YvDPUsYHhS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission672/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission672/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for the professional comments. We are inspired and hope our discussion brings more insights.\n### W1: Ablation study on KITTI\n- We conducted ablation study on Waymo open dataset, because it is a more large-scale dataset than KITTI. Besides, there is no validation set on KITTI. The previous works that conducted ablation study on KITTI split each training sequence into two halves, and use the first half frames for training\nand the second for validation. The overlapping videos cause inaccurate (too high) results that cannot reflect the actual performance well.\n- We also supplement the ablation study on KITTI.\n  \n|Method|HOTA\u2191|MOTA\u2191| FP\u2193| FN\u2193 |IDSW\u2193|\n|-|:-:|:-:|:-:|:-:|:-:|\n|Baseline| 77.6| 86.2| 6.3%|7.4%| 0.1%|\n|+low-quality dets| 78.1|86.5| 7.5%|5.9%| 0.0%|\n|+GNN |78.3 |86.5| 7.6%| 5.8%| 0.0%|\n|+3D representation|78.6 |86.8| 7.4%| 5.8%| 0.0%|\n### W2: More qualitative results\nWe show the qualitative results (videos with tracked objects) in supplementary material zip file. To show more results, we also build a project page https://p3dtrack.github.io/.\n### W3: Inference time\n- We measure the latency on NVIDIA A40 GPU, the same as it was trained on. We use a single GPU to measure the latency. \n- The inference latency is mainly dominated by the object detector, for about 150ms per frame. The tracking stage (feature extraction and object association) takes about 80 ms. If using a real-time object detector, like the speed-optimized YOLO-series detector (YOLOX_x 17.3 ms, YOLOv8x 3.53ms), we believe it will be a (near) real-time system for autonomous driving (sensor input is 10Hz for most autonomous driving systems). Besides, please note the 3D reconstruction is only conducted in pseudo-label generation stage, and it will not affect the inference speed. \n### Q1: Object detector\nCenterNet is a commonly used detector in MOT, so we chose it as our base detector for a fair comparison with SOTA methods. (Because the detector is not the core concern in our method.)\n\nWe also conduct additional experiments on a more modern detector to show the performance with a better detector. We show the results with the YOLOX detector (also widely used recently, especially for ByteTrack-series trackers.)\nThe detection performance is very well on Waymo open dataset. \n||AP VEHICLE |AP PEDESTRIAN |AP CYCLIST|AP ALL|\n|-|:-:|:-:|:-:|:-:|\n|YOLOX|64.22|78.36|57.91|66.83|\n\nBased on it, we can achieve better tracking results.\n|Method| Backbone| MOTA \u2191 |IDF1 \u2191| FN \u2193 |FP \u2193 |ID Sw. \u2193 |MT \u2191 |ML \u2193|\n|-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|QDTrack (Fischer et al., 2022)| ResNet-50 |55.6 |66.2 |514548 |214998 |24309 |17595 |5559|\n|P3DTrack (Ours)| DLA-34 |55.9 |65.6| 638390| 74802| 37478| 13643 |8583|\n|P3DTrack_YOLOX (Ours)| Modified CSP v5|59.2|67.6|549228|90342|51913|16475| 7362|\n\n\n### Q2: At night\nWhen the visibility is low at night, the SfM system will be negatively affected.  However, it only slightly affects pseudo label generation. The network learning 2D boxes and 3D representation is less affected. We show a video on the anonymous project page."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651378460,
                "cdate": 1700651378460,
                "tmdate": 1700651378460,
                "mdate": 1700651378460,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6zcC8kPBD8",
            "forum": "DorP300Q3b",
            "replyto": "DorP300Q3b",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission672/Reviewer_e7hh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission672/Reviewer_e7hh"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to jointly learn 3D features and their tracking association on top of existing 2D detector.\nThe supervision is provided via 3D MVS reconstruction of the static scene from moving camera, which allows extraction of 3D object locations and associate them with corresponding views.\nThis is shown to marginally improve MOTA accuracy on standard datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well structured and readable.\nThe review of state of the art appears complete and up to date.\nThe central idea of joint association via auxiliary depth loss  and its learning approach are novel. The representation itself uses standard building blocks, but the specific architecture is of interest for the computer vision community.\nAblation study is included to validate parameter and component choices. \nImplementation details appear sufficient for replication of results."
                },
                "weaknesses": {
                    "value": "Quantitative results constitute only incremental improvements over SOTA.\nThere is no analysis of failure cases, especially w.r.t non-static classes, which can be missed in the pseudo GT association due to rigid scene assumption."
                },
                "questions": {
                    "value": "Provide class based results, i.e. for pedestrians and cars to show there is no significant static bias."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission672/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission672/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission672/Reviewer_e7hh"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission672/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699021437338,
            "cdate": 1699021437338,
            "tmdate": 1699635994449,
            "mdate": 1699635994449,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YBIp3nXZyC",
                "forum": "DorP300Q3b",
                "replyto": "6zcC8kPBD8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission672/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission672/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful for your valuable comments and constructive advice, which help us a lot to make the paper better.\n### incremental improvements over SOTA\nThe main reason is that we use the weak CenterNet-based object detector with DLA-34 backbone, even worse than FasterRCNN_R50 in QDTrack. On COCO, there is a 1~2AP detection performance gap (39.2 vs 37.4). (We use CenterNet because a single-stage object detector makes it easier to add a 3D representation head.) However, we also show the performance increase with object detector scaling-up. When using a better detector YOLOX to compensate for the detector performance gap, we can outperformance SOTA methods by a large margin.\n|Method| Backbone| MOTA \u2191 |IDF1 \u2191| FN \u2193 |FP \u2193 |ID Sw. \u2193 |MT \u2191 |ML \u2193|\n|-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|QDTrack (Fischer et al., 2022)| ResNet-50 |55.6 |66.2 |514548 |214998 |24309 |17595 |5559|\n|P3DTrack (Ours)| DLA-34 |55.9 |65.6| 638390| 74802| 37478| 13643 |8583|\n|P3DTrack_YOLOX (Ours)| Modified CSP v5|59.2|67.6|549228|90342|51913|16475| 7362|\n\n### Failure cases\nThe common failure case of our method is the occluded scene, like parking lot, distant objects, occluded objects very close to the camera. The 3D location for these cases in naturally hard to estimate.\nThe detailed qualitative results can be found on the anonymous project page: https://p3dtrack.github.io/.\n\n### Non-static object\nFirstly, we show the object-level depth estimation results between car and pedestrian.\n|Category|$\\delta< 1.25\\uparrow$  |$\\delta< 1.25^2\\uparrow$|$\\delta < 1.25^3\\uparrow$  |Abs Rel$\\downarrow$ |Sq Rel$\\downarrow$ |RMSE$\\downarrow$ |RMSE log$\\downarrow$ |\n|-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|Car|0.993|0.995|0.996|0.093|0.558|3.367|0.198|\n|Pedestrian|0.981|0.992|0.994|0.055|0.292|3.086|0.104|\n\nThe results show that even though pedestrians are more likely to be dynamic, the results for pedestrian are still no worse than vehicle.\nThe object tracking results for these two classes are shown in the following Table.\n\n|Category| MOTA \u2191 |IDF1 \u2191| FN \u2193 |FP \u2193 |ID Sw. \u2193 |MT \u2191 |ML \u2193|\n|-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|QDTrack_Car| 55.6 |66.2 |514548 |214998 |24309 |17595 |5559|\n|QDTrack_Pedestrian| 50.3| 58.4|151957 |48636| 6347| 3746 |1866 |\n|Ours_Car| 59.2|67.6|549228|90342|51913|16475| 7362|\n|Ours_Pedestrian|55.7| 57.8|139181|28307 |16829 | 4029|1761 | \n\n\nThen we discuss more about why there is no significant static bias.\nOur method for 3D representation learning contains two main parts: 3D pseudo-label generation and 3DRL with 3D pseudo label.\n- As for 3D pseudo-label generation, \"the points on dynamic objects are mostly ignored\". That means 3D pseudo labels are rarely generated for pedestrians due to their movement. (However, when they are static, such as waiting for a red light, the pseudo label can still be generated.) This is the natural shortage of SfM. \n- Although lacking 3D pseudo labels, we can utilize the \"generalization ability of depth from other objects\" of the object 3D representation learning module. The other objects are mainly vehicles that are learned with 3D pseudo labels. We can generalize the depth of pedestrians from the learned depth of vehicles. This generalization ability is because depth is the class-agnostic attribute of the object, and the network learns depth from the whole image. During the inference stage, the network can leverage the learned depth information of other vehicles in the same image to predict the depth of pedestrians."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651353012,
                "cdate": 1700651353012,
                "tmdate": 1700651353012,
                "mdate": 1700651353012,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "imPi37Lysf",
            "forum": "DorP300Q3b",
            "replyto": "DorP300Q3b",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission672/Reviewer_RXkh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission672/Reviewer_RXkh"
            ],
            "content": {
                "summary": {
                    "value": "A 2D multiple object tracker is proposed that consists of 3 steps, an object detector, a 3d descriptor and then an associator consisting of matching 3d features.   The paper provides good results on datasets from KITTI and Waymo and compares to other tracking algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper provides results on 2 driving datasets.  There is a 3 stage process, the first is acquiring depth values using SFM which is an offline process.  The 2nd stage uses a MLP to derive a 3d representation which is based on clusters of 3d points.  The third stage is the data association.  The results are good, slightly better than the other algorithms compared to."
                },
                "weaknesses": {
                    "value": "The paper does not really compare to state of the art methods for MOT.  There are only 2 driving datasets compared to, the WayMo and Kitty datasets.  Also, the datasets chosen do not really demonstrate the ability to do multiple object tracking as at most there is 2  or at most 3 objects being tracked.  With regards to MOT, the datasets chosen should have been from the MOT challenge https://motchallenge.net.  The authors claim that using 3d features simplifies the problem, which it does for the dataset used, as objects are clearly separated by depth.  In addition, datasets that should have been considered also include GMOT-40 and Animal Track.   Long term pixel tracking is of interest recently which makes sense when objects are not separated by depth as in the examples given, some papers to look at include Tap-vid, Tapir, Particle video revisited, and tracking everything everywhere all at once.  \nI do not see how this method can perform real time tracking at all, first you need to train on a dataset and then you use SFM Colmap which is an offline process.  No generalizations of the method are demonstrated.  It would be interesting to see how well the method works if it was trained on WayMo and tested on Kitty and visa versa."
                },
                "questions": {
                    "value": "Is this more than just a simple tracking by detection?  You do not appear to do a predict and correct by detection or observation as would be necessitated by a kaman filter.\nFor all your examples, the objects are well separated by depth, what happens when they are not?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission672/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission672/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission672/Reviewer_RXkh"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission672/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699027395302,
            "cdate": 1699027395302,
            "tmdate": 1700006098141,
            "mdate": 1700006098141,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cwQUJWQEHl",
                "forum": "DorP300Q3b",
                "replyto": "imPi37Lysf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission672/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission672/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your comment. However, we kindly hope the reviewer can read our paper again, because some comments deviate from the facts.\n\n### Response to the comments in Strengths:\n- Why talk about LiDAR-based 3D MOT and why cite Kalman Fiter in the introduction? \n\nWe observed that using only the Kalman filter works very well for LiDAR-based 3D MOT. Therefore, we hope to help 2D MOT with a 3D representation.\n### W1: The paper does not really compare to state of the art methods for MOT. \nWe tried our best to compare the SOTA methods. As for KITTI dataset, please refer to https://www.cvlibs.net/datasets/kitti/eval_tracking.php for the tracking leaderboard. All published image-based SOTA methods are compared in Table 2. \nFor Waymo Open Dataset, we compared our method with real SOTA QDTrack. For other methods not reported on these two datasets, such as ByteTrack, we implemented it using the official code and reported it in our ablation study (Table 3 Row 2).\nWe think we have sufficiently compared the existing SOTA MOT methods.\n\n### W2: Also, the datasets chosen do not really demonstrate the ability to do multiple object tracking as at most there is 2 objects being tracked.\nIt seems the reviewer is not familiar with these datasets. As the widely-used MOT datasets, KITTI and Waymo have far more than **2 objects** being tracked. More than 100 methods report MOT results on these datasets. \nWe believe that these datasets can demonstrate the capability of MOT.\n\n### W3: With regards to MOT, the datasets chosen should have been from the MOT challenge.\nAs mentioned in Section 4.1, we discussed why we don't choose MOT17 and MOT20 as the evaluation dataset.\n>Please note that other datasets, such as MOT17 and MOT20, include numerous surveillance videos without camera motion. These datasets are not suitable for evaluating our method.\n\nSfM cannot be operated in non-geo-centric videos. Ego-centric video means the camera is moving along with the ego.\n### W4: The authors claim that using 3d features simplifies the problem, which it does for the dataset used, as objects are clearly separated by depth. \nPlease note that as a pure image-based MOT method, objects cannot be **clearly separated by depth**, how we separate them without any depth labels is indeed our contribution. Besides, pretrained depth cannot beat us. (Table 5)\n\n### W5: In addition, datasets that should have been considered also include GMOT-40 and Animal Track.\nThe GMOT-40 and Animal Track datasets have almost no necessary connection with our approach and our ego-centric setting. And compared with the datasets we used, GMOT-40 and Animal Track are much less popular.\n\n### W6: Long term pixel tracking is of interest recently which makes sense when objects are not separated by depth as in the examples given.\nWe appreciate your advice, but we don't know the necessity of using long term pixel tracking methods in our paper for now. Pixel tracking and object tracking are totally different tasks. We agree that it may help to track objects, however, there is no connection with our pseudo 3D representation. \n### Q1: Is this more than just a simple tracking by detection? \nAs shown in the introduction, we summarize the contribution of the paper.\n>(1) We propose a new online 2D MOT paradigm, called P3DTrack. P3DTrack utilizes the\njointly learned 3D object representation and object association.\n(2) We design the 3D object representation learning module, called 3DRL. With 3DRL, the\nobject\u2019s 3D representation is learned from the video and supervised by the 2D tracking\nlabels without any additional annotations from LiDAR or pretrained depth estimator.\n(3) The extensive experiments on the large-scale ego-centric datasets, KITTI and Waymo Open\nDataset (WOD) show that we achieve new state-of-the-art performance.\n\nThe most important contribution lies in the new pseudo 3D representation without depth/LiDAR supervision.\n\n### Q2: You do not appear to do a predict and correct by detection or observation as would be necessitated by a kaman filter. \nAs mentioned in Section 3.3, the proposed tracker infers **similar to DeepSORT**. In DeepSORT, the tracker predicts and corrects the tracklet by detection. Since we did not specifically mention, we don't change this process. \n\n### Q3: IF you are using a Kalman filter, this is linear and will fail when motion is not linear? \nNo. As discussed in many MOT papers, Kalman Filter can work for regular high-frame-rate videos. \n\n### Q4: For all your examples, the objects are well separated by depth, what happens when they are not?\nAs mentioned in Section 3.2, besides 3D representation, we also have appearance model just like other papers. When 3D representation fails, the appearance model can provide available association.\n\n**In summary, it appears that there are many misunderstandings in comments regarding our work and the field of MOT. Please consider reading related work and our paper again.**"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699876914334,
                "cdate": 1699876914334,
                "tmdate": 1699876914334,
                "mdate": 1699876914334,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jrYeZbJEq2",
                "forum": "DorP300Q3b",
                "replyto": "cwQUJWQEHl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission672/Reviewer_RXkh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission672/Reviewer_RXkh"
                ],
                "content": {
                    "comment": {
                        "value": "W1. 2 data driving sets are used for testing, this is limiting to make general comments about applicability to MOT in general.\nW2.  I am familiar with the datasets, they are 2 driving datasets.\nW3. To make general MOT claims, there should be other datasets besides driving ones.\nW4. You use SFM Colmap to get a depth estimate.  SFM only provides estimates to scale, this is not mentioned however since everything is to scale it would not matter.  I am not sure what your 3D representation adds, what if you just used the SFM clustered depth estimate as your representation, I would bet the results would be the same.\nW5.  Why not?  The paper should be clear on what datasets this method works on, the authors hint that it is a general MOT solution so why not demonstrate this.\nW6. Yes no relation to the pseudo 3d representation, however it should be referenced as another approach.\nQ1.  the pseudo representation has not been demonstrated to add more than an average depth in your ablation studies or elsewhere.\nQ2. Ok, same as DeepSort, I guess you use a graph matching.\nQ3.  Ok, you are not using Kalman filter.  I did find the paper a bit difficult to follow and hard to read.\nQ4.  See my previous comments.\n\nnew question:  How does this method function in real time given SFM is an offline process,\nnew question: How does this method generalize?  What if we learned with Wayne and tested with Kitty and visa-versa?\n\nI have updated my ratings, perhaps I was a bit harsh at the beginning"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700006566560,
                "cdate": 1700006566560,
                "tmdate": 1700006566560,
                "mdate": 1700006566560,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9hVvTT8c5q",
            "forum": "DorP300Q3b",
            "replyto": "DorP300Q3b",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission672/Reviewer_kQ8t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission672/Reviewer_kQ8t"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduce a pipeline for solving 2D multiple object tracking by learning 3D object representation, 2D object appearance feature and an object association model.\nThe 3D object representation is learnt from Pseudo 3D object labels created from monocular videos using structure-from-motion approach.\nThe object association model consists of two components: GNN-based feature aggregation and a differentiable matching.\nThe experiments conducting on KITTI and Waymo Open Dataset demonstrate the effectiveness of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Paper is well written with extensive experiments to support the proposed method.\n2. A 2D MOT method that can leverage the power of 3D representation without LiDAR data or a depth estimation model."
                },
                "weaknesses": {
                    "value": "1. The novelty of the paper is limited. The main idea is to generate pseudo 3D object labels from monocular videos so that it can be used to train the model to obtain 3D location / representation. There are main issues and details need to address about the process of generating these pseudo labels. Thus, in terms of the methodology for MOT, there is not much new development in this paper, e.g. GNN-based aggregation, association model learning and appearance using reID feature, etc. are the existing techniques in MOT literature.\n2. The impact of the paper is limited given the ego-centric datasets with LiDAR data widely available."
                },
                "questions": {
                    "value": "1. The author should provide more details on how to filter low speed of ego-motion videos and moving objects. It is also not clear how tracklet of those moving objects being handle.\n2. How can the model learn if there is only loss to supervised static object? The output o^t_j can be any values.\n3. I would like to see how is the quality of pseudo 3D object labels impact on the performance of learned 3D representation. One ablation study can be done is to use real 3D object labels to train the model and compare.\n4. In table 3, there is an increase of ID Sw when using Baseline + 3D representation. How do you explain this behaviour? Is it suppose for 3D representation to help reduce ID Sw."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission672/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699063281300,
            "cdate": 1699063281300,
            "tmdate": 1699635994321,
            "mdate": 1699635994321,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OYSXvvUELW",
                "forum": "DorP300Q3b",
                "replyto": "9hVvTT8c5q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission672/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission672/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kQ8t (Part 1)"
                    },
                    "comment": {
                        "value": "We really appreciate your valuable comments. We are trying our best to address your concerns and are open to any further discussions.\n### W1: Limited novelty\n- Our main novelty and contribution lie in the 3D representation. Specifically, We focus on how to obtain 3D labels, how to generalize the label to other objects, and how to use the (not so accurate) 3D features in MOT. The experiments also show that our 3D representation yields a substantial improvement compared to traditional 2D representation and other pretrained 3D features. As for technical contribution, to obtain 3D labels, we design the two-stage object clustering (IntraPC and InterPC). To generalize the label to other objects, we propose an image-based 3DRL module learning from all 2D labels and static 3D labels, and verify that this learning module can generalize the static labels to dynamic objects. To measure the degree of inaccuracy, we introduce uncertainty-based 3D loss.\nWe believe this learning paradigm can inspire the MOT community to pay attention to mining the 3D information hidden in the video that does not require manual annotation. \n- The feature aggregation and association modules are not the main contribution in our paper. For the online \"Tracking by Detection\" paradigm, most methods share a very similar association module, i.e., encoding the appearance/motion features, aggregating the object features, matching between detections and tracklets. This pipeline design can even back to DeepSORT published in 2017, and further adopted and slightly modified by the following work GMTracker [A], ByteTrack [B], BoT-SORT [C], etc. In this paper, we only utilize the best practices in the data association stage to obtain better performance and do not focus on the novel design in this part.\n- About the details in 3D pseudo label generation, we try our best to solve your concern in the following.\n\n### W2: Limited impact\n- We agree that most **existing** ego-centric datasets have LiDAR sensors. However, with the development of embodied AI and vision-based autonomous driving, hand-held cameras and cameras on vehicles without LiDAR will be widely used. For example, using DJI drones, driving Tesla autonomous-driving vehicles, and biking with a GoPro camera will provide more available data.  We believe our algorithm will have great potential in these fields.\n- Besides, we also report the additional results on the general non-ego-centric dataset MOT17 test set. The results show that even though training 3D representation on only half of the videos that have ego-motion, we still obtain performance improvement, especially compared with pretrained depth-based tracker QuoVadis [D]. Please note in this Table, we change our base detector to YOLOX to align the detector performance with SOTA methods.\n  \n|Method| MOTA\u2191| IDF1\u2191| HOTA\u2191| FP\u2193| FN\u2193| IDSw\u2193|\n|-|:-:|:-:|:-:|:-:|:-:|:-:|\n|ByteTrack| 80.3| 77.3| 63.1 |25491| 83721 |2196|\n|QuoVadis|80.3|77.7|63.1|25491|83721|2103|\n|BoT-SORT| 80.5\t|80.2\t|65.0\t|\t22521\t|86037|1212|\n|P3DTrack (Ours)|80.6 |79.8 |64.9 |22119 |86061 |1197|\n\n[A] He et al. Learnable Graph Matching: Incorporating Graph Partitioning with Deep Feature Learning for Multiple Object Tracking. In CVPR 2021.\n\n[B] Zhang et al. ByteTrack: Multi-Object Tracking by Associating Every Detection Box. In ECCV 2022.\n\n[C] Aharon et al. BoT-SORT: Robust Associations Multi-Pedestrian Tracking. arXiv 2206.14651.\n\n[D] Dendorfer et al. Quo Vadis: Is Trajectory Forecasting the Key Towards Long-Term Multi-Object Tracking? In NeurIPS 2022."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641853897,
                "cdate": 1700641853897,
                "tmdate": 1700641853897,
                "mdate": 1700641853897,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v3Twt7ILma",
                "forum": "DorP300Q3b",
                "replyto": "9hVvTT8c5q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission672/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission672/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kQ8t (Part 2)"
                    },
                    "comment": {
                        "value": "### Q1: Filtering\n- We filter low speed of ego-motion videos according to the ego speed from GPS/IMU. The videos in which the ego-vehicle moves less than 20m will not be considered in the reconstruction. In Waymo open dataset, this threshold can easily separate low-speed and normal videos.\n- We filter the moving objects by the threshold of reconstructed 3D points. Moving objects have fewer reconstructed 3D points because the movement is not consistent with ego and cannot be reconstructed with many 3D points. The threshold is 30 3D points. Typically, there are more than 30 keypoints extracted from the object in one frame. If this threshold is not met, it indicates that there aren't enough matched keypoints between two frames for this object, suggesting that it may be a moving object.\n- About how moving objects are being handled:\n(1) As for 3D pseudo-label generation, \"the points on dynamic objects are mostly ignored\". That means 3D pseudo labels are rarely generated for moving objects, including moving vehicles and most pedestrians. (However, when they are static, such as a vehicle/pedestrian waiting for a red light, the pseudo label can still be generated.) This is the natural shortage of SfM. \n(2) Although the 3D representation label is not generated for moving objects, we can generalize the 3D representation from the static objects. This is because the 3DRL module learns object's 3D representation from a **single image** instead of a video. In the image, network predicts the 3D representation from appearance instead of motion, so moving and static objects are not different. Even though we only learn from static objects, the object's depth can be **generalized** to other moving objects. In detail, we can utilize the \"generalization ability of depth from static objects\" of the object 3D representation learning module.  This generalization ability is because depth is the class-agnostic attribute of the object. \n### Q2: Learning from static object\nJust as mentioned in response to Q1, the 3DRL module learns object's 3D representation from a single image. Like monocular 3D object detector, on the head of CenterNet, we regress the object's 3D position ($H\\times W\\times 3$) from $H\\times W$ feature map. However, the supervision is only on the 2D center of static objects. As mentioned in response to Q1, we learn class-agnostic 3D representation from monocular image, and in an image, moving and static objects are not different in appearance. Moving and static objects share the same prediction head, so $o^t_j$ will not be any values for moving objects.\n\n### Q3: Impact of pseudo label quality\nWe conduct additional experiments using real 3D object labels to show the impact of pseudo label quality. Please note that, in Waymo open dataset, real 3D object labels are from LiDAR (0-75m, VFOV [-17.6, +2.4)), which means 3D labels are fewer than 2D labels. Besides, the 3D object ID GT and 2D object ID GT are not aligned (the same object has a 3D ID and a 2D ID). So, we match the 3D GT and 2D GT using the maximum IoU matching. We also retrain our model to fit the objects in this subset.\n\n|Method| MOTA \u2191 |IDF1 \u2191| FN \u2193 |FP \u2193 |ID Sw. \u2193 |\n|-|:-:|:-:|:-:|:-:|:-:|\n|3D GT|51.4|66.7|69495 |92768 | 11765|\n|3D pseudo label|51.1|64.1| 69611|92884|12557|\n### Q4: Increase of ID Sw\n- It is because in the last row, the object recall is much higher than the baseline. Please refer to the FP and FN metrics in Table 3.\nID Switches can only be compared under similar recall performance. In general, the more objects considered in tracking, the more ID Switches are reported.\nBesides, IDF1 is also the metric to reflect the identification performance. We obtain 5.8 IDF1 improvement (68.1 vs. 62.3).\n- 3D representation can help reduce ID Sw. Please refer to Table 4. In this Table, we only change the 2D/3D representation without modifying other modules and thus maintain a similar recall, so it can reflect the influence of our 3D representation better."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641892637,
                "cdate": 1700641892637,
                "tmdate": 1700641892637,
                "mdate": 1700641892637,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]