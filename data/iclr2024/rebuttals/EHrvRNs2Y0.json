[
    {
        "title": "ResFields: Residual Neural Fields for Spatiotemporal Signals"
    },
    {
        "review": {
            "id": "HSseppnVFW",
            "forum": "EHrvRNs2Y0",
            "replyto": "EHrvRNs2Y0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission573/Reviewer_KYty"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission573/Reviewer_KYty"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for improving the performance of coordinate-based network representation of various temporally varying signals. The key contribution of the method is that rather than having the network learn to represent temporal variation in a completely unstructured way, the temporal variation is instead modeled by a low-rank weight matrix which is added to a base weight matrix that is shared across all time instances. This exploits the temporal consistency in the underlying signals being represented, and results in a more parameter efficient and high-quality representation of the underlying signal. This is benchmarked across various tasks in which coordinate-based networks are used, such as video overfitting, dynamic SDF fitting, and dynamic neural radiance field fitting, where ResFields (the contribution) demonstrate consistent increased performance."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "In my opinion the strengths of the paper are as follows:\n1. The paper is described exceptionally clearly, and the method makes intuitive sense on why it would improve performance for representation on temporal signals. The relative simplicity of the method and lack of reliance on a number of complex hyperparameters makes it more likely that future methods will be influenced and use the contributions.\n2. The evaluations are thorough, across three different temporal signal representation tasks, all demonstrating clear improvements from the ResField contribution. In each of these domains, a number of baseline methods are all compared against. I found that the comparisons on dynamic NeRF especially are thorough, using many different state-of-the-art methods and all showing that ResFields leads to increased performance.\n3. The method is ablated well, and various ablations show the contribution of the method is actual source of the improvements."
                },
                "weaknesses": {
                    "value": "In my opinion, the main weaknesses of the paper are:\n1. To me, it is unclear how much capacity the low-rank representation has to model various dynamic components. While this may not be a problem in something like dynamic NeRF or dynamic SDF where the signal remains relatively constant over time, for overfitting a 2D video, where there may be cuts or completely different scenes, I would be interested to see how the method performs. In this case, it seems like the low-rank weight matrix approximation would be required to model all of the signal, as there is little consistency shared in the underlying signal over time and thus less information to store in the shared weights. Could this potentially be a limitation of the method?\n2. One additional minor weakness is that the method seems like it may add significant amount of computational overhead. For every iteration, the weight matrix of the underlying representation needs to be changed. Does this result in significant slowdown in optimization? It would be an insightful comparison to include this in the baseline comparisons, especially in the dynamic NeRF scenario where speed of fitting is very important. If this adds a significant amount of computational overhead, it is possible that the amount of gains is not worth it, and could be achieved by simply training a standard representation for longer."
                },
                "questions": {
                    "value": "I do not have any additional questions on the manuscript. Overall, as described in the strengths section, I see the paper as a good paper: the method is described well, simple, and makes intuitive sense, and then it is evaluated well across a number of tasks and ablated to show that the improvement comes from the proposed contribution. For this reason I am positive on the paper and leaning towards acceptance. I see some minor weaknesses in the potential capacity of the model, and especially in the potential computational overhead of the method in training speed. Addressing these weaknesses would incline me to raise my scores for the paper.\n\n**Update after author response**\n\nThank you for the detailed response. It has addressed my questions. I have updated my score in accordance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission573/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission573/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission573/Reviewer_KYty"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission573/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698633236547,
            "cdate": 1698633236547,
            "tmdate": 1701025530861,
            "mdate": 1701025530861,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Zg4zBb36bm",
                "forum": "EHrvRNs2Y0",
                "replyto": "HSseppnVFW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission573/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission573/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Related to the capacity of the low-rank representation (weakness 1/2)"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for considering our work as exceptionally clearly described and thoroughly evaluated and ablated, as well as for the insightful comments and questions regarding the proposed method and its performance. In the following, we address the remaining discussion points related to the capacity of the low-rank representation and the potential computational overhead of the proposed framework. \n\n ### 1. The capacity of the low-rank representation for modeling dynamic components of varying complexity.\nWe especially appreciate the suggestion and we will incorporate an additional study to benchmark the learning capabilities of the low-rank weights for signals with multiple diverse dynamic segments.  \n\n\nLearning signals with many independent parts imposes additional challenges for both ResFields and coordinate MLPs in general. The coordinate MLP weights possess the ability to compress a spatio-temporal signal into a compact representation, mainly because the source signal has a significant amount of repetitive information which is effectively represented by the optimized network weights. The shared weights of the ResFields have the same property, as they tend to store information that is common across the entire signal, whereas the low-rank weights accommodate for topological and dynamic changes for effective learning. \n\n\nWe conduct the following experiment to analyze the learning capacity of our low-rank weights in the case of dynamic scenes with varying complexity. We create three videos with different levels of difficulty: 1) A corner case when every frame depicts a novel scene (a video composed from the first 250 images from the DIV2K dataset), 2) the bikes video containing 6 segments, and 3) the cat video containing only 1 segment. All of these three videos are trimmed and cropped to the same length (250 frames) and resolution (\u200a351x510\u200a). \n\n\nThen, we perform the 2D video approximation (analogous to Sec. 4.1.) by learning the mapping from a space-time coordinate to RGB color (10% of pixels are not seen during the training and are left for evaluation). \n\n|  |     | Mean | | Random Video  | (250 segments) | Bikes Video  | (6 segments) | Cat Video  | (1 segment) | \n| :------------- |:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n|  | Rank     | test PSNR  | train PSNR | test PSNR  | train PSNR | test PSNR  | train PSNR | test PSNR  | train PSNR | \nSiren |  | 29.86 | 30.21 | 19.40 | 19.65 | 33.68 | 34.05 | 36.49 | 36.94 |\n +ResFields | 1 | 32.99 | 33.52 | 23.34 | 23.78 | 36.85 | 37.40 | 38.77 | 39.38 |\n +ResFields | 2 | 34.18 | 34.89 | 24.26 | 24.89 | 38.26 | 38.98 | 40.02 | 40.80 |\n +ResFields | 4 | 35.83 | 36.82 | 25.51 | 26.47 | 39.95 | 40.94 | 42.02 | 43.06 |\n +ResFields | 8 | 37.31 | 38.73 | 26.84 | 28.39 | 41.46 | 42.82 | 43.62 | 45.00 |\n +ResFields | 10 | 37.73 | 39.33 | 27.26 | 29.09 | 41.86 | 43.34 | 44.07 | 45.57 |\n +ResFields | 20 | 38.74 | 41.08 | 28.39 | 31.50 | 42.82 | 44.77 | 45.01 | 46.96 |\n +ResFields | 40 | 39.43 | 42.54 | 29.18 | 33.64 | 43.32 | 45.85 | 45.78 | 48.12 |\n\n\nGenerally, we observe that increasing the number of independent video segments indeed has a significant effect on the model performance. This can be attributed to the lack of information that can be shared across the entire signal and efficiently encoded in the base matrix weights. However, despite this, our method successfully improves quality over the vanilla Siren, even in the most challenging case of a random video (19.4 vs 29.18 PSNR for our model with rank 40). This improvement can be attributed to (a) the increased capacity of the pipeline thanks to residual modeling and (b) the ability of the neural network to discover small patterns that could be effectively compressed into shared weights even in the extreme case of a video with completely random frames. The latter behavior is especially apparent in the simplest case when ResFields uses only rank 1 (19.4 vs 23.34 PSNR for the random video sequence). Please also note that increasing the capacity in our case does not lead to significant computational overhead, as we demonstrate in the main submission and further emphasize in the next section."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699998676084,
                "cdate": 1699998676084,
                "tmdate": 1699998676084,
                "mdate": 1699998676084,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JQ2ncpjMd1",
                "forum": "EHrvRNs2Y0",
                "replyto": "HSseppnVFW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission573/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission573/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up"
                    },
                    "comment": {
                        "value": "Dear Reviewer KYty, \n\nWe would like to kindly inquire whether you have had the opportunity to review our updated manuscript, along with the responses addressing your raised discussion points. Your feedback is highly valuable to us, and we welcome any additional questions or comments you may have. \n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672904356,
                "cdate": 1700672904356,
                "tmdate": 1700672904356,
                "mdate": 1700672904356,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wr2L3nVBMZ",
            "forum": "EHrvRNs2Y0",
            "replyto": "EHrvRNs2Y0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission573/Reviewer_ZKNQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission573/Reviewer_ZKNQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose a way to extend MLP-based neural fields to have the capability to model time-dependent signals. The key ideas is to modulate the residual of the MLP weights by a time-dependent matrices decomposed into a vector matrix. This decomposition share a right basis, which allows reusing structures between time steps, providing regularity and potentially helping with generality. The paper shows good quantitative results that support the claim. I believe this idea is simple and effective, and it\u2019s can be a good contribution for the community."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea is very simple. The simplicity of such idea allows it to be combined with different design of neural fields as long as the main training parameters are parameterized by matrices.\n- The quantitative results shows that it\u2019s also effective to certain degree. The ablation studies also seems to support the effectiveness of techniques."
                },
                "weaknesses": {
                    "value": "- (minor) The paper provides little intuition of why this particular factorization is chosen rather than some alternative ways to factorize these matrices. For example, can we make v(t) to be N_ixR_i and M_i to be R_ixM_i? Is there any intuitive reason why this is not a good choice? Maybe this is addressed in the ablation study in Section 4.4, but providing some more intuition is good.\n- (minor) While the idea seems to support any matrix-like weight parameterization, the specific factorization might provide different interpretation depending on the way the network use the weight matrix. Maybe the specific design choice is limited to MLP architecture.\n- Concern about long sequence. The sequence weight at time t is modeled as W_o + \\Delta W_t. When t is very far from the original frame, then the weights \\Delta W_t can have significant weights, so it\u2019s not clear to me whether sharing the same matrix M across all time steps is a good idea. Maybe the author can consider periodically updating the weight W_o once a while?"
                },
                "questions": {
                    "value": "See weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission573/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815583175,
            "cdate": 1698815583175,
            "tmdate": 1699635984462,
            "mdate": 1699635984462,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dUzrcgnJb4",
                "forum": "EHrvRNs2Y0",
                "replyto": "wr2L3nVBMZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission573/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission573/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Alternative weight matrix factorization, key intuition (weaknesses 1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for considering our work simple and effective, and that it can be a good contribution to the community. In the following, we address the remaining discussion points: \n\n### 1. Alternative weight matrix factorization, key intuition. \n\nWe have explored diverse modeling and factorization techniques (Tab. 5, 6) based on prior work, including different tensor factorizations (CP, Tucker), however, we have not considered the proposed example. We will gladly include it in our ablation study (Tab. 6). \n\nThe goal of our parametrization is to achieve high learning capacity while retaining good generalization properties. To achieve this for a limited budget in terms of the number of parameters, we want to allocate as few as possible independent parameters per time step (represented by the vector $v(t)$) and as many globally shared parameters $M_i$. Allocating more capacity towards the shared weights will enable _1)_ the increased capacity of the model due to its ability to discover small patterns that could be effectively compressed in shared weights and _2)_ stronger generalization as the model is aware of the whole sequence through the shared weights. \n\nGiven these two objectives, we have designed $v(t)$ to have very few parameters (R) and $M_i$ to have the most parameters (RxNxM). \n\nIn light of this intuition, the factorization proposed by the reviewer appears to be suboptimal, as it generally allocates more capacity in the per-frame independent part $v(t)$ (NxR), and less into the shared weights $M_i$ (RxM). \n\nWe further verify this intuition quantitatively and show that the alternative design has a lower learning capacity, due to a larger number of independent parameters that need to be re-learned for every time step; for the same reason, this parametrization is more prone to overfitting. Below, we provide the extended Tab. 6 on the video approximation task:\n| Factorization | $v(t)$ |     $M_i$ | Rank | #params | test PSNR | train PSNR |\n| :-------------|:----|:----|:----:|:-------:|:---------:|:----------:|\n| proposed      |${N_i \\times R_i}$ | ${R_i \\times N_i}$          | 10   | 5.4     |     35.22  | 36.35     |\n| proposed      |${N_i \\times R_i}$ | ${R_i \\times N_i}$ | 20   | 10.0    |     35.88  | 37.50     |\n| proposed      |${N_i \\times R_i}$ | ${R_i \\times N_i}$  | 40   | 19.3    |     36.67  | 39.01     |\n|\n| ours          |${R_i}$| ${R_i \\times N_i \\times M_i }$| 10   | 8.7     |     39.59  | 40.80     |\n| ours          |${R_i}$| ${R_i \\times N_i \\times M_i }$| 20   | 16.5     |     40.87 | 42.45     |\n \nQuantitative results confirm that our parametrization achieves higher learning capacity (higher training PSNR) and stronger generalization (higher test PSNR) while using fewer parameters. \n\nSpecifically, our parametrization with the rank of 10 (8.7M parameters) compared to the proposed example with the rank of 40 (19.3M parameters) demonstrates:\n\n- test PSNR: **39.59** vs 36.67\n- train PSNR: **40.8** vs 39.01 \n- the number of parameters: **8.7M** vs 19.3M\n\nWe will include these results and further elaborate on our intuition in the revisited manuscript and the supplementary material."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700049003984,
                "cdate": 1700049003984,
                "tmdate": 1700049413633,
                "mdate": 1700049413633,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CvnvDnYJd3",
                "forum": "EHrvRNs2Y0",
                "replyto": "wr2L3nVBMZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission573/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission573/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up"
                    },
                    "comment": {
                        "value": "Dear Reviewer ZKNQ, \n\nWe would like to kindly inquire whether you have had the opportunity to review our updated manuscript, along with the responses addressing your raised discussion points. Your feedback is highly valuable to us, and we welcome any additional questions or comments you may have. \n\nBest regards,\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672855181,
                "cdate": 1700672855181,
                "tmdate": 1700672855181,
                "mdate": 1700672855181,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8wKMnSQlhR",
            "forum": "EHrvRNs2Y0",
            "replyto": "EHrvRNs2Y0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission573/Reviewer_4JwP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission573/Reviewer_4JwP"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of modeling non-static scenes with NeRF, where introducing a temporal dimension 't' significantly increases the required model capacity. A method for temporal information modeling is proposed, which by employing a low-rank representation, controls the amount of parameters to be learned. This approach enables temporal modeling without affecting the network size. Alongside, this paper innovates by integrating temporal residual layers in neural fields, dubbed ResFields, showcasing an effective way to represent complex temporal signals without increasing the size of Multi-Layer Perceptrons (MLPs), thus offering a promising solution to the capacity bottleneck for modeling and reconstructing spatiotemporal signals."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths:\nThe method proposed in this paper is simple and effective, with clear and easy-to-understand writing, and ample experiments.\nNovelty:\nThis paper introduces a lightweight, plug-and-play module to enhance NeRF's  capability for dynamic objects.\nGeneralization:\nThis paper applies ResFields to various base models and downstream tasks, achieving a certain performance improvement across the board."
                },
                "weaknesses": {
                    "value": "The main issue is that this paper lacks references to and comparisons with closely related methods, such as [1][2][3][4]. These works all address the problem of modeling non-static scenes, which is closely related to the theme of this paper. These methods primarily model a normalized space, then model the relationship between the 3D expression at each moment 't' and the normalized space. The absence of this comparison leads to (1) unclear performance advantages, and (2) uncertainty about whether ResFields could also be applied to these methods to further enhance temporal modeling capability."
                },
                "questions": {
                    "value": "1. The symbols in Equation 5 are not explained.\n2. Why are the results of NGP plus ResFields not shown in Table 1?\n3. Can ResFields, like LoRA[5], provide further dynamic modeling capabilities to a pre-trained NeRF?\n\n[1]Wang C, MacDonald L E, Jeni L A, et al. Flow supervision for Deformable NeRF[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 21128-21137.\n[2]Li Z, Wang Q, Cole F, et al. Dynibar: Neural dynamic image-based rendering[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 4273-4284.\n[3] Wang Y, Han Q, Habermann M, et al. Neus2: Fast learning of neural implicit surfaces for multi-view reconstruction[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 3295-3306.\n[4] Wang Q, Chang Y Y, Cai R, et al. Tracking Everything Everywhere All at Once[J]. arXiv preprint arXiv:2306.05422, 2023.\n[5]Hu E J, Shen Y, Wallis P, et al. Lora: Low-rank adaptation of large language models[J]. arXiv preprint arXiv:2106.09685, 2021."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission573/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission573/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission573/Reviewer_4JwP"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission573/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827439468,
            "cdate": 1698827439468,
            "tmdate": 1700654837827,
            "mdate": 1700654837827,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UmUNmYMd1p",
                "forum": "EHrvRNs2Y0",
                "replyto": "8wKMnSQlhR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission573/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission573/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comparisons to flow-supervised NeRF methods [1,2] (Weakness 1/2)"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for considering our work effective and easy to use, as well as for the insightful comments and references. In the following, we do our best to put the presented ResFields approach in the context of the provided related methods. We provide the quantitative evaluation when possible, and elaborate on the conceptual differences and limitations when a straightforward comparison is not feasible.\n\n### Comparisons to flow-supervised NeRF methods [1,2]\n\nPlease note that both methods [1,2] require optical flow supervision, which in practice is hard to obtain for scenes considered in the paper. The Owlii sequences exhibit a magnitude of uniformly colored texture patterns that has a significant impact on the optical-flow estimators (e.g. RAFT). \n\nUnfortunately, [1] has no published code at the present day, significantly limiting our ability to provide quantitative evaluation against the approach in a timely manner.\n\nWe have evaluated DynIBR [2] on the Owlii benchmark (Tab. 3). However, the method fully fails on the dataset with large camera view angle differences. This behavior can be partially attributed to the reliance of the method on the synthesized depth maps obtained from the off-the-shelf monocular depth regressors, which produce inconsistent results across largely varying camera views. Additionally, DynIBR being an image-based rendering technique in its nature, also exhibits a rather limited novel-view synthesis and geometry reconstruction capability and serves more as a video stabilization technique. Finally, training DynIBR (and its variation with ResFields) is not computationally feasible for us as the original method already requires 8 A100 GPUs for training. \n\nHowever, in the following we provide strong evidence that ResFields approach generally benefits the task of modeling _motion fields_, the fundamental building block of the aforementioned flow-based NeRF methods [1,2].\n\nSpecifically, DynIBR [2] uses a 256-neuron ReLU-MLP to model bi-directional motion fields by predicting coefficients of the DCT bases, while Deformable NeRF [1] uses a deformation network of Nerfies (128-neuron ReLU-MLP) to predict motion as SE(3). In both cases, the NeRF models are explicitly supervised with optical flow to learn motion/scene flow. \n\nTo demonstrate the benefit of ResFields for both of these motion fields, we use 4 sequences of DeformingThings from Sec. 4.2, and learn bi-directional motion fields. We use 80% of tracked mesh vertices to learn the motion field and the remaining 20% for evaluation. As a supervision, we use L1 error between the predicted and the ground truth motion.  \n\nThe table below reports the L1 error (scaled by x10^3) for the evaluation points. Additionally, we have included one additional modeling option from [A], where the motion field is simply predicted as an offset vector. \n\n\n|                        | it/s | flow type | fwd/bwd $l_1$ error |\n| :----------------------|:----:|:---------:|:-------------------:|\n| ReLU-MLP (128 neurons) |  **15.6** | offset | 6.88 / 7.31 |\n| + ResFields |  **15.6** | offset | **3.85 / 3.85** |\n|\n| ReLU-MLP (128 neurons) |  **15.6** | SE(3) | 4.57 / 4.58 |\n| + ResFields |  **15.6** | SE(3) | **2.64 / 2.56** |\n|\n| ReLU-MLP (128 neurons) |  **15.6** | DCT | 3.19 / 3.19 |\n| + ResFields            |  **15.6** | DCT | **2.18 / 2.19** |\n|\n|\n| ReLU-MLP (256 neurons) |  5.6 | offset | 6.50 / 6.44 |\n| + ResFields            |  5.6 | offset | **4.43 / 4.59** |\n|\n| ReLU-MLP (256 neurons) |  5.6 | SE(3) | 4.00 / 4.10 |\n| + ResFields            |  5.6 | SE(3) | **2.88 / 2.84** |\n|\n| ReLU-MLP (256 neurons) |  5.6 | DCT | 2.47 / 2.48 |\n| + ResFields            |  5.6 | DCT | **1.79 / 1.79** |\n\n\nNote that ResFields greatly benefits learning motion fields across all settings. Moreover, the particular architecture from DynIBR (DCT with 256 neurons) is less powerful than its much smaller and faster counterpart (128 neurons) with ResFields for both forward (2.18 vs 2.47) and backward (2.19 vs 2.48) motion while being around **three times faster** (16.5 vs 5.6 training it/s). \n\nReferences:\n\n[A]: Prokudin et. al., \u201cDynamic Point Fields\u201d ICCV 2023"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570199415,
                "cdate": 1700570199415,
                "tmdate": 1700570199415,
                "mdate": 1700570199415,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z9GGqGhVRk",
                "forum": "EHrvRNs2Y0",
                "replyto": "8wKMnSQlhR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission573/Reviewer_4JwP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission573/Reviewer_4JwP"
                ],
                "content": {
                    "comment": {
                        "value": "The rebuttal addresses my concerns well. I will raise the score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654860995,
                "cdate": 1700654860995,
                "tmdate": 1700654860995,
                "mdate": 1700654860995,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]