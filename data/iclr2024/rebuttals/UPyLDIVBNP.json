[
    {
        "title": "Fully Identical Initialization"
    },
    {
        "review": {
            "id": "tkS9y0A8DH",
            "forum": "UPyLDIVBNP",
            "replyto": "UPyLDIVBNP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9336/Reviewer_GXx1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9336/Reviewer_GXx1"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript proposes a method to initialise networks with identity matrices.\nSymmetry of the initialisation is broken by repeating the identity matrix\nand adding small (1e-6) perturbations to the diagonals.\nExtensions for convolutional layers and fixup-like initialisations are also presented.\nExperiments in both vision (CIFAR10 and ImageNet) and language (SST2, TREC6 and BERT pre-training) domains suggest better performance and faster convergence for various architectures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- (quality) Experimental results are presented with error bars.\n - (significance) This initialisation could reduce some of the randomness in training networks.\n   As a result, comparing networks should become easier.\n - (significance) The proposed initialisation should be simple enough to implement, which typically leads to fast adoption by practicioners.\n - (originality) Although the idea to use identity matrices for initialisation has been around for some time, it has typically been discarded as impractical due to the symmetries.\n    This is (to the best of my knowledge) the first work that implements an initialisation scheme that sticks so close to the identity matrix."
                },
                "weaknesses": {
                    "value": "- (clarity) The idea of dynamical isometry has been introduced in (Saxe et al., 2014).\n - (clarity) I would argue that the patch-maintain convolution is not very well motivated.\n   I believe the problem is that I do not understand how this relates to Ghostnets (Han et al., 2020).\n - (clarity) It also took me some time to realise that the channel-maintain convolution (as it is called in the appendix) which is described in the first paragraph of Section&nbsp;3.3.1 is something different from the proposed patch-maintain setting.\n   Note that this channel-maintain setting has also been used in (Xiao et al., 2018).\n - (clarity) The ablation experiment in Section&nbsp;4.4 is claimed to explain why channel-maintain is not as good as patch-maintain.\n   However, the explanation in section&nbsp;4.4 seems to indicate that this is just an ablation of the different components of the proposed solution.\n - (clarity) I can not find any explanation for the legend of Figure&nbsp;7&nbsp;(a).\n - (quality) The results in Table&nbsp;1 seem to correspond to GD, not SGD.\n   A quick experiment with SGD (batch-size 200) learns without problems.\n - (quality) The choice of hyper-parameters is not motivated properly and it is unclear how they were chosen.\n   Moreover, it seems like the same hyper-parameter settings were used for every network.\n   For a fair comparison, hyper-parameters should be set for each method individually.\n - (quality) An experiment without learning rate schedule, weight decay and other extras would be interesting for a more \"raw\" comparison between initialisation strategies.\n   \n\n### References\n\n - Saxe, A. M., McClelland, J. L., & Ganguli, S. (2014). \n   Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.\n   International Conference on Learning Representations.\n   https://arxiv.org/abs/1312.6120\n - Xiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S., & Pennington, J. (2018). \n   Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks. \n   In International Conference on Machine Learning (pp. 5393-5402). PMLR.\n   https://proceedings.mlr.press/v80/xiao18a"
                },
                "questions": {
                    "value": "1. Please, include the references listed in the weaknesses section.\n 2. What is the link between Ghostnet and the patch-maintain scheme?\n 3. Do you have a direct comparison between patch-maintain and channel-maintain schemes for IDInit?\n 4. Can you verify that using SGD instead of GD for the results in Table&nbsp;1 also resolves the stated problem?\n 5. Is it possible to tune the hyper-parameters for each method individually?\n 6. How do weight decay and learning rate schedule interact with the proposed initialisation scheme?\n 7. What is the difference between IDInit-0 and IDInit-10 or Kaiming-10 and Kaiming-40 in Figure&nbsp;7&nbsp;(a)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9336/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698692261182,
            "cdate": 1698692261182,
            "tmdate": 1699637175240,
            "mdate": 1699637175240,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hMpDhnTMkB",
                "forum": "UPyLDIVBNP",
                "replyto": "tkS9y0A8DH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9336/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GXx1 (1/3)"
                    },
                    "comment": {
                        "value": "> (clarity) The idea of dynamical isometry has been introduced in (Saxe et al., 2014).\n\nWe acknowledge the seminal work presented in [1] regarding the concept of dynamical isometry in the context of non-residual networks. The approach outlined in [1] focuses on achieving dynamical isometry by orthogonalizing network weights to ensure that the mean squared singular value of the Jacobian matrix remains close to 1, a significant advancement for non-residual network initialization.\n\nHowever, our work with IDInit diverges from [1] in its application to residual networks, which are among the most powerful and prevalent structures in current deep learning architectures. In technique, IDInit emphasizes the maintenance of identity transit through both the main and residual stems, and empirical results show the effectiveness.\n\nIn light of this, while IDInit and the method in [1] both target dynamical isometry, IDInit's contribution lies in its novel application to residual networks. This extension not only broadens the understanding of dynamical isometry but also showcases its practical utility in some of the most advanced neural network architectures today. We will include a detailed discussion of these distinctions and contributions in the revised manuscript.\n\n> Please, include the references listed in the weaknesses section.\n\nThank you for your suggestion. \\[1\\] pioneered the integration of dynamic isometry in nonlinear networks, enhancing training efficiency. Specifically targeting Convolutional Neural Networks (CNNs), Reference \\[2\\] formulated a mean field theory for signal propagation adhering to dynamic isometry principles, enabling the training of networks with up to 10,000 layers. Different from these approaches, the proposed IDInit maintains identity in both the main and residual stems to preserve dynamic isometry to achieve good performance and fast convergence. Furthermore, we adapt the transformation on the identity matrix to accommodate various scenarios, such as nonsquare matrices and higher-order tensors. We will include these references in the revision.\n\n> (clarity) I would argue that the patch-maintain convolution is not very well motivated. I believe the problem is that I do not understand how this relates to Ghostnets (Han et al., 2020).\n>\n> What is the link between Ghostnet and the patch-maintain scheme?\n\nThanks for the useful comment. Ghostnet \\[3\\] highlights the critical role of channel diversity in enhancing network performance. This insight serves as a foundational motivation for our patch-maintain scheme. In contrast to the channel-maintain approach, which tends to replicate channel features and thus might limit diversity, our patch-maintain scheme introduces a novel strategy to increase channel differentiation. By shifting features, the patch-maintain scheme aims to add variability and uniqueness across channels. This method is designed to capitalize on the principle articulated in Ghostnet \u2013 that channel diversity is beneficial for performance. This linkage between the need for channel diversity (as emphasized by Ghostnet) and our patch-maintain scheme is a key aspect of our work. We will include this discussion in the revision.\n\n\n> (clarity) It also took me some time to realise that the channel-maintain convolution (as it is called in the appendix) which is described in the first paragraph of Section\u00a03.3.1 is something different from the proposed patch-maintain setting. Note that this channel-maintain setting has also been used in (Xiao et al., 2018).\n> \n> (clarity) The ablation experiment in Section\u00a04.4 is claimed to explain why channel-maintain is not as good as patch-maintain. However, the explanation in section\u00a04.4 seems to indicate that this is just an ablation of the different components of the proposed solution.\n>\n> Do you have a direct comparison between patch-maintain and channel-maintain schemes for IDInit?\n\nSorry for the confusion. In this work, we introduce the patch-maintain scheme (i.e., IDIC$_{\\tau}$) as our novel method for transforming a matrix into a convolutional format.\n\nOn the other hand, the channel-maintain method, referenced from prior studies [2][4], serves as a comparison baseline in our experiments, denoted as 'w/o IDIC$_{\\tau}$'. In Section 4.4, we present this comparative analysis, where we demonstrate that the patch-maintain scheme significantly enhances the performance of IDInit. To address the clarity issues raised, we will revise our manuscript for more explicit elaboration."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484487701,
                "cdate": 1700484487701,
                "tmdate": 1700484659996,
                "mdate": 1700484659996,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nOPIHcU1tP",
                "forum": "UPyLDIVBNP",
                "replyto": "hMpDhnTMkB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9336/Reviewer_GXx1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9336/Reviewer_GXx1"
                ],
                "content": {
                    "title": {
                        "value": "Last-minute questions"
                    },
                    "comment": {
                        "value": "- I still do not quite understand why the patch-maintain scheme would lead to more channel diversity. \n  After all, reshaping the kernels to $\\mathbb{R}^{c_\\mathrm{out} \\times c_\\mathrm{in}k^2}$ leads to a matrix with much more columns than rows and therefore a lot of zeros. \n  A quick test with the provided code seems to confirm that this leads to most kernels being completely zero.\n  How can this lead to more channel diversity?\n - I would be very interested in the ablation experiments with individually tuned hyper-parameters for each setting (cf. RTable1 and RTable 2, but maybe even with learning rate scheduling disabled or also tuned).\n - Concerning Table 1 (addressed in response 2/3): I noticed that I did not disable bias parameters as you did in the provided code.\n  This would indicate that this problem can be resolved by simply adding bias parameters, which is typically done anyway.\n  Therefore, I still believe that the results in Table 1 are a contrived example. \n  Would that be a fair assessment?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595473429,
                "cdate": 1700595473429,
                "tmdate": 1700595473429,
                "mdate": 1700595473429,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "urYQq4NGjV",
                "forum": "UPyLDIVBNP",
                "replyto": "tkS9y0A8DH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9336/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Last-minute Questions (3/3)"
                    },
                    "comment": {
                        "value": "Thank you for your observation regarding Table 1. This table was designed to address the convergence issues highlighted in reference [5], where the authors analyzed a network comprising entirely linear layers without biases and optimized it using Gradient Descent (GD). In line with their setting, we constructed a 3-layer fully linear network without biases as our experimental model. This choice was not an attempt to contrive a specific outcome but rather to adhere closely to the conditions set out in [5]. Therefore, this is a fair assessment.\n\nOur primary objective is to demonstrate that the convergence problem identified in [5] is readily solvable. To this end, any evidence that contributes to solving this convergence problem aligns with our goal. Consequently, our findings that SGD with momentum aids in resolving convergence issues, and that momentum can further expedite this process, are pertinent. Similarly, your observation that incorporating bias parameters might facilitate convergence is equally valuable and does not contradict the purpose of our experiment.\n\nIn summary, we appreciate your contribution to this discussion, as it helps uncover various potential solutions to the convergence problem. These findings, including the use of SGD with momentum, smaller batch sizes, and the addition of biases, do not contradict the central premise of Table 1; rather, they provide supporting evidence for the ease of resolving convergence issues. We find these insights intriguing and plan to include them in our next revision to offer a more comprehensive understanding of the IDInit approach.\n\n[5] Bartlett, Peter, et al. \"Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks.\"\u00a0_International conference on machine learning_. PMLR, 2018."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683944934,
                "cdate": 1700683944934,
                "tmdate": 1700684059715,
                "mdate": 1700684059715,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QYoo1SHEO1",
            "forum": "UPyLDIVBNP",
            "replyto": "UPyLDIVBNP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9336/Reviewer_eQMM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9336/Reviewer_eQMM"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the convergence problem in deep networks and proposes a Fully Identical Initialization (IDInit) method that initializes the weights with an identity matrix. The authors propose additional techniques such as momentum, padding, and reshaping to improve convergence and performance.\n\nThe overall method has some interesting aspects:\n* Patch-Maintain Convolution is introduced as a technique to enhance the universality of IDInit for convolutional layers. It reshapes the convolutional kernel initialized with IDInit to increase feature diversity and improve model performance.\n* The issue of dead neurons is tackled by selecting some elements to a small numerical value and increasing trainable neurons.\n* The paper discusses the theoretical analysis of IDInit, including the Jacobian and gradient update equations in residual neural networks.\n\nFinally, the authors address the limitations and potential concerns of IDInit, such as its deterministic nature and the need for further exploration in different scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "### S1 - Good technical contributions\nThe paper's technical contributions are significant in these aspects:\n* IDInit improves the convergence speed, stability, and final performance of deep neural networks, addressing a critical issue in deep learning.\n* The additional techniques proposed, such as Patch-Maintain Convolution and recovering dead neurons, enhance the universality and robustness of IDInit.\n---\n\n### S2 - Theoretical and Experimental analysis\n* The paper discusses the theoretical analysis of IDInit, including the Jacobian and gradient update equations in residual neural networks.\n* The experiments are well-designed and conducted on various network architectures and tasks, demonstrating the effectiveness and superiority of IDInit.\n---\n\n### S3 - Novelty (similar to prior works, but with additional novel contributions)\n* Identity init is not new and has been explored in prior works (e.g. ISONet, ZeroO). However, this paper generalizes the Identity Init to various general architectures and activation functions, which is interesting."
                },
                "weaknesses": {
                    "value": "### W1 - Marginal or no improvement compared to Kaiming init\nMy biggest concern is the Cifar-10 performance compared to the simple Kaiming initialization.\n* Table 2 shows that using SGD optimizer (which gives the best performance all across), Kaiming init obtains almost the same performance (93.36 v/s 93.41 and 94.06 v/z 94.04) as IDInit, while being only slightly slower. \n* This brings into question the practical utility of the proposed initialization.\n\n---\n\n### W2 - Comparisons with other init methods on ImageNet\nIDInit is compared with other initialization methods only on Cifar-10, which is very small-scale. No such comparisons have been shown on ImageNet. I think it's important to see if the proposed init is even useful when training on large-scale datasets.\n\n---\n\n### W3 - Theoretical analysis limitations\nWhile the paper provides a theoretical analysis of IDInit, it mainly focuses on the Jacobian and gradient update equations in residual neural networks. It would be valuable to extend the theoretical analysis to other network architectures and provide a more comprehensive understanding of the underlying principles of IDInit.\n\n---\n\n### W4 - Limited discussion on limitations \nAlthough the paper briefly mentions the limitations of IDInit, such as the deterministic nature and the need for momentum to handle negative eigenvalues, further discussion and analysis of these limitations would provide a more comprehensive understanding of the potential drawbacks and challenges of implementing IDInit in practical scenarios."
                },
                "questions": {
                    "value": "1. Reference to weakness W2, can you please provide more insight into the efficacy of the propose init method on large-scale training datasets, like ImageNet?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9336/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9336/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9336/Reviewer_eQMM"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9336/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698751635067,
            "cdate": 1698751635067,
            "tmdate": 1699637175112,
            "mdate": 1699637175112,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PoSmCqM0Cz",
                "forum": "UPyLDIVBNP",
                "replyto": "QYoo1SHEO1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9336/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eQMM (1/2)"
                    },
                    "comment": {
                        "value": "> My biggest concern is the Cifar-10 performance compared to the simple Kaiming initialization.\n> - Table 2 shows that using SGD optimizer (which gives the best performance all across), Kaiming init obtains almost the same performance (93.36 v/s 93.41 and 94.06 v/z 94.04) as IDInit, while being only slightly slower.\n> - This brings into question the practical utility of the proposed initialization.\n\nThanks for the reviewer\u2019s comment. While both IDInit and Kaiming initialization methods perform similarly in certain settings, our research indicates that IDInit offers superior stability and adaptability across a wider range of scenarios. To verify this claim, we have conducted additional experiments with varied hyperparameters, particularly focusing on weight decay and learning rate, using ResNet-56. \n\nRTable 1. Hyperparameters for IDInit on ResNet-56.\n\n|         | lr=1e0                | lr=1e-1               | lr=1e-2               | lr=1e-3               |\n| ------- | --------------------- | --------------------- | --------------------- | --------------------- |\n| wd=1e-1 | 10.00$_{\u00b10.00}$ | 18.27$_{\u00b10.32}$ | 74.46$_{\u00b13.52}$ | 88.67$_{\u00b10.23}$ |\n| wd=1e-2 | 18.10$_{\u00b11.95}$ | 90.37$_{\u00b10.14}$ | 94.18$_{\u00b10.06}$ | 84.07$_{\u00b10.53}$ | \n| wd=1e-3 | 89.18$_{\u00b10.24}$ | 94.64$_{\u00b10.16}$ | 89.99$_{\u00b10.06}$ | 81.65$_{\u00b10.69}$ |\n| wd=1e-4 | 94.97$_{\u00b10.04}$ | 92.60$_{\u00b10.05}$ | 88.83$_{\u00b10.30}$ | 82.47$_{\u00b10.12}$ |\n\nRTable 2. Hyperparameters for Kaiming on ResNet-56.\n\n|         | lr=1e0                | lr=1e-1               | lr=1e-2               | lr=1e-3               |\n| ------- | --------------------- | --------------------- | --------------------- | --------------------- |\n| wd=1e-1 | 10.00$_{\u00b10.00}$ | 17.39$_{\u00b10.47}$ | 48.98$_{\u00b14.11}$ | 82.36$_{\u00b10.42}$ |\n| wd=1e-2 | 14.43$_{\u00b14.78}$ | 90.10$_{\u00b10.11}$ | 94.19$_{\u00b10.09}$ | 55.44$_{\u00b11.07}$ |\n| wd=1e-3 | 87.78$_{\u00b11.20}$ | 94.64$_{\u00b10.12}$ | 82.44$_{\u00b10.49}$ | 49.85$_{\u00b12.38}$ |\n| wd=1e-4 | 94.89$_{\u00b10.37}$ | 91.52$_{\u00b10.33}$ | 78.42$_{\u00b10.06}$ | 50.32$_{\u00b12.71}$ |\n\nAs shown in RTable 1 and RTable 2, these experiments reveal that IDInit consistently surpasses Kaiming initialization in almost every setting tested. Most notably, in scenarios with smaller learning rates (e.g., 1e-2 and 1e-3), we observe a significant drop in the performance of models initialized with Kaiming. In contrast, IDInit maintains high accuracy levels under these conditions. This stability of IDInit is attributable to its ability to achieve isometry dynamics in the residual stem, a crucial factor for maintaining performance stability.\n\nFurther, we have extended our experimentation to include ResNet-32 on the CIFAR-10 dataset, as detailed in Section F.1 of the uploaded revision PDF. These additional results consistently demonstrate that IDInit not only matches but often exceeds the performance of Kaiming initialization, especially under varying and challenging hyperparameter settings.\n\nIn summary, while Kaiming initialization performs comparably in certain standard scenarios, IDInit exhibits greater performance consistency and stability across a broader spectrum of conditions. This enhanced stability and adaptability make IDInit a more practically valuable choice for diverse neural network applications. We will ensure that these findings are clearly presented in our revised manuscript to underscore the practical utility and advantages of IDInit over traditional initialization methods like Kaiming."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483873973,
                "cdate": 1700483873973,
                "tmdate": 1700483873973,
                "mdate": 1700483873973,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "R6mC9VqiFZ",
            "forum": "UPyLDIVBNP",
            "replyto": "UPyLDIVBNP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9336/Reviewer_FbDy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9336/Reviewer_FbDy"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a technique called Identical Initialization (IDInit), which uses identity matrices and their variants to initialize weights. They discussed the convergence problem and dead neuron problem for common identity initialization and previous works. They explore the application of this technique to non-square matrices, residual architectures, and convolutional operations. Empirical evaluation demonstrate its performance on vision and languages tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written. The authors clearly describe the problem and their methodology. They also conduct extensive empirical evaluations. \nHow to make identity initialization works in practice is an interesting question and I believe it is an novel direction to explore."
                },
                "weaknesses": {
                    "value": "1. Theoretical analysis seems incorrect and its proof lacks details. In Theorem 3.1, the author claims IDI breaks rank constraint such that its residual has rank more than D_0. However, in the proof in the appendix, the author only shows the full matrix has rank more than D_0, which is not the same as the residual. Please provide more details in the proof to justify your claim.\n2. The authors claim that the rank constraint can be broken by IDI even when non-linearity like ReLU is not applied. It seems contradict approximation theory which emphasizes the importance of non-linearity to ensure expressivity. It would be great for authors to provide more insights on this point.\n3. Authors mention that dead neurons problem happens when batch normalization set to 0 or downsampling operation cause 0 filled features. However, these cases are not common in practice and it's better to motivate more on why IDIZ is important.\n4. Insufficient explanation on why momentum is important to solve convergence problem of IDInit. It would be great to provide some theoretical insights to support this factor."
                },
                "questions": {
                    "value": "1. What is the meaning of zero down-sampling in Table 2, is this a special downsampling operation compared to standard downsampling (like avgpooling) in ResNet?  \n2. It would be great to compare IDI, IDI with loose condition, and IDIZ together to show the effectiveness of IDIZ."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9336/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699343232301,
            "cdate": 1699343232301,
            "tmdate": 1699637175008,
            "mdate": 1699637175008,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1IIjWjEBSi",
                "forum": "UPyLDIVBNP",
                "replyto": "R6mC9VqiFZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9336/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FbDy (1/3)"
                    },
                    "comment": {
                        "value": "> Theoretical analysis seems incorrect and its proof lacks details. In Theorem 3.1, the author claims IDI breaks rank constraint such that its residual has rank more than D_0. However, in the proof in the appendix, the author only shows the full matrix has rank more than D_0, which is not the same as the residual. Please provide more details in the proof to justify your claim.\n\nWe appreciate the reviewer's feedback regarding the theoretical analysis and acknowledge the need for further elaboration. However, our claim is indeed correct, and we would like to provide additional details for clarity.\n\nFirstly, it is important to understand that the rank of $\\theta^{(0)}$ exceeding $D_0$\u200b directly leads the distinct values of the feature $\\{x^{(k)}\\}_{k=1}^{L-1}$ to be larger than $D_0$\u200b. Then, since the gradient of $\\theta^{(k)}$ is calculated as $\\frac{\\partial \\mathcal{L}}{\\partial \\theta^{(k)}}=\\frac{\\partial \\mathcal{L}}{\\partial x^{(k)}}\\circ x^{(k-1)}$ for $k\\in \\{1, 2, \\dots, L-2\\}$, the rank of $\\frac{\\partial \\mathcal{L}}{\\partial \\theta^{(k)}}$ will be larger than $D_0$. As a result, with a learning rate $\\mu$, the subsequent $\\theta^{(k)}$ updated as $\\theta^{(k)} = \\theta^{(k)} - \\mu \\frac{\\partial \\mathcal{L}}{\\partial \\theta^{(k)}}$, will also have a rank greater than $D_0$\u200b. This effectively resolves the rank constraint issue. Furthermore, this is also supported by the empirical evidence supporting as presented in Figure 4, where IDInit's success in surpassing the rank of $D_0$\u200b is demonstrated.\n\nWe believe that this additional explanation clarifies the theoretical foundation of our approach and addresses the concerns raised. We will revise the proof in the revised manuscript to ensure a comprehensive understanding of our methods and findings.\n\n\n> The authors claim that the rank constraint can be broken by IDI even when non-linearity like ReLU is not applied. It seems contradict approximation theory which emphasizes the importance of non-linearity to ensure expressivity. It would be great for authors to provide more insights on this point.\n\nWe appreciate the reviewer's point regarding the possible contradiction between IDInit and the approximation theory. To clarify, our claim is that IDInit has the capacity to break the rank constraint even without traditional non-linearities such as ReLU. This does not negate the significance of non-linearities in enhancing expressivity, as emphasized by approximation theory. In more detail, our work demonstrates that IDInit's mechanism allows for an increase in the rank of learned representations through padding the identity matrix repeatedly., independent of non-linear transformations. When non-linearities like ReLU are introduced, as per approximation theory, they indeed augment the expressivity of the IDInit. In summary, IDInit, in conjunction with non-linear activations, provides a compounded benefit to the model's expressivity. Therefore, IDInit's impact is additive to the benefits conferred by non-linearities, rather than contradictory.\n\n> Authors mention that dead neurons problem happens when batch normalization set to 0 or downsampling operation cause 0 filled features. However, these cases are not common in practice and it's better to motivate more on why IDIZ is important.\n\nThank you for the comment on IDIZ in addressing the issue of dead neurons, particularly in relation to batch normalization (BN) and downsampling operations. However, we contend that the scenarios we described are common in practice.\n\n- Batch Normalization: The practice of setting the gamma parameter of the last BN layer in a ResNet block to zero is not only recommended for enhanced performance as indicated in references \\[1\\]\\[2\\]\\[3\\], but also a default setting in widely-used packages like timm \\[4\\]. This approach, while improving performance, can inadvertently lead to dead neurons, a problem IDIZ aims to mitigate.\n\n- Downsampling: In common downsampling practices, there are two prevalent approaches: (i): Using pooling (e.g., avgpool) to reduce feature resolution, coupled with zero-padding to expand channel size (as shown in Line 40 of \\[5\\] and option 'A' in Line 66 of \\[6\\]). (ii): Employing convolution to simultaneously reduce resolution and expand channel size (evident in Line 241 of \\[7\\] and option 'B' in Line 72 of \\[6\\]). In particular, for the widely-used datasets like CIFAR-10, option (i) is frequently preferred.\n\nIDIZ is designed to generalize the utilization of IDInit by solving dead neurons of above conditions which is also common settings. So that IDIZ can be used more widely for addressing the disharmonious nature of identity-control methods, which avoids potential risks in compatibility with other techniques. We believe this is important characteristic to an initialization algorithm.\n\nWe will revise our manuscript to better discuss these points, ensuring that the relevance and utility of IDIZ are clearly and concisely presented."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483202097,
                "cdate": 1700483202097,
                "tmdate": 1700483202097,
                "mdate": 1700483202097,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a9s3Gytxgw",
                "forum": "UPyLDIVBNP",
                "replyto": "1IIjWjEBSi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9336/Reviewer_FbDy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9336/Reviewer_FbDy"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for author's detailed response. However, I still have questions about your response on Theorem 3.1.\n\n=> \"Firstly, it is important to understand that the rank of $\\theta^{(0)}$ exceeding $D_0$ directly leads the distinct values of the feature $x^{(k)}{ }_{k=1}^{L-1}$ to be larger than $D_0$. Then, since the gradient of $\\theta^{(k)}$ is calculated as $\\frac{\\partial \\mathcal{L}}{\\partial \\theta^{(k)}}=\\frac{\\partial \\mathcal{L}}{\\partial x^{(k)}} \\circ x^{(k-1)}$ for $k \\in 1,2, \\ldots, L-2$, the rank of $\\frac{\\partial \\mathcal{L}}{\\partial \\theta^{(k)}}$ will be larger than $D_0$.\"\n\nI think this is still incorrect. First, the rank of $\\theta^{(0)}$ can not exceed $D_0$ as it has the shape of $D_h \\times D_0$ (where $D_h > D_0$). Second, I want to kindly remind the authors that number of distinct values of the feature is not strictly correlated with the rank of the weight derivative matrix. \n\nFor example, considering we have $N$ input vectors $x^{0} \\in D_0$, we know the rank of $span(x^{0,1}, ..., x^{0,N})$ is smaller or equal to $D_0$. For $x^{1} = \\theta^{(0)} x^{0}$, even when $\\theta^{(0)}$ has full rank $D_0$, the rank of $span(x^{1,1}, ..., x^{1,N})$ is still smaller or equal to $D_0$, due to the nature of linear transformation.\nNow we analysis $\\frac{\\partial \\mathcal{L}}{\\partial \\theta^{(k)}}= \\sum^{N}_{i=1} \\frac{\\partial \\mathcal{L}}{\\partial x^{(k,i)}} \\circ x^{(k-1,i)}$ (assuming a full-batch gradient descent, you were using $\\frac{\\partial \\mathcal{L}}{\\partial x^{(k)}} \\circ x^{(k-1)}$ which is a single-batch GD and the matrix is rank-1 only). \n\nTake $x^{1}$ as a demonstration and assume $D_L > D_0$ without losing generality. We know the rank of $\\frac{\\partial \\mathcal{L}}{\\partial \\theta^{(1)}}$ can not exceeed $D_0$ given that \nthe rank of $span(x^{1,1}, ..., x^{1,N})$ is smaller or equal to $D_0$. \n\nIn other words, $\\frac{\\partial \\mathcal{L}}{\\partial \\theta^{(1)}}$ consists of $N$ rank-1 matrices, the summation of which can not exceed $D_0$. Thus, I don't agree the authors' claim that \"the rank of $\\frac{\\partial \\mathcal{L}}{\\partial \\theta^{(k)}}$ will be larger than $D_0$\".\n\nThe authors mention the empirical showcase in Figure 4. Is the 3-layer network in Figure 4 a linear network or a non-linear network? It should be a linear 3-layer network if the authors want to demonstrate Theorem 3.1 empirically."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623731025,
                "cdate": 1700623731025,
                "tmdate": 1700623731025,
                "mdate": 1700623731025,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iUDPCoNU2z",
                "forum": "UPyLDIVBNP",
                "replyto": "R6mC9VqiFZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9336/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> In other words, $\\frac{\\partial \\mathcal{L}}{\\partial \\theta^{(1)}}$ consists of $N$ rank-1 matrices, the summation of which can not exceed $D_0$. Thus, I don't agree the authors' claim that \"the rank of $\\frac{\\partial \\mathcal{L}}{\\partial \\theta^{(k)}}$ will be larger than $D_0$\".\n>\n> The authors mention the empirical showcase in Figure 4. Is the 3-layer network in Figure 4 a linear network or a non-linear network? It should be a linear 3-layer network if the authors want to demonstrate Theorem 3.1 empirically.\n\nThank you for your insightful observations. We acknowledge your point regarding the limitation of $\\frac{\\partial \\mathcal{L}}{\\partial \\theta^{(k)}}$ to $D_0$. \u200bHowever, we maintain that Theorem 3.1 remains valid. In a scenario using full-batch gradient descent for a single round,  $\\theta^{(k)}$ indeed retains a rank of at most $D_0$. Nevertheless, when employing Stochastic Gradient Descent (SGD), the weight update process involves iteratively adding gradients of rank $D_0$. As noted in reference [9], the addition of lower-rank matrices can increase the overall rank. This implies that the rank of $\\theta^{(k)}$ can be increased, provided the gradients are sufficiently independent, achievable through the use of ample training samples.\n\nTo clarify this concept, we utilized a 3-layer network for analysis. Following the notations from Section A.3 in the appendix and assuming $D_h=2D_L=2D_0$\u200b, we demonstrate the process using SGD. After the initial training step with a sample $x_1^{(0)}$\u200b, the gradient is calculated by\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_1^{(1)}}=\\begin{pmatrix*}\n\t\\mu\\Pi & \\mu\\Pi  \\\\\\\\\n\t\\mathbf{0} & \\mathbf{0}\n\t\\end{pmatrix*},$$\nwhere $\\Pi = \\frac{\\partial L}{\\partial x_1^{(3)}} \\circ x_1^{(0)} \\in \\mathbb{R}^{D_L\\times D_0}$. Continuing with a second sample $x_2^{(0)}$\u200b, there is\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_2^{(1)}}=\\begin{pmatrix*}\n(I-\\mu\\Pi)M(I-\\mu\\Pi)x_2^{(0)} & (I-\\mu\\Pi)Mx_2^{(0)}  \\\\\\\\\n-\\mu\\Pi M(I-\\mu\\Pi)x_2^{(0)} & -\\mu\\Pi Mx_2^{(0)} \n\\end{pmatrix*},$$\nwhere $M=\\frac{\\partial \\mathcal{L}}{\\partial x_2^{(3)}}$. Thus, the residual step can be calculated as\n$$\\hat{\\theta}^{(1)}=\\theta^{(1)}-I=\\begin{pmatrix*}\n-\\mu\\Pi - (I-\\mu\\Pi)M(I-\\mu\\Pi)x_2^{(0)} & -\\mu\\Pi - (I-\\mu\\Pi)Mx_2^{(0)}  \\\\\\\\\n\\mu\\Pi M(I-\\mu\\Pi)x_2^{(0)} & \\mu\\Pi Mx_2^{(0)} \n\\end{pmatrix*}.$$\nWithout loss of generality, assuming $rank(\\Pi)=D_0$, the rank of $\\hat{\\theta}^{(1)}$ should exceed $D_0$\u200b.\n\nRegarding your query about Figure 4, the network used in this experiment is indeed a linear 3-layer network, as per the requirements to empirically demonstrate Theorem 3.1. There are no non-linear activation functions involved, ensuring the network\u2019s alignment with the theorem's conditions.\n\nWe appreciate your feedback and will ensure to include a comprehensive revision and discussion of this aspect in our next manuscript update. We hope this response addresses your concerns effectively."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683332234,
                "cdate": 1700683332234,
                "tmdate": 1700683425648,
                "mdate": 1700683425648,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ti4wNGPt8s",
                "forum": "UPyLDIVBNP",
                "replyto": "iUDPCoNU2z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9336/Reviewer_FbDy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9336/Reviewer_FbDy"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your clarification. However, there is a fundamental error in your proof: the vector outer product always has rank-1 if two vectors are non-zero. $rank(\\prod)$ can not be $D_0$ (if $D_0 > 1$) but 1 at most."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684592522,
                "cdate": 1700684592522,
                "tmdate": 1700684592522,
                "mdate": 1700684592522,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jnkwQVfgGN",
                "forum": "UPyLDIVBNP",
                "replyto": "R6mC9VqiFZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9336/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the great insight. Indeed, it is accurate that the rank of the outer product of two non-zero vectors is at most 1. However, for batch data with size $m \\ge D_0$, the matrix $\\Pi$ can still achieve a rank of $D_0$. This is due to the aggregation of multiple rank-1 matrices arising from each instance in the batch. As these rank-1 matrices are derived from different instances, their cumulative effect can lead to an overall increase in the rank of the aggregated matrix $\\Pi$.\n\nFurthermore, we also empirically validate that even with a batch size of 1, through iterative training steps, the rank of $\\hat{\\theta}^{(1)}$ can exceed $D_0$. This phenomenon occurs due to the successive accumulation of gradients over multiple steps.\n\nThanks again for your thoughtful question! We are happy to discuss more if you have any other questions."
                    },
                    "title": {
                        "value": "Thank you for the reply"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688706128,
                "cdate": 1700688706128,
                "tmdate": 1700721027013,
                "mdate": 1700721027013,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HZpi1ZHdCf",
                "forum": "UPyLDIVBNP",
                "replyto": "R6mC9VqiFZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9336/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Updated Proof of Theorem 3.1"
                    },
                    "comment": {
                        "value": "Dear Reviewer FbDy,\n\nWe appreciate your feedback and have taken the opportunity to clarify and revise the proof of Theorem 3.1. This revised proof, along with a detailed explanation, is also included in Section A.3 in the appendix in the uploaded revision PDF. In this revised analysis, we focus on batch data rather than single data points.\n\nAssume weights are updated with the stochastic gradient descent (SGD). Without loss of generality, we set $D_h = 2D_0 = 2D_L$. Given two batches of inputs as $\\{x_1^{(0, 1)}, x_1^{(0, 2)}, \\dots, x_1^{(0, N)}\\}\\in \\mathbb{R}^{D_0}$ and $\\{x_2^{(0, 1)}, x_2^{(0, 2)}, \\dots, x_2^{(0, N)}\\}\\in \\mathbb{R}^{D_0}$, where $N \\ge D_0$ is the batch size. Therefore, the initial gradient of $\\theta^{(1)}$ are\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta^{(1)}} = \n\\begin{pmatrix*}\n    \\Pi & \\Pi \\\\\\\\\n    \\mathbf{0} & \\mathbf{0} \\\\\n\\end{pmatrix*},\n$$\n\nwhere $\\frac{\\partial \\mathcal{L}}{\\partial \\theta^{(0)}} \\in \\mathbb{R}^{D_h \\times D_0}$, $\\frac{\\partial \\mathcal{L}}{\\partial \\theta^{(1)}} \\in \\mathbb{R}^{D_h \\times D_h}$, $\\frac{\\partial \\mathcal{L}}{\\partial \\theta^{(2)}} \\in \\mathbb{R}^{D_L \\times D_h}$, $\\Pi = \\frac{1}{N}\\sum^N_{i=1}\\frac{\\partial L}{\\partial x^{(L)}} \\circ x_1^{(0, i)} \\in \\mathbb{R}^{D_L\\times D_0}$, and $\\mathbf{0}$ is a zero values. $\\circ$ denotes outer production. \n\nAfter training with the second data batch, the gradient is calculated as follows:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta^{(1)}}=\n\\begin{pmatrix*}\n(I-\\mu\\Pi)M(I-\\mu\\Pi)K & (I-\\mu\\Pi)MK  \\\\\\\\\n-\\mu\\Pi M(I-\\mu\\Pi)K & -\\mu\\Pi MK \n\\end{pmatrix*},\n$$\nwhere $M=\\frac{\\partial \\mathcal{L}}{\\partial x_2^{(3)}}$ and $K=\\frac{1}{N}\\sum^N_{i=1}x_2^{(0, i)}$. This leads to the following residual component:\n$$\n\\hat{\\theta}^{(1)} =\n\\theta^{(1)} - I =\n\\begin{pmatrix*}\n-\\mu\\Pi - \\mu(I-\\mu\\Pi)M(I-\\mu\\Pi)K & -\\mu\\Pi - \\mu(I-\\mu\\Pi)MK  \\\\\\\\\n\\mu^2\\Pi M(I-\\mu\\Pi)K & \\mu^2\\Pi MK\n\\end{pmatrix*},\n$$\nWithout loss of generality, assuming $rank(\\Pi)=D_0$, we can conclude\n$$\n    rank(\\hat{\\theta}^{(1)}) \\ge D_0.\n$$\nTherefore, IDInit can break the rank constraint by achieving the rank of $\\hat{\\theta}^{(1)}$ larger than $D_0$.\n\nWe sincerely hope that this revision addresses your concern. If there are any further questions or clarifications needed, please do not hesitate to let us know.\n\nKind regards,\n\nAuthors"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721490026,
                "cdate": 1700721490026,
                "tmdate": 1700721689767,
                "mdate": 1700721689767,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]