[
    {
        "title": "Out-of-domain Fact Checking"
    },
    {
        "review": {
            "id": "uvHVCU3MA3",
            "forum": "T629ezwzxI",
            "replyto": "T629ezwzxI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8415/Reviewer_mmrP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8415/Reviewer_mmrP"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses the challenges in evaluating the accuracy of everyday claims and highlights the limitations of large commercial language models. This work focuses more on improving the generalizability of fact-checking models by training tailored retrieval models. The authors propose an adversarial algorithm to make the retriever component more robust against distribution shifts. This method includes training a bi-encoder on labeled source data and adversarially training separate document and claim encoders using unlabeled target data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed problem is critical and interesting.\n2. The retrieval results seem more robust and the retrieval model conducts more generalized results."
                },
                "weaknesses": {
                    "value": "1. Some unsupervised methods are not compared, such as BM25. These methods have no domain shift problem.\n2. The introduction describes the generalization ability of GPT3/4 on the fact verification task, but no experiments of GPT-3/4 are conducted.\n3. The adversarial learning has been explored in previous work[1]. This work should discuss it.\n4. Lots of related work[2-5] on fact verification is not discussed. I only list some of them.\n\n[1] Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations.\n[2] Fine-grained Fact Verification with Kernel Graph Attention Network.\n[3] Exploring Listwise Evidence Reasoning with T5 for Fact Verification.\n[4] GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification.\n[5] GERE: Generative Evidence Retrieval for Fact Verification"
                },
                "questions": {
                    "value": "Why do you not directly evaluate the fact verification performance of GPT-3/4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8415/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8415/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8415/Reviewer_mmrP"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8415/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697813615536,
            "cdate": 1697813615536,
            "tmdate": 1699637048187,
            "mdate": 1699637048187,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JndhJLJj2a",
                "forum": "T629ezwzxI",
                "replyto": "uvHVCU3MA3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Re: Official Review of Submission8415 by Reviewer mmrP"
                    },
                    "comment": {
                        "value": "Thank you for your time. Below we provide you with a brief overview of our response. Then, you can find our main rebuttal.\n\n__Overview.__ Thank you for the comments. We improved our paper based on your feedback. We added a related work section to Appendix C. We know that reviewing papers is time consuming, so we appreciate your time. However, we found several controversial comments in your review. In the review it is said that unsupervised models have no domain shift problem, and it is also said that BM25 is not reported as a baseline in the paper. We respectfully refute these comments, because we have already reported BM25 in Table 4. Besides, as you can see there is a large performance gap between BM25 and the recent domain adaptation methods that use unlabeled target data. Our findings, as well as, the experiments reported by existing literature (Izacard et al., 2022; Wang et al., 2022; Dai et al., 2023) show that BM25 and other unsupervised models indeed suffer from domain shift. Because these models are unable to use unlabeled target data. Moreover, in the review it is said that it is not clear why we have not compared our method to the GPT models. We would like to clarify that we already answered this question in the introduction section. We reported an experiment and showed that these models are unreliable for everyday fact checking tasks and lack the needed knowledge to perform the task, because their corpus is not updated on a daily basis. In addition to this, below we report evidence that these models were trained on the ground-truth of the fact checking datasets, which makes the comparison unfair. Additionally, in the review it is suggested that we discuss the adversarial method proposed by Xin et al. (2021), and also that we add more citations to fact checking studies. We agree, however, we would like to clarify that: 1) The adversarial method proposed by Xin et al. (2021) is used as a baseline in the GPL paper (Wang et al., 2022), and is outperformed by GPL. Please note that we have used GPL as a baseline in our paper, which is a more recent method. 2) Due to space constraints, in the previous revision of our paper we discussed only the fact checking studies that explored the domain shift problem. To take into account your suggestion, we revised our paper and added a separate related work section to discuss general fact checking studies as well, please see (Appendix C). Due to the lack of space, we added this section in the appendix."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633814131,
                "cdate": 1700633814131,
                "tmdate": 1700633814131,
                "mdate": 1700633814131,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rVNzNLRviQ",
                "forum": "T629ezwzxI",
                "replyto": "uvHVCU3MA3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "__Main rebuttal.__\n\nPlease find below a list of your comments. Each item is followed by a short discussion.\n\n> 1) Reviewer Comment: \u201cSome unsupervised methods are not compared, such as BM25. These methods have no domain shift problem.\u201d\n\nDiscussion: We respectfully refute these comments. First, we would like to clarify that we reported the BM25 method in the previous revision of our paper. Please see the first row in Table 4. Second, as you can see there is a huge gap between this model and GPL and Pomptagator (second and third rows). This is due to the inability of models such as BM25 to exploit unlabeled target data. Contriever uses general datasets, which is why it performs relatively poorly. We would like to clarify that our finding regarding BM25 is consistent with the reports published by existing studies (Izacard et al., 2022; Wang et al., 2022; Dai et al., 2023).\nFor further information about the baselines and experimental setup, and also how we constructed the fact checking pipelines please see (Appendix B, either revision of our paper).\n\n> 2) Reviewer Comment: \u201cThe introduction describes the generalization ability of GPT3/4 on the fact verification task, but no experiments of GPT-3/4 are conducted.\u201d\n\nDiscussion: We agree that large language models have a high generalization capability, however, in the introduction section we did not argue for their ability to perform fact checking. In (Line 6, Page 1, previous revision), we empirically showed that these models are not suitable for fact checking because their pretraining corpus is not regularly updated. Therefore, their parametric knowledge of the world is incomplete and cannot be used for everyday fact checking. Please see Figure 1, in the paper.\n\nFurthermore, it is unfair to compare a large language model, such as GPT-3, to our model. A model with the scale of GPT-3 needs about four A100 80G GPUs only for inference. Regularly finetuning this model to keep it up-to-date requires a lot more resources\u2014not to mention what the requirements of GPT-4 are. On the other hand, our entire model can be stored in a V100 GPU with 16G of memory, and needs only a few hours for finetuning.\n\nMoreover, during our experiments we found evidence that fact checking websites are used as pretraining data for the GPT models. As the datasets used in our experiments (Hanselowski et al., 2019; Augenstein et al., 2019) are collected from fact checking websites, this makes the comparison further unfair. Because this means that these models have access to the ground-truth labels in their parametric knowledge. The evidence that we found is that, for some of the claims that we tried to make predictions for using GPT-3, the model returned pieces of texts that are specific to fact checking websites. For example, when we submitted the claim \u201cAn NPR study determined that 25 million fraudulent votes had been cast for Hillary Clinton in the 2016 presidential election.\u201d The model returned \u201cPants on Fire\u201d, which is a label used on fact checking websites to specify that a claim is completely false. This was despite the fact that we prompted the model with six in-context examples to respond with only False, True, or Neutral.\n\nHaving said all these, if out of curiosity, you would still like to see the comparison, below we report the results. However, this experiment should not be taken into consideration, as the models are not comparable in terms of hardware usage, the GPT model is not suitable for everyday fact checking tasks, and there is evidence that the GPT model is trained on the ground-truth labels.\n\n![Image](https://anonymous.4open.science/r/ICLR-2024-2C75/Screenshot%20gpt.png)\n\nIf the image does not load, please go to: https://anonymous.4open.science/r/ICLR-2024-2C75/Screenshot%20gpt.png\n\nThe GPT model was prompted with four in-context examples in the MultiFC dataset (for False or True), and six in-context examples in the Snopes dataset (for False, True, or Neutral). If the model output was not automatically interpretable (did not contain \u201ctrue\u201d or \u201cneutral\u201d), we labeled the claim as False."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633889785,
                "cdate": 1700633889785,
                "tmdate": 1700634690973,
                "mdate": 1700634690973,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rfdO2ufxf1",
                "forum": "T629ezwzxI",
                "replyto": "uvHVCU3MA3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 3) Reviewer Comment: \u201cThe adversarial learning has been explored in previous work[4]. This work should discuss it. ... Lots of related work[....] on fact verification is not discussed.\u201d\n\nDiscussion: We added a separate new section to discuss general fact checking studies\u2014please see (Appendix C). In the previous revision of our paper we only discussed the fact checking papers that involve domain adaptation, please see (Line 1-9, Page 2, previous revision). In the newly added section, we cover a broader range of studies, including the studies that you asked us to cite. Due to the lack of space, we added this section in the appendix.\n\nRegarding the paper by Xin et al. (2021): their model was used as a baseline by Wang et al. (2022), and was shown to be weaker. We have used the most recent studies as baselines, including that of Wang et al. (2022). However, we added a discussion about the paper in the newly added related work section, please see (Appendix C). Xin et al. (2021) rely on a model called domain classifier to push the representations of source and target data points close to each other. However, because the transformation is happening concurrently to the training of the retrieval encoders, it causes instability in the training. Therefore, they cache the representations of the vectors in the previous steps, and include them in their loss function. As the results by Wang et al. (2022) show, this strategy has not been able to outperform a model like GPL. We are taking a different strategy, by initially training the encoders and then freezing them. Then training two separate encoders using two discriminators.\n\n> 1) Reviewer Question: \u201cWhy do you not directly evaluate the fact verification performance of GPT-3/4?\u201d\n\nDiscussion: We believe that we answered this question. Please see our discussion for (Comment 2).\n\n__References__\n\n[1] Izacard, Gautier, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. \"Unsupervised Dense Information Retrieval with Contrastive Learning.\" Transactions on Machine Learning Research (2022).\n\n[2] Wang, Kexin, Nandan Thakur, Nils Reimers, and Iryna Gurevych. \"GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval.\" In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2345-2360. 2022.\n\n[3] Dai, Zhuyun, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall, and Ming-Wei Chang. \"Promptagator: Few-shot Dense Retrieval From 8 Examples.\" In The Eleventh International Conference on Learning Representations. 2023.\n\n[4] Xin, Ji, Chenyan Xiong, Ashwin Srinivasan, Ankita Sharma, Damien Jose, and Paul N. Bennett. \"Zero-shot dense retrieval with momentum adversarial domain invariant representations.\" arXiv preprint arXiv:2110.07581 (2021)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633949597,
                "cdate": 1700633949597,
                "tmdate": 1700633949597,
                "mdate": 1700633949597,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yInBA2pb1U",
                "forum": "T629ezwzxI",
                "replyto": "JndhJLJj2a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8415/Reviewer_mmrP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8415/Reviewer_mmrP"
                ],
                "content": {
                    "comment": {
                        "value": "Indeed the authors have compared with BM25 methods. I do not think it suffers from the out-of-domain problem. It mainly due to the vocabulary mismatch problem. Thus, I can not agree with your claim."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635940343,
                "cdate": 1700635940343,
                "tmdate": 1700635940343,
                "mdate": 1700635940343,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DUNnLJWZbj",
                "forum": "T629ezwzxI",
                "replyto": "yInBA2pb1U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8415/Reviewer_mmrP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8415/Reviewer_mmrP"
                ],
                "content": {
                    "comment": {
                        "value": "You said you do not compare with GPT3/4 due to the efficiency problem. But the introduction show the out-of-domain problem of LLMs. The claims are also conflict."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636076093,
                "cdate": 1700636076093,
                "tmdate": 1700636076093,
                "mdate": 1700636076093,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ned9D0xsOo",
                "forum": "T629ezwzxI",
                "replyto": "rfdO2ufxf1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8415/Reviewer_mmrP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8415/Reviewer_mmrP"
                ],
                "content": {
                    "comment": {
                        "value": "Additional fact verification work and tasks are also not compared, such as SciFact and CLIMATE-FEVER, which also belong to specific domains. These datasets are more widely used."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636306985,
                "cdate": 1700636306985,
                "tmdate": 1700636306985,
                "mdate": 1700636306985,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qqNwLuW0PM",
                "forum": "T629ezwzxI",
                "replyto": "eNzAiAPzg2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8415/Reviewer_mmrP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8415/Reviewer_mmrP"
                ],
                "content": {
                    "comment": {
                        "value": "I do not care the reason that you do and you do not do. The core is that you overclaim your contribution. You say LLMs face the out-of-domain problem but do not solve it."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643638489,
                "cdate": 1700643638489,
                "tmdate": 1700643638489,
                "mdate": 1700643638489,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iCmb32ymrx",
            "forum": "T629ezwzxI",
            "replyto": "T629ezwzxI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8415/Reviewer_FTvD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8415/Reviewer_FTvD"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses the challenges and potential solutions related to out-of-domain fact-checking, which involves verifying facts that are not covered by traditional domain-specific fact-checking systems. The study presents a detailed case study to demonstrate the challenges, followed by proposed methods such as adversarial training for evidence retriever and representation alignemtn for reader. The effectiveness of these methods is evaluated through comprehensive experiments and ablation study."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This study creatively incorporates various out-of-domain algorithms to enhance the fact-checking process.\n2. The experimental setup is commendable, with a thorough ablation study that effectively showcases the efficacy of the introduced methods.\n3. The paper is eloquently written, ensuring clarity and ease of comprehension for readers."
                },
                "weaknesses": {
                    "value": "1. It would be beneficial to include baselines evaluating prior fact-checking algorithms (SOTA or other general ones) within the experimental setting outlined in this paper (specifically, fine-tuning on source domain and testing on target domain). This would more effectively highlight the necessity and advantages of the methods proposed for out-of-domain fact-checking.\n2. Although the enhancements brought about by the proposed methods are evident in Tables 3 and 4, the cumulative improvements showcased in Table 2 appear modest.\n3. Drawing a direct comparison between the proposed fact-checking system and models like ChatGPT or GPT-4 might not be entirely justifiable. Notably, the proposed system has access to evidence sources, whereas the ChatGPT or GPT-4 versions evaluated do not. It would be intriguing to see a baseline involving a GPT model (without fine-tuning) assessed on your target domain test set."
                },
                "questions": {
                    "value": "1. Were experiments conducted on prior fact-checking algorithms (SOTA or other general ones), evaluated on MultiFC and Snopes using your experimental setups?\n2. In the concluding lines of the 2nd paragraph in section 3.1, you state, \"a model trained only on the out-of-domain data typically does not perform as competitively as a model trained on in-domain data.\" Could you provide clarity on how you differentiate between out-of-domain and in-domain data? Is it implied that \"general\" or \"Misc\" categories are considered out-of-domain because they amalgamate data from diverse topics? Or are the terms \"out-of-domain\" and \"in-domain\" used interchangeably throughout the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8415/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8415/Reviewer_FTvD",
                        "ICLR.cc/2024/Conference/Submission8415/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8415/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698726676281,
            "cdate": 1698726676281,
            "tmdate": 1699785658627,
            "mdate": 1699785658627,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PFI1P0cXJS",
                "forum": "T629ezwzxI",
                "replyto": "iCmb32ymrx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Re: Official Review of Submission8415 by Reviewer FTvD"
                    },
                    "comment": {
                        "value": "We appreciate your review. Please find below our response. We begin by providing you with an overview of the rebuttal.\n\n__Overview.__ Thank you for the comments. In the review it is suggested that we include prior SOTA models, including the model that is finetuned on the source and tested on the target. This must be a miscommunication. We would like to clarify that all of our baselines are state-of-the-art models, as presented in Izacard et al. (2022), Wang et al. (2022), and Dai et al. (2023). These models inherently include the fine-tuning step on the source data. To demonstrate how strong these baselines are, in the next section we report the results of the model that you suggested. We show that it is significantly outperformed by our baselines. Additionally, in the review it is suggested that we experiment with GPT models. While we agree that the comparison can be interesting, we would like to report that we have found evidence that the GPT models were trained on the ground-truth labels of the fact checking datasets\u2014which makes the comparison unfair. Below we show evidence for our claim."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633001996,
                "cdate": 1700633001996,
                "tmdate": 1700633001996,
                "mdate": 1700633001996,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "54V8Kgj2fL",
                "forum": "T629ezwzxI",
                "replyto": "iCmb32ymrx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "__Main rebuttal.__ \n\nPlease find below a list of your comments, followed by a short discussion for each item.\n\n> 1) Reviewer Comment: \u201cIt would be beneficial to include baselines evaluating prior fact-checking algorithms (SOTA or other general ones) within the experimental setting outlined in this paper (specifically, fine-tuning on source domain and testing on target domain).\u201d\n\nDiscussion: below we report the performance of the model that you suggested that we include as a baseline method. This model has a bi-necoder and a reader\u2014similar to other baseline models. The model consists of only finetuning on the labeled source data, and then testing on the target data. For brevity, we omit the full identification of the baseline names, the order of the rows matches that of Table 2 in the paper. We see that in the majority of the cases it is outperformed by all the baselines\u2014the exception cases are M\u2192A and P\u2192A.\n\n![Image](https://anonymous.4open.science/r/ICLR-2024-2C75/Screenshot%20results.png)\n\nIf the image does not load, please go to: https://anonymous.4open.science/r/ICLR-2024-2C75/Screenshot%20results.png\n\nDespite the importance of the domain shift problem in fact checking, there are only three studies that are relevant to this problem, they are the studies by Augenstein et al. (2019), Wadden et al. (2020), and Gupta & Srikumar (2021)---all three were mentioned in the previous revision of our paper, please see (Line 1, Page 2, previous revision). All three studies use the standard retriever-reader pipeline. In order to evaluate the transferability of their pipelines, they propose pretraining the pipeline on the source, and then testing it on the target data\u2014they do not propose any domain adaptation solution. However, as you can see in the table above, this approach is not as effective as the baselines that we used in the experiments.\n\nInstead of relying only on the pretraining approach to evaluate our model, we selected the state-of-the-art domain adaptation methods or the methods that are common for using unlabeled target data for each component of the pipeline. Using these methods, we constructed a set of strong fact checking pipelines that are used as baseline models, reported in Table 2 of the paper. \n\nWhile your doubts are understandable, which might have caused you to reduce your review scores after all the reviews were released, we justifiably claim that our study is by far the most significant work on this topic to date. We kindly request you to skim over our response to other reviewers, and see if we have resolved your doubts. Nonetheless, we are available to respond to reviewers\u2019 questions and defend our research study. We appreciate your fair review.\n\n> 2) Reviewer Comment: \u201cDrawing a direct comparison between the proposed fact-checking system and models like ChatGPT or GPT-4 might not be entirely justifiable. Notably, the proposed system has access to evidence sources, whereas the ChatGPT or GPT-4 versions evaluated do not. It would be intriguing to see a baseline involving a GPT model (without fine-tuning) ...\u201d\n\nDiscussion: Indeed, the opposite is true. The datasets used in our experiments (Hanselowski et al., 2019; Augenstein et al., 2019) are collected from fact checking websites. During our experiments, we found evidence that fact checking websites are used as pretraining data for the GPT models. Thus, this makes the comparison unfair, because this means that these models have access to the ground-truth labels in their parametric knowledge. The evidence that we found is that, for some of the claims that we tried to make predictions for using GPT-3, the model returned pieces of texts that are specific to fact checking websites. For example, when we submitted the claim \u201cAn NPR study determined that 25 million fraudulent votes had been cast for Hillary Clinton in the 2016 presidential election.\u201d The model returned \u201cPants on Fire\u201d, which is a label used on fact checking websites (the PolitiFact website) to specify that a claim is completely false. This was despite the fact that we prompted the model with six in-context examples to respond with only False, True, or Neutral.\n\nAdditionally, and in defense of our study, in the introduction section (Figure 1, previous and current revisions), we reported an experiment showing that large language models are not suitable for everyday fact checking, because their corpus is not regularly updated. Therefore, the model is unaware of most recent events.\n\nOne may argue that why not maintain a copy of a model such as GPT-3 and regularly update it. We would like to clarify that a model with the scale of GPT-3 needs about four A100 80G GPUs only for inference. Regularly finetuning this model to keep it up to date requires a lot more resources\u2014not to mention what the requirements of GPT-4 are. On the other hand, our entire model can be stored in a V100 GPU with 16G of memory, and needs only a few hours for finetuning. Thus, the comparison is not fair."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633080178,
                "cdate": 1700633080178,
                "tmdate": 1700634655613,
                "mdate": 1700634655613,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eBaj6cv2oG",
                "forum": "T629ezwzxI",
                "replyto": "iCmb32ymrx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 1) Reviewer Question: \u201cWere experiments conducted on prior fact-checking algorithms (SOTA or other general ones), evaluated on MultiFC and Snopes using your experimental setups?\u201d\n\nDiscussion: We believe that we answered this question, please see our discussion for (Comment 1).\n\n> 2) Reviewer Question: \u201cthe concluding lines of the 2nd paragraph in section 3.1, you state, \"a model trained only on the out-of-domain data typically does not perform as competitively as a model trained on in-domain data.\" Could you provide clarity on how you differentiate between out-of-domain and in-domain data? Is it implied that \"general\" or \"Misc\" categories are considered out-of-domain because they amalgamate data from diverse topics? Or are the terms \"out-of-domain\" and \"in-domain\" used interchangeably throughout the paper?\u201d\n\nDiscussion: Thank you for the question. We have not used the terms out-of-domain and in-domain interchangeably. We use these terms to characterize the relationship between a set of documents and a domain. If the distribution of the documents is the same as the distribution of the data in the domain, then the documents are called in-domain, otherwise they are called out-of-domain. The data from the two domains of \u201cgeneral\u201d and \u201cMisc\u201d are considered out-of-domain with respect to the target domain, because this data follows a distribution different from that of the target domains. As we mentioned in the paper, please see (Footnote 2, in the paper), the data from two domains (or distributions) follows two different genres of texts.\n\n__References__\n\n[1] Izacard, Gautier, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. \"Unsupervised Dense Information Retrieval with Contrastive Learning.\" Transactions on Machine Learning Research (2022).\n\n[2] Wang, Kexin, Nandan Thakur, Nils Reimers, and Iryna Gurevych. \"GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval.\" In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2345-2360. 2022.\n\n[3] Dai, Zhuyun, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall, and Ming-Wei Chang. \"Promptagator: Few-shot Dense Retrieval From 8 Examples.\" In The Eleventh International Conference on Learning Representations. 2023."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633152249,
                "cdate": 1700633152249,
                "tmdate": 1700633152249,
                "mdate": 1700633152249,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vHyZeNfHxk",
                "forum": "T629ezwzxI",
                "replyto": "iCmb32ymrx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8415/Reviewer_FTvD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8415/Reviewer_FTvD"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the authors' response.\n\nRegarding the discussion about baseline:\n- Thank you for providing additional experiment results of \"finetuning on the labeled source data, and then testing on the target data\", it would be great if the authors could add these numbers to your camera-ready version, which will help future readers of your paper to better understand the advantages of your methods and therefore strengthen your work.\n- Unfortunately, I cannot access the previous version, it says \"No revisions to display.\" when I click the \"Revision\" button.\n- Although the author justifies why their paper didn't include the comparison numbers of prior arts (including Augenstein et al. (2019), Wadden et al. (2020), and Gupta & Srikumar (2021)) by stressing these methods don't propose any domain adoption technique, it would be more interesting to see if newly proposed methods outperformed prior arts on these fact-checking tasks (in same experiments setting), no matter if they proposed specific technique.\n\nRegarding the discussion about the comparison between GPT4 and the proposed method:\n- Thanks for the author's detailed explanation. I agree with the author's concern given the example. However, as the author mentions GPT models are not regularly updated, which makes it more interesting to see how they perform on unseen data. As this may involve creating a new dataset, I wouldn't expect the author to include it in this paper.\n- People are genuinely interested in how GPT models perform on these datasets, which does not necessarily reduce the contribution of this work considering the newly proposed method and GPT models were created at different levels of resources.\n\n\nThank you for answering my questions."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709355635,
                "cdate": 1700709355635,
                "tmdate": 1700709500441,
                "mdate": 1700709500441,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ruIjVxogfL",
            "forum": "T629ezwzxI",
            "replyto": "T629ezwzxI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8415/Reviewer_5Ldi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8415/Reviewer_5Ldi"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on domain adaptation for fact-checking, employing a typical unsupervised domain adaptation setting where two domains are provided: a labeled source domain and an unlabeled target domain. \nThe objective is to improve the performance of a source-domain trained model on the unlabeled target domain. \nFor the fact-checking task, the paper uses a standard retrieve-and-read pipeline comprising a bi-encoder retriever and a reader. \nTo adapt the bi-encoder retriever to the target domain, the paper applies adversarial training to train the bi-encoder on the unlabeled target domain, enabling it to mimic the source-domain encoder. \nFor adapting the reader model, the paper incorporates alignment loss in addition to cross-entropy loss during the training of the source-domain reader, integrating data from the target domain into the training process. \nMoreover, the paper also proposes a  data augmentation method by switching the order of the claim and evidence document in the reader's input. \nThe authors also automate the conversion of two non-domain-adapted fact-checking datasets into datasets with domain labels using a fine-tuned classifier and test the proposed method on these datasets. \nThe results demonstrate that the proposed method outperforms the non-adapted baselines on the target domains."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. Overall, the paper provides a clear and comprehensible description of the proposed method, particularly the setup of the entire work and the articulation of the research problem.\n2. The domain adaptation method proposed in the paper is straightforward and has demonstrated better performance on the tested datasets compared to non-adapted baselines."
                },
                "weaknesses": {
                    "value": "1. I believe that the innovativeness of the domain adaptation method proposed in this article is quite limited. The paper mentions that previous work on fact-checking domain adaptation tasks is scarce, but it fails to elaborate on specific limitations and lacks a detailed comparison and analysis with any domain adaptation methods. From the description in the paper, it seems that the authors have only applied standard domain adaptation techniques, such as adversarial training and alignment loss, to the common retrieve-and-read framework. Consequently, I do not see much innovation in the method, and I find the discussion of related works insufficient. The paper should at least discuss the related domain adaption works.\n2. The paper\u2019s description of data augmentation for training the reader mentions that it addresses the issues of noise due to the lack of gold documents in the target domain and provides more cues to the reader. However, I believe the data augmentation method proposed does not effectively tackle these issues. Simply swapping the order of the claim and evidence document in the input sequence does not reduce noise and might even increase it. As for providing more cues, I do not think this is achieved by merely changing the order of content in the input sequence since it does not add any additional information. The paper also lacks any empirical analysis of this specific data augmentation.\n3. The quality of the created datasets is not rigorously validated. The domain labels in the dataset are labeled based on an external classifier, and the validation of the constructed domain adaptation through a simple 2D projection seems insufficient. There is even no description of how the projection is conducted in the paper.\n4. The paper does not provide adequate explanations for the baselines used, such as why they are reasonable baselines. Only listing the names of the baselines is not enough. Moreover, from the description, it appears that all the baselines used are simple retrieve-and-read methods, without domain adaptation methods, making the comparison with the adapted method in the paper unreasonable. The authors should consider comparing their method with other strong adaptation methods.\n5. The paper lacks an ablation analysis of the components of the proposed method. For instance, the authors do not conduct ablation studies on adversarial training or alignment loss to prove their effectiveness in domain adaptation."
                },
                "questions": {
                    "value": "1. Why existing methods can only solve the fact-checking domain adaptation problem in a very limited way? What are the specific limitations?\n2. Why is it believed that data augmentation can alleviate the noise issue? Why do we need to handle the order issue? Isn\u2019t everything tested in a certain order (claim + evidence) anyway?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8415/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8415/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8415/Reviewer_5Ldi"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8415/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789849548,
            "cdate": 1698789849548,
            "tmdate": 1699637047930,
            "mdate": 1699637047930,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KkZIS3WzJh",
                "forum": "T629ezwzxI",
                "replyto": "ruIjVxogfL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Re: Official Review of Submission8415 by Reviewer 5Ldi"
                    },
                    "comment": {
                        "value": "Thank you for your review. Please see below our response. We begin by providing an overview of our rebuttal, and then, followed by the main response letter.\n\n__Overview.__ We took your comments into consideration and improved our paper. We added clarification about Figure 5 to the second paragraph of Page 8, added explanations about the baselines to Appendix B, and added a new related work section to Appendix C. We are aware that reviewing papers is time consuming, and can be challenging, so we appreciate it. However, we found several controversial comments in your review. \n- In the review it is said that we have only used non-adapted baselines. This must be a miscommunication, because all of our baselines are state-of-the-art domain adaptation methods, as presented in Izacard et al. (2022), Wang et al. (2022), and Dai et al. (2023). In the main rebuttal we will report a non-adapted fact checking model and show that in most cases, it is outperformed by all of our baselines. We agree with you, it is not reasonable to not compare with adapted baselines. \n- In the review it is said that the data augmentation does not add any new information to the data. We respectfully refute this argument. In the main rebuttal (below) we will explain a simple experiment that consists of only one data point for training, and show that our augmentation can improve the accuracy by giving additional information to the model.\n- In the review it is said that the datasets are not validated. We respectfully refute this argument, because we have adopted the protocol used in the seminal work of Hinton & Salakhutdinov (2006) for reporting data separability. We would like to clarify that we didn\u2019t claim that we created new datasets, we have stated this in the previous revision of our paper (please Section 4, second paragraph). Instead, we have proposed a method to construct categories in already existing datasets. There are well-known methods to validate categorization and data separability, which we have adopted in the previous revision of our paper. One of these methods is data visualization, as employed by Hinton & Salakhutdinov (2006), which we used. Another method is reporting quantitative measures (Guo et al., 2020). We reported a measure called A-distance metric which has theoretical justifications (Ben-David et al., 2010) and is specifically proposed for predictions under domain shift. These were in addition to reporting a sample of claims in each domain and reporting the mappings that we used for projecting the labels. In the newer revision of our paper, we are also adding the top LDA topics for each domain as well. Please see the main rebuttal for the exact location of these items in the paper.\n- In the review it is said that the empirical analysis of the augmentation is missing. We respectfully refute this comment, please see Table 5(b), this experiment was already reported. \n- In the review it is said that ablation studies are missing, and said that no experiments on adversarial training and alignment loss are reported. We respectfully refute all these comments. Please see Tables 5(a-c), a set of ablation studies are reported, including the two experiments you mentioned."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631182798,
                "cdate": 1700631182798,
                "tmdate": 1700631182798,
                "mdate": 1700631182798,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "plCWv69qGA",
                "forum": "T629ezwzxI",
                "replyto": "ruIjVxogfL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "__Main rebuttal.__ \\\nBelow we provide a list of your critiques. Each item is followed by a short discussion.\n\n> 1) Reviewer Comment: \u201cThe results demonstrate that the proposed method outperforms the non-adapted baselines on the target domains. \u2026 it fails to elaborate on specific limitations and lacks a detailed comparison and analysis with any domain adaptation methods. \u2026. the authors have only applied standard domain adaptation techniques, such as adversarial training and alignment loss, to the common retrieve-and-read framework. \u2026. The paper should at least discuss the related domain adaptation works. \u2026 it appears that all the baselines used are simple retrieve-and-read methods, without domain adaptation methods, making the comparison with the adapted method in the paper unreasonable. The authors should consider comparing their method with other strong adaptation methods.\u201d\n\nDiscussion: We would like to clarify that all of our baseline models are state-of-the-art domain adaptation models, or they are the models that are commonly used to exploit unlabeled target data when the labeled training data is unavailable. We definitely agree with you that it is unreasonable to propose a domain adaptation model and not to compare it with other domain adaptation methods!\n\nTo demonstrate the strength of our baselines, below we report a non-adapted model. Similar to other models, this model has a bi-necoder and a reader. The model consists of only finetuning on the labeled source data, and then testing on the target data\u2014it does not use any domain adaptation technique. For brevity, we omit the full identification of the baseline names, the order of the rows matches that of Table 2 in the paper. We see that in six out of eight cases all the baselines outperform the non-adapted model\u2014the exceptions are M\u2192A and P\u2192A.\n\n![Image](https://anonymous.4open.science/r/ICLR-2024-2C75/Screenshot%20results.png)\n\nIf the image does not load, please go to: https://anonymous.4open.science/r/ICLR-2024-2C75/Screenshot%20results.png\n\n- In the review it is said that it is not clear what existing domain adaptation methods are and what their limitation is: \\\nDespite the importance of the domain shift problem in fact checking, there are only three studies that are relevant to this problem, they are the studies by Augenstein et al. (2019), Wadden et al. (2020), and Gupta & Srikumar (2021)---all three were mentioned in the previous revision of our paper, please see (Line 1, Page 2, previous revision). The first study (Augenstein et al., 2019) composes a data set called MultiFC. This dataset was collected across multiple fact checking websites, which the authors call them \u201csources/domains\u201d. Their model is the standard retriever-reader pipeline, and their experiments are carried out within each website individually. Their model relies on meta-data collected from webpages. They propose no algorithm for training a model on one domain and testing on another domain. The second study (Wadden et al., 2020) composes a dataset called SciFact, collected from scientific repositories. Their model is the standard retriever-reader. In the experiments section of their study, they report an experiment on domain adaptation: They pretrain their pipeline on the claims extracted from wikipedia and then test it on their dataset. Thus, their solution for domain adaptation is to pretrain the pipeline on one resource and then test it on another resource; beyond this, they propose no domain adaptation method. Apart from their simple solution, their study also has a weakness: the wikipedia claims that they use to pretrain their pipeline, may share some knowledge with the claims in their dataset. This can potentially distort their conclusions. The third study (Gupta & Srikumar, 2021) composes a multilingual fact checking dataset. This dataset consists of claims, and evidence documents retrieved from Google. They use the standard pipeline, and similar to the second study, they evaluate the transferability of their pipeline by training on the data from one website and testing it on another website. Beyond this, they propose no additional solution for domain adaptation. \\\nThus, as we stated in the paper (Line 1-9, Page 2, previous revision), these studies are \u201climited\u201d, as: 1) They use the claims collected from each fact checking website as the claims belonging to one domain. However, it is important to know that the claims collected from various websites may share knowledge, and do not belong to fully independent domains. 2) To tackle the domain shift problem, they propose to pretrain the pipeline on one domain and test on another domain. However, as we see in the table above, the pretraining method is not strong enough, and in most cases is outperformed by our baselines. \\\nWe would like to clarify that these topics were briefly discussed in the previous revision of the paper, please see (Line 1-9, Page 2, previous revision)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631506947,
                "cdate": 1700631506947,
                "tmdate": 1700634613345,
                "mdate": 1700634613345,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pehYMAA72b",
                "forum": "T629ezwzxI",
                "replyto": "ruIjVxogfL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "- In the review it is said that related work is not discussed and said that novelty of our study is limited: \\\nWe added a new related work section, please see (Appendix C), and discussed a set of broad fact checking studies. Due to the lack of space, we added this section in the appendix. Based on the clarifications above, we claim that our study is by far the most innovative and comprehensive study published on this topic to date. Our contributions are: 1) We argued that evaluating the transferability of the fact checking pipeline across fact checking websites is not sufficient (see Section 1, previous revision). 2) We empirically showed that the fact checking pipeline suffers from distribution shift across domains (see Section 2, previous revision). 3) We proposed a novel adversarial domain adaptation model for the retriever component (see Section 3.1, previous revision). 4) We revealed that language models are unable to generalize from the hypothesis to the premise, if they are trained on the premise to predict the hypothesis. Based on this finding, we proposed an augmentation method to enhance the reader (see Section 3.2, previous revision). 5) Proposed a straightforward method to compose a set of domains from existing fact checking datasets and validated the quality of the extracted domains (see Section 4, previous revision). 6) Compared our model with strong domain adaptation baselines. Because the pretraining method, which is used in the previous studies and as we showed in the table above, is not strong enough, we included existing state-of-the-art domain adaptation methods for each individual component to create the baselines. This resulted in a set of fact checking pipelines, with individual components that use state-of-the-art domain adaptation techniques as the baseline models. The results show that our method outperforms these baselines in most cases (see Section 5, previous revision).\n\n> 2) Reviewer Comment: \u201c... I believe the data augmentation method proposed does not effectively tackle these issues. Simply swapping the order of the claim and evidence document in the input sequence does not reduce noise and might even increase it. As for providing more cues, I do not think this is achieved by merely changing the order of content in the input sequence since it does not add any additional information. The paper also lacks any empirical analysis of this specific data augmentation. \u2026 Why do we need to handle the order issue? Isn\u2019t everything tested in a certain order (claim + evidence) anyway?\u201d\n\nDiscussion: We respectfully refute these comments. First, we would like to clarify that we reported the empirical evidence for the efficacy of our augmentation algorithm in the previous revision of our paper. Please see the third row of Table 5(b). As we see there is a clear performance deterioration when we omit the augmentation from the model. Second, we would like to clarify that, as we discussed in the paper (Section 3.3), during our experiments we observed that a language model that is trained on a set of premises to evaluate their hypotheses was not able to infer the premises if it was given the hypotheses. It logically follows that if the training data is augmented with both directions, then, the model can successfully make the inference\u2014no matter what the order of the data is, during the test phase.To support our argument, below we report an elementary example. You can actually test this example and see the result using a GPT-3 model.\n\nIf the training data (in-context example) consists of the following evidence, claim, and label:\n```\nEvidence: \u201cMIT is the alma mater of GHI.\u201d \nClaim: \u201cGHI studied at MIT.\u201d \nLabel: \u201cIs that true or false? True\u201d\n```\nAnd then the test data consists of the following evidence and claim:\n```\nEvidence: \u201cABC studied at University of Illinois.\u201d\nClaim: \u201cUniversity of Illinois is the alma mater of ABC.\u201d\nLabel: \u201cIs that true or false?\u201d\n```\nThen, the model returns \u201cFalse\u201d, indicating that the model has not learned the relationship between \u201calma mater\u201d and \u201cstudy\u201d in the training data. However, if we augment the initial training data with the following reversed example:\n```\nEvidence: \u201cGHI studied at MIT.\u201d\nClaim: \u201cMIT is the alma mater of GHI.\u201d\nLabel: \u201cIs that true or false? True\u201d\n```\nThen the model correctly returns \u201cTrue\u201d for the test data point. This indicates that the augmentation gives additional cues to the model to extract the relationship between \u201calma mater\u201d and \u201cstudy\u201d, and helps it to dilute the potential noise in the data."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631670759,
                "cdate": 1700631670759,
                "tmdate": 1700631670759,
                "mdate": 1700631670759,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p5LIUhkFry",
                "forum": "T629ezwzxI",
                "replyto": "ruIjVxogfL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8415/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 3) Reviewer Comment: \u201cThe quality of the created datasets is not rigorously validated. .... There is even no description of how the projection is conducted in the paper.\u201d\n\nDiscussion: We would like to clarify that the projection is the output of the BERT encoder classifier, transformed into a 2D space (to visualize the data points) using the t-SNE technique\u2014we added the clarification to (the second paragraph of the section about the datasets, on Page 8, new revision). However, we respectfully refute the comment on the lack of dataset validation.\n\nWe would like to clarify that we are using existing datasets, and we have not composed new datasets. This was stated in the previous revision of our paper, please see (second paragraph of Section 4). Our contribution, in this section of the paper, was to propose a method to extract a set of domains from the datasets. There are already established methods for evaluating the quality of textual domains. One of them is data visualization, which is used by the seminal work of Hinton & Salakhutdinov (2006). In Figure 5(a-b), we have used this method. Another approach is to report quantitative measures, discussed by Guo et al. (2020). In this regard, we have used a measure called A-distance, specifically designed for domain adaptation experiments, proposed by Ben-David et al. (2010)---please see (third paragraph, Page 8, previous revision). In addition to these two reports, in the previous revision of our paper we reported a sample of claims from each domain, please see (Appendix A, Table 6), and the mappings used to transfer the Google labels to domain labels, please see (Appendix A, table 7). In the current revision of our paper, we are also adding the top two LDA topics for each domain, please see (Appendix A, new revision).\n\n> 4) Reviewer Comment: \u201cThe paper does not provide adequate explanations for the baselines used, such as why they are reasonable baselines. Only listing the names of the baselines is not enough. Moreover, from the description, it appears that all the baselines used are simple retrieve-and-read methods, without domain adaptation\u2026\u201d\n\nDiscussion: We appreciate the comment. However, we would like to clarify that, in the previous revision of our paper, we had provided more than the names of the baselines. Please see (Appendix B, previous revision), where we explained why we have selected these baselines, how they relate to our experiments, and also how we constructed the fact checking pipelines. Based on your feedback, we provided further clarifications for each baseline, please see (Appendix B, new revision).\n\n> 5) Reviewer Comment: \u201cThe paper lacks an ablation analysis of the components of the proposed method. For instance, the authors do not conduct ablation studies on adversarial training or alignment loss to prove their effectiveness in domain adaptation.\u201d\n\nDiscussion: We respectfully refute all the comments. We have reported a series of ablation studies, in the previous revision of our paper. Please see Tables 5(a-c). These also include the experiments that in the review are claimed to be missing.\n\nPlease see the second and third rows of (Table 5(a), previous revision). These rows demonstrate the ablation study on the adversarial training of the claim and document encoders individually.\\\nPlease also see the second row of (Table 5(b), previous revision) for the results of an ablation study on the alignment loss.\n\n> 1) Reviewer Question: Two questions were asked in the review, one question about the limitations of existing fact checking methods, and another question about the need for data augmentation.\n\nDiscussion: We believe that we answered both questions in the rebuttal. For the first question please see our discussion for (Comment 1), and for the second question please see our discussion for (Comment 2).\n\n__References__\n\n[1] Izacard, G, et al. \"Unsupervised Dense Information Retrieval with Contrastive Learning.\" Tran. on Machine Learning Research (2022).\n\n[2] Wang, K, et al. \"GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval.\" NAACL 2022.\n\n[3] Dai, Z, et al. \"Promptagator: Few-shot Dense Retrieval From 8 Examples.\" ICLR 2023.\n\n[4] Hinton, G, and R. Salakhutdinov. \"Reducing the dimensionality of data with neural networks.\" science 313, no. 5786 (2006).\n\n[5] Ben-David, S, et al. \"A theory of learning from different domains.\" Machine learning 79 (2010).\n\n[6] Guo, H, et al. \"Multi-source domain adaptation for text classification via distancenet-bandits.\" AAAI 2020.\n\n[7] Augenstein, I, et al. \"MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims.\" EMNLP 2019.\n\n[8] Wadden, D, et al. \"Fact or Fiction: Verifying Scientific Claims.\" EMNLP 2020.\n\n[9] Gupta, A, and V Srikumar. \"X-Fact: A New Benchmark Dataset for Multilingual Fact Checking.\" ACL 2021."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632219028,
                "cdate": 1700632219028,
                "tmdate": 1700632219028,
                "mdate": 1700632219028,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RYkyjTr1TR",
                "forum": "T629ezwzxI",
                "replyto": "p5LIUhkFry",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8415/Reviewer_5Ldi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8415/Reviewer_5Ldi"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response.\n\nSome of my questions, such as those regarding the ablation study, have been addressed. However, I still believe that the overall contribution of this work in the domain of adaptation is quite limited.\n\nRegarding the baseline for dense retrieval, aside from Wang, K, et al. 2022, the other studies cited were not even designed for domain adaptation. Moreover, for the reader baseline, which is essentially a classification task, this paper fails to compare with any domain adaptation methods for classification.\n\nConcerning the dataset, I have yet to see a rigorous validation process, such as involving multiple annotators. Only mentioning an A distance or using visualization does not convince me of its adequacy.\n\nAs for data augmentation, I remain skeptical that the method proposed by the authors addresses the problem they proposed. I am not convinced that simply changing the order of inputs can provide the model with more cues. And I am also worried about the generalization ability of this method."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727396345,
                "cdate": 1700727396345,
                "tmdate": 1700727396345,
                "mdate": 1700727396345,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]