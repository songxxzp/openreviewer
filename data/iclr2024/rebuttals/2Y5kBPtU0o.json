[
    {
        "title": "MEND: Meta Demonstration Distillation for Efficient and Effective In-Context Learning"
    },
    {
        "review": {
            "id": "XkAw6Wffl4",
            "forum": "2Y5kBPtU0o",
            "replyto": "2Y5kBPtU0o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2742/Reviewer_RPCQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2742/Reviewer_RPCQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a demonstration compression framework to save retraining LLMs for unseen tasks under in-context scenarios. To be specific, a two-stage training process including knowledge distillation from pre-trained LLMs and fine-tuning on specific tasks endows the framework with both efficiency and effectiveness. Empirical results of decoder-only and encoder-decoder architectures validate the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors proposed an efficient demonstration distillation for in-context learning. \n- The paper is well-organized and easy to follow."
                },
                "weaknesses": {
                    "value": "- More insightful explanations about why the proposed method does not compromise the in-context learning ability of LLMs could make the paper stronger. \n- self-contained notations may help the readers to understand the results better. \n- Figure 5 can be displayed using more contrasting colors"
                },
                "questions": {
                    "value": "- The distillation loss occurs both in the pretraining and fine-tuning stages where a lambda controller balances the influence of distillation. What\u2019s the influence of the lambda? A clear explanation about connecting the training mechanism(two-stage process) and each loss term to the \u201crequire task-specific retraining or compromise in-context learning\u201d would help the reviewers better understand the advantage of the method.   \n\n- In Figure 3, is the size of the distillation vectors of MEND the same size as PromptTuning? \nWhat\u2019s the model size of MEND and FLOPs it introduced?  \n\n\nSeveral questions for Table 4:\n- In the ablation of pre-training,  even if the row is labeled as \u201cNo pretraining\u201d, the fine-tuning term still contains the distillation loss. So what\u2019s the lamda for this row? \n- What does L_hidn mean in Table 4? Is it L_distill ?\n  If it is L_distll, then the second to the last row is the result of pretraining loss for both stages. The performance looks far from comparable to CLM. Can the authors clearly explain this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2742/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698612196161,
            "cdate": 1698612196161,
            "tmdate": 1699636216829,
            "mdate": 1699636216829,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XKHnwGHpc5",
                "forum": "2Y5kBPtU0o",
                "replyto": "XkAw6Wffl4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2742/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission2742/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2742/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1. More insightful explanations about why the proposed method does not compromise the in-context learning ability of LLMs could make the paper stronger.**\n\n**A1.** Thank you for the opportunity to elaborate on why our proposed method, MEND, does not compromise the in-context learning ability of LLMs. The key to this lies in our novel two-stage training procedure. \n- Firstly, our pretraining stage is designed to align the condensed distillation vectors with the word embeddings of the LLM. As evidenced in Table 4, variations without this specialized pretraining show inferior performance compared to vanilla ICL, highlighting the pretraining's role in ensuring the effectiveness of the distilled vectors.\n- Secondly, the fine-tuning stage leverages objective functions that are crucial for capturing the essence of in-context learning. Removing any of these objectives results in a noticeable performance drop, underscoring their importance. This stage essentially harnesses meta-knowledge from both the data and the teacher model, ensuring that MEND can adeptly adapt to new tasks during testing.\n- Furthermore, our approach is supported by findings in relevant literature [1-3], which demonstrate the effectiveness of similar methodologies in enhancing LLMs' learning capabilities. This dual-stage training process is integral to MEND\u2019s ability to maintain, and often enhance, the in-context learning performance of large language models without significant computational overhead.\"\n\n[1] Min, Sewon, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. \"Metaicl: Learning to learn in context.\" arXiv preprint arXiv:2110.15943 (2021).\n\n[2]Ye, Qinyuan, Iz Beltagy, Matthew E. Peters, Xiang Ren, and Hannaneh Hajishirzi. \"FiD-ICL: A Fusion-in-Decoder Approach for Efficient In-Context Learning.\" In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8158-8185. 2023.\n\n[3] Phang, Jason, Yi Mao, Pengcheng He, and Weizhu Chen. \"Hypertuning: Toward adapting large language models without back-propagation.\" In International Conference on Machine Learning, pp. 27854-27875. PMLR, 2023.\n\n**Q2. More contrast color for Figure 5.**\n**A2.** Thank you for your valuable suggestion regarding the visualization in Figure 5. \nWe originally chose a grayscale color scheme to represent the attention map due to the wide range of values, from $10^{-1} to 10^{-8}$ and the focused distribution of attention weights on specific tokens. This resulted in many tokens having very small attention weights, which are represented in lighter colors close to white. \nTo enhance the readability of the figure without compromising its informational content, we are considering not only the addition of x-ticks to better differentiate between tokens from demonstrations. This adjustment will make it easier to discern the differences in attention weights across tokens.\nFurthermore, it is crucial to note the significance of Figure 5 in our paper. \nIt illustrates that even though the demonstrations are condensed, our model, MEND, can still effectively extract and utilize the necessary information from the distilled vectors for accurate label prediction. This highlights the efficacy of MEND in maintaining key information despite significant data compression, a central aspect of our work.\n\n\n\n**Q3.1. Influence of lambda in controlling the importance of distillation loss in meta-training?**\n\n**A3.1.** hank you for the question about the lambda parameter and our training mechanism. The lambda parameter, crucial for balancing distillation loss, is analyzed in Figure 7 of Appendix B. We found that MEND performs optimally when lambda is equal to or greater than 1. This balance is key to our finetuning process.\n\n\n**Q3.2. The connection between our two-stage training mechanism and each loss term and \u201ctask-specific retraining or compromise in-context learning.**\n\n**A3.2.** Thank you for your query. MEND's two-stage training process is designed to learn a meta-knowledge for \ndemonstration distillation that's effective across various tasks without needing retraining for each new task, unlike prompt tuning methods. \nThis process helps maintain performance on par with vanilla ICL. \nThe first stage aligns the distilled vectors with the LLM's word embeddings, while the second stage fine-tunes this alignment, ensuring adaptability to new tasks. \nThis approach differs significantly from methods like HyperTuning, which lack this alignment, resulting in compromised in-context learning performance. \nOur ablation study in Section 5.4 highlights these distinctions and demonstrates MEND's efficacy in preserving in-context learning ability without task-specific retraining.\n\n**(Unfinished)**"
                    },
                    "title": {
                        "value": "Response to Reviewer RPCQ (Part 1)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739333389,
                "cdate": 1700739333389,
                "tmdate": 1700739434831,
                "mdate": 1700739434831,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VL3ShdCCi2",
                "forum": "2Y5kBPtU0o",
                "replyto": "XkAw6Wffl4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2742/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission2742/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2742/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q4.1. In Figure 3, is the size of the distillation vectors of MEND the same size as PromptTuning?** \n \n**A4.1. Yes, in Figure 3, the size of the distillation vectors for MEND is indeed set to 100, which is consistent with the size used in PromptTuning.** \n\n**Q4.2.  What\u2019s the model size of MEND and FLOPs it introduced (Total parameters and FLOPs)?**\n\n**A4.2.** You can refer to the following table for the size of MEND and its relevant FLOPs for demonstration distillation when compared with PromptTuning. \nIt should be noted that the FLOPs include the number of distillations and large language model inference.\n\n| Downstream LLM | Distillation Model | Additional Parameters | Additional TFLOPs |\n|----------------|--------------------|-----------------------|-------------------|\n| gpt2-large     | gpt2               | 137M                  | 0.1               |\n| gpt2-xl        | gpt2               | 137M                  | 0.11              |\n| opt-6.7b       | opt-125m           | 125M                  | 0.11              |\n\n**Q5. In the ablation of pre-training, even if the row is labeled as \u201cNo pretraining\u201d, the fine-tuning term still contains the distillation loss. So, what\u2019s the lambda for this row?**\n\n**A5.** We choose the lambda based on the validation performance. The selection of $\\lambda$ is shown in the following table:\n\n| Method         | Class->Class | non-nli->nli | non-qa->qa | qa->qa |\n|----------------|--------------|--------------|------------|--------|\n| MEND           | 10           | 1            | 1          | 0.1    |\n| No-Pretraining | 10           | 1            | 10         | 1      |\n\nDistillation loss ($L_{diss}$) also positively contributes to the model\u2019s learning in a no-pretraining setting. This also indicates the importance of imitating the teacher model."
                    },
                    "title": {
                        "value": "Response to Reviewer RPCQ (Part 2)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739341997,
                "cdate": 1700739341997,
                "tmdate": 1700739464948,
                "mdate": 1700739464948,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "psmuEmugxj",
            "forum": "2Y5kBPtU0o",
            "replyto": "2Y5kBPtU0o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2742/Reviewer_Uu3b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2742/Reviewer_Uu3b"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on making in-context learning (ICL) with LLMs more efficient and effective. Vanilla ICL requires one to provide a collection of demonstrations, i.e., task-specific example-label pairs, as context to the LLM while running an inference for a test example. However, this leads to long input sequences, increasing the cost of inference with LLMs where the self-attention cost scales quadratically with input sequence length. This paper proposes to utilize a **demonstration distillation model** to compress the long demonstration sequence into a small number of vectors which can be fed into the LLM as a prompt vector (akin to prompt tuning) during inference. There are existing approaches that adopt such an approach to make ICL efficient. However, those approaches often result in performance degradation. This paper proposes a knowledge distillation-based approach to train the demonstration distillation model. Through extensive empirical evaluations, the paper shows that the resulting demonstration distillation model not only realizes efficient inference by reducing the context length for LLM but often also improves the performance compared to vanilla ICL."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The paper proposes a novel solution to improve the demonstration distillation approach to make ICL efficient. The solution utilizes knowledge distillation in two stages -- pre-training and task-specific finetuning -- to improve the demonstration distillation model.\n2) The paper provides thorough empirical evidence that the proposed approach makes the ICL efficient while also being on par with/improving vanilla ICL. \n3) The paper presents a detailed ablation study to highlight the utility of various components of the proposed approach."
                },
                "weaknesses": {
                    "value": "There are no major weaknesses in the paper that the reviewer could find. Please see the questions section below for some clarifying questions.\n\nSome minor comments about improving the quality of presentation are as follows: \n\n1) Consider paraphrasing some sentences to make them clearer:\n\n* On page 2, \"Considering the evident a misalignment where the LLM trains on natural language tokens but infers using distillation vectors...\"\n* In Appendix D, \"This will not only lose the information from the discarded tokens and cannot distill demonstration with large $K$ (e.g. $K > 1000$ (Hao et al., 2022)).\"\n\n2) On page 2, \"...we embarked on a in-depth...\" --> \"...we embarked on **an** in-depth...\"\n3) In Table 2, \"0-shot\" --> \"zero-shot\"\n4) In Appendix D, \"...are shading insights for future work.\" -->  \"...shedding insights for future work.\"?"
                },
                "questions": {
                    "value": "1) After Eq. (1), the paper states \"...where $\\mathcal{C}$ is the unique set of $\\{y\\_i\\}\\_i=1^K$...\". Isn't this a restrictive assumption? There could be tasks where test examples may have a label not present in any of the $K$ demonstrations, e.g., factual QA.\n2) In Section 3.2, during the pre-training phase, does one begin with a pre-trained or randomly initialized demonstration distillation model?\n3) Do bars in Figure 3 include the cost of generating $S_D$ (distillate vectors) during inference?\n4) Why is prompt tuning performance missing from Figure 4a? Similarly, why is vanilla ICL (with truncated demonstration) missing from Figure 4b?\n5) On page 8, the paper states ``Moreover, it is noteworthy that performance improves in most cases when the No Input perturbation is applied. This not only **underscores the significance of labels**...`` Could authors expand on this? Why does having only labels in the context help?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2742/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814007532,
            "cdate": 1698814007532,
            "tmdate": 1699636216727,
            "mdate": 1699636216727,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MAcZuyyltj",
                "forum": "2Y5kBPtU0o",
                "replyto": "psmuEmugxj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2742/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission2742/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2742/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your recognition of the novelty and usefulness of our work. We are happy to answer your questions as follows.\n\n**Q1. Unclear clarification of   $\\{y_i\\}_{i=1}^K$ . There could be tasks where test examples may have a label not present in any of the $K$ demonstrations, e.g., factual QA.**\n\n**A1**. Apologies for any confusion caused by our initial explanation. In our paper, $C$ is the unique set of $\\{y_i\\}_{i=1}^K$ or the set of answer options in question-answering tasks. \n\n\n\n**Q2. In Section 3.2, during the pre-training phase, does one begin with a pre-trained or randomly initialized demonstration distillation model?**\n\n**A2.** In our study, during the pre-training phase of Section 3.2, we start with a pre-trained language model for the two-stage demonstration distillation learning. \n\n**Q3. Do bars in Figure 3 include the cost of generating  (distillate vectors) during inference?**\n\n**A3.** Yes. \n\n**Q4.  Why is prompt tuning performance missing from Figure 4a? Similarly, why is vanilla ICL (with truncated demonstration) missing from Figure 4b?**\n\n**A4.** \n- The exclusion of prompt tuning from Figure 4a is intentional and based on its underlying methodology. Prompt tuning fundamentally differs from the other methods in that it learns soft embeddings from the entire training set rather than utilizing demonstrations from the training dataset as context. Consequently, the volume of demonstrations, which is the focus of Figure 4a, does not impact prompt tuning's performance, making its inclusion in this figure less relevant.\n- In Figure 4b, our focus was on the impact of distilled demonstration length in distillation-based models, which is why vanilla ICL, not using distillation, was initially omitted. Vanilla ICL operates differently, for instance, using only the first token of a demonstration when the distillation vector length is 1, contrasting with our study's emphasis. \n\n**Q5. On page 8, the paper states Moreover, it is noteworthy that performance improves in most cases when the No Input perturbation is applied. This not only underscores the significance of labels... Could authors expand on this? Why does having only labels in the context help?**\n\n**A5.** Thank you for your inquiry. Our observation that performance often improves with the 'No Input' perturbation, where only labels are present in the context, can be attributed to a couple of key factors:\n- No input forces the model to focus solely on the labels, thereby reducing the potential noise or distraction from extraneous or less relevant input data.\n- Labels alone provide a strong signal for the model, enabling it to make more accurate predictions. This could be due to the distilled essence of the task captured in the labels, which the model can leverage more effectively in the absence of other input information. It highlights the model's capacity to extract and utilize the core, task-relevant information from the labels, demonstrating a form of efficiency in its learning process.\n\nThese observations suggest that the model can extract and utilize critical, task-relevant information from the labels effectively, demonstrating an efficient learning process. Additionally, recent studies, such as [1], have shown similar improvements with no-input perturbations, further supporting our findings.\"\n\n[1] Ye, Qinyuan, Iz Beltagy, Matthew E. Peters, Xiang Ren, and Hannaneh Hajishirzi. \"FiD-ICL: A Fusion-in-Decoder Approach for Efficient In-Context Learning.\" In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8158-8185. 2023."
                    },
                    "title": {
                        "value": "Response to Reviewer Uu3b"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739301300,
                "cdate": 1700739301300,
                "tmdate": 1700739499789,
                "mdate": 1700739499789,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FFv6n5uSUN",
            "forum": "2Y5kBPtU0o",
            "replyto": "2Y5kBPtU0o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2742/Reviewer_RDfP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2742/Reviewer_RDfP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes MEND: meta-demonstration distillation. The authors design a distillation method to compress a long text demonstration into a short vector. MEND is designed as a meta-distillation method, such that the distillation model can be applied to unseen tasks. Experiments are provided to demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The proposed distillation method for prompt-tuning is well-motivated. The presentation is very clear. The proposed distillation method is effective and easy to understand.\n\n* The authors investigate several flavors of models to demonstrate the effectiveness of the proposed method. Specifically, the authors use GPT-2 with different sizes and T5 to show that MEND outperforms existing in context learning approaches."
                },
                "weaknesses": {
                    "value": "My main concern is about experimental settings.\n\n* Could the authors explain why GPT-2 and T5 are used? These models are usually considered outdated and more recent models should be used.\n  * For the GPT family, GPT-J, GPT-Neo, OPT are all open-sourced. And the LLaMa models are instruction fine-tuned such that they may show different behavior when facing vectorized demonstrations.\n  * For the T5 model, I suggest using Flan-T5, which shows much stronger performance than T5.\n\n* The authors should consider more baselines. For example, Chain-of-Thought (CoT) can demonstrate stronger performance than vanilla ICL. The authors need to at least compare with the vanilla CoT. I also suggest distilling CoT prompts (demonstrations) and see whether this can further improve the performance of MEND.\n\nI will raise the score if the authors can run some experiments on more recent models."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2742/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699253975194,
            "cdate": 1699253975194,
            "tmdate": 1699636216534,
            "mdate": 1699636216534,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I154BKRcpd",
                "forum": "2Y5kBPtU0o",
                "replyto": "FFv6n5uSUN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2742/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission2742/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2742/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed and constructive review. We would like to address your concerns as follows.\n\n**Q1.1. Could the authors explain why GPT-2 and T5 are used? These models are usually considered outdated and more recent models should be used.**\n\n**A1.1.** Thank you for your question regarding our choice of GPT-2 and T5 as the backbone language models. We selected these models based on their demonstrated capabilities in in-context learning, as highlighted in related works [1-2]. Despite being older models, GPT-2 and T5 are still widely recognized for their effectiveness in this domain.\n\nMoreover, these models embody different architectural styles, with GPT-2 being a decoder-only model and T5 being an encoder-decoder model. This diversity allows us to showcase the general applicability of our proposed method across varying model structures. By achieving positive results with both GPT-2 and T5, we demonstrate that our method is not limited to a specific architecture but is broadly applicable, which is crucial for validating the robustness and versatility of our approach in the context of in-context learning.\n\n[1] Min, Sewon, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. \"Metaicl: Learning to learn in context.\" arXiv preprint arXiv:2110.15943 (2021).\n[2] Phang, Jason, Yi Mao, Pengcheng He, and Weizhu Chen. \"Hypertuning: Toward adapting large language models without back-propagation.\" In International Conference on Machine Learning, pp. 27854-27875. PMLR, 2023.\n\n\n\n**Q1.2. More recent language model should be used.**\n\n**A1.2.** Thanks for the suggestion. We have added an experiment that tests our method on opt-6.7b and flan-t5-xl. You can refer to the general response for more information.\n\n**Q2. The authors should consider more baselines. For example, Chain-of-Thought (CoT) can demonstrate stronger performance than vanilla ICL. The authors need to at least compare with the vanilla CoT. I also suggest distilling CoT prompts (demonstrations) and see whether this can further improve the performance of MEND.**\n\n**A2.** Thank you for the suggestion to include Chain-of-Thought (CoT) baselines.\nOur current dataset doesn't have an ID match with existing CoT sources, which limited our ability to directly compare with standard CoT prompts. \nHowever, we've addressed this by utilizing the BIG-Bench-Hard (BBH) dataset, a well-known CoT evaluation benchmark.\nWe adapted BBH problems into multiple-class classification tasks and evaluated the CoT capabilities of MEND and other baselines like HyperTuning. The results, as shown in the table below, demonstrate the effectiveness of our distillation model in capturing meaningful CoT for the downstream large language model. \n\nNotably, MEND shows superior performance compared to the baseline methods, indicating its enhanced ability to distill Chains for effective language model reasoning.\n\n| Methods     | BBH Accuracy |\n|-------------|--------------|\n| Vanilla ICL | 0.3243       |\n| HyperTuning | 0.3503       |\n| MEND        | **0.3558**   |"
                    },
                    "title": {
                        "value": "Response to Reviewer RDfP"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739232020,
                "cdate": 1700739232020,
                "tmdate": 1700739521454,
                "mdate": 1700739521454,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yAgvWvP0xC",
            "forum": "2Y5kBPtU0o",
            "replyto": "2Y5kBPtU0o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2742/Reviewer_Dw9E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2742/Reviewer_Dw9E"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a new method for efficient in-context learning via direct prediction of demonstration context vectors. The proposed method, called MEND, combines hypernetwork training with distillation of regular in-context learning behavior to achieve high-quality \"prompt vector\" synthesis capabilities. Authors validate MEND on the MetaICL dataset using GPT2 and T5 models, showing performance gains and accuracy improvements compared to other in-context learning baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The proposed approach is well-motivated and achieves significant improvements in each of the studied setups.\n* The paper contains a detailed analysis section along with the ablation study for MEND, justifying the necessity of each component of the method."
                },
                "weaknesses": {
                    "value": "* My primary concern regarding the evaluation is that the models studied in the paper (GPT2-XL, T5-large) are relatively small and not representative of models that actually benefit from in-context learning. In fact, authors acknowledge this limitation in the appendix; I simply believe that having experiments on larger models (for example, training only one distillation model and applying it to larger LMs) would increase the impact of the work.\n* The inference efficiency measurement protocol could likely be improved. First, it is unclear whether key/value caching is used for generation: this should make the impact of additional demonstrations less severe. Also, I think it would be helpful to have a more detailed memory/time breakdown for MEND: measuring only the inference with obtained meta-demonstrations is not sufficient, as the distillation model needs to process input demonstrations into prompts.\n* At times, it was a bit difficult to understand the reasoning of the paper due to grammar errors/typos and word choice. Consider, for examplem, \"the evident an misalignment\" and \"between teacher student's\" on page 2, \"into a condensed vectors\", \"distill the supervisional from demonstrations\", and \"cannot leveraging\" on page 5."
                },
                "questions": {
                    "value": "* In Table 2, what were the standard deviations across runs?\n* How did you format 16 demonstration examples into a single input string for each dataset? For example, there are different ways of joining several demonstrations (space, linebreak etc.). Have you studied the robustness of models/methods to that formatting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2742/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2742/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2742/Reviewer_Dw9E"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2742/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699637994963,
            "cdate": 1699637994963,
            "tmdate": 1699637994963,
            "mdate": 1699637994963,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "y3msy5r3Qs",
                "forum": "2Y5kBPtU0o",
                "replyto": "yAgvWvP0xC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse to Reviewer Dw9e (Part 1)"
                    },
                    "comment": {
                        "value": "Thanks for your valuable time and thoughtful review. We want to resolve your concerns as follows:\n\n**Q1. Other large language model. **\n\n**A1.** Thanks for the suggestion. We have evaluated the generazation of our proposed method on flan-t5-xl and opt-6.7b. You can refer to the general response for more information. \n\n**Q2.1. The inference efficiency measurement protocol could be improved. First, it is still being determined whether key/value caching is used for generation: this should make the impact of additional demonstrations less severe.**\n\n**A2.1** Our experiments did not employ key-value caching across all baseline methods, including MEND, to ensure a consistent and fair comparison framework. Our observation also influenced our decision that key-value caching does not significantly enhance Vanilla ICL's efficiency in the context of our study in all the evaluation metrics. In our efficiency evaluation experimnent on opt-6.7b, we can observe key-value cache reduce the FLOPS but still consume large memory and require much computation time. \n\n|      Method     | TFLOPS | GPU Memory (GB) | Time (Seconds) |\n|:---------------:|--------|-----------------|----------------|\n|   Vanilla ICL   | 19.06  | 2.95            | 1.44           |\n|   PromptTuning  | 4.93   | 0.70            | 0.70           |\n|       MEND      | 5.03   | 0.70            | 0.74           |\n| Key-Value Cache | 0.93   | 2.85            | 2.43           |\n \nAnother key consideration was the potential loss of contextual information from the demonstration interactions when using key-value caching. This loss could necessitate additional meta-training for the language model to effectively represent demonstration examples, which falls outside the scope of our current investigation [1].\n\n[1]Qinyuan Ye, Iz Beltagy, Matthew Peters, Xiang Ren, and Hannaneh Hajishirzi. 2023. FiD-ICL: A Fusion-in-Decoder Approach for Efficient In-Context Learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8158\u20138185, Toronto, Canada. Association for Computational Linguistics.\n\n**Q2.2. Also, it would be helpful to have a more detailed memory/time breakdown for MEND: measuring only the inference with obtained meta-demonstrations is not sufficient, as the distillation model needs to process input demonstrations into prompts.**\n\n**A2.2.**  When evaluating MEND, we calculated the total time inclusive of both obtaining the demonstration distillation and utilizing the distillation for LLM inference. This approach was chosen to provide a comprehensive understanding of the efficiency of our method in real-world scenarios. If the meta-demonstrations were pre-obtained, the efficiency would align closely with that of prompt tuning. However, our focus was to assess the end-to-end efficiency of MEND, reflecting its practical application where demonstrations may not always be pre-processed.\n\n**Q3. Readability improvement.** \n**A3.** Thanks for your suggestion; we have updated our paper based on your comments.\n\n**Q4. In Table 2, what were the standard deviations across runs?**\n**A4.** Thanks for the suggestion.  We have uploaded the standard deviation for Table 2. You can refer to our updated paper.\n\n**(Unfinished)**"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739588574,
                "cdate": 1700739588574,
                "tmdate": 1700741989150,
                "mdate": 1700741989150,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]