[
    {
        "title": "Graph neural processes and their application to molecular functions"
    },
    {
        "review": {
            "id": "AIrci2WHF5",
            "forum": "ogV88XPnK6",
            "replyto": "ogV88XPnK6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8464/Reviewer_cRU3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8464/Reviewer_cRU3"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an approach based on graph neural processes for meta learning for drug discovery. The authors suggest replacing the MLP encoder of vanilla latent/deterministic neural processes with a graph neural network in order to capture higher-order interactions between the input covariables which in this case are atomic and atom-atom bond features. In addition, they propose a fine-tuning approach to adapt parameters after meta-training and an adoption of model-agnostic meta learning for NPs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Phrasing the problem of drug discovery as a meta-learning problem and using graph neural networks as encoders for neural processes is in the reviewer's opinion both original and reasonable.\n- The paper is well written and easy to follow, and the problems of meta-learning in drug discovery is well delineated.\n- The approach of fine-tuning is intuitive and seems reasonable."
                },
                "weaknesses": {
                    "value": "- The main contribution of the paper is the usage of vanilla NPs with an encoder that is an adapted graph neural network of a previously introduced method [1] . In total, the contribution seems too incremental and too little.\n- The description of the methodology itself (molecular graph attentive encoder) is not detailed enough and very superficial (in total 5 lines of the entire manuscript).\n- The fine-tuning and MAML approaches for parameter adaption described in the paper are of little novelty. In addition, the theoretical benefit and motivation of the MAML tuning is not clear to the reviewer since NPs can generally already be considered as meta learnerns.? Empirically, the MAML tuning sometimes improves and sometimes worsens predictive performance (see, e.g., Table~1 FP-CNP with FP-CNP (MAML) or MG-LNP with MG-LNL (MAML)).\n- The experimental section seems very thin and more evaluations with missing competing methods should be made. See, e.g., [2] as a reference.\n- The reference section is incomplete and sometimes incorrect. For instance, the \"Attention is all you need\" paper is from 2017 and not from 2023 and misses the conference information.\n- The authors fail to cite relevant literature on graph neural processes, e.g. [3].\n\n[1] https://pubs.acs.org/doi/10.1021/acs.jmedchem.9b00959\n[2] https://arxiv.org/abs/2205.02708\n[3] https://arxiv.org/abs/2305.18719"
                },
                "questions": {
                    "value": "- Some clarifications of the math of the encoder structure or an illustrative figure would in the reviewer's opinion improve the quality of the manuscript. While background on NPs is explained in sufficient detail (both in the main manuscript as well as the appendix), the actual method is not described at all.\n- The authors could evaluate the case where a NP has both a latent and deterministic encoder. See, e.g., [2]\n- As far as I can tell, the authors do not compare themselves against recent methods such as in [1]. Is this true and if so is there any reason for that?\n- LNPs are generally harder to train then CNPs. Is the poor performance of LNPs due to this fact or how can it be explained? Is it because the authors seemed to have trained only for a fixed number of iterations and not until converge (see Appendix C3.7)?\n\n[1] https://arxiv.org/abs/2205.02708\n[2] https://arxiv.org/abs/1901.05761"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8464/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8464/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8464/Reviewer_cRU3"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8464/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698663775177,
            "cdate": 1698663775177,
            "tmdate": 1699637056448,
            "mdate": 1699637056448,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nYDNb7Jt08",
                "forum": "ogV88XPnK6",
                "replyto": "AIrci2WHF5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8464/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer cRU3 for their time and comments, to which we reply below.\n\n**Insufficient description of the GNN architecture**\n\nWe thank the reviewer for raising this point. We would like to explain that the architecture of the GNN used by our MG-NPs is based on a very popular architecture (Attentive FP [1]) whose paper we reference and can be checked by the readers. The adaptation procedure is a bit trickier to replicate than the architecture (especially the one using MAML), so we used more space on that section instead, given the length constraints. We explain the differences between Attentive FP and our GNN in Appendix C.3.2. In addition, if our article is accepted we will publish our code with the camera-ready version.\n\n**Fine-tuning and MAML for parameter adaptation are of little novelty.** \n\nAs far as we know, we are the first to propose parameter adaptation for NPs during meta-testing. The reason why parameter adaptation could be helpful is that the meta-learning done by a NP, and the adaptation based on the contexts, are not necessarily perfect. Unlike a Gaussian process, which will make accurate predictions on the context points it has seen during its evaluation, a NP has no theoretical guarantees of accuracy on the context points. We exploit context predictions' errors to improve adaptation to meta-test functions during meta-testing, thus helping to solve the issue of the applicability of NPs when the meta-train and meta-test functions diverge.\n\n**Incorrect citations in bibliography**\n\nWe thank the reviewer for catching this error! We have investigating the source and it appears the mistaken citations had been downloaded from arXiv. For example, the button \"Export BibTeX citation\" on the arXiv page of \"Attention is all you need\" ( https://arxiv.org/abs/1706.03762 ) yields a citation without the conference information and with the year 2023. We were unaware arXiv worked in this way. We apologize for this error, which we have now amended.\n\n**Not citing paper \"Graph Neural Processes for Spatio-Temporal Extrapolation\" by Hu et al**\n\nWe thank the reviewer for pointing out this paper, which we were not aware of. Even though the names of the two models are similar, this model is actually very different from ours, since it is designed to learn node signals within a graph, and ours is designed to learn graph signals across many graphs. Therefore, their model is not readily applicable to our task. In addition, their application (weather prediction) is very different from ours (molecular property prediction). Because of these reasons it does not seem appropriate to cite this paper. However, if after reading our explanation the reviewer believes otherwise, we would be happy to reconsider.\n\n**Why would MAML benefit NPs if both are meta-learning methods?**\n\nIndeed, it may be surprising to combine MAML and NPs since both are meta-learning methods. However, as described above, NPs may not predict the context points accurately, which provides an opportunity for parameter adaptation using the loss of the context points. Such adaptation could be done in multiple ways. One is fine-tuning for multiple epochs, and another one is taking a single iteration of gradient descent after having trained on MAML. Here, we simple use MAML as a strategy to achieve fast adaptation. The meta-training regime of a NP trained with MAML is a bit tricky: it involves computing the loss of the NP's context predictions, taking one iteration of gradient descent on the context loss (the MAML inner update), computing the loss of the NP's target predictions, and taking one iteration of gradient descent through the inner update (the MAML outer update).\n\n**Why does MAML sometimes improve and sometimes worsen performance?**\n\nMAML always improves the performance of a single-task model in the low-data regime (Table 1, GNN with MAML vs GNN with just 20 context points), as expected. However, MAML doesn't always improve the performance of NPs. This may be because of training instabilities, an issue that is well documented for MAML [2]. We tried to improve robustness by implementing improvements from MAML++ but we still found high variance across different random seed initializations. Therefore, using MAML on NPs brings about a potential trade-off: MAML may improve the performance of NPs by improving adaptation to meta-test functions by taking a single step of gradient descent, but MAML may also worsen performance by hindering learning during meta-training due to training instabilities.\n\n**References**\n\n[1] Xiong et al 2020. Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism https://pubs.acs.org/doi/10.1021/acs.jmedchem.9b00959\n\n[2] Antoniou et al 2018. How to train your MAML. https://arxiv.org/abs/1810.09502\n\n[3] Chen et al 2022. Meta-learning adaptive deep kernel gaussian processes for molecular property prediction. https://arxiv.org/abs/2205.02708"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719535188,
                "cdate": 1700719535188,
                "tmdate": 1700719535188,
                "mdate": 1700719535188,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "INBHP7LS2Y",
                "forum": "ogV88XPnK6",
                "replyto": "AIrci2WHF5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8464/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Comparison with ADKF-IFT**\n\nWe thank the reviewer for raising this point. ADKF-IFT [3] is now included as a baseline.\n\n**Why do LNPs perform worse than CNPs?**\n\nInitially, we trained CNPs and LNPs for the same amount of iterations in order to promote an equalitarian allocation of resources and a fair comparison. However, we believed the LNP did not perform so well partly because its regularization term promoted underfitting and made reaching a good optimum challenging. Therefore, we attempted trainining during twice the number of iterations (6000 epochs). However, after 3000 epochs the results did not change substantially. In addition, in order to facilitate reaching a good optimum, we trained two versions of each LNP: one where the regularization term of the ELBO-like objective was computed analytically, and one where the regularization term was approximated with a single Monte Carlo sample (Appendix C.3.7). Then, we chose the version which performed best. At this point, we felt that further optimizing the LNP preferentially would have made the comparison unfair towards other models. All in all, we spent considerably more time and effort optimizing the LNP than the CNP.\n\n**References**\n\n[1] Xiong et al 2020. Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism https://pubs.acs.org/doi/10.1021/acs.jmedchem.9b00959\n\n[2] Antoniou et al 2018. How to train your MAML. https://arxiv.org/abs/1810.09502\n\n[3] Chen et al 2022. Meta-learning adaptive deep kernel gaussian processes for molecular property prediction. https://arxiv.org/abs/2205.02708"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719704816,
                "cdate": 1700719704816,
                "tmdate": 1700719704816,
                "mdate": 1700719704816,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uliwYG8yo7",
            "forum": "ogV88XPnK6",
            "replyto": "ogV88XPnK6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8464/Reviewer_DH7x"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8464/Reviewer_DH7x"
            ],
            "content": {
                "summary": {
                    "value": "Meta-learning is crucial in fields such as biology, where a variety of test functions exist and sparse data is typical. Additionally, uncertainty measures are typically of interest, to aid in deciding which of the predictions should undergo costly experimental validation. In this work, the authors benchmark deep neural process, a type of deep models that also model uncertainty, for few-shot learning. The authors show that even small modifications to the test functions can massively affect meta-generalization, and use two approaches to address this: fine-tuning and a single step of gradient descent on a MAML-trained neural process. They benchmark neural processes in DOCKSTRING, a dataset of docking scores of 260k ligands against 58 diverse proteins, using molecular fingerprints and graphs as input representations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tThe work is nice and easy to follow\n-\tElegant and simple experiment to show the disruption of meta-generalization with divergent test functions (Figure 1)\n-\tProvides useful take-aways in few-shot learning experiments"
                },
                "weaknesses": {
                    "value": "- Restricted evaluation to DOCKSTRING"
                },
                "questions": {
                    "value": "Surprisingly, I don\u2019t have any questions regarding the work itself. It was very clear, easy to follow, and thorough in the evaluation of deep NPs for few-shot learning in DOCKSTRING. I believe this is an important work in benchmarking deep NPs that would be of great use to the community."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8464/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698748717073,
            "cdate": 1698748717073,
            "tmdate": 1699637056334,
            "mdate": 1699637056334,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZLlVE2LRFA",
                "forum": "ogV88XPnK6",
                "replyto": "uliwYG8yo7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8464/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer DH7x for their time and comments. We have now added ADKF-IFT as an additional baseline, and have added one figure (Figure 2) where we compare its calibration with that of NPs and a GP on fingerprints."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718491530,
                "cdate": 1700718491530,
                "tmdate": 1700718491530,
                "mdate": 1700718491530,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mfkGTePr9K",
            "forum": "ogV88XPnK6",
            "replyto": "ogV88XPnK6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8464/Reviewer_PKuZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8464/Reviewer_PKuZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies meta-learning approaches for molecular tasks, and focuses on introducing neural process (NP) for this application. Apart from building different NP models (CNP,LNP) for molecules, taking fingerprints (FPs) or molecular graphs (MGs) as input features, this paper emphasizes the challenge of meta-generalization in molecular tasks. To close to real world molecular applications, it sets up experiments with an unusual meta-learning setting: the correlation between training and testing tasks are controlled at a low degree, and the size of context varies in a large range. To deal with, this paper proposes to combine gradient-based adaptation (MAML, fine-tuning) with NP model. The authors tailor DOCKSTRING dataset, and detail empirical results show that MG-CNPc(fine-tuned) has a performance advantage in most cases."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThis paper comprehensively study NP-based models on molecular tasks, including different NP variants, different molecular features, different additional adaptation strategies.\n\n2.\tIt pointed out the challenge that tasks are highly diverse in real world molecular applications. And propose additional adaptation steps should be adopted based on NP models to increase the meta-generalizability.\n\n3.\tData processing and empirical results are shown in detail. It looks convincing that the proposed method could show advantage with such setting."
                },
                "weaknesses": {
                    "value": "1. Lack of novelty. As a representative amortized meta-learner, NP has been widely studied. This paper adopts the most conventional NP models on molecular tasks. \u201cNP+gradient steps\u201d is also a popular way to improve meta-learning performance by combining two adaptation strategies. (in similar fields, there is [1]). It seems little technical contribution in this paper.\n\n2. The authors propose that existing datasets are highly homogeneous across tasks, while in reality the task diversity should be considered. However, there lacks evidence in this paper. No empirical results of existing popular datasets (e.g., fs-mol[2], moleculenet[3]), nor comparing them with real-world cases are provided.\n\n3. Lack of benchmark datasets and baselines. Since the proposed is following a standard meta-learning setting, existing few-shot molecular property prediction methods [4,5,6], should be considered. Among them, [5] is also applicable for regression task, which should be compared on DOCKSTRING. And the proposed method should also be applicable for classification tasks, so it should be tested on [2][3], and compared with [4,5,6].\n\n4. Poor organization of related works. The related works mix everything (i.e. datasets, methods) together, which are hard to read.\n\n[1] Zhang, Q., Zhang, S., Feng, Y., & Shi, J. (2023). Few-Shot Drug Synergy Prediction With a Prior-Guided Hypernetwork Architecture. IEEE Transactions on Pattern Analysis and Machine Intelligence.\n\n[2] Stanley, M., Bronskill, J. F., Maziarz, K., Misztela, H., Lanini, J., Segler, M., ... & Brockschmidt, M. (2021, August). Fs-mol: A few-shot learning dataset of molecules. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)\n\n[3] Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Geniesse, C., Pappu, A. S., ... & Pande, V. (2018). MoleculeNet: a benchmark for molecular machine learning. Chemical science, 9(2), 513-530.\n\n[4] Wang, Y., Abuduweili, A., Yao, Q., & Dou, D. (2021). Property-aware relation networks for few-shot molecular property prediction. Advances in Neural Information Processing Systems, 34, 17441-17454.\n\n[5] Chen, W., Tripp, A., & Hern\u00e1ndez-Lobato, J. M. (2022, September). Meta-learning adaptive deep kernel gaussian processes for molecular property prediction. In The Eleventh International Conference on Learning Representations.\n\n[6] Schimunek, J., Seidl, P., Friedrich, L., Kuhn, D., Rippmann, F., Hochreiter, S., & Klambauer, G. (2023). Context-enriched molecule representations improve few-shot drug discovery. arXiv preprint arXiv:2305.09481."
                },
                "questions": {
                    "value": "Please refer to weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8464/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829257047,
            "cdate": 1698829257047,
            "tmdate": 1699637056195,
            "mdate": 1699637056195,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rACPGLVmTj",
                "forum": "ogV88XPnK6",
                "replyto": "mfkGTePr9K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8464/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank reviewer PKuZ for their time. We reply to their comments below.\n\n**Showing lack of heterogeneity in current benchmarks**\n\nWe would like to explain that when we referred to lack of heterogeneity in current benchmarks, we were referring to the toy datasets used by the broader meta-learning community, not to FS-Mol and MoleculeNet. Examples of the types of datasets we were referring to are the 1D sine function used in the original MAML paper [1], MNIST in the original CNP and LNP publications [2,3], or samples from a Gaussian process model with slightly different hyperparameters (the latter is often used in the NP literature, see e.g.[4] for a recent example). These simple datasets may be useful to develop novel meta-learning models but they do not represent the degree of heterogeneity of scientific datasets. Therefore, models that do well on such datasets may fail catastrophically when faced with divergent meta-train and meta-test functions. We showed this was the case for MAML and the CNP in Figure 1, where even a small change in the 1D sine function led to loss of meta-generalization.\n\n**Comparison with other real-world datasets**\n\nWe thank the reviewer for raising this point, which is a fair one. We made a decision to focus on benchmarking docking scores in this publication and leave bioactivity endpoints for a future, more-biologically-focused publication. There are three compelling reasons to use docking scores when benchmarking drug-discovery algorithms. First, some widely-used bioactivity datasets such as MoleculeNet may present their own flaws, e.g. they present intra-function inconsistencies due to pooling from different publications [5,6]. Second, docking scores are highly useful for drug discovery, which makes them interesting to predict with ML by their own right. Some recent high-impact publications that had ML-prediction of docking scores at their core are [7,8] . Finally, docking depends on the simulation of a 3D interaction between a protein and a molecular ligand. Therefore, a model that is able to learn docking scores must be sophisticated enough to capture the structural relation between the protein target and the molecule. In fact, some docking scores are very challenging to predict, e.g. in the original publication of the dataset we are using (DOCKSTRING), the best model's prediction on ESR2 was a mediocre 0.627, even after training on 230k examples [9].  For this reason, docking scores have been proposed as promising benchmarks for drug discovery [9,10,11].\n\nWe used two different datasets of docking scores: the original DOCKSTRING dataset, and an augmented dataset where scores were transformed either linearly or non-linearly (by taking the minimum or the maximum between each score and the median of the score distribution) and were combined with the quantitative estimate of drug-likelihood (QED).\n\n**References**\n\n[1] Finn et al 2017. Model-agnostic meta-learning for fast adaptation of deep networks. https://arxiv.org/abs/1703.03400\n\n[2] Garnelo et al 2018. Conditional neural processes. http://proceedings.mlr.press/v80/garnelo18a/garnelo18a.pdf\n\n[3] Garnelo et al 2018. Neural processes. https://arxiv.org/abs/1807.01622\n\n[4] Bruinsma et al 2023. Autoregressive Conditional Neural Processes. https://openreview.net/forum?id=OAsXFPBfTBh\n\n[5] Pat Walters.  We need better benchmarks for machine learning in drug discovery. http://practicalcheminformatics.blogspot.com/2023/08/we-need-better-benchmarks-for-machine.html\n\n[6] Chan et al 2023. Embracing assay heterogeneity with neural processes for markedly improved bioactivity predictions. https://arxiv.org/abs/2308.09086\n\n[7] Wong et al 2022. Benchmarking AlphaFold-enabled molecular docking predictions for antibiotic discovery https://www.embopress.org/doi/full/10.15252/msb.202211081\n\n[8] Gentile et al 2022. Artificial intelligence\u2013enabled virtual screening of ultra-large chemical libraries with deep docking. https://www.nature.com/articles/s41596-021-00659-2\n\n[9] Garcia-Ortegon et al 2022. DOCKSTRING: Easy molecular docking yields better benchmarks for ligand design. https://pubs.acs.org/doi/full/10.1021/acs.jcim.1c01334\n\n[10] Huang et al 2021. Therapeutics Data Commons: Machine learning datasets and tasks for drug discovery and development. https://arxiv.org/abs/2102.09548\n\n[11] Ciepli\u0144ski et al 2023. Generative models should at least be able to design molecules that dock well: A new benchmark. https://pubs.acs.org/doi/10.1021/acs.jcim.2c01355"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718249964,
                "cdate": 1700718249964,
                "tmdate": 1700718249964,
                "mdate": 1700718249964,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7sIyDVzAqo",
            "forum": "ogV88XPnK6",
            "replyto": "ogV88XPnK6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8464/Reviewer_P8e8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8464/Reviewer_P8e8"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors study Graph NP's performance in few-shot learning tasks, and propose fine-tuning strategies to further improve GNP's regression performance while maintaining good calibration. They also present a Bayesian optimization case study to showcase GNP's potential advantages."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Writing: Well-organized, easy-to-follow paper.\n2. Significance: Show that graph NPs are competitive in molecular few-shot learning tasks."
                },
                "weaknesses": {
                    "value": "1. Applicability: Focuses on regression tasks only despite abundant data and baselines in classification.\n2. Novelty: Fine-tuning NPs during meta-testing are not novel contributions."
                },
                "questions": {
                    "value": "1. Why didn't you extend Graph NPs into the classification setting, where the amount of data and baselines is abundant?\n2. Could you explain the results presented in Figure F.1, where the $R^2$ did not decrease as the percentage of training points sampled increase?\n3. Have you considered studying the impact of context/target set randomization on calibration of uncertainty estimates?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8464/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8464/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8464/Reviewer_P8e8"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8464/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698986228111,
            "cdate": 1698986228111,
            "tmdate": 1699637056082,
            "mdate": 1699637056082,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "26ylYJBil8",
                "forum": "ogV88XPnK6",
                "replyto": "7sIyDVzAqo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8464/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer P8e8 for their time. We address their comments below.\n\n**Could you explain the results presented in Figure F.1, where the did not decrease as the percentage of training points sampled increase?**\n\nThis is indeed an interesting point. It appears that the NPs' mean prediction $\\mu_\\theta(x)$ was more robust to increasing the percentage of context points during meta-training than the variance prediction $\\sigma_\\theta^2(x)$. As a result, the NLPD deteriorated much more (Figure 4) than the $R^2$ (Figure F.1). This seems reasonable: if the model overfits to the training datapoints and memorizes their $y$ values, the mean predictions will fixate at the $y$ values and the variance predictions will become ever smaller (since smaller variances with the correct mean will lead to higher predictive likelihoods). Therefore, the model will learn to always predict inadequately small variances, which leads to catastrophic NLPD at meta-test times.\n\nIt is worth noting, however, that even if the NLPD deteriorated faster than the $R^2$, the $R^2$ also suffered if the fraction of context points at each iteration during meta-training was too high. This is shown in Figure F1, where the $R^2$ value of test datapoints was always lower at 100% than at 0.8, 2, 4 or 8%, for every meta-train and every meta-test function.\n\n\n**Have you considered studying the impact of context/target set randomization on calibration of uncertainty estimates?**\n\nWe thank the reviewer for making this suggestion. The NLPD in Figure 4 was partly a measure of calibration since poorly calibrated models have larger NLPD. Indeed, when 100% of points of a function were taken as contexts and targets (hence having no randomization of contexts and targets during meta-training) the NLPD increased significantly both for ftrain, dtrain points and for all meta-test points (ftest, dtrain and ftest, dtest). In addition, we have now added to the manuscript Figure F.2, which shows the correlation of predicted variance and prediction error in a MG-CNP sampling a small fraction of training points as contexts and targets at each iteration, as usual, and a MG-CNP taking 100% of points as contexts and targets. The latter displayed low correlation, indicating poor calibration."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717825268,
                "cdate": 1700717825268,
                "tmdate": 1700718959425,
                "mdate": 1700718959425,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]