[
    {
        "title": "Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization"
    },
    {
        "review": {
            "id": "pxbXeYg87y",
            "forum": "tbFBh3LMKi",
            "replyto": "tbFBh3LMKi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4914/Reviewer_jjSi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4914/Reviewer_jjSi"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on unifying offline and online RL to achieve efficient and safe learning. Specifically, this paper proposes Uni-O4, which utilizes an on-policy RL objective for both offline and online learning. For offline learning, this paper combines the advantages of both BPPO and OPE to achieve the desired performance. For online learning, this paper directly utilizes the standard PPO for finetuning. Experiments under offline RL and offline-to-online RL setting demonstrate the effectiveness of Uni-O4. Furthermore, this paper extends the offline-to-online setting to address a practical robotic scenario, transforming it into an online-to-offline-to-online setting. Empirical results highlight the seamless integration across these three stages in Uni-O4."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper investigates an interesting research problem: offline-to-online setting, and online(simulator)-to-offline(real-world)-to-online(real-world) setting in robotic scenarios.\n- This paper performs extensive experiments to derive empirical findings."
                },
                "weaknesses": {
                    "value": "Overall, this is a descent paper. However, in the current manuscript, I think the following concerns should be addressed.\n\n== Major concern ==\n\n- Unclear empirical motivation in Figure 1. What does these variants (Conservatism, Constraint, Off-policy) mean in (a)? How does Q value compare with V value in (b)? Moreover, from (b), it seems that CQL->SAC shows faster improvement that On-policy (V). How this conclude that Q values of SAC exhibit slow improvement? Furthermore, CQL->CQL and CQL->SAC are na\u00efve solutions for offline-to-online RL. What about advanced offline-to-online RL algorithms, such as off2on?\n- The technique seems incremental by just combining BPPO with OPE.\n- I think there exhibits slight overclaiming of the experimental results in Introduction without sufficient comparison of SOTA algorithms.\n> Experimental results show that Uni-O4 outperforms both SOTA offline and offline-to-online RL algorithms.\n    - Insufficient comparison of offline RL, including but not limited to:\n\n    [1] RORL: Robust Offline Reinforcement Learning via Conservative Smoothing.\n\n    [2] Extreme Q-Learning: MaxEnt RL Without Entropy.\n\n    [3] Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization.\n\n    - Insufficient comparison (including PROTO, ODT, E2O, SPOT, etc.) or at least discussion of related works on offline-to-online RL. Particularly, the baselines include AWAC, CQL, IQL, which are naive solutions for offline-to-online RL. PEX presents weak sample-efficiency for above-medium datasets. Cal-ql is not empirically designed for MuJoCo tasks. There is only one relatively strong baseline, i.e., off2on.\n\n    [1] Adaptive policy learning for offline-to-online reinforcement learning\n\n    [2] Actor-Critic Alignment for Offline-to-Online Reinforcement Learning\n\n    [3] A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning\n\n    [4] Efficient online reinforcement learning with offline data\n\n- Minor improvement on MuJoCo tasks in Figure 4. As shown in the figure, off2on significantly outperforms Uni-O4 by a large margin in halfcheetah-medium and halfcheetah-medium-replay. Besides, I also want to point out that 100 D4RL score already achieves expert-level performance in D4RL benchmark. Thus, further improvement on other settings over 100 is not necessary. Thus, I also wonder why this work does not consider random dataset, which presents a significant challenge for online finetuning to achieve expert performance.\n\n- Comparison in Section 5.2 seems not fair enough. Firstly, I want to know which is the claimed baseline WTW in Figure 5? Additionally, given that IQL is not designed specifically for a real-world robotic scenarios, is the comparison between IQL and Uni-O4 fair? (Uni-O4 is revised to adapt to robotic scenarios as stated in the appendix) Maybe a strong baseline can be considered to verify the superiority of Uni-O4.\n\n- I feel a little struggling to follow Section 5.2. Maybe a pseudo-code like A.6 can be provided to make the readers understand the online-offline-online setting more clearly.\n\n- The experimental results in A.3 make me confusing. I cannot identify obvious differences between Figure 11 (a) and (b).\n\n== Minor Concerns ==\n\n- Figure 2 is not that intuitive. Maybe more explanations can make it clearer.\n\n- How many seeds and evaluation trajectories for AntMaze tasks in offline RL setting? Why offline-to-online RL setting does not consider Kitchen, AntMaze and Adroit-cloned and -relocate tasks?\n\n- Why 18 hours training time is **unacceptable** for real-world robot learning?\n\n- Lack of reproducibility statement.\n\n- Maybe more details on baseline implementation for real-world robot tasks can be provided.\n\n- Why this paper does not provide offline training time for comparison?\n\n== Typos ==\n\n- Page 4, above Equation 7: dataset $D$ -> $\\mathcal{D}$\n\n- Page 9, Hyper-parameter analysis, loss 7 -> Equation 7 is an optimization objective."
                },
                "questions": {
                    "value": "See Weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4914/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4914/Reviewer_jjSi",
                        "ICLR.cc/2024/Conference/Submission4914/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4914/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697775478038,
            "cdate": 1697775478038,
            "tmdate": 1700541453027,
            "mdate": 1700541453027,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sLORCbz7PU",
                "forum": "tbFBh3LMKi",
                "replyto": "pxbXeYg87y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Reviewers",
                    "ICLR.cc/2024/Conference/Submission4914/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Reviewer jjSi (1)"
                    },
                    "comment": {
                        "value": "Thanks Reviewer jjSi for the valuable review of our paper. We appreciate the questions you raised and are committed to delivering a comprehensive response to address the issues.\n\n**Q1.1: What do these variants (Conservatism, Constraint, Off-policy) mean in (a)?**\n\n**A1.1:** Here we use offline -> online method to specify the offline-to-online mehtod. In the previous version of Fig. 1(a), the conservatism, constraint, and off-policy represent the offline-to-online methods CQL->CQL, IQL->IQL, and CQL->SAC. We have revised the legend to make it more clear in the updated manuscript.\n\n**Q1.2: How does the Q value compare with the V value in (b)? Moreover, from (b), it seems that CQL->SAC shows faster improvement than On-policy (V). How does this conclude that Q values of SAC exhibit slow improvement?**\n\n**A1.2:** In the motivating example, we aim to compare the na\u00efve off-policy (SAC), conservative method (CQL), and na\u00efve on-policy (ours). During the offline-to-online transition, na\u00efve off-policy methods often encounter distribution shifts when the policy explores out-of-distribution regions. This can result in evaluation errors and a subsequent drop in performance. A simple method to address this challenge is to inherit the conservatism employed during the offline phase. However, relying solely on this conservative strategy can introduce suboptimality during the online phase, resulting in data inefficiency. Therefore, we aim to investigate whether a na\u00efve on-policy RL algorithm can effectively tackle this challenge. Uni-O4 initialized the policy and $V$-function from the offline phase for a standard online PPO. \n\n*Both the $V$ and $Q$ functions can evaluate the performance of the policy. Therefore, we consider it acceptable to compare the $V$-value and $Q$-value across these methods to investigate how the value function guides performance improvement.* In the case of the na\u00efve off-policy method, the $Q$-value exhibits a significant drop and unstable improvement during online training, leading to a decline in policy performance. The conservative method, on the other hand, shows slow improvement in the $ Q$ value, resulting in suboptimality. In contrast, the on-policy method demonstrates stable improvement in the $ V$ value, leading to consistent and efficient fine-tuning.\n\n**Q1.3: How does this conclude that Q values of SAC exhibit slow improvement?**\n\n**A1.3:** The $Q$-value of the off-policy method improves from 27 to 41, while the $V$-value of the on-policy method improves from approximately 36.5 to 47. It is true that the improvement in the $Q$-value is slightly faster than its counterpart. However, two points should be considered: *1) The on-policy method (ours) has a significantly higher initial performance from the offline phase compared to the off-policy method, resulting in higher initial scores for the value function. Therefore, a slightly slower convergence speed is reasonable. 2) The off-policy method experiences a noticeable drop and unstable improvement in the value, whereas this phenomenon is absent in the on-policy method.*\n\n**Q1.4: Furthermore, CQL->CQL and CQL->SAC are na\u00efve solutions for offline-to-online RL. What about advanced offline-to-online RL algorithms, such as off2on?**\n\n**A1.4:** Thank you for your insightful suggestion. It is interesting to investigate how the ensemble value function guides policy improvement. Thus, we have added the off2on as a comparison. Our findings reveal that the $Q$ value of the off2on method demonstrates faster improvement and can eventually converge to the same level as the $V$ value of Uni-O4. However, it is worth noting that the off2on approach experiences performance drops and unstable training during the initial stages, primarily due to the unstable improvement of the $Q$ value."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500064226,
                "cdate": 1700500064226,
                "tmdate": 1700500064226,
                "mdate": 1700500064226,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ezeh0uwnRC",
                "forum": "tbFBh3LMKi",
                "replyto": "PdOx3Arn9s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4914/Reviewer_jjSi"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Reviewers",
                    "ICLR.cc/2024/Conference/Submission4914/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4914/Reviewer_jjSi"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for providing detailed feedback. I acknowledge substantial improvements in this paper, with most of my concerns thoughtfully addressed. Thus, I raise my score to 6."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541430301,
                "cdate": 1700541430301,
                "tmdate": 1700541430301,
                "mdate": 1700541430301,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "i3HHx9c0Ls",
            "forum": "tbFBh3LMKi",
            "replyto": "tbFBh3LMKi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4914/Reviewer_bAiM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4914/Reviewer_bAiM"
            ],
            "content": {
                "summary": {
                    "value": "The article introduces Uni-O4, a new method for combining offline and online reinforcement learning. It eliminates redundancy and enhances flexibility by using an on-policy objective for both phases. Uni-O4 employs ensemble policies and a straightforward offline policy evaluation approach in the offline phase to address mismatches between behavior policy and data. The approach leads to better offline initialization and efficient online fine-tuning for real-world robot tasks and achieves state-of-the-art results in various simulated benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) Despite some minor flaws, this paper is written in a standardized and organized manner, allowing people to quickly capture the core innovative points and ideas of the paper.\n\n2) The Uni-O4 framework proposed in the article unifies the learning objectives of online and offline learning, making the transition from offline learning to online learning smoother.\n\n3) This method has shown excellent performance in various experiments and has also achieved good results in real-world machine experiments."
                },
                "weaknesses": {
                    "value": "1\uff09The behavior cloning method proposed in section 3.1 requires training multiple policy networks, which incurs significant computational overhead. At the same time, it does not mention how to get $\\hat{pi}_{\\beta}$ from a policy set.\n\n2\uff09Definition error, the definition of f used in formulas 6 and 7 is incorrect. Taking the maximum value of multiple distributions cannot guarantee a single distribution (the sum cannot be guaranteed to be 1), and analysis based on this definition is also meaningless. If the code is truly implemented based on this definition, I am skeptical about the final performance of the algorithm.\n\n3) The proposed offline strategy evaluation method relies on the accuracy of the probability transfer model T, and using the transfer model for evaluation will introduce more errors.\n\n4) The entire method has made too many approximations to the problem and lacks corresponding error analysis.\n\n5\uff09 The legend in Figure 3 is missing to know the correspondence between curves and algorithms."
                },
                "questions": {
                    "value": "1) Can you provide a detailed reconstruction method for policy $\\hat{\\pi}_{\\beta}$, whether to select any one from the policy set $\\Pi_n$ or integrate it using the f function to obtain a policy?\n\n2) Is there a way to evaluate the quality of behavior cloning? Can you compare your proposed method of behavior cloning with previous methods?\n\n3) Can we analyze the errors in the approximate part? You can cite the results of previous work to prove it. For this article, you do not need to prove the size of the approximation error. You only need to quantify the approximation error to a certain extent, analyze the potential impact, and find ways to avoid negative effects."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4914/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4914/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4914/Reviewer_bAiM"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4914/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698669004214,
            "cdate": 1698669004214,
            "tmdate": 1699636476777,
            "mdate": 1699636476777,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ssax9HCSmi",
                "forum": "tbFBh3LMKi",
                "replyto": "i3HHx9c0Ls",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Reviewers",
                    "ICLR.cc/2024/Conference/Submission4914/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bAiM (1)"
                    },
                    "comment": {
                        "value": "Thanks Reviewer bAiM for the valuable review of our paper. We appreciate the questions you raised and are committed to delivering a comprehensive response to address the issues.\n\n**Q1: The behavior cloning method proposed in section 3.1 requires training multiple policy networks, which incurs significant computational overhead.**\n\n**A1:** Thank you for your comment. We have thoroughly examined the computational overhead and performance implications associated with different ensemble sizes of policies in Section 5.3 and **Appendix A.11**. Our findings indicate that an ensemble size of 4 strikes a balance between performance and computational overhead. Specifically, the total training time for the offline phase is approximately 265 minutes (with an ensemble size of 4), compared to 200 minutes (with a single policy) using our PyTorch implementation. Despite the slight increase in training time, this trade-off is acceptable considering the significant performance improvement achieved through the ensemble approach.\n\n**Q2: Definition error, the definition of f used in formulas 6 and 7 is incorrect. Taking the maximum value of multiple distributions cannot guarantee a single distribution (the sum cannot be guaranteed to be 1), and analysis based on this definition is also meaningless. If the code is truly implemented based on this definition, I am skeptical about the final performance of the algorithm.**\n\n**A2:** Thank you for pointing this out. We have taken measures to address this concern. Firstly, we have normalized the distribution of the combined policies, as outlined in Proposition 1. Additionally, we have derived a lower bound over the defined distance, as described in Theorem 1. By optimizing this lower bound, we are able to enhance the diversity among behavior policies. Notably, the derived lower bound corresponds to the penalty term mentioned in Equation 7 of the previous version. Therefore, the implementation can be guaranteed based on these derived results."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499736452,
                "cdate": 1700499736452,
                "tmdate": 1700499736452,
                "mdate": 1700499736452,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cZAybIQKh3",
                "forum": "tbFBh3LMKi",
                "replyto": "i3HHx9c0Ls",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Discussion"
                    },
                    "comment": {
                        "value": "Dear reviewer bAiM:\n\nWe appreciate again for your constructive comments and helpful suggestions. Since the Reviewer-Author discussion phase is coming to an end, we would like to post a follow-up discussion.\n\nIn our previous response, we clarified the raised questions and made corresponding improvements in the updated manuscript. We hope to further discuss with you whether your concerns have been addressed. We are always looking forward to your further comments or suggestions."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622497093,
                "cdate": 1700622497093,
                "tmdate": 1700622497093,
                "mdate": 1700622497093,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mGbBYnhTsi",
            "forum": "tbFBh3LMKi",
            "replyto": "tbFBh3LMKi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4914/Reviewer_rY3g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4914/Reviewer_rY3g"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new algorithm called Uni-O4 that unifies offline and online reinforcement learning using an on-policy optimization approach. The key ideas are:\n- Using an on-policy PPO objective for both offline and online learning to align the objectives.\n- In the offline phase, using an ensemble of policies and offline policy evaluation to safely achieve multi-step policy improvement.\n- Seamlessly transferring between offline pretraining and online fine-tuning without extra regularization or constraints.\n- Evaluating Uni-O4 on both simulated tasks like Mujoco and real-world quadruped robots."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Simple and unified design without needing extra regularization or constraints for stability. Avoids issues like conservatism or instability in prior offline-to-online methods.\n- Impressive results surpassing SOTA on offline RL and offline-to-online tasks. Significantly boosts offline performance and enables rapid, stable online fine-tuning.\n- Policy ensemble provides good coverage over offline data distribution. Offline policy evaluation enables safe multi-step improvement.\n- Excellent results on real-world robots - pretraining, offline adaptation, online finetuning. Showcases efficiency and versatility."
                },
                "weaknesses": {
                    "value": "- The complexity of the method, especially regarding the ensemble behavior cloning and disagreement-based regularization, may present a steep learning curve for practitioners."
                },
                "questions": {
                    "value": "- What are the computational overheads associated with the ensemble policies, and how do they impact the method's scalability?\n- Why don't use the ensemble approach to mitigate mismatches instead of other methods for handling the diverse behaviors in the datasets? For example, Diffusion-QL [1] demonstrates that Diffusion model can be used to learn multimodal policy.\n\n[1] Wang, Zhendong, Jonathan J. Hunt, and Mingyuan Zhou. \"Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning.\" In The Eleventh International Conference on Learning Representations. 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4914/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806007395,
            "cdate": 1698806007395,
            "tmdate": 1699636476701,
            "mdate": 1699636476701,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EavzwFDHSP",
                "forum": "tbFBh3LMKi",
                "replyto": "mGbBYnhTsi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rY3g"
                    },
                    "comment": {
                        "value": "Thank you, Reviewer rY3g, for your valuable review of our paper. We sincerely appreciate the questions you have raised and we are fully dedicated to providing a comprehensive response to address all of the concerns.\n\n**Q1: The complexity of the method, especially regarding ensemble behavior cloning and disagreement-based regularization, may present a steep learning curve for practitioners.**\n\n**A1:** We have added a more intuitive explanation and algorithm description, aiming to provide a clearer understanding for practitioners. Uni-O4 adopts a straightforward approach by combining online and offline RL learning without introducing additional regularization during their transfer. \n\nIn detail, the online RL algorithm employed is a standard PPO. While the offline stage involves multiple components such as value functions, transition models, and ensemble BC, it is worth noting that these components are designed to be implementation-friendly, straightforward, and efficient. For the ensemble BC, we train each policy using a standard loss, incorporating an approximate KL-divergence penalty with a combined policy. This penalty term is implemented by sampling data from the offline dataset. These components remain fixed during the multi-step policy optimization stage. \nDuring this stage, AM-Q consists of the well-trained transition model, and the $Q$-function is used to evaluate the performance of the target policy when replacing the behavior policy. By permitting the replacement of the current behavior policy with the target policy, it achieves multi-step optimization. It differs from iteratively updating the behavior policy which leads to accumulated errors that disrupt the desired monotonicity. In contrast, the multi-step method can guarantee monotonicity due to the AM-Q error is bounded.\n\nWe'll also fully open-source our code with detailed documentation to help implement our method, including in a new real-world robot scenario.\n\n\n**Q2: What are the computational overheads associated with the ensemble policies, and how do they impact the method's scalability?**\n\n**A2:** Thank you for this suggestion. We've conducted the computational overheads and the performance associated with the ensemble size of policies in Section 5.3 and **Appendix A.11**. We found that the ensemble size $4$ is a trade-off between performance and the computational overheads. Specifically, the whole training time of the offline phase is around 265 minutes (ensemble size $4$) vs. 200 minutes (single policy) based on Pytorch implementation. It's acceptable due to the significant performance improvement. For offline training, the primary runtime of the offline phase is attributed to the supervised learning stage. To evaluate the runtime, we conducted 2 million steps for $Q, V$ training, 0.5 million steps for ensemble policy training, and 1 million steps for transition model training. However, in practical scenarios, these training steps can be halved, resulting in reduced runtime. \n\n**Q3: Why don't use the ensemble approach to mitigate mismatches instead of other methods for handling the diverse behaviors in the datasets? For example, Diffusion-QL [1] demonstrates that the Diffusion model can be used to learn multimodal policy.**\n\n**A3:** Thank you for the insightful comment. Diffusion-QL [1] is a representative work that uses diffusion policies to handle the multi-modal dataset. It uses a diffusion model as the policy to predict actions and trains the policy with a behavior cloning loss and an extra state-action value term. It is a value-based training paradigm. Indeed, the diffusion policy has the capability to replace ensemble BC during the supervised stage. However, it cannot directly be optimized by the PPO objective during the multi-step policy optimization stage. This kind of policy-based algorithm needs to model the policy's distribution explicitly and rely on differentiating through the policy distribution to compute gradients and perform updates. It is not suitable for optimizing this kind of generative model directly. \n\nAdditionally, we have added diffusion-QL as a baseline for comparison. The performance of Uni-O4 significantly outperforms the Diffusion-QL over several domain tasks. Moreover, Uni-O4 is more computationally efficient than Diffusion-QL, consuming 12.6 hours (Diffusion-QL) vs. 4.4 hours (Uni-O4).\n| Environment/method | Diffusion-QL [1]    | Uni-O4    |\n| ------------------ | ------- | ------- |\n| MuJoCo locomotion average           | 88.0    | 90.7   |\n| AntMaze average   | 69.6    | 74.7    |\n| Adroit-Pen average    | 65.1    | 108.1    |\n| kitchen average  | 69.0    | 72.3    |\n| All average | *72.9* | *86.4* |\n\n[1] Wang Z, Hunt J J, Zhou M. Diffusion policies as an expressive policy class for offline reinforcement learning[J]. arXiv preprint arXiv:2208.06193, 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499518572,
                "cdate": 1700499518572,
                "tmdate": 1700499518572,
                "mdate": 1700499518572,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RMoGlD8dEW",
                "forum": "tbFBh3LMKi",
                "replyto": "mGbBYnhTsi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Reviewers",
                    "ICLR.cc/2024/Conference/Submission4914/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Discussion"
                    },
                    "comment": {
                        "value": "Dear reviewer rY3g:\n\nWe would like to thank you again for your constructive comments and helpful suggestions. Since we are nearly at the end of the discussion phase, we'd like to know whether our response has addressed your concerns and we are always looking forward to your further comments or suggestions."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621891936,
                "cdate": 1700621891936,
                "tmdate": 1700621891936,
                "mdate": 1700621891936,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3hRDRbWl1s",
            "forum": "tbFBh3LMKi",
            "replyto": "tbFBh3LMKi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4914/Reviewer_CZhd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4914/Reviewer_CZhd"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new approach, Uni-O4, to combine offline and online reinforcement learning, which is an important and challenging problem in the field. Uni-O4 can effectively address the mismatch issues between the estimated behavior policy and the offline dataset,  and it can achieve better offline initialization than other methods and be more stable for the later online fine-tuning phase. The experimental results on several benchmark tasks show that Uni-O4 outperforms existing state-of-the-art methods in terms of stability, final performance, and the capability for real-world transferring."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Uni-O4 can seamlessly transfer between offline and online learning, enhancing the flexibility of the learning paradigm.\n\n2. The experiments are sufficient and persuasive. The experiments on real-world robots showed very good performance in the provided videos."
                },
                "weaknesses": {
                    "value": "1. Notions are confusing in this paper, especially after the overloading in Equ. (8).\n\n2. In Fig.2, It is hard to capture the Offline Multi-Step Optimization process, i.e. the sequence relationship of each step.\n\n3. In Sec 3.1:  \"BPPO leads to a mismatch ... due to the presence of diverse behavior policies in the dataset D\",  could authors explain further why the diversity is blamed for the mismatch?\n\n4. Lack of theoretical analysis (to support the motivation of technique details), but it has sufficient experiments thus this point is acceptable I think."
                },
                "questions": {
                    "value": "Suggest to add legends for Fig. 3 or bringing the legend in Fig. 4 forward."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4914/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4914/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4914/Reviewer_CZhd"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4914/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699212962454,
            "cdate": 1699212962454,
            "tmdate": 1700548161566,
            "mdate": 1700548161566,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cAi2eotdQe",
                "forum": "tbFBh3LMKi",
                "replyto": "3hRDRbWl1s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Reviewers",
                    "ICLR.cc/2024/Conference/Submission4914/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CZhd (1)"
                    },
                    "comment": {
                        "value": "Thanks Reviewer CZhd for the valuable review of our paper. We appreciate the questions you raised and are committed to delivering a comprehensive response to address the issues.\n\n**Q1: Notions are confusing in this paper, especially after the overloading in Equ. (8).**\n\n**A1:** Thank you for pointing this out. We have rewritten Section 3.2 \"Multi-step policy ensemble optimization\" to enhance clarity in our presentation. In Equ. 8 (7 in the new version), specifically, we overload the notion of behavior policy for introducing the iteration number. It is necessary for showcasing how Uni-O4 performs multi-step optimization by adding the iteration number as subscript $k$ in behavior policies $\\pi^i_k$ where $i$ specifies the policy index in the ensemble. Furthermore, we have revised the notion of the $Q$ and $V$ -function to be consistent with IQL as $\\widehat{Q_{\\tau}}$ and $\\widehat{V_{\\tau}}$ from $Q_{\\pi_{\\beta}}$ and $V_{\\pi_{\\beta}}$. Because we found that the previous version ($Q_{\\pi_{\\beta}}$ and $V_{\\pi_{\\beta}}$) will cause ambiguity. On the other hand, IQL has the capability to reconstruct the optimal value function, i.e., $\\lim_{\\tau \\rightarrow 1} Q_{\\tau} (s, a) = Q^{\\ast} (s, a)$  based on dataset support constraints [1]. In this work, we exploit the desirable property to facilitate multi-step policy optimization and recover the optimal policy. $Q_{\\tau}$ and $V_{\\tau}$ are the optimal solution. We use $\\widehat{Q_{\\tau}}$ and $\\widehat{V_{\\tau}}$ to denote the value functions obtained through gradient-based optimization.\n\n**Q2: In Fig.2, It is hard to capture the Offline Multi-Step Optimization process, i.e. the sequence relationship of each step.**\n\n**A2:** Based on this comment, we have updated Fig. 2 by incorporating sequence numbers and a detailed description for each step. During the offline multi-step optimization stage, the policy optimization begins with the learned behavior policies. Specifically, each policy undergoes optimization using the PPO loss for a designated number of gradient steps outlined in step 1 of Fig. 2. Subsequently, both the target policy and behavior policy are assessed using AM-Q to determine whether the behavior policy should be replaced by the target policy, as specified in step 2. If the evaluation results meet the OPE conditions, the behavior policy is substituted with its target policy, denoted as step 3. These three steps are iterated until the predetermined number of gradient steps is reached. Also, we sincerely invite the reviewer to check the GIF of Fig. 2 on our website for a clearer presentation."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499137580,
                "cdate": 1700499137580,
                "tmdate": 1700499137580,
                "mdate": 1700499137580,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JuypvJ6EhK",
                "forum": "tbFBh3LMKi",
                "replyto": "3hRDRbWl1s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Reviewers",
                    "ICLR.cc/2024/Conference/Submission4914/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CZhd (2)"
                    },
                    "comment": {
                        "value": "**Q3: In Sec 3.1: \"BPPO leads to a mismatch ... due to the presence of diverse behavior policies in the dataset D\", could authors explain further why the diversity is blamed for the mismatch?**\n\n**A3:** One example that can illustrate this motivation is the presence of multi-modality within a diverse dataset. In other words, the dataset exhibits multiple modes, with the main mode primarily composed of low-return actions. Conversely, the subdominant mode consists of low-density but high-return actions. In such a scenario, standard behavior cloning (BC) is susceptible to imitating the high-density but low-return actions, resulting in a bias towards fitting the main mode. However, during the offline multi-step optimization stage, the policy optimization is constrained by the clip function, making it difficult for the policy to escape this mode. Consequently, this can lead to a sub-optimal policy as it becomes unable to explore the high-return action region.\n\nIn contrast, our ensemble BC approach learns diverse behavior policies that are more likely to cover all modes present in the dataset. This facilitates exploration of the high-return region, enabling the discovery of the optimal policy. To support this claim, we have included a motivating example and conducted additional ablation studies in **Appendix A.5**. These findings demonstrate that the utilization of multiple diverse behavior policies allows for effective exploration of the high-return action region, particularly in the case of the \"medium-replay\" dataset, which contains more sub-optimal data than others. \n\n**Q4: Lack of theoretical analysis (to support the motivation of technique details), but it has sufficient experiments thus this point is acceptable I think.**\n\n**A4:** Thank you for this suggestion. We have reevaluated the approximation employed throughout the entire pipeline and included a thorough analysis to address this concern. During the multi-step policy optimization stage, the decision for behavior policy replacement is based on the proposed AM-Q metric, which incorporates the true transition model $T$ and optimal value function $Q^*$. In the offline setting, however, the agent does not have access to the true model and $Q^*$. But $Q_{\\tau}$ approaches $Q^*$ based on the dataset support constraints. We fit $\\hat{T}$ and $\\widehat{Q_{\\tau}}$ by gradient-based optimization. In this way, the practical AM-Q can be expressed as $\\widehat{J_{\\tau}}(\\pi)$. Thus, the OPE bias is mainly coming from the transition model approximation. We have analyzed and derived the bound of the offline policy evaluation, described by Theorem 2 in Section 3.2, please refer to check out the detail.\n\nBPPO [2] have derived the offline monotonical improvement bound (their Theorem 3) between the target policy and behavior policy, i.e., $\\pi_{k+1}$ and $\\pi_k$. However, since the behavior policy is updated iteratively, this bound can result in cumulative errors that disrupt the desired monotonicity. Consequently, this bound cannot be directly utilized in the offline setting. This limitation is the reason why BPPO requires online evaluation to ensure performance improvement when replacing the behavior policy with the target policy. Given the OPE bound, we can replace the online evaluation with AM-Q to guarantee monotonicity.\n\nAdditionally, we compute the advantage function by $(\\widehat{Q_{\\tau}}-\\widehat{V_{\\tau}})$, which provides an approximation of the optimal $Q^*$ and $V^*$ based on the dataset constraint assumption. In **Appendix A.8**, we conduct experiments to demonstrate this replacement is a superior choice compared to iteratively fitting the value function of the target policy in the offline setting. The results from these experiments prove that iteratively updating the value function will lead to overestimation which causes unstable training performance and potential crashes. \n\n**Q5: Suggest to add legends for Fig. 3 or bringing the legend in Fig. 4 forward.**\n\n**A5:** Thank you for pointing this out. We have addressed this issue in the updated manuscript.\n\n[1] Kostrikov, I., Nair, A., & Levine, S. (2021). Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169.\n\n[2] Zhuang Z, Lei K, Liu J, et al. Behavior proximal policy optimization[J]. arXiv preprint arXiv:2302.11312, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499296023,
                "cdate": 1700499296023,
                "tmdate": 1700499296023,
                "mdate": 1700499296023,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4k9tHYi3qt",
                "forum": "tbFBh3LMKi",
                "replyto": "JuypvJ6EhK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4914/Reviewer_CZhd"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4914/Reviewers",
                    "ICLR.cc/2024/Conference/Submission4914/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission4914/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4914/Reviewer_CZhd"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for authors' explanation"
                    },
                    "comment": {
                        "value": "Thank you for this careful feedback, which addressed most of my concerns. After reading all responses to me and other reviewers,  I am delighted to raise my score from 6 to 8."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548138547,
                "cdate": 1700548138547,
                "tmdate": 1700548138547,
                "mdate": 1700548138547,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]