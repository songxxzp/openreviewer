[
    {
        "title": "An Instance-Level Framework for Multi-tasking Graph Self-Supervised Learning"
    },
    {
        "review": {
            "id": "V3QnNkBUSh",
            "forum": "D3JpYSn7dL",
            "replyto": "D3JpYSn7dL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5122/Reviewer_YwHq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5122/Reviewer_YwHq"
            ],
            "content": {
                "summary": {
                    "value": "This paper designs a multi-tasking graph self-supervised learning framework to improve downstream graph learning performance. Different from the conventional multi-tasking SSL, it proposes a multi-teacher KD framework with an instance-level knowledge integration module, with the parameterized knowledge distillation module, which can output instance-wise knowledge combination weights. The proposed approach suggests a significant performance improvement over other single-pretext tasks, e.g., vanilla MTL, AutoSSL, and ParetoGNN."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well-written and provides sufficient empirical observations of combining multiple self-supervised tasks.\n2. The experiment for node classification is conducted over a broad datasets and the improvement is significant.\n3. Some theoretic results can be directly supported by the empirical results.\n4. The proposed method with weighing different teachers for each node is novel."
                },
                "weaknesses": {
                    "value": "1. The main concern is the experiments in this paper are only related to the classification task in a multi-tasking setting, while the proposed method is task-irrelevant. Although the some vision tasks are involved in the appendix, the graph-related tasks, such as node clustering and link prediction, should be investigated.\n2. The Eq. (4) is quite misleading. The proposed distillation is in an offline fashion, and the optimality of teacher has already in the objective. As a result, the constraint is a little bit unnecessary. Also, the parameters for optimization are not clearly presented in the objective."
                },
                "questions": {
                    "value": "1. In page 2, you claim \u201cSecondly, balancing multiple tasks by loss weighting during the pre-training phase makes it hard to scale the pre-trained model to emerging tasks. To incorporate new tasks, it requires to re-pretrain a new model from scratch.\u201d Is it an over- pessimistic claim for the existing methods? Which may implicit exaggerate the contribution of the proposed method.\n2. How do you perform vision task in Appendix 10? Please provide more details for the graph construction. Maybe you can discuss the performance difference between different methods to build the graph.\n3. Is there more evidence for Figure 4? From Guidance 1, the integrated teacher probability is as close as possible to the true Bayesian probability. What is the specific reason for the failure of the other two schemes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5122/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5122/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5122/Reviewer_YwHq"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5122/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698572606295,
            "cdate": 1698572606295,
            "tmdate": 1699636504433,
            "mdate": 1699636504433,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H3fEVfhKC4",
                "forum": "D3JpYSn7dL",
                "replyto": "V3QnNkBUSh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5122/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YwHq"
                    },
                    "comment": {
                        "value": "Thanks for your insightful comments and constructive suggestions! We mark the contents that already existed (but were missed by reviewers) in red, and those revised or newly added contents in blue in the revised manuscript. Next, we will address the concerns you raised, one by one, as described below:\n\n---\n\n**Q1**: The main concern is the experiments in this paper are only related to the classification task in a multi-tasking setting, while the proposed method is task-irrelevant. Although the some vision tasks are involved in the appendix, the graph-related tasks, such as node clustering and link prediction, should be investigated.\n\n**A1**: We have added performance comparisons of MGSSL, AutoSSL, and ParetoGNN on various graph-related downstream tasks (graph regression, node clustering, link prediction) in **Tables A2/A3/A4** and **Lines 642-654** in **Appendix A.10**. These experiments demonstrate the applicability of MGSSL to various downstream tasks and its superiority over AutoSSL and ParetoGNN.\n\n---\n\n**Q2**: The Eq. (4) is quite misleading. The proposed distillation is in an offline fashion, and the optimality of teacher has already in the objective. As a result, the constraint is a little bit unnecessary. Also, the parameters for optimization are not clearly presented in the objective.\n\n**A2**: The parameters to be optimized in Eq. (4) during KD are the student model ($\\{\\theta, \\omega\\}$) and the weighting function $\\lambda_\\gamma(k,i)$ (parameterized by $\\gamma$). Therefore, although each teacher model is frozen before KD, the integrated teacher (the optimality of teacher) $\\sum_{k=1}^K\\lambda_\\gamma(k,i) \\widetilde{\\mathbf{h}}\\_i^{(k)}$ changes as $\\lambda_\\gamma(k,i)$ is updated during KD. As a result, Eq. (4) is essentially more like an online KD, and perhaps we have not made it clear what the parameters of optimization are, leading you to misinterpret it as \"an offline fashion\". To make it clear, we have added the above explanation to **Lines 127-130** of the revised manuscript.\n\n---\n\n**Q3**: In page 2, you claim \u201cSecondly, balancing multiple tasks by loss weighting during the pre-training phase makes it hard to scale the pre-trained model to emerging tasks. To incorporate new tasks, it requires to re-pretrain a new model from scratch.\u201d Is it an over- pessimistic claim for the existing methods? Which may implicit exaggerate the contribution of the proposed method.\n\n**A3**: Thanks for your insightful comment. We have modified the previous expression (somewhat over-pessimistic),\n\n> \"To incorporate new tasks, it requires to re-pretrain a new model from scratch.\"\n> \n\nto a more moderate expression,\n\n> \"To incorporate new tasks, it requires to re-weight the losses of new tasks and existing tasks to re-pretrain the model.\" in the revised manuscript.\u201d\n>\n\n---\n\n**Q4**: How do you perform vision task in Appendix 10? Please provide more details for the graph construction. Maybe you can discuss the performance difference between different methods to build the graph.\n\n**A4**: To adapt MGSSL to vision tasks, we conduct graph construction by taking images as nodes and connecting the k-Nearest Neighbors (kNN) of each image to build edges. As you suggested, we have also provided the results of constructing the graph by thresholding in **Table A5** of **Appendix A.10**, where two images with cosine similarity greater than 0.7 will be connected by an edge. The results show that constructing the graph by kNN outperforms thresholding, and we speculate that this is because kNN guarantees the balance of node degrees in the constructed graph and prevents the over-squeezing problem that is common in graph learning. We have added the above explanation to **Lines 659-669** of the revised manuscript.\n\n---\n\n**Q5**: Is there more evidence for Figure 4? From Guidance 1, the integrated teacher probability is as close as possible to the true Bayesian probability. What is the specific reason for the failure of the other two schemes?\n\n**A5**: Among the three schemes in Figure 4, only \"Adaptive (ours)\" follows Guideline 1, while \"Averaged\" and \"Weighted\" are two heuristic (vanilla) schemes that have nothing to do with Guideline 1. The reasons for the failure of \"Averaged\" and \"Weighted\" are that they cannot differentiate important teachers from irrelevant ones, and may mislead the student in the presence of low-quality teachers, as the manuscript described. By contrast, \"Adaptive (ours)\" follows Guideline 1, which enables the integrated teacher probability to be as close as possible to the true Bayesian probability by adaptive teacher weighting. To make it clear, we have added the above explanation to **Lines 186-193** of the revised manuscript.\n\n--- \n\nIn light of these responses, we hope we have addressed your concerns. If we have left any notable points of concern unaddressed, please do share and we will attend to these points. We sincerely hope that you can appreciate our efforts on responses and revisions and raise your score."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699943532144,
                "cdate": 1699943532144,
                "tmdate": 1700017546788,
                "mdate": 1700017546788,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ajnKKgQHM4",
                "forum": "D3JpYSn7dL",
                "replyto": "V3QnNkBUSh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5122/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your insightful and helpful comments once again. We greatly appreciate your feedback. We have carefully responded to each of your questions point-by-point.\n\nUnlike previous years, there will be no second stage of author-reviewer discussions this year. As the deadline for the discussion is approaching, we kindly request you to inform us if you have any additional questions.\n\nBest regards,\n\nAuthors."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527595929,
                "cdate": 1700527595929,
                "tmdate": 1700527595929,
                "mdate": 1700527595929,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ifo6XqXn27",
                "forum": "D3JpYSn7dL",
                "replyto": "ajnKKgQHM4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5122/Reviewer_YwHq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5122/Reviewer_YwHq"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. The final decision will be released soon, after the discussion with other reviewers and AC."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637656784,
                "cdate": 1700637656784,
                "tmdate": 1700637656784,
                "mdate": 1700637656784,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3OVFJzkR0s",
                "forum": "D3JpYSn7dL",
                "replyto": "V3QnNkBUSh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5122/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks again for your insightful review and keeping a positive score for acceptance. If there is anything else that needs clarification, please don't hesitate to contact us!\n\nBest regards, Authors."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654604834,
                "cdate": 1700654604834,
                "tmdate": 1700654615524,
                "mdate": 1700654615524,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oZB165S2BX",
            "forum": "D3JpYSn7dL",
            "replyto": "D3JpYSn7dL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5122/Reviewer_UNJF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5122/Reviewer_UNJF"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents the MGSSL, which is a a novel multi-teacher knowledge distillation framework for instance-level Multi-tasking Graph SSL. This framework conducts knowledge integration in the fine-tuning phase, distilling the knowledge from multi-teacher models to the student models, which achieves an instance-level integration module. The authors provided sufficient theory to prove this framework can benefit from a wider range of teachers, and the experimental part also achieved competitive results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "My opinions towards the strengths of this papers are:\n   + Originality. This paper proposes the idea of integrating knowledge in the fine-tuning phase, and thus achieved a breakthrough at the instance-level requirements for the first time.\n   + Quality. This paper has clear method description, sufficient experiments, and competitive results.\n   + Clarity. The figures of the methods plus theoretical proof clearly expressed the method of the paper. \n   + Significance. A framework theoretical guideline brings new ideas to solve Graph Self-Supervised Learning problems."
                },
                "weaknesses": {
                    "value": "First of all, the authors say that they integrate knowledge from multiple teacher models to the student models. I wonder what knowledge is integrated into the student models. I think the knowledge may be the common knowledge from the multiple teacher models. The authors may present more analysis. \n\nSecond, I find the paper not easy to follow. The symbols maybe complex for me to understand.  As a result, I suggest the authors to simplify and well structure the used symbols."
                },
                "questions": {
                    "value": "My two concerns are as the Weaknesses part. Pls refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5122/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698721567304,
            "cdate": 1698721567304,
            "tmdate": 1699636504330,
            "mdate": 1699636504330,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jKHbPomyC0",
                "forum": "D3JpYSn7dL",
                "replyto": "oZB165S2BX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5122/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UNJF"
                    },
                    "comment": {
                        "value": "Thanks for your insightful comments and constructive suggestions! We mark the contents that already existed (but were missed by reviewers) in red, and those revised or newly added contents in blue in the revised manuscript. Next, we will address the concerns you raised, one by one, as described below:\n\n---\n\n**Q1**: I wonder what knowledge is integrated into the student models. I think the knowledge may be the common knowledge from the multiple teacher models. The authors may present more analysis.\n\n**A1**: We follow previous work [1] in decomposing knowledge into high- and low- frequency components, which are measured by cosine similarity and KL-divergence, respectively (see Appendix D of [1] for details on how to measure high/low frequency knowledge). We provide a comparison of high- and low-frequency knowledge for the five teacher and student models in **Figure A2** and **Lines 711-721** of **Appendix A.16**, from which it can be observed that (1) low-frequency knowledge (a.k.a., common knowledge) from multiple teachers can be well learned by the student, and (2) the student model differently learn high-frequency knowledge from each teacher.\n\n[1] Wu L, Lin H, et al. Extracting Low-/High-Frequency Knowledge from Graph Neural Networks and Injecting it into MLPs: An Effective GNN-to-MLP Distillation Framework[J]. AAAI, 2023.\n\n---\n\n**Q2**: I find the paper not easy to follow. The symbols maybe complex for me to understand. As a result, I suggest the authors to simplify and well structure the used symbols.\n\n**A2**: Thanks for your constructive suggestions. We have simplified and reorganized the symbols used in this paper and provided a list of symbols in **Table A10** of **Appendix A.17**. We hope this helps you understand and appreciate our contributions. If anything is still unclear, please do not hesitate to contact us.\n\n---\n\nIn light of these responses, we hope we have addressed your concerns. If we have left any notable points of concern unaddressed, please do share and we will attend to these points. We sincerely hope that you can appreciate our efforts on responses and revisions and thus raise your score."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699943054187,
                "cdate": 1699943054187,
                "tmdate": 1700017497960,
                "mdate": 1700017497960,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UMNsLHGcxg",
                "forum": "D3JpYSn7dL",
                "replyto": "oZB165S2BX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5122/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your insightful and helpful comments once again. We greatly appreciate your feedback. We have carefully responded to each of your questions point-by-point.\n\nUnlike previous years, there will be no second stage of author-reviewer discussions this year. As the deadline for the discussion is approaching, we kindly request you to inform us if you have any additional questions.\n\nBest regards,\n\nAuthors."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527584483,
                "cdate": 1700527584483,
                "tmdate": 1700527584483,
                "mdate": 1700527584483,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "E5E7wyGnpf",
            "forum": "D3JpYSn7dL",
            "replyto": "D3JpYSn7dL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5122/Reviewer_RhiN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5122/Reviewer_RhiN"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel approach in graph self-supervised learning called Multi-teacher Knowledge Distillation Framework for Instance-level Multitasking Graph Self-Supervised Learning (MGSSL). MGSSL uses multiple teachers for \ndifferent tasks, integrating their knowledge for each instance, and distills it into a student model. This method shifts from loss weighting in pre-training to knowledge integration in fine-tuning, allowing compatibility with numerous tasks without retraining from scratch. Theoretical justifications are provided, and extensive experiments show that MGSSL's performance is comparable to state-of-the-art methods by combining simple classical tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The proposed multi-teacher knowledge distillation framework for instance-level multitasking graph self-supervised learning (MGSSL) is a novel approach. It addresses the limitations of existing methods by focusing on instance-level requirements and scalability to new tasks.\n\n+ The paper successfully identifies and tackles key challenges in the field: the need for localized combinations of tasks for different instances, poor scalability to emerging tasks, and the lack of theoretical guarantees for performance improvement with more tasks.\n\n+ Providing theoretical justification for the potential of MGSSL to benefit from a wider range of teachers (tasks) adds credibility and depth to  the research.\n\n+ The extensive experiments and the resulting performance being comparable to state-of-the-art competitors lend strong empirical support to the proposed method."
                },
                "weaknesses": {
                    "value": "- An evaluation of the scalability and computational costs of the proposed framework, especially in large-scale settings, would be a valuable addition.\n\n- The author only compares their work with AutoSSL in Table 1. Since AutoSSL is a work from 2021, I suggest that the author should compare their work with more recent studies. This would further enhance the quality of the paper."
                },
                "questions": {
                    "value": "Please see my comments in Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5122/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698926771901,
            "cdate": 1698926771901,
            "tmdate": 1699636504242,
            "mdate": 1699636504242,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SGDrDdehGL",
                "forum": "D3JpYSn7dL",
                "replyto": "E5E7wyGnpf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5122/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RhiN"
                    },
                    "comment": {
                        "value": "Thanks for your insightful comments and constructive suggestions! We mark the contents that already existed (but were missed by reviewers) in red, and those revised or newly added contents in blue in the revised manuscript. Next, we will address the concerns you raised, one by one, as described below:\n\n---\n\n**Q1**: An evaluation of the scalability and computational costs of the proposed framework, especially in large-scale settings, would be a valuable addition.\n\n**A1**: The discussion and analysis on the computational complexity and overhead of MGSSL has been provided in **Lines 694-709**, **Appendix A.15** of the original manuscript. Did you miss this part of the experiment? Following your suggestion, we have added the computational cost of MGSSL on a larger ogbn-products dataset (consisting of 2,449,029 nodes and 61,859,140 edges) to demonstrate its excellent scalability in **Table A9** in the revised manuscript.\n\n---\n\n**Q2**: .The author only compares their work with AutoSSL in Table 1. Since AutoSSL is a work from 2021, I suggest that the author should compare their work with more recent studies. This would further enhance the quality of the paper.\n\n**A2**: One latest work, ParetoGNN, published in ICLR'2023, has been included in the comparison in **Table 1**.  Additionally, we provide more comparisons of MGSSL with AutoSSL and ParetoGNN on various graph-related tasks in **Tables A2/A3/A4** and **Lines 642-654** in **Appendix A.10**.\n\n---\n\nIn light of these responses, we hope we have addressed your concerns. If we have left any notable points of concern unaddressed, please do share and we will attend to these points. We sincerely hope that you can appreciate our efforts on responses and revisions and thus raise your score."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699942827618,
                "cdate": 1699942827618,
                "tmdate": 1700017480440,
                "mdate": 1700017480440,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LuwlAC40La",
                "forum": "D3JpYSn7dL",
                "replyto": "E5E7wyGnpf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5122/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your insightful and helpful comments once again. We greatly appreciate your feedback. We have carefully responded to each of your questions point-by-point.\n\nUnlike previous years, there will be no second stage of author-reviewer discussions this year. As the deadline for the discussion is approaching, we kindly request you to inform us if you have any additional questions.\n\nBest regards,\n\nAuthors."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527572206,
                "cdate": 1700527572206,
                "tmdate": 1700527572206,
                "mdate": 1700527572206,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wHeFUqG7gg",
            "forum": "D3JpYSn7dL",
            "replyto": "D3JpYSn7dL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5122/Reviewer_xyJe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5122/Reviewer_xyJe"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the problem of graph self-supervised learning and presents a multi-teacher knowledge distillation framework. Specifically, it trains multiple teachers with different pretext tasks, then integrates the knowledge of different teachers for each instance separately by two parameterized knowledge integration schemes (MGSSL-TS and MGSSL-LF), and finally distills it into the student model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper is clearly motivated and well-written.\n2.\tTheoretical analysis is provided to show that the proposed method has the potential to benefit from more teachers."
                },
                "weaknesses": {
                    "value": "1.\tExperiments on other downstream tasks (e.g. node clustering, link prediction, and partition prediction as in AutoSSL and ParetoGNN) are missing.\n2.\tIt seems better to add the discussion and comparison on related works about graph knowledge distillation [abc] and recent self-supervised graph learning [def].\na.\tQuantifying the Knowledge in GNNs for Reliable Distillation into MLP, ICML2023.\nb.\tExtracting Low-/High- Frequency Knowledge from Graph Neural Networks and Injecting it into MLPs: An Effective GNN-to-MLP Distillation Framework, AAAI2023.\nc.\tKnowledge Distillation Improves Graph Structure Augmentation for Graph Neural Networks, NeurIPS2022.\nd.\tDecoupled Self-supervised Learning for Graphs, NeurIPS2022.\ne.\tGraph Self-Supervised Learning with Accurate Discrepancy Learning, NeurIPS2022.\nf.\tGraphMAE: Self-Supervised Masked Graph Autoencoders, KDD2022.\n3.\tTypo: In the line2 of page6, \u201c\u2026this paper proposes two\u2026\u201d is miswritten as \u201c\u2026this paper propose stwo\u2026\u201d."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5122/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698936418593,
            "cdate": 1698936418593,
            "tmdate": 1699636504118,
            "mdate": 1699636504118,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Pz6pJ3GUNq",
                "forum": "D3JpYSn7dL",
                "replyto": "wHeFUqG7gg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5122/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xyJe"
                    },
                    "comment": {
                        "value": "Thanks for your insightful comments and constructive suggestions! We mark the contents that already existed (but were missed by reviewers) in red, and those revised or newly added contents in blue in the revised manuscript. Next, we will address the concerns you raised, one by one, as described below:\n\n---\n\n**Q1**: Experiments on other downstream tasks (e.g. node clustering, link prediction, and partition prediction as in AutoSSL and ParetoGNN) are missing.\n\n**A1**: We have added performance comparisons of MGSSL, AutoSSL, and ParetoGNN on various graph-related downstream tasks (graph regression, node clustering, link prediction) in **Tables A2/A3/A4** and **Lines 642-654** in **Appendix A.10**. These experiments demonstrate the applicability of MGSSL to various downstream tasks and its superiority over AutoSSL and ParetoGNN.\n\n---\n\n**Q2**: It seems better to add the discussion and comparison on related works about graph knowledge distillation and recent self-supervised graph learning.\n\n**A2**: Thanks for your valuable suggestion. We have added the discussion on graph knowledge distillation and recent graph self-supervised learning in **Lines 591-602** and **Lines 626-640** of **Appendix A.9**, and the six papers you mentioned are all included, cited, and discussed. Moreover, we have included GraphMAE as an additional baseline in **Table 2**.\n\n---\n\n**Q3**: Typo: In the line2 of page6, \u201c\u2026this paper proposes two\u2026\u201d is miswritten as \u201c\u2026this paper propose stwo\u2026\u201d.\n\n**A3**: All typos have been corrected in the revised manuscript.\n\n--- \n\nIn light of these responses, we hope we have addressed your concerns. If we have left any notable points of concern unaddressed, please do share and we will attend to these points. We sincerely hope that you can appreciate our efforts on responses and revisions and thus raise your score."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699942523431,
                "cdate": 1699942523431,
                "tmdate": 1700017461781,
                "mdate": 1700017461781,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rP3DGMZ0Y1",
                "forum": "D3JpYSn7dL",
                "replyto": "wHeFUqG7gg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5122/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your insightful and helpful comments once again. We greatly appreciate your feedback. We have carefully responded to each of your questions point-by-point.\n\nUnlike previous years, there will be no second stage of author-reviewer discussions this year. As the deadline for the discussion is approaching, we kindly request you to inform us if you have any additional questions.\n\nBest regards,\n\nAuthors."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527555589,
                "cdate": 1700527555589,
                "tmdate": 1700527555589,
                "mdate": 1700527555589,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]