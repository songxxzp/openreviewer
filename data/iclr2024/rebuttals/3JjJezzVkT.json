[
    {
        "title": "The Marginal Value of Momentum for Small Learning Rate SGD"
    },
    {
        "review": {
            "id": "1YL3KsX9N3",
            "forum": "3JjJezzVkT",
            "replyto": "3JjJezzVkT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6171/Reviewer_EHVB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6171/Reviewer_EHVB"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the role of momentum in SGD training. When the learning rate is small but the gradient noise is not small, the paper shows that the trajectories of SGD and SGDM are close. Therefore, the momentum component in SGDM adds only a small effect on top of SGD. This paper also conducts experiments to verify that the optimization and generalization of SGDM and SGD are close when the learning rate is small and the mini-batch size is not large (such that the gradient noise is not small)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Theoretically, momentum provably improves the (minimax) convergence rate of GD for (strongly convex) smooth optimization. Practically, momentum is often used with SGD (as GD is expensive in practical machine learning optimization). In the presence of gradient noise, will momentum still offer benefits? This is the central question studied in this work. \n\nFirst, this work presents a theory, showing that when the learning rate tends to zero, the trajectories of SGD and SGDM are close in two regimes. The first regime is when the gradient noise is large (with $O(1/\\eta)$ variance) but the total running distance is constant. The second regime allows the total running steps to be large but requires an overparameterized model where the global solutions form a manifold. \n\nSecond, this work conducts experiments to verify that for both training and test, SGDM and SGD are close when the learning rate is small and when the batch size is not too large. \n\nThe writing is clear overall. Some places require clarifications. See more in the next section. \n\nI did not check the proof."
                },
                "weaknesses": {
                    "value": "1. I feel the introduction/motivation could use some polishment. Specifically, the benefit of momentum is for stabilizing GD with a large learning rate ($>2/L$, in terms of improving the minimax convergence rate for (strongly convex) smooth optimization) as discussed in the first paragraph of the introduction. However, I did not recall a theoretical result that claims acceleration of momentum for GD with a learning rate. \n\nOne of the main contributions of this paper is to show that momentum does not change SGD trajectory much when the learning rate is sufficiently small. Therefore, momentum is not very helpful. \n\nI am not sure how surprising this is, since the benefit of momentum is only claimed for GD with a very large learning rate. \n\nAlso, the condition that $1-\\\\beta\\^{(n)}_k = \\\\Theta ((\\\\eta^{(n)})\\^{\\\\alpha} )$ (in definition 3.1) for $\\alpha \\in [0,1)$ (in Theorem 3.5) basically says that the history gradient will be forgotten fast (in a nearly exponential rate) when SGDM runs $\\\\Theta(1/\\\\eta)$ steps. So it's not very surprising that SGDM will be close to SGD. \n\n\n2. In the last paragraph of Section 3, the paper briefly mentions a phenomenon for $\\\\alpha>1$. However, the theorem only covers $\\\\alpha <1$. Could the author elaborate on what happens to SGDM when $\\\\alpha>1$?\n\n\n3. In the paragraph after Lemma 2.4, $\\\\sigma$ is set to be $1/\\sqrt{\\eta}$ to ensure a non-trivial gradient noise scaling. However, later in Theorem 3.5, it allows that $\\\\sigma \\le \\eta^{-1/2}$, and in Theorem 4.5, it allows $\\\\alpha = \\\\Theta(1)$. These seem to be different from the prior discussions. Could the authors clarify the scaling of the noise?  In particular, can Theorem 3.5 be applied to GD and GDM (i.e., $\\\\sigma = 0$)?\n\n\n4. This paper mentions two versions of SGDM, that is, eq (1) used in practice and eq (3) used in theory. I appreciate the usage of $\\\\gamma$ and $\\\\eta$ to differentiate the learning rate for those two versions, which helps highlight the difference in the effective optimization length. \n\nIn the ImageNet experiment, the learning rate for eq (1) is manually rescaled when compared to SGD (to align with the theory setup in eq (3)), which is good. However, for training ResNet-32 on CIFAR-10, the paper writes that \"We first grid search to find the best learning\nrate for the standard SGDM ($\\\\ell=1$), and then we perform SGD and SGDM with that same learning rate for different levels of $\\\\ell$\". I am wondering if the learning rate for SGD needs a rescaling here."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6171/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6171/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6171/Reviewer_EHVB"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6171/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698448608651,
            "cdate": 1698448608651,
            "tmdate": 1699636670592,
            "mdate": 1699636670592,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "p5IWFMQYao",
                "forum": "3JjJezzVkT",
                "replyto": "1YL3KsX9N3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6171/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6171/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments. \n\n---\n\n**Q1:** I did not recall a theoretical result that claims acceleration of momentum for GD with a learning rate.\n\n**A1:** The result quoted by the introduction comes from [1] (Theorem 9). Specifically it shows that the heavy-ball momentum with optimal hyperparameter constants enjoys a faster convergence rate than GD in the neighborhood of a local minimizer, and when the hessian is ill-conditioned the optimal learning rate with momentum can exceed $2/L$.\n\n- [1] B.T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1\u201317, 1964. ISSN 0041-5553.\n\n---\n\n**Q2:** Is it surprising that momentum does not help when the LR is sufficiently small? The benefit of momentum over standard GD has only been claimed in the large LR regime\n\n**A2:** There are works that claim the role of momentum (for instance [2]) for GD in terms of the implicit regularization in the setting where the LR is sufficiently small, and we are writing to further show that when there are enough gradient noises, then adding momentum does not help implicit regularization. \n\n- [2] Avrajit Ghosh, He Lyu, Xitong Zhang, and Rongrong Wang. Implicit regularization in heavyball momentum accelerated stochastic gradient descent. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= ZzdBhtEH9yB\n\n---\n\n**Q3:** The condition on $1-\\beta$ in Definition 3.1 says that the past gradient will be quickly forgotten, so it is not surprising that SGDM is close to SGD in that case. What happens when $\\alpha > 1$?\n\n**A3:** When $\\alpha>1$, there will be no limiting dynamics as $\\eta\\to 0$.  For any non-zero initial momentum, the parameter will follow the initial momentum up to the scale $O(\\eta^{1-\\alpha})$, so when $\\eta$ is small enough the trajectory will tend to explode as the parameters go unbounded. In general when $\\alpha>1$, it takes longer time for the momentum to adapt to local curvature than for the parameter to go out of local areas, therefore conceptually such a momentum choice makes optimization uncontrollable.\n \nEven when the initial momentum is zero and the initial point is near a strongly-convex local minimizer, the asymptotic convergence rate will still be $O(1/\\eta^{(1+\\alpha)/2})$ for $\\alpha>1$, strictly slower than the rate $O(1/\\eta)$ for the case when $\\alpha<1$, which is not preferred for optimization.\n\nEmpirically, setting $\\alpha>1$ at LR annealing also produces very bad performance. We believe the above are the reasons.\n\n---\n\n**Q4:** How does the scaling of the noise work? Lemma 2.4 and Theorem 3.5 appear to contradict. Can Theorem 3.5 be applied to full-batch GD with and without momentum?\n\n**A4:** Yes Theorem 3.5 can be applied to full-batch GD where $\\sigma = 0$. We write $\\sigma\\leq \\eta^{-1/2}$ to make the theorem more general, but the most interesting case is $\\sigma= \\eta^{-1/2}$. As Lemma 2.4 shows, when $\\sigma= \\eta^{-1/2}$, the gradient noise has non-negligible impact on the loss curve, while when $\\sigma\\ll \\eta^{-1/2}$, the impact of the gradient noise is negligible and SGD will have a similar trajectory as GD.\n\nWe realized that the paragraph after Lemma 2.4 may cause confusion, as our results actually hold for a range of $\\sigma$ as stated in the theorems. We will fix the confusion in the revision, and thank you for the suggestion.\n\n---\n\n**Q5:** Does the learning rate for SGD need a rescaling in the CIFAR-10 experiments?\n\n**A5:** Yes in all the experiments the learning rate needs to be rescaled so that $\\eta$ matches for SGD and SGDM (or $\\eta = \\gamma/(1-\\beta)$). We will add that into the experiment details in the revision."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6171/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541093407,
                "cdate": 1700541093407,
                "tmdate": 1700541093407,
                "mdate": 1700541093407,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5erYPzx51E",
                "forum": "3JjJezzVkT",
                "replyto": "p5IWFMQYao",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6171/Reviewer_EHVB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6171/Reviewer_EHVB"
                ],
                "content": {
                    "title": {
                        "value": "Follow up questions"
                    },
                    "comment": {
                        "value": "Thank you for the comments. \n\nQ1. I was trying to say that \"I did not recall a theoretical result that claims acceleration of momentum for GD with a **small** learning rate.\" Sorry for missing \"small\" in my initial comment. \n\nAs the acceleration of momentum is only justified for GD with a large learning rate ($>2/L$), I find it not surprising that momentum does not contribute when the learning rate is small. \n\n\nQ2. Thank you for pointing out [2]. As Theorem 3.5 can be applied to GD by taking $\\\\sigma\\^2 = 0$, would Theorem 3.5 contradict the message in [2] in this case? Have I misunderstood anything?  \n\n\nQ5. Could you please clarify whether or not the learning rate has been properly rescaled in the current experiments? Would some of the current experiments need to be redone for a fair comparison?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6171/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688607325,
                "cdate": 1700688607325,
                "tmdate": 1700688607325,
                "mdate": 1700688607325,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "34sWWRbcBa",
            "forum": "3JjJezzVkT",
            "replyto": "3JjJezzVkT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6171/Reviewer_fSFX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6171/Reviewer_fSFX"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to investigate the advantages of incorporating momentum in the Stochastic Gradient Descent with Momentum (SGDM) method for mitigating the variance associated with stochastic gradients. To mitigate oscillations along the high-curvature direction, the authors make the assumption that the learning rate is small. Within this context, they demonstrate that, subject to specific assumptions, Stochastic Gradient Descent (SGD) and SGDM yield identical training error results. Additionally, similar to previous studies, they construct a Stochastic Differential Equation (SDE) for the continuous version of SGDM/SGD and, through an analysis of the SDE's limiting behavior, reveal that both SGD and SGDM exhibit a similar implicit bias."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Prior research has noted that the advantage of incorporating momentum in the training of neural networks is somewhat limited, and there hasn't been a definitive theoretical outcome regarding the effectiveness of momentum in the stochastic setting. Therefore, demonstrating that momentum does not significantly reduce variance or improve generalization can lead to savings in computational resources and memory usage during deep neural network training. A noteworthy technical innovation in this work is the introduction of a novel Stochastic Differential Equation (SDE) to represent the diminishing learning rate in Stochastic Gradient Descent with Momentum (SGDM) and Stochastic Gradient Descent (SGD)."
                },
                "weaknesses": {
                    "value": "The assumptions that the paper considers to show that SGDM approximate SGD is too restrictive. It would be great if they can justify these too restrictive assumptions."
                },
                "questions": {
                    "value": "In Theorem 3.5, the paper demonstrates that the trajectory of Stochastic Gradient Descent with Momentum (SGDM) approximates that of Stochastic Gradient Descent (SGD) with a specific learning rate. While it may be somewhat expected that these two methods, both being first-order optimization techniques, would exhibit some similarity, the importance of this result lies in its ability to establish a specific condition under which SGDM behaves similarly to SGD.\n\nHowever, as you rightly suggest, a more generalized result would be valuable. If the paper could establish conditions where, for certain small learning rates, SGDM and SGD consistently approximate each other, it would offer broader insights into the relationship between these methods and provide more guidance on when to expect their similarities. This would enhance our understanding of the interplay between learning rates and optimization algorithms, potentially leading to more informed choices in practical applications."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6171/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723864075,
            "cdate": 1698723864075,
            "tmdate": 1699636670477,
            "mdate": 1699636670477,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rWoVhH23sM",
                "forum": "3JjJezzVkT",
                "replyto": "34sWWRbcBa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6171/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6171/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments.\n\n---\n\n**Q1:** The assumptions that the paper considers to show that SGDM approximate SGD is too restrictive. It would be great if they can justify these too restrictive assumptions.\n\n**A1:** The assumptions we make are standard from a series prior works [1-4] on the trajectory analysis of stochastic updates, and theoretical findings from those works have been predictive of complex real-world settings (e.g., training vision models and fine-tuning language models, see [2,5] for instance). \n\nSpecifically, for training smooth loss on a neural network with smooth activations (Swish, sigmoid, GeLU etc) and weight decay, our assumptions will hold. In practical settings, all the data points and networks weights are actually mathematically bounded for a fixed number of steps, and the corresponding assumptions mostly serve for mathematical rigor.\n\nReferences:\n- [1] Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations. Journal of Machine Learning Research, 20(40): 1\u201347, 2019.\n- [2] Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling SGD with stochastic differential equations (SDEs). Advances in Neural Information Processing Systems, 34:12712\u2013 12725, 2021a.\n- [3] Li, Zhiyuan, Tianhao Wang, and Sanjeev Arora. \"What Happens after SGD Reaches Zero Loss?--A Mathematical Framework.\" In International Conference on Learning Representations. 2021.\n- [4] Sadhika Malladi, Kaifeng Lyu, Abhishek Panigrahi, and Sanjeev Arora. On the SDEs and scaling rules for adaptive gradient algorithms. Advances in Neural Information Processing Systems, 2022.\n- [5]  Elkabetz, Omer, and Nadav Cohen. \"Continuous vs. discrete optimization of deep neural networks.\" Advances in Neural Information Processing Systems 34 (2021): 4947-4960. Blog http://www.offconvex.org/2022/01/06/gf-gd/."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6171/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700537615672,
                "cdate": 1700537615672,
                "tmdate": 1700537690733,
                "mdate": 1700537690733,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "14S0jqWAuv",
            "forum": "3JjJezzVkT",
            "replyto": "3JjJezzVkT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6171/Reviewer_RZuq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6171/Reviewer_RZuq"
            ],
            "content": {
                "summary": {
                    "value": "This work compares SGDM and SGD in the setting of small learning rate and gradient noise is large enough to produce instability. Two main results are presented, both indicating that the training trajectory of SDGM is close to that of SGD. The first result states that within $O(1/\\eta)$ steps of training, the trajectories of SGD and SGDM approximate each other with distance $O(\\sqrt{\\eta/(1-\\beta)})$. The second result states that within $O(1/\\eta^2)$ steps of training, the trajectories of both SGD and SGDM move slowly after reaching a manifold where the local minimizer is located."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The main results, i.e., Theorem 3.5 and Theorem 4.5, are strong in terms of both implications and proof techniques.\n2. The results are presented in an orderly manner.\n3. The result that the training trajectories of SGDM and SGD are similar is intriguing."
                },
                "weaknesses": {
                    "value": "1. Although the implications of Theorem 3.5 is clear, the statement of the theorem is a bit confusing. Specifically, the averaged learning schedule $\\bar\\eta_k$ is introduced, and it does not appear again until the appendix.\n2. It would be interesting to see other concrete types of hyperparameter schedule apart from the constant ones, both theoretically and empirically.\n3. It would also be interesting to see experiments demonstrating the main theoretical contribution of this paper, i.e., the trajectories rather than the loss / accuracy stay close."
                },
                "questions": {
                    "value": "1. Do the two regimes, i.e. $t=O(1/\\eta)$ and $t=O(1/\\eta^2)$ have any connections? Can we understand from the results provided in this work what's going on when $t=\\Omega(1/\\eta)$ but $t=o(1/\\eta^2)$?\n2. Could the authors clarify why a **class** of hyperparameter schedule $\\{\\eta_k^{(n)}, \\beta_k^{(n)}\\}$, indexed by $n$, is considered in both main theorems? It seems more straightforward to me to consider only one choice of hyperparameter schedule."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6171/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6171/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6171/Reviewer_RZuq"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6171/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804226784,
            "cdate": 1698804226784,
            "tmdate": 1699636670336,
            "mdate": 1699636670336,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YqF20H0QRn",
                "forum": "3JjJezzVkT",
                "replyto": "14S0jqWAuv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6171/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6171/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments.\n\n---\n\n**Q1:** Although the implications of Theorem 3.5 is clear, the statement of the theorem is a bit confusing. Specifically, the averaged learning schedule is introduced, and it does not appear again until the appendix.\n\n**A1:** The averaged learning rate schedule is defined in our theorem as the formula in the parentheses directly followed. We will find a better way to introduce this notion in the revision to eliminate the confusion.\n\n---\n\n**Q2:** It would be interesting to see other concrete types of hyperparameter schedule apart from the constant ones, both theoretically and empirically. It would also be interesting to see experiments demonstrating the main theoretical contribution of this paper, i.e., the trajectories rather than the loss / accuracy stay close.\n\n**A2:** Our theory is proved for a general hyperparameter scheduling (as long as its scale is controlled), so the conclusions hold for non-constant schedules. Also, for the experiments, we used the 3-stage annealing learning rate schedule and added a linear warm-up, which is technically not constant. In the future, we can surely provide more experiments with more varying hyperparameter schedules.\n\nOur theory prescribes that the trajectories are close according to any test function (Definition 3.4), and we used training and test curves which are the most empirically concerned test functions. In the future we can add more test functions to show the closeness of the trajectory distributions. \n\n---\n\n**Q3:** Can we understand what\u2019s going on when $t=\\Omega(1/\\eta)$ and $t=O(1/\\eta^2)$? That is, between the two regimes?\n\n**A3:** The two regimes are separate in our results because they entail different assumptions on the landscape. The $\\Omega(1/\\eta)$-step result holds for general smooth landscapes, while for the trajectory to be tractable for $O(1/\\eta^2)$ steps, we need stronger assumptions on the landscape (i.e. the existence of a minimizer manifold). The two regimes also have different levels of tolerance of the noise scale $\\sigma$.\n\nWhen the $O(1/\\eta^2)$-trajectory is tractable, the first $O(1/\\eta)$ steps in the trajectory actually follows an SGD trajectory that decrease the loss from initial to almost zero. At the end of the first $O(1/\\eta)$ steps, the parameter will be $o(1)$ close to the minimizer manifold, and it stays in the neighborhood of the manifold for $O(1/\\eta^2)$ steps and do a $O(\\eta)$ progression along the manifold every $O(1/\\eta)$ steps.\n\n---\n\n**Q4:** Why is a class of hyperparameter schedules used in the main theorems?\n\n**A4:** For a specific hyperparameter scheduling, we can indeed use our theorem to bound the distance between SGDM and SGD trajectories. \n\nWe write a class of schedules in our theorem to make the theorem more general and to emphasize the dependence of our result on the scale of $\\eta$, e.g. when $\\eta\\to 0$ the trajectories indeed have limiting distributions at a rate $O(\\eta^{(1-\\alpha)/2})$ for some limiting hyperparameter schedule."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6171/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536045813,
                "cdate": 1700536045813,
                "tmdate": 1700536045813,
                "mdate": 1700536045813,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UEqtBZZFOP",
            "forum": "3JjJezzVkT",
            "replyto": "3JjJezzVkT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6171/Reviewer_ReS6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6171/Reviewer_ReS6"
            ],
            "content": {
                "summary": {
                    "value": "The authors have proved theoretically the marginal benefit of momentum in training when the learning rate is small and the noise is dominant. The theory has been verified with some experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper has demonstrated the limited benefit of momentum theoretically in some regimes, which helps suggest when not to use momentum.\n2. The article has illustrated the idea with warmup examples, which makes it easier to digest."
                },
                "weaknesses": {
                    "value": "1. The analysis is based on the fact that the momentum effect is dominated by other factors when the learning rate is negligible. This is somewhat intuitive and needs to dive deeper into what the special role of momentum is. \n\n2. There needs to be clear examples of what scenarios the theory fits in. For example, with what class of loss and neural network does the theory fit in?\n \n3. The paper seems rushed in polishing. There are some links of reference in the paper that need to be added, for example, in line 2 on page 8."
                },
                "questions": {
                    "value": "1. Could you show some examples of what kind of networks, loss..etc the theory fits in.\n2. It will be nice to make more clear comparison of when with and without momentum."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concern"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6171/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699035431318,
            "cdate": 1699035431318,
            "tmdate": 1699636670222,
            "mdate": 1699636670222,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HiOi4CoWzD",
                "forum": "3JjJezzVkT",
                "replyto": "UEqtBZZFOP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6171/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6171/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments.\n\n---\n\n**Q1:** The analysis is based on the fact that the momentum effect is dominated by other factors when the learning rate is negligible. This is somewhat intuitive and needs to dive deeper into what the special role of momentum is.\n\n**A1:** We are not sure about what is excluded from \u201cother factors\u201d, and we appreciate it if more clarifications are provided.\n\nThe main message we wish to convey through this paper is that momentum has no special role when the learning rates are small, and tuning the momentum parameter has very limited benefits as model performance is agnostic to the choice of momentum parameters over a range of hyperparameter scales. This conclusion is supported by both our theoretical and empirical findings.\n\nFor some clarifications of our results, first, we believe that it is incorrect to claim that the momentum effect is dominated by changing the curvature-induced instability, rather than changing the noise-induced instability (defined in Lemma 2.4). Adding momentum actually modifies the noise part of parameter updates more than the curvature part. Second, simple intuitions like Section 3.1 cannot offer any suggestions on the number of steps when adding momentum accumulates non-negligible deviations. For instance, as our setting involves a $O(\\sqrt{\\eta})$-normed update per step, two adjacent SGD updates may differ by $O(\\sqrt{\\eta})$, therefore after $O(1/\\sqrt{\\eta})$ steps an SGDM trajectory will deviate from an SGD trajectory by the above intuition, which only accounts for $O(\\sqrt{\\eta})$ training loss decrease. Therefore even when the learning rate is small enough, it is not trivial to conclude that momentum has a negligible effect over the whole trajectory of $O(1/\\eta)$ or $O(1/\\eta^2)$ steps, when the training loss goes from initial to almost optimal and plateaus.\n\n---\n\n**Q2**: There needs to be clear examples of what scenarios the theory fits in. For example, with what class of loss and neural network does the theory fit in?\n\n**A2:** For a short clarification, as our theory is a proof of the general non-convex landscape, it aims to model model training with any smooth loss, any neural networks architecture with any smooth activations (with polynomial growth) and small learning rates. For instance training with square loss for regression or logistic loss for classification falls into the assumptions. The boundedness of noise covariance and hessian are satisfied when the weights of the network have bounded moments during the course of the training, which is satisfied when weight decay is involved for general neural networks. \n\nThe assumptions of the paper are standard in a series of trajectory analysis works [1-4].\n\nReferences\n- [1] Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations. Journal of Machine Learning Research, 20(40): 1\u201347, 2019.\n- [2] Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling SGD with stochastic differential equations (SDEs). Advances in Neural Information Processing Systems, 34:12712\u2013 12725, 2021a.\n- [3] Li, Zhiyuan, Tianhao Wang, and Sanjeev Arora. \"What Happens after SGD Reaches Zero Loss?--A Mathematical Framework.\" In International Conference on Learning Representations. 2021.\n- [4] Sadhika Malladi, Kaifeng Lyu, Abhishek Panigrahi, and Sanjeev Arora. On the SDEs and scaling rules for adaptive gradient algorithms. Advances in Neural Information Processing Systems, 2022.\n\nFurthermore, our experiment results further corroborate that our conclusion holds for general small-batch training from image tasks to natural language tasks, even when the learning rate is large. In practice, this implies that model performance is agnostic to the choice of momentum parameters when the batch sizes are scaled down."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6171/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533633085,
                "cdate": 1700533633085,
                "tmdate": 1700533633085,
                "mdate": 1700533633085,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]