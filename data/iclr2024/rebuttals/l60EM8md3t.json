[
    {
        "title": "Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation"
    },
    {
        "review": {
            "id": "EC6LDQyCEM",
            "forum": "l60EM8md3t",
            "replyto": "l60EM8md3t",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission848/Reviewer_Rq97"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission848/Reviewer_Rq97"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a novel method via learning-to-match mechanism, approaching current audio-text retrieval framework and learn the embedding space. The method itself is based on optimal transport, with solid mathematical foundations. The method itself leads to notable  improvements in cross-modal retrieval on common audio datasets, with event detection as the downstream task and additional analysis. Although there are some gaps between the framework design and the scenarios it targets to, it is a well-formed study, which the reviewers learns a lot from."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The study itself is well-motivated, novel and practical.\n2. The mathematical foundation is good.\n3. The experiments have been done on commonly-known datasets and the improvements are clearly-observed."
                },
                "weaknesses": {
                    "value": "1. Apart from the performance, it would be good to also show the acquired network architecture, and run-time efficiency for cross-modal inference.\n2. The reference of \"noise\" is not clear and potentially confusing, even with clear references. When talking about the noise, it can be many things. Especially for speech people who are very likely refer to this paper, seems like the definition of \"noise\" is different from real-world interruption - it is totally fine, but please spend some text on clarifying it in the introduction and methodology (aka Section 3.3).\n\nAlso some minor issues:\n1. Although I did not find significant grammatical flaws, please do a thorough check on the language usage. For example, at the beginning of Section 4, \"Cross-modal matching is an essential task.....\"sounds a bit weird."
                },
                "questions": {
                    "value": "Most of the questions have been asked as weaknesses in the above section. Please answer them. \n\nI also have additional trivial questions.\n1. Do you think if it is possible to open-source the models?\n2. Do you think your models will be benefitted by further fine-tuning the network model or pre-trained encoders? Or you think basically that's it (which is totally fine)?\n3. Do you think your model can be adopted to other sound-related tasks, with rather little amount of data for domain adaptation? Or maybe collection of experts is needed?\n4. Would you classify your approach as essentially an \"unsupervised\" or \"supervised\" contrastive loss?\n5. Do you see any possibility on developing a \"parameter-free\" variant of your framework?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The reviewer does not find any notable concern on the ethical issues."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission848/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698218017865,
            "cdate": 1698218017865,
            "tmdate": 1699636012398,
            "mdate": 1699636012398,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HLbvXk2dfE",
                "forum": "l60EM8md3t",
                "replyto": "EC6LDQyCEM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission848/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission848/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Rq97"
                    },
                    "comment": {
                        "value": "We appreciate the valuable time and feedback of reviewer to improve your work. We have revised our manuscript and would like to address your concerns outlined below\n\n**Q1:** Apart from the performance, it would be good also to show the acquired network architecture and run-time efficiency for cross-modal inference.\n\n**A1:** The focus of our work is to learn an effective joint embedding space for audio and text retrieval. We acknowledge the importance of run-time efficiency and leave that to our future work.\n\n**Q2:** The reference of \"noise\" is not clear and potentially confusing, even with clear references ...\n\n**A2:** We totally agree with your suggestion and have added a sentence in the third paragraph of the introduction to articulate noisy data in the audio-text matching task. \n\n**Q3:** Some minor issues in language usage\n\n**A3:** We have proofread the manuscript and corrected all language usage.\n\n**Q4:** Do you think if it is possible to open-source the models?\n\n**A4:** We will definitely publish our source code and pretrained models to the community.\n\n**Q5:** Do you think your models will be benefitted by further fine-tuning the network model or pre-trained encoders? Or you think basically that's it (which is totally fine)?\n\n**A5:** We leverage pre-trained encoders for our work(also used by prior works) to improve performance and help models converge quickly. Therefore, good pre-trained encoders could help to improve the performance of audio-text retrieval tasks. Regarding further fine-tuning, we think that if we finetune our pre-trained models on a diverse and high-quality dataset, it could enhance the model\u2019s performance like most deep learning models.\n\n**Q6:** Do you think your model can be adopted to other sound-related tasks, with rather little amount of data for domain adaptation? Or maybe collection of experts is needed?\n\n**A6:** Our model could be used for other downstream tasks by finetuning the pretrained model, for example, text-to-audio generation. However, we need an appropriate decoder/adapter model for decoding embedding space to data space.\n\n**Q7:** Would you classify your approach as essentially an \"unsupervised\" or \"supervised\" contrastive loss?\n\n**A7:** We would like to classify our approach as a supervised method since the m-LTM needs to use aligned audio-text pairs for training.\n\n**Q8:** Do you see any possibility on developing a \"parameter-free\" variant of your framework?\n\n**A8:** We assume that you are asking about replacing the Mahalanobis distance with a parameter-free one, like kernel distance. We thought about that but left it for future work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission848/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263324243,
                "cdate": 1700263324243,
                "tmdate": 1700281886987,
                "mdate": 1700281886987,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u5uFIb9w6D",
                "forum": "l60EM8md3t",
                "replyto": "HLbvXk2dfE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission848/Reviewer_Rq97"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission848/Reviewer_Rq97"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for addressing all my questions with details! I do not have further questions to ask for now."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission848/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557979855,
                "cdate": 1700557979855,
                "tmdate": 1700557979855,
                "mdate": 1700557979855,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JfqIx6y4K2",
            "forum": "l60EM8md3t",
            "replyto": "l60EM8md3t",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission848/Reviewer_Mpor"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission848/Reviewer_Mpor"
            ],
            "content": {
                "summary": {
                    "value": "This paper describes an approach for audio-text retrieval using a minibatch version of Learning-to-Match with an optimal transport optimization .  The result of this approach is strong retrieval performance across three datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The algorithmic design of this approach is well motivated and (for the most part) well-described.  (The application of Projection Gradient Descent is effective.)\n\nThe performance is particularly good compared to triplet and contrastive loss."
                },
                "weaknesses": {
                    "value": "Retrieval applications have an expectation of scaling.  Ideally a single query would be used to retrieve one or more corresponding examples from an extremely large source.  However, in this paper the datasets (particularly the test sets) have a fairly small source to retrieve from (a few thousand examples typically).  It would strengthen the work substantially to demonstrate the capabilities of the algorithm to scale to instances where there are orders of magnitude more examples to retrieve from than queries.\n\nThere is a claim that the correspondence in Figure 1 is obviously better in figure 1b than figure 1a.  I think this is not as obvious as claimed.  I don\u2019t think this image adds particularly value  to the understanding of the work."
                },
                "questions": {
                    "value": "Partial Optimal Transport appears to be performed by noising some percentage of the data prior to learning.  When this noise is applied with a batch size of 2, there is a substantial likelihood that both batch elements will contain incorrect (corrupted) examples.  Is there any risk in this from an algorithmic perspective?  \n\nHow does this approach deal with instances where no match is available?  \n\nThe authors describe one of the positive attributes of this work as identifying a joint embedding space for both speech and text through which to perform retrieval.  Has this embedding space been used for other downstream tasks? This would strengthen the argument that this is a good joint space rather than solely useful for retrieval.\n\nHow effectively does the learned representation work across corpus?  For example, when training on Audiocaps, how effective is retrieval of Clotho test sets and vice versa? This would be a more effective measure of the robustness of the algorithm than the intra-corpus analyses.\n\nThe delta_gap described in table 4 is the difference between the means of the embeddings.  First, is this gap an absolute value? it\u2019s remarkable that mean(f(x_i)) is always greater than mean(g(y_i)).  Second, is the corpus mean of text and audio embeddings a reasonable measure of performance? it seems like this measure could be easily gamed by normalizing the embedding spaces to have a 0-mean, but this wouldn\u2019t add anything to the transferability of the embedding.  Also distance in the embedding space isn\u2019t well defined.  It\u2019s not clear that a unit of distance in the triplet loss space can reasonably be compared to a unit in the m-LTM space."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission848/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772821608,
            "cdate": 1698772821608,
            "tmdate": 1699636012297,
            "mdate": 1699636012297,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6kiHLtIrEJ",
                "forum": "l60EM8md3t",
                "replyto": "JfqIx6y4K2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission848/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission848/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Mpor (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the valuable time and feedback of reviewer to improve your work. We have revised our manuscript and would like to address your concerns outlined below\n\n**Q1:** Retrieval applications have an expectation of scaling. Ideally a single query would be used to retrieve one or more corresponding examples from an extremely large source. However, in this paper the datasets (particularly the test sets) have a fairly small source to retrieve from (a few thousand examples typically). It would strengthen the work substantially to demonstrate the capabilities of the algorithm to scale to instances where there are orders of magnitude more examples to retrieve from than queries.\n\n**A1:** We agree that scalability is important for retrieval applications. An accurate similarity function for ranking is also crucial. In this work, we focus on measuring the similarity of examples in a rich and expressive joint embedding space designed for audio-text retrieval problems. We leave the investigation of the scalability issue to future work.\n\n**Q2:** There is a claim that the correspondence in Figure 1 is obviously better in figure 1b than figure 1a. I think this is not as obvious as claimed. I don\u2019t think this image adds particularly value to the understanding of the work.\n\n**A2:**  Figure. 1 demonstrates the joint embedding space for zero-shot sound event detection setting. We would like to show that the joint embedding space learned from our framework is more expressive than contrastive learning. All sound event labels are described in plain text by the template \u201cthis is a sound of {sound event}.\u201d Each text embedding in Figure. 1 is equivalent to a sound event label. The sound event detection is now converted to an audio-text retrieval task, therefore, a compact and clustering embedding space is beneficial to the performance. Figure. 1b shows that the m-LTM framework encourages the compact clustering of audio and event texts in the embedding space compared with contrastive loss shown in Figure. 1a. The claim is supported by the performance of zero-shot sound event detection experiment shown in Table. 3.\n\n**Q3:** Partial Optimal Transport appears to be performed by noising some percentage of the data prior to learning. When this noise is applied with a batch size of 2, there is a substantial likelihood that both batch elements will contain incorrect (corrupted) examples. Is there any risk in this from an algorithmic perspective?\n\n**A3:** We agree that there is a risk in this when the batch size is very small. The underlying assumption of our algorithm is that there is at least a proportion of matched examples in a batch. If a batch is too small, either there are no matched examples, or all are matched. Hence, small batches are not the focus of this algorithm.  The intuition of utilizing POT for noise correspondence data is to discard as many noisy pairs as possible. In the case of a small batch size, there is a risk of a whole minibatch of training data being noisy, POT regularization fails to handle that circumstance since it always imposes a percentage of matching(the mass parameter for POT) between two sources. The risk of a corrupted minibatch is critical for all existing methods, including my proposed approach. However, with a large enough minibatch, POT regularization is able to mitigate the harm of misaligned training data.\n\n**Q4:** How does this approach deal with instances where no match is available?\n\n**A4:** The POT approach deals with misaligned instances within a minibatch by a parameter, which forces only to match a percentage of audio-text pairs, the total transportation mass between two sources $0 \\leq s \\leq 1$. The total transportation mass of POT acts as a regularizer to discard the less certain matches, therefore, it is able to mitigate the harmfulness of mismatched training data.\n\n**Q5:** The authors describe one of the positive attributes of this work as identifying a joint embedding space for both speech and text through which to perform retrieval. Has this embedding space been used for other downstream tasks? This would strengthen the argument that this is a good joint space rather than solely useful for retrieval.\n\n**A5:** We conducted a zero-shot setting for a downstream task, sound event detection, on the ESC-50 test set in the Table. 3 to illustrate the expressiveness of the learned join embedding space."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission848/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700290129073,
                "cdate": 1700290129073,
                "tmdate": 1700295274747,
                "mdate": 1700295274747,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UuNplWdhZM",
                "forum": "l60EM8md3t",
                "replyto": "JfqIx6y4K2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission848/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission848/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Mpor (2/2)"
                    },
                    "comment": {
                        "value": "**Q6:** How effectively does the learned representation work across corpus? For example, when training on Audiocaps, how effective is retrieval of Clotho test sets and vice versa? This would be a more effective measure of the robustness of the algorithm than the intra-corpus analyses.\n\n**A6:** To answer your question, we compare our method with the baselines in the cross-corpus setting. As shown in the table below, the m-LTM is more robust across audio-text retrieval datasets than triplet and contrastive loss. Regarding training on the AudioCaps and testing on the Clotho dataset, our proposed method acquires the highest performance, 15.01%, in terms of R@1 for the text-to-audio retrieval task, compared with 10.41% and 12.31% for triplet and contrastive loss respectively. The same observation is seen for the setting training on Clotho and then testing on AudioCaps dataset.\n\n|     Train-Test    |  Method |       |  T->A |       |       |  A->T |       |\n|:-----------------:|:-------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n|                   |         |  R@1  |  R@5  |  R@10 |  R@1  |  R@5  |  R@10 |\n|                   | Triplet | 10.41 | 27.38 | 38.04 | 11.39 | 28.42 | 39.80 |\n| AudioCaps->Clotho |    CL   | 12.31 | 30.93 | 43.44 | 13.87 | 33.30 | 43.54 |\n|                   |  m-LTM  | 15.01 | 35.42 | 47.71 | 19.42 | 37.03 | 48.61 |\n|                   | Triplet | 14.33 |   40  | 54.42 | 17.34 | 43.05 | 55.69 |\n| Clotho->AudioCaps |    CL   | 14.64 | 38.64 | 53.58 | 17.76 | 42.11 | 55.59 |\n|                   |  m-LTM  | 17.38 | 43.78 | 58.87 | 22.04 | 47.85 | 61.65 |\n\n**Q7:** The delta_gap described in table 4 is the difference between the means of the embeddings. First, is this gap an absolute value? it\u2019s remarkable that mean(f(x_i)) is always greater than mean(g(y_i)). Second, is the corpus mean of text and audio embeddings a reasonable measure of performance? it seems like this measure could be easily gamed by normalizing the embedding spaces to have a 0-mean, but this wouldn\u2019t add anything to the transferability of the embedding. Also distance in the embedding space isn\u2019t well defined. It\u2019s not clear that a unit of distance in the triplet loss space can reasonably be compared to a unit in the m-LTM space.\n\n**A7:** The gap value is the length of the discrepancy vector $\\vec{\\Delta_{gap}} = \\frac{1}{n}\\sum_{i=1}^n f(x_i) - \\frac{1}{n}\\sum_{i=1}^n g(y_i)$ which is subtracting vector between the mean of embedding vector of two modalities, we described it in the second paragraph of section 5.1. Therefore, the gap is always positive. The discrepancy between the two modalities proved that it is an effective metric for studying the transferability of a jointed embedding space[1]. We use Euclidean distance on the embedding space to measure the modality metric. To achieve a fair comparison, all embedding is normalized to unit vectors and then used to compute the modality gap.\n\n[1] Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission848/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700290701245,
                "cdate": 1700290701245,
                "tmdate": 1700521956638,
                "mdate": 1700521956638,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3HgmiRExXY",
            "forum": "l60EM8md3t",
            "replyto": "l60EM8md3t",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission848/Reviewer_efev"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission848/Reviewer_efev"
            ],
            "content": {
                "summary": {
                    "value": "The authors revisit the \"Learning to Match\" (LTM) framework (Li et al., 2018), investigating its utilization to learn cross-modal embeddings for text-to-audio and audio-to-text retrieval.\n\nLTM is based on entropic inverse optimal transport (eq. 3), where the goal is to learn the underlying ground metric c which minimizes the KL divergence of the optimal transfer plan $\\pi^{XY}$ determined from (eq. 3) and the empirical joint distribution (eq. 4). Here c is taken as the Mahalanobis distance (eq. 9) between the text and audio embeddings, and a minibatch version (m-LTM) is proposed, where deep networks are used to learn the embeddings, making the parameters of the cost function c the Mahalanobis matrix, and the parameters of the embedding networks.\n\nResults on the AudioCaps and Clotho datasets show large gains over previous approaches (table 1), and large gains over triplet and bi-directional contrastive (eq. 1) losses (table 2). Large gains in terms of zero shot sound event detection (table 3) and modality gap (table 4) are also shown. Large gains in noise tolerance are also shown (table 5). Ablations around using POT for additional robustness show small but consistent gains, and ablations on utlizing Mahalanobis vs. L2 distance also show small but consistent gains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Well motivated, inverse optimal transport seems worth exploring in the context of deep networks and minibatch training.\n- Strong results. The approach consistently outperforms existing SOTA text-audio retrieval results on the most popular datasets, and the most widely used contrastive objectives.\n- Generally well presented."
                },
                "weaknesses": {
                    "value": "- As their results are much better than previous approaches and standard contrastive training methods, I feel that this warrants further investigation. The training sets for AudioCaps and Clotho are rather small at 46K and 5K audio examples, respectively, and so regularization may be a very important factor. Their m-LTM approach is entropy regularized, while their Triplet and Constrastive baselines are not. An entropy-regularized constrastive loss baseline is the most natural analog here, and would more firmly establish the importance of the optimal transport formulation. This feels essential to establishing the significance of the method and results.\n- The m-LTM method presented is somewhat lower in novelty as a variation on LTM that investigates the use of minibatch training and deep networks, but 'revisiting' is explicitly called out in the title, and this seems a worthy exploration.\n- There are a few grammatical errors throughout the paper, but the paper is in general well structured, and adequately well written. A glaring exception is the abstract, which is in really poor shape. Authors, please resolve this.\n- In table 6, the best results for the ground metric hyperparameter $\\epsilon$ are at 0.05, but no values less than it are tested, while the results at 0.5 are very poor, suggesting that results for $\\epsilon<0.05$ should be instead included."
                },
                "questions": {
                    "value": "See previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission848/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699022729807,
            "cdate": 1699022729807,
            "tmdate": 1699636012228,
            "mdate": 1699636012228,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GYGMOsPeJQ",
                "forum": "l60EM8md3t",
                "replyto": "3HgmiRExXY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission848/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission848/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer efev (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the valuable time and feedback of reviewer to improve your work. We have revised our manuscript and would like to address your concerns outlined below\n\n**Q1:** A comparison with the entropy regularized contrastive loss baseline\n\n**A1:** Below are the results of the baseline (CL with entropy regularized) suggested by Reviewer efev, in comparison with our method and the one without entropy regularization (CL w/o). More specifically, the baseline is trained by using contrastive loss and entropy regularized on the matching matrix, $L(\\theta,\\phi) = L_{CL} + \\epsilon*L_{ent}(R)$, where $\\mathcal{L}_{CL}$ is contrastive loss described in the Equation. 1, R is the ranking matrix between two sets of audio and caption and $\\epsilon$ is the coefficient of entropy regularized term. We have reported the experiment in Table. 7 in Appendix A.4. The experimental results demonstrate that entropy regulation can boost audio-text retrieval performance slightly, however, there is a significant gap in performance between the m-LTM and contrastive loss with entropy regularization.\n\n**Q2:** The m-LTM method presented is somewhat lower in novelty as a variation on LTM that investigates the use of minibatch training and deep networks, but 'revisiting' is explicitly called out in the title, and this seems a worthy exploration.\n\n**A2:** We agree that adapting the learning-to-match framework for deep learning is a valuable exploration. However, we acknowledge that the title and abstract can not be modified during the discussion stage. We will definitely change our title regarding your comment when submitting the camera-ready version.\n\n**Q3:** There are a few grammatical errors throughout the paper, but the paper is, in general well structured and adequately well-written. A glaring exception is the abstract, which is in really poor shape. Authors, please resolve this.\n\n**A3:** We have corrected all the grammatical errors and will definitely restructure the abstract when submitting the camera-ready version. Here is our new abstract:\n\nThe Learning-to-match(LTM) framework proves to be an effective inverse optimal transport approach for learning the underlying ground metric between two sources of data, facilitating subsequent matching. However, the conventional LTM framework faces scalability challenges, necessitating the use of the entire dataset each time updating parameters of ground metric. In adapting LTM to the deep learning context, we introduce the mini-batch Learning-to-match (m-LTM) framework for audio-text retrieval problems. This framework leverages mini-batch subsampling and Mahalanobis-enhanced family of ground metrics. Moreover, to cope with misaligned training data in practice, we propose a variant using partial optimal transport to mitigate the harm of misaligned data pairs in training data. We conduct extensive experiments on audio-text matching problems using three datasets: AudioCaps, Clotho, and ESC-50. Results demonstrate that our proposed method is capable of learning rich and expressive joint embedding space, which achieves SOTA performance. Beyond this, the proposed m-LTM framework is able to close the modality gap across audio and text embedding, which surpasses both triplet and contrastive loss in the zero-shot sound event detection task on the ESC-50 dataset. Notably, our strategy of employing partial optimal transport with m-LTM demonstrates greater noise tolerance than contrastive loss, especially under varying noise ratios in training data on the AudioCaps dataset."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission848/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288233772,
                "cdate": 1700288233772,
                "tmdate": 1700348651223,
                "mdate": 1700348651223,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]