[
    {
        "title": "MiniFold: Simple, Fast and Accurate Protein Structure Prediction"
    },
    {
        "review": {
            "id": "HhuLTkGKMx",
            "forum": "SjgfWbamtN",
            "replyto": "SjgfWbamtN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8942/Reviewer_FoqV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8942/Reviewer_FoqV"
            ],
            "content": {
                "summary": {
                    "value": "AlphaFold2 remains the state of the art for protein structure prediction. However, the model has poor complexity characteristics and high peak memory usage, and inference on longer proteins often runs for several minutes or more. The authors propose MiniFold, a barebones, single-sequence architecture built with select modules from AlphaFold2. Despite the model's small parameter count, it achieves a high fraction of ESMFold's single-sequence performance on CAMEO proteins and runs faster."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is clearly written and easy to follow. Some of the ablations are fairly surprising (e.g. recycling) and, if supported by better evaluations (see below), would improve our understanding of the all-important AlphaFold2 architecture. The proposed model is fast and fairly performant, and could make a useful addition to the structure prediction toolbox."
                },
                "weaknesses": {
                    "value": "- Since MiniFold only predicts the backbone structure, the comparison to architectures like AlphaFold2 and ESMFold is a little bit unfair. It's unclear from this manuscript whether ablated components are actually unnecessary or necessary to predict the positions of atoms excluded here.\n- There are a lot of important baselines missing here. RGN2 (Chowdhury et al., 2021) is a lightweight structure predictor with a language model that purports to be faster than AlphaFold2 by six orders of magnitude. Optimized versions of the full AlphaFold2 like FastFold (Cheng et al., 2022), UniFold (Li et al., 2022), and recent versions of OpenFold (Ahdritz et al., 2022) also exist but are not tested here. RosettaFold2 is also missing.\n- CAMEO evaluation is not sufficient to tease out the differences between structure prediction models. The gold standard is CASP proteins, which often reveal much larger gaps between models than can be seen in CAMEO figures alone. ESMFold famously underperformed AlphaFold2 at CASP15 by a very wide margin, and I suspect the limited capacity of MiniFold could hold it back even further here.\n- Originality isn't the only criterion here, but I'm not sure if this paper has many new insights that would be of interest to the broader machine learning community. The observation that triangle attention isn't strictly necessary was already noted in the original AlphaFold2 paper. Other more surprising claims (recycling isn't necessary) need to be supported by additional evidence, like a CASP evaluation. As I mentioned above, this isn't the first paper to present an optimized, protein-language-model based lightweight alternative to AlphaFold2 either.\n\nBits and bobs:\n\n> Our analysis reveals that the bulk of the compute is spent in the Evoformer blocks, which is responsible for predicting the pairwise interactions between the protein residues. Specifically, the Triangular attention layers of the Evoformer have O(n3) time and memory complexity, which hinders scaling to longer sequences.\n\n- This is a bit grandiose. The complexities of the AlphaFold2 modules are already well-known.\n\n> The main limitation of AlphaFold2, however, is its computational cost\n\n- I wouldn't call this the \"main\" limitation. The model isn't that great at proteins without deep MSAs, e.g..\n\n>We constructed a training set from the AlphaFold database. In particular, we first cluster the sequences from Uniref50 (Suzek et al., 2007) at 30% sequence similarity, and then select the structures with an average plDDT score above 0.7. This results in a high quality, diverse dataset of roughly 10 million structures.\n\n- If you're training on AF2 structures, especially exclusively, then a lot of your claims about certain components of AF2 being unnecessary could be called into question; this is effectively a form of distillation, not an independent competing architecture.\n\n>Our results show that MiniFold is competitive with other protein language model based structure prediction models, achieving over 95% of the state-of-the-art ESMFold at a fraction of the computational cost.\n\n- 95% is somewhat misleading. OpenFold shows that the full AlphaFold2 model reaches comparable lDDT levels almost immediately; almost all of the training time is spent closing roughly the same gap as the one between MiniFold and the full AlphaFold2 model."
                },
                "questions": {
                    "value": ">We perform several modifications to the Evoformer architecture, as shown in figure 2. First, we eliminate the sequence-level encoding and keep only the pairwise representation and update. Remarkably, this reduces the number of parameters from over 600 million to merely 3 million. In fact, we argue that the representational capacity of the Evoformer is influenced by the depth and complexity of its operations rather than by its parameter count.\n\n- Where does 600 million come from? AlphaFold2 has about 93 million parameters.\n\n>In addition, we eliminate the recycling operation used in both AlphaFold2 and ESMFold, as we found it only provides very minimal benefits that did not outweigh the additional computational cost.\n\n- This contradicts the ablations in the AlphaFold2 paper. In what sense does it only provide minimal benefits? On which sequences?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8942/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8942/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8942/Reviewer_FoqV"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8942/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697687333926,
            "cdate": 1697687333926,
            "tmdate": 1699637125887,
            "mdate": 1699637125887,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JA2qV11U9O",
                "forum": "SjgfWbamtN",
                "replyto": "HhuLTkGKMx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8942/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their comments, which helped us improve the paper! Please find our response below:\n\n> Since MiniFold only predicts the backbone structure, the comparison to architectures like AlphaFold2 and ESMFold is a little bit unfair. It's unclear from this manuscript whether ablated components are actually unnecessary or necessary to predict the positions of atoms excluded here.\n\nThis is certainly a fair point. However, the no-IPA ablation in the AlphaFold2 paper seems to suggest that removing the structure module does not have a significant impact on performance. Therefore we opted to remove it to gain further speed. Yet, for this to make sense, we also needed to demonstrate that the MiniFormer had sufficiently learned the underlying 3D structure, which the coordinate realizer shows. The RGN2 model did not predict full-atom either, and calls the AlphaFold2 refinement code to do so.\n> There are a lot of important baselines missing here. RGN2 (Chowdhury et al., 2021) is a lightweight structure predictor with a language model that purports to be faster than AlphaFold2 by six orders of magnitude. Optimized versions of the full AlphaFold2 like FastFold (Cheng et al., 2022), UniFold (Li et al., 2022), and recent versions of OpenFold (Ahdritz et al., 2022) also exist but are not tested here. RosettaFold2 is also missing.\n\nThank you for pointing this out. Omitting these models was intentional for the following reasons:\n RGN2 does not perform well on structure prediction and was proposed in the context of design and orphan proteins, at a time where AlphaFold2 did not have good support for single sequence prediction. A recent publication benchmarking on CASP shows this: https://www.sciencedirect.com/science/article/pii/S0959440X2300101X. In figure 2 it is clear that RGN2 severely underperforms compared to OmegaFold and ESMFold.\nESMFold is built on top of OpenFold. Thus when benchmarking speed, we directly compared to Openfold\u2019s implementations. The OpenFold repository also mentions that FastFold\u2019s implementations have been merged in, and focused largely on the attention modules, which we removed in this work. Furthermore, the speed-ups shown in these repositories are much smaller than the ones we report, while the memory savings involve a chunking strategy which reduces memory at the cost of speed, while our approach results in memory savings that also produce speedup.\nRosettaFold2 is largely similar to AlphaFold2, and includes a structure track using the expensive SE(3) Transformer. We did not think it added a meaningfully different data point beyond AlphaFold2.\n\n> CAMEO evaluation is not sufficient to tease out the differences between structure prediction models. The gold standard is CASP proteins, which often reveal much larger gaps between models than can be seen in CAMEO figures alone. ESMFold famously underperformed AlphaFold2 at CASP15 by a very wide margin, and I suspect the limited capacity of MiniFold could hold it back even further here.\n\nThank you for this suggestion. We totally agree with this comment, and have produced thorough experiments on CASP14 (see global answer). This analysis shows that we remain very competitive with ESMFold but only with the 40 layer model, and with recycling enabled. In this setting, our speedup is reduced to up to 20x, but our memory savings of up to 40x remain, which is still significant. We have modified the paper abstract, and included these new results."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733152653,
                "cdate": 1700733152653,
                "tmdate": 1700733262518,
                "mdate": 1700733262518,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MD9vC5jkgT",
                "forum": "SjgfWbamtN",
                "replyto": "HhuLTkGKMx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8942/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Originality isn't the only criterion here, but I'm not sure if this paper has many new insights that would be of interest to the broader machine learning community. The observation that triangle attention isn't strictly necessary was already noted in the original AlphaFold2 paper. Other more surprising claims (recycling isn't necessary) need to be supported by additional evidence, like a CASP evaluation. As I mentioned above, this isn't the first paper to present an optimized, protein-language-model based lightweight alternative to AlphaFold2 either.\n\nWe think there are several important contributions in our work that distinguishes it from previous work and that would be of interest to the machine learning community:\n\n1. First, we produced a model that is very close to ESMFold (<3% drop) and results in massive speedup and memory savings. This could enable many more in the research community being able to work on the protein structure prediction task, especially when only limited compute is available. While ESMFold was a great first step in this direction, it remained very expensive to train from scratch or fine-tune.\n\n2. Our proposed kernels are broadly applicable, beyond this architecture.  In particular, the memory efficient 2 layer feed-forward could be applied to any Transformer model.\n\n3. We contribute several analyses that provide better understanding of the AlphaFold2 architecture. The first one is to show that the distogram alone, without any other learned projection, has learned the 3D structure, which shows that the structure module is largely a decoding mechanism. We also showed that the uncertainty of the distogram is a great predictor of structure accuracy. Finally, we showed that the sequence track can be completely eliminated in ESMFold, reducing the parameter count to just a few million parameters. This result emphasizes that the power of the architecture lies in the expressivity of its operations and not in its parameter count.\n\n4. We obtained a powerful model by using just a single loss function, and a single stage of training.\n\nRegarding the question of recycling, we acknowledge that our statements were misleading based on CAMEO results. CASP14 results clearly show the importance of recycling. We thank the reviewer for flagging this, and have modified the paper accordingly.\n\n>This is a bit grandiose. The complexities of the AlphaFold2 modules are already well-known.\n\nThis was not our intention, we have modified the text. On the other hand, some reviewers commented on the ESM language model being heavy, even though it only covers 1% of the compute time. We do believe that this is indeed a counter-intuitive piece of information given the very large number of parameters in the language model (3 billion).\n\n> I wouldn't call this the \"main\" limitation. The model isn't that great at proteins without deep MSAs, e.g..\n\nThis is totally fair, we have changed the text.\n\n> If you're training on AF2 structures, especially exclusively, then a lot of your claims about certain components of AF2 being unnecessary could be called into question; this is effectively a form of distillation, not an independent competing architecture.\n\nWe did not intend to say that these were completely unnecessary, but rather that thanks to this distillation setting, it may be possible to simplify the architecture, which seems to be the case. In fact, our main objective was to speed-up ESMFold, which also utilizes AlphaFold2 structures for training.\n\n> 95% is somewhat misleading. OpenFold shows that the full AlphaFold2 model reaches comparable lDDT levels almost immediately; almost all of the training time is spent closing roughly the same gap as the one between MiniFold and the full AlphaFold2 model.\n\nWe have trained a larger model which has further reduced the gap to < 3% but agree that these last few percent are not negligible. Yet we also do not want to trivialize this result. Reaching this performance required careful experiments to identify opportunities for speed-up that did not result in very large performance drops.\n\n> Where does 600 million come from? AlphaFold2 has about 93 million parameters.\n\nThe Evoformer in ESMFold uses different hyperparameters, resulting in > 600M parameters in the folding trunk.\n\n> This contradicts the ablations in the AlphaFold2 paper. In what sense does it only provide minimal benefits? On which sequences?\n\nAs mentioned previously, this was indeed a mistake on our part. Recycling is important on CASP14. We have modified the text accordingly. We did make an interesting observation that recycling the distogram is sufficient to recover the performance drop compared to ESMFold."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733206213,
                "cdate": 1700733206213,
                "tmdate": 1700733500332,
                "mdate": 1700733500332,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wVDDkPguB4",
            "forum": "SjgfWbamtN",
            "replyto": "SjgfWbamtN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8942/Reviewer_mayn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8942/Reviewer_mayn"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an efficient protein structure prediction method. The authors use ESM-2 to extract residue features and construct pairwise representations. The pairwise features are used to predict pairwise distance between all residues. Finally, they recover  3D coordinates from\nthe predicted distogram based on multi-dimensional scaling."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is clearly written.\n2. The main strength of MiniFold is good efficiency. The proposed MiniFold achieves over 100x speedup.\n3. The authors simplified the modeling complexity and did GPU kernel optimization."
                },
                "weaknesses": {
                    "value": "1. The proposed methods only generate C-alpha atoms. It could not reconstruct full-atoms.\n2. Most of the neural modules were copied directly from existing literature, limiting the novelty.\n3. The authors do not provide code for checking."
                },
                "questions": {
                    "value": "1. Could the author provide TMScore comparisons against AlphaFold, Omegafold, and ESMFold on long proteins (protein size>1000)? \n\n2. Have you tried to reconstruct full-atoms? \n\n3. Considering the importance of the structural prediction task, it should be carefully examined. Could the authors provide the source code?\n\n4. Does the inference time cost take into account the overhead of the coordinate realization module?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8942/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8942/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8942/Reviewer_mayn"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8942/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698683522763,
            "cdate": 1698683522763,
            "tmdate": 1699637125761,
            "mdate": 1699637125761,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "95BmSXjLgG",
                "forum": "SjgfWbamtN",
                "replyto": "wVDDkPguB4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8942/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their comments. Please find the answer to your points below.\n\n> Most of the neural modules were copied directly from existing literature, limiting the novelty.\n\nWe think there are several important contributions in our work that distinguishes it from previous work and that would be of interest to the machine learning community:\n\n1. First, we produced a model that is very close to ESMFold (<3% drop) and results in massive speedup and memory savings. This could enable many more in the research community being able to work on the protein structure prediction task, especially when only limited compute is available. While ESMFold was a great first step in this direction, it remained very expensive to train from scratch or fine-tune.\n\n2. Our proposed kernels are broadly applicable, beyond this architecture.  In particular, the memory efficient 2 layer feed-forward could be applied to any Transformer model.\n\n3. We contribute several analyses that provide better understanding of the AlphaFold2 architecture. The first one is to show that the distogram alone, without any other learned projection, has learned the 3D structure, which shows that the structure module is largely a decoding mechanism. We also showed that the uncertainty of the distogram is a great predictor of structure accuracy. Finally, we showed that the sequence track can be completely eliminated in ESMFold, reducing the parameter count to just a few million parameters. This result emphasizes that the power of the architecture lies in the expressivity of its operations and not in its parameter count.\n\n4. We obtained a powerful model by using just a single loss function, and a single stage of training.\n\n\n> Could the author provide TMScore comparisons against AlphaFold, Omegafold, and ESMFold on long proteins (protein size>1000)?\n\nWe did not get the opportunity to do so in the limited time available but will aim to include these results in the camera ready.\n\n> Have you tried to reconstruct full-atoms?\n\nThere are a variety of deep learning based methods for predicting all-atom structures from C alpha atoms, such as DLPacker (Misiura et al. 2021). These methods are computationally efficient and reasonably accurate. We can use these methods to predict full-atom structure given C alpha atom coordinates predicted by MiniFold. As a side note, RGN2 also uses external software to predict full-atom structure since it only predicts C alpha atom coordinates.\n\nMisiura et al., DLPacker: Deep Learning for Prediction of Amino Acid Side Chain Conformations in Proteins. Proteins, 2021\n\n> Considering the importance of the structural prediction task, it should be carefully examined. Could the authors provide the source code?\n\nWe will release the code upon publication.\n\n> Does the inference time cost take into account the overhead of the coordinate realization module?\n\nIt does not. The main reason here is that we argue that for the large-scale applications of interest such as virtual screening, full coordinate recovery may not be important. Yet, to make this point, it\u2019s important to know if the output of the evoformer has already learned the structure and that the structure module is mostly decoding this information rather than making a significant contribution to performance. We have added an analysis using the ESMFold distogram which confirms this result. One could also train a small structure module or a single projection from the Evoformer embeddings similar to what is proposed in the AlphaFold2 ablation studies."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732832965,
                "cdate": 1700732832965,
                "tmdate": 1700733594803,
                "mdate": 1700733594803,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Y4KdixQT2u",
            "forum": "SjgfWbamtN",
            "replyto": "SjgfWbamtN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8942/Reviewer_cGwq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8942/Reviewer_cGwq"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the protein folding problem, an important and time-consuming task. The authors proposed MiniFold that can infer a structure with 100x acceleration and tolerable accuracy loss. To achieve this, they carefully studied each block in EvoFormer, removed unnecessary blocks, and proposed a new pipeline. Experimental results demonstrate the effectiveness of their algorithm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, I pretty much enjoy reading this paper. The motivation and intuition are clear, and the way the authors solve the problems is reasonable. In addition, the contribution is indeed significant, especially under large-scale screening demand. The paper is well-written and organized, and the demonstration is straightforward."
                },
                "weaknesses": {
                    "value": "Currently, I give a score of 6. I am happy to increase my score if the below weaknesses (and questions) can be appropriately addressed.\n\n-  First of all, the authors clearly have dived into the (time and performance) ablation of AlphaFold2, ESMFold and MiniFold, which will be great to present. For example, how do time and performance change if we remove the triangular attention blocks? How is the performance change if we recycle MiniFold once / twice / third times?Theses results are not only useful for the design of MiniFold, but are also knowledge that people are curious about.\n- The second question relates to the third part of MiniFold (structural determination based on distance matrix). Why can't you directly build the 3D $C_\\alpha$ backbone based on the distance matrix (assuming that the matrix is filled)? In addition, how do you determine the side-chain angle?  \n- Third, I noticed that you built the model on a smaller (but pre-trained) ESMFold, right? In fact, one of the reasons why ESM and AF2 are so huge is that they train *from scratch*. I am not saying that using previous embeddings is not good, but an intermediate path to address the efficiency problems is to distill or prune the ESM model, or to use it as a teacher model to train a smaller model (since you have an infinitely large dataset now). Do you know any work about how this would be compared to MiniFold? What do you think is the pros and cons of different solution paths?\n- A minor point, I don't think \"removing\" cycling can be counted as your acceleration. This is entirely a time/accuracy trade-off, and can be easily achieved. I'd suggested the authors cycle MiniFold as well and compare the time.\n- Last but I am indeed curious: you mention that the Multiplicative layer can replace attention. Can you show how the performance changes after the replacement? In addition, this is also applicable to other fields, so I am curious why do you think this replacement can work well, and is it a domain-specific thing or not?"
                },
                "questions": {
                    "value": "I have asked many questions above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8942/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8942/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8942/Reviewer_cGwq"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8942/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698741467182,
            "cdate": 1698741467182,
            "tmdate": 1699637125650,
            "mdate": 1699637125650,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NJlFa8z7ZW",
                "forum": "SjgfWbamtN",
                "replyto": "Y4KdixQT2u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8942/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their comments. Please find our answers to your points below:\n\n> First of all, the authors clearly have dived into the (time and performance) ablation of AlphaFold2, ESMFold and MiniFold, which will be great to present. For example, how do time and performance change if we remove the triangular attention blocks? How is the performance change if we recycle MiniFold once / twice / third times?Theses results are not only useful for the design of MiniFold, but are also knowledge that people are curious about.\n\nRunning full ablations using the Triangular Attention blocks is difficult as the memory requirements get so large that we could not train more than a very small number of layers with it. This makes it difficult to compare with full sized models. This being said, in our early experiments on small models we observed very little loss in performance when omitting the triangular attention, so long as we kept the triangular multiplicative update. Furthermore, our results show that we are able to nearly match ESMFold without it, which is another positive signal.\n\n> The second question relates to the third part of MiniFold (structural determination based on distance matrix). Why can't you directly build the 3D backbone based on the distance matrix (assuming that the matrix is filled)? In addition, how do you determine the side-chain angle?\n\nThis is precisely the issue, the matrix is not filled. The model only predicts distances up to 25 A, with the last bin covering all distances larger than 25. Therefore we first need to approximate the missing distances, which we do using the all pairs shortest path algorithm. Once that is done, we can use MDS to get the coordinates. However because of the approximation of long distances, MDS will not be precise so we need a few steps of gradient descent to fix it.\n\n> Third, I noticed that you built the model on a smaller (but pre-trained) ESMFold, right? In fact, one of the reasons why ESM and AF2 are so huge is that they train from scratch. I am not saying that using previous embeddings is not good, but an intermediate path to address the efficiency problems is to distill or prune the ESM model, or to use it as a teacher model to train a smaller model (since you have an infinitely large dataset now). Do you know any work about how this would be compared to MiniFold? What do you think is the pros and cons of different solution paths?\n\nESMFold is not trained from scratch. The ESM-2 language model is trained first, and then ESMFold is trained on top of the ESM language model. Here similarly we assume that we also have the ESM2 language model and train the folding model from scratch. Furthermore, the ESM2 language model is very cheap compared to the Evoformer (only 1% of total compute time) so we do not believe it needs further optimization.\nPruning ESMFold is an interesting idea but difficult to do as it would likely require fine-tuning the model through the pruning process. Furthermore, unstructured pruning leads to smaller model footprint but rarely improves speed. Structured pruning approaches may be utilized but tend to not perform as well. Here we take a simpler route, by identifying parts of the architecture that are not the main driver of performance, and successfully training a streamlined model from scratch.\nRegarding student-teacher, note that we are using AF2 predicted structures for training, which is arguably already a student-teacher setting.\n\n> A minor point, I don't think \"removing\" cycling can be counted as your acceleration. This is entirely a time/accuracy trade-off, and can be easily achieved. I'd suggested the authors cycle MiniFold as well and compare the time.\n\nWe did not intend to make it one of our contributions, but to highlight the extra savings from removing recycling. We fully agree that an ablation including recycled manifold is important and have added this to the paper. The results show that recycling is important on CASP14 targets but not on the CAMEO test set.\n\n> Last but I am indeed curious: you mention that the Multiplicative layer can replace attention. Can you show how the performance changes after the replacement? In addition, this is also applicable to other fields, so I am curious why do you think this replacement can work well, and is it a domain-specific thing or not?\n\nIt\u2019s not so much a replacement since both the triangular attention and triangular multiplicative updates are present in the original architecture. In our early experiments, we found that the triangular multiplicative seemed to perform well even in the absence of the triangular attention. The triangular attention being very computationally heavy, we opted to remove it."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732593312,
                "cdate": 1700732593312,
                "tmdate": 1700732624795,
                "mdate": 1700732624795,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8DQ8UNAOgL",
            "forum": "SjgfWbamtN",
            "replyto": "SjgfWbamtN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8942/Reviewer_NKzF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8942/Reviewer_NKzF"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents MiniFold, a highly optimized protein structure prediction model that achieves over 100x speedup compared to ESMFold while retaining 95% of its accuracy. MiniFold simplifies the Evoformer architecture by removing unnecessary components like sequence encoding, triangular attention, and recycling. It also implements efficient GPU kernels and a simple coordinate recovery module."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The analysis to identify key components of Evoformer that enable high performance protein folding is insightful. This allows simplifying the architecture while maintaining accuracy.\n- The 100x speedup over ESMFold enables the application of protein folding models to high-throughput tasks involving millions of sequences. This is a major contribution."
                },
                "weaknesses": {
                    "value": "- The prediction process is faster, but final performance significantly decreases.\n- Removing IPA is disadvantageous, as the structure module is less costly than Evoformer.\n- Kernels are implemented with OpenAI's Triton, not CUDA; a full-page explanation is unnecessary due to well-known engineering improvements.\n- The analysis of kernels is wrong. For example, \"This reduces the number of reads from 5 to 1, and the number of writes from 4 to 1\". The Wx and Vx are matrix multiplication operators, which will call GEMM kernels, thus these read/write cannot be merged. We usually can only save the read/write times for element-wise operators.\n- The method relies on a computationally demanding pretrained protein language model; simplification would be beneficial.\n- Coordinate recovery omits chirality consideration, potentially negatively impacting performance.\n- In-depth analysis of uncertainty estimation technique is needed for better understanding of robustness."
                },
                "questions": {
                    "value": "- I notice there are confidence scores, but where you do inject randomness to generate a distribution?\n- Could training with MSAs further improve MiniFold's accuracy? What optimizations would be needed?\n- In Sec 4.2, how do you perform MiniFormer with recycling based on MDS?\n- Do you end-to-end optimze the coordinates with gradient back-propagation, or just the Distogram?\n- Do you include the time cost of ESM when compute time cost in Sec 4.2?\n- Does the comparison in Sec 4.2 include the gradient backward time?\n- Are there standalone efficiency comparsion (compared with the ones without kernels) for the two optimized kerenls?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8942/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758070221,
            "cdate": 1698758070221,
            "tmdate": 1699637125535,
            "mdate": 1699637125535,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "55OKfoUO5e",
                "forum": "SjgfWbamtN",
                "replyto": "8DQ8UNAOgL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8942/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their comments. We have addressed each point below.\n\n> The prediction process is faster, but final performance significantly decreases.\n\nWe agree that there is a performance decrease compared to ESMFold, but it\u2019s important to note that the model we used only had 10 layers and was therefore at a disadvantage compared to ESMFold\u2019s 48 layers. We have added a 40 layer model for a closer comparison. This model results in an improvement in performance which reduces the gap to under 3%. Longer training, and matching the full 48 layer model should bring the numbers very close, which we plan on exploring. Comparisons to the AlphaFold2 numbers are less meaningful as they utilize MSA\u2019s as input, and we do not.\n\n> Removing IPA is disadvantageous, as the structure module is less costly than Evoformer.\n\nFor the applications mentioned (virtual screening), it\u2019s not clear that obtaining the exact coordinates is necessary. Thus the question we posit is whether the structure module improves performance or is rather decoding the distogram from the Evoformer. This is in line with the ablation studies in the AlphaFold2 paper which showed that replacing the IPA with a single projection was sufficient for decoding (see section 1.13 in the AlphaFold2 supplementary material). Here we wondered if we would lose performance by removing such projection too, and our results show that coordinate recovery from predicted distograms is very effective. This realization strategy is not meant to be used in practice. Instead it is used to show that the distogram has indeed learned a valid structure.\n\n> Kernels are implemented with OpenAI's Triton, not CUDA; a full-page explanation is unnecessary due to well-known engineering improvements.\n\nWe felt it was important to highlight how we used triton to achieve this, as it involved important design decisions that are non-trivial, and beyond the use of the Triton language. This is particularly relevant for the feed-forward kernel which could be implemented in several different ways, with different trade-offs. This being said, we agree that some of the details of the analysis could be moved to the appendix, which we did. Thank you!\n\n> The analysis of kernels is wrong. For example, \"This reduces the number of reads from 5 to 1, and the number of writes from 4 to 1\". The Wx and Vx are matrix multiplication operators, which will call GEMM kernels, thus these read/write cannot be merged. We usually can only save the read/write times for element-wise operators.\n\nThe part that only requires a single read and write is meant to refer to entries of the input x which has shape B x N x N x D, where B is the batch size, N the sequence length and D the hidden dimension. The input is what dominates the memory usage and need only be read once and written back once per B x N x N vector of dimension D. Reads and writes can be merged because the operations are in fact elementwise with respect to the first three dimensions, so we are computing Wx_bij and Vx_bij and apply the subsequent operations while Wx_bij and Vx_bij are still in SRAM. We\u2019re not sure which GEMM kernels the reviewer is referring to here, and how they affect this analysis.  Based on our design, Triton generates a custom kernel, which is able to load data just once. The fact that we are doing matrix multiplication within the kernel does not make it a standard GEMM kernel. Please let us know if we missed part of the point being made here.\n\n> The method relies on a computationally demanding pretrained protein language model; simplification would be beneficial.\n\nIt turns out that the throughput of the language model, despite its size, is excellent. In fact, when looking at throughput the language model only covers 1% of the total compute time, compared to 82% for the Evoformer and 17% for the structure module at a sequence length of 256. Thus we did not see any reason to focus our efforts on the language model.\n\n> Coordinate recovery omits chirality consideration, potentially negatively impacting performance.\n\nWe trained a simple chirality flipped predictor using just the torsion angle along the C-alpha trace, and found that we could identify full flips with near perfect accuracy. The intuition here is that alpha helices are almost always right-handed, which is easily detectable.\n\n> In-depth analysis of uncertainty estimation technique is needed for better understanding of robustness.\n\nWe have added more results on the uncertainty measurements, including a comparison between distogram entropy and a pLDDT predictor (see figure 4).\n\n> I notice there are confidence scores, but where you do inject randomness to generate a distribution?\n\nWe do not inject any randomness. Since the distance prediction task is a classification over bins, we can compute a categorical distribution over the bins. The confidence score is then the negative entropy of this distribution averaged over every pair."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732486642,
                "cdate": 1700732486642,
                "tmdate": 1700732486642,
                "mdate": 1700732486642,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ICGfdkYzd8",
                "forum": "SjgfWbamtN",
                "replyto": "8DQ8UNAOgL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8942/Reviewer_NKzF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8942/Reviewer_NKzF"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Author Rebuttal"
                    },
                    "comment": {
                        "value": "In the discussion about kernel analysis, it appears that the authors may not have extensive expertise in high-performance computing. Regardless of the method employed, be it Triton or CUDA, merely combining everything into a single kernel does not ensure that the operators are effectively fused to minimize the intermediate global memory write/read costs. Kernel fusion is typically applied to element-wise operators, which are constrained by memory bandwidth rather than computational capacity. Conversely, matrix multiplications are computationally intensive and generally utilize highly-optimized GEMM kernels to achieve peak performance. Fusing these GEMM kernels with element-wise operators and saving the intermediate memory cost is a complex task, even for a CUDA expert. This task's complexity is amplified in automated translation scenarios with Triton. If the authors themselves struggle with manual kernel optimization, relying on Triton for automatic, efficient kernel fusion seems overly optimistic, casting doubt on the credibility of their analysis results."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700742594382,
                "cdate": 1700742594382,
                "tmdate": 1700742594382,
                "mdate": 1700742594382,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]