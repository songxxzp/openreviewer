[
    {
        "title": "REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes"
    },
    {
        "review": {
            "id": "OSzf3qCUHO",
            "forum": "Gf15GsnfTy",
            "replyto": "Gf15GsnfTy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission181/Reviewer_Athy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission181/Reviewer_Athy"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies a factorized Q-learning approach to solve continuous control problems. The approach builds on the recent Decoupled Q-Networks agent and makes two extensions: (1) critic ensembling to mitigate value variance, (2) a regularization objective to mitigate credit assignment issues stemming from exploratory sub-actions. The resulting REValueD agent improves learning efficiency as measured by gradient step count on common DeepMind Control Suite tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tThe ensembling and regularization objective are natural extensions to the DecQN agent \n-\tThe resulting REValueD agent yields strong performance on the tasks being studied\n-\tAblations on individual components highlight the benefit of each addition in isolation\n-\tThe tabular FMDP experiment in Appendix G is a nice addition\n-\tThe paper is well-motivated, structured and written"
                },
                "weaknesses": {
                    "value": "-\tThe results focus on DeepMind Control Suite tasks from **proprioceptive inputs**. The primary baseline additionally evaluated on MetaWorld, Control Suite from vision, and an Isaac Gym-based locomotion task with a single set of hyperparameters (some minor variations). The results presented here would benefit from additional evaluations on more diverse environments.\n-\t**Control Suite performance** (Figure 1): the results would benefit from displaying reference performance of a strong recent continuous actor-critic agent. The benchmark results (throughout) are provided with \u201cGradient steps\u201d on the x-axis, which deviates from the conventional \u201cTime steps\u201d metric. Readability would be improved by adding Figure 1 with \u201cTime steps\u201d to the Appendix and providing the necessary conversion values between \u201cGradient steps\" and \"Time steps\u201d (e.g. gradient steps per time steps ratio).\n-\t**Increasing discretization** (Figure 2 & 4): the robustness of REValueD over increasing discretizations is impressive. Figure 6 of the DecQN paper showed approximately the same performance on both Dog-Walk and Finger-Spin when using 3 vs 21 bins \u2013 do you have an intuition for where this difference is coming from (Figure 6 of DecQN yields ~950 on Finger-Spin and ~850 on Dog-Walk for 21 bins). Is this in part due to the hyperparameter modifications over the original DecQN agent mentioned in Appendix B?\n-\t**Stochastic environments** (Figure 3 & 5): the DecQN paper also introduced a distributional DecQN agent based on the C51 critic in Appendix I with experiments in stochastic environments in Appendix J. Distributional critics can be viewed as an alternative method to ensembling in accounting for variability resulting from exploratory sub-actions, potentially side-stepping the computational overhead of explicit ensembles. Discussion and/or experimental comparison between DecQN+C51 or a distributional version of REValueD and ensembled REValueD would further strengthen the paper (this was briefly mentioned in the conclusion).\n-\tA potential downside of ensembles is the **computational overhead** required for training. It would be interesting to see how the plots compare when plotting time on the x-axis. While Appendix B notes that there is \u201cminimal slowdown\u201d for relatively small ensemble sizes, it would be helpful to quantify this."
                },
                "questions": {
                    "value": "-\tHow would REValueD ensembling compare to distributional DecQN?\n-\tHow was the ensembling set up to ensure efficiency / what is the computational overhead?\n-\tWhat was the benefit of altering certain hyperparameters (exploration decay, Polyak averaging) and are these sufficient to explain the performance mismatch between DecQN in Figure 2 & 4 (REValueD) vs Figure 6 (DecQN)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission181/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission181/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission181/Reviewer_Athy"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission181/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697993694975,
            "cdate": 1697993694975,
            "tmdate": 1699635943641,
            "mdate": 1699635943641,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mWtxwPcf8C",
                "forum": "Gf15GsnfTy",
                "replyto": "OSzf3qCUHO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission181/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission181/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Athy"
                    },
                    "comment": {
                        "value": "**Further experimental results**\n\nThank you for your valuable feedback on the scope of our evaluations. We agree that expanding the range of environments tested would provide a more comprehensive understanding of REValueD's effectiveness. The current focus on DeepMind Control Suite tasks from proprioceptive inputs was a starting point to establish the method's performance. We acknowledge the importance of testing in more diverse environments as these could offer insights into the versatility and robustness of our approach under varying conditions. As such, we have added results for DecQN and REValueD in a selection of Metaworld tasks (Appendix J). We see that REValueD still maintains superior performance over DecQN in these tasks, generally with smaller variance. \n\n**Control suite performance**\n\nFor the metric used in our results, we agree that using 'Time steps' instead of 'Gradient steps' on the x-axis would align better with conventional standards and enhance readability. We have made the necessary adjustments to the figures to display 'Time steps' (the ratio was 5 env steps per update for all non-quadruped environments, in which case we used 10).\n    \nRegarding the inclusion of a continuous actor-critic agent as a baseline, our focus is on discrete action spaces in this study. Particularly in the context of DecQN and REValueD, the focus was driven by the specific challenges and characteristics of high-dimensional discrete action spaces. Since continuous control isn't the primary focus of our work, we believe that adding a continuous actor-critic baseline might not directly contribute to the comparative analysis intended for our research. However, we appreciate the suggestion and will consider it for potential future studies where a comparison with continuous control methods would be more relevant.\n\n**Increasing discretisation and sensitivity to hyper-parameters**\n\nThank you for pointing out this discrepancy between the performance in Fig 6 of Seyde et al. and Figs 2,4  of our paper.  In our revised paper, we have now plotted figures with environmental interactions on the x-axis which allows a more direct comparison.  For dog-walk, after 1M environment interactions the score in Seyde et al. is ~650 and in ours it is ~550.  This discrepancy appears to be the result of poor performance on a single seed in which our DecQN was unable to learn past a score of ~50.  This can be seen in Fig 2 in which the variance of scores is uncharacteristically high.  Removing this seed from our results, we average ~650, more in line with Seyde et al.  \n    \nFor finger-spin, after 1M environment interactions the score in Seyde et al. is ~850 and in ours it is ~750.  In this instance, we don\u2019t believe this is the result of an \u201coutlier\u201d seed (variance in scores is low), rather this may be due to slight difference in implementation and/or hyperparameter adjustments as you have suggested. \n\nAs for the reason for the differences in changing the hyper-parameters, the simple answer is that when we initially started experimenting with REValueD, this is how our existing code base was implemented. We did test to see whether they effected performance, but there was no significant changes between the implementations.\n\n**Computational overhead**\n\nYour suggestion to include timing results is indeed pertinent and would offer a more comprehensive understanding of the practical implications of using ensembles, particularly in terms of training time.\n    \nIn our manuscript, though we cited a paper which claimed 'minimal slowdown' for relatively small ensemble sizes, we agree that providing empirical data to quantify this would be significantly beneficial. To address this, on the same machine we ran 3 seeds each for walker-walk and dog-walk, for DecQN and REValueD. Each seed ran for 500k env interactions (100k updates). We have included the results in Table 3, Appendix C. We observe around a 25\\% slow down using REValueD, but it is important to consider that REValueD is more sample efficient than DecQN and so less environment interactions are needed to achieve a superior performance. We have also changed the wording from 'minimal' to 'minor' \n\n**Comparison with distributional DecQN**\n\nYou make an important point about distributional critics being an alternative to ensembles in addressing variability due to exploratory sub-actions, and their potential advantage in computational efficiency.\n\nWhilst we briefly mentioned the potential of distributional reinforcement learning in our conclusion, we agree that incorporating a more detailed discussion and experimental comparison would strengthen our paper. We have added a comparison between DecQN with a distributional critic in Appendix L, finding that REValueD maintains its superior performance."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission181/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699918710580,
                "cdate": 1699918710580,
                "tmdate": 1699918710580,
                "mdate": 1699918710580,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UrimDjRUSh",
                "forum": "Gf15GsnfTy",
                "replyto": "mWtxwPcf8C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission181/Reviewer_Athy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission181/Reviewer_Athy"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your replies! The added experiments and evaluations make the paper more well-rounded. I still think that a conventional continuous control actor-critic method would provide a valuable reference for readers to better assess the strong performance of the method. All in all a nice approach with strong results on the tasks considered!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission181/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505587759,
                "cdate": 1700505587759,
                "tmdate": 1700505587759,
                "mdate": 1700505587759,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yJkBbHulg5",
            "forum": "Gf15GsnfTy",
            "replyto": "Gf15GsnfTy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission181/Reviewer_bJ1N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission181/Reviewer_bJ1N"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes REValueD to enable Q-learning algorithms on tasks with high-dimensional discrete action space. With the same concept of value decomposition, REValueD improves DecQN by mitigating the target variance with an ensemble of critics and mitigating the effects of exploratory actions with a regularization loss. Experiments on the DeepMind Control Suite tasks show that REValueD consistently outperforms DecQN and another baseline."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThis paper is well-written and well-structured. Although the ideas of the ensemble and regularization is incremental, they are straightforward and make sense.\n2.\tThe theoretical analysis shows that REValueD could reduce the target variance of DecQN while maintaining the expectation of the target difference of DecQN unchanged with the help of the ensemble technique.\n3.\tThe experiments demonstrate the effectiveness of the proposed method and ablation studies are given to further validate each component of REValueD."
                },
                "weaknesses": {
                    "value": "1.\tIn the experiments, only two baselines (DecQN and BDQ) are compared although the authors list several works in the related works.\n2.\tAn introduction and analysis of the action pace and sub-action space in humanoid and dog tasks may help the readers understand the setting and motivation more clearly."
                },
                "questions": {
                    "value": "1.\tIn Equation (3.1), the value-decomposition form is the direct sum operator and REValueD follows this form, why do the authors not use other value-decomposition forms such as weighted sum?\n2.\tCould the authors explain more about the design of |\u03b4i|? Why \u201cfor large |\u03b4i|, the reward/next state is likely influenced by the effect of other sub-actions.\u201d?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission181/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission181/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission181/Reviewer_bJ1N"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission181/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698218277235,
            "cdate": 1698218277235,
            "tmdate": 1699635943550,
            "mdate": 1699635943550,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K4a27SFpIP",
                "forum": "Gf15GsnfTy",
                "replyto": "yJkBbHulg5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission181/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission181/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer bJ1N"
                    },
                    "comment": {
                        "value": "**Choice of value decomposition form**\n\nIn our study, we focused on the DecQN value-decomposition, which utilises the mean operator. This choice was driven by our specific interest in exploring and building upon the DecQN framework. While other forms of value-decomposition, such as the  weighted sum, offer interesting alternatives, our study aimed to investigate the implications of the DecQN approach in particular. We acknowledge that other decomposition forms could lead to different  insights and outcomes, and agree that exploring these alternatives would be a valuable avenue for future work.\n\n**Description of action and sub-action spaces for dog/humanoid**\n\nWe have added a brief description of the action and sub-action spaces in the humanoid/dog in Appendix C, under environment details. We have elaborated on the unique challenges posed by the humanoid and dog tasks in the DeepMind Control Suite, such as the large number of joints (sub-actions) that need to be controlled and the intricate interdependencies among these joints. This will provide readers with a clearer understanding of the setting and underline the motivation behind our approach, particularly highlighting why these tasks are well-suited for evaluating the performance of REValueD and the implications of our method on managing such complex action spaces.\n\n\n**Design of $|\\delta_i|$**\n\nRegarding the design of $|\\delta_i|$ and its implications, we appreciate the opportunity to clarify this aspect. The concept of $|\\delta_i|$ is central to our approach in handling the credit assignment issue. The rationale behind the design is that a large $|\\delta_i|$ represents the difference between the current and target utility values for a sub-action. Therefore, a large $|\\delta_i|$ might indicate significant impact from other sub-actions or external factors. This is because in complex environments, especially with multiple sub-actions, the effect of an individual sub-action is often intertwined with others. Therefore, when $|\\delta_i|$ is large, it indicates that the reward or the state transition is likely not solely determined by the sub-action in question, but rather is influenced by a combination of several sub-actions.\n\n**Baselines**\n\nWhilst we only compared to two main baselines in the main paper, we have added in the revised version comparisons to BDQ equipped with an ensemble, as well as DecQN equipped with a distributional critic (DecQN-C51), in Appendix K and L, respectively."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission181/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699918911999,
                "cdate": 1699918911999,
                "tmdate": 1699918911999,
                "mdate": 1699918911999,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XvB0JIOuxT",
                "forum": "Gf15GsnfTy",
                "replyto": "hxfn1VTyNh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission181/Reviewer_bJ1N"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission181/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission181/Reviewer_bJ1N"
                ],
                "content": {
                    "comment": {
                        "value": "The reviewer appreciates the authors' response. My main concerns are addressed including the baselines and descriptions of the action space. I would like to maintain my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission181/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631280602,
                "cdate": 1700631280602,
                "tmdate": 1700631280602,
                "mdate": 1700631280602,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "frtKoVXrkS",
            "forum": "Gf15GsnfTy",
            "replyto": "Gf15GsnfTy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission181/Reviewer_fnTY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission181/Reviewer_fnTY"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies RL in large discrete action space with factorization structure. Building upon the value decomposition idea proposed in past work, this paper presents some theoretical analysis which motivates the use of an ensemble of critics to control the variance as well as a new regularization term for better coordination over sub-actions. Experiments on discretized version of DeepMind control suite shows competitive performance."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Writing is clear. \n- Strong empirical results showing better performance than the baselines considered. \n- Ablation studies show the effect of regularization term and ensemble size, which is helpful for future readers who may want to adopt the proposed method."
                },
                "weaknesses": {
                    "value": "- The theoretical analysis on \"DecQN target estimation bias and variance\" should be made more rigorous, and some of the conclusions in this paper seems to contradicts previous results in an uncited related work. \n- The justification for the \"Regularized value-decomposition\" seems more of a heuristic rather than a formal justification. \n- The authors should consider adding a few missing baselines to the main experiment in Figure 1 for a fairer comparison\n\nThese are elaborated in Questions below."
                },
                "questions": {
                    "value": "- Bias and Variance\n  - In Tang et al. 2022, the authors showed that using Q decomposition may **increase bias** and **decrease variance**. In theorem 1 of this paper, the authors claim using Q decomposition may **decrease bias** and **increase variance**. These results seem to contradict each other. Could you clarify any difference in the analyzed settings? \n  - Eqn (3.1) is valid to write since we are defining the parameterization of the Q-function $Q_{\\theta}$, however, in Eqn (4.2) for the term related to $U_i^{\\pi_i}$, the decomposition of the true Q-function $Q^{\\pi}$ might not exist, as discussed in Tang et al. 2022. Could you elaborate what assumptions is the current analysis operating under? \n\n- Fairer comparisons with baselines\n  - The proposed method appears to be modifying DecQN to incorporate (1) ensemble and (2) regularization. In Fig 1, REValueD should be compared to ensembled DecQN and ensembled BDQ for a more \"apples-to-apples\" comparison. \n\n- Other questions: \n  - In Fig 2, why does BDQ perform well on this task but not the tasks in Fig 1? Would BDQ with ensemble perform even better? \n  - Minor presentation suggestion: in Table 2, can you bold best results for each task to make it easier to read? \n\n- References:\n  - Tang et al. Leveraging Factored Action Spaces for Efficient Offline Reinforcement Learning in Healthcare. NeurIPS 2022. \nhttps://openreview.net/forum?id=wl_o_hilncS"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission181/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698512676168,
            "cdate": 1698512676168,
            "tmdate": 1699635943468,
            "mdate": 1699635943468,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qacro4K5Ej",
                "forum": "Gf15GsnfTy",
                "replyto": "frtKoVXrkS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission181/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission181/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer fnTY"
                    },
                    "comment": {
                        "value": "**Comparison with Tang et al. 2022**\n\nThank you for bringing this piece of research to our attention, which we have now included in our Related Work section.  While both studies operate in the realm of Q-function decomposition within factored action spaces, there are two key distinctions with respective to their theoretical analyses.\n    \nFirst and foremost, our analysis focuses on the bias/variance of the error between the true and (function) approximated Q-values, whereas Tang et al. focus on the fundamental properties of the Q-values themselves. The outcomes of our respective analyses reflect how these related, but distinct, attributes change when moving from the standard Q-learning approach to one based on value-decomposition, and hence these analyses can be thought of as complementary rather than contradictory.  \n    \nSecond, there is a minor difference in the value-decomposition itself.  We use the decomposition proposed by Seyde et al., which involves taking a _mean_ of utility values to approximate the global Q-values, whereas Tang et al. take the _sum_ of utility values.  For completeness, we have added a section in Appendix B that analyses this sum variant under our framework, and we find this time that both the bias and variance are higher.  We could of course mirror Tang et al\u2019s. analysis using the mean instead of the sum, but this is beyond the intended scope of our paper which is interested in the error as stated above.\n\nIn summary, the apparent discrepancies concerning variance can be attributed to varying methodologies and distinct theoretical frameworks, which when taken together provide complementary perspectives and insights into Q-learning in FMDPs using value-decomposition.\n\n**Q-value decomposition**\n\nOur analysis assumes that the Q-function can be decomposed for practical approximation, an approach extended to the true Q-function in Equation (4.2). While, as Tang et al. 2022 highlight, such decomposition may not perfectly align with the true Q-function in practice, we posit that it offers a close enough approximation for the purposes of reinforcement learning, where exact representations are often unattainable. This assumption underpins our theoretical framework and the development of REValueD. The strong empirical results, acknowledged by the reviewer, provide further evidence that this decomposition approach is adequate in many practical tasks.\n\n**Performance of BDQ**\n\nThank you for pointing out that a more like-for-like comparison of BDQ would be to compare to BDQ with an ensemble. In Appendix K we have added some comparisons with BDQ-ensemeble in the environments where the bin size is 3, and in the variants with larger bin sizes, and find that the performance remains largely unchanged when equipping BDQ with an ensemble. \n\nAs for the performance of BDQ in Figures 1 vs. 2, it is important to note that the x-axis scale (also note as per request of reviewer Athy we have changed x-axis scale from updates to environment interactions) changes slightly between Figures 1 and 2 for the dog-walk task. In Figure 1 we cut the figure off after 750k environment interactions, at which point BDQ obtains a score of ~500, whereas BDQ in Figure 2 is achieving approximately this score after 1M env interactions for $n = 10, 30$, so it is taking more environment interactions to achieve the same score when using a higher bin size, though the performance does not diminish as quickly as DecQN when using a higher bin size. \n\n**Presentation suggestions**\n\nRegarding your suggestion for Table 2, we agree that bolding the best results for each task would significantly enhance readability and quick comprehension of the table. We have incorporated this in the revised version of the manuscript."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission181/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699919312903,
                "cdate": 1699919312903,
                "tmdate": 1699919312903,
                "mdate": 1699919312903,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]