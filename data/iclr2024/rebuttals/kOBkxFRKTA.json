[
    {
        "title": "Dynamic Sparse Training with Structured Sparsity"
    },
    {
        "review": {
            "id": "qDmxGc30Jj",
            "forum": "kOBkxFRKTA",
            "replyto": "kOBkxFRKTA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6355/Reviewer_HJFn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6355/Reviewer_HJFn"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a dynamic sparse training method with constant fan-in constraint. The method is validated on several computer vision datasets and architectures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The introduced sparsity pattern is quite flexible and allows achieving performance close to unstructured sparsity while being more hardware-friendly. Neuron ablation procedure looks very sound and improves noticeably performance of the method. The appendix involves theoretical analysis motivating the good optimization behavior of the proposed SRigL and the choice of constant fan-in sparsity pattern. \n\nMethod is validated on several models and attains pretty good performance at high sparsity.\n\nThe algorithm is simple and leads to speedups even without the need to write highly-customized code."
                },
                "weaknesses": {
                    "value": "I think the proposed SRigL with constant fan-in sparsity should be compared to a similar sparse training procedure with N:M sparsity. One could perform it as follows: with some interval, a fraction of weights with the smallest magnitude among the group of M consecutive weights is dropped and the same fraction with high gradient magnitude is regrown. Indeed, the difference from original RigL is that the space of possible updates is much more constrained - i.e. for 2:4 sparsity one can prune a single non-zero weight inside the group of 4 and regrow one among the zeros. Likely, such strategy would perform poorly, but this comparison would motivate the necessity for constant fan-in sparsity. The other alternative is RigL with blocked sparsity that prunes whole groups of consecutive `block_size` weights. This sparsity pattern is known to be more CPU-speedup friendly compared to unstructured sparsity. \n\nMethod lacks comparison with dedicated inference engines that leverage sparsity. DeepSparse engine [1] achieves significant speed-ups for unstructured sparsity, especially on Intel CPUs. \n\n*Minor* In the Appendix E plots w/o ablation seem to be absent on Figure 7. \n\n---\n[1] https://github.com/neuralmagic/deepsparse"
                },
                "questions": {
                    "value": "Wall-clock timings for structured sparsity look suspicious. Does it mean that N:M sparsity with higher sparsity may be slower than the one with lower sparsity? Indeed, it is hardly possible to achieve linear speed-up, but I would expect at most saturation for high N:M sparsity. \n\nHow well does the method perform when combined with quantization? One is expected to achieve even higher speed-ups for sparse+8bit quantized model.  \n\nHow much does the method explore new connections compared to RigL? Would be interesting to compare some measure similar to In-Time Over-Parameterization, introduced in [1] for RigL and SRigL. \n\n---\n[1] https://arxiv.org/abs/2102.02887"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6355/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6355/Reviewer_HJFn",
                        "ICLR.cc/2024/Conference/Submission6355/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6355/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698342934152,
            "cdate": 1698342934152,
            "tmdate": 1700330828774,
            "mdate": 1700330828774,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uj1DqCsJpL",
                "forum": "kOBkxFRKTA",
                "replyto": "qDmxGc30Jj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6355/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6355/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to HJFn (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your review. \n\n> I think the proposed SRigL with constant fan-in sparsity should be compared to a similar sparse training procedure with N:M sparsity. One could perform it as follows: with some interval, a fraction of weights with the smallest magnitude among the group of M consecutive weights is dropped and the same fraction with high gradient magnitude is regrown. Indeed, the difference from original RigL is that the space of possible updates is much more constrained - i.e. for 2:4 sparsity one can prune a single non-zero weight inside the group of 4 and regrow one among the zeros. Likely, such strategy would perform poorly, but this comparison would motivate the necessity for constant fan-in sparsity. The other alternative is RigL with blocked sparsity that prunes whole groups of consecutive block_size weights. This sparsity pattern is known to be more CPU-speedup friendly compared to unstructured sparsity.\n\nIt has been previously demonstrated that N:M sparsity can be learned without significant generalization performance degradation in the dense-to-sparse DST paradigm. Further, you are correct that as M decreases, model performance tends to degrade. At the limit, unstructured sparsity takes M to equal an entire layer of weights. We fall somewhat in between, with constant fan-in effectively being an N:M sparsity pattern where M is the dense fan-in and N is the sparse fan-in. \n\nWhile we agree that this would be an interesting experimental setup, we have motivated why we believe constant fan-in is worth exploring in Section 4, *even if N:M sparsity can be learned in a DST setting*.  To briefly summarize the noted benefits of constant fan-in vs. N:M sparsity: \n* Flexibility of constant fan-in to arbitrary sparsity ratios\n* Enabling the use of layer-wise sparsity distributions rather than a single global sparsity level\n* Hardware support for N:M sparsity is strictly limited to 2:4 sparsity at this time. However, we demonstrate speed-ups for arbitrary sparsity ratios *without* the use of any specialized hardware. \n\nIn any case, we hope to explore a wider variety of fine-grained and structural constraints in our upcoming work intended to compliment this paper. \n\n> Method lacks comparison with dedicated inference engines that leverage sparsity. DeepSparse engine [1] achieves significant speed-ups for unstructured sparsity, especially on Intel CPUs.\n\nTime permitting, we will add a comparison to DeepSparse to our updated paper prior to the end of the rebuttal period.\n\n> Minor In the Appendix E plots w/o ablation seem to be absent on Figure 7.\n\nThank you, we will update the plot in our revised paper. \n\n> Wall-clock timings for structured sparsity look suspicious. Does it mean that N:M sparsity with higher sparsity may be slower than the one with lower sparsity? Indeed, it is hardly possible to achieve linear speed-up, but I would expect at most saturation for high N:M sparsity.\n\nWe believe you are referring to Figure 4, which shows increased wall-clock timings for structured sparsity at 99% vs. 80-95%. The key consideration here is that the sparsity level refers to the overall sparsity of the network, *not the structured sparsity level*. Specifically, for the 99% sparse model, there are 482 neurons active whereas there are 344, 318, and 322 neurons active at sparsity levels 95, 90, and 80, respectively. SRigL does not enforce any specific sparsity requirements on the structural constraint, rather the structural sparsity is learned based on gamma-sal. For highly sparse models such as the 99% case here, we have found that this methodology will yield relatively fewer ablated neurons since most neurons have very few sparse weights and thus a higher percentage of those weights are likely to be considered salient, prohibiting ablation of the corresponding neuron. \n\nThis is stated in Appendix H, albeit briefly. Here\u2019s the relevant excerpt:\n\u201cThe apparent slow down in 99% structured sparse benchmarks compared to other sparsities is due to the fact that SRigL ablates fewer neurons at 99% sparsity. At extreme sparsities, each  neuron has very few active weights resulting in more neurons being considered as salient by SRigL.\u201d\n\n> How well does the method perform when combined with quantization? One is expected to achieve even higher speed-ups for sparse+8bit quantized model.\n \nTime permitting, we will include an 8-bit quantized model in our wall-clock benchmarks for the revised paper. We speculate at this stage that 8-bit quantization will decrease wall-clock timings for all compressed representations examined (SRigL, structured, CSR)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6355/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245674168,
                "cdate": 1700245674168,
                "tmdate": 1700245674168,
                "mdate": 1700245674168,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h5TS3ZVSX2",
                "forum": "kOBkxFRKTA",
                "replyto": "qDmxGc30Jj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6355/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6355/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to HJFn (2/2)"
                    },
                    "comment": {
                        "value": "> How much does the method explore new connections compared to RigL? Would be interesting to compare some measure similar to In-Time Over-Parameterization, introduced in [1] for RigL and SRigL.\n\nWe measured ITOP for all networks during training. We agree that the ITOP rate is of interest and we will include an additional appendix section with these values. To summarize at this stage for the purposes of discussion:\n* For ResNet 18 and 50, we find that SRigL and RigL have very comparable ITOP rates. Overall the final ITOP value is primarily predicated on the sparsity level. Ie.., at 80% sparsity Rigl/SRigl achieve a final ITOP rate of about 0.8 and almost 1.0 at 50% sparsity. \n* For ViT, SRigL has a significantly lower ITOP rate of about 0.4 and 0.5 at 90% and 80% sparsity, respectively. In comparison, RigL achieves an ITOP rate of 0.48 and 0.74 at the same corresponding sparsity levels. We see much higher ITOP rates if ablation is disabled, suggesting that much of the parameter space that remains unexplored is present in ablated neurons. \n* For MobileNet, initial experiments suggest a 0.1 to 0.2 increase in ITOP for SRigL vs. RigL."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6355/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245693139,
                "cdate": 1700245693139,
                "tmdate": 1700245693139,
                "mdate": 1700245693139,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DO4mOwcWGI",
                "forum": "kOBkxFRKTA",
                "replyto": "h5TS3ZVSX2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6355/Reviewer_HJFn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6355/Reviewer_HJFn"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifications and comments. I read your response and had a look at the revised version of the manuscript. Since my concerns were addressed, I decide to raise the score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6355/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330812070,
                "cdate": 1700330812070,
                "tmdate": 1700330812070,
                "mdate": 1700330812070,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zQkcTPiOJe",
            "forum": "kOBkxFRKTA",
            "replyto": "kOBkxFRKTA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6355/Reviewer_qWXP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6355/Reviewer_qWXP"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Structured RigL (SRigL), an advancement in Dynamic Sparse Training (DST). SRigL combines structured sparsity with neural network training, resulting in models that are both computationally efficient and performant. The method achieves faster real-world inference times on CPUs and outperforms other sparse training techniques in various neural network architectures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) SRigL creatively combines structured N:M sparsity with a constant fan-in constraint which is from existing DST methodologies.\n\n(2) The paper provides extensive empirical evidence to show its superior performance.\n\n(3) SRigL's ability to achieve faster real-world inference times on CPUs is of paramount importance."
                },
                "weaknesses": {
                    "value": "(1) an important baseline is missing [1].\n\n(2) Lack of novelty: A combination of sparse training and N: M sparsity have been shown in previous study [1]. \n\n[1] Yin, Lu, et al. \"Dynamic Sparsity Is Channel-Level Sparsity Learner.\"  NeurIPS 2023."
                },
                "questions": {
                    "value": "(1) with comparison to [1], what's the advantages of SRigL?\n\n(2) Why AST and SR-STE is not comparable to SRigl?  it would be better to report the results of AST and SR-STE since they are very related works."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6355/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6355/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6355/Reviewer_qWXP"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6355/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676984179,
            "cdate": 1698676984179,
            "tmdate": 1699636701075,
            "mdate": 1699636701075,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TEkZyl2u7C",
                "forum": "kOBkxFRKTA",
                "replyto": "zQkcTPiOJe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6355/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6355/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to qWXP (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your review. \n\n> (1) an important baseline is missing [1]...Yin, Lu, et al. \"Dynamic Sparsity Is Channel-Level Sparsity Learner.\" NeurIPS 2023.\n\nThank you for providing this reference. Indeed, Yin et al.\u2019s observation of the existence of sparse-amenable channels matches our own. However, we explore a combination of structured with fine-grained sparsity in addition to using very different implementation approaches. For instance, Chase includes several important additional components to obtain their reported performance metrics as detailed in their ablation study (Fig. 4 of their paper). Most notably, they use a gradual pruning schedule (dense-to-sparse) and a soft memory bound. While these components improve generalization performance, they result in a higher memory footprint during training than truly end-to-end sparse training. \n\nFurthermore, we find that our work is complementary to Chase as we explore a different channel level saliency criterion (min. % saliency weights vs. UMM in Chase) and also demonstrate that pruning-amenable channels can be exploited **in addition to fine-grained sparsity**. This is an important finding in its own right as it suggests that we can obtain benefits of both structured and fine-grained sparsity, without incurring the cost of retraining or fine-tuning. \n\nFinally, it is worth noting that Chase is a *contemporaneous* work as it was published within the last four months. As per the ICLR 2024 FAQ for reviewers, we are not required to compare to Chase directly. In any case, we are happy to cite Chase in our camera-ready version to complement our work and provide further evidence for the emergence of structure during unstructured sparse training. \n\n> (2) Lack of novelty: A combination of sparse training and N: M sparsity have been shown in previous study [1].\n\nChase does **not** use N:M sparsity as far as we are aware, so we are unable to respond to this question specifically. Could you clarify which study you are referring to? \n\nHowever, sparse training has been previously demonstrated with N:M sparsity in general. As we note in Section 4, there are some specific benefits of constant fan-in sparsity over N:M sparsity that warrant further consideration:\n* Flexibility of constant fan-in to arbitrary sparsity ratios\n* Enabling the use of layer-wise sparsity distributions rather than a single global sparsity level\n* Hardware support for N:M sparsity is strictly limited to 2:4 sparsity at this time. However, we demonstrate speed-ups for arbitrary sparsity ratios *without* the use of any specialized hardware. \n\nFurther to the above, we are the first work to: investigate the constant fan-in constraint in detail, demonstrate learning a hybrid structured + fine-grained sparsity, and we provide further evidence regarding the existence of neuron ablation in unstructured DST methods. In our opinion, these contributions represent a reasonable degree of novelty to justify publication. \n\n> (1) with comparison to [1], what's the advantages of SRigL?\n\nWe include both structured and fine-grained sparsity in our method, enabling further acceleration during inference. We also developed different saliency measures for determining when to ablate neurons / channels. Finally, our method does not require setting the target % channels to prune (Sc) as the number of channels pruned is actually learned directly by SRigL. See above for more details. \n\nIn our opinion, our work compliments Chase by: 1) providing further evidence of neuron ablation in unstructured DST methods; 2) introducing the constant fan-in constraint; 3) demonstrating learning a hybrid structured + fine-grained sparsity."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6355/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244572873,
                "cdate": 1700244572873,
                "tmdate": 1700244572873,
                "mdate": 1700244572873,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lBCSmfRpV2",
                "forum": "kOBkxFRKTA",
                "replyto": "zQkcTPiOJe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6355/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6355/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to qWXP (2/2)"
                    },
                    "comment": {
                        "value": "> (2) Why AST and SR-STE is not comparable to SRigl? it would be better to report the results of AST and SR-STE since they are very related works.\n\nAlgorithms such as SET, RigL, and SRigL are sparse-to-sparse DST algorithms, whereas SR-STE and AST are dense-to-sparse algorithms. There are some important advantages of sparse-to-sparse training compared to dense-to-sparse training:\n\n* Sparse-to-sparse training has lower computational overhead as the network is trained with the sparse weights throughout training. The advantages of sparsity can be realized immediately during the training process rather than only being reserved for inference.\n* Sparse-to-sparse training has lower memory overhead since the non-active weights can be removed before training begins. This enables training of sparse models for which a dense counterpart would not be possible to represent at a fixed memory budget. As we have seen in recent years, increasing scale yields significant benefits. Sparse-to-sparse algorithms may be able to push model scale even further than the current limits.\n* SR-STE also updates the network connectivity every mini-batch as opposed to every 100 mini-batches. This adds additional computational overhead during training and requires storing the dense parameters throughout training.\n\nWe hope we have made a clear case that sparse-to-sparse algorithms have some important properties that cannot be replicated with a dense-to-sparse algorithm. As network training costs continue to grow, we believe the advantages of sparse-to-sparse training are worth continuing to explore despite recent successes in the dense-to-sparse paradigm.\n\nIn terms of reporting these results, we have endeavored to do so where possible. SR-STE results are included in Table 3 and we will add AST as well for clarity. We note that SRigL achieves results comparable to AST on Resnet50/Imagenet at 80/90% sparsity. See Table 3 of AST paper for this comparison."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6355/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244583548,
                "cdate": 1700244583548,
                "tmdate": 1700244583548,
                "mdate": 1700244583548,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pUVGVXbgmr",
                "forum": "kOBkxFRKTA",
                "replyto": "zQkcTPiOJe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6355/Reviewer_qWXP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6355/Reviewer_qWXP"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. Your response has addressed my concerns and I will keep my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6355/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652139532,
                "cdate": 1700652139532,
                "tmdate": 1700652139532,
                "mdate": 1700652139532,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0IjgaygBCe",
            "forum": "kOBkxFRKTA",
            "replyto": "kOBkxFRKTA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6355/Reviewer_UhkR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6355/Reviewer_UhkR"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for dynamic sparse training that leads to structured N:M sparsity, here realized as a constant fan-in degree for neurons. The authors modify RigL to achieve this. The proposed constant fan-in N:M sparsity can theoretically achieve faster inference speeds on new GPU chips that are produced for general consumers but can specifically accelerate this type of structure.\n\nThe authors show that the output norm variance can in expectation be reduced with constant fan-in sparsity. The authors experimentally show that their proposed SRigL leads to the similar performance as RigL while enforcing the imposed structure."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper tackles an important issue in deep learning, is well-motivated, and written in a clear fashion. The motivation for the benefits of structured sparsity are clear. The main method is simple to explain, and seems to work approximately as good as RigL.\nI have reviewed the paper before, and appreciate that the authors include a wall-clock time comparison of a fully connected layer on a CPU."
                },
                "weaknesses": {
                    "value": "I have reviewed this paper before and still remain unconvinced by the author's argumentation on the main benefits of SRigL. While the authors claim that sparse-to-sparse training such as SRigL is beneficial over dense-to-sparse methods in terms of memory usage and computational time, the evidence presented for this is marginal. While the benefit of sparse models at inference time is obvious, the benefit of sparsifying models at training time (that reach the same generalization error) should be either faster training times or lower memory footprint. The authors show no evidence that SRigL trains models faster than dense-to-sparse methods such as SR-STE. On a similar note, a real-world quantitative evaluation of the difference in memory footprint for dense-to-sparse methods (such as SR-STE) vs. SRigL would make this paper much more convincing."
                },
                "questions": {
                    "value": "For the wall-clock time comparison, it seems that the median for at least 5 runs is shown. Given that the inference time is in the microsecond range, why not show the median of millions of forward passes, to reduce noise?\n\nWhat should be the main takeaway from you showing SRigL x2 and SRigL x5 in Figure 3? It seems to be included to inflate the presentation of results of SRigL, but RigL performs the same under x2 or x5 training times. Either include RigL x2 and RigL x5, or omit both from Figure 3."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "-"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6355/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6355/Reviewer_UhkR",
                        "ICLR.cc/2024/Conference/Submission6355/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6355/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772302502,
            "cdate": 1698772302502,
            "tmdate": 1700819709458,
            "mdate": 1700819709458,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dygVYZJ8xh",
                "forum": "kOBkxFRKTA",
                "replyto": "0IjgaygBCe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6355/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6355/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to UhkR"
                    },
                    "comment": {
                        "value": "Thank you for your second review! \n\n> I have reviewed this paper before and still remain unconvinced by the author's argumentation on the main benefits of SRigL. While the authors claim that sparse-to-sparse training such as SRigL is beneficial over dense-to-sparse methods in terms of memory usage and computational time, the evidence presented for this is marginal. While the benefit of sparse models at inference time is obvious, the benefit of sparsifying models at training time (that reach the same generalization error) should be either faster training times or lower memory footprint. The authors show no evidence that SRigL trains models faster than dense-to-sparse methods such as SR-STE. On a similar note, a real-world quantitative evaluation of the difference in memory footprint for dense-to-sparse methods (such as SR-STE) vs. SRigL would make this paper much more convincing.\n\nUnfortunately, the engineering effort to realize an end-to-end acceleration of SRigL is out of the scope of this work. We have endeavored to demonstrate that, **in theory**, sparse-to-sparse methods such as SRigL can be accelerated and compressed during training. This is a tantalizing possibility as it will enable larger-sparse models to be trained that would otherwise not be possible on a fixed VRAM budget. We agree that showing this acceleration in practice is ideal; however, our goal with this work is to establish whether the proposed constant fan-in constraint negatively affects generalization performance. Having established this in this work, we are now working on demonstrating real-world end-to-end training speed-ups in an upcoming, complimentary work.\n\nAs an aside, the SR-STE repository has not been updated in over a year and we have had trouble getting the authors to clarify their build process. Notably, they did not include a requirements file or similar implementation details as required to reproduce their results (ie., they have not published code for language models, object detection, nor instance segmentation). \n\nFurthemore, while the SR-STE authors published theoretical FLOPs calculation, they did not include any runtime characteristics which may have been used for a direct comparison. \n\n> For the wall-clock time comparison, it seems that the median for at least 5 runs is shown. Given that the inference time is in the microsecond range, why not show the median of millions of forward passes, to reduce noise?\n\nWe used a blocked autorange with a minimum number of runs set to 5 for our benchmarks. For the results reported, the median number of runs was 438 and the minimum was 15. Given the relatively narrow standard deviation ranges, we believe this is sufficient to demonstrate the statistical significance of our results. \n\n> What should be the main takeaway from you showing SRigL x2 and SRigL x5 in Figure 3? It seems to be included to inflate the presentation of results of SRigL, but RigL performs the same under x2 or x5 training times. Either include RigL x2 and RigL x5, or omit both from Figure 3.\n\nThe main takeaway is that, like RigL and other DST methods, SRigL can be trained for longer to reach higher generalization performance; enabling the use of sparsities up to 90% while matching the generalization performance of a dense network. We also wanted to demonstrate how training the network for longer yields more neurons being ablated, similar to RigL\u2019s off the shelf ablation. These are salient details for the reader that are not intended to show a benefit of SRigL over RigL but rather to demonstrate that the extended training behavior is similar to RigL. We note that we explicitly state that \u201cSRigL yields **similar** generalization performance as RigL across each sparsity and training duration considered\u2026**Similar to RigL**, we observe that SRigL generalization performance improves with increasing training time\u201d. We hope that our intent is made clear by these statements.  \n\nDue to compute limitations, we were unable to train RigL for extended durations on our own code base and therefore do not include it in our plots. We do however report RigL\u2019s extended training duration results in Table 1."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6355/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244366291,
                "cdate": 1700244366291,
                "tmdate": 1700244366291,
                "mdate": 1700244366291,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bqGG34IfQA",
            "forum": "kOBkxFRKTA",
            "replyto": "kOBkxFRKTA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6355/Reviewer_LY1T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6355/Reviewer_LY1T"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents Structured RigL (SRigL), a Dynamic Sparse Training (DST) method that excels in training sparse neural networks. SRigL achieves state-of-the-art performance in DST by incorporating fine-grained N:M sparsity and continuous fan-in constraints for sparse interstructures. Through heuristic analysis and neuron removal, SRigL outperforms existing methods across various neural network architectures, demonstrating a substantial 3.6x/2x speedup on CPU at 90% sparsity compared to equivalent dense or unstructured sparse layers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The authors propose SRigL method that learns a SNN with constant fan-in fine-grained structured sparsity while maintaining generalization comparable to RigL even at high sparsity levels across various network architectures.\n* Experimental results show that the SRigL method can not only improve the efficiency of parameters and memory, but can also enable acceleration during training.\n* The proposed SRigL demonstrates minimal accuracy drop even at high sparsity levels exceeding 90% in both ResNet and ViT architectures."
                },
                "weaknesses": {
                    "value": "* The proposed SRigL primarily demonstrates its efficacy on networks with relatively high redundancy, such as ResNet or ViT. However, there is a lack of experimentation on networks with lower redundancy, such as MobileNet.\n* The experimental results are limited to vision tasks.\n* While the comparison from the perspective of Dynamic Sparse Training (DST) in Table 3 is crucial, I believe that the results regarding structured pruning performance are equally significant. However, there is a notable absence of experiments, and comparisons with other techniques with 'structured' attributes are challenging.\n* A minor concern is the absence of a comprehensive figure that provides an overview of the entire process of SRigL."
                },
                "questions": {
                    "value": "* In Section 4.4, the description of Algorithm 1 mentions, \"The algorithm to accelerate our condensed sparsity representation is shown in Algorithm 1, demonstrating its embarrassingly parallel nature.\" I'm interested in knowing the throughput on real GPU or CPU based on the sparsity levels in the matmul unit-test.\n* While discussing \"Constant fan-in sparsity,\" it occurs to me that there might be performance variations across tasks and networks depending on the information included by input features. Have there been any experiments applying this approach to tasks other than vision?\n* It is anticipated that the proposed SRigL method is significantly influenced by the $\\gamma_{sal}$ value. I think this sensitivity might be more pronounced in networks with lower redundancy, such as MobileNet. Has there been an observation of trends by sweeping through different $\\gamma_{sal}$ values? If so, what were the findings?\n* How does the application of SRigL affect the practical outcomes (latency, throughput) in the context of \"end-to-end sparse training\" on GPUs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6355/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6355/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6355/Reviewer_LY1T"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6355/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814664301,
            "cdate": 1698814664301,
            "tmdate": 1699636700759,
            "mdate": 1699636700759,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pJZannAEbr",
                "forum": "kOBkxFRKTA",
                "replyto": "bqGG34IfQA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6355/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6355/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to LY1T (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review and suggestions to improve the paper. \n\n> The proposed SRigL primarily demonstrates its efficacy on networks with relatively high redundancy, such as ResNet or ViT. However, there is a lack of experimentation on networks with lower redundancy, such as MobileNet.\n\nThank you for this suggestion. We are currently running MobileNet experiments and can confirm that initial results show RigL and SRigL have comparable performance. We will update our paper with the mobilenet results as soon as they are completed. \n\n> The experimental results are limited to vision tasks.\n\nThe goal of this paper is to introduce the constant fan-in constraint and our observations and methods related to neuron ablation. We agree that a more diverse task set is preferable; however, in our opinion the empirical evidence provided is reasonably compelling for inclusion in the conference. Future work will aim to exploit similar methods to SRigL on other tasks. \n\n> While the comparison from the perspective of Dynamic Sparse Training (DST) in Table 3 is crucial, I believe that the results regarding structured pruning performance are equally significant. However, there is a notable absence of experiments, and comparisons with other techniques with 'structured' attributes are challenging.\n\nOur primary intent was to investigate constant fan-in in the context of Dynamic Sparse Training. However, we agree this is an interesting perspective and will add a table summarizing current SoTA structural pruning methods vs. SRigL to the revised paper before the end of the discussion period. \n\n> A minor concern is the absence of a comprehensive figure that provides an overview of the entire process of SRigL\n\nWe were somewhat limited by the page limit for this initial submission but will include an overview figure in the revised figured to help clarify the high-level details of our method. \n\n> In Section 4.4, the description of Algorithm 1 mentions, \"The algorithm to accelerate our condensed sparsity representation is shown in Algorithm 1, demonstrating its embarrassingly parallel nature.\" I'm interested in knowing the throughput on real GPU or CPU based on the sparsity levels in the matmul unit-test.\n\nTo be clear, our results in Figure 4 and appendix H are on a real-world CPU. In addition, we will include GPU benchmarks in our revised paper before the end of the rebuttal deadline. We can report that our GPU implementations significantly outperform both the dense and CSR benchmarks, especially at high batch sizes and high sparsities. \n\n We are also working on benchmarking a Raspberry Pi 4 as a prototypical example of an edge device. We hope to include these experimental results in our revised rebuttal paper before the end of the discussion period.\n\n> While discussing \"Constant fan-in sparsity,\" it occurs to me that there might be performance variations across tasks and networks depending on the information included by input features. Have there been any experiments applying this approach to tasks other than vision?\n\nNot at this time. Our initial goal was to establish whether the proposed constant fan-in constraint would significantly affect generalization performance on a variety of network architectures. We intend to explore more diverse tasks in an upcoming complimentary work. \n\n> It is anticipated that the proposed SRigL method is significantly influenced by the gamma-sal value. I think this sensitivity might be more pronounced in networks with lower redundancy, such as MobileNet. Has there been an observation of trends by sweeping through different gamma-sal values? If so, what were the findings?\n\nIn our experiments we found that gamma-sal did not significantly affect generalization performance. The critical threshold was to enable ablation by requiring *a single salient weight per neuron*. However, we found that this integer limit did not transfer well between network architectures. For instance, for convolutional networks the sparse fan-in per neuron is on the order of 5-20. But for ViT the sparse-fan in is 1 to 2 orders of magnitude larger than our convolutional network experiments. Therefore, the gamma-sal hyperparameter enables us to set a threshold that takes the network topology into consideration. \n\nWe did perform sweeps on the gamma-sal value. See Appendix E, Figures 6.a), 6.b), and 7. You will note the low variance in performance w.r.t. gamma-sal for the ResNet networks. ViT has a higher variance of about 3% in final performance when higher gamma-sal values are used."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6355/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244216705,
                "cdate": 1700244216705,
                "tmdate": 1700244216705,
                "mdate": 1700244216705,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6g1SpG8jZx",
                "forum": "kOBkxFRKTA",
                "replyto": "bqGG34IfQA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6355/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6355/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to LY1T (2/2)"
                    },
                    "comment": {
                        "value": "> How does the application of SRigL affect the practical outcomes (latency, throughput) in the context of \"end-to-end sparse training\" on GPUs?\n\nSRigL can be used to improve latency / throughput during training by exploiting the structured sparsity that is included in the method. The forward / backward passes can be accelerated for 99% of mini-batches, whenever a topology update is not required. Every $\\Delta$T steps the network would need to uses the dense gradients for calculating the regrown scores. However, it is trivial to extend SRiGL to SET or other DST methods which do not require dense gradient information; in this case, every forward and backward pass can be accelerated while taking advantage of the hybrid structured + fine-grained we introduce in this work. \n\nAdditionally, the constant fan-in constraint can be used at inference time in addition to realize further decreases in latency."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6355/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244236887,
                "cdate": 1700244236887,
                "tmdate": 1700244236887,
                "mdate": 1700244236887,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DHVp1MbxIV",
                "forum": "kOBkxFRKTA",
                "replyto": "bqGG34IfQA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6355/Reviewer_LY1T"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6355/Reviewer_LY1T"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed answers and results.\n\nI believe that the proposed SRigL method can achieve acceleration not only during inference but also in the training phase by reducing computation costs. If the proposed method had been compared to RigL in terms of throughput improvement during DST execution on a GPU, I would have gladly raised my score. However, I \bthink that I did not receive sufficient answers to this question in the author's response, so I would like to keep my rating."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6355/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665362937,
                "cdate": 1700665362937,
                "tmdate": 1700665362937,
                "mdate": 1700665362937,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KCZvgDQTBg",
            "forum": "kOBkxFRKTA",
            "replyto": "kOBkxFRKTA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6355/Reviewer_43nv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6355/Reviewer_43nv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes SRigL with a type of structured sparsity - constant fan-in sparsity that can be applied to dynamic sparse training, leading to real-world clock-time savings while maintaining the optimal performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The research topic of structured DST is timely and important for the ML community. With the popularity of LTH and DST, sparse neural networks have received upsurging attention due to its promising capacity to reduce training/inference costs while maintaining the original performance of dense counterparts. However, the benefits of sparse NNs can largely constraint by the limited support from common hardware - GPUs. Research works to improve the progress of this direction make significant contributions to the community.  \n\n2. This paper provides a comprehensive and precise related work that covers the most state-of-the-art structured/unstructured sparse training approaches. I very much appreciate such rich related works that provide enough credits and credits to previous works. \n\n3. The detailed step of SRigL in Section 3 provides a good overview to understand the methodology. \n\n4. SRigL is able to find small-sparse NNs that enjoy better real-world wall-clock for online inference than structured (SRigL with only neuron ablation) and unstructured NNs. \n\n5. Compared with N:M sparsity, SRigL enjoys uniform layer-wise sparsity, which is more desirable for performance."
                },
                "weaknesses": {
                    "value": "(1) Partial of the ideas used in SRigL has some certain levels of overlaps with the previous work (Chase: https://arxiv.org/pdf/2305.19454.pdf). For instance, Chase also uncovers that a large proportion of channels (termed sparse amenable channels) tend to be sparse during DST. They also perform channel pruning to produce a mixture of structured and unstructured sparsity at the end of training. It is better to clarify the difference and similarity between Chase and SRigL, even though Chase does not introduce the hardware-friendly constant fan-in sparsity for the unstructured part. \n\n(2) Can SRigL also accelerate the training process for the online inference, with real-world wall-clock saving?\n\n(3)  In many cases in Table 1 and 3, SRigL w/ ablation even outperforms SRigL w/o ablation, which is a bit counter-intuitive. Cause SRigL w/ ablation essentially produces a smaller-sparse model if I understand correctly, which would decrease the model capacity. Can the authors elaborate more about this? Does  SRigL w/o ablation means only pruning these dean channels without weight regrowing?"
                },
                "questions": {
                    "value": "(1) What is the technical difference between SRigL and Chase? \n\n(2) Can SRigL also accelerate the training process for the online inference, with real-world wall-clock saving? Moreover, I noticed that the Constant Fan-in sparsity can be accelerated by GPUs with some custom CUDA implementation, such as Schultheis & Babbar (2023). I am wondering how difficult to make SRigL accelerated by GPUs using the CUDA implementation provided by Schultheis & Babbar (2023). \n\n(3) Why SRigL w/ ablation outperforms SRigL w/o ablation? I suppose that SRigL w/o ablation is essentially the unstructured version of RigL. \n\n(4) Since the real-world clock-time is measured on CPU. I am wondering how different it is to implement GPU kernel to support SRigL in common GPUs?\n\nOverall, I think this paper is a good asset and I am willing to increase my score if the above weaknesses can be resolved."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6355/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6355/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6355/Reviewer_43nv"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6355/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698875054903,
            "cdate": 1698875054903,
            "tmdate": 1700481432395,
            "mdate": 1700481432395,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ufWomopdbP",
                "forum": "kOBkxFRKTA",
                "replyto": "KCZvgDQTBg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6355/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6355/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to 43nv (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review and insightful questions. \n\n> (1) Partial of the ideas used in SRigL has some certain levels of overlaps with the previous work (Chase: https://arxiv.org/pdf/2305.19454.pdf). For instance, Chase also uncovers that a large proportion of channels (termed sparse amenable channels) tend to be sparse during DST. They also perform channel pruning to produce a mixture of structured and unstructured sparsity at the end of training. It is better to clarify the difference and similarity between Chase and SRigL, even though Chase does not introduce the hardware-friendly constant fan-in sparsity for the unstructured part.\n\nThank you for providing this reference. Indeed, Yin et al.\u2019s observation of the existence of sparse-amenable channels matches our own. However, we explore a combination of structured with fine-grained sparsity in addition to using very different implementation approaches. For instance, Chase includes several important additional components to obtain their reported performance metrics as detailed in their ablation study (Fig. 4 of their paper). Most notably, they use a gradual pruning schedule (dense-to-sparse) and a soft memory bound. While these components improve generalization performance, they result in a higher memory footprint during training than truly end-to-end sparse training. \n\nFurthermore, we find that our work is complementary to Chase as we explore a different channel level saliency criterion (min. % saliency weights vs. UMM in Chase) and also demonstrate that pruning-amenable channels can be exploited **in addition to fine-grained sparsity**. This is an important finding in its own right as it suggests that we can obtain benefits of both structured and fine-grained sparsity, without incurring the cost of retraining or fine-tuning. \n\nFinally, it is worth noting that Chase is a *contemporaneous* work as it was published within the last four months. As per the ICLR 2024 FAQ for reviewers, we are not required to compare to Chase directly. In any case, we are happy to cite Chase in our camera-ready version to complement our work and provide further evidence for the emergence of structure during unstructured sparse training. \n\n> (2) Can SRigL also accelerate the training process for the online inference, with real-world wall-clock saving?\n\nIn principle yes, we can accelerate the training process by compressing our model to a structured sparse representation for the forward and backward passe for all mini-batches between $\\Delta$T. However, implementation of this acceleration has not yet been completed. We are currently exploring this in a separate work. Fine-grained sparsity in general is more challenging to accelerate on the backward pass; typically it is required that the fine-grained structured is transposable (ie.,sparsity  constraint applies equally to rows and columns of weight matrices). Since we do not not exploit a transposable constraint in this work, we cannot accelerate the fine-grained structure in practice with currently available commodity hardware. See [1] for more details. \n\n> (3) In many cases in Table 1 and 3, SRigL w/ ablation even outperforms SRigL w/o ablation, which is a bit counter-intuitive. Cause SRigL w/ ablation essentially produces a smaller-sparse model if I understand correctly, which would decrease the model capacity. Can the authors elaborate more about this? Does SRigL w/o ablation means only pruning these dean channels without weight regrowing?\n\nYour understanding is partially correct, as we ablate neurons the model becomes less wide (fewer neurons / channels). However, since we redistribute the parameters from ablated neurons to active neurons, the target sparsity and active parameters per layer remains fixed throughout the training process. So the model\u2019s capacity itself is arguably fixed, but the smaller-sparse model has more connections per neuron than the wide-sparse model at initialization. \n\nWe observed that unstructured DST methods reallocate parameters to specific neurons, without any explicit mechanism for doing so. We speculate that the increased performance from neuron ablation is due to the DST algorithm concentrating its parameter budget on highly relevant and salient features that are present within the active neurons; however, more work is required to confirm this speculation and introduce a more rigorous theoretical basis for understanding this phenomena. We hope to follow this work up with such an analysis in the future."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6355/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243555761,
                "cdate": 1700243555761,
                "tmdate": 1700243555761,
                "mdate": 1700243555761,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L723acY8My",
                "forum": "kOBkxFRKTA",
                "replyto": "KCZvgDQTBg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6355/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6355/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to 43nv (2/2)"
                    },
                    "comment": {
                        "value": ">(1) What is the technical difference between SRigL and Chase?\n\nSee above. \n\n> (2) Can SRigL also accelerate the training process for the online inference, with real-world wall-clock saving? Moreover, I noticed that the Constant Fan-in sparsity can be accelerated by GPUs with some custom CUDA implementation, such as Schultheis & Babbar (2023). I am wondering how difficult to make SRigL accelerated by GPUs using the CUDA implementation provided by Schultheis & Babbar (2023).\n\nYes, the hybrid structured / fine-grained sparsity learned by SRigL can be accelerated during training, unlike purely unstructured DST methods or typical N:M methods. Training with fine-grained structure requires transposable constraints as per [1]. During inference, we can exploit Schultheis & Babbar\u2019s (2023) GPU kernels with ease. *We are adding GPU benchmarking using these kernels to our paper and these will be available before the close of the discussion period.*\n\n> (3) Why SRigL w/ ablation outperforms SRigL w/o ablation? I suppose that SRigL w/o ablation is essentially the unstructured version of RigL.\n\nSee above for our speculation on the cause. However, we note that SoTA unstructured DST methods *already ablate neurons*; therefore, we find empirically that enabling such ablation in more structured settings such as SRigL benefits overall accuracy and runtime characteristics of the sparse network. \n\nOne point of clarification, SRigL without ablation still has the constant fan-in constraint applied to it which is a significant difference compared to RigL, which learns a purely unstructured sparse mask. \n\n> (4) Since the real-world clock-time is measured on CPU. I am wondering how different it is to implement GPU kernel to support SRigL in common GPUs?\n\nThanks to Schultheis & Babbar\u2019s (2023), we can report these timings now. See above. These kernels work on existing commodity hardware without any specialized requirements. \n\n[1] I. Hubara, B. Chmiel, M. Island, R. Banner, J. Naor, and D. Soudry, \u201cAccelerated Sparse Neural Training: A Provable and Efficient Method to Find N:M Transposable Masks,\u201d in Advances in Neural Information Processing Systems, Curran Associates, Inc., 2021, pp. 21099\u201321111."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6355/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243596159,
                "cdate": 1700243596159,
                "tmdate": 1700243596159,
                "mdate": 1700243596159,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "51HBSyJjXX",
                "forum": "kOBkxFRKTA",
                "replyto": "L723acY8My",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6355/Reviewer_43nv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6355/Reviewer_43nv"
                ],
                "content": {
                    "title": {
                        "value": "My concerns are all addressed"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nI thank the authors for the detailed and honest answers to my question. I love the fact you acknowledge what is lacking to accelerate training with SRigL. I would like to see the discussion with Chase in the camera ready version. \n\nI believe this work provides a good example for hardware-friendly sparse training. I look forward to seeing the follow-up work on training speedup. \n\nOverall, I am happy with the rebuttal. I will increase my score to 8 and push for an acceptance."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6355/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481411424,
                "cdate": 1700481411424,
                "tmdate": 1700481411424,
                "mdate": 1700481411424,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]