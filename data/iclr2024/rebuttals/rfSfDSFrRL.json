[
    {
        "title": "Gated recurrent neural networks discover attention"
    },
    {
        "review": {
            "id": "kskuEVbvmM",
            "forum": "rfSfDSFrRL",
            "replyto": "rfSfDSFrRL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5288/Reviewer_1u2u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5288/Reviewer_1u2u"
            ],
            "content": {
                "summary": {
                    "value": "This work analyzes recent developments in linear gated RNN/SSMs in the context of linear attention. The work shows how to construct a set of parameters in gated RNNs that can exactly implement linear self-attention. The paper also shows how LSTMs can be constructed in this way as well, but GRUs cannot. Synthetic experiments are performed that show the gated RNNs can learn the attention construction in a student-teacher setup. Experiments are then performed that show gated RNNs can find the linear attention solution when trained on an in-context learning linear regression task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Overall the paper provides an interesting analysis of the connection between gated linear RNNs and linear self-attention.\n\n- The paper makes a nice connection that shows how linear self-attention can be exactly implemented within the weights of a gated RNN (if a quadratic increase in parameters is used). The investigation into LSTMs and GRUs is also interesting.\n\n- The experiments flow nicely from showing it is possible for the gated RNNs to learn the linear attention solution in a teacher-student setup, to then showing that when trained from scratch they can also learn the solution in the linear regression task. The additional experiments related to overparameterization and nonlinearities and identification are also interesting."
                },
                "weaknesses": {
                    "value": "- Figure 1 is helpful, but the paper would benefit from also formalizing the construction in equations, either in the main paper in Section 3.1 or in the Appendix. I found myself having to stare at Figure 1 and the description in Section 3.1 longer than probably necessary, whereas I think a bit of math (in particular with dimensions clearly defined) along with the figure and description would make this much easier to see.\n\n- The experiments are demonstrative, but very toy, and have a lack of diversity. This is mostly ok for this type of paper, but it is unclear how well the results generalize. Perhaps analyzing and experimenting with additional tasks could be helpful. An additional toy task that might have been interesting is the associative recall/inductive head tasks from https://arxiv.org/pdf/2302.10866.pdf, https://arxiv.org/pdf/2212.14052.pdf, https://arxiv.org/abs/2209.11895. In particular, the H3 work also proposes a construction of how softmax attention can solve these tasks. Given that these tasks are of great interest to those studying language modeling with linear RNNs/SSMs, connecting with this prior work might broaden the audience of this work.\n\n- More discussion and analysis around some of the results would strengthen the paper. \n   - In particular, the compression result from Figure 3.B where the gated RNNs can solve the linear regression task with a size smaller than the theoretical construction size. Are there other tasks where this is not the case? E.g. perhaps the associative recall task from the point above? More analysis and experimentation around this point would strengthen the paper\n  - While potentially more difficult, I would have also appreciated more discussion, analysis, experiments around the GRU results presented in Figure 4.B, since it does so well despite not reflecting the linear attention solution. Again, perhaps an additional experiment might be insightful."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5288/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5288/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5288/Reviewer_1u2u"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5288/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698623430043,
            "cdate": 1698623430043,
            "tmdate": 1700534710480,
            "mdate": 1700534710480,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Whj1TaS9tn",
                "forum": "rfSfDSFrRL",
                "replyto": "kskuEVbvmM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5288/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their on-point feedback.\n\n-   Here is the precise description of the values the different weight matrices take. The key-values are stored in the first $d^2$ recurrent neurons and the queries in the last $d$ ones (indices $d^2+1$ to $d^2 + d$.\n    \n    - For the input gating: $W\\_\\mathrm{x}^\\mathrm{in}$ and $W^\\mathrm{in}\\_\\mathrm{g}$ are matrices of size $(d^2 + d) \\times (d+1)$ with\n    \n    $$ (W\\_\\mathrm{x}^\\mathrm{in})\\_{i,j} = \\left \\\\{ \\begin{array}{ll} {(W_V)}\\_{i/d,j} &\\text{ if } j \\leq d \\text{ and } i \\leq d^2 \\\\\\\\ {(W\\_Q)}\\_{i-d^2,j} & \\text{ if } j \\leq d \\text{ and } i > d^2 \\\\\\\\ 0 & \\text{ otherwise} \\end{array} \\right . $$\n    \n    $$ {(W\\_\\mathrm{g}^\\mathrm{in})}\\_{i,j} = \\left \\\\{ \\begin{array}{ll} {(W_K)}\\_{i\\,\\mathrm{mod}\\,d,j} &\\text{ if } j \\leq d \\text{ and } i \\leq d^2 \\\\\\\\ 1 & \\text{ if } j = d+1 \\text{ and } i > d^2\\\\\\\\ 0 & \\text{ otherwise} \\end{array} \\right . $$\n    \n    - For the recurrent neurons: $\\lambda$ is a vector of size $d^2 + d$ with\n    \n    $$ \\lambda\\_i = \\left \\\\{ \\begin{array}{ll} 1 & \\text{ if } i \\leq d^2\\\\\\\\ 0 & \\text{ otherwise } \\end{array} \\right . $$\n    \n    - For the output gating: $W^\\mathrm{out}\\_\\mathrm{x}$ and $W^\\mathrm{out}\\_\\mathrm{g}$ are matrices of size $d^2 \\times (d^2 + d)$ with\n    \n    $$ {(W\\_\\mathrm{x}^\\mathrm{out})}\\_{i,j} = \\left \\\\{ \\begin{array}{ll} 1 &\\text{ if } j \\leq d \\text{ and } i = j \\\\\\\\\n     0 & \\text{ otherwise} \\end{array} \\right . $$\n    \n    $$ {(W\\_\\mathrm{g}^\\mathrm{out})}\\_{i,j} = \\left \\\\{ \\begin{array}{ll} 1 & \\text{ if } j > d^2 \\text{ and } i = j \\\\, \\mathrm{mod} \\\\, d \\\\ 0 & \\text{ otherwise} \\end{array} \\right . $$\n    \n    For the readout matrix: $D$ is of size $d \\times d^2$ with\n    \n    $$ D\\_{i,j} = \\left \\\\{ \\begin{array}{ll} 1 & \\text{ if } i = j / d\\\\\\\\ 0 & \\text{ otherwise } \\end{array} \\right . $$\n    \n    We will include this in the appendix of the next version of the paper.\n    \n-   The associative recall task is a great suggestion. We included promising preliminary results on this task in the global response.\n    \n-   For the first point, we refer the reviewer to our global response. Regarding the second point, we agree with the reviewer that this is a very intriguing result. We have been investigating the reasons behind this performance but so far remained unsuccessful. We believe that this is an interesting direction for future work.\n    \n\nWe believe the new experimental results we provide following the reviewer advice greatly improve the paper and we hope that they may convince the reviewer to reconsider their score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700210868652,
                "cdate": 1700210868652,
                "tmdate": 1700210868652,
                "mdate": 1700210868652,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ec03H0m8YY",
                "forum": "rfSfDSFrRL",
                "replyto": "Whj1TaS9tn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5288/Reviewer_1u2u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5288/Reviewer_1u2u"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the clarification and the additional associative recall experiment and the new analysis and discussions. I think this will be an interesting paper for the community to read and I have increased my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534744782,
                "cdate": 1700534744782,
                "tmdate": 1700534744782,
                "mdate": 1700534744782,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AdCt06VghU",
            "forum": "rfSfDSFrRL",
            "replyto": "rfSfDSFrRL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5288/Reviewer_pjc5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5288/Reviewer_pjc5"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a construction proof to demonstrate that gated linear recurrent units can learn linear autoregressive self-attention exactly.  The first experimental results (section 4) show that the theoretical result holds in practice: a GLRU network trained as the student to a linear self-attention network learns to imitate its teacher with vanishingly small error.  The second experimental result is more interesting: it shows that, when LARSA and GLRU are taught using exactly the same in-context linear regression data, they take exactly the same gradient updates."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The title of the original Transformer paper (Attention is All You Need) suggested that the Transformer is nothing more or less than a more pathlength-efficient implementation of the same set of functions that an RNN can learn.  The exact nature of the near-equivalence between Transformers and RNNs has been harder to describe than that simple first title suggested.  This paper's experimental results on the gradient update for the in-context linear regression problem are a demonstration of the closest link between Transformers and GLRUs that I have seen yet."
                },
                "weaknesses": {
                    "value": "My enthusiasm is tempered by the rather extreme limitations placed on both the Transformers and the GLRUs in this paper.  Linear self-attention is far less powerful than softmax self-attention, and as demonstrated in this paper, linear gated recurrence is less powerful than nonlinear gated recurrence, so a proof of equivalence between them, while of some theoretical interest, doesn't seem to be of very high impact."
                },
                "questions": {
                    "value": "Is there any reason to believe that the demonstrated equivalence would continue to hold for neural nets that include nonlinearities?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5288/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5288/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5288/Reviewer_pjc5"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5288/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760663927,
            "cdate": 1698760663927,
            "tmdate": 1699636529217,
            "mdate": 1699636529217,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DE9TMrxRVd",
                "forum": "rfSfDSFrRL",
                "replyto": "AdCt06VghU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5288/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s valuable time in reviewing our paper and would like to nuance their point of view on the limitations imposed on the two architectures we consider.\n\n- As argued in the global response, there is a growing literature showing how extensions of linear self-attention layers can reach, if not surpass, Transformers-level performance. The interest of linear self-attention thus goes beyond its amenability to theoretical analysis.\n- While we provide a theoretical construction for the linear case in the main text, we show how this still holds when the gating is nonlinear in Appendix A.1. We agree with the reviewer that it makes learning more difficult, as shown in Figure 4.A. However, we think that it still helps understand the inductive bias behind architectures such as LRUs, by showing that they are more akin to reproducing linear self-attention like behaviors than other types of RNNs.\n\nWe made clearer the reasons why we think our paper can have a great impact in the global response. We encourage the reviewer to consider those arguments and hope they may convince them to change their score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700209270737,
                "cdate": 1700209270737,
                "tmdate": 1700209270737,
                "mdate": 1700209270737,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DNUcz0NeHA",
            "forum": "rfSfDSFrRL",
            "replyto": "rfSfDSFrRL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5288/Reviewer_imQt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5288/Reviewer_imQt"
            ],
            "content": {
                "summary": {
                    "value": "The paper takes a theoretical and empirical study to relate RNNs and attention models. The authors first show a theoretical construction that simulates a single linear attention head using a gated RNN. The idea behind the construction is simple, gated RNNs accumulate key-value matrix products at each time step and use an output gated unit to compute the output using the accumulated products and the queries at each step. However, such a construction requires $O(d^4)$ parameters to simulate a $3d^2$ parameter linear attention.\n\nInterestingly, in multiple numerical experiments to mimic linear attention, the authors still observe that such over-parametrization in gated RNNs is necessary to simulate linear attention. The authors conduct multiple structural probing experiments on trained gated RNNs to find the simulation of their construction. Furthermore, they show that existing RNN-based architectures fail to properly mimic linear attention. The authors end with interesting in-context experiments on linear regression and showcase differences in mechanisms of different RNN-based architectures."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The main strength of the paper lies in its clinical approach to connecting RNNs and attention models, which is an important question to understand for architecture design. It is an interesting approach to have a theoretical construction to understand the importance of gates in RNN models. Furthermore, the role of over-parametrization for such models has been pointed out by their theoretical construction and empirical experiments. \n\nIn addition, the in-context experiments on linear regression provide two significant observations for any future work to follow, (a) gated RNNs can simulate one step GD with even fewer neurons, and (b) other sequence-to-sequence models can perform the same task but without necessarily mimicking the behavior of one-layer attention. Thus, I believe this paper opens up interesting questions for mechanistic interpretability."
                },
                "weaknesses": {
                    "value": "The main weakness of this paper lies in its slightly difficult presentation of experimental details. Here, I point out some of the difficulties that I faced when reading this paper. I additionally pose a few questions that I believe might strengthen the authors' claims.\n\n(a) There are many experimental statements whose details aren't clear from the current version.\n\n1. \"First, we observe that only perfect memory neurons ($\\lambda = 1$) and perfect forget neurons ($\\lambda = 0$) influence the network output.\" \n \nIn Figure 2, \" Only recurrent neurons with perfect memory (\u03bb = 1, dark blue) or no memory at all (\u03bb = 0, light grey) influence the output, consistently with the theory.\" \n\nHow do the authors verify this? Is this related to the pruning experiment that the authors conduct later, where they remove the neurons with any other $\\lambda$ values?\n\n2. In Figure 2, \"The block structure almost perfectly matches the one of our construction\". I don't understand the block structure that the authors refer to.\n\n3. Again in Figure 2, the statement \"For each output coordinate  ... which can be generated in a way that is coherent with the structure of our construction\" is extremely difficult to parse. \n\n4. In Table 2, what do the terms $x_i^j y _1$ for different $i, j$ even mean? Notations would help readers parse the results of the probing experiments.\n\n(b)  The experiments conducted in sections 2 and 5 are with a fixed dimension. How does the loss behavior change with different parameter counts at different dimensions? Such a plot can give an empirical dependence on the order of parameters necessary with dimension.\n\n\n(c) The linear regression experiments show that with sparsity in the key-value matrix, the gated RNN models can simulate more efficiently than the theoretical construction. It would be interesting to conduct similar experiments in section 2, where the authors impose low-rank/sparse constraints on the key-value matrix product and observe the empirical behavior of loss with different parameter counts.\n\n\nOverall, I believe this paper will be an interesting read to the community. The current paper presentation is difficult to parse at different experimental details. Hence, I would like to interact with the authors during the rebuttal period with the questions that I posed above."
                },
                "questions": {
                    "value": "Please see my questions in the previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5288/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698853856416,
            "cdate": 1698853856416,
            "tmdate": 1699636529127,
            "mdate": 1699636529127,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tvbWHEr5HB",
                "forum": "rfSfDSFrRL",
                "replyto": "DNUcz0NeHA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5288/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback on our work and for pointing out sources of confusion. We provide some clarifications below that we hope will be helpful:\n\n(a) \n   1.  At the end of learning, not all recurrent neurons have their $\\lambda$ being 0 or 1 (c.f. figure in the global answer). However, inspection of the input and output gating weights reveals that those neurons will always receive 0 as input and that they are ignored by the output gating. This is what we mean by \u201cwe observe that only perfect memory neurons (\u03bb = 1) and perfect forget neurons (\u03bb = 0) influence the network output\u201d. Therefore, we can safely prune those neurons (and their corresponding input and output weights) from the network without changing its behavior. We plot the resulting network in Figure 2.A. \n   2. In our construction the input and output gating matrices have a characteristic 2x2 block structure. For example, if we look at the input gating, one of the matrix has top right and bottom left blocks that are equal to 0. We recover the same structure in $W_x^\\mathrm{in}$ in Figure 2.A. The same thing holds for the 4 other weights of the gating matrix.\n   3. We will make this caption clearer. If this may help the reviewer, here is a more detailed explanation. In Figure 2.A, the output gating weights do not fully match the structure of the construction (that is one matrix with non zero values on the left side, and one with non-zero values on the right side). However, it turns out that the last 3 neurons in between output gating and the $D$ matrix (rows 11, 12 and 13 in the output gating matrices, and the corresponding columns in $D$) are functionally equivalent to one. The input weights of this neurons are described in the right part of Figure 2.B (the row and columns vectors in the right part of Fig.2.B). The nice thing is that the corresponding weights for this neuron match the expected structure: the row vector only has zero entries in the first 10 columns, and the column vector has zero entries in the last 4 rows. As a consequence, it is possible to rewrite the output gating weights (and $D$) in a way that matches the construction, while not changing at all the behavior of the network. For the mathematical details, see Appendix B.2.\n   4. Indeed, the caption was not sufficiently explaining the table. The input at time $t$ consists in the concatenated vectors $x_t=(x_{t,1},x_{t,2},x_{t,3})^\\top$ and $y_t=(y_{t,1},y_{t,2},y_{t,3})^\\top$. The instantaneous function for each output neuron can implement a polynomial of degree 4 in these terms. The table shows the coefficient associated to the terms of the polynomial implemented by the first neuron after training. Interestingly, the only terms without a negligible coefficients are $x_{t,1}^2y_{t,1}, x_{t,2}^2y_{t,1}$ and $x_{t,3}^2y_{t,1}$, where $x^2$ refer to $xx$. This is virtually identical to the polynomial implemented by a single step gradient descent algorithm (GD), as shown in the table.\n\n(b) In all our experiments, we vary the number of neurons and as, a consequence, the number of parameters. The reviewer might be interested in Figure 5 in the appendix, in which we look at how the loss behaves as a function of the number of parameters of the different models. We are happy to provide more information if needed.\n\n(c) We thank the reviewer for this great suggestion. We report the result of this experiment in the global answer to all reviewers."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700209184575,
                "cdate": 1700209184575,
                "tmdate": 1700209184575,
                "mdate": 1700209184575,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ALb88CCyPh",
            "forum": "rfSfDSFrRL",
            "replyto": "rfSfDSFrRL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5288/Reviewer_2JoC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5288/Reviewer_2JoC"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a construction of a gated RNN that implements self-attention (linear) and provides a conceivable path towards RNNs that can learn self-attention. The construction relies on GLUs with a simplified rule for describing input and output gating. The authors conduct several experiments demonstrating activated neurons in the RNN correspond to scores in that would be expected in the construction. They also demonstrate parity with a linear self-attention mechanism. The authors then study features of these networks, in particular with linear regression and gradient descent, observing the impact of nonlinearity and sparsity. This work provides a theoretical foundation with which to study other approximations of self-attention."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The construction is novel and draws a clear connection with the special case of linear self-attention.\n- The explanation and construction of gated recurrent networks is clear, and the correspondence with self-attention is transparent and intuitively explained, i.e. in Figure 1.\n- The idea can guide development of attention implementations with other architectures which may have implications for efficiency. Given a general foundation, future work can use similar styles of constructions to proceed."
                },
                "weaknesses": {
                    "value": "- Overall, the thrust of the contribution of the paper needs to be much more clearly articulated.\n  - Why is this particular construction good?\n  - What is the methodology that is general enough here to use for future constructions?\n  - How, explicitly, does the authors' approach pave the way for future contributions?\n  - Why do the learned ideas (e.g. linear regression) strengthen the thrust of the paper.\n\nIf these ideas can be articulated more clearly in a response here and in the manuscript, I would likely change my score.\n\nRegarding presentation:\n- Worth noting that citations in the PDF version of the paper don't appear linked to citations (for me)\n- Worth mentioning that GLUs in their initial construction from Dauphin et al were actually used in gated convolutional models, which resembled RNNs in their hierarchy, but were different\n- While the regularization task presented in the manuscript is valuable, not having a sequence learning task holds back some of the strength of the empirical results."
                },
                "questions": {
                    "value": "- Section 3.2 discusses the invertibility of the value matrix per the number of hidden neurons the RNN needs to store KVs. Under which conditions is this matrix invertible?\n- In Section 4.1, how do the number of activated neurons in the construction correspond to activated attention weights? Is this correspondence clear?\n- In Section 4.2, the authors describe overparameterization insofar as twice as many neurons are needed to replicate the behavior of self-attention with the RNN construction. What might the effect of regularization be here, implicit or otherwise?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5288/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5288/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5288/Reviewer_2JoC"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5288/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699516272555,
            "cdate": 1699516272555,
            "tmdate": 1699636529015,
            "mdate": 1699636529015,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9znRecA65p",
                "forum": "rfSfDSFrRL",
                "replyto": "ALb88CCyPh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5288/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their valuable time reviewing our paper. We have answered part of their questions in the global response and we provide additional details in the following:\n\n- **Invertibility of the value matrix.** We require the matrix $W_V$ to be invertible to decrease the number of neurons required by our construction, which of course depends on the self-attention layer we want to replicate. See the global discussion for a more comprehensive discussion on this topic.\n- **Section 4.1.** In this experiment, we observe three things when looking at the weights of the network (we provide a visualization of those weights in the global answer) at the end of learning:\n    - Most of the weights in the input gating are 0s.\n    - There are some $\\lambda$ values in the recurrent that are not 0 or 1.\n    - Most of the weights in the output gating are 0s.\n    \n    It turns out that recurrent neurons whose $\\lambda$ is not equal to 0 or 1 have corresponding weights in the input and output gating to be 0. As a consequence, they are effectively ignored by the network. If we remove those neurons, we end up with as many neurons as in the construction. The weights of the network after this pruning are displayed in Figure 2.\n    \n- **Section 4.2.** As highlighted in the previous point, not all recurrent neurons end up being used at the end of learning. However, having those extra neurons is important for learning as some neurons will get \u201ckilled\u201d during learning. This is why overparametrization (in the sense of more recurrent neurons than the construction) is needed and we empirically observe that having twice as many neurons as in the construction is enough to reach 0 training loss. Yet, it turns out that only the neurons from the construction are actually used."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700209044665,
                "cdate": 1700209044665,
                "tmdate": 1700209044665,
                "mdate": 1700209044665,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]