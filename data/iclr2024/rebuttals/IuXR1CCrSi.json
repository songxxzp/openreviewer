[
    {
        "title": "Talk like a Graph: Encoding Graphs for Large Language Models"
    },
    {
        "review": {
            "id": "yPVtn9xVCg",
            "forum": "IuXR1CCrSi",
            "replyto": "IuXR1CCrSi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6105/Reviewer_SLdh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6105/Reviewer_SLdh"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes to understand the graph reasoning abilities of LLMs through a benchmark and experiments. Compared to existing works, this paper uniquely focuses on how to encode graph structures in natural language and different types of graphs, as well as their impact on model performance. Experiments demonstrate that the choice of natural language instantiation and graph structures indeed have an impact on LLMs' ability for graph reasoning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ reasoning on graphs with LLMs is an important research question\n+ the experiments are extensive"
                },
                "weaknesses": {
                    "value": "- Since the authors claim the GraphQA benchmark as a novel contribution, it would be great to include at least some description of the benchmark dataset in the main paper. How is the benchmark constructed? What are the hyperparameters in random graph generation? What are the statistics of GraphQA? A brief description of the benchmark in the main paper, accompanied by full details in the appendix, will best help readers understand the scale and validity of the study.\n\n- In equ(2), is it $\\max_{g}$ instead of $\\max_{g,Q}$?\n\n- It would be nice to have at least a one-sentence description of each graph task in section 3.1. In section 3.5, the *disconnected graph task* is mentioned but it is not introduced at the beginning of section 3.1.\n\n- Since one of the main arguments of this work is \"how to encode graphs in natural language affect performance\", it would be great to present Table 1 results aggregated by graph encoding functions. It would also be nice to provide hypotheses as to why certain encoding approaches are particularly bad for LLM performance.\n\n- I'm not sure about the uniqueness of some of the findings in this work. Experiments 1-4 in Section 3.1 basically prove two things: 1) LLMs are sensitive to variations in prompt, and 2) larger LMs are generally more capable. While these findings are well established in LLM research, the four experiments simply corroborate them in the graph reasoning domain. I wonder if the authors might have more interpretations of these results beyond those already established in general LLM research.\n\n- For section 4, I wonder if the authors conducted a control experiment, i.e. the only difference among problem subsets is the graph construction algorithm. What factors are specifically fixed in Section 4? It would also be great to provide hypotheses as to why LLMs are better/worse at handling certain graph types."
                },
                "questions": {
                    "value": "please see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6105/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698103535017,
            "cdate": 1698103535017,
            "tmdate": 1699636659357,
            "mdate": 1699636659357,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5FwVRQ6viV",
                "forum": "IuXR1CCrSi",
                "replyto": "yPVtn9xVCg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6105/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6105/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comment by Authors"
                    },
                    "comment": {
                        "value": "### Details on GraphQA\n\nThanks for the suggestion. We added some task descriptions in the main paper (Section 3.1). We also added a separate section for GraphQA in the appendix (A.2) and moved the descriptions under this section and also added more details on it. We are committed to open-source the code and data upon acceptance of the paper.\n\n### Equation 2\n\nWe updated Equation 2 to the following to better reflect the goal of this work:\n\nOur training input $D$ to the graph-based prompt system is a set of ${G, Q, S}$ triples, where $G$ is a graph, $Q$ is a question asked to the LLM, and $S$ is a solution to $Q$, ($S \\in W$).  We seek to find a $g(.)$ and $Q$ that maximize the expected score from the model ($\\text{score}_f$) of the answers over the training dataset $D$.\n\n\\begin{equation}\n  \\max_{g, q} E_{G, Q, S \\in D} \\text{ score}_f(g(G), q(Q), S)  \n\\end{equation}\n\nPlease let us know if this is not clear and we can elaborate more.\n\n### Adding task descriptions\n\nWe added the task descriptions to the beginning of Section 3.1. We also added mode detail for the disconnected node task in Section 3.5.\n\n### Presenting Table 1 aggregated by graph encoding\n\nThanks for the great suggestion. We already reported aggregated results in Table 6 in the appendix as the average ranking of each encoder. We also added Table 7 with mean and standard deviation aggregated for each encoder. The results in Table 7 also confirms our hypothesis from Table 6 that the incident encoder outperforms the rest. We posit that the success of incident encoding can be attributed to two key factors. Firstly, it leverages integer node encoding (e.g., *node 0* or *node 1*), as we previously emphasized the advantages of this approach in Section 3.1.1. Secondly, incident edge encoding effectively captures the one-hop neighbourhood around a graph, outperforming methods that simply list edges in a random order.\n\n### Uniqueness of some of the findings in this work\n\nWe agree that many of the findings are in-line with the existing work in the literature. We think that demonstrating the persistence of these (some established) results, in the context of graph reasoning is essential, especially given the widespread adoption of using LLMs for reasoning.\n\nWe also showed that linear scaling does not hold for some of the tasks studied here. We observed some non-linear scaling in cycle check (see Figure 3-cycle check) and also in the new results added for the path existence task (checking if a path exists from one node to the other) upon reviewer FDET\u2019s suggestion :\n\n|            | Palm 2 XXS | PaLM 2 XS | PaLM 2 S |\n|------------|------------|-----------|----------|\n| zero-shot  | **80.9**  | 57.5     | **83.7** |\n| zero-cot   | 53.9       | 25.9      | 83.2     |\n| few-shot   | 44.8       | 60.5      | 81.9     |\n| cot        | 53.0       | **63.6**  | 82.4     |\n| cot-bag    | 54.9       | 44.3      | 81.7     |\n\nAnother finding of this work was that incident encoding performed the best across all the tasks (discussed in Section A.3).\n\n### Section 4 experiment setup\n\nThe generated graphs for this section are sampled randomly from the generator using similar statistics as input. To be able to report results for this section, we encoded graphs from each graph generator using the nine graph encoding functions and aggregated the results over the graph generator. We believe this setup gives us a controlled experiment to measure the effect of the graph generator.\nWe reported some hypotheses in Section 4.2. Here\u2019s a summary:\n\n* For each of the tasks, LLMs have a prior bias about the problem. For instance, for the cycle check task, LLMs have a prior bias saying there is a cycle in the graph. For instance, for the cycle check task, LLMs have a prior bias that assumes there is a cycle in the graph. Therefore, the performance is better for graphs with cycles (e.g., complete graphs) compared to those with no cycles (e.g., star graphs) (Figure 4 visualizes how these graphs differ).\n\n* For some of the tasks with counting (e.g., node degree, node count, and edge count), the fewer the number of edges in the graph, the easier it is for the LLM to reason. For instance, star graphs have the best performance, and complete graphs have the lowest (again, Figure 4 visualizes how these graphs differ)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6105/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173216239,
                "cdate": 1700173216239,
                "tmdate": 1700173249743,
                "mdate": 1700173249743,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UNyDYswpbH",
                "forum": "IuXR1CCrSi",
                "replyto": "5FwVRQ6viV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6105/Reviewer_SLdh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6105/Reviewer_SLdh"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for the detailed response and I do not have any outstanding concerns."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6105/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455247832,
                "cdate": 1700455247832,
                "tmdate": 1700455247832,
                "mdate": 1700455247832,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xykE9kudcU",
            "forum": "IuXR1CCrSi",
            "replyto": "IuXR1CCrSi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6105/Reviewer_FDET"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6105/Reviewer_FDET"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of reasoning on graphs with large language\nmodels (LLMs) and provides a comprehensive exploration of encoding graph-\nstructured data as text that can use LLMs. The paper claims that the LLM\nperformance in graph reasoning tasks varies on three crucial fronts: (1) the\nmethod used to encode the graph, (2) the nature of the graph task itself, and\n(3) the inherent structure of the graph. The paper has provided comprehensive\nexperiments on graph reasoning using LLMs by providing them with text prompts\nthat are constructed from the graphs. In these, the paper analyzes the effect of\na variety of graph-to-text encoding and question encoding functions as well as\ngraph structures on LLMs performance. Different methods such as Zero-shot,\nFew-shot, and Chain-of-Thought methods have been considered for prompting.\nTo analyze the impact of different graph structures on performance, the paper\nhas generated random graphs using previous approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper has provided detailed discussions of their results along with\nreasonable and meaningful conclusions.\n\n- The paper is also well-organized and easy to read.\n\n- The experiments are comprehensive as they include important factors\nthat can impact the performance of LLMs on graph reasoning. These\nare encoding the input graph to text, the structure of the input graph,\nrephrasing the question, complexity of the LLM, and prompting method."
                },
                "weaknesses": {
                    "value": "- The graph, node, and edge encoding functions are simple and inefficient.\nThe paper could use more advanced and recent graph-to-text generation\ntechniques (i.e. [1]). Evaluating only the defined encoding methods cannot\nsupport the general claims about the power of LLMs in graph reasoning.\n\n\n- The proposed graph encoding approaches are similar i.e. the Friendship,\nPolitician, Social network, GOT, and SP all depict alternative ways of\nstating two nodes are \u201cconnected\u201d. Therefore, evaluating them shows\nthe power of LLMs in interpreting the names rather than exhibiting their\nability to understand underlying relations and exploit neighborhoods within\na graph. This could have been considered in increasing the diversity of\nencoding functions.\n\n- It might be good to introduce previous random graph generation methods.\nAdding some detail of these methods (even in the appendix) can be helpful\nto understand how they are different.\n\n- The proposed benchmark tasks (except for edge existence) do not involve\nreasoning. They can be inferred without reasoning (by counting, simple arithmetic operations, and memorizing the graph structure). More\nchallenging tasks (e.g., node classification) can enrich the experiments.\n\n- In Experiment 2, authors compare question and application rephrasing\nmethods, while the difference between these two is not clear. Authors can\nadd a few examples of rephrasing a question with these methods in the\nmain body or appendix of their paper.\n\n[1] Yi Luan Mirella Lapata Rik Koncel-Kedziorski, Dhanush Bekal and Han-\nnaneh Hajishirzi. Text Generation from Knowledge Graphs with Graph\nTransformers. In NAACL, 2019."
                },
                "questions": {
                    "value": "It would be great if some of the points raised in the weakness section are addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6105/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6105/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6105/Reviewer_FDET"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6105/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698617728153,
            "cdate": 1698617728153,
            "tmdate": 1700192252571,
            "mdate": 1700192252571,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3O2p3JFOTJ",
                "forum": "IuXR1CCrSi",
                "replyto": "xykE9kudcU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6105/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6105/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comment by Authors"
                    },
                    "comment": {
                        "value": "### Graph-to-text generation techniques (i.e. [1]) and diversity of the encoding functions\n\nWow, great suggestion!  In this paper, we specifically studied the class of fixed text encoding functions because we observed that many users of LLM systems are relying on these fixed transformations.  Our results show that your intuition is exactly right (fixed encodings have limitations), and investigating this area is very interesting follow up work!  \n\nFor this particular reference [1], we searched for an existing model to try to use it \u201cout of the box\u201d, but could not find any available.  Unfortunately we do not have time in the review cycle to appropriately train this as a baseline and collect its results (as running this experiment takes significant time already).\n\nWhile we initially intended to explore fixed text encoding functions, our goal was to introduce diversity in node and edge encodings. This involved employing a range of node encodings (integers, alphabet letters, popular character names) and edge encodings (friendship, co-authorship, parentheses). By combining these, we achieved a diverse set of graph encoding functions (see Appendix A.1 for more details). The choice of the encoding functions was informed by the typical user practices associated with LLMs.\n\n### Introduce random graph generation methods in the appendix.\n\nWe added descriptions for the random graph generator functions in the appendix in Section A.2.2.\n\n### More challenging tasks\n\nThanks for the great suggestion! We have added a synthetic node classification task based on GraphWorld [2], and are currently waiting for the results of this experiment to complete.  (We will update here when this is done)\n\nWe would like to note that some of the diverse tasks in our benchmark do share elements of graph classification (especially w.r.t. to motif detection).  For example, The cycle check task needs to do multi-hop reasoning over the structure of the graph in order to determine the existence of a cycle. Also, note that the node degree counting could be viewed as the simplest node regression task.  (More complex versions of this task might try to compute advanced node-level graph properties.)  Similarly, the node and edge count and cycle check might be viewed as  graph classification tasks.\nUpon your suggestion on adding more complex tasks, we conducted experiments on a new task for  path existence which requires multi-hop reasoning. Here are the results:\n\n| Prompting| Palm 2 XXS | PaLM 2 XS | PaLM 2 S |\n|------------|------------|-----------|----------|\n| zero-shot  | **80.9**  | 57.5     | **83.7** |\n| zero-cot   | 53.9      | 25.9     | 83.2     |\n| few-shot   | 44.8      | 60.5     | 81.9     |\n| cot        | 53.0      | **63.6** | 82.4     |\n| cot-bag    | 54.9      | 44.3     | 81.7     |\n\n### Adding examples for experiment 2.\n\nWe added this to the paper. Please let us know if this is not clear. Here\u2019s the examples for your references:\n\n| Task              | Graph question encoder                                  | Application question encoder                       |\n|-------------------|---------------------------------------------------------|-----------------------------------------------------|\n| Edge existence    | Is node Christopher connected to node Michael?          | Are Christopher and Michael friends?                |\n| Node degree       | What is the degree of node 14?                           | How many friends does Christopher have?              |\n| Node count        | How many nodes are in this graph?                         | How many people are mentioned in this information?  |\n| Edge count        | How many edges are in this graph?                         | How many friendships are given in this information?  |\n| Connected nodes   | List all the nodes connected to node Christopher.         | List all the people who are friends with Christopher.|\n\n[1] Yi Luan Mirella Lapata Rik Koncel-Kedziorski, Dhanush Bekal and Han- naneh Hajishirzi. Text Generation from Knowledge Graphs with Graph Transformers. In NAACL, 2019.\n\n[2] Palowitch, John, Anton Tsitsulin, Brandon Mayer, and Bryan Perozzi. \"Graphworld: Fake graphs bring real insights for gnns.\" In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 3691-3701. 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6105/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172501248,
                "cdate": 1700172501248,
                "tmdate": 1700172501248,
                "mdate": 1700172501248,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ye3uFqEA8D",
                "forum": "IuXR1CCrSi",
                "replyto": "3O2p3JFOTJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6105/Reviewer_FDET"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6105/Reviewer_FDET"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal!"
                    },
                    "comment": {
                        "value": "Thanks for the clarifications and additional results. \nBased on the rebuttal and the quality of the paper, I am increasing my score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6105/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700192227106,
                "cdate": 1700192227106,
                "tmdate": 1700192227106,
                "mdate": 1700192227106,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Cvtxu9g3d1",
                "forum": "IuXR1CCrSi",
                "replyto": "xykE9kudcU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6105/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6105/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Adding a Node Classification Task"
                    },
                    "comment": {
                        "value": "Your suggestion to explore a node classification task proved to be immensely valuable.\nInspired by your suggestion, we used our stochastic block model graph generator to create two distinct blocks representing soccer and baseball enthusiasts. We then labeled a subset of nodes and asked the LLM to determine whether an unspecified node belonged to the soccer or baseball group. This exercise aimed to assess the LLM's ability to exploit the homophily in a given graph.\n\nThe results for various encoders were as follows:\n\n| Encoding         | Accuracy |\n|------------------|-------|\n| Overall          | 58.4  |\n| Adjacency        | 54.8  |\n| Incident         | 55.2  |\n| Co-authorship    | 68.4  |\n| Friendship       | 74.6  |\n| SP               | 65.8  |\n| GOT              | 51.2  |\n| Social network   | 55.4  |\n| Politician       | 54.2  |\n| Expert           | 46.4  |\n\nThe results of this experiment also confirm the major findings in the paper. First, the LLM didn't do well on the node classification task either and only just beat the majority baseline by a small amount (51.2% to 58.4%). Second, the choice of the graph encoder has a significant impact on the LLM reasoning. As this task requires more reasoning (compared to some of the tasks that require more of memorization), some of the encoders with more textual information (e.g., friendship) proved to be more powerful. We can add this experiment to the main paper.\n\nThis exercise further demonstrated the versatility of our benchmark, accommodating new tasks and effectively evaluating the performance of LLMs across diverse graph generators. We will open-source the code and data to facilitate future research in this direction."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6105/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511606933,
                "cdate": 1700511606933,
                "tmdate": 1700512016357,
                "mdate": 1700512016357,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "E1sFug9eat",
            "forum": "IuXR1CCrSi",
            "replyto": "IuXR1CCrSi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6105/Reviewer_52xH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6105/Reviewer_52xH"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides an extensive investigation into the capabilities of LLMs in understanding graph structure. The authors explore various factors such as the graph encoding function, prompting questions paradigm, relation encoding, model capacity, and reasoning in the presence of missing edges. The implications of these variables on LLM's graph reasoning and understanding abilities are also carefully examined. Moreover, the authors also investigate the implications of graph structure by randomly generating diverse graphs for evaluation and analyze the results from the impact of graph structure, distractive statements in graph encoding, and the selection of few-shot examples in few-shot learning. This work presents some interesting findings in graph encoding methods, the nature of graph tasks, and the graph structure. The paper yields intriguing findings concerning graph encoding methods, the nature of graph tasks, and the graph structure itself."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is overall well-written and well-organized, I enjoy reading it.\n2. The experiment results are extensive, making it a solid work.\n3. I like the analysis in bulletin list style, which helps readers to capture the most important information.\n4. There are some interesting findings in this paper."
                },
                "weaknesses": {
                    "value": "1. In the introduction, the authors mention two limitations in the existing LLMs and one of them is difficulty in incorporating fresh information, but how could the graph structure data solve this problem? I would encourage authors to elaborate more on this statement.\n2. In section 3.5 experiment 5, the task description is too brief for readers to understand the experimental settings. What is specifically the \"disconnected nodes task\" and how to generate this data is not clear.\n3. The motivation for each experiment setting is not clear enough, I encourage authors to give their motivation in each experiment to help readers understand the necessity for the experiment.\n4. For simple tasks such as node degree, node count, edge count, etc. There are some efficient, accurate, and reliable algorithms to do that with programming, so why not just let LLMs write code for these tasks and execute the code to solve these problems? \n5. I believe the motivation of this work is not strong enough. Yes, there are graphs everywhere, and reasoning on graphs is essential, but why do we need LLMs to do reasoning on graphs? The LLMs are trained on unstructured textual data, making it hard to generalize to graph data. Moreover, we also have reliable and fast algorithms to solve these basic graph problems, so I believe LLMs might not be a good tool for these basic graph problems."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6105/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6105/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6105/Reviewer_52xH"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6105/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634980552,
            "cdate": 1698634980552,
            "tmdate": 1699636659057,
            "mdate": 1699636659057,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "td3wKXg3Q3",
                "forum": "IuXR1CCrSi",
                "replyto": "E1sFug9eat",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6105/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6105/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comment by Authors"
                    },
                    "comment": {
                        "value": "### 1. How can graphs help LLMs maintain freshness of information?\n\nGraphs are very general data structures -- in this work we are measuring how well structured data can be injected into and interpreted by LLMs.  The connection to freshness goes as follows:\n\nConsider the example of question answering with a LLM which has access to a knowledge graph (KG) or other structured database. \nWe can either perform a one-time update [via pre-training or fine-tuning] of the LLMs with the KG information or instead we can provide the KG as in-context information as part of the LLM\u2019s prompt. With the former case, it\u2019s more difficult to update the LLM with fresh information. However, in the latter case, it\u2019s easier to update the database (and the in-context information) as new information becomes available. We updated the second paragraph in the introduction to elaborate on this.\n\n### 2. Section 3.5 experiment 5: disconnected nodes task\nIn this task, we provide a graph description to the LLM, specifying the nodes and edges, and ask about the nodes that are \\emph{not} directly connected to a given node. We updated Section 3.5 to better explain this.\n\n### 3. Adding motivations for each experiment.\nThanks! We highlighted the motivation for our experiments in the paper.\n\n### 4 and 5. Motivation for using LLMs for graph tasks.\n\nGreat question!  Our motivation is not to suggest that LLMs replace the reliable and efficient graph algorithms we all grew up with -- instead it's quite the opposite.  We believe that graph tasks are an ideal lens for understanding multi-step reasoning with LLMs.  By quantifying LLMs (poor) performance on these tasks, we highlight a significant gap with current technologies and open the door for future research in the area.\n\nThis is especially interesting, because many tasks that people ask of LLMs today reduce to graph algorithms (or have graph algorithms as essential components of their solution).  For instance, when using LLMs in problems requiring deductive logical reasoning, the LLM should be able to perform edge existence, and reason about connected/disconnected nodes.  However, there is not much work actually evaluating these capabilities.  This is especially important for cases when the question is textual (and therefore common sense knowledge is needed) and it is not obvious what out-of-the-box graph algorithm might even apply."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6105/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700171883791,
                "cdate": 1700171883791,
                "tmdate": 1700171883791,
                "mdate": 1700171883791,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qgGSyqk07R",
            "forum": "IuXR1CCrSi",
            "replyto": "IuXR1CCrSi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6105/Reviewer_PSY3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6105/Reviewer_PSY3"
            ],
            "content": {
                "summary": {
                    "value": "This work presents the first comprehensive study on encoding graph-structured data as text for large language models (LLMs). Graphs are widely used to represent complex relationships in various applications, and reasoning on graphs is crucial for uncovering patterns and trends. The study reveals that LLM performance in graph reasoning tasks depends on three key factors: the graph encoding method, the nature of the graph task itself, and the structure of the graph considered. These findings provide valuable insights into strategies for improving LLM performance on graph reasoning tasks, with potential performance boosts ranging from 4.8% to 61.8%, depending on the specific task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* It is a valuable problem for encoding graph-structured data as text for LLMs.\n* Many factors are taken into considerations, and detailed analyses are provided. \n* The findings provide valuable insights into strategies for improving LLM performance on graph reasoning tasks."
                },
                "weaknesses": {
                    "value": "1. One concern is about the experiment. The paper explores encoding graph-structured data as text for **LLMs**. However, only one type of LLM is compared (PaLM). It would be better to make comparisons with other LLMs, like GPT3/4 and Llama to make the findings more convincing.\n\n2. Another concern is about the novelty. The proposed graph encoder function g() in this paper is a mapping from graph space to textual space. Several previous paper [1-3] explores describing graph neighbors in natural language, and it would be better to tell the difference of this work. \n\n[1] Guo, Jiayan, Lun Du, and Hengyu Liu. \"GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking.\" arXiv preprint arXiv:2305.15066 (2023).\n\n[2] Chen, Zhikai, et al. \"Exploring the potential of large language models (llms) in learning on graphs.\" arXiv preprint arXiv:2307.03393 (2023).\n\n[3] Ye, Ruosong, et al. \"Natural language is all a graph needs.\" arXiv preprint arXiv:2308.07134 (2023)."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6105/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6105/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6105/Reviewer_PSY3"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6105/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817405074,
            "cdate": 1698817405074,
            "tmdate": 1699636658934,
            "mdate": 1699636658934,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JXEJxakcAP",
                "forum": "IuXR1CCrSi",
                "replyto": "qgGSyqk07R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6105/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6105/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comment by Authors"
                    },
                    "comment": {
                        "value": "### comparisons with other LLMs\n\nWe appreciate the reviewer's valuable suggestion to conduct experiments on other LLMs. We run our experiments using GPT3.5. This additional evaluation provides further insights into the performance of our approach and confirms the key findings observed on Palm 1 and 2.\n\nGraph encoder functions have a substantial impact on the performance of LLMs on graph-based tasks, with the node degree task exhibiting a performance variance of 5.4% to 66.4%. Integer node encoding enhances arithmetic performance for node degree, node count, and edge count tasks. The incident encoding outperforms other encoding functions by a significant margin on the connected node task due to its ability to make information more readily accessible for LLM utilization.\n\n\n\n| Encoding function | Edge Existence | Node degree | Node count | Edge count | Connected nodes | Cycle check |\n|-------------------|----------------|-------------|------------|------------|------------------|-------------|\n| Overall           | 77.0           | 42.2        | 98.8       | 37.6       | 46.7             | 86.1        |\n| Adjacency         | 73.0           | 5.4         | 99.2       | **46.2**   | 59.2             | 84.2        |\n| Incident          | 80.2           | **66.4**    | 96.2       | 12.6       | **75.2**         | 84.6        |\n| Co-authorship     | **82.0**       | 42.6        | 99.8       | 39.4       | 37.2             | 86.4        |\n| Friendship        | 73.8           | 43.8        | **100.0**  | 41.8       | 41.2             | 87.4        |\n| SP                | 74.0           | 44.2        | 99.8       | 39.0       | 38.6             | 86.4        |\n| GOT               | 75.8           | 41.0        | 99.0       | 39.4       | 38.6             | **88.6**    |\n| Social Network    | 78.6           | 47.6        | 99.8       | 38.6       | 40.6             | 86.6        |\n| Politician        | 78.6           | 45.4        | **100.0**  | 39.8       | 40.2             | 84.2        |\n| Expert            | 75.7           | 43.5        | 95.3       | 41.7       | 49.2             | 86.6        |\n\n### Novelty wrt [1, 2, 3]: \n\nThanks for the great question. Our work uniquely investigates how properties of the graph structure (via synthetic generation) influence the choice of graph encoding function. This is completely novel, as no other related work examines these interactions. This novel approach enables us to systematically study these interactions, which have not been explored in previous research. Additionally, we conducted extensive scaling experiments with LLMs of varying parameter sizes which no other work has done. Finally, we have significant differences from each work individually, for example:  unlike [1], we study a diversity of LLMs (and have added more models in the rebuttal), unlike [2] we vary prompting and graph tasks, and unlike [3], we use a black box model without model weights.. Furthermore, we conducted experiments on varying question encoder functions to evaluate their impact on performance.\n\nIn order to clearly state the novelty of our work, we have added this following comparison table to the paper:\n\n|                               | Guo et al.[1] | Chen et al. [2] | ye et al. [3] | Ours         |\n|-------------------------------|-------------------|--------------------------|-----------------------|--------------|\n| Synthetic generation          | &#10005;          | &#10005;                 | &#10005;              | &#10004;   |\n| Black-box model               | &#10004;        | &#10004;               | &#10005;              | &#10004;   |\n| Scaling experiments           | &#10005;          | &#10005;                 | &#10005;              | &#10004;   |\n| Varying question encoding function | &#10005;     | &#10005;                | &#10005;              | &#10004;   |\n| Varying edge encoding function | &#10005;          | &#10005;                 | &#10005;              | &#10004;   |\n| Varying graph structure        | &#10005;          | &#10005;                 | &#10005;              | &#10004;   |\n| Varying graph encoding function | &#10004;        | &#10005;                 | &#10005;              | &#10004;   |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6105/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700171717071,
                "cdate": 1700171717071,
                "tmdate": 1700171717071,
                "mdate": 1700171717071,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]