[
    {
        "title": "What does automatic differentiation compute for neural networks?"
    },
    {
        "review": {
            "id": "Y08wFpgFTp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1623/Reviewer_84GA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1623/Reviewer_84GA"
            ],
            "forum": "8vKknbgXxf",
            "replyto": "8vKknbgXxf",
            "content": {
                "summary": {
                    "value": "### Edit after rebutal: \nI updated my score and my assessement of the paper after reading authors response. \n\nThis is an important topic which oughts to be discussed. The authors claim original results on correctness of automatic differentiation in a nonsmooth context based on the existence of bias parameters for each layer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The results are among the only positive ones available in the litterature regarding soundness of AD for nonsmooth neural networks."
                },
                "weaknesses": {
                    "value": "### Edit after rebutal: \nthe authors responded in a satisfactory way to the concerns raised below. \n\n- Theorem 1 is a minor extension of Theorem 3.6 in Lee et al. (2023). \n- Lemma 2 and Lemma 3 are complicated formulations of essentially known facts\n- Lemma 3 and Theorem 4 are contradictory\n- I suspect that Theorem 4 is flase, as well as theorem 6.\n- Conditions 1 and 2 are very complicated without an intuitive explaination of what they mean.\n- Theorem 8 lacks discussion on the rate of positive outcomes and complexity."
                },
                "questions": {
                    "value": "Lemma 2 and Lemma 3:  the example in Lemma 2 is explicitely mentioned in Kakade and Lee 2018 and the mechanism in Lemma 3 is exaclty the same, namely \"incompatibility with addition\". A similar comment holds for Lemma 7. Is there anything else beyond these lemmas? Why not refering to the fact that this is a known issue?\n\nLemma 3 and Theorem 4 are contradictory. D- and D+ are never explicitely defined, so I assume that for a piecewise analytic function it corresponds to the derivative of the function on the left and on the right at a piece change point. In this case, the Bouligand derivative is {D-,D+} and the clarke is its convex hull. So in Theorem 4, choosing all lambda = 0, one is in the setting of Lemma 3. I suspect that Theorem 4 is false for this reason. Is my reasoning correct?\n\nI really have troubles to understand the difference between Condition 1 and Condition 2 with \"trivial max-pool\". This is very complicated and I believe there should be a qualitative description of what these conditions mean. For example I cannot tell: is one of the condition more general than the other? Similarly: is there is a difference between Theorem 4 and Theorem 6? Lastly, due to the same concern as above, I suspect that Theorem 6 is false.\n\nTheorem 8 is of little use without a discussion on:\n- When does the algorithm stop with Pl non empty? Does it occur often? Does it always occur?\n- What is the complexity of the algorithm? How much overhead does it represent compared to AD?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1623/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1623/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1623/Reviewer_84GA"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1623/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697185128705,
            "cdate": 1697185128705,
            "tmdate": 1700045142748,
            "mdate": 1700045142748,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yh254qY3x4",
                "forum": "8vKknbgXxf",
                "replyto": "Y08wFpgFTp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 84GA"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for their time and effort to provide valuable comments. We address all comments of the reviewer, and provide pointers to the corresponding updates in the revised paper. All the updates are color coded in the revised version.\n\n**D- and D+ are never explicitly defined.** \n\nWe defined $D^-$ and $D^+$ in Section 2.1 of the initial draft; they denote the left- and right-hand derivatives as the reviewer expected. To improve readability, we have added a pointer to these definitions right after Theorem 4 in the revised draft.\n\n**Lemma 3 and Theorem 4 are contradictory.**\n\nLemma 3 and Theorem 4 (or Theorem 6) do not contradict each other. Theorem 4 shows that AD computes an element of the Clarke subdifferential of a network (satisfying Condition 1 with distinct bias parameters) if the following condition holds: the proxy gradient $D^{AD} \\rho_l(x)$ of $\\rho_l$ (i.e., the activation function in the $l$-th layer) can be written as $\\lambda_l D^-\\rho_l(x) + (1-\\lambda_l) D^+\\rho_l(x)$, where $\\lambda_l \\in [0,1]$ should be *fixed across all $x \\in\\text{ndf}(\\rho_l)$ and the layer $l$*. That is, the proxy gradient should be a convex combination of the left- and right-hand derivatives, where the same combination weight $\\lambda_l$ is used for all inputs $x$ (within the $l$-th layer).\n\nOn the other hand, our counterexample in Lemma 3 uses $\\rho_l =\\text{HardSigmoid}$ with $D^{AD} \\rho_l(-3) = D^{AD} \\rho_l(3) = 0$. That is, $D^{AD} \\rho_l(x)$ uses the left-hand derivative at $x=-3$ and the right-hand derivative at $x=3$. Due to this, there *cannot exist* a combination weight $\\lambda_l \\in [0,1]$ such that $D^{AD} \\rho_l (x) = \\lambda_l D^-\\rho_l(x) + (1-\\lambda_l) D^+\\rho_l(x)$ for all $x \\in\\text{ndf}(\\rho_l)$. Hence, this counterexample does not satisfy the condition in Theorem 4, and thus it is not contradictory to Theorem 4. We have clarified this point in the revised version (page 6).\n\n**I suspect that Theorem 4 is false, as well as theorem 6.**\n\nAs we explained above, Theorems 4 and 6 are not contradictory to Lemma 3. We formally prove that the two theorems are true, and present their proofs in Appendices D-E. \n\n**Theorem 1 is a minor extension of Theorem 3.6 in Lee et al. (2023).**\n\nWe believe our Theorem 1 is more than a minor extension of Theorem 3.6 in (Lee et al., 2023). First, our theorem considers a larger class of neural networks than the previous theorem: the latter restricts a network to have no residual connections (page 9 of (Lee et al., 2023)), while the former does not have this restriction (page 4 of our paper). Second, for the same network, our Theorem 1 proves the same conclusion given in Theorem 3.6 of (Lee et al., 2023), but under a weaker assumption. The latter theorem states that if proxy gradients are in the Bouligand subdifferential, then AD computes an element of the Clarke subdifferential. Our Theorem 1 extends this result as follows: the same conclusion holds even if we use a wider class of proxy gradients (namely those in the Clarke subdifferential). We believe this extension is an important addition to a line of recent works (e.g., [Bertoin+, Boursier+]) on understanding the effects of the choice of proxy gradients. We have clarified this point in the revised version (page 5).\n\nIn addition, we would like to emphasize that our Theorem 6 further generalizes Theorem 3.6 in (Lee et al., 2023) by considering an even larger class of neural networks. The latter theorem considers networks that do not contain usual residual connections, convolutional layers, and normalization layers such as BatchNorm (pages 4 and 9 of (Lee et al., 2023)) and do not allow minibatched inputs; hence, this prior result is not applicable to most convolutional networks used in practice. In contrast, our Theorem 6 allows a minibatch setup and a network with general residual connections, convolutional layers, and normalization layers (page 4 of our paper); thus, this result is applicable to a wider range of neural networks including practically-used convolutional networks. \n\n[Bertoin+] Numerical influence of ReLU'(0) on backpropagation, NeurIPS, 2021.\\\n[Boursier+] Gradient flow dynamics of shallow ReLU networks for square loss and orthogonal inputs, NeurIPS, 2022."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1623/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700009478800,
                "cdate": 1700009478800,
                "tmdate": 1700010151242,
                "mdate": 1700010151242,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ar4fa3uG5u",
                "forum": "8vKknbgXxf",
                "replyto": "Y08wFpgFTp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Lemmas 2 and 3 are complicated formulations of essentially known facts. Why not referring to the fact that this is a known issue?**  \n\nWe believe Lemmas 2 and 3 are not just a mere reformulation of known facts. As the reviewer pointed out, we indeed proved these two lemmas motivated by the known fact: the Clarke subdifferential is generally \u201cincompatible with addition\u201d, e.g., as in the example given by Kakade and Lee (2018). However, this incompatibility does not always guarantee the incorrectness of AD. For example, we showed that AD always outputs an element of the Clarke subdifferential under some conditions (Theorems 1, 4, 5, 6, and 8). Given this, what we essentially proved in Lemmas 2 and 3 is the following: under certain conditions, the incompatibility issue can actually show up in some neural networks. In this regard, Lemmas 2 and 3 are not an immediate consequence of the known incompatibility issues.\n\nFollowing the reviewer\u2019s suggestion, we have added the following clarification to the revised version (page 6): \u201cLemmas 2 and 3 are based on the incompatibility of the Clarke subdifferential with addition (Clarke et al., 1998; Kakada and Lee, 2018).\u201d\n\n**Conditions 1 and 2 are very complicated without an intuitive explanation of what they mean.**\n\nCondition 1 states that (i) each analytic pre-activation function $\\tau_l$ should have no bias parameters at all, or have bias parameters that should not be \u201cshared\u201d across multiple neurons in the same layer (e.g., as in fully-connected layers), and (ii) each activation function $\\rho_l$ should apply the same piecewise-analytic function elementwise. Similarly, Condition 2 states that (i) each analytic pre-activation function $\\tau_l$ should have no bias parameters at all, or have bias parameters that can be shared across multiple neurons in the same layer (e.g., as in convolutional layers), and (ii) each activation function $\\sigma_l$ should be the composition of a maxpool function and a elementwise piecewise-analytic function. To improve readability, we have added this explanation to the revised version (page 4).\n\n**Any difference between Condition 1 and Condition 2 with trivial maxpools? Any difference between Theorem 4 and Theorem 6?**\n\nCondition 2 with only trivial maxpools is a strict generalization of Condition 1: (i) the former includes the latter as a special case, and (ii) the former allows shared bias parameters while the latter allows only distinct bias parameters. (i) can be observed by setting $M_l=N_l=C_l$ and $A_{l,c}=e_c{\\bf 1}_B^\\top$. (ii) follow directly from the definition of the two conditions. We stated these observations before and after Condition 2 in the initial draft. We have made this difference clearer in the revised draft (page 4).\n\nSince Condition 2 with only trivial maxpools is a strict generalization of Condition 1, Theorem 6 is a strict generalization of Theorem 4 that allows shared bias parameters (which are commonly used in convolutional layers and normalization layers)."
                    },
                    "title": {
                        "value": "Response to Reviewer 84GA"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1623/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700009548049,
                "cdate": 1700009548049,
                "tmdate": 1700009573918,
                "mdate": 1700009573918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0ibLUqHuCo",
                "forum": "8vKknbgXxf",
                "replyto": "Y08wFpgFTp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 84GA"
                    },
                    "comment": {
                        "value": "**Theorem 8 lacks discussion on the rate of positive outcomes and complexity.**\n\n**(1) When does the algorithm stop with non-empty $\\mathcal P_l$?**\n\nOne condition that guarantees $\\mathcal P_l$ to be non-empty is the condition in Theorem 6: when a network satisfies Condition 2 with shared bias parameters and only trivial maxpools. For networks without shared bias parameters or with non-trivial maxpools, we could not (and we expect to be challenging to) find a generic sufficient condition that guarantees the non-emptiness of $\\mathcal P_l$, which in turn implies the correctness of AD. This is why we proposed Theorem 8 (with Algorithm 1) in the first place, and why we empirically checked $\\mathcal P_l \\neq \\emptyset$ using experiments.\n\n**(2) Does it occur often? Does it always occur?**\n\nAs stated in Section 4 of the initial draft, we empirically observed that $\\mathcal P_l$ was always non-empty during the training of the following networks: fully-connected networks with various activation functions (ReLU6, HardTanh, HardSigmoid) on the MNIST dataset, and various convolutional networks (ResNet18, VGG11, VGG11-BN) on the CIFAR-10 dataset. Together with Theorem 8, this implies that AD always computed an element of the Clarke subdifferential in our experiments."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1623/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700009789502,
                "cdate": 1700009789502,
                "tmdate": 1700009789502,
                "mdate": 1700009789502,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gZyzYyig2h",
                "forum": "8vKknbgXxf",
                "replyto": "Y08wFpgFTp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 84GA"
                    },
                    "comment": {
                        "value": "**(3) What is the complexity of the algorithm? How much overhead does it represent compared to AD?**\n\n**Theoretical complexity.** We analyze the computational complexity of Algorithms 1 for the $l$-th layer as follows. To simplify the notation, assume that memory read/write, addition/multiplication, and computing a proxy gradient take a unit cost. Let $\\mathcal A \\subset [M_l] \\times [B]$ be the set of indices $i$ of pointwise activation functions $\\rho_l$ whose non-differentiable points are touched (i.e., Line 5 of Algorithm 1 is true). For each index $j \\in [N_l] \\times [B]$ of maxpool neurons, let $\\mathcal S_j \\subset [M_l] \\times [B]$ be the set defined in Line 11 of Algorithm 1, and let $\\mathcal B \\subset [N_l] \\times [B]$ be the set of $j$\u2019s that satisfy $|\\mathcal S_j| \\geq 2$ (i.e., Line 12 of Algorithm 1 is true). First, for each element in $\\mathcal A$, Algorithm 1 adds a constraint to $\\mathcal P_l$. Next, for each index $j \\in \\mathcal B$, Algorithm 1 computes $\\mathcal T_j$, which requires at most $|\\mathcal S_j|$ number of backward passes of AD (up to the $l$-th layer), and then it adds at most $|\\mathcal S_j|-1$ constraints to $\\mathcal P_l$. Finally, Algorithm 1 checks whether $\\mathcal P_l$ is empty or not, which can be done by solving a linear programming problem (see the last paragraph of Section 3). Hence, the worst-case time complexity of Algorithm 1 can be written as $O(|\\mathcal A|+(\\sum_{j\\in\\mathcal B}|\\mathcal S_j|) \\cdot C_l+D_{W_l,|\\mathcal A| - |\\mathcal B| + \\sum_{j\\in\\mathcal B} |\\mathcal S_j|})$, where $C_l$ denotes the cost of a backward pass of AD up to the $l$-th layer and $D_{n,k}$ denotes the cost of solving a linear programming problem with $n$ variables and $k$ constraints.\n\n**Empirical overhead.** For neural networks that have shared bias parameters and use maxpools and ReLUs (with $D^{AD}\\text{ReLU}(0)=0$) as the only non-differentiable activation functions, Algorithm 1 can be simplified to Algorithm 2 which does not care about whether any input to ReLU touches zero or not (i.e., does not care $\\mathcal A$ discussed above; see the second paragraph on page 9). We used Algorithm 2 to check $\\mathcal P_l \\neq \\emptyset$ in our experiments, and empirically observed that Algorithm 2 incurred not much computational overhead: for training ResNet18 on the CIFAR-10 dataset, the average running times per epoch were 419s with Algorithm 2 and 237s without our algorithms, i.e., additional computational overhead was ~77% of the running time of the vanilla learning algorithm. We further observed that solving linear programming did not incur much overhead (<1%); almost all overhead (>99%) was from computing $\\mathcal S_j$ and $\\mathcal T_j$, and this overhead can be significantly reduced if we optimize our naive implementation of Algorithm 2 (e.g., by implementing a native GPU kernel for computing $\\mathcal S_j$). This relatively small overhead of Algorithm 2 was due to two phenomena we observed (shown in the second and fourth columns of Table 2): ties in maxpools occurred mostly in the first layer, so the backward passes of AD done in Algorithm 2 were very fast; and the number of constraints in $\\mathcal P_l$ was typically small, so checking the emptiness of $\\mathcal P_l$ was very fast.\n\n**Summary.** Theoretically, Algorithm 1 (and Algorithm 2) has a worst-case time complexity that depends on the number of ties arising in maxpools (i.e., $|\\mathcal S_j|$) and the number of non-differentiability touches in pointwise activation functions (i.e., $|\\mathcal A|$). Empirically, we observed that the overhead of running Algorithm 2 was relatively low in the training of neural networks: for ResNet18 on CIFAR-10, it was ~77% of the total training time without running Algorithm 2."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1623/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700009838782,
                "cdate": 1700009838782,
                "tmdate": 1700010544909,
                "mdate": 1700010544909,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hAGhRmM7zw",
                "forum": "8vKknbgXxf",
                "replyto": "Y08wFpgFTp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 84GA"
                    },
                    "comment": {
                        "value": "**Theorem 8 is of little use without the aforementioned discussion.**\n\nWe emphasize that our Theorem 8 has made important contributions in both theoretical and empirical perspectives, and we believe these points would be more important than the above discussion on the complexity. Theoretically, our Theorem 8 is a strict generalization of Theorem 4.7 in [Lee+ 2023], one of best known sufficient conditions for AD to compute a Clarke subderivative. More precisely, our Theorem 8 not only includes the previous theorem as a special case, but also covers many more cases such as convolutional networks with residual connections and normalization layers (which cannot be covered by the previous theorem). To our knowledge, our Theorem 8 is the first sufficient condition that is applicable to practical neural networks. Empirically, our Theorem 8 enables us to verify that AD actually computed a Clarke subderivative in several practical learning scenarios (Section 4). To our knowledge, there has been no prior work that empirically verified (or theoretically proved) that AD always outputs a Clarke subderivative in certain learning scenarios; our work is the first such work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1623/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700009880145,
                "cdate": 1700009880145,
                "tmdate": 1700009880145,
                "mdate": 1700009880145,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ueSu6jx8zb",
                "forum": "8vKknbgXxf",
                "replyto": "Y08wFpgFTp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1623/Reviewer_84GA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1623/Reviewer_84GA"
                ],
                "content": {
                    "title": {
                        "value": "Clarification on Theorem 8 and Algorithms 1 and 2"
                    },
                    "comment": {
                        "value": "Thanks for the clarification, I believe that this discussion on complexity of Algorithm 1, 2 and Theorem 8 should appear in the paper or in its appendix."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1623/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700038275404,
                "cdate": 1700038275404,
                "tmdate": 1700045234559,
                "mdate": 1700045234559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bwuZNrM927",
                "forum": "8vKknbgXxf",
                "replyto": "Ar4fa3uG5u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1623/Reviewer_84GA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1623/Reviewer_84GA"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors rebutal"
                    },
                    "comment": {
                        "value": "Let me thank the authors for their detailed answer. I must say that I was very suspicious about the claims that the authors made in their initial draft and the main reason is that I could not understand the mechanisms which would make the chain rule valid. This is due to my own fast reading and the heaviness of the notations, conditions etc ... \n\nI would like to revise my jugement about the content of the paper, I believe that the results contained in the paper represent considerable improvements of the state of the art and they do look reasonable, although I did not read all proof details. The addition made to the paper, in particuliar the initial table provides nice clarification.\n\nI still believe that the notations are very heavy and the paper would benefit from examples:\n\nWell known cases of failure of AD are the following at x = 0: \n- x -> relu(x) - relu(-x) \n- x -> relu(x) - relu(x) \n- x -> relu(-relu(x)) \n- maybe other ones\n\t\nThe paper would be much more accessible with a qualitative discussion following condition 1 and 2 to describe how such examples are ruled out by the condition, or after Theorems 4, 5, 6 to describe how the chain rule failure is handled on these examples. Also provide an intuition why algorithms 1 and 2 allow to handle chain rule failure after theorem 8, since relu without bias parameter is a maxpool.\n\nI still believe that Theorem 1 is a relatively minor extension of Theorem 3.6 of Lee et al: skiped connection could be modeled by appropriate choice of linear maps and enlarged layer sizes, and if the chain rule works for Bouligand, it should also work for Clarke, the main contribution of Lee et al. being the identification of the systematic presence of bias parameters as a sufficient condition to bypass failures of the chain rule."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1623/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700044920933,
                "cdate": 1700044920933,
                "tmdate": 1700044920933,
                "mdate": 1700044920933,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s3uju5Fxiz",
            "forum": "8vKknbgXxf",
            "replyto": "8vKknbgXxf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1623/Reviewer_XRNZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1623/Reviewer_XRNZ"
            ],
            "content": {
                "summary": {
                    "value": "This article provides an important step in the direction of providing a theoretical backbone to the behemoth topic of neural networks -- in particular, showing Clarke stationarity is the gold standard for this class of optimization problems, and they provide (modest, but appreciable) steps in this direction. The contribution does not appear to be huge, but the importance of the problem itself and the quality if the writing and results (if they are true) is worthy of publication.\n\nDue to a medical emergency, I unfortunately did not have the time to check the mathematical details and proofs of this article. To\naccount for this lack of availability, I am providing a low-confidence review with my impressions based on the results and impact to the research community."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "In my opinion, the question studied in this article is one of the\nmost fundamental and pressing questions to modern-day theoretical\nmachine learning. I am happy to see work in this direction."
                },
                "weaknesses": {
                    "value": "Of course, this is also a very challenging problem.\nAppropriately, this article appears to be a modest step in the\nright direction, with nothing particularly groundbreaking.\nNonetheless, the impact of this work would still be quite relevant. \n\nSubdifferential analysis is a very technical and detailed topic,\nand I must express my lack of confidence in the validity of the results.\nIt also appears that the authors have mixed up a Bouligand (directional) subdifferential\nand a Mordukhovich (sequential) subdifferential on page 3 of their\narticle. \nFurthermore -- and this not a qualm with the article specifically -- \nI have doubts about the ability of an ML\nconference (where reviewers have essentially two weeks to review 3+\narticles) to appropriately verify the correctness of the proofs\nin the 10+ pages of the (often un-reviewed or under-reviewed)\nappendices. I am hopeful that other referees have sufficient time to\nverify the proofs here, because if these results *are* indeed\naccurate, it is my opinion that this article would absolutely be\nworthy of publication."
                },
                "questions": {
                    "value": "Due to a medical emergency, I unfortunately do not have the time to properly vet this article. I offer my deep apologies."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1623/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1623/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1623/Reviewer_XRNZ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1623/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698694411094,
            "cdate": 1698694411094,
            "tmdate": 1699636091008,
            "mdate": 1699636091008,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0v5FOe84L9",
                "forum": "8vKknbgXxf",
                "replyto": "s3uju5Fxiz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XRNZ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive evaluation and valuable comments. We address all comments of the reviewer as follows.\n\n**It appears that the authors have mixed up a Bouligand (directional) subdifferential and a Mordukhovich (sequential) subdifferential on page 3 of their article.**\n\nWe believe our definition of a Bouligand subdifferential (on page 3) is standard in the literature (e.g., see [Cui+, Definition 4.3.1], [Walther+, Definition 3.2], [Burke+, Definition 9], [Stechlinski+, page 275]). A Mordukhovich subdifferential is a different notion of a generalized derivative, which is known to contain a Bouligand subdifferential (e.g., see [Walther+, Proposition 3.8], [Burke+, page 238]). We clarify that our paper does not use a Mordukhovich subdifferential (and does not mention it).\n\n[Cui+] Modern Nonconvex Nondifferentiable Optimization, SIAM, 2021.\\\n[Walther+] Characterizing and Testing Subdifferential Regularity in Piecewise Smooth Optimization, SIAM Journal on Optimization, 2019.\\\n[Burke+] The Subdifferential of Measurable Composite Max Integrands and Smoothing Approximation, Mathematical Programming, 2019.\\\n[Stechlinski+] Generalized Sensitivity Analysis of Nonlinear Programs, SIAM Journal on Optimization, 2018."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1623/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700010376734,
                "cdate": 1700010376734,
                "tmdate": 1700010376734,
                "mdate": 1700010376734,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lQvXeroLsy",
            "forum": "8vKknbgXxf",
            "replyto": "8vKknbgXxf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1623/Reviewer_uFDP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1623/Reviewer_uFDP"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the important problem of what is computed by automatic differentiation (AD) when the activation functions are not continuously differentiable, which is the most common case in practice with activations like the ReLU or its variants.\n\n---\n\nThe main results are several theorems (Thm 1, Thm 4, Thm 5, Thm 6, and Thm 8) that give sufficient conditions on a broad class of neural network architectures and training regimes (batch sizes) to ensure that AD always produces an element of the clarke subdifferential. Besides these sufficient conditions, there are several results showing how tight these results are, by proving that relaxing some of the sufficient conditions will result in AD producing something that is not an element of the clarke subdifferential.\n\nThere are two conditions considered to ensure that AD produces a clarke subgradient and both of them are centered on bias parameters. In the fist condition, the bias parameters must be distinct and the activations are applied componentwise. In the second condition, the bias parameters can be shared and more general activations (e.g., maxpooling, maxpool2d) are considered.\n\nThe strategy for showing that these conditions guarantee AD computes a clarke subgradient is to construct a sequence of points converging to the current point, on which the neural network is actually differentiable, and showing that the limits of the gradient on this set are converging to what is computed by AD, i.e., directly showing the definition of the Clarke subgradient is satisfied for what is computed by AD.\n\n---\n\nThe ultimate goal of the paper is theoretical but it does include some empirical validations of the theoretical claims made. The claims are tested by running sgd on fully connected networks and convolutional networks, both with activations that are not continuously differentiable.\n\nIn the fully connected case (they use 2 hidden layers with dimensions 256 and 64 respectively, trained on MNIST with batchsize 128), they check whether or not the sufficient condition to ensure AD computes a clarke subgradient is satisfied, and indeed it always is using three different activations (ReLU6, HardTanh, and HardSigmoid). They also confirm that the points where the activations are not continuously differentiable are seen by the algorithm (0 times for ReLU6, 9.8 times for HardTanh, and 13.8 times for HardSigmoid; all averaged over 5 runs).\n\nIn the convolutional case, they use the ordinary ReLU activation but combine with maxpools (they test 3 architectures - VGG11, VGG11-BN (batchnorm), and ResNet18 all on CIFAR10). They observe that their sufficient condition to ensure that AD computes a clarke subgradient is always satisfied here.\n\n---\n\nSmall comment: I find the usage of the word safe in the paper to be a bit weird. Whether or not AD computes a clarke subgradient doesn't make it safe or unsafe, even if it does not compute subgradients AD might still converge to a clarke-stationary point (see Bolte-Pauwels 2020). I also found this sentence \u201cThese correctness results show that AD computes the standard derivative at most inputs, yet provide no information about what it computes at the remaining inputs\u201d to be incorrect, Bolte-Pauwels 2020 *does* provide information about what is computed at the remaining inputs - an element of the conservative field is computed and this object can still be used to show convergence of sgd-like algorithms."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper studies an important problem and gives strong theoretical results that apply to a broad class of neural network architectures and training regimes. Despite being theoretical in nature, the results are quite practical since many of the sufficient conditions are checkable in practice, as demonstrated in secion 4 with experiments on some realistic architectures."
                },
                "weaknesses": {
                    "value": "I feel that the empirical validation section is missing something; it's always checked that the sufficient conditions derived in the paper are holding but it's never empiricaly validated that these sufficient conditions are actually sufficient for guaranteeing that AD computes a clarke subgradient. That being said, I still found the experiments are convincing."
                },
                "questions": {
                    "value": "Something that I didn't understand regarding the notion of distinct bias parameters - is it ever an issue that some bias parameters might be equal during training?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1623/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752245817,
            "cdate": 1698752245817,
            "tmdate": 1699636090908,
            "mdate": 1699636090908,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R7Wo15Kc41",
                "forum": "8vKknbgXxf",
                "replyto": "lQvXeroLsy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uFDP"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for their positive evaluation and thoughtful feedbacks. We address all comments of the reviewer, and provide pointers to the corresponding updates in the revised paper. All the updates are color coded in the revised version.\n\n**I find the usage of the word safe in the paper to be a bit weird. Whether or not AD computes a Clarke subgradient doesn't make it safe or unsafe.** \n\nWe thank the reviewer for pointing this out. According to the reviewer\u2019s comment, we have made the following changes in the revised version (pages 2 and 9) to avoid the word \u201csafe\u201d:\n\n> Before revision: \u201cThen, is AD unsafe without distinct bias parameters and the trivial minibatch size?\u201d // \u201cOur results and analyses would contribute to a better understanding and safer use of AD.\u201d\n\n> After revision: \u201cThen, without distinct bias parameters and the trivial minibatch size, when does AD compute an element of the Clarke subdifferential?\u201d // \u201cOur results and analyses would contribute to a better understanding of AD.\u201d\n\n**I also found this sentence to be incorrect: \u201cThese correctness results show that AD computes the standard derivative at most inputs, yet provide no information about what it computes at the remaining inputs.\u201d**\n\nWe thank the reviewer for pointing this out. We agree that this sentence is too strong so that it makes an incorrect claim. To weaken this sentence and clarify the work of [Bolte+ 20] (and other works), we have made the following changes in the revised version (pages 1 and 2):\n\n> Before revision: \u201cThese correctness results show that AD computes the standard derivative at most inputs, yet provide no information about what it computes at the remaining inputs.\u201d // \u201cWe remark that some recent works such as (Bolte and Pauwels, 2020a;b; Lee et al., 2020; Huot et al., 2023) proved the correctness of AD with respect to fundamentally new notions of generalized derivatives (e.g., conservative or intensional derivatives).\u201d\n\n> After revision: \u201cThese correctness results show that AD computes the standard derivative at most inputs, yet often provide little information about what it computes at the remaining inputs.\u201d // \u201cWe remark that some recent works such as (Bolte and Pauwels, 2020a;b; Lee et al., 2020; Huot et al., 2023) proved the correctness of AD over all inputs, with respect to fundamentally new notions of generalized derivatives (e.g., conservative or intensional derivatives).\u201d\n\n**I feel that the empirical validation section is missing something; it's always checked that the sufficient conditions derived in the paper are holding but it's never empirically validated that these sufficient conditions are actually sufficient for guaranteeing that AD computes a clarke subgradient. That being said, I still found the experiments are convincing.**\n\nWe are not sure if we correctly understand this comment, but we provide an answer to the following question: \u201cIf AD satisfies the sufficient condition given in Theorem 8 (or Theorem 5), then is the output of AD always an element of the Clarke subdifferential?\u201d Our answer to this question is \u201cYes\u201d, which is stated in Theorem 8 (or Theorem 5). If this question is not what the reviewer intended to ask, please let us know.\n\nWe also note that our sufficient condition in Theorem 8 is not necessary in general. Namely, there is an example in which our condition is violated (i.e., $\\mathcal P_l = \\emptyset$) but AD returns an element of the Bouligand subdifferential. Consider a network $f=g+h-h$, where $g=\\text{ReLU}(x)$ with $D^{AD}g(0)=0$ and $h=\\text{ReLU}(x)$ with $D^{AD}h(0)=1$. Then, for an input $x=0$, $\\mathcal P_l = (-\\infty, 0) \\cap (0, \\infty) = \\emptyset$. However, for this input, AD does return an element of the Bouligand subdifferential: $D^{AD} f(0) = 0 \\in \\partial^B f(0)$. We think developing necessary and sufficient conditions for verifying the correctness of AD is an interesting research direction."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1623/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700010260896,
                "cdate": 1700010260896,
                "tmdate": 1700010260896,
                "mdate": 1700010260896,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k2tPz2GJYL",
                "forum": "8vKknbgXxf",
                "replyto": "lQvXeroLsy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uFDP"
                    },
                    "comment": {
                        "value": "**Something that I didn't understand regarding the notion of distinct bias parameters - is it ever an issue that some bias parameters might be equal during training?**\n\nWe thank the reviewer for raising this question. In our paper, \u201cdistinct\u201d bias parameters essentially means that the bias parameters used in different neurons should be all distinct, as variables not as values. It is fine for these bias parameters to take the same values as long as they are distinct as variables.\n\nFor instance, consider two pre-activation functions $\\tau : \\mathbb{R}^2 \\times \\mathbb{R}^1 \\to \\mathbb{R}^2$ and $\\tau\u2019 : \\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}^2$ defined by $\\tau(x_1, x_2, b_1) = (x_1+b_1, x_2+b_1)$ and $\\tau\u2019(x_1, x_2, b_1, b_2) = (x_1+b_1, x_2+b_2)$. Then, $\\tau$ does not have distinct bias parameters since the two outputs of $\\tau$ use the same bias parameter $b_1$. In contrast, $\\tau\u2019$ has distinct bias parameters since all of its outputs use distinct bias parameters. Here, the values of $b_1$ and $b_2$ are not important and they do not change the fact that $\\tau\u2019$ has distinct bias parameters."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1623/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700010306081,
                "cdate": 1700010306081,
                "tmdate": 1700010306081,
                "mdate": 1700010306081,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ynGHkZahZZ",
                "forum": "8vKknbgXxf",
                "replyto": "R7Wo15Kc41",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1623/Reviewer_uFDP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1623/Reviewer_uFDP"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comprehensive response. Regarding the empirical evaluation, what I meant was that it would be useful to see an *empirical* validation of the statement:\n\n\"If AD satisfies the sufficient condition given in Theorem 8 (or Theorem 5), then is the output of AD always an element of the Clarke subdifferential?\"\n\nfor some neural network where one can actually compute explicitly the Clarke subdifferential."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1623/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415967596,
                "cdate": 1700415967596,
                "tmdate": 1700415967596,
                "mdate": 1700415967596,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Y7FzJBhIIc",
            "forum": "8vKknbgXxf",
            "replyto": "8vKknbgXxf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1623/Reviewer_DHhS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1623/Reviewer_DHhS"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends the previous results on the characteristics of auto-differentiation (AD) in modern neural networks. Existing methods were limited in cases where the ADs of individual operations are limited to Bouligand subdifferentials, which is extended to Clarke subdifferentials in this paper. The paper shows that if biases are distinct and the batch size is one, then the overall gradient of a fully connected network using AD is always correct (is an instance of Clarke subdifferentials). If the batch size is larger than one, ADs of individual operations should be a convex combination of left-side and right-side derivatives to have a correct gradient. If there are no distinct biases, then having linearly independent features whenever non-differential boundaries are touched. The paper also provides conditions for CNN (presence of max-pool,  shared bias). Experiments demonstrate that the conditions provided in the paper are empirically correct."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Even though auto-differentiations are widely used in practice, it is not well-known how it behaves on non-differentiable regions. These are usually ignored in practice. However, since most modern deep learning is usually performed numerically, these regions can sometimes be problematic. In this respect, accurate knowledge regarding the actual behavior can be beneficial.\n\nThis paper extends the existing results to wider conditions with more practical settings, i.e., Clarke subdifferentials, general fully-connected networks, and CNN as well. This significantly improves the usability of such knowledge in real situations (such as defining the individual gradient operation so that the overall gradient can be a Clarke subdifferential.)\n\nI did not check the proofs in detail, but they seem correct."
                },
                "weaknesses": {
                    "value": "There are a wide variety of recent network structures and they go beyond the conditions assumed in this paper. However, considering the nature of incremental improvement in this kind of theoretical work, I believe it is a sufficient contribution.\n\nOn page 6, it is said that the input and the hidden dimension are typically larger than the batch size. However, this can be somewhat controversial. There are indeed cases where large batch sizes are used (e.g., early self-supervised learning) for several reasons, e.g., training stability, training time/speed, etc."
                },
                "questions": {
                    "value": "On page 6, when batch size is larger than one, a convex combination of the left-side and right-side derivatives is required. Here, the equation seems to suggest that any element that touches the boundary must have the same combination weights. Is this correct?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1623/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830365133,
            "cdate": 1698830365133,
            "tmdate": 1699636090835,
            "mdate": 1699636090835,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xSOA7mEcUJ",
                "forum": "8vKknbgXxf",
                "replyto": "Y7FzJBhIIc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DHhS"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for their positive evaluation and valuable comments. We address all comments of the reviewer, and provide pointers to the corresponding updates in the revised paper. All the updates are color coded in the revised version.\n\n**There are a wide variety of recent network structures and they go beyond the conditions assumed in this paper. However, considering the nature of incremental improvement in this kind of theoretical work, I believe it is a sufficient contribution.** \n\nWe thank the reviewer for appreciating that our paper makes a sufficient contribution. We would like to add that our results already cover a wide range of network architectures, including fully-connected networks, convolutional networks (with residual connections, normalization layers, and maxpools), and transformer-based networks (with attention layers). Nevertheless, our results do not cover some networks, such as recurrent neural networks and the networks that use non-pointwise non-differentiable functions except maxpools (e.g., bilinear interpolations).\n\n**On page 6, it is said that the input and the hidden dimension are typically larger than the batch size. However, this can be somewhat controversial. There are indeed cases where large batch sizes are used for several reasons.** \n\nWe thank the reviewer for pointing this out. According to the reviewer\u2019s comment, we have made the following change in the revised version (page 6):\n\n> Before revision: \u201cSince the input and hidden dimensions $N_0, \\ldots , N_{L\u22121}$ are typically larger than the minibatch size $B$, this condition can be easily satisfied in practical learning setups, which we empirically demonstrate in Section 4.\u201d\n\n> After revision: \u201cIf the input and hidden dimensions $N_0, \\ldots, N_{L\u22121}$ are larger than the minibatch size $B$ (which often occurs in practical learning setups), this condition can be easily satisfied; we empirically demonstrate this in Section 4.\u201d\n\n**On page 6, when batch size is larger than one, a convex combination of the left- and right-side derivatives is required. Here, the equation seems to suggest that any element that touches the boundary must have the same combination weights. Is this correct?**\n\nYes, you are correct. The combination weight $\\lambda_l$ should remain the same for all inputs $x$ within the same layer (say the $l$-th layer). That is, for each layer $l$, there should exists a single constant $\\lambda_l$ such that $D^{AD}\\rho_l(x) = \\lambda_l D^{-}\\rho_l(x) + (1-\\lambda_l) D^{+}\\rho_l(x)$ for all $x$. Note that $\\lambda_l$ can be different for different $l$."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1623/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700010186368,
                "cdate": 1700010186368,
                "tmdate": 1700010186368,
                "mdate": 1700010186368,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zilr4yx3K5",
                "forum": "8vKknbgXxf",
                "replyto": "xSOA7mEcUJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1623/Reviewer_DHhS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1623/Reviewer_DHhS"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the answers."
                    },
                    "comment": {
                        "value": "Thank you for the answers. I'm also satisfied with the revision. I'll keep my original score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1623/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575699390,
                "cdate": 1700575699390,
                "tmdate": 1700575699390,
                "mdate": 1700575699390,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sKtlHyOb3w",
            "forum": "8vKknbgXxf",
            "replyto": "8vKknbgXxf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1623/Reviewer_FQtB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1623/Reviewer_FQtB"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of automatic differentiation of neural networks with non-smooth operations such as ReLu activation and max-pooling. It provides some theoretical results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The theoretical results are rich.\n2. The numerical results are consistent with the theoretical results to some extent."
                },
                "weaknesses": {
                    "value": "1. The results in the theorems are not intuitive and are difficult to follow.\n2. It is not clear how the numerical or theoretical result given by gradient descent differs from that given by subgradient descent."
                },
                "questions": {
                    "value": "1. Are the results in Table 2 given by gradient descent, subgradient descent, or Clarke subgradient descent?\n2. Are the theoretical results applicable to other nonsmooth activation functions such as step function?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1623/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698892306406,
            "cdate": 1698892306406,
            "tmdate": 1699636090760,
            "mdate": 1699636090760,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XhiIcBY8yT",
                "forum": "8vKknbgXxf",
                "replyto": "sKtlHyOb3w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1623/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FQtB"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive evaluation and valuable comments. We address all comments of the reviewer, and provide pointers to the corresponding updates in the revised paper. All the updates are color coded in the revised version.\n\n**The results in the theorems are not intuitive and are difficult to follow.**\n\nIn the initial draft, we gave an informal overview of our results in Section 1 (pages 2-3), and provided a more detailed yet still high-level description of our results right after each result (pages 5-8). For example, Theorem 1 is informally introduced in Section 1: \u201cTheorem 1 shows that AD always computes an element of the Clarke subdifferential, if the minibatch size is one and all non-differentiable neurons have distinct bias parameters.\u201d The theorem is explained in more detail yet still informally in Section 3.1: \u201cTheorem 1 states that if a network $\\Psi$ has distinct bias parameters and the minibatch size is one, then AD computes an element of the Clarke (or Bouligand) subdifferential of $\\Psi$ as long as the proxy gradient $D^{AD} \\rho_l(x)$ is an element of the Clarke (or Bouligand) subdifferential for all $l$ and $x$.\u201d We hope these informal and/or high-level explanations would make readers easier to follow our results.\nIn the revised draft, we have further improved readability by adding more explanations on our problem setup (Conditions 1 and 2) and more discussions on our theoretical results (pages 4\u20136).\n\n**It is not clear how the numerical or theoretical result given by gradient descent differs from that given by subgradient descent.**\n\nWe do not compare gradient descent and subgradient descent algorithms. The main objective of our paper is to understand the output of AD when neural networks contain non-differentiable activation functions such as ReLU and maxpools. Our theoretical results aim to characterize sufficient conditions under which AD always computes an element of the Clarke subdifferential (Theorems 1, 4, 5, 6, and 8), and other conditions under which AD might not do so (Lemma 2, 3, and 7). Our numerical results aim to verify whether our sufficient conditions are satisfied or not when we train neural networks via minibatch stochastic gradient descent (SGD), where the \u201cgradients\u201d used in SGD are computed by AD. We empirically observed that our sufficient conditions were always satisfied, i.e., AD always computed an element of the Clarke subdifferential in our experiments.\n\n**Are the results in Table 2 given by gradient descent, subgradient descent, or Clarke subgradient descent?**\n\nAs we noted in Section 4, all the results in Table 2 are given by minibatch stochastic gradient descent (SGD) algorithm implemented in PyTorch, where the \u201cgradients\u201d used in SGD are computed by AD (which is standard in practice). Hence, this learning algorithm is in general not a gradient, subgradient, or Clarke subgradient descent algorithm, since AD might not compute the true gradient or a (Clarke) subgradient for general neural networks (e.g., Lemmas 2 and 3). However, in our experiments, we verified that our sufficient conditions for AD to compute an element of the Clarke subdifferential were always satisfied. This result implies that the networks in Table 2 were in fact trained by a Clarke subgradient descent algorithm.\n\n**Are the theoretical results applicable to other nonsmooth activation functions such as step functions?**\n\nAll of our results hold for any piecewise-analytic activation function (Definition 1), which includes ReLU, Leaky-ReLU, HardSigmoid, HardTanh, etc. The step function, on the other hand, is not piecewise-analytic since it is discontinuous, so our results do not apply to it.\n\nRegarding step functions, we note that the Clarke subdifferential has been defined and studied only for continuous functions (especially locally Lipschitz functions), and not for discontinuous ones (see, e.g., [Clarke+, Chapter 2.6], [Cui+, Chapter 4.3]). Nevertheless, computing a Clarke subderivative of a locally Lipschitz network that may contain discontinuous activation functions could be an interesting research problem: e.g., $\\Psi(x) = {\\bf 1}[x>0] + {\\bf 1}[x\\le0] = 1$ is an example of such networks.\n\n[Clarke+] Optimization and nonsmooth analysis, SIAM, 1990.\\\n[Cui+] Modern Nonconvex Nondifferentiable Optimization, SIAM, 2021."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1623/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700009977100,
                "cdate": 1700009977100,
                "tmdate": 1700009977100,
                "mdate": 1700009977100,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]