[
    {
        "title": "Graph is All You Need? Lightweight Data-agnostic Neural Architecture Search without Training"
    },
    {
        "review": {
            "id": "O54Ag7xaQw",
            "forum": "aKivEaIbN2",
            "replyto": "aKivEaIbN2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6348/Reviewer_UMqx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6348/Reviewer_UMqx"
            ],
            "content": {
                "summary": {
                    "value": "- The authors propose a novel, cheap, training-free, and data-agnostic neural architecture search (NAS) method.\n- The proposed method converts an architecture to a graph and utilizes extracted graph measures such as average degree as a proxy for the performance of the architecture.\n- The empirical results show that the proposed method can find architectures with better performance in computational costs compared to the baseline methods.\n- Surprisingly, the proposed method can find the best architecture among 200 randomly sampled architectures from NAS-Bench 201 dataset in 217 CPU seconds without any GPU computations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method is simple and easy to understand.\n- The proposed conversion method seems novel.\n- The authors suggest techniques for further speeding up the proposed method by using surrogate models.\n- The empirical results show the superiority of the proposed method.\n- The paper contains the comparison among various graph measures in its appendix."
                },
                "weaknesses": {
                    "value": "- I think the paper needs more justification about the importance of the training-free data-agnostic NAS.\n- The experimental results are evaluated on only simple benchmarks. To show the practicality of the proposed method, I think a it would be more helpful if you provide a comparison with other non-training-free NAS methods such as DARTS-PT on a larger search space such as DARTS space."
                },
                "questions": {
                    "value": "- I am curious about the correlation between model inference time and proxy metrics. Since the proposed \"average degree\" measures the connectivity between channels, a higher average degree may lead to a slower inference speed.\n- The ultimate goal of NAS is to find the optimal architecture in any way possible. Even if the proposed method has a high correlation with model performance, it is uncertain whether it will be significantly helpful in quickly finding the best architecture. This is because one must train selected architectures to evaluate the actual performance and verify whether it is a good architecture, and it seems that most of the time will be spent in this verification process during the NAS procedure. Can you kindly and elaborately explain why training-free NAS is an important issue, along with practical scenarios?\n- Of course, unlike existing non-training-free NAS methods, since the proposed method has near-zero search cost, most of the time can be spent on validation. Nonetheless, I am curious whether it can find a better architecture than baselines such as DARTS-PT within the same computational budget in the DARTS space.\n\nIf my concerns are addressed, I promise to raise the score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "There aren't any ethical concerns related to this paper."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6348/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6348/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6348/Reviewer_UMqx"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6348/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789613652,
            "cdate": 1698789613652,
            "tmdate": 1699636699669,
            "mdate": 1699636699669,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JfVwLqRv1g",
                "forum": "aKivEaIbN2",
                "replyto": "O54Ag7xaQw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6348/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6348/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comments. Below is the one-to-one response:\n\n- Weakness 1st point\n\nNeural architecture search finds candidate architectures, trains them to obtain their accuracy and uses the accuracy as feedback to modify its search strategy. NAS is computationally expensive and slow and training the architecture consumes the majority of the time. Training-free NAS techniques developed proxy scores in lieu of accuracy. This avoids the need to train the architecture altogether and the training-free metric is computed on a randomly initialized model. These metrics are relatively fast to compute. For instance, our average degree takes only 1 CPU second per architecture. \nThe candidates are now ranked using these proxy scores instead. Thus, training-free NAS accelerates the search drastically. \n\nTo save further time, a lot of these training-free NAS algorithms compute the proxy scores only on a few mini-batches of the data. This score might not generalize well across the entire dataset and has a high variance based on the mini-batches passed. To address all the above mentioned issues, we developed a data-agnostic training-free NAS method.\n\n- Weakness 2nd point\n\nIn NAS, the search algorithm searches for a candidate architecture and uses its validation accuracy to determine the candidate\u2019s efficacy. All the training-free metrics  are used in lieu of accuracy. Instead of ranking the candidate models based on the accuracy and selecting the best model found so far, one could use the proxy scores. These training-free metrics can be combined with any existing NAS algorithm. Thus it is essential that all these training-free metrics have a very high correlation between the ranking of the candidates by their score and the ranking by the validation accuracy. We already reported the correlation metrics on more complex search spaces such as ENAS, PNAS, DARTS etc.  in Table 4 of our paper. \n\nNDS-DARTS is a NAS Benchmark developed for DARTS search space. The accuracy of the best architecture found by our metric is 93.23% . NASWOT got 90.6% on the same search space. On NAS-Bench-201 search space, DARTS-PT reported a test accuracy of 88.11% (table 4 of the paper [2]). Our method using average degree yielded an architecture with 90.2% test accuracy.\n\n- Question 1st point\n\nFollowing the reviewer's comments, we record the inference time for architectures of 10 highest average degrees, 10 lowest average degrees, 10 highest test accuracies and 10 lowest test accuracies in NAS-Bench-201. In this case, the reviewer's intuition is right that the architectures correspond to a higher average degree tend to have a longer inference time. The inference time is recorded in the NAS-Bench-201 benchmark. \n\n|                                       |   High average degree | Low average degree | High test accuracy | Low test accuracy |\n| -------------------------- | ------------------------- | ----------------------- | ---------------------- | --------------------- |\n| Average inference time |            23.10 ms          |             10.44 ms       |            20.29 ms     |           13.74 ms      |\n\n- Question 2nd point\n\nIn NAS, the search algorithm searches for a candidate architecture and uses its validation accuracy to determine the candidate\u2019s efficacy. The goal of the algorithm is to find the architecture with the best test accuracy. Each candidate architecture is trained and then its validation accuracy is used as the feedback for the search algorithm. NAS is very computationally expensive, requiring every candidate architecture to be trained. To alleviate this problem, training-free NAS was introduced [3, 4, 5].\n\nThe training-free metrics are used in lieu of accuracy. The training-free metric is computed on a randomly initialized model and is generally fast to compute. Instead of ranking the candidate models based on the validation accuracy and selecting the best model found so far, the candidate models are now ranked using this training-free metric. \n\nIn particular, our average degree metric just randomly initializes the candidate architecture and computes the average degree of the model. This is extremely fast and takes only a ~1 CPU second for each model.\n\n- Question 3rd point\n\nNDS-DARTS benchmark [1]  is created by randomly sampling architectures from the DARTS search space. The best performing architecture in this search space has the test accuracy of 95.06 on CIFAR10. When we searched on the search space using the average degree as the metric, the test accuracy of the best model we found was 93.23%. Thus, our method generalizes to larger search spaces as well. \n\nDARTS-PT also performed architecture search on the NAS-Bench-201 benchmark and reported a test accuracy of 88.11% (Table 4 of their paper [2]). NAS using average degree on NAS-Bench-201 yielded an architecture with 90.2% test accuracy."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6348/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700252656718,
                "cdate": 1700252656718,
                "tmdate": 1700252656718,
                "mdate": 1700252656718,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FAqqCG7wKL",
                "forum": "aKivEaIbN2",
                "replyto": "JfVwLqRv1g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6348/Reviewer_UMqx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6348/Reviewer_UMqx"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my inqueries.\n\nI think there might be a slight misinterpretation regarding the second point of my question.\n\nI'm fine with the search cost being 1 second. However, even if a promising set of candidates is quickly identified, to determine if these candidates are truly effective, end-to-end actual training is necessary to measure the test accuracy, which will consume most of the time and cost.\n\nConsider a hypothetical scenario: \n\n======================================\n\nWe aim to identify an architecture with a test accuracy higher than $c$. Suppose evaluating a single architecture requires 4 GPU hours. Assume that there are two NAS methods having the following properties.\n\n- Method A requires 1 second to rank whole candidates but the first architecture that has a test accuracy higher than $c$ is ranked 50th.\n- Method B requires 100 GPU hours for searching but can find test accuracy higher than $c$ at once.\n\n=======================================\n\nIn this context, Method A, despite its rapid ranking ability, results in an overall NAS time of 1 second + (4 GPU hours \u00d7 50), which exceeds Method B's total of 100 GPU hours + (4 GPU hours \u00d7 1).\n\nCan you provide experimental evidence or scenarios to validate the effectiveness of the zero-shot approach compared to non-zero-shot NAS methods (such as [1,2,3,4]) in the end-to-end NAS process? It can be illustrated through the plot comparing 'the best test accuracy versus total runtime', where 'total runtime' encompasses both the search time and test accuracy evaluation time.\"\n\nI am aware of the limited time remaining in the review period. However, for the camera-ready version, I recommend elaborating on why the proposed method is beneficial in the overall end-to-end process.\n\n[1] Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS, Shi et al., NeurIPS 2020.\n\n[2] Rethinking Architecture Selection in Differentiable NAS, Wang et al., ICLR 2021.\n\n[3] $\\beta$-DARTS: Beta-Decay Regularization for Differentiable Architecture Search, Ye et al., CVPR 2022.\n\n[4] Shapley-NAS: Discovering Operation Contribution for Neural Architecture Search, Xiao et al., ICLR 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6348/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510385411,
                "cdate": 1700510385411,
                "tmdate": 1700510385411,
                "mdate": 1700510385411,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ssRRbjzjuj",
            "forum": "aKivEaIbN2",
            "replyto": "aKivEaIbN2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6348/Reviewer_EKQj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6348/Reviewer_EKQj"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes NASGraph, a training-free Neural Architecture Search (NAS) method that relies on a graph-based interpretation of neural architectures. NASGrPh first converts neural architectures to computational graphs and then utilizes properties of the generated graphs as proxy of validation accuracies of corresponding architectures. NASGraph utilizes graph blocks to modularize neural architectures and determine the connectivity of graph nodes. The proposed conversion method allows NASGraph to construct a computational graph that reflects the forward propagation process of a neural architecture. The effectiveness of NASGraph is verified across various NAS benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- While this is not the first work to perform architecture search with graph-based representations of neural architectures, I believe the proposed approach is far more thorough in incorporating the actual computations that occur within neural architectures. NASGraph goes above and beyond simply converting neural architectures into DAGs by considering how the inputs are being processed and mapped to outputs during the forward propagation process.\n- The final proxy metric to the validation accuracy of neural architectures (the average degree of the graph measure) makes sense and is theoretically-grounded.\n- The experiments that span various NAS benchmarks are extensive and comprehensive."
                },
                "weaknesses": {
                    "value": "- It appears that NASGraph assumes the inputs to the neural architecture and subsequent graph blocks is non-negative. Does the analysis hold even with non-negative inputs? Many of modern neural architectures utilize activation functions that could yield negative inputs/outputs (e.g., gelu activation). Can NASGraph generalize beyond relu-based architectures?\n- In a similar vein, can the proposed method generalize to non-conv-based architecture spaces? Such as ViTs and MLPMixers?\n- The authors define the final score of neural architectures (the average degree of the graph measure) in Section 4.2 under the performance evaluation section. I think it would be more appropriate to move this definition to somewhere towards the end of Section 3 because by the end of Section 3, the reader is left hanging without the knowledge of how NASGraph actually ranks neural architectures.\n- In Section 4.2, the authors mention that they explore other graph measures as well. Any idea why the average degree works best out of the compared graph measures? Also, intuitive explanation to what each one of these graph measures actually indicates/implies would be helpful."
                },
                "questions": {
                    "value": "Please refer to the Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6348/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6348/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6348/Reviewer_EKQj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6348/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698982330645,
            "cdate": 1698982330645,
            "tmdate": 1699636699553,
            "mdate": 1699636699553,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g8jaq7LGtb",
                "forum": "aKivEaIbN2",
                "replyto": "ssRRbjzjuj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6348/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6348/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comments. Below is the one-to-one response:\n\n- Weakness 1st point\n\nWhen there is non-negative input to the graph block involved, we need a threshold to create non-negative edge weights in our graph. When the input is larger than the threshold, we consider it as active. Otherwise, we consider it as inactive. Therefore, our method can be applied to GeLU activation as well.\n\n- Weakness 2nd point\n\nGiven that we convert a neural network to a graph and our proposed framework only considers the input/output tensor, in principle, our method is applicable to most search spaces. However, owing to the lack of publicly available NAS benchmarks for those search spaces, we cannot evaluate it on those search spaces right now.\n\n- Weakness 3rd point\n\nWe appreciate this valuable suggestion. We will modify section 4.2 accordingly: the average degree is now introduced at the bottom of Section 3.\n\n- Weakness 4th point\n\nIntuitively, each graph edge indicates if the previous graph block (defined in the context of the NASGraph) contributes to the current graph block. An extreme case is that the graph block is Zero operation (used in NAS-Bench-201). In this case, the previous graph block does not contribute to the current graph block. There will be no graph edge connecting these two graph blocks. The performance of such an architecture is expected to be inferior. In short, the more the number of edges (higher average degree), the better the performance of the corresponding architecture would be. To further illustrate this, in Section A.12 of our paper, we plotted the average degrees of the architectures with the highest and the lowest accuracy in the NASBench 201 benchmark. As expected, the architecture with the highest accuracy has a very high average degree compared to the other one. \n\nIn addition to the average degree, we explore other graph measures such as wedge, resilience, and density. We refer the reviewer to Section A.8 in the appendix."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6348/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700251291590,
                "cdate": 1700251291590,
                "tmdate": 1700251291590,
                "mdate": 1700251291590,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HqxFrLD5MP",
            "forum": "aKivEaIbN2",
            "replyto": "aKivEaIbN2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6348/Reviewer_bWfm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6348/Reviewer_bWfm"
            ],
            "content": {
                "summary": {
                    "value": "Neural Architecture Search (NAS) is inherently as expensive task. Zero cost proxies aim at bypassing this compute by using cheap to evaluate statistics on architectures to predict the most optimal architectures. This paper proposes a novel graph-based zero cost proxy based on properties of graph representations of architectures.  The method is evaluated on multiple NAS-Bench-Suite Zero tasks yielding competitive performance.  The method overcomes several issues with current zero cost proxies e.g. data dependency, GPU requirement, and operation preference."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper in general is well written and the results/evaluation well presented\n- Results on the NAS-Bench-Suite-Zero tasks are quite competitive. Evaluation of complementary nature of the proxy, from table 5 is interesting. \n- The method is fairly novel and interesting"
                },
                "weaknesses": {
                    "value": "- I went through the example in Figure 1 and the caption and the example itself is still unclear to me. Could the authors please elaborate on this?\n- I find the evaluation of the method quite weak especially since the authors do not compare against the MeCo proxy https://openreview.net/pdf?id=KFm2lZiI7n. On an initial glance it seems that MeCo outperforms the proposed proxy on most of the benchmarks. The code for MeCo is publicly accessible and I encourage the authors to compare their work with MeCo in terms of the correlation metric and search time. \n- I currently find the search spaces to be very limited to cell-based spaces or benchmarks. Since recently there have been efforts to scale Neural Architecture Search to weight-entangled spaces like AutoFormer [1], OFA [2] and HAT[3], it would be great to evaluate the method on these spaces. Note though these spaces don't have a tabular benchmark for evaluation, they do provide a pre-trained surrogate model for fast evaluation. \n- Since there has been growing interest in transformer spaces, is the proxy search space agnostic and directly applicable to transformer spaces? This was unclear to me from the paper.\n\n[1] Chen, M., Peng, H., Fu, J. and Ling, H., 2021. Autoformer: Searching transformers for visual recognition. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 12270-12280).\n\n[2] Cai, H., Gan, C., Wang, T., Zhang, Z. and Han, S., 2019. Once-for-all: Train one network and specialize it for efficient deployment. arXiv preprint arXiv:1908.09791.\n\n[3] Wang, H., Wu, Z., Liu, Z., Cai, H., Zhu, L., Gan, C. and Han, S., 2020. Hat: Hardware-aware transformers for efficient natural language processing. arXiv preprint arXiv:2005.14187"
                },
                "questions": {
                    "value": "- Check weaknesses: Could the authors compare against MeCo (https://openreview.net/pdf?id=KFm2lZiI7n) and could the authors evaluated on weight entangled macro-architecture spaces like AutoFormer[1], OFA[2] and HAT[3]?\n- How does the cuda memory consumption and search time of the method compare against other proxies?\n- Reproducibility is still am important challenge in the NAS literature . For reproducibility, could the authors make the code publicly available?\n\n\n[1] Chen, M., Peng, H., Fu, J. and Ling, H., 2021. Autoformer: Searching transformers for visual recognition. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 12270-12280).\n\n[2] Cai, H., Gan, C., Wang, T., Zhang, Z. and Han, S., 2019. Once-for-all: Train one network and specialize it for efficient deployment. arXiv preprint arXiv:1908.09791.\n\n[3] Wang, H., Wu, Z., Liu, Z., Cai, H., Zhu, L., Gan, C. and Han, S., 2020. Hat: Hardware-aware transformers for efficient natural language processing. arXiv preprint arXiv:2005.14187"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6348/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6348/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6348/Reviewer_bWfm"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6348/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699437622997,
            "cdate": 1699437622997,
            "tmdate": 1699636699441,
            "mdate": 1699636699441,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BvRbd3Mp2j",
                "forum": "aKivEaIbN2",
                "replyto": "HqxFrLD5MP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6348/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6348/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comments. Below is the one-to-one response:\n\n- Weakness 1st point:\n\nAs detailed in Sec. 3, we group neural layers (or operations such as conv, bn and relu) as graph blocks as shown in Figure 1 (a). Then, we do one single forward propagation to determine if the previous graph block contributes to the current graph block in a channel-wise manner. If there is a contribution, we build a graph edge. Otherwise, there is no graph edge. Each channel of the input tensor (or the output tensor) is aggregated to a graph node. This way, we are able to build a new type of graph given an architecture. The connectivity between graph nodes is determined by the forward propagation, i.e., outputs of graph blocks. We do a neural architecture search by comparing graph measures.\n\n- Weakness 2nd point\n\nWe appreciate the suggestion of making comparisons to MeCo, a recent publication just accepted to NeurIPS 2023 and to be presented in December 2023. However, we want to bring to the reviewer\u2019s attention that this is an unfair ask, because our ICLR submission was made back in Sep. 2023. To our best knowledge, the earliest public publication record of MeCo is Sep. 21st 2023, on NeurIPS\u2019s Openreview system (we did not find any arxiv version). This situation makes it impossible to compare to MeCo at the time of our submission, not to mention that MeCo should be considered as a concurrent work. Despite the impossibilities, we follow the reviewer\u2019s suggestion to include the comparison in the rebuttal and the updated manuscript. But we hope our review rating will not be negatively and unfairly judged by not having comparisons to a concurrent work that is only publicly available around the ICLR submission deadline. \n\nWe compared our method with MeCo on  NAS-Bench-101, NAS-Bench-201, Trans-NASBench 101, NDS-DARTS and NDS-ENAS. While it is true that MeCo has competitive performance on NAS-Bench-101 and NAS-Bench-201 as reported in the MeCO paper, we observed that MeCo does not generalize well on other benchmarks. As stated in [1], [2], there is no one proxy that performs the best across all the search spaces. We compared MeCo on the Trans-NASBench 101 search space and evaluated it on the NDS-DARTS and NDS-ENAS search spaces, as those search spaces were already reported in our paper. We used one A40 NVIDIA GPU and one AMD EPYC 7232P CPU to run Meco. Our method runs only on a CPU and does not require a GPU.\n\nTrans-NASBench-101 results:\n\n|                                   | surface_norm | class_scene | room_layout  | Average Running Time  |\n|-------------------------|-----------------|---------------|-----------------| --------------------------- |\n| MeCo                        |      0.65           |        0.62       |       -0.25        |     6.88 GPU hours        |\n| MeCo opt                  |      0.67           |        0.64       |        0.26        |     6.90 GPU hours        |\n| Avg degree (ours)     |      0.66           |        0.70       |        0.37        |     2.93 CPU hours        |\n| Wedge (ours)            |      0.72           |        0.74       |        0.42        |     3.04 CPU hours        |\n\nIt is important to note that while most metrics perform well on NASBench 101 and 201, their performance drops on Trans-NasBench-101.  We used the publicly available Meco code to evaluate it on NDS search space and the correlation is given in the table below.\n\n|                                |     NDS-DARTS*       |   NDS-ENAS*   | Average Running Time |\n| --------------------- | ----------------------- | ----------------- | -------------------------- |\n|  MeCo                    |            0.31              |         0.19          |        36.50 GPU hours   |\n| Avg degree (ours)  |            0.62              |         0.57           |         3.34 CPU hours    |\n\n*There is no performance reported on NDS search space, so we ran experiments using the publicly available code and computed the correlation. We did encounter some bugs while trying to run Meco\u2019s code, but we fixed it to the best of our knowledge. \n\n[1] https://iclr-blog-track.github.io/2022/03/25/zero-cost-proxies/  \n[2] NAS-Bench-Suite-Zero: Accelerating Research on Zero Cost Proxies, Krishnakumar et al.\n\n- Weakness 3rd point\n\nAs the reviewer rightly pointed out, there are no publicly available benchmarks for AutoFormer, OFA and HAT. While OFA does have a surrogate model, the error introduced by the surrogate model would definitely hamper us from determining the true correlation. AutoFormer and HAT do not even have surrogate models to predict the accuracies given an architecture. We do not have the computational power to evaluate 1000 architectures in these search spaces to compute the correlation."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6348/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700251064491,
                "cdate": 1700251064491,
                "tmdate": 1700451297297,
                "mdate": 1700451297297,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]