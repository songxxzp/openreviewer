[
    {
        "title": "Temporal Generalization Estimation in Evolving Graphs"
    },
    {
        "review": {
            "id": "uwdoMDNSCs",
            "forum": "HFtrXBfNru",
            "replyto": "HFtrXBfNru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4683/Reviewer_e5fg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4683/Reviewer_e5fg"
            ],
            "content": {
                "summary": {
                    "value": "The paper analyzes and tackles the challenge of temporal representation distortion in GNNs as the graph evolves over time, so as to yield better generalization estimation. Based on a few assumptions, the authors theoretically establish that such distortion is strictly increasing over time with a lower bound. Furthermore, based on the pre-trained GNN and an RNN that performs temporal generalization estimation, the authors propose a solution that incorporates an adaptive feature extractor that operates through self-supervised graph reconstruction, aiming to estimate and adjust for the distortion. Under a synthetic random graph setting,  a closed-form generalization error is also derived. Experiments on synthetic and real-world benchmarks demonstrate the effectiveness of the proposed method in terms of substantially improving estimation accuracy and generalization ability."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe general motivation for analyzing the generalization of evolving graphs is interesting and important. It is also interesting to see the theoretical analysis with a synthetic random graph demonstration with a closed-form generalization error.\n2.\tThe presentation quality of this paper is in general satisfactory, in terms of the organization and figure demonstration.\n3.\tThe experiments and analysis on the proposed method itself are relatively sufficient including model component ablation and hyper-parameter study."
                },
                "weaknesses": {
                    "value": "1.\tOne major concern is the lack of baseline methods. The proposed method is only compared with a linear regression, which turns out to be a naive baseline. The methods mentioned in the related work D.2 are not compared. Moreover, only the MAPE with standard error is evaluated. More evaluation metrics are expected.\n2.\tIt will be better if the methods are evaluated on a few more state-of-the-art backbones to further enhance the effectiveness of the proposed method.\n3.\tThe assumptions made to conduct theoretical analysis and bound derivation could be a concern as they are often too strong. Evolving graphs also contain nodes disappearing in many cases(e.g. sensor networks with possible malfunction over time), which means the assumption of an ever-growing network can be often invalid. Moreover, the zero-mean requires a non-min-max preprocessing of features. The authors need to be specific about the claims.\n4.\tIt would be better to sketch the connection between the information loss and the form in Equation 2, section 3.2.\n5.\tIt would be better to define some notations such as the function l, and generalization error beforehand for better understanding of the idea."
                },
                "questions": {
                    "value": "Please see the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N.A."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4683/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727708812,
            "cdate": 1698727708812,
            "tmdate": 1699636449826,
            "mdate": 1699636449826,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pvisc1q9tT",
                "forum": "HFtrXBfNru",
                "replyto": "uwdoMDNSCs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4683/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4683/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response tot Reviewer e5fg"
                    },
                    "comment": {
                        "value": "Thanks for your time and valuable comments that may help us for further improvement. We hope the following responses will serve to address your concerns. \n\n**Q1: Additional baselines and evaluation metrics.**\n\nR1: Thanks for your valuable suggestions. We have conducted additional two baselines (DoC and Supervised) with 4 classical evaluation metrics for regression task (MAPE, RMSE, MAE and Standard Error). The performance comparison is shown in Table 1 of the general response. Our SMART consistently outperforms both Linear and DoC, and closely approach the supervised methods. We provide more detailed information in Appendix H.3 in our revised paper. Regarding the methods discussed in related work in Appendix D.2, these methods focus on how to improve model performance on evolving graphs, instead of on how to characterize the model performance dynamics in graph evolution. Since the target is different,  these methods therefore can not be used for estimating generalization dynamics. \n\n**Q2: Performance on latest GNN backbone.**\n\nR2: Thanks for your comment. To fully validate and compare the effectiveness of different methods, we add Graph Transformer backbone besides GCN, GAT and GraphSAGE. As shown in Table 5, first of all, our SMART shows consistent best performance among different architectures. For different graph neural network architectures, they tend to capture the structure and feature information from different aspects. Due to the simple model structure of GCN, our SMART shows advanced prediction performance among other architectures, which is also consistent with the theory of classical statistical machine learning.\n\nTable 5. Performance comparison of different GNN architectures\n\n| Dataset | Model | GCN | GAT | GraphSAGE | TransformerConv |\n| --- | --- | --- | --- | --- | --- |\n| OGB-arXiv | Linear | 10.5224 | 12.3652 | 19.5480 | 14.9552 |\n|  | DoC | 13.5277\u00b11.4857 | 12.2138\u00b11.222 | 20.5891\u00b10.4553 | 10.0999\u00b10.1972 |\n|  | SMART | 2.1897\u00b10.2211 | 3.1481\u00b10.4079 | 5.2733\u00b12.2635 | 3.5275\u00b11.1462 |\n| DBLP | Linear | 16.4991 | 17.6388 | 23.7363 | 18.2188 |\n|  | DoC | 4.3910\u00b10.1325 | 13.8735\u00b14.1744 | 11.9003\u00b11.8249 | 9.0127\u00b12.6619 |\n|  | SMART | 3.4992\u00b10.1502 | 6.6459\u00b11.3401 | 9.9651\u00b11.4699 | 6.4212\u00b11.9358 |\n| Pharmabio | Linear | 32.3653 | 29.0404 | 31.7033 | 31.8249 |\n|  | DoC | 8.1753\u00b10.0745 | 7.4942\u00b10.0702 | 6.6376\u00b10.0194 | 5.3498\u00b10.2636 |\n|  | SMART | 1.3405\u00b10.2674 | 1.2197\u00b10.2241 | 3.1448\u00b10.6875 | 2.7357\u00b11.1357 |\n\n**Q3: Assumptions in theoretical analysis, including ever-growing and zero-mean normalized feature.**\n\nR3:  Thanks for your valuable suggestions. We generalize these two assumptions in the following way. (1) Regarding the ever-growing assumption of graphs, we modify it to the setting where nodes appearing after deployment have a probability of disappearing (Assumption 3-3). (2) Regarding the zero-mean normalized feature, we modify it to the setting where the feature vector has a constant mean conditioned on all previous graph states (Assumption 3-4). The completed assumptions please refer to Assumption 3 in Appendix C.2. Based on this assumption, we further extend the unavoidable representation distortion theorem to Theorem 4, and the proof of Theorem 4 can be found in Appendix C.4 and C.5. \n\n**Q4: The connection between information loss and graph reconstruction.**\n\nThanks for your comments. Please refer to the response to Q3 in general response. \n\n**Q5: The definition of function $I$ and the generalization error.**\n\nR5: Thanks for your advice. In our paper, $I$ denotes the mutual information. For two random variable $X$ and $Y$, the mutual information $I$ is denoted as follows:\n\n$$I(X ; Y)=\\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log \\frac{p(x, y)}{p(x) p(y)}$$\n\nGeneralization error refers to the performance of a trained model when making predictions on new, unseen data compared to the performance on the data it was trained on. In our work, the generalization error we estimate is the performance of the trained GNN model on the test samples during the graph evolution process. We have added the above definition to the revised paper, and we believe that this will be more helpful for readers to understand our work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4683/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406232825,
                "cdate": 1700406232825,
                "tmdate": 1700406232825,
                "mdate": 1700406232825,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZYBcLdPgIN",
            "forum": "HFtrXBfNru",
            "replyto": "HFtrXBfNru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4683/Reviewer_JYus"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4683/Reviewer_JYus"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an important question: how to ensure that GNN gives a good prediction for graphs that are rapidly evolving in time. The authors first give a theoretical proof to show that under mild conditions, as graph evolves, graph representation distortion is not avoidable (the loss is lower bounded). Secondly, the authors propose information losses in two phases: (1) information loss induced by RNN and (2) information loss induced by representation distortion. Then, the authors have proposed SMART that contains contrastive graph construction. The augmented feature graphs is randomly adding or dropping edges, and then structure reconstruction loss and feature reconstruction loss are used on the contrasted examples. The authors have verified the effectiveness of SMART on the barabasi-albert random graph, and performed extensive experiments to showcase SMART's effectiveness with evolving graphs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "S1. The paper targets evolving graph prediction, which is a challenging problem in the research community. It is novel to consider the evolving graph using information loss perspective, and the contrastive solution to resolve such problem is very convincing.\n\nS2. The paper is clearly written. The authors give a clear problem definition with theoretical justification. There are several architecture and losses considerations in the methodology, yet each component is clearly addressed. The author has also provided a theoretical proof on the barabasi-albert random graph to showcase the effectiveness of SMART.\n\nS3. The empirical results presented in the paper show that the proposed method SMART outperforms linear regression model, using the same GCN/GAT/GraphSage backbone structures and evaluated on four different datasets, showcasing the quality and robustness of SMART. \n\nS4. Given the ubiquity and increasing reliance on evolving graph in real-world applications, the capability to adapt to evolving graph is important. SMART can adapt to various graph learning architectures and has potential impact for studying the changing graphs in time.\n\nS5.  The ablation study is well-written and consider various loss configurations and hyper-parameter configueratons."
                },
                "weaknesses": {
                    "value": "W1. The paper does not have a related work section. There should be at least some descriptions of the previous work which showcases that GNNs performances can suffer from the representation distortion over time, and the performance degradation occurs.\n\nW2. There appears to be an inconsistency of mentioning the information loss in Section 3.2 and introducing SMART with contrastive loss in Section 3.3. The information loss is introduced, but not used in experiment settings (MPAE and standard error are used instead) or the later contrastive loss calculation. There should at least some experiments and measurements that connect information loss to contrastive learning post deployment, and to show that SMART is able to reduce the information loss induced by representation distortion. A theoretical justification is also fine.\n\nW3. The data augmentation in graph is too naive. The authors have only considered to randomly add or drop edges, which should only work for graphs without edge labels. While the random data augmentation technique works for barabasi-albert random graphs, it should not be effective for the cases where edge labels are also changing (for example, people change their relationship status with other people). The authors should consider more complex data augmentation techniques in graphs."
                },
                "questions": {
                    "value": "Q1. Theorem 1 is evaluated only for one layer GNN with a Leaky ReLU activation. Is Theorem 1 adjustable to multi-layer GNN and different GNN backbone structures such as GCN/GAT/GraphSage?\n\nQ2. Figure 3, what is the y-axis prediction loss? Why not use other evaluation metrics to showcase the GNN prediction performances deterioration?\n\nQ3. Is there a particular reason that RNN is used instead of other time-series model to capture the temporal variation?\n\nQ4. Will the contrastive graph reconstruction improve the feature extractor during the pre-deployment phase? Why just limit the contrastive learning and reconstruction to post-deployment?\n\nQ5. How will the proposed algorithm be used towards more complex real-time graphs? For example, how to deal with graphs that contain changing node labels and changing edge labels? How to apply the methods to heterogeneous graphs or spatial temporal graphs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4683/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4683/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4683/Reviewer_JYus"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4683/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809968883,
            "cdate": 1698809968883,
            "tmdate": 1699636449736,
            "mdate": 1699636449736,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CzTqcuPlZH",
                "forum": "HFtrXBfNru",
                "replyto": "ZYBcLdPgIN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4683/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4683/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JYus (1/2)"
                    },
                    "comment": {
                        "value": "Thanks for appreciating our work and give very detailed reviews. We are greatly encouraged that you appreciated our contributions including novel and convincing methods, problem significance, clear writing and potential impact of our study. We hope the following response will serve to address your concern and improve your confidence. \n\n**Q1: Supplement to related work.**\n\nR1: Thanks for your comments. Please refer to the response to Q4 in general response. \n\n**Q2: Clarification of information loss by representation distortion and graph reconstruction.**\n\nR2: Thanks for your comments. Please refer to the response to Q3 in general response. \n\n**Q3: More data augmentation techniques in graphs.**\n\nR3: We have added three graph data augmentation methods to generate different views for self-supervised graph reconstruction in a contrastive manner, i.e., DropEdge [5], DropNode [6] and Feature Mask [7]. Table 3 shows the experimental results, and our SMART shows the best performance on three datasets. We observe that different data augmentation methods on performance have similar performance.\n\nTable 3. Performance comparison with different graph data augmentation methods.\n\n| Method | OGB-arXiv | DBLP | Pharmabio |\n| --- | --- | --- | --- |\n| Linear | 10.5224 | 16.4991 | 32.3653 |\n| DoC | 9.5277\u00b11.4857 | 4.3910\u00b10.1325 | 8.1753\u00b10.0745 |\n| SMART (DropEdge) | 2.1897\u00b10.2211 | 3.4992\u00b10.1502 | 1.3405\u00b10.2674 |\n| SMART (DropNode) | 1.7163\u00b10.3783 | 3.8166\u00b10.5124 | 0.9050\u00b10.2606 |\n| SMART (Feature Mask) | 1.8733\u00b10.2729 | 3.0183\u00b10.4073 | 0.7352\u00b10.4001 |\n\n**Q4: Extension of theoretical analysis for more complex GNN model.**\n\nR4: Thanks for your comments. As we mentioned in the general response, we generalize Theorem 1 to a more general class of multi-layer graph neural network architectures (i.e., Theorem 3). Specifically, we consider the model where a feedforward neural network of an arbitrary depth is concatenated with the graph convolution network. The detailed theoretical analysis please refer to Appendix C.1 and the proof can be found in Appendix C.3.\n\n**Q5: Explanation of Figure 3.**\n\nR5: Thanks for your comment. Y-axis indicates the prediction loss of GNN model on test datasets during graph evolution. It is also possible to use other performance indicators, while the loss on the test sample is just a simple and intuitive indicator.\n\n**Q6: The selection of time series models to capture the temporal variations.**\n\nR6: To capture the temporal dynamics of GNN generalization variation, we propose an RNN-based method (i.e. LSTM) and achieve satisfied estimation performance. In our paper, we used the vanilla LSTM model to demonstrate the importance of temporal encoding via time series models. We do agree that under a more complex scenario, advanced time series model may achieve better performance. \n\nTo further investigate this problem, we additionally replace the LSTM with other two time series model Bi-LSTM and TCN. As depicted in Table 4, our SMART achieves the best performance among three different time series models on both OGB-arXiv and Pharmabio datasets. On DBLP dataset, SMART achieves the best performance using LSTM, while SMART equipped with Bi-LSTM and TCN show comparable performance with DoC. In order to maintain consistency in the experiment, we adopted the same hyperparameter settings. From the perspective of parameter size, the parameter quantity of Bi-LSTM is nearly twice that of LSTM, while the parameter quantity of TCN is very small. We observe that as the parameter size increases, the prediction error generally decreases first and then increases. From these preliminary results, it is indeed important to choose appropriate models and parameter settings based on the complexity of the data.\n\nTable 4. Performance comparison with different time series model.\n\n| Method | OGB-arXiv | DBLP | Pharmabio |\n| --- | --- | --- | --- |\n| Linear | 10.5224 | 16.4991 | 32.3653 |\n| DoC | 9.5277\u00b11.4857 | 4.3910\u00b10.1325 | 8.1753\u00b10.0745 |\n| SMART (LSTM) | 2.1897\u00b10.2211 | 3.4992\u00b10.1502 | 1.3405\u00b10.2674 |\n| SMART (Bi-LSTM) | 4.3601\u00b10.5816 | 4.2402\u00b10.6996 | 1.1219\u00b10.0397 |\n| SMART (TCN) | 1.5725\u00b10.2500 | 4.5970\u00b10.2607 | 1.8830\u00b10.4161 |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4683/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406032917,
                "cdate": 1700406032917,
                "tmdate": 1700406032917,
                "mdate": 1700406032917,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F2je8otxkq",
                "forum": "HFtrXBfNru",
                "replyto": "QMAnOweFaZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4683/Reviewer_JYus"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4683/Reviewer_JYus"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Thank you for the explanations. My concerns are mostly addressed and I would recommend this paper for acceptance."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4683/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603917616,
                "cdate": 1700603917616,
                "tmdate": 1700603917616,
                "mdate": 1700603917616,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GyYYHmFUaB",
            "forum": "HFtrXBfNru",
            "replyto": "HFtrXBfNru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4683/Reviewer_qkRe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4683/Reviewer_qkRe"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method (SMART) for performing generalization error estimation on temporal node classification tasks. The authors show that representations will become increasingly distorted as time increases, and propose to use both a structure graph reconstruction and feature construction loss to improve representation quality. The adapted features are feed into an RNN that is trained to predict the loss. The authors also propose two theorems: one which shows that distortion is strictly increasing over time, and the other one is specific to the Barbasi Albert graph and is used to argue the benefits of SMART."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper studies an interesting problem (generalization gap prediction for temporal node classification) that has not been well-studied in the graph representation learning literature before. \n\n- The authors conduct experiments across a variety of datasets, and conduct several ablations to demonstrate the benefits of each of the components of SMART. \n\n- The authors attempt to provide some theoretical bounds and use these, as well as an information theory framework, to ground the proposed method."
                },
                "weaknesses": {
                    "value": "- The authors only compared to a single linear regression baseline. While other methods for generalization prediction are not specific to temporal graph data, it should still be possible to consider some. Maybe some from [1] could be adapted? In this vein, I think there should be more citations to other generalization error predictors and a dedicated related works section in the main paper. \n\n- The theorems need to be discussed more. For example, in theorem 1, I don't think that beta has been defined. Furthermore, while the authors make the strong but acceptable assumption that the graph will only add edges, I was wondering if it was also assumed that the underlying graphs were homophilous. Do the authors expect the proposed method to work on heterophilus graphs as well? \n\n- The writing could be improved. For example, its not clear why the approach is referred to as \"constrastive.\" From my understanding the authors do augment the graph, but the overall loss is purely reconstruction based. \n\n[1] Predicting with Confidence, Guillory et al. ICCV 2021(https://arxiv.org/abs/2107.03315)"
                },
                "questions": {
                    "value": "Please see the weaknesses above.\n\nI'd appreciate some clarifications about the theorems and potential baselines as mentioned above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4683/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4683/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4683/Reviewer_qkRe"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4683/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698897485669,
            "cdate": 1698897485669,
            "tmdate": 1699636449655,
            "mdate": 1699636449655,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3sSMJBHSfa",
                "forum": "HFtrXBfNru",
                "replyto": "GyYYHmFUaB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4683/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4683/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qkRe"
                    },
                    "comment": {
                        "value": "Thanks for your time and constructive suggestions. We are pleased for your positive comments. In the response below, we provide answers to your questions in order to address the concerns and increase your confidence. \n\n**Q1: Model comparison with other baselines.**\n\nR1: Thank you very much for pointing out a relevant paper [1] in the field of computer vision. After in-depth reading, we adopt the DoC method proposed in this paper as a baseline for performance comparison. We present some additional experimental results in Q2 of the general response, where 4 evaluation metrics are shown in Table 1. We observe that in our experimental settings,  SMART shows consistent better performance than DoC. This may be due to the reason that DoC method is optimized for the setting where the testing distribution slightly differs from training data. However, in our setting, graph evolution is very fast and the evolving graphs are usually very different from their initial states. We also provide more detailed discussion in Appendix H.3 in our revised paper.\n\n[1] Devin Guillory, et al. \"Predicting with Confidence on Unseen Distribution.\" ICCV, 2021. \n\n**Q2: The definition of beta and the consideration of homogeneity assumption of graphs in theoretical analysis.**\n\nR2: Thanks for your valuable advice. In our theoretical analysis, $\\beta$ is the slope ratio for negative values instead of a flat slope in ReLU. We have added an explanation of $\\beta$ in our revised paper. Regarding the homogeneity of graphs, we do not make any assumptions in both theoretical analysis and empirical study. Meanwhile, we conduct a data analysis of the changes of homogeneity ratio over time on all the datasets as shown in Figure 10 of Appendix G. We observe that different datasets exhibit diverse variations in the homogeneity ratio of graphs and that SMART consistently outperforms the other two self-supervised baselines in all cases. \n\n**Q3: Explanation of contrastive way in self-supervised graph reconstruction.**\n\nR3: Constrastive self-supervised learning is a popular and effective learning strategy to enrich the supervised signals. It can be divided into two stages: first is the graph augmentation and then is the contrastive pretext tasks. Therefore, a general learning objective of contrastive self-supervised learning approaches is denoted as follows:\n\n$$\\theta^*, \\varphi^* = \\arg \\min_{\\theta, \\varphi} \\mathcal{L}(p_{\\varphi}(f_{\\theta}(\\tilde{A}^{(1)}, \\tilde{X}^{(1)}), f_{\\theta}(\\tilde{A}^{(2)}, \\tilde{X}^{(2)})))$$\n\nwhere $\\tilde{A}^{(1)}$ and $\\tilde{A}^{(2)}$ are two augmented graph adjacency matrices, $\\tilde{X}^{(1)}$ and $\\tilde{X}^{(1)}$ are two node feature matrices under different augmentations. In our setting, We perform data augmentation (e.g., randomly dropout node, dropout edge, mask feature) on the graph data with a probability of 0.5, keeping it unchanged with a probability of 0.5. Afterwards, we define a pretext task is to minimize the distance of two augmented graphs via link prediction and node feature reconstruction. We note that our proposed SMART is actually a special setting in the general contrastive learning framework."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4683/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700405892446,
                "cdate": 1700405892446,
                "tmdate": 1700405892446,
                "mdate": 1700405892446,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FJYcX0AdRa",
            "forum": "HFtrXBfNru",
            "replyto": "HFtrXBfNru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4683/Reviewer_btk3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4683/Reviewer_btk3"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of estimating the generalization performance of GNNs on evolving graphs. The authors propose SMART, which estimates the generalization performance of GNNs without the need for manual annotation post-deployment. SMART employs self-supervised contrastive graph reconstruction to update the feature extractor, minimizing information loss during the dynamic evolution process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1. The problem is of significant importance in dynamic graph learning, and the authors provide theoretical proofs to demonstrate that representation distortion is inevitable.\n\nS2. The authors derive a closed-form expression of the generalization error bound on synthetic data and verify the effectiveness of the proposed method.\n\nS3. The paper is well-written, and I have no complaint about the presentation of the paper."
                },
                "weaknesses": {
                    "value": "W1. The theoretical analysis is limited to single-layer GCN models. It would be nice to see an extension to more complex models.\n\nW2. The baseline only includes simple linear regression models. However, a better baseline should be a model that continuously acquires new node labels and gets retrained on the new data, to examine whether the generalization curve of such retraining is close to the prediction of the proposed method. This can more intuitively demonstrate the advantages of the method.\n\nW3. The paper shows that in the citation networks, the performance drop of pretrained models is mainly due to the increase of nodes and edges, while the category distribution does not change significantly. However, in real-world applications, changes in labels are very common, which will lead to concept drift and cause the model's performance to decrease dramatically. It is worth discussing whether the proposed method is applicable to such scenarios with frequent label changes and concept drift."
                },
                "questions": {
                    "value": "See W1-W3 for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4683/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4683/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4683/Reviewer_btk3"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4683/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698990251091,
            "cdate": 1698990251091,
            "tmdate": 1699636449585,
            "mdate": 1699636449585,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JEp7lqxdIG",
                "forum": "HFtrXBfNru",
                "replyto": "FJYcX0AdRa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4683/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4683/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer btk3"
                    },
                    "comment": {
                        "value": "Thanks for your time and valuable comments. We are encouraged that you appreciated our technical contributions including problem significance, theoretical proof and well-written presentation. In the response below, we provide answers to your questions in order to address the concerns. \n\n**Q1: Extension of theoretical analysis to a more complex scenario.**\n\nR1: Thanks for your comments. As we mentioned in the general response, we generalize Theorem 1 to a more general class of multi-layer graph neural  network architectures (i.e., Theorem 3). Specifically, we consider the model where a feedforward neural network of an arbitrary depth is concatenated with the graph convolution network. For the detailed theoretical analysis, please refer to Appendix C.1 and the proof can be found in Appendix C.3. We also generalize the assumption that allows missing nodes and non-zero feature bias in  evolution.\n\n**Q2: More model comparisons, e.g. a supervised manner.**\n\nR2: Thanks for your suggestion. We have added a supervised baseline and compared it with our SMART. Please refer to Q2 in general response. We also provide more detailed information in Appendix H.3 in our revised paper. Overall, SMART achieves comparable performance on dataset OGB-arXiv, while achieving slightly worse performance on dataset Pharmabio, when compared to the supervised baseline.\n\n**Q3: Generalizability of SMART for label changes and concept drift.**\n\nR3: We agree with the reviewer that the graph topology $A$, feature vectors $X$ on all nodes and even labels $Y$ can change during the graph evolution. In other ways, the joint distribution $P(A,X,Y)$ can change during the graph process. Using the definition of the conditional probability, we must have   $P(A,X,Y)=P(Y|A,X)P(A,X)$. This implies that the change of joint distribution $P(A,X,Y)$ can be due to the change of $P(Y|A,X)$,$P(A,X)$or  both of them. In this paper, we aim at the problem where only $P(A,X)$changes and theoretically showed the inevitable representation distortion even without of change of $P(Y|A,X)$. We proposed the algorithm SMART mainly to address the case where only $P(A,X)$ and thus do not consider the setting where the label changes and concepts drift. But we do believe techniques focusing on the changing labels can be fused with our algorithm for handling the case where both distribution $P(Y|A,X)$ and $P(A,X)$ evolve."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4683/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700405760954,
                "cdate": 1700405760954,
                "tmdate": 1700407164816,
                "mdate": 1700407164816,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OeJ4aKUBVH",
            "forum": "HFtrXBfNru",
            "replyto": "HFtrXBfNru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4683/Reviewer_jJDp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4683/Reviewer_jJDp"
            ],
            "content": {
                "summary": {
                    "value": "This study investigates the representation distortion of graphs during evolution. The authors proposed SMART to estimate the temporal generalization performance of GNN. The authors provided theoretical proofs as well as numerical experiments, showing the distortion of representation is inevitable and providing a way of estimating generalization loss."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is a novel study trying to analyze the representation distortion from an information theory perspective, which I think is an important and interesting question.  The authors theoretically establish a lower bound and conducted various numerical experiments using both synthetic datasets and real world datasets. Introduction is straightforward. The results seem promising. The ablation study is convincing and sufficient."
                },
                "weaknesses": {
                    "value": "The authors mainly compared their results with linear regression. The authors may add more model comparisons to make the results more convincing."
                },
                "questions": {
                    "value": "Figure 1 is a bit confusing: what\u2019s the x axis? And what are the nodes at each year?\n\nFor the information loss, the authors considered the loss from RNN as well as from representation distortion. Can the authors comment on the loss of information from graphs to their low dimensional representations and their effects on the model.\n\nMore evaluation metrics besides MAPE should be considered for the synthetic datasets.\n\nInterestingly GCN seems achieved the best performance in the three compared GNNs, can the authors comment on the contribution of different GNN structures to the performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4683/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699073214220,
            "cdate": 1699073214220,
            "tmdate": 1699636449455,
            "mdate": 1699636449455,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lsCEpfHhv1",
                "forum": "HFtrXBfNru",
                "replyto": "OeJ4aKUBVH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4683/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4683/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jJDp"
                    },
                    "comment": {
                        "value": "Thank you for your positive and constructive comments. We are pleased that you acknowledged the novelty and significance of our focused problem, promising results and straightforward presentation. In the following response, we answer your comments/questions point by point.\n\n**Q1: More model comparisons to make the results more convincing.**\n\nR1: We have added two additional baselines, DoC and Supervised, and conducted performance comparison on 3 datasets. The experimental results are shown in the general response (Table 1), and we also provide more detailed experimental results (Table 9-10, Figure 12-16)  in Appendix H.3 in our revised paper.\n\n**Q2: Clarify the meaning of Figure 1.**\n\nR2: Thanks for your valuable suggestion. Figure 1 depicts the changes of the graph scale and GCN classification performance nearly 30 years on Pharambio dataset. Therefore, the x axis is the range of year. Each node in the graph means a paper in an academic co-authorship graph. \n\n**Q3: The impact of information loss in graph representation on the generalization estimator.**\n\nThanks for your comments. Please refer to the response to Q3 in general response. \n\n**Q4: More evaluation metrics for the synthetic datasets.**\n\nR4: We have added MAPE, RMSE and MAE on the synthetic datasets. The results are presented in following Table 2. The detailed analysis please refer to Appendix F. Since the DoC method only applies to classification problems using the average confidence, we thus do not compare it with our SMART on the synthetic setting.\n\nTable 2. Performance comparison on different Barab\u00e1si\u2013Albert graph setting.\n| Barab\u00e1si\u2013Albert (BA) Random Graph |  | Linear |  |  | SMART (Ours) |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n|  |  | MAPE | RMSE | MAE | MAPE | RMSE | MAE |\n| BA ($\\mathcal{N}_0$ = 1000) | $m=2$ | 79.2431 | 0.0792 | 0.0705 | 7.1817\u00b11.2350 | 0.0057\u00b10.0008 | 0.0042\u00b10.0006 |\n|  | $m=5$ | 74.1083 | 0.0407 | 0.0398 | 4.2602\u00b10.5316 | 0.0039\u00b10.0004 | 0.0035\u00b10.0004 |\n|  | $m=10$ | 82.1677 | 0.1045 | 0.0925 | 9.1173\u00b10.1331 | 0.0077\u00b10.0010 | 0.0071\u00b10.0009 |\n| Dual BA ($\\mathcal{N}_0$ = 1000, $m_1$=1) | $m=2$ | 61.8048 | 0.0676 | 0.0615 | 7.9038\u00b11.8008 | 0.0088\u00b10.0022 | 0.0069\u00b10.0017 |\n|  | $m=5$ | 67.6442 | 0.0827 | 0.0771 | 3.8288\u00b10.1706 | 0.0049\u00b10.0013 | 0.0040\u00b10.0010 |\n|  | $m=10$ | 38.4884 | 0.0298 | 0.0297 | 1.9947\u00b10.1682 | 0.0026\u00b10.0002 | 0.0023\u00b10.0003 |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4683/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700405658228,
                "cdate": 1700405658228,
                "tmdate": 1700405658228,
                "mdate": 1700405658228,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]