[
    {
        "title": "Endowing Protein Language Models with Structural Knowledge"
    },
    {
        "review": {
            "id": "m8eCw4BPAq",
            "forum": "eP6ZSy5uRj",
            "replyto": "eP6ZSy5uRj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7966/Reviewer_XuQU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7966/Reviewer_XuQU"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel approach to incorporate structural information into the protein representation learning process. By converting protein structural information into graph structures, the model employs a Graph Neural Network (GNN) to process the structural graph information of proteins. It then integrates the structural representations learned by the GNN with the representations of the ESM-2 model, ultimately resulting in a protein representation that has been enriched with structural information. The paper validates the model's performance on various protein function prediction tasks and compares the impact of different factors such as model size, pre-training strategies, and the amount of structural information on the model's effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ This paper introduces a protein representation learning method that integrates protein structural information with sequence-based models, which has a positive impact on the development of protein representation learning models.\n+ The model proposes the direct integration of protein structural information into the pre-trained ESM-2 model, reducing the training cost and resource requirements.\n+ The model's performance is validated across various model sizes, revealing insights into the impact of scaling on such models."
                },
                "weaknesses": {
                    "value": "+ The main reported results in the paper (Table 1), where the model's performance is compared to ESM-Gearnet MVC, show only a slight advantage, and this advantage is likely due to the use of a stronger backbone model (ESM-2 vs. ESM-1b). Therefore, the results may not be very persuasive in demonstrating a good performance improvement for the PST model.\n+ The approach of incorporating protein structural information into the ESM model using a GNN has been previously proposed in other papers[1]. While this paper just applies this approach to the field of protein representation learning, it lacks novelty, and it does not discuss the differences between these methods.\n+ Table 1 does not report the performance of ESM-2 under end-to-end training conditions, and it does not specify important hyperparameters of the compared models, such as model size, which is a crucial factor affecting relative model performance.\n\n[1] Zheng, Zaixiang, et al. \"Structure-informed Language Models Are Protein Designers.\" (2023)."
                },
                "questions": {
                    "value": "+ Has the paper explored the impact of different backbone models on performance? For example, ESM-1b vs. ESM-2.\n+ Since there are pre-trained GNN models designed for protein structural graph, such as GearNet[1], did the paper investigate the performance impact of using such pre-trained GNN models?\n\n[1] Zhang, Zuobai, et al. \"Protein Representation Learning by Geometric Structure Pretraining.\" The Eleventh International Conference on Learning Representations. 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7966/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7966/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7966/Reviewer_XuQU"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7966/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698049283908,
            "cdate": 1698049283908,
            "tmdate": 1700638915978,
            "mdate": 1700638915978,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gQ6NYsajMk",
                "forum": "eP6ZSy5uRj",
                "replyto": "m8eCw4BPAq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7966/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to reviewer XuQU"
                    },
                    "comment": {
                        "value": "> The main reported results in the paper (Table 1), where the model's performance is compared to ESM-Gearnet MVC, show only a slight advantage, and this advantage is likely due to the use of a stronger backbone model (ESM-2 vs. ESM-1b). Therefore, the results may not be very persuasive in demonstrating a good performance improvement for the PST model.\n\nIn our new results, detailed under point 3 in the general comments, we have observed that PST models with fixed representations outperform all fine-tuned models, including ESM-Gearnet MVC, across all function prediction tasks. This finding combined with the enhanced practicality in terms of reduced computational resource requirements, clearly underscores the advantage of PST over fine-tuned models like ESM-Gearnet-MVC. \n\nMoreover, based on the results in a very recent preprint (https://arxiv.org/pdf/2303.06275.pdf), that we cite in our revised manuscript, we observe that the performances of ESM-Gearnet-MVC based on ESM-1b are very similar or even better than the one obtained using ESM-2, on most of the considered tasks. \n\n> The approach of incorporating protein structural information into the ESM model using a GNN has been previously proposed in other papers[1]. While this paper just applies this approach to the field of protein representation learning, it lacks novelty, and it does not discuss the differences between these methods.\n\nFirst, it is worth highlighting that our proposed architecture is significantly different from LM-Design, as elaborated in point 2.i of the general comment. Second, the central objective of LM-Design centers around protein design such as inverse folding, whereas our work primarily aims to enhance protein representations specifically for various tasks related to protein function prediction.\n\n> Table 1 does not report the performance of ESM-2 under end-to-end training conditions, and it does not specify important hyperparameters of the compared models, such as model size, which is a crucial factor affecting relative model performance.\n\nWe have added the performance of ESM-2 under end-to-end training, and we notice that its performance is inferior compared to the PST (finetuned). \nMoreover, we have added the number of parameters of the ESM-2 model and of the corresponding PST model, as well as the ones of the baselines we run ourselves, in the appendix B.4. The number of parameters of the other models are not reported in the Gearnet paper.\n\n> Has the paper explored the impact of different backbone models on performance? For example, ESM-1b vs. ESM-2. \n\nWe appreciate the reviewer's suggestion to explore the impact of various backbone models. Nonetheless, our current focus is on the state-of-the-art PLM, namely ESM-2, as it presents more relevance and potential for advancement compared to its predecessor, ESM-1b.\nMoreover, our investigations into the impact of model size have indicated that the quality of the base sequence model is directly proportional to the performance of the corresponding PST in general. Hence, it is a reasonable inference that a PST model built upon ESM-2 would likely surpass one based on ESM-1b.\n\n> Since there are pre-trained GNN models designed for protein structural graph, such as GearNet[1], did the paper investigate the performance impact of using such pre-trained GNN models?\n\nWe would like to point out that in contrast to previous approaches that use a deep GNN to encode the structural information and add the encoded features atop a PLM, PST adopts a different approach. Specifically, we use a different two-layer GIN model (as described in section 4.1.2) at __each self-attention layer__ within the base ESM-2 model. Therefore, the weights learned in each GIN model should differ from one layer to another, making the application of a deep, pre-trained GNN model inapplicable in this context. We have clarified this by adding a brief introduction of our PST method at the beginning of Section 3.2 in the revised manuscript. See also our general comments above."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518683691,
                "cdate": 1700518683691,
                "tmdate": 1700518683691,
                "mdate": 1700518683691,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6qHHH6sZF9",
                "forum": "eP6ZSy5uRj",
                "replyto": "gQ6NYsajMk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7966/Reviewer_XuQU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7966/Reviewer_XuQU"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your patient response. Your answers have helped clarify some issues, and I have reassessed the paper's score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638905134,
                "cdate": 1700638905134,
                "tmdate": 1700638905134,
                "mdate": 1700638905134,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ViNjCu5S4N",
            "forum": "eP6ZSy5uRj",
            "replyto": "eP6ZSy5uRj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7966/Reviewer_hmnV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7966/Reviewer_hmnV"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel framework, the Protein Structure Transformer (PST), aimed at enhancing the efficiency of protein language models (PLMs) by seamlessly integrating structural information derived from protein structures. Building on the foundation of the ESM-2 model, the PST refines the self-attention mechanism using structure extractor modules and leverages recent advances in graph transformers. \n\nThe model can be further pretrained on databases like AlphaFoldDB, enhancing its performance. An observation in the experiment is that improvements can be achieved by finetuning just the structure extractors, addressing concerns about parameter efficiency.\n\nThe PST demonstrates superior performance over ESM-2 in various prediction tasks related to protein function and structure."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors combine advances in graph transformers with existing protein language models to enhance the performance of protein function prediction tasks, showcasing a fusion of existing techniques to create something novel.\n\n2. The empirical findings presented are comprehensive and demonstrate the PST\u2019s performances on the EC, GO and some tasks from the ProteinShake benchmark, and specially parameter efficiency in training compared to the existing methods."
                },
                "weaknesses": {
                    "value": "1. While the integration of a structure extractor with a PLM is presented as a unique proposition for function prediction tasks, it exits parallel studies in the protein representation field. For instance, LM-Design [1] proposes a similar structure adapter into PLMs that endows PLMs with structural awareness, and the structure adapter could access an arbitrary additional structure encoder (GNNs, ProteinMPNN etc.). As a result, this might raise concerns about novelty in comparison to existing methods.\n\n2. A limitation in the study is the lack of an empirical exploration regarding the selection of different structure encoder. While the paper presents results based on a specific structure extractor module, it would have been insightful to see comparisons or benchmarks against various other structure encoding methodologies.\n\n[1] Structure-informed Language Models Are Protein Designers, ICML 2023."
                },
                "questions": {
                    "value": "1. The paper primarily focuses on the integration of a structure extractor with the ESM-2 model. However, with the availability of larger models like ESM2-3b, have there been empirical studies to assess the impact and advantages of the structure extractor? Specifically, as the scale of the PLM increases, does the benefit of adding a structure extractor diminish or remain consistent? It would be crucial to understand the interplay between model size and the structure extractor's efficacy."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7966/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7966/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7966/Reviewer_hmnV"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7966/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807910495,
            "cdate": 1698807910495,
            "tmdate": 1699636980081,
            "mdate": 1699636980081,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pFLlPpljoB",
                "forum": "eP6ZSy5uRj",
                "replyto": "ViNjCu5S4N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7966/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to reviewer hmnV"
                    },
                    "comment": {
                        "value": "> While the integration of a structure extractor with a PLM is presented as a unique proposition for function prediction tasks, it exits parallel studies in the protein representation field. For instance, LM-Design [1] proposes a similar structure adapter into PLMs that endows PLMs with structural awareness, and the structure adapter could access an arbitrary additional structure encoder (GNNs, ProteinMPNN etc.). As a result, this might raise concerns about novelty in comparison to existing methods.\n\nFirst, it is worth highlighting that our proposed architecture is significantly different from LM-Design, as elaborated in point 2.i of the general comments. Second, the central objective of LM-Design centers around protein design such as inverse folding, whereas our work primarily aims to enhance protein representations specifically for various tasks related to protein function prediction.\n\n> A limitation in the study is the lack of an empirical exploration regarding the selection of different structure encoder. While the paper presents results based on a specific structure extractor module, it would have been insightful to see comparisons or benchmarks against various other structure encoding methodologies.\n\nThank you for the suggestion. Our research draws significant inspiration from Chen et al. 2022 [1], leading us to choose GIN as our structural extractor due to its balance of performance and efficiency. Additionally, we believe that the quantity of structural information used in the structural extractor is more influential than the specific type of structural extractor chosen. We thus explored the impact of varying amounts of structural information in our study, as demonstrated in Section 4.4 and Figure 2.\n\n> The paper primarily focuses on the integration of a structure extractor with the ESM-2 model. However, with the availability of larger models like ESM2-3b, have there been empirical studies to assess the impact and advantages of the structure extractor? Specifically, as the scale of the PLM increases, does the benefit of adding a structure extractor diminish or remain consistent? It would be crucial to understand the interplay between model size and the structure extractor's efficacy.\n\nOur work includes scalability studies for various model sizes that can be accommodated by most GPUs, as depicted in Figures 2, 3, and 4. These studies indicate that the benefits of utilizing structural extractors are maintained across different scales. Although the advantages might diminish slightly for very large models, the utilization of structural information is still expected to offer benefits.\nRegarding extremely large models like ESM2-3b, we were only able to perform inference with it, as fitting a PST model of this size exceeded our capabilities. Interestingly, in the Variant Effect Prediction (VEP) task, our results show that ESM2-3b is outperformed by smaller versions of ESM2 models. For instance, ESM2-650M and the corresponding PST achieved a higher mean Spearman's correlation (0.489 and 0.501 respectively) compared to ESM2-3b's 0.456, highlighting the effectiveness of smaller models in certain applications.\n\n_Reference_\n\n[1] Dexiong Chen, Leslie O\u2019Bray, and Karsten Borgwardt. Structure-aware transformer for graph representation learning. ICML 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518614934,
                "cdate": 1700518614934,
                "tmdate": 1700518614934,
                "mdate": 1700518614934,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YrpLkS7qSy",
            "forum": "eP6ZSy5uRj",
            "replyto": "eP6ZSy5uRj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7966/Reviewer_GUNq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7966/Reviewer_GUNq"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends a pre-trained protein language model, ESM-2, with information from predicted structures. Architecturally, PST extends ESM-2 by adding a GNN module to the Transformer. PST also includes additional pre-training steps based on predicted structures from AlphaFold. When adapted to a range of downstream tasks, PST outperforms ESM-2."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper is clearly written.\n* The experiments attempt to offer careful comparisons with a strong model from prior work, ESM-2. PST outperforms ESM-2 on a range of protein tasks. An improvement can be observed even when only tuning the structure-related parameters.\n* The paper offers strong evidence that structural information from predicted structures can offer improved performance. The proposed methods for incorporating structural information seem reasonable."
                },
                "weaknesses": {
                    "value": "* A controlled comparison between ESM-2 and PST might have considered continuing MLM pre-training for ESM-2 for an equivalent number of steps as PST pre-training. However, the experiments showing that tuning only the structure-related parameters (section 4.4) is sufficient to improve performance partially address this.\n* While the improvement from PST over ESM-2 is largest for smaller model sizes, the efficiency improvement is partially offset by the need to predict a structure."
                },
                "questions": {
                    "value": "* nit: The BERT citation is not formatted correctly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7966/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807938538,
            "cdate": 1698807938538,
            "tmdate": 1699636979916,
            "mdate": 1699636979916,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7LBYjHxLe9",
                "forum": "eP6ZSy5uRj",
                "replyto": "YrpLkS7qSy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7966/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to reviewer GUNq"
                    },
                    "comment": {
                        "value": "> A controlled comparison between ESM-2 and PST might have considered continuing MLM pre-training for ESM-2 for an equivalent number of steps as PST pre-training. However, the experiments showing that tuning only the structure-related parameters (section 4.4) is sufficient to improve performance partially address this.\n\nThe ESM-2 models have been pre-trained on the Uniref-50, a dataset much larger than the structure dataset (i.e. Swissprot) used for pre-training the PST models. Consequently, further MLM pre-training of ESM-2 on Swissprot could potentially lead to overfitting, negatively affecting model's performance in downstream tasks. Moreover, we believe that \"updating only the structure-related parameters is sufficient to improve performance\" should be interpreted as a strength of our method, rather than a weakness, as elaborated in the point 2.iv in the general comments.\n\n> While the improvement from PST over ESM-2 is largest for smaller model sizes, the efficiency improvement is partially offset by the need to predict a structure.\n\nWhile we acknowledge this is a good point, it is worth noting that structures can generally be sourced from precomputed databases (such as ESMFold and AlphaFoldDB), and are reusable for various tasks. Therefore, we argue that the additional computational costs due to structure predictions are quickly compensated by the reduced costs due to the data- and parameter efficiency of our models.\nMoreover, the effectiveness of our model based on fixed embeddings, without any additional finetuning, significantly reduces computational demands compared to fine-tuned models. This further strengthens our argument regarding the cost-effectiveness of our approach.\n\n> nit: The BERT citation is not formatted correctly.\n\nThank you for noticing this point, we have addressed the formatting issue in the revised manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518557955,
                "cdate": 1700518557955,
                "tmdate": 1700518557955,
                "mdate": 1700518557955,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bKmjNlQ2yA",
                "forum": "eP6ZSy5uRj",
                "replyto": "7LBYjHxLe9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7966/Reviewer_GUNq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7966/Reviewer_GUNq"
                ],
                "content": {
                    "comment": {
                        "value": "Thank for you for your reply. I confirm my original score of leaning towards acceptance.\n\n> we believe that \"updating only the structure-related parameters is sufficient to improve performance\" should be interpreted as a strength of our method\n\nYes, this was the intended interpretation of my comment."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582687324,
                "cdate": 1700582687324,
                "tmdate": 1700582687324,
                "mdate": 1700582687324,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "z1mtpu8HoH",
            "forum": "eP6ZSy5uRj",
            "replyto": "eP6ZSy5uRj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7966/Reviewer_daTw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7966/Reviewer_daTw"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel framework called Protein Structure Transformer (PST) that enhances transformer protein language models specifically on protein structures, resulting in superior performance on several benchmark datasets. The PST integrates structural information with structure extractor modules, which are trained to extract structural features from protein sequences. The authors evaluate the performance of the PST on several benchmark datasets and compare it to the ESM-2 model. The results show that the PST outperforms the ESM-2 model on several tasks, including protein secondary structure prediction, contact prediction, and remote homology detection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper introduces a novel framework that enhances transformer protein language models specifically on protein structures, which is an important area of research in bioinformatics.\n- The PST outperforms the ESM-2 model on several benchmark datasets, demonstrating the effectiveness of the proposed framework.\n- The paper provides a detailed description of the PST framework and the structure extractor modules, which could be useful for researchers interested in developing similar models."
                },
                "weaknesses": {
                    "value": "- Overall speaking, the novelty of this paper is enough. There are plenty of works that integrated the graph representation in protein representations, even alphafold. The authors mentioned about the minimal training cost and parameter efficient, while I acknowledge the good point. This is hard to be a strong strength. \n- The framework actually is a GIN + ESM model, with the ESM freezed and GIN and head tuned. In Figure 1, however, the GIN is not clearly  presented. If GNN in the figure is the extractor, it means the GNN would be processed multiple L layers, this is somehow not necessary or why for this design?\n- Small question, the GIN is randomly initialized, what's the difference between zero-initialized as you mentioned in the paper? Besides, is the node embedding in GIN is initialized by ESM token embedding?\n- The paper does not provide a detailed analysis of the limitations of the proposed framework or potential areas for improvement."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7966/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698900690370,
            "cdate": 1698900690370,
            "tmdate": 1699636979813,
            "mdate": 1699636979813,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HplMtAhYD5",
                "forum": "eP6ZSy5uRj",
                "replyto": "z1mtpu8HoH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7966/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to reviewer daTw"
                    },
                    "comment": {
                        "value": "> Overall speaking, the novelty of this paper is enough. There are plenty of works that integrated the graph representation in protein representations, even alphafold. The authors mentioned about the minimal training cost and parameter efficient, while I acknowledge the good point. This is hard to be a strong strength.\n\nPlease see the point \"Novelty and impact of our work\" in the general comments.\n\n> The framework actually is a GIN + ESM model, with the ESM freezed and GIN and head tuned. In Figure 1, however, the GIN is not clearly presented. If GNN in the figure is the extractor, it means the GNN would be processed multiple L layers, this is somehow not necessary or why for this design?\n\nNote that in the transformer architecture, the node embeddings used for each GNN corresponds to the residue embeddings of the respective layer, and these embeddings can vary from one layer to another. Consequently, the query, key, and value matrices have to be re-computed at each layer. Please see also our first point of the general comments for a clarification of the PST architecture.\n\n> Small question, the GIN is randomly initialized, what's the difference between zero-initialized as you mentioned in the paper? Besides, is the node embedding in GIN is initialized by ESM token embedding?\n\nSee __Clarification of the PST architecture__ in our general response, where we clarified our general approach, and should address general questions about the methodology. Now to answer the reviewer's questions: i) the weights in GIN, i.e., $\\theta$ is randomly initialized while $W_s$ is initialized to zero such that the _PST model without training_ makes the same prediction as the base ESM-2 model. ii) The node embedding in the GIN before each self-attention layer is given by the residue embedding at the respective layer. We have added a clarification of PST in Section 3.2 of the revised version.\n\n> The paper does not provide a detailed analysis of the limitations of the proposed framework or potential areas for improvement.\n\nWe kindly direct the reviewer's attention to the limitations discussed in section 4.4 and Appendix A."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518517263,
                "cdate": 1700518517263,
                "tmdate": 1700518517263,
                "mdate": 1700518517263,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yeamZxcPVK",
            "forum": "eP6ZSy5uRj",
            "replyto": "eP6ZSy5uRj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7966/Reviewer_wJCU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7966/Reviewer_wJCU"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the Protein Structure Transformer (PST) built on the protein language models (PLM), such as ESM-2, to incorporate structural information into PLM for the purpose of obtaining structural-aware protein representations. It takes the 2D-ordered protein graph as input and devises the structure extractor modules within self-attention architecture. The experiments show its superiority compared to the vanilla PLMs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper presents the simple yet effective structure extractor modules to inject structural knowledge into the vanilla sequence-based PLMs, thus enhancing the representation ability of PLMs.\n2. The improved performance on several downstream tasks is promising. Especially, the parameter-efficiency Sructrual Only models can match the full model on a variety of tasks, which indicates the PST can serve as the flexible plug-in modules to any PLMs to enhance their representation ability."
                },
                "weaknesses": {
                    "value": "1. The paper writing style needs to be further improved. The Method spends a lot of pages introducing the principle of ESM-2, which is not the contribution of the paper. On the contrary, the introduction of the PST framework is vague. How the node embedding output by GNN is further incorporated into the residue embedding of the original ESM? Did they just add, concentrate? or viewed as the individual token embedding which is conducted with self-attention to the residue embedding of the original ESM? I highly suggest the authors to add more details.\n2. The introduction of structural knowledge of 3D protein structures is limited. This paper just compressed the 3D structures into 2D graphs without considering the 3D geometric features, such as the SE(3)-Equivariant features, which have already been confirmed to be critical in modeling the 3d protein structures or the protein-protein docking patterns by many works, such as Alphafold2, EquiDock, etc. The only considered feature is the distances between nodes severed as the edge attributes while performing less-satisfied on the downstream tasks.\n3. The paper only adopts the subset of AlphaFoldDB. I wonder if the author ever tried other structural databases such as metagenomic protein databases predicted by ESMFold? How do the data quality and quantity affect the performance of PST?\n4. The downstream tasks adopted in this work are limited. As the protein structure information decides the physical and chemical properties of proteins, the downstream tasks should not only be performed on protein structure prediction tasks, but also on other tasks, like protein solubility prediction,  secondary structural prediction, etc. Like xTrimoPGLM (chen et al. ) and Ankh (Elnaggar, et al) do.\n5. Besides the ablation studies claimed in the pre-training strategy, the author should add Seq-only to get rid of the reason solely brought by the sequence-based training.\n6. \"While PST typically surpasses its base ESM-2 counterpart at similar model sizes, this performance gain tapers off with increasing model size. \"Does this observation indicate that if we adopt a huge amount of protein sequence data and also the large scale of PLMs,   the structural knowledge can be well-captured on the sequence-based pertaining, like xTrimoPGLM with 100B dominates on most of the downstream tasks?\n7. From the Table.1, although the PST with fixed representation outperforms the GearNet MVC, the end-to-end PST(fintuned) lies behind the GearNet MVC on the fold classification tasks. How to explain this?\n\n\n\nGanea, et al. \"Independent se (3)-equivariant models for end-to-end rigid protein docking.\" arXiv preprint arXiv:2111.07786 (2021).\n\nChen, et al. \"xTrimoPGLM: unified 100B-scale pre-trained transformer for deciphering the language of protein.\" bioRxiv (2023): 2023-07.\n\nElnaggar, et al. \"Ankh\u2625: Optimized protein language model unlocks general-purpose modeling.\" bioRxiv (2023): 2023-01."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7966/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7966/Reviewer_wJCU",
                        "ICLR.cc/2024/Conference/Submission7966/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7966/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699606421397,
            "cdate": 1699606421397,
            "tmdate": 1700642401275,
            "mdate": 1700642401275,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pDSuxlDxVm",
                "forum": "eP6ZSy5uRj",
                "replyto": "yeamZxcPVK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7966/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to reviewer wJCU"
                    },
                    "comment": {
                        "value": ">1. The paper writing style needs to be further improved. \n\nWe thank the reviewer for the suggestion and have updated the manuscript accordingly (please see __Clarification of the PST architecture__ in our general comments). As a result, a thorough description of the ESM-2 model is crucial as it lays the groundwork for the mechanism discussed above.\n\n>2. The introduction of structural knowledge of 3D protein structures is limited.\n\nWe acknowledge the reviewer's point about the importance of using additional geometric information in several structure prediction tasks. However, its impact on protein function prediction, the primary focus of our work, remains less evident. To elucidate how the incorporation of different amounts of geometric information impacts PST models, we direct the reviewer's attention to our ablation study presented in Section 4.2 under \"Amount of structural information required for refining ESM-2\". Our results suggest that while incorporating more structural features, such as the distance information used in PiFold [1], improves training accuracy, it negatively affects the model's performance in downstream tasks, as shown in Figure 2.\n\n> 3. The paper only adopts the subset of AlphaFoldDB. \n\nWe did not train our PST model on ESMFold, which is out of the computational capacity in our group. However, our results indicate that fine-tuning of ESM-2 with structural data, even on a relatively modest-sized structure dataset such as Swissprot (~550k structures) can already enhance the quality of protein representations. We are eager to explore the full potential of PST on larger datasets like ESMFold in future work.\n\n> 4. The downstream tasks adopted in this work are limited. \n\n- First, our method was evaluated on a wide range of tasks, such as enzyme commission number prediction and GO term prediction given a protein as used to evaluate Gearnet and DeepFRI. To extend our analysis, we use the novel ProteinShake tasks to evaluate the quality of the embeddings on other biologically meaningful tasks such as binding site prediction, structural class prediction and protein family prediction. We believe that these tasks collectively provide a thorough overview of a wide range of protein-related applications.\n- Second, our focus was primarily on protein function prediction task, which is a common focus of many representation learning approaches such as Gearnet and DeepFRI, allowing us to benchmark our representations against existing approaches.\n- Third, we found that the suggested benchmarks did not contain UniProt IDs which prevented us from retrieving the exact corresponding structures from AlphaFoldDB needed for direct comparison on the Ankh datasets. Instead we were able to apply the Ankh model to our datasets for the EC and GO tasks (see updated Table 1). Regarding xTrimPGLM, we could not easily find the model weights online.\n\n> 5. Besides the ablation studies claimed in the pre-training strategy, the author should add Seq-only to get rid of the reason solely brought by the sequence-based training.\n\nFirst, we want to clarify that in the \"pretraining strategies\" section, the \"Struct Only\" strategy specifically consists of updating __only the structure extractors__ in the PST model. In this strategy, the weights of the base ESM-2 model remain unchanged during training, allowing the trained PST model to also generate the sequence representations (which is the same as the base ESM-2's representations) at inference, by bypassing the structure extractors in the trained PST. The \"Struct Only + Seq\" strategy simply takes the average of both structure and sequence representations. This was fully detailed in our submitted version. We have further included Figure 6 to illustrate the difference between these strategies in the revised manuscript.\n\nAs for the \"sequence-only training\" referenced by the reviewer, we believe this corresponds to the ESM-2 model. We encourage the reviewer to refer to Section 4.3 of our paper, where we present an extensive comparison between PST and ESM-2."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518362144,
                "cdate": 1700518362144,
                "tmdate": 1700518362144,
                "mdate": 1700518362144,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lextn6GFPm",
                "forum": "eP6ZSy5uRj",
                "replyto": "VchBeDc7ld",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7966/Reviewer_wJCU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7966/Reviewer_wJCU"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' response. It addresses several of my concerns. Thus I improved the paper's score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642625435,
                "cdate": 1700642625435,
                "tmdate": 1700642625435,
                "mdate": 1700642625435,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]