[
    {
        "title": "Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages"
    },
    {
        "review": {
            "id": "LYYpPYojvw",
            "forum": "Kuh5qgCGCp",
            "replyto": "Kuh5qgCGCp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3695/Reviewer_BZ1h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3695/Reviewer_BZ1h"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces MpM, a framework to train multilingual multimodal models for image-to-text and text-to-image generation (mostly English+Chinese). For image-to-text, the model (VisCPM-Chat) is typically pretrained on English image captions by freezing the LM, and then fine-tuned on instruction data translated into the target language(s). The model shows competitive performance when evaluated on machine-translated versions of LLaVA Test Set and UniMM-Bench as evaluated by an off-the-shelf model (GPT-4). For text-to-image, both the text model and the image decoder are frozen, and cross-attention layers are trained using English data. The resulting model (VisCPM-Paint) achieves a lower FID score than previous models on 30K samples from COCO Val obtained through machine translation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed framework, MpM, corroborates and extends the line of work on multilingual transfer through frozen language models, showing it to be more effective than previous models on the chosen tasks according to the evaluation measures adopted by the authors.\n2. The plot in Figure 2 is interesting, showing how most of the Wukong text data might be poorly aligned with the corresponding images.\n3. Human evaluation on Chinese Drawbench shows how the proposed model is often preferred to the baselines.\n4. It is interesting to see how much English data helps during fine-tuning in the ablation study, perhaps because it bridges the gap between the style and language axes."
                },
                "weaknesses": {
                    "value": "1. The paper claims to \u201cgeneralize to other languages in a zero-shot manner.\u201d This is not true, and very misleading. In fact, the chat model had to be fine-tuned on target language data, otherwise it would generate text in English. Calling the behavior \u201cquasi-zero-shot\u201d is also (i) different from the literature, and especially (ii) misleading: the model cannot generate text in the target language without supervision. Such claims, while valid for the text-to-image model, should be removed.\n2. The paper claims to propose \u201can effective training paradigm for training large multimodal models in low-resource languages.\u201d However, none of the languages tested were low-resource. There is a significant difference in the representations of the LM model between high- and low-resource languages, as well as in the quality of translations that an MT system can provide for those languages. None of these things are true for Chinese. Any claims about performance and applicability to low-resource languages need to be removed as they are never verified.\n3. The paper also claims to be multilingual. Yet, most of the experiments are done in a bilingual setup (English+Chinese) or in a few, high-resource European languages. These languages share the same script, similar topology. Claims about multilinguality should be supported by a better selection of languages that increase diversity in scripts, topology, and resource availability.\n4. My other major concerns are related to the experimental setup.\n- (4a) The model is evaluated on unusual benchmarks for multilingual multimodal modeling. These datasets are obtained through machine translation of the corresponding English dataset. The authors claim to fix minor errors (only for Chinese, and not even for all the other 6 European languages), yet do not provide any indication of the quality of the resulting test data. \n- (4b) The evaluation scores are based on GPT-4, which might not understand languages beyond English well. This is an hypothesis that might or might not hold true, but the fact that such concern exists, I believe it is enough to undermine the evaluation setup used by the authors.\n- (4c) There exists multilingual multimodal benchmarks. For instance, IGLUE [1] tests for 4 multimodal tasks in 20 languages, with data *manually* collected by native speakers. MaRVL [2] and XM3600 [3] even include images sourced from the countries where a language is spoken. These benchmarks are adopted in the literature and completely missing (even as part of the related work) in this paper.\n5. There is a lack of discussion of relevant related work. Models like {m,x}UNITER [2], CCLM [4] and TD-MML [5] are missing. The latter in particular shows how machine translation data can be very helpful during pretraining if filtering out poor translations, which provides a contrasting point to the one made by the authors in this paper. A discussion of these findings and how they differ is important to guide the community.\n\n---\n\n[1] Bugliarello et al. IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages. ICML\u201922.\n\n[2] Liu et al. Visually Grounded Reasoning across Languages and Cultures. EMNLP\u201921.\n\n[3] Thapliyal et al. A Massively Multilingual Multimodal Evaluation Dataset. EMNLP\u201922.\n\n[4] Zeng et al. Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training. ACL\u201923.\n\n[5] Qiu et al. Multilingual Multimodal Learning with Machine Translated Text. EMNLP\u201922."
                },
                "questions": {
                    "value": "1. The low CLIP scores in Wukong might be related to misunderstanding of Chinese captions from CLIP. You say that you perform manual inspection, can you elaborate more on your findings and their relation to CLIP scores?\n2.  It would be useful for the reader to have more information about the baselines, so that one can easily understand what changes among models when comparing their results."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3695/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698404959625,
            "cdate": 1698404959625,
            "tmdate": 1699636326191,
            "mdate": 1699636326191,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2vzaz0RMwn",
                "forum": "Kuh5qgCGCp",
                "replyto": "LYYpPYojvw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3695/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BZ1h"
                    },
                    "comment": {
                        "value": "Thanks for your time and constructive reviews. Following, we present the response to address your concerns and questions. If you feel our responses effectively address your concerns, please kindly reconsider your evaluation.\n\n**Q1: Claim about 'quasi-zero-shot' Term**       \nIn MLLM literature, to the best of our knowledge, there has been no report on the phenomenon where a model exclusively pre-trained on English multimodal data, can provide accurate responses in English for instructions in other languages. In this context, the foundational MLLM capability has been successfully generalized from English to the target language, requiring merely a switch in language to fully realize this capability. On the one hand, this demonstrates a remarkable strength: the model's ability to comprehend and respond to Chinese multimodal instructions without being exposed to any Chinese multimodal data. On the other hand, such capability is imperfect as the model cannot respond in Chinese, whereas only very little instructing tuning data is needed to switch the language. Therefore, we use the term \u201cquasi-zero-shot\u201d to describe this subtle capability. \n\nWe appreciate your valuable feedback and have refined the claim to a more precise expression that \"generalize to other languages in a (quasi-)zero-shot manner\", based on the quasi-zero-shot performance on image-to-text generation and zero-shot performance on text-to-image generation.\n\n*****\n\n**Q2: Language Choice**\n- The claim about low-resource language. In this paper, we refer to the low-resource language under a multimodal scenario. Please refer to the general response for a more detailed discussion. We have revised our paper to avoid misunderstanding with the term \u2018low-resource\u2019 language.\n- Language Diversity. Thanks for your advice! We have expanded our evaluation on a larger range of languages based on the IGLUE benchmark, which covers more diverse languages and obtains good performance, such as Russian in Cyrillic script with a 47.68 score on xGQA. We leave a wider range of language models in future work.\n\n*****\n\n**Q3: Experimental Setup**\n  - **Quality of test data**  \n  For the Chinese test data, we manually check the data quality. This operation has been adopted in previously well-recognized benchmarks such as IGLUE to ensure the test data quality.      \n  For other languages, note that directly using machine translation to translate the test benchmark without manual checking is also widely adopted in the literature [1][2][3]. Nonetheless, we agree that evaluation on manually validated benchmarks can better support the findings. During the response period, we additionally evaluate the performance of these languages on manually validated benchmark IGLUE.\n  - **Using GPT-4 as evaluator**\n    - **Feasibility**: GPT-4 has been widely accepted as an evaluator for AI chatbots, as also discussed in AlpacaEval [4], MTBench [5], and LLaVA Bench [6]. For example, the AlpacaEval benchmark is an authoritative benchmark for evaluating LLMs in following open-ended instructions. It uses GPT-4 as an automatic evaluator and shows a 0.94 Pearson correlation with human judgment. Our focus is to evaluate the performance of models in open-ended multimodal chats. Compared with traditional string-match-based metrics, GPT-4 evaluation is found to be more robust against the high diversity of natural language. \n    - **Multilingual Capability**: As for the multilingual capability, GPT-4 presents its strong multilingual capability on MMLU across languages in its report, which shows only a slight drop in the language we chose compared with English. We note that previous works [7][8] in the literature also use GPT-4 (ChatGPT) to evaluate multilingual tasks. For example, GEMBA [7] is a machine translation metric based on GPT-4 evaluation. We believe these results from the literature show the soundness of using GPT-4 to evaluate mainstream European languages.\n  - **Performance on IGLUE benchmark**  \n  Thanks for your valuable advice. We perform an additional evaluation on the IGLUE. Please refer to the general response for the results.\n\n[1] Ding, Ming, et al. Cogview: Mastering text-to-image generation via transformers. NeurIPS 2021.\n    \n[2] Ye, Fulong, et al. AltDiffusion: A Multilingual Text-to-Image Diffusion Model. Arxiv 2023.  \n\n[3] \u017dagar, Ale\u0161, and Marko Robnik-\u0160ikonja. Slovene SuperGLUE benchmark: translation and evaluation. LREC 2022.\n\n[4] AlpacaEval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval.\n\n[5] Zheng, Lianmin, et al. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. NeurIPS 2023.\n\n[6] Liu, Haotian, et al. Visual instruction tuning. NeurIPS 2023.\n\n[7] Kocmi, Tom, and Christian Federmann. Large language models are state-of-the-art evaluators of translation quality. EAMT 2023.\n\n[8] Lai, Viet Dac, et al. ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning. Arxiv 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559193528,
                "cdate": 1700559193528,
                "tmdate": 1700559193528,
                "mdate": 1700559193528,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rdfirs9r5S",
                "forum": "Kuh5qgCGCp",
                "replyto": "LYYpPYojvw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3695/Reviewer_BZ1h"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3695/Reviewer_BZ1h"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response, and for running additional experiments (and sorry for my late reply)!\n\nI'm considering increasing my score according to your response. Yet, I have three large concerns still:\n\n**General response:** I do not agree that 100M image--text pairs are low-resource data. Take Swahili as an example, while there are 10s of GB of text data [1], can you find 100M image--text pairs for it?\n\n**Q1:** given that you fine-tune the model on multimodal instructions in the target language, I would strongly prefer not to have any \"zero-shot\" string in your adaptation method. I still find \"quasi-zero-shot\" misleading.\n\n**Q3:** The languages that you selected for IGLUE evaluations are mostly high-resource. Would it be possible to evaluate on MaRVL or WIT?\n\n---\n\n[1] Conneau et al. Unsupervised Cross-lingual Representation Learning at Scale. ACL 2020."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741934351,
                "cdate": 1700741934351,
                "tmdate": 1700742157532,
                "mdate": 1700742157532,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9obN1Bsfyh",
            "forum": "Kuh5qgCGCp",
            "replyto": "Kuh5qgCGCp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3695/Reviewer_T6ab"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3695/Reviewer_T6ab"
            ],
            "content": {
                "summary": {
                    "value": "Text-to-image and image-to-text generation efforts have primarily been focussed on English only due to lack of large-scale, high quality data. This paper proposes MPM, an effective training paradigm for training large multimodal models in low-resource languages. They show that this technique enables competitive zero-shot performance on Chinese language as compared to models trained on the language data. This leverages the Bilingual Dual-coding Theory which states that visual semantics are largely language agnostic. MPM divides the non-English multimodal learning into two consecutive stages: multilingual alignment and multimodal alignment. The former focuses on building a multilingual model by using a pretrained multilingual large language model, while the latter culminates in a multimodal model spanning multiple languages on top of the multilingual model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper shows a simple yet effective technique of using a mulitlingual LLM as a pivot to transfer multilingual image-text alignment across different languages. \n- The model is trained on English data only but still manages to perform better on Chinese tasks than models trained on Chinese data. This is highly encouraging in making the state-of-the-art techniques on image-to-text and text-to-image generation available in other languages."
                },
                "weaknesses": {
                    "value": "None."
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3695/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3695/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3695/Reviewer_T6ab"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3695/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781963714,
            "cdate": 1698781963714,
            "tmdate": 1699636326106,
            "mdate": 1699636326106,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sms8Ejuncp",
                "forum": "Kuh5qgCGCp",
                "replyto": "9obN1Bsfyh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3695/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer T6ab"
                    },
                    "comment": {
                        "value": "Thanks for your recognition of our work!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558024190,
                "cdate": 1700558024190,
                "tmdate": 1700558024190,
                "mdate": 1700558024190,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EKpij2HEFH",
            "forum": "Kuh5qgCGCp",
            "replyto": "Kuh5qgCGCp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3695/Reviewer_mPhK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3695/Reviewer_mPhK"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies a simple idea to use multilingual language models as the pivot to connect different languages for multimodal applications. The idea is based on the assumption that visual semantics are language agnostic, thus aligning a visual modal to a pre-trained (and sometimes frozen) multlilingual language model could support multilingual applications even without aligned multimodal data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is an engineering style paper, with great qualitative examples. The idea itself is simple and intuitive (but however the execution and writing is a bit complicated, see the next section). \n2. This paper promised to open-source the code and model weights. This is going to have a positive impact on the community thanks to the open sourced model weights."
                },
                "weaknesses": {
                    "value": "1. The contribution of the MPM training paradigm is unclear. Taking the image-to-text generation as an example, in the multimodal pre-training step the proposal is to freeze multilingual LLM and only tune the visual module; in the instruction tuning step the proposal is to finetune everything on the datasets from pivot language and the target language. The multimodal pre-training step is quite straightforward as it aligns a trainable vision model to a pre-trained and frozen LLM; the instruction tuning step is a standard finetuning setup. If the contribution of this paper is to use a pre-trained multilingual LLM to support multilingual downstream tasks, then much more in-depth analysis should be done to justify the point. Examples include understanding the MPM design choices, how freezing LLM affects results, how translated pairs affect instruction tuning results, how many translated pairs are enough and what the trend looks like. \n2. Most of the tables present absolute comparisons, showing how great the proposed method is. However that might be less interesting from a reader's perspective without proper baselines and ablation tests as discussed in the above section (due to different dataset mixtures and tuning tricks used). It's high recommended to take a few tables, and perform multiple in-depth analysis and comparisons with controlled study, to make sure there are enough take aways and confidence in the current MPM design choices."
                },
                "questions": {
                    "value": "See the above sections."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3695/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786644717,
            "cdate": 1698786644717,
            "tmdate": 1699636326006,
            "mdate": 1699636326006,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OWonQu5FOB",
                "forum": "Kuh5qgCGCp",
                "replyto": "EKpij2HEFH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3695/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mPhK"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable suggestion and acknowledgment. Following, we present the response to address your concerns and questions.          \n\n**Q1: Contribution of MPM training paradigm.**         \nWe clarify the contribution of MPM in the general response. Please refer to the general response.\n\n*****\n\n**Q2: Analysis of MPM effectiveness**          \nTo realize strong multimodal capability across different languages, data strategy is one of the most important choices, considering the highly imbalanced data resource situation in different languages. Therefore, we focus on this perspective, while answers to some advised questions can be found in the paper. \n- **How freezing LLM affects results**: We discuss the effect of freezing LLM in Sec. 3.2. Freezing LLM can speed up training and avoid the negative influence of the low-quality text in the multimodal pre-training dataset.          \n- **How translated pairs affect instruction tuning results**: The effect of translated pairs is also discussed in Sec. 3.2. We found that translated pairs of instructing tuning datasets can switch the model to respond in the target language.\n- **How many translated pairs are enough and what the trend looks like**: Our experiments show that merely 150K translated pairs during the SFT period are enough to switch on capabilities in the target language. Investigating the tight bounds and trends is a good idea that can lead to deeper understanding and stronger conclusions. Due to the time limit during the response period, we will incorporate a more detailed investigation in the final version.\n\n*****\n\n**Q3: In-depth analysis and comparisons with controlled study**           \nWe highly agree that analysis in controlled comparison is valuable for readers. For the main focus, we conducted a comprehensive ablation study with controlled comparison to analyze the effect of data from different sources (Table 5 and discussion in Sec. 5.3), which yields the following findings:\n- We ablate the languages of the pretraining dataset. The results show that relying solely on a native Chinese dataset is insufficient for achieving good performance for both image-to-text and text-to-image tasks.\n- We ablate the languages of the instruction tuning dataset. The results show that English data also plays a crucial role in improving the chat capability of the model in Chinese during the instruction tuning stage.\n- We ablate the filter process to the native Chinese dataset. The results show that the filtering process applied to the native dataset is essential for the model performance in Chinese.\n- We ablate the translated dataset in the pretraining stage. The results show that incorporating the native Chinese dataset and the translated Chinese dataset during the pre-training stage yields a marginal improvement in the model performance.\n\nWe believe the analysis above offers a helpful know-how handbook for building strong non-English MLLMs. We will explore more data and training strategies as kindly suggested in our future work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557974637,
                "cdate": 1700557974637,
                "tmdate": 1700557974637,
                "mdate": 1700557974637,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gCGXmcOO1X",
            "forum": "Kuh5qgCGCp",
            "replyto": "Kuh5qgCGCp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3695/Reviewer_scbv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3695/Reviewer_scbv"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a method for training a multilingual and multimodal model without needing to rely on large amounts of data in all of the target languages. Instead, the method relies on multilingual and multimodal pivoting, using a high-resource language to provide the target distributions for the lower-resource language. The method is realized in an image-text and text-to-image model named VisCPM-Chat/Paint, respectively. The model is trained by aligning the low-resource Chinese data against the high-resource English data using the CPM-Bee bilingual Zho/Eng language model; the method is also shown to generalize to LLaMA."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1: Conceptually simple approach to learning a multilingual and multimodal model. Makes good use of the vast amount of English language resources.\n\nS2: Extensive experiments with relevant benchmark datasets.\n\nS3: The approach is shown to generalize to more than one language model (LLaMA and CPM-Bee)"
                },
                "weaknesses": {
                    "value": "W1: There are some inconsistencies in the argumentation used throughout the paper. For example, in Section 4.1, the authors argue that translating a large-scale pretraining dataset will consume substantial computing resources, which is a fair argument. However, in Section 4.2, the authors describe that they train a version of their model called VisCPM-Chat+ using 1\n36 million translated examples from LAION-COCO. The authors could improve their argumentation if they clarify whether it\nis or is not challenging to create such translated examples. Bear in mind that other researchers have already translated\npretraining datasets, e.g. Thapliyal+ EMNLP 2022.\n\nW2: The motivation for this paper is to create multimodal models for low-resourced languages via the proposed pivoting method but the main languages used in the experiments are Chinese and English, neither of which could ever be described as low-resource languages. The remaining languages used for the experiments in Section 5.1.3 are also hardly low-resource: German, French, Spanish, Italian, and Portuguese. Using these languages and claiming that the method applies to low-resource settings affects the credibility of the conclusions that one can draw for actual low-resource languages. See, for example, Joshi+ ACL 2020 for a discussion on low-resource languages.\n\nJoshi, Pratik, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. \"The State and Fate of Linguistic Diversity and Inclusion in the NLP World.\" In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 6282-6293. 2020.\n\nThapliyal, A. V., Tuset, J. P., Chen, X., & Soricut, R. (2022, December). Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (pp. 715-729)."
                },
                "questions": {
                    "value": "1. Why do you think Chinese can be considered a low-resource language? See W2 for more details on my concerns.\n2. In Section 5.1, why is it methodologically sound to use GPT-4 to evaluate model output?\n3. In Figure 2, why is a CLIPscore threshold of 0.18 used to define high-quality? How was this determined?\n4. In Section 4.2, what was the dataset of 100M image-text pairs used to align with the frozen LLM for 180K steps?\n5. Did you also use M2M-100 to translate the 136M examples in the COCO-LAION dataset?\n6. Which version of M2M-100 did you use for translation?\n7. In Section 5.2.2, you describe an FID of 9.5 or 9.9 \"comparable\" to Stable Diffusion FID is 8.6. Is this a reasonable claim?\n8. How does your model perform on a larger set of languages in the multilingual multimodal benchmark IGLUE by Bugliarello+ ICML 2022?\n\nBugliarello, E., Liu, F., Pfeiffer, J., Reddy, S., Elliott, D., Ponti, E. M., & Vuli\u0107, I. (2022, June). IGLUE: A benchmark for transfer learning across modalities, tasks, and languages. In International Conference on Machine Learning (pp. 2370-2392). PMLR."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3695/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3695/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3695/Reviewer_scbv"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3695/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698864139656,
            "cdate": 1698864139656,
            "tmdate": 1699636325915,
            "mdate": 1699636325915,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aX5fGFKbzI",
                "forum": "Kuh5qgCGCp",
                "replyto": "gCGXmcOO1X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3695/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer scbv"
                    },
                    "comment": {
                        "value": "We are grateful for your time and the careful review. We present the response to address your concerns and questions. Please kindly reconsider your evaluation if you feel our responses effectively address your concerns.\n\n**Q1: Why translate the dataset?**       \nIt is certainly true that the translation of the 136M examples did consume substantial computational resources. In our experiments, we employed the bilingual LLM, CPM-Bee 10B, for translation. The translation process consumed 4,533 GPU hours on NVIDIA A100 80G for 136 million examples. We would like to clarify that the goal of this operation is to empirically validate the hypothesis of MPM in a controlled setting, verifying whether translated datasets are necessary for training MLLMs. Our experimental results show that marginal improvement can be achieved even with high-quality large-scale datasets translated by strong bilingual LLMs. Considering the cost vs improvement, the conclusion is that translating pretraining data is unfavored for training non-English MLLMs. We hope the empirical results contribute to a better understanding of the role of data translation in training non-English MLLMs.\n\n*****\n\n**Q2: The meaning of low-resource languages**          \nThanks for your suggestion. Please refer to the general response for a more detailed discussion. In general, we focus on the low-resource nature of languages in the multimodal scenario. We have revised our paper to avoid confusion with the terminology \"low-resource languages\" in pure language resources.\n\n*****\n\n**Q3: Methodological soundness to use GPT-4 for evaluation**        \nGPT-4 has been widely accepted as an evaluator for AI chatbots, as also discussed in AlpacaEval [1], MTBench [2], and LLaVA Bench [3]. For example, the AlpacaEval benchmark is an authoritative benchmark for evaluating LLMs in following open-ended instructions. It uses GPT-4 as an automatic evaluator and shows a 0.94 Pearson correlation with human judgment. Our focus is to evaluate the performance of models in open-ended multimodal chats. Compared with traditional string-match-based metrics, GPT-4 evaluation is found to be more robust against the high diversity of natural language. Therefore, we follow previous works and use GPT-4 to evaluate MLLM conversation performance.\n\n*****\n\n**Q4: CLIP score threshold**         \nWe referred to Taiyidiffusion's criteria for data volume and filtering standards (including CLIP score) in training Chinese text-to-image models. We conducted a thorough manual check for the correlation between image-text alignment and the CLIP score. Here are some specific findings about image-text pairs with low CLIP scores:\n- Broken Images: Some image URLs are broken or redirected to the generic default image for downloading. \n- Unrelated captions: Some captions are entirely irrelevant to the associated image content.\n- Incomplete sentences: Some captions are truncated or missing punctuation\n\n*****\n\n**Q5: Dataset composition**           \nThe 100M image-text pairs used to align with the frozen LLM consist of COCO, Visual Genome, CC3M, CC12M, and Laion-COCO. We introduce related training details in Appendix Sec. C.\n\n*****\n\n**Q6: Choice of Machine Translation System**          \nWe use the CPM-Bee 10B for all En-Zh translations due to its better performance. For other languages, we use M2M100 12B for translation in German, French, Spanish, Italian, and Portuguese.\n\n*****\n\n**Q7: Conclusion about FID performance**         \nWe draw a conclusion on the performance comparison between our model (9.5/9.9 FID) and Stable Diffusion (8.6 FID) referring to the literature. For instance, Unidiffuser[4], with an FID of 9.7, concluded in their paper that their model's performance on single text-to-image generation tasks is \"on par\" with custom diffusion models such as Stable Diffusion.\n\n*****\n\n**Q8: Performance on IGLUE**          \nThanks for your advice. Please refer to the general response for the performance of MPM on IGLUE.\n\n*****\n\n[1] AlpacaEval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval.\n\n[2] Zheng, Lianmin, et al. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. NeurIPS 2023.\n\n[3] Liu, Haotian, et al. Visual instruction tuning. NeurIPS 2023.\n\n[4] Bao, Fan, et al. One transformer fits all distributions in multi-modal diffusion at scale. ICML 2023"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557117782,
                "cdate": 1700557117782,
                "tmdate": 1700557117782,
                "mdate": 1700557117782,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]