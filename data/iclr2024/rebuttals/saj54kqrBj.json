[
    {
        "title": "Self-Tuning Self-Supervised Anomaly Detection"
    },
    {
        "review": {
            "id": "Sl3Ol4M9MY",
            "forum": "saj54kqrBj",
            "replyto": "saj54kqrBj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2588/Reviewer_iRPB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2588/Reviewer_iRPB"
            ],
            "content": {
                "summary": {
                    "value": "This paper mainly studies an adaptive method to search the optimal data augmentation function for self-supervised anomaly detection task. It formulates the learning of potential abnormal distribution (i.e. true anomaly-generating mechanism) as a second-order optimization problem. To this end, two aspects are mainly studied: (1) a new validation loss for differentiable distribution matching of augmented training data and the unlabeled testing anomalies; (2) some differentiable augmentation functions for optimizing their learnable control factors and detector parameters alternatively. Experiments on both semantic and non-semantic anomalies demonstrates the effectiveness."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper is well-motivated and clearly presented. Researchers could gain a lot of insight into this paradigm and the potential impact on discriminative learning with synthetic anomaly is significant. Besides, the authors provide theoretical evidence and clear illustrations for better understanding, including their idea, method and some demonstrative examples.\n\n- The optimal augmentation function seems to alleviate the limitation and difficulty of artificially synthesized pseudo anomalies, due to the better consistency with real anomalies. \n\n- The parameters are optimized during end-to-end training, which is feasible and ideal."
                },
                "weaknesses": {
                    "value": "- My major concern of this paper lies in the strong assumption, here are a couple of aspects:\n\n1.\tThe authors evaluate MV Tech AD with the data being collected according to given abnormal patterns. I wonder if the proposed framework can simultaneously deal with multiple different augmentations, since this case is more general in real-world anomaly detection scenarios and is more likely to avoid overfitting to the optimal **a**. It will be helpful to show the results with **UN**split original classes in MV Tech AD.\n\n2.\tThe assumption that the validation and test distributions are consistent may be too strong for the setting of anomaly detection task (*e.g.* new anomaly pattern or even mixing anomalies makes **a** not optimal). It may not be reasonable to omit test images drawn from unseen distributions in real scenarios.\n\n- The author believes that the heuristic function $S$ will be high (higher variance of anomaly scores) *only if* augmentation parameters are initialized better (more separable distributions). However, this may not always hold, as the optimization of a mismatched *strong/hard* augmentation (as long as the decision boundary divides testing distribution into any two parts) may also lead to high variance of anomaly scores. I wonder if there are any theoretical or intuitive explanations about this issue.\n\n- The authors use CutDiff and Rot for non-semantic and semantic shift detection respectively. But I would like to know could ST-SSAD learn zero-angle Rot for non-semantic defects and zero-size CutDiff for semantic shifts in end-to-end optimization when using both augmentations together.\n\n- The term ``Ratio invariance\u2019\u2019 may be imprecise. With the augmentation quantity changed, $L_{val}$ could be changed together because the included $\\frac{\\sqrt{N}}{{\\Vert {Z}^{c}\\Vert}_{F}}$ is different (I notice that this have no negative impact on optimization target).\n\n- There are some writing issues, *e.g.* superscript $^{n}$ should be $^{a}$ in **Lemma 2**."
                },
                "questions": {
                    "value": "Please refer to the \"weaknesses\" part for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2588/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698395140964,
            "cdate": 1698395140964,
            "tmdate": 1699636196344,
            "mdate": 1699636196344,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZYbHpoQZf1",
                "forum": "saj54kqrBj",
                "replyto": "Sl3Ol4M9MY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2588/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2588/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer iRPB"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive and detailed feedback. We provide our answers to the weaknesses and questions pointed out by the reviewer. Some of our answers point to the common responses.\n\n> **W1.** The authors evaluate MV Tech AD with the data being collected according to given abnormal patterns. I wonder if the proposed framework can simultaneously deal with multiple different augmentations, since this case is more general in real-world anomaly detection scenarios and is more likely to avoid overfitting to the optimal a. It will be helpful to show the results with UNsplit original classes in MV Tech AD.\n\nOur current approach can be subpar on test data that contains anomalies of multiple types, for which tuning the discrete choice of augmentation functions becomes a new challenge.  Our work presents a framework to tune the *continuous* hyperparameters of an augmentation function, provided with the discrete choice. Please refer to **C2** of our common responses for possible hyperparameter optimization (HPO) approaches we laid out as future work.\n\n> **W2.** The assumption that the validation and test distributions are consistent may be too strong for the setting of anomaly detection task (e.g. new anomaly pattern or even mixing anomalies makes a not optimal). It may not be reasonable to omit test images drawn from unseen distributions in real scenarios.\n\nWe believe that transductive learning is common in real-world anomaly detection tasks. Please  refer to **C1** of our common responses for an extended discussion on transductive vs. inductive learning in the context of anomaly detection. Nevertheless, we also conducted additional experiments dividing unlabeled test data into validation and (new) test data, and then using only the validation data for computing the alignment loss. Please refer to **C3 #2** of our common responses for the results.\n\n> **W3.** The author believes that the heuristic function $S$ will be high (higher variance of anomaly scores) only if augmentation parameters are initialized better (more separable distributions). However, this may not always hold, as the optimization of a mismatched strong/hard augmentation (as long as the decision boundary divides testing distribution into any two parts) may also lead to high variance of anomaly scores. I wonder if there are any theoretical or intuitive explanations about this issue.\n\nWe remark that this heuristic is employed only after optimizing for the alignment first. In other words, we use it only to choose from among models that are already optimized to align with the test data as well as possible. Since a decision boundary dividing the test data would not yield a low validation loss in the first place, the heuristic is not likely to be exposed to such scenarios in practice.\n\n> **W4.** The authors use CutDiff and Rot for non-semantic and semantic shift detection respectively. But I would like to know could ST-SSAD learn zero-angle Rot for non-semantic defects and zero-size CutDiff for semantic shifts in end-to-end optimization when using both augmentations together.\n \nAs discussed in detail in **C2** of our common responses, our current model is designed to tune the *continuous* hyperparameters of a given augmentation function. Given a set of augmentation functions, making a discrete choice between the different functions or optimizing a mixture of functions together poses a new challenge. For example, with both CutDiff and Rotate included, the ratio of augmented samples to generate with the two functions becomes another hyperparameter. Being the first solution to the automatic model selection for SSAD, we expect our work will trigger several follow-up works.\n\n> **W5.** The term \u201cRatio invariance\u201d may be imprecise. With the augmentation quantity changed,  $L_\\mathrm{val}$  could be changed together because the included $\\sqrt{N} / \\| Z^C \\|_F$ is different (I notice that this have no negative impact on optimization target).\n\nThat is correct. The total distance normalization is affected by the size of augmented data, and thus our loss function is not exactly invariant to it. However, the effect is negligible, since that mainly changes the scale of embeddings, rather than their distributions. For preciseness, we have changed the term into \"ratio robustness\" in the revised manuscript.\n\n> **W6.** There are some writing issues, e.g. superscript $^n$ should be $^a$ in Lemma 2.\n\nThank you for finding out the typo. We have updated the revised manuscript accordingly."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700400579568,
                "cdate": 1700400579568,
                "tmdate": 1700400579568,
                "mdate": 1700400579568,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "msMClEhHD8",
            "forum": "saj54kqrBj",
            "replyto": "saj54kqrBj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2588/Reviewer_ht7A"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2588/Reviewer_ht7A"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose learnable augmentations for transductive anomaly\ndetection.  In the transductive scenario, the test set, including\nanomalies to be detected, are available for training.  They called\ntheir method Self-Tuning Self-Supervsied Anomaly Detection (ST-SSAD).\nThe augmentations help simulate anomalies.  Using the proposed\nvalidation loss, ST-SSAD tries to make the original training instances\ntogether with their augmentations, similar to the test set.  The\nvalidation loss is based on the distance, in the representation space,\nbetween each test instance and the mean of training instances, as well\nas the mean of augmented instances.  Instances in representation space\nare transformed to have unit total pairwise squared distance.  Binary\ncross entropy loss is used as the training loss.  For learnable\naugmentations, they proposed differentiable CutDiff (local\naugmentation) and rotation (global augmentation).\n\nST-SSAD was compared with multiple baseline algorithms on two datasets\nwith different types of anomalies. Empirical results indicate ST-SSAD\ngenerally outperforms the baselines.  Ablation studies indicate the\nproposed components of ST-SSAD contribute to higher performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Allowing augmentations to be learnable/differentiable is interesting.\nExamples were presented to show augmentations can match anomalies.\nEmpirical results indicate ST-SSAD generally outperforms the\nbaselines.  The paper is generally well written."
                },
                "weaknesses": {
                    "value": "During evaluation, each anomaly type is separated.  The proposed\nST-SSAD seems to assume only one anomaly type exists in the test set.\nThat is, the user might need to use ST-SSAD for each anomaly type.\nHow to handle multiple anomaly types in the same test set is not\nclear.\n\nDetails are in questions below."
                },
                "questions": {
                    "value": "1.  In the experiments, anomaly types are separately evaluated.  That\nseems to mean that the anomaly type is known, and the augmentation\nparameters are learned to match the anomaly type.  This seems to be\nassumed in Equation 3 because the second term calculates the mean of\nthe augmented instances.  That is, Eq. 3 seems to be finding an\naugmentation and its parameters to match the anomaly type.  If that is\ncorrect, how can multiple types of anomalies in the same test set be\nhandled?\n\n2.  While the augmentation parameters are learnable, the augmentation\ntypes such as cut and rotation need to be specified.  Also, to be\ndetected, seemingly an anomaly type in the test set needs to match one\nof the augmentation types.  Could the matching be relaxed?  That is,\nthe user potentially does not need to know what the anomaly types are.\n\n3.  What is the size of $D_{aug}$?\n\n4.  Eq 2, equation on the right for $z_i^c$: should $z$ be $z_i$ and\nthe summation is over another index such as $j$ to not confuse with\n$i$?\n\n5.  Eq 3: mean(.) seems to be similar to $1/N\\sum_{i=1}^{N}z_{i}$ in\nEq 2.  If so, using mean(.) in Eq 2 would make the presentation\nconsistent.  If not, what is the difference?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2588/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698687751310,
            "cdate": 1698687751310,
            "tmdate": 1699636196185,
            "mdate": 1699636196185,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CBtjApqgGx",
                "forum": "saj54kqrBj",
                "replyto": "msMClEhHD8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2588/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2588/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ht7A"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive and detailed feedback. We provide our answers to the weaknesses and questions pointed out by the reviewer. Some of our answers point to the common responses.\n\n> **W1.** During evaluation, each anomaly type is separated. The proposed ST-SSAD seems to assume only one anomaly type exists in the test set. That is, the user might need to use ST-SSAD for each anomaly type. How to handle multiple anomaly types in the same test set is not clear.\n\n> **Q1.** In the experiments, anomaly types are separately evaluated. That seems to mean that the anomaly type is known, and the augmentation parameters are learned to match the anomaly type. This seems to be assumed in Equation 3 because the second term calculates the mean of the augmented instances. That is, Eq. 3 seems to be finding an augmentation and its parameters to match the anomaly type. If that is correct, how can multiple types of anomalies in the same test set be handled?\n\nAs the reviewer accurately pointed out, our approach does not currently address test data that contain anomalies of multiple types, for which tuning the discrete choice of augmentation functions becomes a new challenge. Our work presents a framework to tune the *continuous* hyperparameters of an augmentation function, provided with the discrete choice. We refer to **C2** of our common responses for possible hyperparameter optimization (HPO) approaches we laid out as future work toward tackling a mixture of anomalies in test data.\n\n> **Q2.** While the augmentation parameters are learnable, the augmentation types such as cut and rotation need to be specified. Also, to be detected, seemingly an anomaly type in the test set needs to match one of the augmentation types. Could the matching be relaxed? That is, the user potentially does not need to know what the anomaly types are.\n\nAs we show in Section 4.4, our approach succeeds if the augmentation function is aligned with the true anomalies at the functional level. As discussed in **C2** of our common responses, the *discrete* choice of the augmentation from a *set* of possible functional forms can be addressed via black-box hyperparameter optimization (HPO). Our work presents a framework to tune the *continuous* hyperparameters provided with the discrete choice. We expect several follow-up works, not only proposing new differentiable augmentation functions but also more elaborate HPO techniques to select from this set, as laid out in **C2.**\n\n> **Q3.** What is the size of $D_{aug}$?\n\nWe set the size of $D_{aug}$ to be the same as that of $D_{trn}$ in all experiments. This was to avoid introducing an additional hyperparameter. To show robustness, we conducted additional experiments varying the ratio of augmented data over the size of training data. Please refer to **C3 #1** of our common responses for the results.\n\n> **Q4.** Eq 2, equation on the right for $z_i^c$: should $z$ be $z_i$ and the summation is over another index such as $j$ to not confuse with $i$?\n\nThat\u2019s correct. We have changed Eq. (2) accordingly in the revised manuscript. Thank you for pointing it out.\n\n> **Q5.** Eq 3: mean(.) seems to be similar to $1/N \\sum_{i=1}^N z_i$ in Eq 2. If so, using mean(.) in Eq 2 would make the presentation consistent. If not, what is the difference?\n\nBoth expressions are almost the same: mean(.) in Eq. (3) and $1/N \\sum_{i=1}^N z_i$ in Eq. (2). The only difference is that mean(.) in Eq. (3) is an operator on a set of vectors, while in Eq. (2) we computed the mean of rows in a matrix. It would make them consistent if we treat both as sets or both as matrices, but we believe the current way is more intuitive and easier to understand."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700399484815,
                "cdate": 1700399484815,
                "tmdate": 1700399484815,
                "mdate": 1700399484815,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "itLzGY0wgi",
                "forum": "saj54kqrBj",
                "replyto": "CBtjApqgGx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2588/Reviewer_ht7A"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2588/Reviewer_ht7A"
                ],
                "content": {
                    "title": {
                        "value": "comments on authors' response"
                    },
                    "comment": {
                        "value": "Thanks for the response.  C2 has some preliminary ideas, and results from them would strengthen the paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611912250,
                "cdate": 1700611912250,
                "tmdate": 1700611912250,
                "mdate": 1700611912250,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yINgrqyOgm",
            "forum": "saj54kqrBj",
            "replyto": "saj54kqrBj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2588/Reviewer_gndU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2588/Reviewer_gndU"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces ST-SSAD, a novel approach for self-supervised anomaly detection (SSAD). It addresses the challenge of selecting proper data augmentation functions to generate pseudo-anomalies that are close to real anomalies. ST-SSAD offers two main contributions: an unsupervised validation loss using an unlabeled test dataset for tuning augmentation and differentiable augmentation functions for end-to-end hyperparameter tuning. Experimental results on two testbeds demonstrate performance improvements through the systematic augmentation tuning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper addresses the challenge of end-to-end augmentation tuning in SSAD. \n- The idea of employing differentiable augmentation, such as CutDiff, is interesting and demonstrates potential applicability to other domains beyond anomaly detection.  \n- The paper is well organized and clearly written overall."
                },
                "weaknesses": {
                    "value": "- A major concern is that ST-SSAD replies on the entire test dataset during training and tuning. While the authors mention transductive learning, this approach is not quite realistic, particularly in the context of anomaly detection. The tuning result will be overly sensitive to the specific anomaly types in the test data and may not generalize well. The paper lacks clarification, experimental results, or in-depth discussion on this issue, which significantly limits the applicability and advantages of the proposed method.    \n\n- The mean distance loss is proposed for ratio invariance with theoretical properties, but no experimental result validating this invariance is provided. \n\n- The method still requires prior knowledge about anomalies and heavily depends on it. For example, the augmentation functions of either local (CutDiff) or global (rotation) augmentations are considered and therefore, it works well only when anomalies closely resemble specific shapes that these functions can reflect. The method will also fail in the case where rotated samples are considered normal. \n\n- The authors state that 'we focus on the performance of each anomaly type rather than overall accuracy.' However, in real-world scenarios, it is common to encounter various types of anomalies. Therefore, it will be more crucial to investigate such practical scenarios. \n\n- It was mentioned that \"there are no direct competitors on end-to-end augmentation hyperparameter tuning...\"; however, it is essential to include performance comparison with the latest models that clearly distinguish train and test data. The results in Tables 1 and 2 appear to be more like an ablation study, so it is difficult to assess whether the proposed method truly outperforms the latest models in a meaningful way."
                },
                "questions": {
                    "value": "- Can you provide results using a validation set that is disjoint from the test set? \n\n- I guess the proposed method may be quite sensitive to the proportion of abnormal samples in the test set. Can you provide experimental results or a discussion addressing this issue? And please provide the ratio between normal and abnormal samples in the presented results of the current manuscript. \n\n- Is it possible for the model to learn effectively in a scenario where abnormal samples are inherently present in the training set but remain unlabeled?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2588/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698750846519,
            "cdate": 1698750846519,
            "tmdate": 1699636195953,
            "mdate": 1699636195953,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t8JPrpfVMe",
                "forum": "saj54kqrBj",
                "replyto": "yINgrqyOgm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2588/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2588/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gndU"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive and detailed feedback. We provide our answers to the weaknesses and questions pointed out by the reviewer. Some of our answers point to the common responses.\n\n> **W1.** A major concern is that ST-SSAD replies on the entire test dataset during training and tuning. While the authors mention transductive learning, this approach is not quite realistic, particularly in the context of anomaly detection. The tuning result will be overly sensitive to the specific anomaly types in the test data and may not generalize well. The paper lacks clarification, experimental results, or in-depth discussion on this issue, which significantly limits the applicability and advantages of the proposed method.\n\nWe believe that transductive learning is common in real-world anomaly detection tasks, whereas generalization is a matter for inductive learning. Please refer to **C1** of our common responses. \n\n> **W2.** The mean distance loss is proposed for ratio invariance with theoretical properties, but no experimental result validating this invariance is provided.\n\nWe have conducted additional experiments varying the ratio of augmented data over the size of training data. Please refer to **C3 #1** of our common responses for the result.\n\n> **W3.** The method still requires prior knowledge about anomalies and heavily depends on it. For example, the augmentation functions of either local (CutDiff) or global (rotation) augmentations are considered and therefore, it works well only when anomalies closely resemble specific shapes that these functions can reflect. The method will also fail in the case where rotated samples are considered normal.\n\nAs we discussed in Section 4.4, our approach succeeds if the augmentation function is aligned with the true anomalies at the functional level. Nevertheless, as the first systematic approach to the automatic model selection for self-supervised anomaly detection, we believe our work can lead to numerous future works that extend our approach to new differentiable augmentations that capture other types of real-world anomalies. Please refer to **C2** of our common responses for more discussion on the problem setup and suggested directions of future works.\n\n> **W4.** The authors state that 'we focus on the performance of each anomaly type rather than overall accuracy.' However, in real-world scenarios, it is common to encounter various types of anomalies. Therefore, it will be more crucial to investigate such practical scenarios.\n\nOur current approach does not address test data that contain anomalies of multiple types, for which tuning the discrete choice of augmentation functions becomes a new challenge. Our work presents a framework to tune the *continuous* hyperparameters of an augmentation function, provided with the discrete choice. As we answered to W3, please refer to **C2** of our common responses for possible hyperparameter optimization (HPO) approaches we laid out as future work toward tackling a mixture of anomalies in test data.\n\n> **W5.** It was mentioned that \"there are no direct competitors on end-to-end augmentation hyperparameter tuning...\"; however, it is essential to include performance comparison with the latest models that clearly distinguish train and test data. The results in Tables 1 and 2 appear to be more like an ablation study, so it is difficult to assess whether the proposed method truly outperforms the latest models in a meaningful way.\n\nPlease note that it is difficult to conduct a fair comparison between our approach and other works on anomaly detection, since the extent of \u201cfair\u201d hyperparameter tuning is hard to define between different lines of works. A vast majority of work on unsupervised anomaly detection does not address the unsupervised hyperparameter (HP) tuning problem at all.\n\nOur results show significant performance boost over two most prominent unsupervised approaches; DeepSVDD (one-class) and DeepAE (reconstruction-based). On the self-supervised side, we show that tuning augmentation significantly outperforms arbitrarily choosing the augmentation, including the CutOut and CutPaste approaches (CO and CP in Table 1) proposed by Devries & Taylor (2017) and Li et al. (2021), respectively.  Since we used the detector network and the score function used in (Li et al., 2021) throughout all experiments, we can consider the RS-CP and RD-CP baselines in Table 1 as \"untuned\" versions of the CutPaste approach (Li et al., 2021)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700399013892,
                "cdate": 1700399013892,
                "tmdate": 1700399013892,
                "mdate": 1700399013892,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hve782Gdxq",
                "forum": "saj54kqrBj",
                "replyto": "yINgrqyOgm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2588/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2588/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gndU (continued)"
                    },
                    "comment": {
                        "value": "> **Q1.** Can you provide results using a validation set that is disjoint from the test set?\n\nYes, we have conducted additional experiments dividing unlabeled test data into validation and (new) test data, and then using only the validation data for computing the alignment loss. Please refer to **C3. #2** of our common responses for the results.\n\n> **Q2.** I guess the proposed method may be quite sensitive to the proportion of abnormal samples in the test set. Can you provide experimental results or a discussion addressing this issue? And please provide the ratio between normal and abnormal samples in the presented results of the current manuscript.\n\nOur datasets already vary in the proportion of anomalies they contain in the test data. The following table shows the percentage of anomalies across different tasks in the MVTec dataset, where the minimum and maximum are 0.147 and 0.404, respectively. Similarly in the SVHN dataset, the fraction is between 0.279 and 0.722, since the different classes contain different numbers of examples. Thus, we believe that our approach is not sensitive to the ratio of anomalies in test data, as long as the anomalies are separable from the normal data and thus the alignment can be measured by our validation loss.\n\nobj_type | ano_type | fraction\n--- | --- | ---\ncable | bent_wire | 0.183 \ncable | cable_swap | 0.171 \ncable | combined | 0.159 \ncable | cut_inner_insulation | 0.194 \ncable | cut_outer_insulation | 0.147 \ncable | missing_cable | 0.171 \ncable | missing_wire | 0.147 \ncable | poke_insulation | 0.147 \ncarpet | color | 0.404 \ncarpet | cut | 0.378 \ncarpet | hole | 0.378 \ncarpet | metal_contamination | 0.378 \ncarpet | thread | 0.404 \ngrid | bent | 0.364 \ngrid | broken | 0.364 \ngrid | glue | 0.344 \ngrid | metal_contamination | 0.344 \ngrid | thread | 0.344 \ntile | crack | 0.340 \ntile | glue_strip | 0.353 \ntile | gray_stroke | 0.327 \ntile | oil | 0.353 \ntile | rough | 0.312 \n\n> **Q3.** Is it possible for the model to learn effectively in a scenario where abnormal samples are inherently present in the training set but remain unlabeled?\n\nYes, this refers to the setting where training data is not \u201cclean\u201d (i.e. anomaly-free) but is contaminated with some anomalies. We have conducted additional experiments by varying the degree of contamination in the training data. Please refer to **C3 #3** of our common responses for the results."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700399267354,
                "cdate": 1700399267354,
                "tmdate": 1700399267354,
                "mdate": 1700399267354,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]