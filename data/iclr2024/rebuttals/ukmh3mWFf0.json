[
    {
        "title": "Attributed Graph Clustering via Coarsening with Modularity"
    },
    {
        "review": {
            "id": "jxqiUl2JI0",
            "forum": "ukmh3mWFf0",
            "replyto": "ukmh3mWFf0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9264/Reviewer_s7a2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9264/Reviewer_s7a2"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new method for clustering attributed graphs. The method is heavily based on a recent graph coarsening method, termed Featured Graph Coarsening (FGC), which was proposed by Kumar et al (2023) for coarsening attributed graphs. The authors in this paper introduces a modularity-based regularization term to the original optimization objective of FGC, and empirically show that the new formulation is useful for clustering attributed graphs. In addition, the authors demonstrate graph neural networks (GNNs) can be integrated in the proposed framework to further enhance the clustering performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The empirical comparisons presented in the main text, especially those in Table 1, are extensive."
                },
                "weaknesses": {
                    "value": "- Given that the proposed optimization objective in Equation 5 comes from simply adding a modularity-based regularization term $\\mbox{tr}(C^TBC)$ to the original objective function of FGC (Kumar et al, 2023), the originality of this work is very limited, both methodology-wise and technical-wise. It looks like this paper simply (1) adds a regularization term to an existing work and (2) conducts some experiments on a few selected datasets. If there is something very novel, I would recommend the authors emphasize on those aspects. \n\n- The overall quality/clarity of this paper can be significantly improved. First of all, there are a couple of ambiguous statements and overstatements. Here are some examples:\n  - On page 1, in the second paragraph, the authors say that \"... the Fiedler vector (the eigenvector of the second smallest eigenvalue of the Laplacian) produces a graph cut minimal in the weights of the edges (Fiedler, 1973).\" As a reader I find it difficult to understand what \"a graph cut minimal in the weights of the edges\" means in this context. If the authors mean minimum cut, then I don't think this statement is correct. The relationship between the Fiedler vector and the minimum cut is not mentioned in the original paper (Fiedler, 1973), and in general it does not give rise to a minimum cut.\n  - On page 1, in the second paragraph, the authors say that \"... they assume each node gets mapped to one cluster, which is not consistent with real graph data.\" This is a clear overstatement. As far as I know, most of the node classification benchmarks used by the GNN community has non-overlapping clusters/classes. These include the 3 citations networks in Table 1, and several other attributed datasets from Table 3 in the appendix.\n\n  In addition, the presentation can be greatly improved if proper definitions/citations are provided in the right place. Here are some examples:\n    -  The term Feature Graph Coarsening (FGC) first appears at the end of page 1 without a citation. The authors should cite Kumar et al (2023) here.\n    - Similarly, the abbreviation GNN appears at the end of page 1, but the full definition \"Graph Neural Network (GNN)\" only appears much later in Section 2, at the end of page 2.\n    - In the first paragraph of Section 3.3, it is unclear what \"the volume of inter-cluster edges\" means. The authors should define it.\n    - In Equation 1, the expression $\\delta(c_i,c_j)$ is not defined.\n    - In the first paragraph of Section 4.1, it is unclear what \"the original graph is smooth\" means. If the authors mean that the graph has smooth signals, they should just say \"the original graph has smooth signals\".\n\n  There are also many typos and misplacements of mathematical symbols. I highly recommend the authors read the paper thoroughly and fix all the typos.\n\n- Empirically, even though the authors compared with a number of other methods as shown in Table 1, the experiments are only carried over 3 small datasets. Table 3 in the appendix has more results on other datasets, but the authors only compare with two other methods, and one of them is just FGC.\n\n- The proposed method has a lot of parameters, i.e. $\\alpha,\\beta,\\gamma,\\lambda$ in Equation 5. It is unclear how to select these parameters and how robust are the results with respect to the choice of these parameters."
                },
                "questions": {
                    "value": "In the experiments, how did you pick the parameters $\\alpha,\\beta,\\gamma,\\lambda$? How robust are the clustering results with respect to the choice of the parameters?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9264/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698410463501,
            "cdate": 1698410463501,
            "tmdate": 1699637167002,
            "mdate": 1699637167002,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NOOYqK1lGV",
                "forum": "ukmh3mWFf0",
                "replyto": "jxqiUl2JI0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, thank you for taking out the time to read our paper. We are glad to hear that you think our empirical comparisons are comprehensive. We aim to address the concerns raised by you:\n\n### **Weaknesses**\n\n1. `Proposed objective adds a modularity-based regularizer term to FGC, limited originality?, experiments on few selected datasets?` \\\n\tThe modularity term can not be regarded as a regularization term as can be seen from the ablation studies Fig 4b that it \"guides\" the model a majority of the way not to mention the significant increase in performance from baseline methods FGC (**+153 %**) and DMoN (**+40 %**), which shows that our contribution is to **allow potentially any coarsening method to work in a clustering setting**, as there are lots of **theoretical benefits** in common coarsening algorithms that have not been studied in clustering. Moreover, that is why we have included very **comprehensive analyses** surrounding the new objective function, with theoretical guarantees as well. We include a heavy load of supporting proofs ranging from **proofs of convexity, proofs of KKT optimality, proofs of convergence, complexity analysis, ablation studies on the behavior of the different loss terms, and how drastically different it is from FGC, and even recovery on a degree-corrected stochastic block model**. This is not incremental. \\\nAlso, our experiments are not on a few selected datasets but rather range from the **benchmark attributed datasets** used in graph clustering to **non-attributed datasets**, and **even to very large datasets** that most literary works skip out on. For example, CoauthorCS, CoauthorPhysics, AmazonPhoto, AmazonPC, and ogbn-arxiv, even though our work is not specialized for very large datasets. \\\nWe have also extensively worked on **integrating our methods** natively with **various GNN architectures**, which was not done in FGC, and also performing ablation studies, including **contributions of the loss terms** and how they **work together**, the **differences in evolution of latent space** for different models, comparison of runtime including **complexities**, **comparison of modularity** to make it as comprehensive as possible, both empirically and theoretically. \n2. `Difficult to understand what \"a graph cut minimal in the weights of the edges\" means in this context. If the authors mean minimum cut, then I don't think this statement is correct. The relationship between the Fiedler vector and the minimum cut is not mentioned in the original paper (Fiedler, 1973), and in general it does not give rise to a minimum cut.` \\\n\tA graph cut minimal in the weights of the edges means that a graph cut such that the sum of edges it \"cuts\" is minimum over all such possible cuts. This can also be written as the number of edges in the cut. We referenced the Fiedler paper for the Fiedler vector. \\\n\tIt is a well known result that the eigenvector related to the second smallest eigenvalue gives us a partition such that the number of edges being cut is minimal, and this result is part of Newman's landmark 2006 work on modularity, also referenced in other places in our paper. We have added this citation there.\n3. `Authors state\"... they assume each node gets mapped to one cluster, which is not consistent with real graph data.\" This is a clear overstatement. As far as I know, most of the node classification benchmarks used by the GNN community has non-overlapping clusters/classes.` \\\n\tYes, that is what we are saying here as well, except that the real world applications of graphs are far wider that the benchmark citation datasets, for example, social networks, which in general have overlapping clusters/\"groups\". \n4. `Typos/clarifications`\\\n\tThank you for these. We have moved the references and abbreviations earlier. We have gone over the paper with a spellchecker and fixed a few typos. If you find any more, please tell us the location. For clarification:\n\t- Volume of inter-cluster edges means the total number of edges in-between the clusters and not inside them.\n\t- $\\delta(c_i,c_j)$ is the common Kronecker delta which is 1 only when $c_i=c_j$ and 0 otherwise\n\n(Continued in 2/2)"
                    },
                    "title": {
                        "value": "Reply to Reviewer's Comment (1/2)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700432172399,
                "cdate": 1700432172399,
                "tmdate": 1700479067145,
                "mdate": 1700479067145,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xAFKFInOwO",
                "forum": "ukmh3mWFf0",
                "replyto": "jxqiUl2JI0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer's Comment (2/2)"
                    },
                    "comment": {
                        "value": "5. `Authors compared with a number of methods in Table 1 but over 3 small datasets. Table 3 appendix has more results on other datasets, but only compared with two other methods.` \\\n\tThis is because most of the recent papers in graph clustering don't experiment on large graphs and instead just choose the smallest out of these (AmazonPhoto) and show results on that. Some examples are GDCL[IJCAI'21] (None), GALA[ICCV'19] (None), AGE[KDD'20] (None), MVGRL[ICML'20] (None), HSAN[AAAI'23] (AmazonPhoto), SCGC[IEEE TNNLS'23] (AmazonPhoto), DCRN[AAAI'22] (AmazonPhoto). Filling just one column in the table with 5 columns for multiple papers makes it look like the table is incomplete. However, we still provide the data about our method in regards to DMoN and FGC. \\\n\tMost of these papers only include the 6 datasets from our table 1 and table 2 and one additional dataset.\n6. `Lot of parameters `$\\alpha,\\beta,\\gamma,\\lambda$`, unclear how to select, and how robust are the results with changes?` \\\n\tIn the paragraph just above Section 6, we mention the observed sensitivity of hyperparameters (The order of sensitivity is $\\alpha>\\gamma>\\beta>\\lambda$). Selection is done starting from previous values from FGC and then adjusting accordingly for the modularity term (i.e., general hyperparameter optimization, we can use any method such as a grid search or Bayesian optimization, etc.). \n\n**Questions**\n\nAnswered above.\n\nWe hope this has answered your questions and encouraged you to improve your rating. If there are any more, feel free to leave a comment."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477964325,
                "cdate": 1700477964325,
                "tmdate": 1700478965992,
                "mdate": 1700478965992,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7K6IpN4YEC",
                "forum": "ukmh3mWFf0",
                "replyto": "xAFKFInOwO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9264/Reviewer_s7a2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9264/Reviewer_s7a2"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, I have read all reviews and also all replies to the reviews. I will keep the score. Thank you for the efforts for providing detailed responses."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667423103,
                "cdate": 1700667423103,
                "tmdate": 1700667423103,
                "mdate": 1700667423103,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "235nadf9Y3",
            "forum": "ukmh3mWFf0",
            "replyto": "ukmh3mWFf0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9264/Reviewer_fD7q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9264/Reviewer_fD7q"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a novel framework called Q-FGC for attributed graph clustering and its integration with deep learning-based architectures such as Q-GCN, Q-VGAE, and Q-GMM-VGAE. The authors conducted experiments on real-world benchmark datasets and demonstrated that incorporating modularity and graph regularizations into the coarsening framework improves clustering performance. Furthermore, integrating the proposed method with deep learning-based architectures significantly enhances clustering performance. The algorithms proposed in this paper are proven to be convergent and faster than existing state-of-the-art algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The authors proposed a novel optimization-based attributed graph clustering framework called Q-FGC.\n\n2) The proposed algorithms are provably convergent and much faster than state-of-the-art algorithms."
                },
                "weaknesses": {
                    "value": "1) The submitted title in the system (Attributed Graph Clustering via Coarsening with Modularity) is different from the title in the paper (ATTRIBUTED GRAPH CLUSTERING VIA MODULARITY AIDED COARSENING).\n\n2) The research motivation is not sufficiently novel or clearly expressed. Additionally, there is an inconsistency between the motivation presented in the introduction and the abstract. For example, Dirichlet energies are stated in the Abstract, but they are not mentioned in Section Introduction. Besides, the reasons for using them are not explained. Reorganizing the abstract and introduction is recommended, particularly in the section discussing the motivation.\n\n3) The novelty of this paper is not strong. The proposed method for improving the performance of graph clustering relies on modifying the existing FGC method. Furthermore, the paper fails to explain the shortcomings of the current FGC method and how incorporating modularity would enhance its performance. Overall, the impact of this paper on the field is not significant.\n\n4) The related work section is not comprehensive enough. For instance, the paper does not cite important references such as Kumar M, Sharma A, Saxena S, et al. \"Featured Graph Coarsening with Similarity Guarantees\" presented at ICML 2023.\n\n5) The experimental section lacks sufficient detail in its description. For example, the experimental setup is missing information. More specifically, the authors did not provide detailed experimental settings for the baselines. Additionally, the experimental section merely presents the experimental results without providing an explanation for the superior performance of the proposed algorithm in this paper.\n\n6) The writing of the paper needs to be improved. There are also some typos in this paper.\n- The algorithm (Feature Graph Coarsening (FGC)) is first given, but no references are given.\n- There is a lack of punctuation in many parts of the paper. For example, \u201cWe compare the performance of our method against three types of existing state-of-the-art methods based on the provided input and type of architecture: a) methods that use only the node attributes b) methods that only use graph-structure c) methods that use both graph-structure and node attributes. The last category can be further subdivided into three sets: i) graph coarsening methods ii) GCN-based architecures iii) VGAE-based architectures and contrastive methods iv) largely modified VGAE architectures\u201d should be \u201cWe compare the performance of our method against three types of existing state-of-the-art methods based on the provided input and type of architecture: a) methods that use only the node attributes; b) methods that only use graph-structure; c) methods that use both graph-structure and node attributes. The last category can be further subdivided into three sets: i) graph coarsening methods; ii) GCN-based architectures; iii) VGAE-based architectures and contrastive methods; iv) largely modified VGAE architectures.\u201d). \n- Several algorithms in Table 1 are missing references, and some do not provide experimental results. \n- Figure 2 and Figure 4 do not have a caption (it is suggested to separate the tables in Figure 2(a) and Figure 4(a) from the figure itself).\n- Equation 8 and Eqn. 8 => Eq. (8)\n- Table 2a  => Table 2(a)\n- In Section 5.2, the authors state that \u201cQ-GCN is composed of 3 GCN layers\u201d, but only two hidden sizes of 128 and 64 are provided.\n- In section 5.2: \u201cWe surpass all existing methods\u2026\u201d  => \u201cOur proposed model surpasses all existing methods...\u201d\n- The font size of the x-axis values in Figure 4(b) is too small.\n- In section 5.1, \u201cGCN-based architecures\u201d   => \u201cGCN-based architectures\u201d"
                },
                "questions": {
                    "value": "1) In the Motivation of the introduction section, when stating that \"We aim to utilize the feature graph coarsening framework (which does not perform well on clustering, as seen in the results) for graph clustering.\", does the phrase \"the results\" refer to the experimental results in the experimental section? It is recommended to provide specific details on which results are being referred. Also, why does the feature graph coarsening framework not perform well on clustering?\n\n2) Can more analysis be done in the experimental section on experiment settings? For example, could you please provide the experimental settings for the baselines and the hyperparameter settings for the proposed method, including learning rate, number of training iterations, dataset partitioning, and so on?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9264/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698457506356,
            "cdate": 1698457506356,
            "tmdate": 1699637166881,
            "mdate": 1699637166881,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sXQ0LYA902",
                "forum": "ukmh3mWFf0",
                "replyto": "235nadf9Y3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer's Comment (1/2)"
                    },
                    "comment": {
                        "value": "Dear reviewer, we thank you for taking out the time to read our paper and share your valuable insights. We will try to address all your concerns.\nWe found that your comment was generated using AI on gptzero; we still hope that you have read the paper and just gave it pointers you got from reading to frame the answer.\n\n### **Weaknesses:**\n\n1. `The submitted title in the system (Attributed Graph Clustering via Coarsening with Modularity) is different from the title in the paper (Attributed Graph Clustering via Modularity Aided Coarsening).` \\\n\tIt looks like this was an error.\n2. `The research motivation is not sufficiently novel or clearly expressed. Additionally, there is an inconsistency between the motivation presented in the introduction and the abstract. For example, Dirichlet energies are stated in the Abstract, but they are not mentioned in Section Introduction. Besides, the reasons for using them are not explained. Reorganizing the abstract and introduction is recommended, particularly in the section discussing the motivation.` \\\n\tFor novelty concerns, refer to next answer. We do mention Dirichlet energy and all the other terms right under the motivations paragraph inside Introduction section. Also, we have explained the purpose of each term in detail in Section 4.1. We have tried to reorganize a bit.\n3. `The novelty of this paper is not strong. The proposed method for improving the performance of graph clustering relies on modifying the existing FGC method. Furthermore, the paper fails to explain the shortcomings of the current FGC method and how incorporating modularity would enhance its performance. Overall, the impact of this paper on the field is not significant.` \\\n\tIn the first paragraph of Section 4.1, we have written that the FGC method is not able to perform on clustering, because the coarsening ratio is too low in clustering tasks. We have multiple paragraphs throughout the paper showcasing the benefits of modularity, starting from the Section 1 Introduction (3rd paragraph) to Section 3.3 in Background to the whole of Section 4 Proposed Method. Basically, this is what our whole paper is about. \\\n\tThe impact of the paper on the field is significant because:\n\t- There is a significant increase in performance from baseline methods FGC (**+153 %**) and DMoN (**+40 %**), which shows that our contribution is to **allow potentially any coarsening method to work in a clustering setting**, as there are lots of **theoretical benefits** in common coarsening algorithms that have not been studied in clustering. Moreover, that is why we have included very **comprehensive analyses** surrounding the new objective function, with theoretical guarantees as well. We include a heavy load of supporting proofs ranging from **proofs of convexity, proofs of KKT optimality, proofs of convergence, complexity analysis, ablation studies on the behavior of the different loss terms, and how drastically different it is from FGC, and even recovery on a degree-corrected stochastic block model**. This is not incremental. \\\n\t- Also, our experiments are not on a few selected datasets but rather range from the **benchmark attributed datasets** used in graph clustering, to **non-attributed datasets**, and **even to very large datasets** that most literary works skip out on. For example, CoauthorCS, CoauthorPhysics, AmazonPhoto, AmazonPC and ogbn-arxiv, even though our work is not specialized for very large datasets. \\\n\t- We have also extensively worked on **integrating our methods** natively with **various GNN architectures**, which was not done in FGC and also performing ablation studies including **contributions of the loss terms** and how they **work together**, the **differences in evolution of latent space** for different models, comparison of runtime including **complexities**, **comparison of modularity** to make it as comprehensive as possible, both empirically and theoretically. \n4. `The related work section is not comprehensive enough. For instance, the paper does not cite important references such as Kumar M, Sharma A, Saxena S, et al. \"Featured Graph Coarsening with Similarity Guarantees\" presented at ICML 2023.` \\\n\tThe related work section includes papers on Graph Coarsening/Pooling, Modularity Optimization and Deep Graph Clustering. The work you have referenced is itself an extension of FGC (Featured Graph Coarsening), JMLR'23, by the same authors, which our method inherits from and references multiple times.\n\n(continued in 2/2)"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481188984,
                "cdate": 1700481188984,
                "tmdate": 1700482978898,
                "mdate": 1700482978898,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YnB7wmCddf",
                "forum": "ukmh3mWFf0",
                "replyto": "235nadf9Y3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer's Comment (2/2)"
                    },
                    "comment": {
                        "value": "5. `The experimental section lacks sufficient detail in its description. For example, the experimental setup is missing information. More specifically, the authors did not provide detailed experimental settings for the baselines. Additionally, the experimental section merely presents the experimental results without providing an explanation for the superior performance of the proposed algorithm in this paper.`\\\n\tWe refer the reviewer to the paragraph after Section 5.2, which is about Training Details and our experimental setup is explained in detail there (Also refer to answer of Question#2). We explain the performance benefits of modularity all throughout the paper, as written in the answer to #3. We also analyze our results and methods in Section 5.5 Ablation Studies.\n6. `The writing of the paper needs to be improved. There are also some typos in this paper.`\n    - `The algorithm (Feature Graph Coarsening (FGC)) is first given, but no references are given.`\n    - `There is a lack of punctuation in many parts of the paper.`\n    - `Several algorithms in Table 1 are missing references, and some do not provide experimental results.`\n    - `Figure 2 and Figure 4 do not have a caption (it is suggested to separate the tables in Figure 2(a) and Figure 4(a) from the figure itself).`\n    - `Equation 8 and Eqn. 8 => Eq. (8)`\n    - `Table 2a => Table 2(a)`\n    - `In Section 5.2, the authors state that \u201cQ-GCN is composed of 3 GCN layers\u201d, but only two hidden sizes of 128 and 64 are provided.`\n    - `In section 5.2: \u201cWe surpass all existing methods\u2026\u201d => \u201cOur proposed model surpasses all existing methods...\u201d`\n    - `The font size of the x-axis values in Figure 4(b) is too small.`\n    - `In section 5.1, \u201cGCN-based architecures\u201d => \u201cGCN-based architectures\u201d` \\\n\tThank you for all these, we have moved the references and abbreviations first and fixed typos. And we have added semicolons in the paragraph you requested. Also, we have added references in Table 1. All of them already have experimental results, so we are not sure what you mean by \"do not provide experimental results\". \\\n\tFigure 2 and 4 are divided into 2a), 2b) and 4a),4b), all four of which have captions. We have grouped them because the table and graphs are small and would not fit in the space constraints otherwise. \\\n\tWe use the latex command `\\ref` to refer to tables, figures and equations and that generates the output 4a and not 4(a). We are following the ICLR custom latex style and format provided on the conference website. This is important to keep references linked (i.e. they will take you to the related figure/table/eqn when clicked on). \\\n\tAs visible from the architecture, Q-GCN has 3 GCN layers. And just like any other GCN or even MLP model, the dimensions change as: `input_size`--(GCN1)-->`hidden_size 1`--(GCN2)-->`hidden_size 2`--(GCN3)-->`output_size` \\\n\tFor the x-axis in 4b: We tried to improve this before submission but it is rendered as latex text within the generated plot pdf (using the plotly package), and still somehow increasing the size was making it being rendered as blurry. If you zoom in it is better. We have tried to make this better by changing the latex to `$\\large{ ... }$`\n\n### **Questions:**\n\n1. `In the Motivation of the introduction section, when stating that \"We aim to utilize the feature graph coarsening framework (which does not perform well on clustering, as seen in the results) for graph clustering.\", does the phrase \"the results\" refer to the experimental results in the experimental section? It is recommended to provide specific details on which results are being referred. Also, why does the feature graph coarsening framework not perform well on clustering?` \\\n\tYes, we are talking about the primary results of the paper. We mention in Section 4.1 that FGC alone does not perform well in clustering because the coarsening ratio in clustering (~ < 0.001) is much lower than the ones it is designed for (0.1 - 0.01).\n2. `Can more analysis be done in the experimental section on experiment settings? For example, could you please provide the experimental settings for the baselines and the hyperparameter settings for the proposed method, including learning rate, number of training iterations, dataset partitioning, and so on?` \\\n\tWe have included the information about values of hyperparameters as part of code since those values cannot be analyzed. `We can add this to the supplementary material.`\n\n\nWe hope these answers gave you clarity about the method and answered your questions. We also hope you would consider raising our rating. We would be happy to answer or discuss anything more."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481279811,
                "cdate": 1700481279811,
                "tmdate": 1700481279811,
                "mdate": 1700481279811,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T4udePcu7p",
                "forum": "ukmh3mWFf0",
                "replyto": "YnB7wmCddf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9264/Reviewer_fD7q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9264/Reviewer_fD7q"
                ],
                "content": {
                    "comment": {
                        "value": "Actually, I spent a lot of time reading this paper carefully. However, I still found it difficult to understand, and some reviewers also took this view. Therefore, I believe that the writing of papers needs to be improved. \n\nIn addition, the authors have not addressed several concerns. Here are some examples.\n- I observed that the author did not provide detailed experimental settings for the **baselines**. The author's response focused on the experimental settings of their proposed method rather than the baselines of this paper. \n- The authors responded that \u201cAll of them already have experimental results\u201d in Table 1. However, the ARI of the VGAECD-OPT algorithm on the PubMed dataset is not given, and the authors did not explain it. \n- The authors responded that they referenced a study related to FGC multiple times. They did do that in Sections **Background**, **Proposed Method** and **Experiments**. However, I mentioned that it should be listed in Section **Related Works** because analyzing this study can help readers better understand the contributions of the proposed Q-FGC. \n\nAll things considered, I am sorry to inform the author that I maintain the original score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622140407,
                "cdate": 1700622140407,
                "tmdate": 1700622140407,
                "mdate": 1700622140407,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1pC3WpnnPs",
            "forum": "ukmh3mWFf0",
            "replyto": "ukmh3mWFf0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9264/Reviewer_s2b9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9264/Reviewer_s2b9"
            ],
            "content": {
                "summary": {
                    "value": "This work proposed a node clustering model based on modularity maximization for attributed graphs. The clustering process is modeled as an optimization-based graph coarsening problem, and the final pseudo labels are retrieved from supernode relationships. However, despite the progress made in this work, I cannot recommend acceptance for it to be present at top-tier conferences such as ICLR. See my comments below for details."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The literature review part is pretty detailed and comprehensive.\n* The paper is well-organized and easy to follow.\n* The proposed method is flexible and can be combined with various representation learning backbones.\n* Promising clustering performances are obtained on widely used graph datasets."
                },
                "weaknesses": {
                    "value": "* The proposed clustering model (problem (5)) is a trivial combination of different previous works, none of (5) is designed by the authors, so the technical contribution of this work is marginal.\n* The experiments are not inspiring. The authors conducted different experiments but presented their results without analyzing the reasons behind the scenes. See \"Questions\" below for a few ones I raised.\n* The ablation studies part is trivial and not informative at all.\n    * Visualization and Comparison of running times are generally not regarded as ablation studies.\n    * Modularity Metric Comparison is interesting but its conclusion is pretty trivial:\n> Even though modularity is a good metric to optimize for,\nmaximum modularity labelling of a graph does not always correspond to the ground truth labelling.\nFor this reason, it is important to have the other terms in our formulation as well.\n\n      This is a common sense known as the \"no free lunch theorem\" in machine learning. We generally would like the ablation studies to uncover special and important characteristics of the proposed method, rather than trivial observations."
                },
                "questions": {
                    "value": "* In problem (5), why do you propose to optimize both $\\tilde{X}$ and $C$ and use $\\lVert C\\tilde{X}-X\\rVert_F^2$ to encourage the consistency, rather than optimizing $C$ only as K-means does?\n* Keep the last question in mind, why do you optimize $C$ only in Section 4.2? That makes the experiments inconsistent with your proposal. In addition, what's the difference between the two strategies in terms of clustering performance?\n* In Figure 2(b), what makes the proposed Q-GMM-VGAE faster than its backbone model GMM-VGAE? Are the experimental settings fair?\n* In Table 1, SCGC and MVGRL have better performance but you marked the proposed method bold, why? Is it a typo?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9264/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9264/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9264/Reviewer_s2b9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9264/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698508083416,
            "cdate": 1698508083416,
            "tmdate": 1699637166779,
            "mdate": 1699637166779,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9nZw9ozHmj",
                "forum": "ukmh3mWFf0",
                "replyto": "1pC3WpnnPs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer's comments (1/2)"
                    },
                    "comment": {
                        "value": "Dear reviewer, thank you for your valuable insights and concerns. We are glad to hear that you found the paper well-organized, flexible, and promising. We will try to address the concerns here:\n\n### **Weaknesses**\n\n1. `Trivial combination of previous works, technical contribution is marginal?` \\\nWe note that the modularity term has existed in literature since [Newman, 2006]. However, our work here was to recognize what was lacking in FGC for making it level with the current state of the art in clustering. We worked on proving theoretical guarantees, for example, the Lipschitz continuity of the spectral approximation to the modularity term. There is a significant increase in performance from baseline methods FGC (**+153 %**) and DMoN (**+40 %**), which shows that our contribution is to **allow potentially any coarsening method to work in a clustering setting**, as there are lots of **theoretical benefits** in common coarsening algorithms that have not been studied in clustering. Moreover, that is why we have included very **comprehensive analyses** surrounding the new objective function, with theoretical guarantees as well. We include a heavy load of supporting proofs ranging from **proofs of convexity, proofs of KKT optimality, proofs of convergence, complexity analysis, ablation studies on the behavior of the different loss terms, and how drastically different it is from FGC, and even recovery on a degree-corrected stochastic block model**. This is not incremental. \\\n\tAlso, our experiments are not on a few selected datasets but rather range from the **benchmark attributed datasets** used in graph clustering, to **non-attributed datasets**, and **even to very large datasets** that most literary works skip out on. For example, CoauthorCS, CoauthorPhysics, AmazonPhoto, AmazonPC and ogbn-arxiv, even though our work is not specialized for very large datasets. \\\n\tWe have also extensively worked on **integrating our methods** natively with **various GNN architectures**, which was not done in FGC and also performing ablation studies including **contributions of the loss terms** and how they **work together**, the **differences in evolution of latent space** for different models, comparison of runtime including **complexities**, **comparison of modularity** to make it as comprehensive as possible, both empirically and theoretically. \n2. `Experiments -> \"Questions\" below.`\nIn Questions.\n3. `The ablation studies part is trivial and not informative at all.`\n\t- `Visualization and Comparison of running times are generally not regarded as ablation studies.` \\\n\tWe refer the reviewer to papers DCRN[AAAI'22], HSAN[AAAI'23], SDCN[WWW'20], AGE[KDD'20], SCGC[IEEE TNNLS'23]. Going by these papers in highly regarded graph clustering literature, ablation studies and analyses are usually combined into one, and analyses do include visualization analysis and complexity analysis in all these papers. Note that some papers just have an ablation study on parameters. That is why we have included **both** kinds of ablation studies.\n\t- `Modularity Metric Comparison is interesting but its conclusion is pretty trivial:` \\\n\tWe wanted to show that our modularity maximization term does contribute to the performance and actually increases the modularity too, since that is a crucial part of our method.\n\t- `No free lunch theorem, Ablation studies for special characteristics, rather than trivial observations` \\\n\tThe reviewer might think this is a trivial and/or direct conclusion from the formulation because of their experience, however, it is very important to explain the downfalls of modularity in the method, _as pointed out by other reviewers_. For a more *flavorful* observation, observe how in Fig 4b), $\\alpha$ + $\\beta$ or $\\beta$ + $\\lambda$ are worse as compared to $\\beta$ alone - we hypothesize this is because these terms want to optimize the model in different directions in the high-dimensional parameter space - either all 3 are in different directions or $\\alpha$ and $\\lambda$ are in the same direction different to $\\beta$. Only when all three are together, these effects combine and push the model in a shared direction (which can be noted by seeing the increase in performance of $\\alpha + \\beta + \\lambda$ by around 23%(Cora)/50%(CiteSeer) over just $\\beta$). This statement comes from intuition when we think theoretically about the differing outcomes if these terms were optimized independently. \\\n\tMoreover, observe how the plots of latent space evolution pan out so differently by just changing a GCN layer to a GMM, while the whole VGAE architecture including the losses remains the same.\n\n(continued in 2/2)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475454324,
                "cdate": 1700475454324,
                "tmdate": 1700478912599,
                "mdate": 1700478912599,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l2wRu6Woko",
                "forum": "ukmh3mWFf0",
                "replyto": "1pC3WpnnPs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer's comments (2/2)"
                    },
                    "comment": {
                        "value": "### **Questions:**\n\n1. `Why optimize both `$C$,$\\tilde{X}$` with `$||C\\tilde{X} - X||_{2}^2$` rather than optimizing `$C$` only as K-means does?` \\\n\tThe method is based on graph coarsening, and we typically want to find the output features $\\tilde{X}$ in coarsening and not just $C$/loading matrix.\n2. `Why do you optimize `$C$` only in Section 4.2? Are experiments inconsistent?. What's the difference between the two in terms of clustering performance?` \\\n\tWe optimize only for $C$ for the reasons mentioned in the 2nd paragraph of section 4.2. No, this does not make the experiments inconsistent as Q-FGC (optimization-based method) still optimizes for both $C$ and $\\tilde{X}$. We change this in the GNN architectures because $\\tilde{X}$ can't be learnt directly from $X$ using gradient descent and would require manual gradient calculations in PyTorch or to use a separate optimizer just for $\\tilde{X}$. \n\tWe appreciate the author's suggestion and this could be a good point for a future work.\n3. `In Figure 2(b), why is Q-GMM-VGAE faster backbone GMM-VGAE? Are the experimental settings fair?` \\\n\tYes, all the experimental settings were the same, which is essential to any analysis. We modified the implementation of GMM-VGAE to suit our needs and optimized some operations to be vector operations - this is also available in the anonymous code repository. We also made some important normalizations in the implementation to **greatly improve numerical stability** of the GMM (in the original implementation, a lot of the times $C$ would come out to be all zeros because of an unnormalized operation in exponentiating the log probabilities).\n4. `In Table 1, SCGC and MVGRL have better performance but you marked the proposed method bold, why? Is it a typo?` \\\n\tYes, they should be highlighted in ACC and ARI fields for cora and citeseer. We have fixed this.\n\nWe hope these answer your concerns and make you consider raising your rating, and we are here for any more questions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475505734,
                "cdate": 1700475505734,
                "tmdate": 1700478923608,
                "mdate": 1700478923608,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xZNnP4Ocuu",
            "forum": "ukmh3mWFf0",
            "replyto": "ukmh3mWFf0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9264/Reviewer_LHRF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9264/Reviewer_LHRF"
            ],
            "content": {
                "summary": {
                    "value": "The article develops a framework for unsupervised learning relying on modularity maximization jointly with attributed graph coarsening to solve a task of clustering of graph.\nThe main points are 1) to propose that in an optimization-based approach, where a Block Majorization-Minimization algorithm allows to solve the problem. Then, 2) the method is also integrated in GNN architectures for graph clustering. \nThe work describes several aspects of related works, and conducts extensive numerical experiments to check the usefulness of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea of using modularity for graph coarsening is not novel (it dates back to 20 years),  yet its incorporation in on coarsening techniques, in an integrated way, appears to be novel and interestong.\n\n- The article contains extensive numerical experiments, assessing when the proposed method works well. \n\n- There are good theoretical results on the method in Section 4. \n\n- Showing that the method integrates with GNNs is relevant and useful (even though it's, on my opinion), a little bit too detailed."
                },
                "weaknesses": {
                    "value": "- The results are a little bit disappointing as, according to Fig 4(b) and the final results, the full loss of eq (6) is not really needed. The modularity term does already a good job by itself, and the others appear to merely modify the results slightly -- even in a weird way as using only 1 term (relaxation of the constraint  or the $\\ell_{1,2}$ norm regularizer) degrades the performance.\n\n- The article is written is a dense way, possibly too dense, and one has trouble to identify the saillant points. \nI am not sure that the description of the integration of the method in 3 different deep learning methods for graphs is needed in the main text. The most relevant would be enough and it would leave more space to answer the questions asked underneath.\n\n- the literature on modularity maximization does not appear to be well quoted. In this context, quoting 1 or 2 of the existing surveys would be expected and useful for the readers. Also there is a body of literature showing the limitations of the modularity, from its intrinsic resolution limit to it being considered 'harmful', and the present article does not say a word on that and on the impact of the weaknesses of the modularity to the present work."
                },
                "questions": {
                    "value": "- In Fig 4(b) : why are the cases \\alpha + \\beta or \\beta + \\lambda so worse as compared to \\beta alone ?\n\n- There is always the possibility that the structure, E, are not aligned with the features, X. What would then happen ? The methods forces the smoothness of X on (V,E); is it always the case ? If it's not, is this supposition detrimental ? \n\n- What would happen if the clusters happen to be affected by the mentioned limits of the modularity ? \n\n\n- On the other side, modularity has been improved in the mast 10 years using the Non-backtracking random walks, then the Bethe-Hessian ansatz and several variations around that, to detect better clusters or modules. Could these improvements"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "none"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9264/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792446930,
            "cdate": 1698792446930,
            "tmdate": 1699637166649,
            "mdate": 1699637166649,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "89s9lccbm9",
                "forum": "ukmh3mWFf0",
                "replyto": "xZNnP4Ocuu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, we are glad you took the time to read our paper thoroughly. You have correctly understood the novelty of this paper.\n\nWe would like to address your concerns here:\n\n### **Weaknesses**\n\n1. `Acc Fig 4(b), full loss not needed. The modularity term does a good job by itself, and the others merely modify slightly -- even in a weird way as using only 1 term (relaxation of the constraint or the norm regularizer) degrades the performance.` \\\n\tIt is important to note that even though some of the terms do the heavy lifting, the other regularization terms do contribute to performance and more importantly, change the nature of $C$ : The term $\\omega$ corresponds to the smoothness of signals in the graph being transferred to the coarsened graph; you can imagine this would affect $C$ by encouraging local \"patches\"/groups to belong to the same cluster. The term $\\gamma$ ensures that the coarsened graph is connected - i.e. preserving inter-cluster relations, which simple contrastive methods destroy; this affects $C$ by making it so that $\\Theta_C$ has minimal multiplicity of 0-eigenvalues (which tells us how many connected components there are, and we want just 1 big connected component). Also refer to answer of Question 1. \\\n\tWe feel this is a valuable addition to our paper and will include this explanation there too.\n2. `Article written in a dense way, trouble in identifying the salient points. Description of the GNN integration not needed in the main text. The most relevant would be enough and it would leave more space to answer the questions asked underneath` \\\n\tWe apologize that you feel this way, we were trying to fit a lot of theory *and* experiments/results in our main text since we felt all of it was important to the paper. However, because of the strict page limit it might feel dense. We have additionally provided theoretical results and more visualizations in the supplementary material. Regarding subsection \"4.2 Integration with GNNs\", we felt it was important to add an architecture in the maintext since we are comparing our method majorly with other deep learning methods. However, we have tried to reduce it in size.\n3. `Literature on modularity maximization not well quoted. There is a body of literature showing limitations of modularity, article does not mention the weaknesses of modularity.` \\\nWe mention some excerpts here from the paper that do highlight the weakness of modularity, and why it is essential to have other optimization terms:\n    > The usage of these (modularity-maximization) algorithms has plummeted over the years because they rely solely on the topological information of graphs and ignore node features. \n\n    > Even though modularity is a good metric to optimize for, maximum modularity labelling of a graph does not always correspond to the ground truth labelling. .... By optimizing modularity, we can get close to the optimal model parameters (at which NMI would be 1), but will be slightly off-course. We can think of the other terms as correcting this trajectory\n\n    We have also added a mention of the resolution limit of modularity in the introduction and its weakness to small clusters.\n\n(continued in 2/2)"
                    },
                    "title": {
                        "value": "Reply to Reviewer's Comment (1/2)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434199169,
                "cdate": 1700434199169,
                "tmdate": 1700479040753,
                "mdate": 1700479040753,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gNnqUfWfoP",
                "forum": "ukmh3mWFf0",
                "replyto": "xZNnP4Ocuu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer's Comment (2/2)"
                    },
                    "comment": {
                        "value": "### **Questions**\n\n1. `In Fig 4(b) : why are the cases \\alpha + \\beta or \\beta + \\lambda so worse as compared to \\beta alone ?` \\\n\tThis is the main takeaway from the plot; we hypothesize this is because these terms want to optimize the model in different directions in the high-dimensional parameter space - either all 3 are in different directions or $\\alpha$ and $\\lambda$ are in the same direction different to $\\beta$. Only when all three are together these effects combine and push the model in a shared direction (which can be noted by seeing the increase in performance of $\\alpha + \\beta + \\lambda$ by around 23%(Cora)/50%(CiteSeer) over just $\\beta$). This statement comes from intuition when we think theoretically about the differing outcomes if these terms were optimized independently.\n2. `There is always the possibility that the structure, E, are not aligned with the features, X. What would then happen ? The methods forces the smoothness of X on (V,E); is it always the case ? If it's not, is this supposition detrimental ?` \\\n\tNo, this is certainly not always the case - however, it is observed in most graph clustering and coarsening benchmark datasets (including heterogeneous). We believe it would be detrimental if that was the case, but since the smoothness term does not have a big impact on performance, it should not be disastrous. Upon more research, we have found this paper and In it's section 4.2, it shows some datasets with their calculated smoothness values: https://openreview.net/pdf?id=rkeIIkHKvS; \n3. `What would happen if the clusters happen to be affected by the mentioned limits of the modularity ?` \\\n\tEven if the clusters were small enough to be hit by the resolution limit of modularity, it would still detect that cluster but probably have extra nodes from other clusters because of the regularization terms that enforce connectedness and the balanced $\\mathcal{l}_{1,2}$ norm which ensures each cluster has at least 1 node.\n4. `On the other side, modularity has been improved in the mast 10 years using the Non-backtracking random walks, then the Bethe-Hessian ansatz and several variations around that, to detect better clusters or modules. Could these improvements` \\\n\tIt looks like the question was cut off on openreview, however, we will try to answer it: The Bethe-Hessian definitely has interesting spectral properties and the \"deformed Laplacian\" view seems like a good concept too. Yes, it looks like these improvements could be integrated into our method as part of a future work. \n\nWe hope these answer your queries, we are here to answer any more/follow-ups.\n\nThank you for your insightful contributions!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478895866,
                "cdate": 1700478895866,
                "tmdate": 1700479053198,
                "mdate": 1700479053198,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aVj4P784t9",
                "forum": "ukmh3mWFf0",
                "replyto": "xZNnP4Ocuu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9264/Reviewer_LHRF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9264/Reviewer_LHRF"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nI appreciate the detailed answers and changes made. In light of the different reviews, I will not change my rating, still thinking that the contribution is borderline and that there are aspects that remain yet to be improved and clarified in the present submission."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671053516,
                "cdate": 1700671053516,
                "tmdate": 1700671053516,
                "mdate": 1700671053516,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v0DQhLPO5n",
            "forum": "ukmh3mWFf0",
            "replyto": "ukmh3mWFf0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9264/Reviewer_iqpP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9264/Reviewer_iqpP"
            ],
            "content": {
                "summary": {
                    "value": "A new graph clustering method is proposed that combines graph coarsening with a modularity regularization term. They show how the objective can be optimized within a GNN-based architecture. They show experiments that show that the method performs well on several datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper combines many machine learning techniques.\n\nThe resulting method seems to outperform existing methods in the experiments."
                },
                "weaknesses": {
                    "value": "I don't know whether this paper is intended to be read by people from the field of community detection, but I can confirm that this paper is very hard to understand for someone with that background.\n\nThe introduction gives a bad overview of community detection. It does not refer to a good reference for modularity [1]. And it mentions that \"In theory, a higher value of modularity is associated with better quality clusters.\", which is a puzzling statement because modularity is a heuristic that does not have a strong theoretical underpinning. In addition, the paper mentions that the usage of modularity maximization has \"plummeted over the years because they rely solely on the topological information\", which I don't think is the case. Modularity maximization is still one of the most widely-used community detection methods, despite theoretical shortcomings [2,3]. Finally, it is mentioned that modularity maximization \"requires intensive computations\", but the Louvain algorithm runs in nearly linear time, and you can't really go faster than that.\n\nThe \"Deep Graph Clustering\" paragraph of the introduction is incredibly difficult to understand. It uses a lot abbreviations like ARGA, ARVGA, DAEGC, SDCN that are not (properly) introduced. After reading it, I still have no idea what is meant with \"Deep graph clustering\".\n\nThe NMI measure is biased towards fine-grained clusterings [4], while ARI also has its disadvantages [5]. I would recommend to use AMI and/or the correlation coefficient to measure the similarity between clusterings [5].\n\nThe paper contains many typo's, language and notation errors. \n\nThey refer to Supplementary D for a summary of the datasets, but Supplementary D does not describe datasets at all.\n\n[1] Newman, M. E., & Girvan, M. (2004). Finding and evaluating community structure in networks. Physical review E, 69(2), 026113.\n[2] Fortunato, S., & Barthelemy, M. (2007). Resolution limit in community detection. Proceedings of the national academy of sciences, 104(1), 36-41.\n[3] Peixoto, T. P. (2023). Descriptive vs. inferential community detection in networks: Pitfalls, myths and half-truths. Cambridge University Press.\n[4] Vinh, N. X., Epps, J., & Bailey, J. (2009, June). Information theoretic measures for clusterings comparison: is a correction for chance necessary?. In Proceedings of the 26th annual international conference on machine learning (pp. 1073-1080).\n[5] G\u00f6sgens, M. M., Tikhonov, A., & Prokhorenkova, L. (2021, July). Systematic analysis of cluster similarity indices: How to validate validation measures. In International Conference on Machine Learning (pp. 3799-3808). PMLR."
                },
                "questions": {
                    "value": "What is the difference between graph clustering, community detection and graph coarsening? The way I understand it, community detection is merely clustering of graph nodes based on the graph topology. Graph coarsening (as described in this paper) seems to be similar to blockmodeling [1]. At any rate, the differences between these three things (that seem to be combined in this paper), need to be clearly explained.\n\nThe method makes use of the constraint $X=C\\tilde{X}$. If we substitute this constraint into the first term of (5), it would simplify to\n$\\text{tr}(\\tilde{X}^\\top C^\\top\\Theta C\\tilde{X})=\\text{tr}(X^\\top\\Theta X)$, which would simplify the optimization significantly. However, instead of enforcing this constraint exactly, the paper simply introduces an error term $\\|X-C\\tilde{X}\\|$, which seems unelegant to me. Why don't you enforce this constraint exactly?\n\nYou mention that the log determinant term can be written as the sum of the log of the eigenvalues, and that this ensures that a 'minimal' number of eigenvalues are zero. However, doesn't this ensure that *not a single* eigenvalue is zero?\n\nIs the complexity that is described in the \"Complexity analysis\" paragraph the complexity of a single iteration or of all the iterations until convergence?\n\nI see that you rescaled the NMI, ARI and modularity to percentages. This is okay for NMI (though I'm not a fan of it), but for ARI it is confusing because ARI can be negative. For modularity, I have no idea how this rescaling is done, because the upper bound of modularity is smaller than 1 (and incredibly expensive to compute).\n\nWhy do you draw lines in Figure 2b instead of making a table? The points that are connected don't correspond to consecutive things.\n\n[1] Peixoto, T. P. (2019). Bayesian stochastic blockmodeling. Advances in network clustering and blockmodeling, 289-332."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9264/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9264/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9264/Reviewer_iqpP"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9264/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833797228,
            "cdate": 1698833797228,
            "tmdate": 1699637166513,
            "mdate": 1699637166513,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QUXLqcawE7",
                "forum": "ukmh3mWFf0",
                "replyto": "v0DQhLPO5n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their appreciation and concern for our paper. We will do our best to resolve them.\n\n### **Weaknesses**\n\n1. `Bad overview of community detection. It mentions \"In theory, a higher value of modularity is associated with better quality clusters.\", which is puzzling because modularity does not have a theoretical underpinning. Paper mentions that usage of modularity maximization has \"plummeted over the years because they rely solely on the topological information\", which I don't think is the case, is still one of the most widely-used community detection methods, despite theoretical shortcomings. Finally, it is mentioned that modularity maximization \"requires intensive computations\", but the Louvain algorithm runs in nearly linear time.` \\\n\tWe have added a reference to (Newman, 2006) in the introduction. We refer the reviewer to the same (Newman, 2006) literary work introducing modularity, in which he states (quoted directly from Section 3 just after Eqn 17 referring to https://arxiv.org/pdf/physics/0605087.pdf)\n    > This benefit function is called modularity. It is a function of the particular division of the network into groups, with larger values indicating stronger community structure. Hence we should, in principle, be able to find good divisions of a network into communities by optimizing the modularity over possible divisions.\n\n    We have added another mention on the drawbacks of modularity as requested. \\\n    Modularity maximization methods such as the Louvain and Leiden algorithms have been usually superseded by GNN based which take important node features into account, especially in social network analysis. We are not saying that they are anywhere near obsolete - just that for a wide range of graph applications, node features are very important. \\\n    We were talking about the modularity maximization algorithms given by Girvan and Newman here which took $O(n^3)$ time, and not about Louvain/Leiden algorithms. There is no known complexity of the Louvain algorithm, with some papers claiming it to be $\\mathcal{O}(n \\log n)$, while some claim it to be $\\mathcal{O}(e)$, although only on sparse graphs. We have clarified this in the paper.\n2. `\"Deep Graph Clustering\" paragraph is difficult to understand. Uses abbreviations like ARGA, ARVGA, DAEGC, SDCN that are not (properly) introduced.` \\\n\tThose are the abbreviations used by existing literature and that is why references have been provided for all of the mentioned methods. Unfortunately, It is not feasible to explain each method there as that itself would take a major portion of the strict 9-page space constraint, leaving no space for theory, method/optimization/architecture, results and ablation studies.\n3. `The NMI measure is biased towards fine-grained clusterings, while ARI also has its disadvantages. I would recommend to use AMI and/or the correlation coefficient to measure the similarity between clusterings` \\\n\tWe did consider this in the initial phase. However, the papers we compare our work with all take the primary metric as NMI. It is also given in the reference you mentioned [5] that AMI as well has its drawbacks, such as not following the distance property and not having linear complexity, whereas NMI does (and we perform experiments on large graphs as well, so complexity is a concern for us). However, we will look into the Correlation Distance measure for future works since it satisfies the properties we need. \\\n\t**Thank you for this contribution!**\n4. `Supplementary D does not describe datasets; typos.` \\\n\tSupplementary Material D does describe datasets. We have added a missing reference to it's Table 2 since it got moved to the next page, which contains the dataset summaries. As for typos, we have tried to fix the ones we could find.\n\n(Continued in 2/2)"
                    },
                    "title": {
                        "value": "Reply to Reviewer's Comment (1/2)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467652524,
                "cdate": 1700467652524,
                "tmdate": 1700479831403,
                "mdate": 1700479831403,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Uo0kSw9uVP",
                "forum": "ukmh3mWFf0",
                "replyto": "v0DQhLPO5n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9264/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer's Comment (2/2)"
                    },
                    "comment": {
                        "value": "### **Questions**\n\n1. `What is the difference between graph clustering, community detection and graph coarsening? The way I understand it, community detection is merely clustering of graph nodes based on the graph topology. Graph coarsening (as described in this paper) seems to be similar to blockmodeling [1]. At any rate, the differences between these three things (that seem to be combined in this paper), need to be clearly explained.` \\\n\tAll three terms are broadly similar. Community detection is a general term for clustering. Coarsening, on the other hand, aims to combine a few nodes together (usually neighboring) while not making large groups. Essentially, it looks like the \"grouping size\" and purpose is the only differentiating factor. Our paper shows that potentially any coarsening method can be used in clustering, as there are theoretical benefits in common coarsening algorithms that have not been studied in clustering yet.\n2. `The method makes use of the constraint `$X=C\\tilde{X}$`. If we substitute this constraint into the first term of (5), it would simplify to `$\\text{tr}(\\tilde{X}^\\top C^\\top\\Theta C\\tilde{X})=\\text{tr}(X^\\top\\Theta X)$`, which would simplify the optimization significantly. However, instead of enforcing this constraint exactly, the paper simply introduces an error term `$|X-C\\tilde{X}|$`, which seems unelegant to me. Why don't you enforce this constraint exactly?` \\\n\tNote that we cannot just assume a constraint to be true and substitute its value accordingly; it is a constraint to be checked. Also it is very difficult to solve for this constraint exactly because it involves both $C$ and $\\tilde{X}$ as variables, making it non-convex; that is why we add a relaxation of the constraint.\n3. `You mention that the log determinant term can be written as the sum of the log of the eigenvalues, and that this ensures that a 'minimal' number of eigenvalues are zero. However, doesn't this ensure that not a single eigenvalue is zero?` \\\n\tYes, it tries to ensure that but any graph Laplacian will always have at least 1 zero eigenvalue (because there is always at least 1 connected component in any graph). So it is safe to add this term.\n4. `Is the complexity that is described in the \"Complexity analysis\" paragraph the complexity of a single iteration or of all the iterations until convergence?` \\\n\tWe have described the complexity of one iteration/epoch, and compared it with other methods in Supplementary Material Section K. \n5. `I see that you rescaled the NMI, ARI and modularity to percentages. This is okay for NMI (though I'm not a fan of it), but for ARI it is confusing because ARI can be negative. For modularity, I have no idea how this rescaling is done, because the upper bound of modularity is smaller than 1 (and incredibly expensive to compute).` \\\n\tWe have simply multiplied the values by 100; both modularity and ARI can be negative.\n6. `Why do you draw lines in Figure 2b instead of making a table? The points that are connected don't correspond to consecutive things.` \\\n\tThe difference in the values and the impact is better understood as a graph. The points connected represent the same dataset. If we were to just use points (we tried that before), it would not look obvious which points belong to which dataset.\n\nWe hope these answer your questions along with the PDF that will be updated soon and that you will consider raising your rating."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479851687,
                "cdate": 1700479851687,
                "tmdate": 1700479851687,
                "mdate": 1700479851687,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]