[
    {
        "title": "A Distributional Analogue to the Successor Representation"
    },
    {
        "review": {
            "id": "TKgBjgz99O",
            "forum": "OMwD6pGYB4",
            "replyto": "OMwD6pGYB4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7962/Reviewer_jmar"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7962/Reviewer_jmar"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a distribution version of success feature, coined as the successor measure. The successor measure is defined as, conditioned on an initial state $x$, the Dirac distribution conditioned on a random trajectory induced by the policy. The paper proposes the Bellman backup of such successor measure, and proposes a recipe to estimate such successor measure using the proposed $\\delta$-model. Finally the paper evaluates the proposed method on some simple environments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper proposes a new method for distributional RL by estimating the occupancy measure of the policy, instead of its expectation, which seems like a natural way for the problem (vs. measuring the distribution of the value function.)\n\n2. The paper includes good explanations for the new mathematical definitions which help the reviewer to understand the new concepts."
                },
                "weaknesses": {
                    "value": "1. A few concepts are quite confusing along the paper. First, why is it necessary to redefine the occupancy measure/state(-action) distribution as successor representation or successor measure?\n\nSecond, why is it necessary to define the new \"random occupancy measure\", which is a distribution over distribution, but the inner distribution is just the Dirac distribution which is determined by the realization of the sample of the outer distribution? From my understanding, if one wants a distribution over the value function, suppose they have already have the occupancy measure, they could easily define the distribution of the value function (which is in $\\Delta(\\mathbb{R}^1)$) by projecting the occupancy measure to the reward vector. I think this is also indicated by prop. 1: to define the distributional return, one needs to take one expectation over the random occupancy measure. \n\nOverall, the theory results are rather limited. Most results seem like straight forward extension from the occupancy measure results to the random occupancy measure version. \n\n2. The significance of section 5.2 is unclear to me. How to tune MMD does not directly relate to the significance of the paper, and many description seems not significant (for example, the detailed description of the median trick).\n\n3. Since the theory contribution is rather limited, the experiment of the paper should be greatly improved. First, the paper should compare with other distribution RL methods, and the current benchmarks are also pretty easy."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7962/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7962/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7962/Reviewer_jmar"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7962/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698550639198,
            "cdate": 1698550639198,
            "tmdate": 1700668951440,
            "mdate": 1700668951440,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L9AqumLe07",
                "forum": "OMwD6pGYB4",
                "replyto": "TKgBjgz99O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7962/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7962/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback. We suspect that there has been a fundamental misunderstanding about what the DSM represents, and we hope to clarify this here. Particularly, we would like to address the following statements from the review:\n\n> The successor measure is defined as, conditioned on an initial state, the Dirac distribution conditioned on a random trajectory\u2026\n\n> ...why is it necessary to define the new \"random occupancy measure\", which is a distribution over distribution, but the inner distribution is just the Dirac distribution which is determined by the realization of the sample of the outer distribution?\n\nThe DSM is not a Dirac distribution conditioned on a random trajectory, and the \u201cinner distributions\u201d are also not Dirac distributions. The DSM is a probability distribution over distributions over the state space. Notably, the DSM has support on many distributions over the state space, and the distributions on its support are supported generally on the whole state space. Perhaps the reviewer is referring to the $\\delta$-model parameterization of the DSM; however, the claims are still not accurate:\n- $\\delta$-models are supported on a collection of Diracs, not just one. That is, the $\\delta$-models can be supported on arbitrarily many occupancy measures (controlled by the number of model atoms). It is important to note that while these are Diracs, they are Diracs each located on a probability distribution over the state space. So, the locations of the model atom Diracs are infinite dimensional.\n- Each model atom is represented as a generative model of state. Our architecture permits iid sampling from these model atoms, which enables us to leverage the unbiased sample-based MMD estimator to learn the locations of the model atoms.\n- Thus, there are two layers of samples. Sampling from a $\\delta$-model produces a generative model (one of the particles of the $\\delta$-model), and the generative model samples themselves can draw arbitrarily-many iid state samples to represent the corresponding occupancy measure.\n\n> ...why is it necessary to redefine the occupancy measure/state(-action) distribution\u2026\n\nAs the reviewer points out, these concepts are tightly related to the occupancy measure. However, this is standard nomenclature in the literature; the successor representation/measure is often referred to as the discounted occupancy measure.\n\n> From my understanding, if one wants a distribution over the value function, suppose they have already have the occupancy measure, they could easily define the distribution of the value function (which is in P(R)) by projecting the occupancy measure to the reward vector.\n\n> ... First, the paper should compare with other distribution RL methods, \u2026\n\nOur goal is far more ambitious than learning a distribution over the return. Indeed, this can be accomplished with standard distributional RL techniques like you suggest. Moreover, with just the occupancy measure, \u201cprojecting onto the reward\u201d only produces a point estimate of the return, not a distribution \u2013 indeed, this is how transfer with the SR works, and this process is shown in equation (4) of the paper. Our approach is modeling another \u201caxis of distribution\u201d beyond the SR/occupancy measure, and that is how we can \u201cproject\u201d our DSM onto a reward function to produce a distribution over returns. To achieve this functionality, it is necessary to properly model the interaction between random occupancy measures, and no other method has done this.\n\n> The significance of section 5.2 is unclear\u2026\n\nAs for the significance of Section 5.2, this is a good question. What distinguishes our problem setting from those where MMD optimization is commonly applied is that, due to bootstrapping, the target distributions that we model are non-stationary, and they can change over time (especially towards the beginning of training). Even with static target distributions, kernel optimization tricks are commonplace as you pointed out. However, existing RL algorithms based on MMD optimization (e.g., Nguyen-Tang et al. (2021), Zhang et al. (2021)) did not address this problem. Especially given the complex space of distributions being modeled in our work, we hypothesized that careful treatment of adaptive kernel parameters could substantially improve the quality of the resulting $\\delta$-models. We tested this hypothesis by ablating the adaptive kernel, and found that this was a crucial factor in the success of $\\delta$-model training \u2013 we invite the reviewer to consult Appendix D (particularly, Figure 9, right), for these results.\n\n### References\n- Thanh Nguyen-Tang, Sunil Gupta, Svetha Venkatesh. \"Distributional reinforcement learning with maximum mean discrepancy.\" AAAI Conference on Artificial Intelligence (2021).\n- Pushi Zhang, Xiaoyu Chen, Li Zhao, Wei Xiong, Tao Qin, Tie-Yan Liu. \"Distributional reinforcement learning for multi-dimensional reward functions.\" Neural Information Processing Systems (NeurIPS), 2021."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7962/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700022561551,
                "cdate": 1700022561551,
                "tmdate": 1700029149251,
                "mdate": 1700029149251,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "683zUKDsdL",
                "forum": "OMwD6pGYB4",
                "replyto": "TKgBjgz99O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7962/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7962/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough review and the time you have dedicated to our paper. We have carefully addressed each of your questions and concerns in our rebuttal. We hope our clarifications align with your expectations, and we are eager to answer any further questions you have.\n\nKind regards, Authors."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7962/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338593894,
                "cdate": 1700338593894,
                "tmdate": 1700338593894,
                "mdate": 1700338593894,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vn4svjksps",
                "forum": "OMwD6pGYB4",
                "replyto": "683zUKDsdL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7962/Reviewer_jmar"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7962/Reviewer_jmar"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer response"
                    },
                    "comment": {
                        "value": "I appreciate the author's detailed response to address my previous confusion. My major concerns are addressed but I still believe the work would benefit more from more challenging benchmarks and more aims towards better explaining the concepts (I agree the current version is already doing a great job, but sometimes it could still be confusing to the reader). I will raise my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7962/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668935628,
                "cdate": 1700668935628,
                "tmdate": 1700668935628,
                "mdate": 1700668935628,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iJARuKRBq6",
            "forum": "OMwD6pGYB4",
            "replyto": "OMwD6pGYB4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7962/Reviewer_JnMG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7962/Reviewer_JnMG"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends the successor representation to distributional RL, and proposes the $\\delta$-model that learns the distributional successor measure."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of combining distribution RL with successor representation is interesting, which combines the merits of both SR and distributional RL. I think this is a promising and important direction. The theoretical analysis is sound."
                },
                "weaknesses": {
                    "value": "The experiment benchmark (windy grid world) is somehow toy compared with other distributional RL papers."
                },
                "questions": {
                    "value": "1. As for Eq.7, are there any requirements for the reward function $r$ besides deterministic? For instance, does it require $r$ to be linear? BTW, is $r$ assumed to be known in experiments?\n\n2. Can you further discuss the relationship between previous work like Zhang et al 2021b, which also learn multi-dimensional return distribution via MMD?\n\n3. For distributional RL papers, a common benchmark is visual input environments like Atari. I think current benchmark (windy gridworld) is somehow toy. It will be helpful to see experiments with larger scale. What\u2019s more, besides zero-shot policy evaluation, another advantage of SR is multitask training. Can the proposed method be combined with the multitask training?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7962/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7962/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7962/Reviewer_JnMG"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7962/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698725787269,
            "cdate": 1698725787269,
            "tmdate": 1700544391024,
            "mdate": 1700544391024,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZUcqp0TuAA",
                "forum": "OMwD6pGYB4",
                "replyto": "iJARuKRBq6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7962/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7962/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and positive feedback. We agree with the reviewer that the DSM is a promising and important direction of research. We address the questions outlined by the reviewer below and would like to reiterate that traditional distributional RL and the DSM are solving different problems and aren\u2019t directly comparable. We look forward to engaging with the reviewer during the rebuttal period and hope the reviewer may consider revising their score based on our discourse.\n\n\n> For distributional RL papers, a common benchmark is visual input environments \u2026\n\nRegarding learning a DSM from pixels, one of the reasons we modeled lower-dimensional state spaces is that we believe these results are indicative of how the method will perform if one were to scale to high-dimensional observations for the following reason: We believe learning high-dimensional data distributions isn\u2019t the best way to scale a method like DSM. Compounding the difficulty of learning a distribution over images, DSM must model a distribution over distributions of images. As evidence of this claim, performing dimensionality reduction and operating in a structured low-dimensional latent space seems to yield state-of-the-art results in model-based RL (e.g., Schrittwieser et al. 2020, Hafner et al. 2021) and generative modeling of images (e.g., Esser et al. 2021, Rombach et al. 2022). Given such latent embeddings, our current $\\delta$-models can learn the DSM as shown in our paper. Thus, we believe that supplementing DSM with a dimensionality-reduction component is out of the scope of this paper, but would make for a very exciting follow-up work.\n\n> As for Eq.7, are there any requirements for the reward function \u2026\n\nRegarding the requirements of the reward function, this is a strength of the DSM model: all that we require is that the reward function is measurable, which encompasses essentially any reward function. In particular, linearity is not required. The reward function can be a black box; all we need is the ability to query the reward. \n\n> Can you further discuss the relationship between previous work like Zhang et al 2021b \u2026\n\nThe work on distributional RL with multi-dimensional reward functions (e.g., Zhang et. al 2021b) is indeed closely related, as the reviewer suggests. Multi-dimensional distributional RL methods can simply model the distribution over random finite-dimensional vectors corresponding to returns from reward signals seen at train time. As such, one could use a multi-dimensional return distribution function like that learned by Zhang et al. to infer the return distributions for linear combinations of the reward functions that are used for training. On the other hand, the DSM is modeling a complex distribution over the simplex on the state space, which requires a novel approach to distribution estimation using generative models, as highlighted by Reviewer 9gwA. Notably, it is this distinction that allows the DSM to predict return distributions for arbitrary bounded measurable reward functions.\n\n>  Is r assumed to be known in experiments?\n\nIn our experiments, we assumed access to reward function queries (as is commonplace with SR models). There are several instances where this can be applied, such as goal-reaching,  target-tracking tasks, or continuous control tasks that require engineered reward functions. However, more generally, we can also learn a reward function from data, for instance, by supervised learning from reward samples, using IRL from expert trajectories, or learning a reward function from preferences as in RLHF.\n\n> What\u2019s more, besides zero-shot policy evaluation, another advantage of SR is multitask training. Can the proposed method be combined with the multitask training?\n\nWith regard to multitask training, can you please clarify or give an example of the type of setting you had in mind? Like SR methods, our DSM algorithm is policy-dependent but is otherwise task-agnostic. Our response to Reviewer 9gwA also discusses this, but please let us know if this does not address your question.\n\n### References\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer. \"High-Resolution Image Synthesis with Latent Diffusion Models.\" IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\n- Patrick Esser, Robin Rombach, and Bj\u00f6rn Ommer. \"Taming transformers for high-resolution image synthesis.\" IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n- Danijar Hafner, Timothy P. Lillicrap, Mohammad Norouzi, Jimmy Ba. \"Mastering Atari with Discrete World Models.\" International Conference on Learning Representations (ICLR), 2021.\n- Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap, David Silver. \"Mastering Atari, Go, chess and shogi by planning with a learned model.\" Nature Vol. 588, Pages 604-609, 2020."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7962/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700022306579,
                "cdate": 1700022306579,
                "tmdate": 1700022306579,
                "mdate": 1700022306579,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gxbSSEjw3K",
                "forum": "OMwD6pGYB4",
                "replyto": "iJARuKRBq6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7962/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7962/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough review and the time you have dedicated to our paper. We have carefully addressed each of your questions and concerns in our rebuttal. We hope our clarifications align with your expectations, and we are eager to answer any further questions you have.\n\nKind regards, Authors."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7962/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338579373,
                "cdate": 1700338579373,
                "tmdate": 1700338579373,
                "mdate": 1700338579373,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "APcKKX0Jq0",
                "forum": "OMwD6pGYB4",
                "replyto": "gxbSSEjw3K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7962/Reviewer_JnMG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7962/Reviewer_JnMG"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. Previously the major concern is the relatively simple experiment setting, as well as other questions. We thank the authors for clarifications, but the concerns on experiment setting remains. However, I would like to raise the score since I think the idea is quite interesting."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7962/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544364633,
                "cdate": 1700544364633,
                "tmdate": 1700544364633,
                "mdate": 1700544364633,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "L0FO6owlD4",
            "forum": "OMwD6pGYB4",
            "replyto": "OMwD6pGYB4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7962/Reviewer_9gwA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7962/Reviewer_9gwA"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel distributional RL algorithm that learns the distributional successor measure from training samples. This method allows a clean separation of transition structure, i.e., state occupancy measure, and reward, enabling zero-shot risk-sensitive policy evaluation."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method cast the problem of learning the return distribution into learning the distribution of random occupancy measure, decoupling the transition structure and reward functions and thus enabling zero-shot risk-sensitive policy evaluation. In this sense, the proposed method is novel.\n\n2. This paper presented a practical algorithm for training the $\\delta$-models, adopting diverse generative models to approximate the distribution of random occupancy measure. The training procedure itself has merit and can be potentially beneficial to the other distribution estimation tasks."
                },
                "weaknesses": {
                    "value": "1. Similar to the conventional successor feature and successor measure, the decoupling of the transition structure and reward functions still assumes a fixed policy and transition dynamic, limiting the usefulness of the proposed method.\n\n2. The usefulness of the distributional SM is quite limited at this point. I would recommend discussing more about the potential applications of the learned distributional SM other than the zero-shot distributional policy evaluation."
                },
                "questions": {
                    "value": "Will the setting of $\\gamma = 0.95$ limit the usefulness of the proposed method in practice when we care about the return of a long episode?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7962/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7962/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7962/Reviewer_9gwA"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7962/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698735433731,
            "cdate": 1698735433731,
            "tmdate": 1699636979050,
            "mdate": 1699636979050,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6xDSnkrmOD",
                "forum": "OMwD6pGYB4",
                "replyto": "L0FO6owlD4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7962/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7962/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their kind comments and overall positive review. We agree with the reviewer that the DSM is novel and that insights from our $\\delta$-model method are useful beyond estimating the DSM. We hope that our response below helps provide additional motivation for the problem we introduce and inspire exciting future work on the DSM. We look forward to an engaging discussion period. \n\nThe reviewer makes a fair claim that the theory of DSM (and other SM/SR tasks) is limited to fixed policies. Despite this, the successor representation has garnered significant interest in many facets of RL, for example, representation learning, exploration, and temporal abstractions which could conceivably benefit from a distributional perspective; examples are given in the second paragraph of Section 7.\n\nOur principal goal with this work was to address the difficulty of representing and learning a DSM, which has not been done previously, and we suspect that future work can devise control methods based on the DSM. There are some significant challenges in this direction, which are not specific to the DSM model itself:\n- Even in distributional RL, there is generally no guarantee that return distributions will converge under greedy distributional updates.\n- In risk-sensitive optimal control, it is generally necessary to employ policies that are either non-stationary or non-Markovian, and it is not immediately clear how to model occupancy measures or perform zero-shot evaluation for these types of policies.\n- The work of Touati et. al (2021) introduces the \u2018forward-backward\u2019 representation of the successor measure, which enables an agent to learn the SM for all optimal policies simultaneously. But in the distributional case, this necessitates a definition of \u2018optimal policy\u2019, which has its own challenges, as mentioned in the previous point.\n\nSolving any of these problems would be an incredible contribution in their own right, and we leave this to future work. That said, we believe there are exciting applications for DSM as presented in this work. Unlike any other algorithm we are aware of, the DSM allows us to generalize across tasks and risk-sensitive criteria which has many potential applications; one such example is the zero-shot risk-sensitive policy selection procedure presented in Section 6.\n\n> Will the setting of $\\gamma=0.95$ limit the usefulness of the proposed method in practice when we care about the return of a long episode?\n\nRegarding the limitation of the setting of the discount factor in our experiments, we are happy to report results for larger discount factors \u2013 we are running these experiments and will report back. Note, however, that the DSM already handles larger horizons more gracefully than the comparable $\\gamma$-models: in their experiments, they could not achieve reasonable performance by training with $\\gamma=0.95$ directly; instead, they found it necessary to introduce a heuristic schedule for $\\gamma$. Beyond that, even with the discount schedule, they could only achieve good results with a density model, which required further assumptions on the MDP dynamics. On the other hand, thanks to the n-step bootstrapping approach to generative TD learning that we introduce in this work, we can successfully learn $\\gamma$- and $\\delta$-models for $\\gamma=0.95$ (and we will confirm the same for $\\gamma=0.99$) without any heuristics like the discount schedule or additional assumptions about the dynamics."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7962/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700022184019,
                "cdate": 1700022184019,
                "tmdate": 1700029220004,
                "mdate": 1700029220004,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DZsfctf6Zd",
                "forum": "OMwD6pGYB4",
                "replyto": "L0FO6owlD4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7962/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7962/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough review and the time you have dedicated to our paper. We have carefully addressed each of your questions and concerns in our rebuttal. We hope our clarifications align with your expectations, and we are eager to answer any further questions you have.\n\nKind regards, Authors."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7962/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338560626,
                "cdate": 1700338560626,
                "tmdate": 1700338560626,
                "mdate": 1700338560626,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aPvlCOCpH1",
                "forum": "OMwD6pGYB4",
                "replyto": "L0FO6owlD4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7962/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7962/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Performance with larger discount factor"
                    },
                    "comment": {
                        "value": "In response to the reviewer\u2019s concern that our method may be limited to relatively small discount factors, we launched a $\\delta$-model training run in the Pendulum environment with a longer horizon ($\\gamma = 0.99$). We used a n-step bootstrapping estimator for the $\\delta$-model targets with $n=15$. We found that, even in the early stages of training, the return distribution predictions from the DSM are quite accurate, and have very similar quality to those reported for $\\gamma = 0.95$. You may find the corresponding plot in the supplementary material, under the name `dsm-gamma-099.pdf`.\nWe hope this alleviates the reviewers\u2019 concerns about the feasibility of training $\\delta$-models for longer horizons."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7962/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590852953,
                "cdate": 1700590852953,
                "tmdate": 1700590852953,
                "mdate": 1700590852953,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ptwXg6L7jL",
            "forum": "OMwD6pGYB4",
            "replyto": "OMwD6pGYB4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7962/Reviewer_xHdx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7962/Reviewer_xHdx"
            ],
            "content": {
                "summary": {
                    "value": "They investigate the distributional counterpart of discounted occupancy measures, which they refer to as the distributional success measure (DSM). Leveraging the forward Bellman equation for DSM, they introduce a novel approach for approximating DSM. Their approach entails modeling DSM as a collection of generative models, referred to as $\\delta$-models, and employing a maximum mean discrepancy as a loss function to quantify the dissimilarity between \"distributions over distributions\" stemming from the Bellman equation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Their research problem is both intriguing and relatively novel within the RL community. Their approach to estimating DSM appears to be innovative, although I do have some fundamental concerns that I will address later."
                },
                "weaknesses": {
                    "value": "I have some reservations regarding the current proposed methods.  \n* Firstly, it remains unclear why equal weights are utilized in equation (10). It seems plausible that we should consider learning these weights.\n* Secondly, there is a lack of guidance on selecting the value of  $m$ in equation (10), or determining how many $m$ values are required.\n* Thirdly, there appears to be a dearth of theoretical justification for the effectiveness of this modeling and approximation approach. While equation (10) may seem suitable if it exactly represents the true SDM, practical implementation would not align with this ideal scenario."
                },
                "questions": {
                    "value": "* Questions are written in a previous paragraph. \n\n* Let's say we have X = {0,1} (binary). Then, a set of $P(X)$ is parametrized by just one parameter $\\mu \\in [0,1]$. Sp, $P(P(X))$ is a set of distributions over $[0,1]$. So, if I understand correctly, learning SDM is equivalent to estimating a distribution over  $[0,1]$. Even in this simple case, does the author's approach have any theoretical guarantee? (finite $m$ and equal weights look restrictive?)\n\n\n### Suggestion for presentation \n\n* Equation (4) may appear somewhat elementary to researcher within the RL community. The current phrasing, such as \"Blier et al. 2021 derived this equation...,\" seems inaccurate and should be revised. I believe that this equation had already gained widespread recognition prior to the work of Blier et al. (2021), as it is commonly featured in numerous standard RL texts and papers.\n\n* As a related point, I generally believe the author should not emphasize whether the definition pertains to discrete or continuous spaces to such an extent. The transformation from a discrete space (when a base measure is the counting measure) to a continuous space (when the base measure is a Lebsgue measure)  is typically straightforward for individuals with a basic understanding of probability. Therefore, in Section 3.2, the statement \"though this result is novel in the case of more general state spaces\" may be somewhat misleading. I suggest that this aspect should not be categorized as \"novel.\" Instead, I recommend that the author simply highlight the distinctions between the two contexts (SDM and standard distribution RL with rewards).\n\n* It is somewhat unclear which parameters are precisely optimized throughout the entire algorithm. As I understand it, the author optimizes parameters for all generative models simultaneously in equation (16). It would be beneficial to present the algorithm using an algorithmic environment for clarity."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7962/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7962/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7962/Reviewer_xHdx"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7962/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698990660694,
            "cdate": 1698990660694,
            "tmdate": 1699636978930,
            "mdate": 1699636978930,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s6mCYlTsM7",
                "forum": "OMwD6pGYB4",
                "replyto": "ptwXg6L7jL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7962/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7962/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to weaknesses"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments and engagement with our work, and are glad to hear that the reviewer finds the DSM novel, and the $\\delta$-model an innovative means of learning the DSM.\n\nWe view the reviewer's main concerns as relating to the possibility of theoretical analysis of $\\delta$-models, and the possibility of investigating alternative approximations to the DSM. We believe that both of these are important directions for future work. We also believe that our introduction of the DSM learning problem, together with the empirically successful approach of training $\\delta$-models, are already substantial contributions in their own right. We respond to all comments in detail below, and eagerly await further discussion on these points.\n\n> Firstly, it remains unclear why equal weights are utilized in equation (10)...\n\nOur choice to use equally weighted particles to approximately represent probability distributions is motivated by several considerations:\n- Prior work in distributional reinforcement learning. Both the quantile representation introduced by Dabney et al. (2018), and the equally weighted particle representation introduced by Nguyen-Tang et al. (2021) have this form.\n- Prior work in computational statistics. Using equally weighted particles to approximate distributions is also a common approach in computational statistics, such as Wasserstein/MMD gradient flow approximations (see e.g. Chizat and Bach, 2018, Arbel et al., 2019) and herding (Chen et al., 2010).\n- This parameterization can still effectively assign weight to regions of the space of occupancy measures. For instance, if a given measurable set of the space of occupancy measures should have more probability mass, the trained $\\delta$-model can (and does) place more model atoms in that measurable set. \n\nWe agree with the reviewer that this is not the only choice of approximate representation that could be made. We view this work, introducing the distributional successor measure, as opening up a new research question within distributional RL: how to approximate and learn the DSM effectively. The delta-models introduced in this paper, together with the experimental results, provide an initial answer to this question, and we expect future work to continue to develop understanding in this area.\n\n> Secondly, there is a lack of guidance on selecting the value of m in equation (10)...\n\nTrained $\\delta$-models better approximate the true DSM as $m$ increases. Figure 9 in Appendix C shows that the Wasserstein distance between the DSM and the MC return distribution indeed decreases as we increase the number of model atoms.\n\n> Let's say we have $X = 0,1$\u2026 [are there] any theoretical guarantee[s]?\n\nThe reviewer\u2019s understanding of the DSM here is correct. It is true that we cannot necessarily model the exact DSM with the EWP parameterization \u2013 indeed, it is impossible to guarantee this, since the space of DSMs is far too large to search and represent. Our goal here, like in distributional RL, is to find a good approximation to the DSM among a tractable model class (which is the space of EWP representations here). Further discussion about convergence is given below.\n\n> Thirdly, there appears to be a dearth of theoretical justification \u2026\n\nThe reviewer is correct that convergence theory for the algorithm has not been established. Our approach is based closely on approximate dynamic programming methods, as well as an MMD-based loss whose behavior in the case of fixed targets has been rigorously analyzed (Arbel et al., 2019). In particular, in our work, we have established that the distributional SM can be computed by distributional dynamic programming (see Appendix F.1), as is the case of the return distribution function in standard distributional RL. What is missing is the behavior of this algorithm under stochastic approximation of the distributional Bellman operator, as well as the consequences of projecting the targets onto the space of EWP representations.\n\nThe quantile TD-learning framework and QR-DQN algorithm of Dabney et al. (2018), which is closely related to our approach, had also not established this theory, which was only very recently resolved by Rowland et al. (2023). This has been a recurring trend in distributional RL, where new important algorithms and their analyses have often come separately. Notably, distributional RL with EWP representation and an MMD objective was employed by Nguyen-Tang et al. (2021) and the multidimensional distributional RL work of Zhang et al. (2021), despite the (ongoing) lack of convergence results in both works. We are definitely in agreement with the reviewer that theoretical analysis of these algorithms is a crucial direction for distributional RL as a whole, and we strongly believe that our contribution presents substantial value to the community in providing a starting point for future theoretical and practical innovations, as earlier works in distributional RL have done in the past."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7962/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700021927289,
                "cdate": 1700021927289,
                "tmdate": 1700029714021,
                "mdate": 1700029714021,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MuB3DGd5Hj",
                "forum": "OMwD6pGYB4",
                "replyto": "ptwXg6L7jL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7962/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7962/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough review and the time you have dedicated to our paper. We have carefully addressed each of your questions and concerns in our rebuttal. We hope our clarifications align with your expectations, and we are eager to answer any further questions you have.\n\nKind regards, Authors."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7962/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338543382,
                "cdate": 1700338543382,
                "tmdate": 1700338543382,
                "mdate": 1700338543382,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XhwPpXCVL2",
                "forum": "OMwD6pGYB4",
                "replyto": "MuB3DGd5Hj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7962/Reviewer_xHdx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7962/Reviewer_xHdx"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response! I will reconsider the rating."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7962/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585267350,
                "cdate": 1700585267350,
                "tmdate": 1700585267350,
                "mdate": 1700585267350,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]