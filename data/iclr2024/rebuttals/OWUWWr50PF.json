[
    {
        "title": "Deterministic Error Bounds for Euclidean Clustering"
    },
    {
        "review": {
            "id": "x9LmUEvO5k",
            "forum": "OWUWWr50PF",
            "replyto": "OWUWWr50PF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2110/Reviewer_aXjr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2110/Reviewer_aXjr"
            ],
            "content": {
                "summary": {
                    "value": "The Euclidean Clustering problem is a widely studied clustering problem. Based on the observation that the features\nof a given clustering dataset could lie near a subspace whose projection operator encodes the clustering, in this paper, the authors give a deterministic error bound with a singular value decomposition. If the optimal clusters are \"well separated\", the proposed method could  deterministically guarantee the correctness of the solution. Experiments show that the proposed method achieve better performance on both synthetic and real-world datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method is simple but effective\n\n2. Under certain data distribution assumptions, i.e., the optimal clusters are \"well separated\", the proposed method gives a deterministic guarantee on the correctness of the solution"
                },
                "weaknesses": {
                    "value": "1. The notion of \"well separated\" clusters has also been widely used in Lloyd's type methods and approximation algorithms design for $k$-means clustering. Such as in [1]-[2], an $\\alpha$-perturbation resilient assumption were introduced. They assume that for each optimal cluster, the data points within it are closer to each other than to the data points in other optimal clusters, which is very similar to the \"not too scattered\" distribution proposed in this paper. However, this paper did not give detailed analysis of time complexity and memory usage of the proposed method. It is unclear to me that whether the proposed method can be generalized to handle large-scale datasets since there already exists massively parallel $k$-means clustering method [3] that could be used to handle large-scale datasets with an optimal guarantee on clustering cost. \n\n2. The experimental parts do not give detailed descriptions about the data sizes of the real-world datasets used in this paper.\n\n[1] Cohen-Addad V, Schwiegelshohn C. On the local structure of stable clustering instances[C]//2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS). IEEE, 2017: 49-60.\n\n[2] Angelidakis H, Makarychev K, Makarychev Y. Algorithms for stable and perturbation-resilient problems[C]//Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing. 2017: 438-451.\n\n[3] Cohen-Addad V, Mirrokni V, Zhong P. Massively Parallel $k$-Means Clustering for Perturbation Resilient Instances[C]//International Conference on Machine Learning. PMLR, 2022: 4180-4201."
                },
                "questions": {
                    "value": "1. Could the authors provide detailed analysis of the time and space complexities for the proposed method.\n\n2. Could the authors compare the data distribution assumptions made in this paper for finding the optimal guarantee of the solution, with the perturbation-resilient assumptions used in approximation design for $k$-clustering problems.\n\n3. Could the authors provide detailed data description about the sizes of the real-world datasets used in the experiments. I also wonder the scalability of  the proposed method for handling large-scale datasets, such as the million-scale datasets used in [1] and other large-scale datasets.\n\n[1] Ren J, Hua K, Cao Y. Global Optimal K-Medoids Clustering of One Million Samples[J]. Advances in Neural Information Processing Systems, 2022, 35: 982-994."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Since this is a theoretical results, I don't this there is any ethics concerns in this paper."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2110/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2110/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2110/Reviewer_aXjr"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2110/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698717697160,
            "cdate": 1698717697160,
            "tmdate": 1699636143704,
            "mdate": 1699636143704,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "cpYXHMYump",
            "forum": "OWUWWr50PF",
            "replyto": "OWUWWr50PF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2110/Reviewer_NErf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2110/Reviewer_NErf"
            ],
            "content": {
                "summary": {
                    "value": "We are given a set of points that follow some assumptions on how they are generated. In particular, there are k centers mu_1,..., mu_k, and each point is represented by adding noise to one of those k center (in that case, we say that the point belongs to the cluster of that center) The paper discusses the problem of finding the centers and what cluster each point belongs to. They show that there exists an algorithm that solves this problem given some identifiability solution (main theorem), i.e. given that the centers are enough separated, there is not too much variance in the points within a cluster, and we can observe enough points from each cluster. They discuss that this solution is also related to solve clustering problems. The algorithm is simple and based on a singular value decomposition of the data. They run extensive experiments on synthetic data for both verifying their main theorem and to show that their algorithm provides still a reasonable solution when those assumptions do not hold. They also run an analysis on five real datasets related to single-cell sequencing, showing the advantage of their methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The technical part of the paper is well-written and easy to follow. \nI think the applications of their results to real datasets on single-cell sequencing provides significant value."
                },
                "weaknesses": {
                    "value": "I think the paper fails to provide a discussion and comparison with previous work on related problems, and the relation with the clustering problem is not completely clear.\n\n-k-means with separated data-. It seems to be known that if the data is separated, then it is possible to obtain a near-optimal clustering even using EM methods as variants of Lloyd's algorithm, see for example the papers [Ostrovsky, Rafail, et al. \"The effectiveness of Lloyd-type methods for the k-means problem.\" Journal of the ACM (JACM) 59.6 (2013): 1-22]. \nOr [Jaiswal, Ragesh, and Nitin Garg. \"Analysis of k-means++ for separable data.\" International Workshop on Approximation Algorithms for Combinatorial Optimization. Berlin, Heidelberg: Springer Berlin Heidelberg, 2012.] for k-means++\n\n-Learning a mixture of distributions-. The paper is very similar to existing results where it is assumed that the data is generated from a mixture of distributions, and the goal is to identify the parameters of these distributions (and the distributions of the mixture that each point in their data belongs to). This problem has been extensively studied theoretically in other papers that the authors fail to cite and compare to (e.g., [1,2], note that some of this work also involves spectral methods as a singular value decomposition and a projection step). In particular, it is known that under identifiability those assumptions, it is possible to recover the parameters of these distributions (and what distribution each point belongs to).\n\n-Relation to Clustering-. I think that the relation and the comparison with the problem of clustering is unclear.\nFirst of all, the authors discuss the problem of k-means (or in general, center-based clustering), which is an *unsupervised* problem. \nThe goal of k-means clustering is to find k points, also called centers, such that the sum of the squared distance of each point in the dataset to the closest center is minimized. This is a well-defined optimization problem that has been extensively studied in the literature. This is different than the problem (1) studied by the authors.  In particular, there could be multiple optimal solutions to the k-median problem (2). It is not clear what is  \"the true clustering of the samples\": this creates confusion when comparing (2) and (1). \n\nAdditionally on clustering:\n\n- In k-means clustering, the literature wants a multiplicative approximation guarantee as the optimization problem is NP-hard. \"In contrast to traditional local minima guarantees ... \"The k-means++ is an initialization method that provides a multiplicative approximation guarantee and not a simple heuristic. \n\n- \"This paper gives a closed-form solution to Euclidean clustering, also known as k-means clustering\". This opening sentence is too strong and false: k-means clustering is a NP-hard problem so it is hard to solve it in general. They can provide a closed-form solution only under some separability assumptions, but still, their closed-form solution requires knowing what the optimal clustering is to compute the right threshold (||Z||). \n\n- (Minor) I have never seen before the equivalence \"k-means = Euclidean clustering\" in the abstract.\n\n- \"In contrast to traditional local minima guarantees \", there exists a lot of literature to solve the k-mean problems that provide multiplicative error with respect to the answer (e.g.,  k-means++, or PTAS for k-means) even without any prior assumptions on the data.\n\n- \"[No practical algorithm provides correctness guarantees...]\". The correctness guarantee is misleading as in center-based clustering (as k-means), we are looking to minimize an unsupervised objective, not accuracy with respect to some underlying labeling. With this interpretation, K-means++ provides a guarantee on the quality of the solution for the k-means problem, and e.g., local search heuristic provides guarantees for k-median [3].\n\n\n-----\n\nMoreover, in order to obtain the closed-formula solution of the main result, it seems that the algorithm requires the knowledge of ||Z||, meaning that it needs to know the optimal clustering to set the right threshold. \n\n\n[1]: Vempala, Santosh, and Grant Wang. \"A spectral algorithm for learning mixture models.\" Journal of Computer and System Sciences 68.4 (2004): 841-860.\n[2]: Dasgupta, Sanjoy. \"Learning mixtures of Gaussians.\" 40th Annual Symposium on Foundations of Computer Science (Cat. No. 99CB37039). IEEE, 1999.\n[3]: Arya, Vijay, et al. \"Local search heuristic for k-median and facility location problems.\" Proceedings of the thirty-third annual ACM symposium on Theory of computing. 2001."
                },
                "questions": {
                    "value": "(1) Could you clarify the relation of your work with respect to existing work that identifies data generated by mixture of separated distributions as e.g. [1,2] (see Weakness)?\n\n(2) Could you clarify the relation of your work with existing work on clustering separated data (see e.g., papers mentioned in Weakness)?\n\n(2) Can you clarify the relation of the solution of problem (1) with k-means clustering? In particular, what happens if there are multiple optimal solution for the k-means problem?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2110/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2110/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2110/Reviewer_NErf"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2110/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805579929,
            "cdate": 1698805579929,
            "tmdate": 1699636143628,
            "mdate": 1699636143628,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "OAtQ8EINaH",
            "forum": "OWUWWr50PF",
            "replyto": "OWUWWr50PF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2110/Reviewer_CV5n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2110/Reviewer_CV5n"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel algorithm for the well-known and well-researched k-means clustering problem. The authors demonstrate that their algorithm returns the correct clustering under certain instance structure assumptions. They also provide experimental results, validating their claim and comparing them with baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is well-written and the experiments consider a diverse set of synthetic and real-world datasets. The theoretical results are presented in a clear and concise manner, with all details provided."
                },
                "weaknesses": {
                    "value": "I am not convinced of the novelty and importance of the results in this work. The only theoretical result is under a very strong assumption on the structure of the input points. This is in contrast to many other fast and efficient algorithms that perform well (theoretical and practical) without any assumption on the input points for k-means. As a result, the results in this work are not particularly interesting in the general setting.\n\nFurthermore, there are a number of other works that consider well-separated instances (similar to this work) and present efficient algorithms that work deterministically. This work does not provide a clear comparison with those works, which is a significant issue. Without such a comparison, it is not clear if this work has any advantage over the known algorithms in the literature.\n\nFinally, the experimental section is outdated. K-means is a well-studied problem, and there are even papers dedicated to comparing the performance of known algorithms on different datasets. This paper does not compare its results with the best algorithms, such as greedy k-means++ or random walk based algorithms. This is another significant issue with this work."
                },
                "questions": {
                    "value": "I discussed three main weaknesses of this work, please consider providing reasons if you do not agree with them."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2110/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698854806885,
            "cdate": 1698854806885,
            "tmdate": 1699636143552,
            "mdate": 1699636143552,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "BhSRSQITco",
            "forum": "OWUWWr50PF",
            "replyto": "OWUWWr50PF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2110/Reviewer_CdDR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2110/Reviewer_CdDR"
            ],
            "content": {
                "summary": {
                    "value": "Authors introduced a new method for solving K-means clustering. They provided theoretical analysis that showed the error of the estimation of the assignment is bounded by the combination of number of clusters, number of samples, and variance within each cluster. Theoretical results also show that, under mild assumptions, the method is guaranteed to deterministically recover true cluster centroids and assignment. Authors also showed experimental results to support their findings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Motivation and goal are clear. Reasoning and proofs in Section 3 and 4 look solid."
                },
                "weaknesses": {
                    "value": "Although authors claim a closed form solution for the K-means clustering problem, in practice, due to its relaxation, the algorithm is not closed form anymore but requires alternating algorithms that are sensitive to initialization. Thus, theoretical guarantees in Section 3 don't apply any more. \n\nAccording to Figure 2, the proposed method is slow, compared to other iterative methods. Authors didn't provide the detailed algorithm, nor its complexity. Looks like linear to K and n, but much steeper than Lloyd's.\n\nI'm also confused by Hierarchical Clustering and Robust Estimation in page 6. They're only future possibilities. They are not concrete analysis. They seem not quite related to the rest of the paper, not contributing to any claims in the paper either.\n\nData used in the paper are very simple, and details of the datasets and experiments are missing, e.g. dimensionality, number of samples. Some visualizations would better help readers fully appreciate the effectiveness of the method in clustering those samples."
                },
                "questions": {
                    "value": "In 6 Experiments, first paragraph: \"entries of each $\\pmb{\\mu}^{\\star}_{k}$ are draw i.i.d. according to a standard normal distribution.\"\n\nFirst, a major issue we have in the K-means problem is that the best K is unknown. However, K is known in the synthetic data. The number of classes in the cell sequencing data, later in the section, are also known. And it seems K is known throughout the paper. I wonder when K is unknown what is the performance of the method compared to the baselines. Does it change the theories in Section 3?\n\nSecond, this normal distribution assumption is too strong for me. And there's no visualization of the data. We don't know whether the data fall into the preset that \"clusters are sufficiently separated, and data in each cluster not too scattered\". How does Gaussian mixture models compare to the proposed method in this case? Authors also claim that \"out results guarantee ... independent of its distribution (normal, poisson, etc.)\" on top of page 2, so it would be great see the results with other distributions. I wonder if authors also experimented with other types of data. What are the dimensions of the sequence data? Are they approximately normally distributed around their centroids? \n\nThird, please discuss more about Table 1. Why proposed method outperforms most baselines in Buenrostro6 and Buenrostro7 but not in other datasets? What are the contributing factors of performing the best in each dataset? I see AONMF is also consistently good across all datasets, but proposed CF is pretty moderate in Larry, Nestorowa, and Stassen."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2110/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699171404560,
            "cdate": 1699171404560,
            "tmdate": 1699636143461,
            "mdate": 1699636143461,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]