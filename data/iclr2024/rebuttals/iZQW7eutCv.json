[
    {
        "title": "pEBR: A Probabilistic Approach to Embedding Based Retrieval"
    },
    {
        "review": {
            "id": "OJUwNXMM1H",
            "forum": "iZQW7eutCv",
            "replyto": "iZQW7eutCv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1603/Reviewer_xamY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1603/Reviewer_xamY"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers a MLE rather than frequentist approach to training retrieval embeddings. The model posits a PDF for the relevance of a document to a query based on the inner product of the representation of the query and the document (Beta distribution). Subsequent optimization for the model parameters yields the vector representation. The dataset used contains 87M clicks and the result improves over the DSSM model on both precision (by small largin) and recall (by a bigger margin, especially on tail data)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Improved precision compared to DSSM, especially on tail queries. Recall is better than DSSM but by a small margin.\n2. The model produces a variable number of results based on relevance cutoff. So where there are more results relevant to a query, the model can retrieve more of them compared to DSSM."
                },
                "weaknesses": {
                    "value": "1. Assessment is sparse. The baseline chosen, DSSM, is rather old (from 2013). \n2. Comparison on one dataset and its unclear if it is public."
                },
                "questions": {
                    "value": "1. Can you comment on the dependence on the amount of training data? Would the precision and recall be higher with much smaller click data than 87M clicks?\n2. Why not compare to SoTa model for retrieval?\n3. Have you tried evaluated it on other public datsets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1603/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1603/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1603/Reviewer_xamY"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1603/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839953491,
            "cdate": 1698839953491,
            "tmdate": 1699636088724,
            "mdate": 1699636088724,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ahrSdVZOXg",
                "forum": "iZQW7eutCv",
                "replyto": "OJUwNXMM1H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1603/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1603/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point to Point Responses"
                    },
                    "comment": {
                        "value": "Thanks very much for the valuable comments. Below are our point to point responses:\n\nResponses to the weaknesses:\n1. Since our proposed method is applicable to various types of networks, thus we choose the classical DSSM model as a representative to present the model performance.\n2. It is the real data in our online e-commerce system, we consider to publish it in the future.\n\nResponses to the questions:\n1. In general, larger dataset will result in better performance. However, our method has advantages with smaller datasets since we introduce probabilistic distribution assumption to improve the generalization ability.\n2. Since our proposed method is applicable to various types of networks, thus we choose the classical DSSM model as a representative to present the model performance.\n3. Not yet, but we consider to publish our dataset in the future."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1603/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700395566112,
                "cdate": 1700395566112,
                "tmdate": 1700395566112,
                "mdate": 1700395566112,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZCOenN5O91",
            "forum": "iZQW7eutCv",
            "replyto": "iZQW7eutCv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1603/Reviewer_vdaa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1603/Reviewer_vdaa"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a probabilistic framework for performing retrieval with embedding models. The rationale is that standard techniques that usually employ fixed number of items to retrieve or with a tuned score will impact precision and recall metrics for different queries. The authors propose a probabilistic approach in the setting of two-tower approaches, by extending the InfoNCE loss. The approach is evaluated in a dataset of user click logs and compared with two baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The work is around dense embedding retrieval which is an important topic specially in industrial applications with large catalogs of items.\nIt is interesting to see this probabilistic approach for retrieving items as it avoids using standard approaches which may bring inefficiencies."
                },
                "weaknesses": {
                    "value": "It would be great if the authors could enhance the related work with probabilistic embedding approaches especially few from the domain of images and metric learning and also draw some parallels.For example, Probabilistic Embeddings for Cross-Modal Retrieval, CVPR 2021.\nOne could use such an approach for performing retrieval tasks as it models uncertainty.\n\nIs very difficult to assess the result. Could you please give more details for the dataset? What type of user log are these? How the model that you train looks like? What features do you have? How large is the set of unique items? The dataset as well as the architecture used is described very briefly.\n\nWhy you select such a large number for k? Does this artificially inflate the metrics you measure? Usually we would try to retrieve a small set of elements to feed to a ranker.\n\nHow significant is the result that you achieve? How this affects the ranking stage?\n\nThe experimentation part is very weak and is hard to assess the effectiveness of the approach."
                },
                "questions": {
                    "value": "Please previous comments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1603/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699307581042,
            "cdate": 1699307581042,
            "tmdate": 1699636088665,
            "mdate": 1699636088665,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Hhcb0wOMTa",
                "forum": "iZQW7eutCv",
                "replyto": "ZCOenN5O91",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1603/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1603/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point to Point Responses"
                    },
                    "comment": {
                        "value": "Thanks very much for the valuable comments. Below are our point to point responses:\n\n1. We think our work quite different from the probabilistic embedding work presented at CVPR 2021. As the probabilistic embedding is mainly focus on the learning of polysemy, our work aims to learn popularity. Therefore, we didn't delve into a detailed discussion of probabilistic embedding. But it is also a kind of probabilistic method, we will consider to discuss more about these methods in the future.\n2. The datasets are user click logs collected from a large-scale e-commerce system, which have tens of millions of unique queries and items. The model we employed is based on the DSSM architecture which has a query tower and an item tower. The query tower takes the input of user input query, and the item tower takes the attributes of item like title, brand, model, color, size and so on.\n3. The choice of $k$ depends on the efficiency requirements of online system. We choose a large $k$ for two reasons: (1) our e-commerce system is large-scale, which have billions of candidate items and we need to retrieval enough items for subsequent ranking stage. (2) We have cascade ranking modules which serves like a funnel, progressively reducing the computation at each stage.\n4. We have achieve significant improvement in online A/B tests. It retrieves more items for popular queries, while less items for specific queries, thus the total number for ranking stage does not change much.\n5. About the baseline method, we didn't find any previous work similar to ours, and our proposed method is applicable to various types of networks, thus we choose the classical DSSM model as a representative to present the model performance."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1603/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700394148295,
                "cdate": 1700394148295,
                "tmdate": 1700394148295,
                "mdate": 1700394148295,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Gr7eJUzno4",
                "forum": "iZQW7eutCv",
                "replyto": "Hhcb0wOMTa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1603/Reviewer_vdaa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1603/Reviewer_vdaa"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the replies.\n\nStill, the details of the system and datasets are unknown to me. Even in your reply everything is very vague. No reproducibility for this paper is possible.\n\nNo evaluation on public datasets."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1603/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643863136,
                "cdate": 1700643863136,
                "tmdate": 1700643863136,
                "mdate": 1700643863136,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sCJdUUicps",
            "forum": "iZQW7eutCv",
            "replyto": "iZQW7eutCv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1603/Reviewer_2BmG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1603/Reviewer_2BmG"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a probabilistic approach to embedding-based retrieval, which allows for the design of a dynamic retrieval cutoff strategy tailored to different types of queries."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is generally well-written with a clear motivation from the weaknesses of existing frameworks. The authors present empirical evidence to the central research problem."
                },
                "weaknesses": {
                    "value": "1. The authors claim that the paper is the first to introduce probabilistic modelling into embedding based retrieval, which remains doubtful to me. Probabilistic embedding has a long history in machine learning, as well as probabilistic information retrieval, at least dating back to probabilistic ranking principle (Robertson, 1977), which essentially seeks to model the relevance of items to a query. Such literature was not reviewed in the paper. Furthermore, this formulation for modelling the retrieval probability and learning embeddings based on contrastive losses is not new to the community (see [1] and the references therein). \n\n2. Given the development of embedding retrieval models, the chosen baseline DSSM is quite old (2013). which cannot substantiate the usefulness of the proposed method over existing approaches. \n\n[1] Chun et al. (2021). Probabilistic Embeddings for Cross-Modal Retrieval."
                },
                "questions": {
                    "value": "1. In Section 3.3.2, the current model pEBR also relies on the chosen threshold $t$. Could the authors explain how sensitive it is to the model performance and at what value one should set it in practice? \n\n2. The choice of fixed threshold for $K$ items is mainly for saving inference cost. As detailed in Appendix A, it seems that computing the probabilistic CDF threshold can be slow, thus practically undesirable given the time constraints. If this system is in place, could the authors verify this has little impact on the product experience? \n\n3. The authors claim that without the probabilistic assumption, the model can fall short of generalisation ability. Any empirical evidence to support this, in comparison with the frequentist approaches?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1603/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1603/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1603/Reviewer_2BmG"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1603/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699404818213,
            "cdate": 1699404818213,
            "tmdate": 1699636088585,
            "mdate": 1699636088585,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jnRoO0Tfn6",
                "forum": "iZQW7eutCv",
                "replyto": "sCJdUUicps",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1603/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1603/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point to Point Responses"
                    },
                    "comment": {
                        "value": "Thanks very much for the valuable comments. Below are our responses:\n\nResponses to weakness:\n1. We know that there have been several literatures about probabilistic embeddings. However, we mainly focus on learning the probabilistic distribution of the relevance score of query and item, which is distinct from the learning of probabilistic embeddings. Furthermore, most probabilistic embeddings aim to capture the polysemy of query or item, our work addresses the learning of popularity.\n2. About the baseline method, we didn't find any previous work similar to ours, and our proposed method is applicable to various types of networks, thus we choose the classical DSSM model as a representative to present the model performance.\n\nResponses to questions:\n1. We need to consider both the effectiveness and efficiency of an online system. Larger $t$ brings better retrieval performance but require more online computations. In practice, we conduct several A/B tests to choose different $t$ that can balance the effectiveness and efficiency.\n2. Yes, the probabilistic CDF threshold requires more computing, but it has not yet being complex enough to impact system efficiency.\n3. Retrieval system usually suffers from the long-tail problem. The frequentist approaches are unable to effectively learn the probabilistic distribution of long-tail queries, as these queries are always short of training data."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1603/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700392029669,
                "cdate": 1700392029669,
                "tmdate": 1700392029669,
                "mdate": 1700392029669,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rVafUpzkUZ",
                "forum": "iZQW7eutCv",
                "replyto": "jnRoO0Tfn6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1603/Reviewer_2BmG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1603/Reviewer_2BmG"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "Thanks for the responses. Given that the emphasis is on practicality of the system, it seems a common concern among the reviewers that the proposed method needs further comparison against popular methods and even on more public datasets. If the proposed method were superior or at least competitive,  we could truly know how useful it is and I would be willing to increase the score. I highly suggest the authors consider additional experimentation. Otherwise, the proposal is not convincing to us."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1603/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434470053,
                "cdate": 1700434470053,
                "tmdate": 1700434470053,
                "mdate": 1700434470053,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]