[
    {
        "title": "Improving protein optimization with smoothed fitness landscapes"
    },
    {
        "review": {
            "id": "04hqX2C9yJ",
            "forum": "rxlF2Zv8x0",
            "replyto": "rxlF2Zv8x0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1409/Reviewer_zgX5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1409/Reviewer_zgX5"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript proposes a set of techniques for protein optimization. The first is a method for smoothing protein fitness landscapes. The second is a technique to optimizing in this landscape using the Gibbs With Gradients procedure, which has previously been shown to provide excellent results for discrete optimization. The authors also design two new optimization tasks based on the GFP and AAV datasets, which are designed to be more difficult than previous variants. Finally, the authors demonstrate empirically that their method performs competitively with the state-of-the-art."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "### Originality\nAlthough the GWG optimization procedure has been used in other contexts, the application to protein optimization is novel. To my knowledge, also the specific graph-based formulation of the regression problem itself is new.\n\n### Quality\nThe paper seems technically sound. Code was provided to ensure reproducibility, and the authors provide additional details about the method in the supporting material. \n\n### Significance\nThe paper does not give much insight into why the method outperform earlier approaches (see below for details), but the empirical results are convincing, which by itself could be sufficient to have impact on the growing subcommunity in ICML interested in protein modelling and design."
                },
                "weaknesses": {
                    "value": "My main concern with the paper is that I - after reading it - do not feel much wiser about promising methodogical directions for protein modelling going forward.\nWhat I lack in the paper is perhaps more of a motivation of why particular modelling choices were made. For instance, why is the Tikhunov regularization a meaningful choice in the context of protein optimization? Intuitively to me, it seems like a fairly crude choice, ignoring much of what we know about proteins already (e.g. that certain amino acids are biochemically similar to others). The paper also provides no biological intuition about why we would expect the smoothness would help. Presumably, the idea must be that there are different length scales to the problem, and that we can ignore the short length scales and focus on the longer ones - but it is not obvious to be why that would be the case for proteins. Is part of the explanation that experimental data is typically quite noisy? But if that's the case, you would assume that you would get similar behavior by using a simple GP with observation noise - just using a kernel based on edit distance - or based on Eucledian distance in one-hot space. The paper would be much more satisfying for me if the smoothing procedure was motivated more clearly, and perhaps even validated independent of the optimization procedure (I assume you would hope that the smoothed regressor would extrapolate better?)\n\nMy other serious concern is about the empirical evaluation of the model. As far as I can see, when we evaluate an optimization model against an oracle, there is a risk that we end up optimizing against extrapolation artefacts of the oracle, in particular if we end up evaluating it far away from the data it was trained on. My concern is whether your method has an unfair advantage compared to the baselines, because it uses the same CNN architecture for both the model and the oracle - and could therefore be particularly well suited for exploiting these artefacts. To rule out this concern, it would be interesting to see how the model performs against an oracle trained using a completely different model architecture."
                },
                "questions": {
                    "value": "Page 4,\n*\"Edges, E, are constructed with a k-nearest neighbor graph around each node based on the Levenshtein distance 3.\"*\nIn real-world cases, the starting point is often a saturation mutagenesis experiment, where a lot of candidates will be generated with the same edit distance from the wild type (e.g. edit distance 1). In such cases, won\u2019t the fixed k-out degree lead to an arbitrary graph structure (I mean, if the actual number of equidistant neighbors is much larger than k)?\n\nPage 6, *\"4.1 Benchmark\"*\nIt was difficult to follow exactly what \"develop a set of tasks\" implies. Since the benchmarks are built from existing datasets, the authors should make it clearer exactly what they are \"developing\": is it only the starting set, or do they also restrict themselves to a subsample of the entire set? In table 1 and 2, are both *Range*, *|D|*, and *Gap* specifically selected for, or does e.g. *|D|* arise out of a constraint on *Range* and *Gap*?\n\nPage 6. *\"Oracle\"*\nSince you are using a CNN both as your oracle, and as the basis for your smoothed landscape model, isn\u2019t there a risk that your model is just particularly well suited for exploiting the extrapolation artifacts of the oracle? (repetition of concern stated above).\n\n### Minor comments:\nPage 1, *\"but high quality structures are not available in many cases\"*\nAfter AlphaFold, many would consider that high quality structures are now available in most cases.\n\nPage 2, *\"mutation is proposed renewed gradient computations.\"*\nSomething is wrong in this sentence\n\nPage 3, *\"in-silico oracles provides a accessible way for evaluation and is done in all prior works.\"*\nThis is not entirely accurate. People have optimized against actual experiments (e.g. Gruver, ..., Gordon-Wilson, 2023) - or optimized to find the optimal candidate in a fixed set of experimentally characterized proteins.\n\nPage 4, eq (2) *\"H(x)\"*\nAs far as I can see, H(x) has not been introduced(?)\n\nPage 6. *\"we utilize a simpler CNN that achieves superior performance in terms of Spearman correlation and fewer false positives.\"*\nWas this correlation measured on the GFP test set provided by TAPE after fitting on the training set?. If so, it's odd that the original TAPE paper did not find the CNN-based ResNet to outperform the transformer (actually, the transformer performance was dramatically higher). Please clarify.\n\nPage 6. *\"Recall the protein optimization task is to use D\"*\nPerhaps help the reader by rephrasing to \"Recall the protein optimization task is to use the starting set D\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1409/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1409/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1409/Reviewer_zgX5"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1409/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698270345380,
            "cdate": 1698270345380,
            "tmdate": 1700650859473,
            "mdate": 1700650859473,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iVjuiRad2E",
                "forum": "rxlF2Zv8x0",
                "replyto": "04hqX2C9yJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1409/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1409/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and detailed feedback. We appreciate the valid concerns and hope our response can address them. We have grouped together related points and itemized our response accordingly.\n\n## Weaknesses\n\n> My main concern with the paper is that I - after reading it - do not feel much wiser about promising methodogical directions for protein modelling going forward. What I lack in the paper is perhaps more of a motivation of why particular modelling choices were made\u2026The paper also provides no biological intuition about why we would expect the smoothness would help. \n\nThe core takeaway of our work is to demonstrate the benefits of regularization through smoothing. We argue our modeling choices are based on protein knowledge. As we say in our introduction, \u201cProteins can be notorious for highly non-smooth fitness landscapes: fitness can change dramatically with single mutations, fitness measurements contain experimental noise, and most protein sequences have zero fitness.\u201d It is well-known that deep learning models perform poorly with noisy and small datasets, i.e. curse of dimensionality. Knowing this, it becomes **crucial to remove the undesirable properties of protein data** and apply regularization if we want good performance with deep learning. For the purpose of protein engineering, **we care about finding the best sequences and not exactly modeling the true (un-smooth) fitness landscape**.\n\n> For instance, why is the Tikhunov regularization a meaningful choice in the context of protein optimization? Intuitively to me, it seems like a fairly crude choice, ignoring much of what we know about proteins already (e.g. that certain amino acids are biochemically similar to others).\n\nThe purpose of Tikhunov regularization is to train a surrogate model of the fitness landscape that **is synergistic with DL-based sampling algorithms**. Our algorithm of choice is Gibbs With Gradients (GWG) which performs exceptionally well on (non-protein) discrete optimization benchmarks, but has not found success in protein sequence optimization. **The theory of GWG explicitly states the convergence of the Markov chain and its performance are related to the smoothness of the model**. We applied Tikunhov regularization to smooth the model and validated that GWG can achieve state-of-the-art performance if the correct regularization is applied.\n\n> Presumably, the idea must be that there are different length scales to the problem, and that we can ignore the short length scales and focus on the longer ones - but it is not obvious to be why that would be the case for proteins. Is part of the explanation that experimental data is typically quite noisy?\n\nDoes short length scales refer to short mutational gaps (i.e. requiring few mutations to improve over the wild type)? If so, many protein engineering problems can require the need for long length scales. For instance, Somermeyer et al [1] shows multiple GFP proteins found in nature with cgreGFP having the highest fluorescence but only 41% sequence similarity with the avGFP used in our work. A priori it can be unknown how many mutations are needed to improve over the starting sequences. Furthermore, even for short length scales, the data can be limited and noisy which is where smoothing can still help \u2013 we see this in our results where most fail to get non-zero fitness using a un-smoothed model. \n\n> But if that's the case, you would assume that you would get similar behavior by using a simple GP with observation noise - just using a kernel based on edit distance - or based on Euclidean distance in one-hot space.\n\nOne of our baselines, BOqei, is a GP. The GP was implemented as part of a baseline in CoMs [2] and included in follow-up works (e.g. [3]) where it was found to perform poorly especially for long proteins like GFP where it achieved the worst performance. GPs in general suffer from scaling to high-dimensional problems such as protein sequences [4].\n\n> The paper would be much more satisfying for me if the smoothing procedure was motivated more clearly,\n\nThank you for the direct feedback! We hope our above answers have motivated smoothing. The introduction (third paragraph) mentions our motivation and Figure 1 provides geometric intuition of why smoothing helps with optimization. We have added a paragraph to the discussion about our modeling choices that depart from known fitness landscapes."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700284980222,
                "cdate": 1700284980222,
                "tmdate": 1700284980222,
                "mdate": 1700284980222,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XY2Pb6aHLH",
                "forum": "rxlF2Zv8x0",
                "replyto": "04hqX2C9yJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1409/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1409/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response cont."
                    },
                    "comment": {
                        "value": "> and perhaps even validated independent of the optimization procedure (I assume you would hope that the smoothed regressor would extrapolate better?)\n\nWe have included regression results that validate with smoothing in table 10 and 11. The results can be found in our global response. We took our unsmoothed and smoothed models trained on the hard and medium splits, and evaluated on the held-out data for which we have the true experimental fitness values for. We see that, as expected, smoothed models have higher training MAE but lower test MAE than their unsmoothed counterparts across all benchmarks, indicative of improved extrapolation as a result of smoothing.\n\n> My other serious concern is about the empirical evaluation of the model. As far as I can see, when we evaluate an optimization model against an oracle, there is a risk that we end up optimizing against extrapolation artefacts of the oracle, in particular if we end up evaluating it far away from the data it was trained on. \n\nThis is a valid concern. However, we point out the prior protein engineering publications at ICLR, NeurIPS, and ICML have all used in-silico models to evaluate sequences. We have followed this practice. Some works have included experimental validation, but these authors have resources from biological collaborators or pharma companies [5].\n\n> My concern is whether your method has an unfair advantage compared to the baselines, because it uses the same CNN architecture for both the model and the oracle - and could therefore be particularly well suited for exploiting these artefacts. To rule out this concern, it would be interesting to see how the model performs against an oracle trained using a completely different model architecture.\n> Page 6. \"Oracle\" Since you are using a CNN both as your oracle, and as the basis for your smoothed landscape model, isn\u2019t there a risk that your model is just particularly well suited for exploiting the extrapolation artifacts of the oracle? (repetition of concern stated above).\n\nWe state in our work, \u201cto ensure a fair comparison, we use the same model architecture in all baselines.\u201d **We have ruled out the possibility of our method solely exploiting the model architecture.**\n\n## Questions\n\n> Page 4, \"Edges, E, are constructed with a k-nearest neighbor graph around each node based on the Levenshtein distance 3.\" In real-world cases, the starting point is often a saturation mutagenesis experiment, where a lot of candidates will be generated with the same edit distance from the wild type (e.g. edit distance 1). In such cases, won\u2019t the fixed k-out degree lead to an arbitrary graph structure (I mean, if the actual number of equidistant neighbors is much larger than k)?\n\nGreat point! A crucial component of our method is to augment the starting sequences by randomly mutating sequences to extrapolate the graph further than an edit distance of 1. With site saturation data, the starting sequences may have an arbitrary graph structure, but the more important sequences are the augmented ones that the model has not seen during training. These augmented sequences are >1 edit distance from the wild type and provide an approximation of how the model extrapolates. These predictions are smoothed if they are noise and we believe this procedure leads to improved sampling of better sequences.\n\n> Page 6, \"4.1 Benchmark\" It was difficult to follow exactly what \"develop a set of tasks\" implies. Since the benchmarks are built from existing datasets, the authors should make it clearer exactly what they are \"developing\": is it only the starting set, or do they also restrict themselves to a subsample of the entire set? In table 1 and 2, are both Range, |D|, and Gap specifically selected for, or does e.g. |D| arise out of a constraint on Range and Gap?\n\nWe apologize for the confusion. Our benchmark trains the oracle on the entire dataset while the difficulties (which we call tasks) limit the starting set each method (including ours) has access to. |D| arises out of the choices for range and gap. Our choices for range and gap were chosen after experimenting with the values where we saw all the baselines failed on the hard starting set. We have changed the formatting of Table 1, placing the |D| column on the right, rather than the middle\n\n> Page 1, \"but high quality structures are not available in many cases\" After AlphaFold, many would consider that high quality structures are now available in most cases.\n\nWhile AlphaFold2 has made a revolution in the availability of structures, high quality structures are not readily available. The AlphaFold2 database paper [7] states only 58% of predicted residues have a confident prediction of which 36% have very high confidence. Furthermore, prediction of complexes lags behind the prediction of single chain. Lastly, an important point is AlphaFold2 has poor accuracy of point mutations [8]."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700286391381,
                "cdate": 1700286391381,
                "tmdate": 1700288377568,
                "mdate": 1700288377568,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HcPH2y6go6",
                "forum": "rxlF2Zv8x0",
                "replyto": "04hqX2C9yJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1409/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1409/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response cont. cont."
                    },
                    "comment": {
                        "value": "> Page 2, \"mutation is proposed renewed gradient computations.\" Something is wrong in this sentence\n\nThank you for this catch! We have corrected it.\n\n> Page 3, \"in-silico oracles provides a accessible way for evaluation and is done in all prior works.\" This is not entirely accurate. People have optimized against actual experiments (e.g. Gruver, ..., Gordon-Wilson, 2023) - or optimized to find the optimal candidate in a fixed set of experimentally characterized proteins.\n\nIndeed, Gruver et al does perform in-vitro evaluation for antibodies. We have corrected the statement, \u201cmost prior works.\u201d Note Gruver et al makes use of in-silico screening to first filter their designs before running in-vitro experiments.\n\n> Page 4, eq (2) \"H(x)\" As far as I can see, H(x) has not been introduced(?)\n\nWe mention H(x) is the 1-Hamming ball in a foot note. We have now included a mathematical description in the text, \"$H(x) = \\{y \\in V^M : d_(x, y) \\leq 1 \\}$ is the 1-ball around $x$ using Hamming distance.\"\n\n> Page 6. \"we utilize a simpler CNN that achieves superior performance in terms of Spearman correlation and fewer false positives.\" Was this correlation measured on the GFP test set provided by TAPE after fitting on the training set?. If so, it's odd that the original TAPE paper did not find the CNN-based ResNet to outperform the transformer (actually, the transformer performance was dramatically higher). Please clarify.\n\nYes, we evaluated the measurement on the held out set provided by TAPE. Note TAPE used a ResNet which has far more parameters and is a more complicated model than the simple 1D CNN used in our work and in [6] where they observed a similar finding of the CNN outperforming TAPE.\n\n> Page 6. \"Recall the protein optimization task is to use D\" Perhaps help the reader by rephrasing to \"Recall the protein optimization task is to use the starting set D\"\n\nThank you for this improvement. We have applied the suggestion.\n\n[1] https://elifesciences.org/articles/75842.pdf \n\n[2] https://arxiv.org/pdf/2202.08450.pdf \n\n[3] https://arxiv.org/abs/2203.04115 \n\n[4] https://arxiv.org/abs/1810.12283 \n\n[5] https://arxiv.org/abs/2203.12742 \n\n[6] https://www.biorxiv.org/content/10.1101/2021.11.09.467890v1 \n\n[7] https://www.nature.com/articles/s41586-021-03828-1\n\n[8] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10019719"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287246134,
                "cdate": 1700287246134,
                "tmdate": 1700288392749,
                "mdate": 1700288392749,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LlcQA1Om55",
                "forum": "rxlF2Zv8x0",
                "replyto": "HcPH2y6go6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1409/Reviewer_zgX5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1409/Reviewer_zgX5"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their detailed rebuttal. The new table with regression results is a welcome addition, and I appreciate the authors attempt to motivate the smoothing for me.\n\nAlso thanks to the reviewers for pointing out that the same model architecture was used in all baselines. I had missed that. I still think that there is a potential issue here, since you might primarily be testing the impact of smoothing on the artefacts of the model, rather than the actual biological fitness landscape. But at least it helps that you are doing the same for all baselines.\n\nI'll increase my score to a 6."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650841403,
                "cdate": 1700650841403,
                "tmdate": 1700650841403,
                "mdate": 1700650841403,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TkHZ4iHW2P",
                "forum": "rxlF2Zv8x0",
                "replyto": "h0gG35lPju",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1409/Reviewer_zgX5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1409/Reviewer_zgX5"
                ],
                "content": {
                    "title": {
                        "value": "Response to clarification"
                    },
                    "comment": {
                        "value": "Yes. I understand that you train two models. But as far as I can see, in the optimization setting, you will apply them both in an out-of-domain setting by pushing them outside the space of observed mutations. It is not clear to me that the smoothness properties of the real biological fitness landscape are preserved even by the non-smoothed model out-of-domain, and therefore it is unclear whether the improvements you see would translate to a real scenario.\n\nWhat others have done is to optimize against models trained across the entire space of proteins (e.g. stability) - which presumably would make such 'oracles' more robust in the extrapolation setting. But I completely agree that it is difficult to assess this completely without running the actual experiments.\n\nI do acknowledge that your regression experiments demonstrate that the smoothing has a positive effect in-domain, so thanks for including those."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728311666,
                "cdate": 1700728311666,
                "tmdate": 1700728311666,
                "mdate": 1700728311666,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gcrzF0MrTh",
            "forum": "rxlF2Zv8x0",
            "replyto": "rxlF2Zv8x0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1409/Reviewer_7UDm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1409/Reviewer_7UDm"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduces a method called Gibbs sampling with Graph-based Smoothing (GGS) that uses Tikunov regularization and graph signals to smooth the protein fitness landscape, improving the ability to create diverse, functional sequences."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Figure 1 is very helpful in the understanding of this approach.\n\nMy understanding of the section described in Section 3.2 is relatively clear.\n\nI think the Fitness, Diversity, and Novelty scores to be interpretable and helpful.\n\nI think it is encouraging that graph-based smoothing (GS) helps almost all other methods in Table 3. It\u2019s also great that this is a relatively straightforward procedure."
                },
                "weaknesses": {
                    "value": "\u201cWhile dWJS is an alternative approach to fitness regularization, it was only demonstrated for antibody optimization. To the best of our knowledge, we are the first to apply discrete regularization using graph-based smoothing techniques for general protein optimization.\u201d - This doesn\u2019t seem justifiably novel. Proteins are proteins.\n\nGenerally, I wouldn\u2019t use the term \u201cfitness\u201d when describing protein function. Rather, I would use phenotype or function, as fitness is a broad, poorly defined subset of fitness.\n\nFigure 5 is a reason why these function predictors should not be called \u201coracles\u201d, because mapping the effect of mutation to function is difficult itself. I\u2019d prefer \u201cprotein function approximator\u201d, or something along those lines.\n\n\u201cThese were chosen due to their long lengths, 237 and 28 residues\u201d What do you mean here? 28 isn\u2019t that long. I realize it is in the context of a larger protein, but I\u2019d be clear about that."
                },
                "questions": {
                    "value": "For the smoothing procedure, it\u2019d be great to show the amount of error introduced into the labels of the sequences. For instances where either a reasonable oracle model exists, or sequences with large hamming distances have been measured, and this smoothing procedure is introduced, what is the correlation of function values before and after?\n\n\u201cTo control compute bandwidth, we perform hierarchical clustering (Mullner, 2011) on all the se- \u00a8 quences in a round and take the sequence of each cluster with the highest predicted fitness using f\u03b8.\u201d Why not use the \u201cnoisy model\u201d for this, because it is the oracle for the true fitness of a sequence?\n\n\u201cSection 4.1 presents a set of challenging tasks based on the GFP and AAV proteins that emulate starting optimization with a noisy and limited set of proteins.\u201d I would like the authors to be clear by what the mean by \u201cnoisy\u201d. Is it experimental noise? Is the landscape too sparsely sampled? Where is this noise coming from, and what relative distribution does it have?\n\nGenerally, I feel like Figure 5 is a distraction from the broader utility of the work. I\u2019d just cite Dallago 2021 like you did for the use of CNNs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1409/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722787338,
            "cdate": 1698722787338,
            "tmdate": 1699636068919,
            "mdate": 1699636068919,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FLmrRJS5me",
                "forum": "rxlF2Zv8x0",
                "replyto": "gcrzF0MrTh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1409/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1409/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and feedback. Below we address the issues and comments.\n\n## Weaknesses\n\n> \u201cWhile dWJS is an alternative approach to fitness regularization, it was only demonstrated for antibody optimization. To the best of our knowledge, we are the first to apply discrete regularization using graph-based smoothing techniques for general protein optimization.\u201d - This doesn\u2019t seem justifiably novel. Proteins are proteins.\u201d\n\nWe apologize for the poor choice in wording. dWJS is a concurrent work that performs smoothing by training their energy-based model with denoising. We meant to highlight the differences in smoothing approaches where we instead use graph-based smoothing in protein optimization. We have replaced the line with, \u201cdWJS trains by denoising to smooth a energy-based model whereas we apply discrete regularization using graph-based smoothing techniques.\u201d \n\n> Generally, I wouldn\u2019t use the term \u201cfitness\u201d when describing protein function. Rather, I would use phenotype or function, as fitness is a broad, poorly defined subset of fitness.\n\nWe internally have debated over the right terminology. We noticed the protein engineering field has settled on using fitness landscape rather than function landscape [1,2,3]. We chose to stick with fitness to follow this convention. Our very first sentence mentions \u201cfitness can be defined as performance on a desired property or function.\u201d We would prefer to keep the wording to avoid major changes, but are happy to change all occurrences if the reviewer views this as a major issue.\n\n> Figure 5 is a reason why these function predictors should not be called \u201coracles\u201d, because mapping the effect of mutation to function is difficult itself. I\u2019d prefer \u201cprotein function approximator\u201d, or something along those lines.\n\nThank you for the suggestion! We have changed the mention of oracle everywhere to \u201ctrained evaluator model.\u201d We agree the term oracle places too much trust in the in-silico evaluations.\n\n> \u201cThese were chosen due to their long lengths, 237 and 28 residues\u201d What do you mean here? 28 isn\u2019t that long. I realize it is in the context of a larger protein, but I\u2019d be clear about that.\n\nThank you for pointing this out. We have changed this sentence to highlight the data availability, \u201cthese were chosen due to their relatively large amount of measurements, 56,806 and 44,156 respectively, with sequence variability of up to 15 mutations from the wild-type.\u201d\n\n## Questions\n\n> For the smoothing procedure, it\u2019d be great to show the amount of error introduced into the labels of the sequences. For instances where either a reasonable oracle model exists, or sequences with large hamming distances have been measured, and this smoothing procedure is introduced, what is the correlation of function values before and after?\n\nWe have included error analysis of the train and test splits for predicting the true fitness. See the global response. We see there is an improvement in generalization error to the held out test set with smoothing. The effect is greater with GFP than AAV.\n\n> \u201cTo control compute bandwidth, we perform hierarchical clustering (Mullner, 2011) on all the se- \u00a8 quences in a round and take the sequence of each cluster with the highest predicted fitness using f\u03b8.\u201d Why not use the \u201cnoisy model\u201d for this, because it is the oracle for the true fitness of a sequence?\n\nWe do not use the noisy model, denoted $f_{\\tilde{\\theta}}$, because the predictions from the noisy model are inaccurate (cf. above table). We use the smoothed model $f_{\\tilde{\\theta}}$ instead for higher quality predictions. We find using the noisy model for our method leads to very poor performance (2nd to last row in tables 3 and 4).\n\n> \u201cSection 4.1 presents a set of challenging tasks based on the GFP and AAV proteins that emulate starting optimization with a noisy and limited set of proteins.\u201d I would like the authors to be clear by what the mean by \u201cnoisy\u201d. Is it experimental noise? Is the landscape too sparsely sampled? Where is this noise coming from, and what relative distribution does it have?\n\nThis is a great point! By noisy, we mean the experimental noise. We briefly mention this in the introduction, \u201cfitness measurements contain experimental noise.\u201d We have clarified the sentence to say, \u201cSection 4.1 presents a set of challenging tasks based on the GFP and AAV proteins that emulate starting with experimental noise and a sparsely sampled fitness landscape.\u201d\n\n> Generally, I feel like Figure 5 is a distraction from the broader utility of the work. I\u2019d just cite Dallago 2021 like you did for the use of CNNs.\n\nThank you for the suggestion. We have cited Dallago and removed Figure 5.\n\n[1] https://www.nature.com/articles/nrm2805 \n\n[2] https://www.biorxiv.org/content/10.1101/2021.11.09.467890v1 \n\n[3] https://www.nature.com/articles/s41592-019-0496-6"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287662686,
                "cdate": 1700287662686,
                "tmdate": 1700287662686,
                "mdate": 1700287662686,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CnvJzdmAeS",
            "forum": "rxlF2Zv8x0",
            "replyto": "rxlF2Zv8x0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1409/Reviewer_Ayis"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1409/Reviewer_Ayis"
            ],
            "content": {
                "summary": {
                    "value": "This study proposes to smooth the protein fitness landscape to facilitate protein fitness optimization using gradient based techniques. This is motivated by the ruggedness of protein fitness landscape which makes optimization challenging. A graph based smoothing technique for fitness landscape followed by Gibbs with Gradient sampling is used to perform protein fitness optimization. Evaluation of their method has been done on train sets designed from GFP and AAV with two degrees of difficulty defined by the mutational gap between the starting set and the optimum in the dataset (not included in the starting set).  Their method shows better performance than others in the proposed benchmark. The proposed graph smoothing technique has been shown to help with other methods as well."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Designing train sets with varying difficulties for the task of optimization.\nProposing a new method for smoothing the protein fitness landscape before optimization."
                },
                "weaknesses": {
                    "value": "The proposed method has many hyperparameters to tune. \nGiven certain properties of protein fitness landscape, smoothing can hurt if not done properly."
                },
                "questions": {
                    "value": "1)\tPlease explain why after smoothing, the diversity and novelty of the final set of sequences decreases.\n2)\tIn defining train sets with varying levels of difficulty only two medium (mutation gap 6) and hard (mutation gap 7) levels have been used. What happens if you make this harder (higher than 7)? Also, should we assume that for less mutational gap all methods perform comparably?\n3)\tAs stated in the paper, single mutations can dramatically change the fitness. In the smoothing performed, similar sequences are enforced to have similar fitness. Have you investigated where smoothing can be detrimental?\n4)\tHow is the number of proposals ($N_{\\text{prop}}$) per sequence set?\n5)\tHave you tried smaller sizes for the starting set? In real world problems the size of the starting set could be much smaller than 2000? \n6)\tWas the oracle only used at the end for performance evaluation? In AdaLead, did you use the oracle as the fitness landscape or $f_\\theta$?\n7)\tMention the augmented graph size (how does it change with the size of the sequence)\n8)\tMinor: In Eq 5, $X_0$ should be X."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1409/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1409/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1409/Reviewer_Ayis"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1409/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784387797,
            "cdate": 1698784387797,
            "tmdate": 1699636068850,
            "mdate": 1699636068850,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2SnONTzXBv",
                "forum": "rxlF2Zv8x0",
                "replyto": "CnvJzdmAeS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1409/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1409/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and feedback. Below we address the issues and comments.\n\n## Weaknesses\n\n> The proposed method has many hyperparameters to tune. Given certain properties of protein fitness landscape, smoothing can hurt if not done properly.\n\nWe require four method-specific hyperparameters that we analyze in Section 4.3 and Appendix C.1 as well as providing sweeps for each of them. (We don\u2019t consider CNN model and training hyperparameters since we use standard choices from prior works.) **Please note that other works require equal or more method-specific hyperparameters than ours: COMs requires six, DynaPPO requires 4, GFlowNets require 4 hyperparameters**. Our work is forthcoming about the hyperparameters and provides analysis into each of them:\n\n1. *Sampling temperature* $\\tau$. Appendix C.1 provides a table of sweeping over different temperatures. Temperature is not specific to us since every MCMC method needs to tune the temperature for the respective need of balancing the sample quality and diversity.\n\n2. *GWG round* $R$. Again the number of rounds is not specific to our method but is required for GWG. Figure 3 shows $R$ needs to be high enough for the Markov chain to mix and reach its stationary distribution.\n\n3. *Number of nodes in the protein graph* $N_{nodes}$ is a hyperparameter specific to our method. Figure 3 shows higher $N_{nodes}$ to give better performance, but higher values lead to slower run time. Our choice of $N_{nodes}=250,000$ results in our method taking < 1 hour to sample and still achieves state-of-the-art result.\n\n4. *Smoothness weight* $\\gamma$ is the other hyperparameter specific to our method. Figure 3 shows the choice $\\gamma$ is important for the final performance as too high can lead to lower performance. **We explicitly state in Section 5 that determining $\\gamma$ is a current limitation of our method and automatic ways of choosing $\\gamma$ is a important future direction.** Choosing $\\gamma$ is non-trivial since protein landscapes are very difficult. Nevertheless, our work demonstrates the importance of careful smoothing is in protein optimization.\n\n## Questions\n\n> 1. Please explain why after smoothing, the diversity and novelty of the final set of sequences decreases.\n\nOur metrics reflect the tradeoff between exploitation and exploration where a method needs to maximize the reward (fitness in our case) while discovering the multiple modes of the distribution. Reward and diversity are often at odds since many diverse sequences can be sampled but each have low reward (and vice versa).\nTables 3 and 4 show this general trend in methods without smoothing where higher diversity and novelty leads to the worse fitness (see GFN-AL and BOqei). Smoothing will reduce diversity by flattening the local and noisy variation in the fitness landscape (see the unsmoothed and smoothed landscapes in Figure 1). Our results show smoothing is effective to increase fitness across methods while avoiding low quality sequences that could artificially increase diversity."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700285878564,
                "cdate": 1700285878564,
                "tmdate": 1700285878564,
                "mdate": 1700285878564,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EvNCz4aoc4",
            "forum": "rxlF2Zv8x0",
            "replyto": "rxlF2Zv8x0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1409/Reviewer_PzPY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1409/Reviewer_PzPY"
            ],
            "content": {
                "summary": {
                    "value": "The paper propose a smoothing method on fitness function given a protein sequence. Assume that the given original data set is small, authors proposed a sampling augmentation method and a TV smoothing regulariser. After which MCMC algorithm is use to further optimise the fit."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Authors presented some good results on benchmark datasets."
                },
                "weaknesses": {
                    "value": "The paper is hard to read and understand. I itemise areas for improvements.\n\n1. Having one figure to show overall flow of logic could help. Fig1 seems to do the job. There are some confusion between training and sampling. I understand that the author first train f(x) and then use f(x) as a surrogate function for MCMC optimisation. This point does not come out naturally.\n\n2. construction of KNN graph could be described more clearly. (see Eq above Eq(1))\n\n3. Symbols of Eq.(2) are ill defined. The authors should provide in the appendix some details of GWG and reference the appendix in the main text.\n\n4. Eq.(4) should give the acceptance rate. while q are the probability of trial moves. x and x' are two states for jumping in this one MC step. Usual notation is q(x|x') vs q(x'|x), notation of Eq.(4) certainly is not of this form. Instead i^loc and j^sub and being used. The same i^loc and j^sub cannot appear in both numerator and denominator of Eq.(4).\n\n5. Eq.(4) what is the temperature of this move? It seems the temperature is set to 1. Why is the temperature 1? Is there any annealing process?\n\n6. Clustered sampling section should be explained better."
                },
                "questions": {
                    "value": "Is there a way to test that the surrogate function by itself is good enough? The authors look at the overall performance that could infer to correctness of the surrogate function.\n\n\nsee above section on 'weakness'"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1409/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699172969363,
            "cdate": 1699172969363,
            "tmdate": 1699636068776,
            "mdate": 1699636068776,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MJHjTGPyfP",
                "forum": "rxlF2Zv8x0",
                "replyto": "EvNCz4aoc4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1409/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1409/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and feedback. We notice the reviewer gave low scores for Soundedness, Presentation, and Contribution. However, we are unable to find reasoning behind the poor Soundedness and Contribution.\nWe kindly ask if the reviewer can elaborate on their low scores to help improve our work and address them in the discussion period.\nWe believe the clarity has improved with the reviewer\u2019s insightful comments. See our responses below.\n\n## Weaknesses and questions\n\n> The paper is hard to read and understand. I itemise areas for improvements.\n\nWe thank the reviewer for pointing out issues regarding clarity.\n\n> 1. Having one figure to show overall flow of logic could help. Fig1 seems to do the job. There are some confusion between training and sampling. I understand that the author first train f(x) and then use f(x) as a surrogate function for MCMC optimisation. This point does not come out naturally.\n\nIndeed, Figure 1 is meant to convey the overall flow of logic. By using the same notation, $f_\\theta$, in the training (top row) and sampling (bottom row), we hoped it explains training and inference with $f_\\theta$. We have updated figure 1 to clearly indicate that sampling follows after training and the same model $f_\\theta$ is used in both training and sampling.\n\n> 2. construction of KNN graph could be described more clearly. (see Eq above Eq(1))\n\nThank you for the suggestion. We have added a line near Eq (1) to state: \u201cThe graph construction algorithm can be found in Algorithm 4.\u201d Our graph construction pseudocode used to be in Algorithm 2 but we have placed it in its own Algorithm now for clarity.\nWe are unsure what the reviewer meant by \u201c(see Eq above Eq(1))\u201d since this equation refers to the definition of Total variation.\n\n> 3. Symbols of Eq.(2) are ill defined. The authors should provide in the appendix some details of GWG and reference the appendix in the main text.\n\nWe apologize for confusion and typos. We have clarified in the text that we use the one-hot representation, have rewritten Eq (2) to explicitly describe the matrix algebra, and made sure every symbol is described. Please see our updated writing in Section 3.3.\nWe believe section 3.3 fully describes the GWG algorithm as introduced in Grathwohl et al. Note Eq (4), (5), and (6) in Grathwohl et al are given as Eq (2), (3) in our work while Algorithm 3 in the appendix describes the full GWG algorithm (Algorithm 1 in Grathwohl et al). Our original submission states, \u201cWe provide the GWG algorithm in Algorithm 3.\u201d We have added in Section 3.3, \u201cIn this section, we describe the GWG procedure for protein sequences.\u201d We are happy to answer any specific concerns or questions the reviewer has regarding GWG.\n\n> 4. Eq.(4) should give the acceptance rate. while q are the probability of trial moves. x and x' are two states for jumping in this one MC step.\n\nThe acceptance rate differs based on the learned energy function which is non-trivial to analyze.\nWe are not sure what the reviewer means by \u201cEq. (4) should give the acceptance rate\u201d when there is no theoretical method to derive the acceptance rate when $f_\\theta$ is a neural network (to the best of our knowledge).\nHowever, we have added in the appendix tables 10 and 11 which includes a column containing the acceptance rates for each task and difficulty including whether we use our smoothed method or not.\nWe note that the acceptance rate is higher for the smoothed version of our method as further evidence smoothing is necessary for GWG to work.\n\n**GFP**\n| Difficulty | Smoothed | Acc. Rate |\n|------------|----------|-----------|\n| Medium     | No       | 0.61      |\n|            | Yes      | **0.62**      |\n| Hard       | No       | 0.01      |\n|            | Yes      | **0.43**      |\n\n**AAV**\n| Difficulty | Smoothed | Acc. Rate |\n|------------|----------|-----------|\n| Medium     | No       | 0.01      |\n|            | Yes      | **0.82**      |\n| Hard       | No       | 0.30      |\n|            | Yes      | **0.78**      |\n\n> Usual notation is q(x|x') vs q(x'|x), notation of Eq.(4) certainly is not of this form. Instead i^loc and j^sub and being used. The same i^loc and j^sub cannot appear in both numerator and denominator of Eq.(4).\n\nThank you for this catch! This was a mistake and we have corrected the MH-step with q(x|x') vs q(x'|x)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700282885637,
                "cdate": 1700282885637,
                "tmdate": 1700283322919,
                "mdate": 1700283322919,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kGmpYGbOuv",
                "forum": "rxlF2Zv8x0",
                "replyto": "DkMdiIO7So",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1409/Reviewer_PzPY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1409/Reviewer_PzPY"
                ],
                "content": {
                    "title": {
                        "value": "keep score"
                    },
                    "comment": {
                        "value": "I like to keep my score. I can see efforts in improving the paper. if the author work on the paper again for future submission, it should become good enough."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620626493,
                "cdate": 1700620626493,
                "tmdate": 1700620626493,
                "mdate": 1700620626493,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]