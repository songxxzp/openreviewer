[
    {
        "title": "UC-NERF: Neural Radiance Field for under-calibrated multi-view cameras"
    },
    {
        "review": {
            "id": "wP6pNbKC1c",
            "forum": "bLKcCe7hYh",
            "replyto": "bLKcCe7hYh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2194/Reviewer_ehJ8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2194/Reviewer_ehJ8"
            ],
            "content": {
                "summary": {
                    "value": "This work proposed UC-NeRF, a method for novel view synthesis in under-calibrated multi-view camera systems. The authors propose a three-step approach to address these challenges:\n\n1. Layer-based color correction: This step rectifies the color inconsistency in different image regions by applying color correction to each layer of the image pyramid.\n\n2. Virtual warping: This step generates more viewpoint-diverse but color-consistent virtual views for color correction and 3D recovery. The authors show that virtual warping benefits color correction and edge sharpness.\n\n3. Spatiotemporally constrained pose refinement: This step is designed for more robust and accurate pose calibration in multi-camera systems. The authors demonstrate that this step improves the accuracy of depth estimation in large-scale outdoor scenes.\n\nThe paper includes experimental results on several datasets and comparisons with other methods. The authors show that UC-NeRF achieves state-of-the-art performance in novel view synthesis and improves the sensing capabilities of multi-camera systems."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The problem setting is interesting. The proposed method can be an enhancement of current NeRF techniques.\n\n- The proposed method is sound and effective, regardless of its simplicity - complexity is not a criterion for us to judge whether a paper is good or not.\n\n- The authors did exhaustive experiments to show the effectiveness of the proposed method. Ablation studies also show the effectiveness of each module."
                },
                "weaknesses": {
                    "value": "- I think the introduction of this paper is not well written. It took me some time to understand why this work needs virtual warping and what are the differences between single-camera NeRF and multi-camera NeRF (since each camera in a multi-camera system can be deemed as a single camera).\n\n- The virtual warping step relies on the MVS method to generate dense depth maps, which may not generalize to street views (I'm not certain about this) and may need further pertaining.\n\n- In Eq.(5), the author does not explain what is $\\mathbf{b}$ and $\\mathbf{d}$ denote.\n\n- In Eq. (7), it is unclear whether the relative transformation $\\Delta \\mathbf{T}_k$ is optimized.\n\n- The final training loss is missing, e.g. $\\mathcal{L} = \\mathcal{L}_{\\text{pho}} + \\lambda_1 \\mathcal{L}_{\\text{reg}} +  \\lambda_1 \\lambda_2 \\mathcal{L}_{\\text{rpj}} $. \n\n- The pose refinement step is quite straightforward. Since the relative pose constraints $\\Delta \\mathbf{T}$ in the same rig can be obtained through calibration, I think it is naive to decompose the camera pose into the ego pose and a relative transformation $\\Delta \\mathbf{T}$. Moreover, the pose refinement step requires point correspondences, which could introduce outliers since it is well known that SOTA point matching methods are prone to repetitive structures and moving objects."
                },
                "questions": {
                    "value": "- The pose refinement step relies on keypoints, which could be a shortcoming. Did the author consider DBARF (CVPR 2023) and FlowCam (NeurIPS 2023), which jointly optimize consecutive camera poses and NeRF?\nActually, due to the vibrations during driving, the relative camera poses in a rig could change. I think the author mentioned it in the introduction, but the case is not handled in the formulation (Eq. (7)).\n\n- Did you reimplement Zip-NeRF or use others' reimplementation of Zip-NeRF? If it is the latter case, the URL should be provided since Zip-NeRF does not release its code."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2194/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2194/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2194/Reviewer_ehJ8"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2194/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756234374,
            "cdate": 1698756234374,
            "tmdate": 1699670528070,
            "mdate": 1699670528070,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "w8pMAOAA3F",
                "forum": "bLKcCe7hYh",
                "replyto": "wP6pNbKC1c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ehJ8 (Part 1/2)"
                    },
                    "comment": {
                        "value": "**1) The writing of introduction.**\n\nThanks for your suggestions. We have revised the introduction to better understand the multi-camera system and the motivation for virtual warping.\n\n**2) The usage of MVS methods.**\n\nWe use the public model of CER-MVS without finetuning. Virtual warping can enhance color correction and rendering quality, regardless of the depth sparsity. We also demonstrate its effectiveness with sparse lidar input in Tab. 1 below.\n\nTable 1. The performance of rendering with different sources of depth for virtual warping. Tested on a randomly selected Waymo scene.\n| Depth source| PSNR \u2191   | SSIM \u2191   | LPIPS \u2193 |\n|-------|-------|-------|-------|\n|No virtual warping | 28.74| 0.874 | 0.338|\n|Lidar virtual warping | $\\underline{29.04}$ | $\\underline{0.879}$ | $\\underline{0.335}$ |\n| MVS virtual warping (Ours) | $\\mathbf{29.49}$ | $\\mathbf{0.881}$ | $\\mathbf{0.334}$|\n\n**3) The explanation of $\\textbf{b}$ and $\\textbf{d}$ in Eq.5.**\n\nThanks for your suggestion. In Eq. 5, $\\textbf{b}$ and $\\textbf{d}$ should be $\\textbf{x}$ and $\\textbf{y}$ respectively, which represent the translation term of the color correction matrix. We have revised them in the paper.\n\n**4) Whether the relative transformation is optimized?**\n\nYes. The relative transformation is optimized. \n\n**5) The final training loss is missing.**\n\nThanks for your suggestion. We add the final training loss in Sec. 3.5 of the revised paper. \n\n**6) The pose refinement step relies on key points, which could introduce outliers.**\n\nWe have included the RANSAC[1] algorithm for filtering outliers before bundle adjustment.\nGenerally, pose optimization is based on point correspondences or photometric consistency. However, images taken by different cameras have significant color differences, which violates the assumption of photometric consistency. Furthermore, joint optimization of NeRF, color correction, and poses based on photometric loss only might not provide sufficient constraints. \n\nIn contrast, urban scenes are typically endowed with rich textures, which allow for the extraction of a substantial number of key points. We posit that the establishment of image correspondences serves as a more effective and direct strategy for pose optimization. We demonstrate that optimizing poses through explicit pixel correspondences is better than NeRF-pose joint optimization with photometric loss in the multi-camera setting in Tab. 6 and Fig. 12 of our revised paper (in Appendix A.3.2). \n\n**7) The consideration of DBARF[2] and FlowCam[3].**\n\nThanks for your suggestion. We finetune FlowCam and DBARF to our Waymo scenes, with results in Tab. 2 below. The performance of our method is better than FlowCam and DBARF in a multi-camera setting. DBARF optimizes poses through cost feature maps between images, which can be affected by significant color differences in images of different cameras. FlowCam requires the calculation of optical flow for pose optimization. However, given the restrained overlap between images from different cameras, the effectiveness of current optical flow techniques and subsequent pose estimation is limited in a multi-camera setting. \n\nTable 2. Comparisons of our method with FlowCam and DBARF. Tested on a randomly selected Waymo scene.\n| Method | PSNR \u2191   | SSIM \u2191   | LPIPS \u2193 |\n|-------|-------|-------|-------|\n|DBARF | 23.03 | 0.672 | 0.472 |\n|FlowCam | 23.50 | 0.721 | 0.401 |\n|UC-NeRF (Pose Refinement Only) | $\\textbf{27.79}$ | $\\textbf{0.867}$ | $\\textbf{0.363}$|"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413136083,
                "cdate": 1700413136083,
                "tmdate": 1700577506827,
                "mdate": 1700577506827,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "09o1IpYZ4L",
                "forum": "bLKcCe7hYh",
                "replyto": "wP6pNbKC1c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ehJ8 (Part 2/2)"
                    },
                    "comment": {
                        "value": "**8) The change of the relative camera poses.**\n\nThanks for your suggestion. As you pointed out, the relative pose between cameras may change during driving, but such considerable changes are typically caused by accumulated shakes over a vehicle's long-term drive like months. NeRF models generally operate on short video sequences lasting from minutes to hours, during which the changes in the relative poses between cameras are subtle. \nOn the contrary, directly modeling the transformation between cameras without our proposed spatiotemporal constraint is feasible, but it may lead to less accurate pose estimation. This is primarily because of the fact that images captured by a moving vehicle share limited overlaps, especially among the images captured by different cameras. This could potentially result in insufficient connections between images taken by different cameras, making the pose optimization relatively less constrained and unstable. Considering the two factors, we propose to utilize the spatiotemporal constraint to enhance the pose refinement.\n\nWe add an experiment in Tab. 3 below to analyze the spatiotemporal constraint. The experiment is conducted in ten commonly used self-driving sequences without any special selection. Compared to directly modeling the transformation without any constraint, our spatiotemporal constrained pose refinement leads to higher rendering quality of the reconstructed NeRF with more accurate camera poses. Fig. 12 of our revised paper also illustrates the improvement of rendering from this constraint.  \n\nTable 3. Effectiveness of spatiotemporal constraint tested on ten Waymo scenes.\n| Method  | PSNR \u2191   | SSIM \u2191   | LPIPS \u2193 |\n|-------|-------|-------|-------|\n| W/O Spatiotemporal Constraint | 27.89 | 0.835 | 0.368|\n| W Spatiotemporal Constraint  | **28.13** | **0.842** |**0.356**|\n\n**9)  Reference on ZipNerf.**\n\nThanks. We have cited the reimplementation of Zip-NeRF that we use in the revised paper. \n\n---\n\n[1] Fischler M A, Bolles R C. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography[J]. Communications of the ACM, 1981, 24(6): 381-395.\n\n[2] Chen Y, Lee G H. DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 24-34.\n\n[3] Smith C, Du Y, Tewari A, et al. FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow[J]. Advances in Neural Information Processing Systems, 2023"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413314125,
                "cdate": 1700413314125,
                "tmdate": 1700413314125,
                "mdate": 1700413314125,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oPOTNmX00u",
                "forum": "bLKcCe7hYh",
                "replyto": "w8pMAOAA3F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2194/Reviewer_ehJ8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2194/Reviewer_ehJ8"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the reply"
                    },
                    "comment": {
                        "value": "Thanks for the detailed reply from the authors. Your reply answered most of my questions. However, I'm still worried that the point correspondence could be a limitation. And some of your explanations may not be accurate:\n- \"We have included the RANSAC[1] algorithm during bundle adjustment\". I think the RANSAC is used to filter wrong correspondences before the bundle adjustment instead of during the bundle adjustment since traditional RANSAC is not differentiable.\n- \"which is robust to the outliers of key points.\" I think in dynamic scenes, RANSAC is not enough to filter those wrong matches, e.g. matches on a moving vehicle or pedestrians. Since your experiments are conducted on urban street views, I think it could be a problem.\nAnyway, this is just my concern, and I think it could be improved in the future. Therefore, I will keep my score unchanged."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700566126170,
                "cdate": 1700566126170,
                "tmdate": 1700566126170,
                "mdate": 1700566126170,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YlcaeRppi6",
                "forum": "bLKcCe7hYh",
                "replyto": "wP6pNbKC1c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your valuable feedback"
                    },
                    "comment": {
                        "value": "Thank you very much for your valuable feedback. Here is our reply.\n\n* Thank you for your correction. We have revised our response to \"before\".\n\n*  We appreciate your mention of the challenge posed by dynamic objects. Pose estimation under dynamic scenes is a specialized task, where many solutions are based on key points. For example, Mask-SLAM[1] and DynaSLAM[2] filter out key points in areas that may be dynamic objects (vehicles and pedestrians) through semantic segmentation. As our paper mainly addresses the challenges brought by introducing multiple cameras, we did not initially consider the issue of dynamic objects. But the solutions for filter dynamic objects, such as the semantic segmentation, can be easily added to our pose optimization module.\n\nFinally, we truly appreciate your recognition and suggestions for our work.\n\n---\n\n[1] Kaneko M, Iwami K, Ogawa T, et al. Mask-SLAM: Robust feature-based monocular SLAM by masking using semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2018: 258-266.\n\n[2]Bescos B, F\u00e1cil J M, Civera J, et al. DynaSLAM: Tracking, mapping, and inpainting in dynamic scenes[J]. IEEE Robotics and Automation Letters, 2018, 3(4): 4076-4083."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577347370,
                "cdate": 1700577347370,
                "tmdate": 1700578057746,
                "mdate": 1700578057746,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wMF7wlLWhZ",
            "forum": "bLKcCe7hYh",
            "replyto": "bLKcCe7hYh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2194/Reviewer_oEhA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2194/Reviewer_oEhA"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents UC-NeRF, a method for new view image synthesis in multicamera systems. They introduce models for color correction, virtual warping, and pose refinement to improve upon the results of Zip-NeRF and NeRF. Each of these operations defines a loss function L_{sky}, L_{reg}, and L_{rpj}. The results seem to suggest that they are achieving state-of-the-art results. Yet, the code to verify this claim is not available. Further details may be needed to implement their ideas completely."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper presents state-of-the-art results. They benchmark the performance of UC-NeRF with several other strategies that have been recently introduced. Their ablation study suggests that each term in the loss function improves the results."
                },
                "weaknesses": {
                    "value": "It would be great if the authors could share their code; even promising to share upon acceptance will be understandable. NeRF code is readily available. Otherwise, the authors should increase the clarity of their presentation to explain how their ideas could be implemented and the results reproduced for verification."
                },
                "questions": {
                    "value": "In 3.5, I understand that UC-NeRF is NeRF trained on the original NeRF\u2019s photometric loss and L_{sky} and L_{reg}, but you are also using L_{rpj}, correct? Is the total loss the sum of the individual losses? Are there weights on the losses before adding them?\n\nWhat is mathbf{b} and mathbf{d} in (5)?\n\nDefine d_v and d_o in (6)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no ethics concerns"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2194/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698800278371,
            "cdate": 1698800278371,
            "tmdate": 1699636153240,
            "mdate": 1699636153240,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XMjD25836X",
                "forum": "bLKcCe7hYh",
                "replyto": "wMF7wlLWhZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oEhA"
                    },
                    "comment": {
                        "value": "**1) The share of our code.**\n\nWe promise to share our code upon acceptance. \n\n**2) The formulation of the loss.**\n\n$L_{rpj}$  is used for pose refinement only. The total loss for NeRF training is the weighted sum of the photometric loss, $L_{sky}$ and $L_{reg}$. We add the formulation of the loss and revise the training strategy in Sec. 3.5. Please check out general response 1 about training strategy and loss formulation.\n\n**3) The definition of $\\mathbf{b}$, $\\mathbf{d}$, $d_v$, $d_o$.**\n\nMany thanks. In Eq. 5, $\\mathbf{b}$ and $\\mathbf{d}$ should be $\\mathbf{x}$ and $\\mathbf{y}$ respectively, which represent the translation term of the color correction matrix. In Eq. 6, $d_v$ and $d_o$ refer to the pixel depth in\nvirtual views and the corresponding pixel depth in original real views. All required definitions have been added to the paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410776119,
                "cdate": 1700410776119,
                "tmdate": 1700413990954,
                "mdate": 1700413990954,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JkkMpI9G0z",
                "forum": "bLKcCe7hYh",
                "replyto": "XMjD25836X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2194/Reviewer_oEhA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2194/Reviewer_oEhA"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your pledge to share the code. Your commitment is an important point as it increases transparency, gives others the chance to confirm the results, and provides a solid stepping stone for further advancing the field."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666139261,
                "cdate": 1700666139261,
                "tmdate": 1700666139261,
                "mdate": 1700666139261,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uBQWUrumyb",
                "forum": "bLKcCe7hYh",
                "replyto": "wMF7wlLWhZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your reply"
                    },
                    "comment": {
                        "value": "Thanks for your reply and understanding. We will definitely share the code."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668185135,
                "cdate": 1700668185135,
                "tmdate": 1700668279756,
                "mdate": 1700668279756,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zM4WNDgeaL",
            "forum": "bLKcCe7hYh",
            "replyto": "bLKcCe7hYh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2194/Reviewer_5Tdv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2194/Reviewer_5Tdv"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces UC-NeRF, a novel approach designed specifically for under-calibrated multi-view camera systems, addressing the challenges faced when applying NeRF techniques in such setups. The method incorporates layer-based color correction, virtual warping, and spatiotemporally constrained pose refinement to achieve exceptional performance in novel view synthesis and enhance the sensing capabilities of multi-camera systems. The contributions of the paper encompass the introduction of a new dataset tailored for under-calibrated multi-view camera systems, a novel layer-based color correction method, and an algorithm for spatiotemporally constrained pose refinement. The effectiveness of UC-NeRF is demonstrated through experiments conducted on the new dataset, and comparisons are made against state-of-the-art methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. This paper is well-written and easy to follow. \nS2. The proposed method is technically sound. \nS3. The experiment design especially the ablation study is solid and the results are noticeable."
                },
                "weaknesses": {
                    "value": "W1. The novelty of this paper is somewhat limited to me:\nW1-1. In terms of the first key innovation, namely layer-based color correction, why we can not use some classical multiple views color correction solutions in the structure-from-motion field as a pre-processing step instead of a module inside the NeRF? It should be justified. Besides, some existing NeRFs also addresses similar problem such as RAWNeRF and block-NeRF, what are the main differences between the proposed method and these works? \nW1-2 In terms of the spatiotemporally constrained pose refinement, there are some similar NeRFs that also consider the spatial and temporal connections between cameras for pose optimization. Name a few but not completed lists such as BARF (Lin et al. ICCV 2021) and BAD-Nerf (Wang et al. CVPR 2023). What is the novelty of the proposed method over these works? \n\nW2. The experiment comparisons are limited since only Mip-NeRF was used. Why not compare to some large-scale NeRDs such as block-NeRF or multi-views NeRFs such as MC-NeRF and NeRF-MS. The authors should justify the reason."
                },
                "questions": {
                    "value": "Please check the weaknesses listed above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2194/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809107992,
            "cdate": 1698809107992,
            "tmdate": 1699636153154,
            "mdate": 1699636153154,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XRK2zm2KK8",
                "forum": "bLKcCe7hYh",
                "replyto": "zM4WNDgeaL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5Tdv \uff08Part 1/2)"
                    },
                    "comment": {
                        "value": "**1) Why we can not use some classical multiple views color correction solutions in the structure-from-motion field as a pre-processing step instead of a module inside the NeRF?**\n\nFirst, limited overlap in images from different cameras hampers classical color correction methods, still causing color inconsistencies in NeRF training.\nSecondly, traditional multi-view color correction methods [1,2] propagate the reference colors to the corresponding 2D pixels in the target frames, which can be challenging to ensure global color consistency across a large set of images. However, NeRF can directly learn the color correction in the 3D space, thus naturally ensuring global color consistency in the rendered 2D images. \n\n**2) What are the main differences between the proposed color correction method and existing works, such as Block-NeRF and Raw-NeRF[3]?**\n\nExisting works, including Block-NeRF, alleviate the inconsistent color supervision by modeling image-dependent appearance with a global latent code for each image, which does not perform well in our setting (as illustrated in Fig. 11 and Tab. 5 of our paper). Compared to them, **(i)** we propose the concept of layer-wise color correction, which approximates color correction in foreground and sky regions as different affine transformations. This not only avoids the disentanglement of attributes unrelated to color inconsistency when the latent code implicitly models the image-dependent appearance, but also ensures the ability to model different color transformations in different regions of a single image. \n**(ii)** we provide a novel module ``Virtual Warping'' to further improve color correction. Learning one color correction for each image leads to overfitting when the training images lack color and viewpoint diversity. Virtual warping provides diverse color observations from novel viewpoints of different cameras for each 3D region, alleviating the overfitting of color correction and improving the 3D recovery.\n\nRaw-NeRF inputs the noisy mosaicked linear raw images from the sensor rather than the common images processed by a camera pipeline. Thus, it cannot be directly applied to our scenario. \n\n**3) What is the novelty of the proposed method over existing NeRF-pose joint optimization methods, such as BARF, and BAD-NeRF[4]?**\n\nExisting methods, including BARF and BAD-NeRF, jointly optimize poses with NeRF by photometric loss. However, they do not consider two difficulties of pose optimization in multi-camera systems.\n\n**(i)** Images taken by multiple cameras exhibit noticeable color inconsistencies. Joint optimization of NeRF, color correction, and poses based on photometric loss only might not provide sufficient constraints. Therefore, we optimize the poses more directly by establishing pixel correspondences. We add an experiment to demonstrate our superiority to BARF in Tab. 1 below. BAD-NeRF is tailored for the rendering of blurring images, which is not comparable with our method.\n\nTable 1. Comparison of our pose refinement method with BARF. Tested on a randomly selected Waymo scene.\n| Method  | PSNR \u2191   | SSIM \u2191   | LPIPS \u2193 |\n|-------|-------|-------|-------|\n|BARF|  28.65 | 0.856 |  0.373 |\n|UC-NeRF (Ours) | **29.14** | **0.867** | **0.355** | \n\n**(ii)** Inaccurate relative transformations between different cameras will cause rendering artifacts. However, directly optimizing the camera transformations without any constraint is unstable and inaccurate in our setting, since the images captured by multiple cameras share limited overlaps. We introduce a constraint that explicitly models the relative transformations between cameras as temporally consistent but optimizable variables to achieve more accurate pose refinement."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700412243647,
                "cdate": 1700412243647,
                "tmdate": 1700412243647,
                "mdate": 1700412243647,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CESSMqYc28",
                "forum": "bLKcCe7hYh",
                "replyto": "zM4WNDgeaL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5Tdv \uff08Part 2/2)"
                    },
                    "comment": {
                        "value": "**4) The experiment comparisons are limited since only Mip-NeRF was used. Why not compare to some large-scale NeRFs such as block-NeRF or multi-views NeRFs such as MC-NeRF[5] and NeRF-MS[6].**\n\nThanks for your suggestion. We have compared the strategies used for color correction and pose refinement in Block-NeRF in Tab. 5, Tab. 6, Fig. 11, and Fig. 12 of our revised paper (in Appendix A.3.2). \nBlock-NeRF does not release its code. According to its paper, it is built on Mip-NeRF. For a fair comparison, we replace our color correction strategy and pose refinement strategy in ZipNeRF with the strategies used in Block-NeRF. (Block-NeRF uses the color correction strategy from NeRF-in-the-wild and pose refinement strategy same as NeRF$--$.) \n\nWe add an experiment in Tab. 2 below, demonstrating our method's superiority over NeRF-MS. NeRF-MS lacks specific modeling of color correction for multi-camera systems, potentially correcting appearance unrelated to color inconsistency and resulting in inferior performance.  MC-NeRF is a contemporaneous work posted on arXiv in September 2023 without public codes. Therefore, we cannot compare it with ours within the limited discussion time. It focuses on optimizing the intrinsics of different cameras during pose optimization while we propose the spatiotemporal constraint between different cameras to enhance pose optimization. The discussion of these works is also updated in the related work of our revised paper.\n\nTable 2. Comparison of our method with NeRF-MS in a randomly selected Waymo scene.\n| Method  | PSNR \u2191   | SSIM \u2191   | LPIPS \u2193 |\n|-------|-------|-------|-------|\n| NeRF-MS | 26.87 | 0.828 | 0.392 |\n| UC-NeRF (Color Correction Only) | **28.15** | **0.851** | **0.374** |\n\n---\n[1] Ye S, Lu S P, Munteanu A. Color correction for large-baseline multiview video[J]. Signal Processing: Image Communication, 2017, 53: 40-50.\n\n[2] Lu S P, Ceulemans B, Munteanu A, et al. Spatio-temporally consistent color and structure optimization for multiview video color correction[J]. IEEE Transactions on Multimedia, 2015, 17(5): 577-590.\n\n[3] Mildenhall B, Hedman P, Martin-Brualla R, et al. Nerf in the dark: High dynamic range view synthesis from noisy raw images[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 16190-16199.\n\n[4] Wang P, Zhao L, Ma R, et al. BAD-NeRF: Bundle Adjusted Deblur Neural Radiance Fields[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 4170-4179.\n\n[5] Gao Y, Su L, Liang H, et al. MC-NeRF: Muti-Camera Neural Radiance Fields for Muti-Camera Image Acquisition Systems[J]. arXiv preprint arXiv:2309.07846, 2023.\n\n[6] Li P, Wang S, Yang C, et al. NeRF-MS: Neural Radiance Fields with Multi-Sequence[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 18591-18600."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700412568686,
                "cdate": 1700412568686,
                "tmdate": 1700412697771,
                "mdate": 1700412697771,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NN1bEU2xj3",
            "forum": "bLKcCe7hYh",
            "replyto": "bLKcCe7hYh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2194/Reviewer_9RAi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2194/Reviewer_9RAi"
            ],
            "content": {
                "summary": {
                    "value": "In this work, a novel method tailored for novel view synthesis is proposed for under-calibrated multi-view camera systems. In particular, a layer-based color correction is designed to rectify the color inconsistency in different image regions. To generate more viewpoint-diverse but color-consistent virtual views for color correction and 3D recovery, the authors further propose the virtual warping technique. And a spatiotemporally constrained pose optimization strategy is presented to explicitly model the spatial and temporal connections between cameras for pose optimization. Experiments on the Waymoand and NuScenes datasets show that this work achieves high-quality renderings with a multi-camera system and outperforms other baselines by a large margin."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The proposed layer-based color correction well addresses color inconsistencies in the training images, especially for those taken by different cameras.\n+ The virtual warping strategy naturally expands the range of the training views for NeRF, enhancing its effectiveness in learning both the scene's appearance and geometry.\n+ The experimental results look promising, and the proposed work significantly leads state-of-the-art methods."
                },
                "weaknesses": {
                    "value": "- The whole pipeline seems verbose since three independent modules are stitched together with few connections. Could the proposed UC-NeRF be trained in an end-to-end manner? Additionally, the efficiency comparisons of different methods are expected to be provided in the experiments.\n- The first two contributions, i.e., Layer-based Color Correction and Virtual Warping, are kind of trivial and have limited novelty. They are constructed based on existing methods like the pretrained segmentation model, the MVS model, and a geometric consistent check approach. The procedures of these two parts perform a preprocessing-like role in the proposed method. The authors are suggested to give more clarifications and highlight their specific contributions.\n- For the color correction part, it seems that the accuracy of the correction performance highly depends on the sky segmentation. However, the cases shown in the paper only contain clean skies and sunny weather. I am wondering how this work performs under diverse weather conditions. Because this work aims at multi-camera systems that are widely used in outdoor scenes (such as autonomous driving), the real-world application would be preferred over the method itself.\n- For the proposed Spatiotemporally Constrained Pose Refinement, please clarify its relationship and difference to the bundle adjustment."
                },
                "questions": {
                    "value": "How's the time cost to filter out inaccurate depths through a geometric consistency check?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2194/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2194/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2194/Reviewer_9RAi"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2194/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699104858158,
            "cdate": 1699104858158,
            "tmdate": 1700668409733,
            "mdate": 1700668409733,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gOnO2h62Fx",
                "forum": "bLKcCe7hYh",
                "replyto": "NN1bEU2xj3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9RAi (Part 1/2)"
                    },
                    "comment": {
                        "value": "**1\uff09 The whole pipeline seems verbose since three independent modules are stitched together with few connections. Can the proposed UC-NeRF be trained in an end-to-end manner?**\n\nYes, our NeRF is trained end-to-end with the learnable color correlation and virtual warping as data augmentation. The pose refinement is performed as a preprocessing step to provide accurate poses for NeRF training. \nOur method presents a unified framework for solving the problems of neural rendering with under-calibrated multi-view cameras, where the three modules are designed to tackle the key problems in this setting, such as inaccurate poses, and inconsistent color images.\nThe proposed three modules can complement each other and contribute as a whole to improve the final rendering results, which are correlated with each other and effective.\n\n**2\uff09 The efficiency comparisons of different methods.**\n\nThanks for your suggestion. We add Tab. 1 below to the paper for efficiency comparison. Note that our training time includes both steps (pose refinement and NeRF training) described in Sec. 3.5. Zip-NeRF is more efficient than other methods except Instant-NGP, which is specifically designed for NeRF acceleration. Since our method is built upon Zip-NeRF, our method consumes a bit more time than Zip-NeRF but achieves a significant improvement in rendering quality.\n\nTable 1. Efficiency Analysis. Tested on one NVIDIA Tesla V100 GPU with image resolution $1920 \\times 1280$.\n| Method  | Training | Inference (per image) | PSNR |\n|-------|:-: | :-:|  :-:| \n| Mip-NeRF |20h| 70s| 22.42| \n|Mip-NeRF-360 | 14h | 42s | 24.46| \n|Instant-NGP | 30min | 0.35s |23.84|\n|S-NeRF | 15h | 80s | 24.89|\n|Zip-NeRF | 2h | 2s | 26.21|\n|Ours | 3h | 3.2s | 28.13 | \n\n**3\uff09 The highlight of the first two contributions.**\n\nThe first two modules are embedded in the NeRF framework for end-to-end training. They respectively address a specific challenge of color correction during training of NeRF in multi-camera systems, i.e., **(i)** the way to model color correction and **(ii)** color correction in limited observations.\n \n**(i)** Existing methods do not perform well in color correction in our setting, as shown in Fig. 11 and Tab. 5 (in Appendix A.3.2) of our revised paper. They generally alleviate the inconsistent color supervision by modeling image-dependent appearance with a global latent code for each image. However, the lack of explicit modeling for color correction from the latent code can lead to the disentanglement of attributes unrelated to color inconsistency. Moreover, the capacity of a global latent code to uniformly correct colors in different regions of an image is limited, especially when different regions correspond to different color transformations. We propose the concept of layer-wise color correction. \nWe approximate color corrections in foreground and sky regions as different affine transformations. This strategy not only avoids the disentanglement of attributes unrelated to color inconsistency when the latent code implicitly models the image-dependent appearance, but also ensures the ability to model different color transformations in different regions of a single image.\n\n**(ii)** The limited observations used for color correction. Learning one color correction for each image leads to overfitting when the training images lack color and viewpoint diversity, especially for areas observed by side-view cameras, which have fewer observations and limited overlapping with front-view areas. Virtual warping provides a novel way to improve color correction. It provides diverse color observations from more viewpoints of different cameras for each 3D region, alleviating the overfitting of color correction and improving the 3D recovery. \n\n**4) The performance under diverse weather conditions.**\n\nThanks for your suggestion. Our datasets have included sunny and overcast weather, and we add rainy and night conditions here. As shown in Tab. 2 below, our approach notably improves across these scenarios. Please check the rendered images in the Fig. 13 (in Appendix A.3.2) of the revised paper and the video in the supplementary material.\n\nTable 2. Comparison of diverse weather conditions. Tested on Waymo Segment-100170 (Rainy) and Waymo Segment-150908 (Night).\n| Method  | PSNR (Rainy) \u2191 | SSIM (Rainy) \u2191 | LPIPS (Rainy) \u2193 | PSNR (Night) \u2191 | SSIM (Night) \u2191 | LPIPS (Night) \u2193 |\n|-------|:-: | :-:|  :-:| :-: | :-:|  :-:|\n| Zip-NeRF |27.65 | 0.831 | 0.434 | 30.69 | 0.864 | 0.512 |\n| UC-NeRF (Ours) | **30.03** | **0.866** | **0.387** | **31.32** | **0.869** | **0.491**|"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411647138,
                "cdate": 1700411647138,
                "tmdate": 1700566798898,
                "mdate": 1700566798898,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ilPZG2KAHQ",
                "forum": "bLKcCe7hYh",
                "replyto": "NN1bEU2xj3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9RAi (Part 2/2)"
                    },
                    "comment": {
                        "value": "**5) The relationship and difference to the bundle adjustment.**\n\nBundle adjustment (BA) is a widely used technique that involves simultaneously adjusting pose parameters and 3D point locations.\nIn our framework, we perform BA to optimize poses, and additionally incorporate the spatiotemporal constraint between cameras to eliminate errors of the under-constrained pose estimation of small-overlapping cameras in a multi-camera system. \n\n**6) How much does the time cost to filter out inaccurate depths through a geometric consistency check?**\n\nThe geometric consistency check takes about 0.3 seconds per depth map. Thus, for a 300-image scene, it could be done in 90 seconds."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411748067,
                "cdate": 1700411748067,
                "tmdate": 1700411748067,
                "mdate": 1700411748067,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QfT11HV3rR",
                "forum": "bLKcCe7hYh",
                "replyto": "ilPZG2KAHQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2194/Reviewer_9RAi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2194/Reviewer_9RAi"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the Response"
                    },
                    "comment": {
                        "value": "Thanks for your response. It addressed some of my concerns. However, I am still confused about the relationship between the proposed Spatiotemporally Constrained Pose Refinement and BA. In your reply, did you mean jointly performing BA and the proposed spatiotemporal constraint? Such an implementation may be less significant to the contribution. More details are expected to be discussed here."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658264673,
                "cdate": 1700658264673,
                "tmdate": 1700658264673,
                "mdate": 1700658264673,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l11T6w541r",
                "forum": "bLKcCe7hYh",
                "replyto": "NN1bEU2xj3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9RAi about BA and the proposed spatiotemporal constraint"
                    },
                    "comment": {
                        "value": "Thanks for your feedback. We greatly appreciate your questions and suggestions concerning our pose optimization module. First, we would like to emphasize that our UC-NeRF is designed to address the problem of color inconsistency in NeRF training under a multi-camera system, and pose refinement is only a part of this. It works in conjunction with other modules to tackle the challenges of color inconsistency. \n\nIn our spatiotemporally constrained pose refinement module, we optimize the poses using bundle adjustment (BA). However, BA is a general pose optimization technique that relies on different energy functions to optimize poses. Many works enhance pose accuracy through the design of energy functions, such as Pixel Perfect SFM[1] rewrites reprojection loss as pixel's feature loss, and NIM-SLAM [2] rewrites pixel's photometric loss as multi-scale patch\u2019s photometric loss. For our multi-camera setting, we also redesign the energy function. We consider the prior of the cameras, i.e., their spatiotemporal relationships, and add additional constraints to the poses. We design the energy function in the form of Eq.1 below:\n\n$$\nL=\\sum_{((i, k),(j, l)) \\in \\mathcal{E}}\\left\\|\\mathbf{p}_l^j-\\Pi_l\\left(\\left(\\mathbf{T}^j \\Delta \\mathbf{T}_l\\right)^{-1} \\mathbf{T}^i \\Delta \\mathbf{T}_k \\Pi_k^{-1}\\left(\\mathbf{q}_k^i\\right)\\right)\\right\\|^2. \\tag{1}\n$$\n\n$\\mathbf{T}^{i}_{k}$, which is the kth camera's pose at time $i$, is modeled as  $\\mathbf{T}^i \\Delta \\mathbf{T}_k$ with two parts: the ego pose $\\mathbf{T}^i$, and the temporally consistent and optimizable relative transformation of the cameras $\\Delta \\mathbf{T}_k$. The inclusion of this constraint significantly improves the rendering results, as shown in Tab. 1 below and Fig.12 in the appendix of the paper. \n\nTable 1. Effectiveness of spatiotemporal constraint tested on ten Waymo scenes.\n| Method  | PSNR \u2191   | SSIM \u2191   | LPIPS \u2193 |\n|-------|-------|-------|-------|\n| W/O Spatiotemporal Constraint | 27.89 | 0.835 | 0.368|\n| W Spatiotemporal Constraint  | **28.13** | **0.842** |**0.356**|\n\nOtherwise, independently modeling camera poses without the proposed spatiotemporal constraint is feasible, but it may lead to less accurate pose estimation. This is primarily because images captured by a moving vehicle share limited overlaps, especially among the images captured by different cameras. This could potentially result in insufficient connections between images taken by different cameras, making the pose optimization relatively less constrained and unstable. \n\nIn addition, we want to state the motivation for using BA. We have tried various strategies for joint optimization of NeRF and pose in multi-camera systems. However, due to the color differences between images, joint optimization of NeRF, pose, and color correction through photometric loss only might not provide sufficient constraints. Tab. 2 below (also shown as Tab. 6 in our paper) also demonstrates that explicitly establishing pixel correspondence to optimize poses shows more significant improvement than jointly learning poses, NeRF, and color correction.\n\nTable 2. Comparison of our pose refinement method with nerf-pose joint refinement methods BARF, NeRF-- (the same pose refinemment strategy in Block-NeRF). Tested on a randomly selected Waymo scene.\n\n| Method  | PSNR \u2191   | SSIM \u2191   | LPIPS \u2193 |\n|-------|-------|-------|-------|\n|NeRF$--$ | 28.48 | 0.851 | 0.383 |\n|BARF|  28.65 | 0.856 |  0.373 |\n|UC-NeRF (Ours) | **29.14** | **0.867** | **0.355** | \n---\n[1] Lindenberger P, Sarlin P E, Larsson V, et al. Pixel-perfect structure-from-motion with featuremetric refinement[C]//Proceedings of the IEEE/CVF international conference on computer vision (ICCV). 2021: 5987-5997. \n\n[2] Li H, Gu X, Yuan W, et al. DENSE RGB SLAM WITH NEURAL IMPLICIT MAPS[C]//The Eleventh International Conference on Learning Representations (ICLR). 2022."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664667771,
                "cdate": 1700664667771,
                "tmdate": 1700667297643,
                "mdate": 1700667297643,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v0A8KCiwWJ",
                "forum": "bLKcCe7hYh",
                "replyto": "l11T6w541r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2194/Reviewer_9RAi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2194/Reviewer_9RAi"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for the detailed reply and insightful analysis. I would like to increase my original rating."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668528310,
                "cdate": 1700668528310,
                "tmdate": 1700668528310,
                "mdate": 1700668528310,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CZnb9odop1",
            "forum": "bLKcCe7hYh",
            "replyto": "bLKcCe7hYh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2194/Reviewer_oaML"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2194/Reviewer_oaML"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a neural rendering system for automotive multi-camera temporally captured data (which they should explicitly mention in title and is misleading if not mentioned), which accounts for color variation, extrinsic errors and lack of sufficient training data typically effecting previous automotive applied Nerf methods. They first handle extrinsic errors by SLAM and again do something similar to SLAM as post-processing to recalibrate the cameras across time. To handle color variation across cameras at a given time instant, an affine color correction matrix is learnt separately from sky and foreground (as they can get sky mask from previous work) and thus there are separate NeRF models. Since NeRF requires dense sampling of input images, they also propose to get novel views via a separate pre-existing MVS method and use that depth map to render novel views. These novel views are then added to the training set of images. A new NeRF optimizing function is then defined taking all these into account. The results show improvement compared to many existing SOTA methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper handles a relevant problem in real life data. The proposed method looks sound. The references look adequate. The comparative results look good."
                },
                "weaknesses": {
                    "value": "- First, I would like the title to be more specific. Its very misleading as the paper does use automotive setting (sky+fg) as necessary input. The title seems to suggest that its a generic method for uncalibrated cameras. Also all results are on automotive. Please correct it.\n\n- In Eq. 5, what is b and d. I think the authors missed defining those params.\n\n- In Eq.7, isn't it  better to reduce reprojection error as a function of direct 4x4 transformation between the cameras across time. For example directly modeling the transformation between cameras labeled T^i*delta_T1 and T^j * delta_T1 in Fig4. The reason for this is that at time t=0, its convenient to bring all the three cameras in Fig4 to the car's coordinate system, but later assuming that this transformation is fixed and won't perturb due to camera shake, bad roads, bumps etc. is an unrealistic assumption. Then propagating this incorrect assumption across time from T^i to T^j can lead to erroneous extrinsics estimation in Eq. 7.\n\n- When virtual views are created and added to the training set as discussed in Section 3.3, it has holes either due to low confidence or occlusion as shown in Fig 9, then how does the sky segmentation from Yin 2022 perform in these images. What happens to the mask value in the missing image regions in virtual view and how does in impact Equation 3.\n\n- The training strategy is not very clear. So, you train using Eq.4 upto some convergence, Then you get A and C color correction matrices. Then again you apply the corrected A and C matrices to virtual views in Section 3.3. Then you get color corrected virutal views. Then you again train the original set of spatial+temporal data but this time include virtual views? I think Section 3.5 needs more explanation because it joins all your individual modules and is critical to understand how the complete system is working.\n\n- Section 4.4 is redundant I think. It has nothing to do with the main goal of your paper and that space could have been used to explain your main parts e.g. Section 3.5 in detail.\n\n\n- In Fig5, the part of the image where the road appears to merge, the green region adjoining the bright sky appears to be hazy in the proposed result compared to Zip-Nerf results. In other words Zip-Nerf results are much sharper in that region. What could be the reason for that?"
                },
                "questions": {
                    "value": "Kindly address the weakness as much as possible. I will update my review based on the rebuttal. Currently its borderline for me."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2194/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699274966451,
            "cdate": 1699274966451,
            "tmdate": 1699636152973,
            "mdate": 1699636152973,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WtiJYIdZYU",
                "forum": "bLKcCe7hYh",
                "replyto": "CZnb9odop1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oaML"
                    },
                    "comment": {
                        "value": "**1) The title needs to be more specific.**\n\nThanks for your suggestion. Our method aims at novel view rendering with under-calibrated multi-camera systems, where autonomous driving is one of the typical scenarios. Moreover, public autonomous driving datasets like Waymo are widely used benchmarks, making it convincing to compare our method with other works using the datasets. We are also open to incorporating ''autonomous driving'' in the title if needed.\n\n**2) The missing definition of \u2018b\u2019 and \u2018d\u2019.**\n\nThanks for pointing this out. **b** and **d** should be **x** and **y** as the translation term in the color correction matrix. Please check our general response 2 about the typos and symbols.\n\n**3) Isn't it better to reduce reprojection error as a function of direct 4x4 transformation between the cameras across time?**\n\nThanks for your suggestion. As you pointed out, the relative pose between cameras may change during driving, but such changes are typically caused by accumulated shakes over a vehicle's long-term driving like several months. NeRF models generally operate on very short video sequences lasting from minutes to hours, during which the changes among the relative poses of the multiple cameras are subtle. On the contrary, directly modeling the transformation between cameras without the proposed spatiotemporal constraint is feasible, but it may lead to less accurate pose estimation. This is primarily due to the fact that images captured by a moving vehicle share limited overlaps, especially among the images captured by different cameras. This could potentially result in insufficient connections between images taken by different cameras, making the pose optimization relatively less constrained and unstable. \nConsidering both factors, we propose to leverage the spatiotemporal constraint to enhance the pose refinement.\n\nWe add an experiment in Tab. 1 below to analyze the spatiotemporal constraint. The experiment is conducted in ten commonly used self-driving sequences without any special selection. Compared to directly modeling the transformation without any constraint, our spatiotemporal constrained pose refinement leads to higher rendering quality of the reconstructed NeRF with more accurate camera poses. Fig. 12 of our revised paper also illustrates the improvement of rendering from this constraint.\n\nTable 1. Effectiveness of spatiotemporal constraint tested on ten Waymo scenes.\n| Method  | PSNR \u2191   | SSIM \u2191   | LPIPS \u2193 |\n|-------|-------|-------|-------|\n| W/O Spatiotemporal Constraint | 27.89 | 0.835 | 0.368|\n| W Spatiotemporal Constraint  | **28.13** | **0.842** |**0.356**|\n\n**4) How does the segmentation model perform in virtual views?**\n\nWe do not employ any segmentation in virtual views. Both the sky mask and color of a pixel in the virtual view are obtained from the corresponding pixel in the original frames of the input sequence based on the warping by depth.\n\n**5) The training strategy is not very clear. Sec. 4.4 is redundant. Explain Sec. 3.5 in detail.**\n\nThanks for your suggestion. We revise the training strategy in Sec. 3.5 (detailed in general response 1) and move Sec. 4.4 to the appendix.\n\n**6) The green region adjoining the bright sky appears to be hazy.**\n\nThanks for the observation. The observed blurriness may stem from some random factors, such as sky segmentation errors or insufficient supervision of the regions due to few samplings. We have noticed similar blurriness in the rendering results of Zip-NeRF, whereas our results are clearer, as shown in Fig. 13 of our revised paper. On the whole, our rendering quality considerably outperforms that of Zip-NeRF in numerous areas, like consistent colors, sharp details, and clear logos."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411266224,
                "cdate": 1700411266224,
                "tmdate": 1700446457056,
                "mdate": 1700446457056,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2AD6de4MR9",
                "forum": "bLKcCe7hYh",
                "replyto": "WtiJYIdZYU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2194/Reviewer_oaML"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2194/Reviewer_oaML"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the rebuttal. I am satisfied with the responses and paper changes except for the response for title change. The paper explicitly uses sky mask which is very specific to autonomous related works, while the current paper title seems to reflect that the solutions for uncalibrated is a generic solution. I think the current title is quite misleading and definitely changed. In rebuttal authors say \"...where autonomous driving is one of the typical scenarios...\", but this is THE only scenario shown in paper and the proposed pipeline does use the specific information of sky and foreground objects."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642229723,
                "cdate": 1700642229723,
                "tmdate": 1700642229723,
                "mdate": 1700642229723,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uTciUEW4k1",
                "forum": "bLKcCe7hYh",
                "replyto": "CZnb9odop1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oaML about the title"
                    },
                    "comment": {
                        "value": "Thanks for your valuable feedback. We greatly appreciate your suggestion regarding the title and agree with it. Accordingly, we have incorporated the concept of 'autonomous driving' into the title in the most recent revision of our paper."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644172489,
                "cdate": 1700644172489,
                "tmdate": 1700668403248,
                "mdate": 1700668403248,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]