[
    {
        "title": "Unveiling the Pitfalls of Knowledge Editing for Large Language Models"
    },
    {
        "review": {
            "id": "AMTc9vaxXb",
            "forum": "fNktD3ib16",
            "replyto": "fNktD3ib16",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2526/Reviewer_YAyi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2526/Reviewer_YAyi"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the potential pitfalls related to LLMs knowledge editing, including Knowledge Conflict and Knowledge Distortion.\nTo achieve this target, two benchmark datasets and several innovative evaluation metrics are also introduced.\nWith these settings, this paper conducts experiments from two aspects among four common editing approaches on two LLMs, and proposes a Multi-Label Edit (MLE) solution."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper puts forward a valuable research problem, i.e., to discuss the potential risk of editing knowledge encoded in LLM. I believe this could help researchers and practitioners better understand and manipulate the knowledge encoded in LLM, so as to obtain a better model.\n2. Extensive baselines are employed for analyzing the proposed research question.\n3. Several new measures are designed to quantify the degree of knowledge conflicts in experiments."
                },
                "weaknesses": {
                    "value": "My major concerns lie in the following three aspects:\n1. The \u201cKnowledge Conflict\u201d proposed in this paper is confusing to me.\n2. The experimental settings of \u201cKnowledge Distortion\u201d are vague and incomplete.\n3. The proposed MLE solution is unclear."
                },
                "questions": {
                    "value": "My questions mainly comes from the above three concerns.\n1. The knowledge conflict mentioned in your paper, i.e., \u201ce1: Marie\u2019s husband is Pierre \u2192 Jacques and e2: Jacques\u2019s wife is Marie \u2192 Maurice\u201d, seems to be the \u201cData Collision\u201d rather than \u201cEditing Conflict\u201d.\nIf this type of conflict only exists at the data level, then the evaluation in this paper is meaningless.\nIn addition, I guess you want to emphasize the conflict between different editing operations, as mentioned in Section 2.2 \u201cthere is a possibility that interference occurs between different edits, causing the former edit invalid.\u201d.\nCould you further clarify this problem and provide more suitable examples?\n\n2. The \u201cKnowledge Distortion\u201d is a very promising research question that I pay attention to. However, the evaluation in this aspect is:\na) vague:\nHow many the (s,r) pairs did you evaluate to calculate the results in Table 2 ? (Is there five?)\nHow are the values in Table 2 calculated? Are they arithmetic mean values?\nWhy is the JS divergence chosen? Is the asymmetric KL divergence inappropriate? and why?\n\nb) incomplete:\nWhy only evaluate triples under the same (s, r), and other knowledge is not affected? (e.g, with same (s,.,.) and same (., r, .) or others (., ., .))?\n\n3. The proposed MLE method is unclear. Could you explain what the multi-label is and what is its function?\nHow does this method alleviate the problem of \u201cKnowledge Distortion\u201d?\nAdditionally, is this method effective for the first question (\"Knowledge Conflict\") mentioned in your paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2526/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2526/Reviewer_YAyi",
                        "ICLR.cc/2024/Conference/Submission2526/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2526/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698655045918,
            "cdate": 1698655045918,
            "tmdate": 1700635179836,
            "mdate": 1700635179836,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "y44DbIXgqL",
                "forum": "fNktD3ib16",
                "replyto": "AMTc9vaxXb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2526/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2526/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YAyi (Part 1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the time you dedicated to reviewing our work. Below, we would like to provide further clarification.\n\n---\n\n> **The Essence of Knowledge Conflict**\n\nAs the number of edit samples increases, we're concerned about the potential correlation between subsequent edits and those previously applied. If the editing process overlooks the correlation between edit samples and if these samples point towards different or even opposing editing targets, it could introduce conflicts into the model, resulting in a Knowledge Conflict issue. **In essence, Knowledge Conflict arises from logical overlaps in multiple edits during the editing process, leading to incomplete internal knowledge updates within the model and resulting in inconsistency.** We will further explain this definition in the following response.\n\n---\n\n> **\u201ce1: Marie\u2019s husband is Pierre \u2192 Jacques and e2: Jacques\u2019s wife is Marie \u2192 Maurice\u201d, seems to be the \u201cData Collision\u201d rather than \u201cEditing Conflict\u201d\u2026Could you further clarify this problem and provide more suitable examples?**\n\nIntuitively, the Coverage Edit appears as a complete Editing Conflict. However, this type of Editing Conflict doesn't necessarily cause what we refer to as Knowledge Conflict. If, after applying $e_1: (s, r, o_1 \\rightarrow o_2)$, the model can update the results of (s, r, ?) to $o_2$, and then, upon applying $e_2: (s, r, o_2 \\rightarrow o_3)$, further update the outcome edited by $e_1$ to $o_3$, then there wouldn't be an inconsistency in the model's knowledge on this matter. If, however, after applying $e_2$, the model retains the results from $e_1$, it creates uncertainty in the predictions for s and r, which we term as Knowledge Conflict.\n\n**Understanding the scenario of Reverse Edit might be more challenging, but its essence is similar to the aforementioned Coverage Edit.** From Coverage Edit to Reverse Edit, and finally to the Composite Edit, we illustrate their differences in Editing Scope in Figure 2. From the perspective of Editing Conflict you mentioned, **Composite Edit entirely lacks any Editing Conflict. However, it may still lead to internal knowledge inconsistency**, marking the fundamental distinction between Editing Conflict and Knowledge Conflict.\n\n---\n\n> **The \u201cKnowledge Distortion\u201d is a very promising research question that I pay attention to. However, the evaluation in this aspect is: a) vague: How many the (s,r) pairs did you evaluate to calculate the results in Table 2 ? (Is there five?) How are the values in Table 2 calculated? Are they arithmetic mean values? Why is the JS divergence chosen? Is the asymmetric KL divergence inappropriate? and why?**\n\nWe apologize for causing confusion in some implementation details. Here, we'll address your queries, and we plan to supplement related content in the main text or appendix.\n\n**Dataset Detail:** We utilized 2500 (s, r) pairs to derive the results in Table 2, covering relations samples from 1-to-2 to 1-to-10.\n\n**Experiment Detail:** The results in Table 2 are based on arithmetic mean values among the samples. Regarding the choice of JS divergence over KL divergence, we simply believe that when considering differences between two distributions, asymmetry is unnecessary. Additionally, computation of JS divergence is more efficient."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2526/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408567206,
                "cdate": 1700408567206,
                "tmdate": 1700408637548,
                "mdate": 1700408637548,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QwDsczlMxh",
                "forum": "fNktD3ib16",
                "replyto": "AMTc9vaxXb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2526/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2526/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YAyi (Part 2/2)"
                    },
                    "comment": {
                        "value": "> **Why only evaluate triples under the same (s, r), and other knowledge is not affected? (e.g, with same (s,.,.) and same (., r, .) or others (., ., .))?**\n\nIn the \"Vanilla Evaluation\" mentioned in $\\S$2.1, **Locality is generally used to evaluate performance on $(\\cdot, r, \\cdot)$ and $(\\cdot, \\cdot, \\cdot)$.** The paper didn't further conduct experiments on this metric. Additionally, **there's still some controversy in the evaluation on $(s, \\cdot, \\cdot)$**. In some instances, we believe that other relations with the same s (e.g., born in and mother tone) are correlated, and the model's performance on these relations should also update as part of demonstrating generalization. However, with certain relations (e.g., born in and married with) having no correlation, the evaluation of these triplets forms a part of Locality assessment. Hence, this paper only considers cases with the same s, r pair.\n\n---\n\n> **Could you explain what the multi-label is and what is its function? How does this method alleviate the problem of \u201cKnowledge Distortion\u201d? Additionally, is this method effective for the first question (\"Knowledge Conflict\") mentioned in your paper?**\n\nWe also apologize for the confusion in this section and will provide an explanation here. We plan to update this part in the subsequent revision to enhance understanding of our research.\n\n**Multi-Label Edit (MLE):** In our dataset, \"Multi-Label\" refers to the \"n\" correct objects in the mentioned 1-to-n relations. Due to the distortion in prediction distributions before and after editing among these objects, the model exhibits a bias toward edited objects while completely ignoring other correct objects. Our MLE method is a theoretical attempt to address this issue. Its conclusion is that by simultaneously editing multiple correct objects, we maintain the structure of the 1-to-n relation post-editing. **We believe that MLE fundamentally mitigates the issue of sample imbalance introduced when editing a single sample.** Unfortunately, this method doesn't address the Knowledge Conflict problem as it discusses knowledge update issues unrelated to the model's internal knowledge structure.\n\nWe sincerely hope the above answers could resolve your concerns!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2526/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408668893,
                "cdate": 1700408668893,
                "tmdate": 1700461867414,
                "mdate": 1700461867414,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9G4quKx1gk",
                "forum": "fNktD3ib16",
                "replyto": "AMTc9vaxXb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2526/Reviewer_YAyi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2526/Reviewer_YAyi"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks the authors for their reply. I have read the authors' response and decide to increase the rating a bit. Some issues are remaining:\n\n(1). I find it challenging to fully grasp the concept of \"editing conflict\" as mentioned in your response. On the one hand, this concept in your rebuttal is articulated as \"the conflicts caused by ignoring relevance between samples\" (i.e., see the sentence \u201cIn essence, \u2026\u201d in your response). However, it appears to be later translated into the \"ill-result caused by an unsuccessful editing operation\" (i.e., \u201c\u2026the model retains the results from e1\u2026\u201d). This transition is still confusing to me.\n(2). Even the proposed \u201cconflict\u201d is valid, the significance of the \u201cediting conflict\u201d is still questionable. Specifically, even if there is a conflict between two different editing, this is a natural phenomenon; because there probably be a conflict between \u201cold\u201d and \u201cnew\u201d knowledge and it is the point of knowledge editing. From this point of view, the \u201cconflict\u201d mentioned in your submission may be meaningless. Furthermore, as mentioned in your response, the proposed MLE method could not solve the \"knowledge conflict\" issue. Hence, it seems that the contribution of this submission is limited."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2526/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635510957,
                "cdate": 1700635510957,
                "tmdate": 1700635510957,
                "mdate": 1700635510957,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1nhtaL1BmX",
            "forum": "fNktD3ib16",
            "replyto": "fNktD3ib16",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2526/Reviewer_XF6v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2526/Reviewer_XF6v"
            ],
            "content": {
                "summary": {
                    "value": "This work pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs, and introduces new\nbenchmark datasets to evaluate LLMs after knowledge finetuning with proposed innovative evaluation metrics."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Novel benchmarks and evaluation metrics are developed in the paper\n2. With empirical analysis, the authors develop a simple method, a.k.a Multi-Label Edit, to alleviate Knowledge Distortion in LLMs"
                },
                "weaknesses": {
                    "value": "1. The novelty of the developed method is quite low and the real contribution of this paper is the development of new benchmarks equipped with evaluation metrics."
                },
                "questions": {
                    "value": "Thanks for your efforts in investigating two pivotal concerns in LLMs, specifically Knowledge Conflict and Knowledge Distortion, which has been widely discussed in NLP community nowadays. However, there remains some issues that I need to discuss with you.\n\n\n1. Fig. 2 illustrates that after the process of Round-Edit, LLMs tend to assign higher probabilities to the knowledge facts stored in recent corpus and gradually forget the knowledge stored in model parameters. Nonetheless, I believe that the demonstration of Knowledge Distortion in Fig. 2 is not a unique issue limited to LLMs, but rather a prevalent concern across all current Deep Learning models. \nAnd I believe that the solution to this issue is to develop more reasonable retrieval-based LLMs, which answers questions based on the knowledge context, rather than finetune-based Knowledge Editing methods mentioned in this article. With retrieval-based LLMs, you just need to modify the knowledge facts stored in the context, then LLMs can directly answer the user question according to the context. In such consideration, I suppose the contribution of this paper is limited, and it will be better to include evaluation of retrieval-based LLMs as baselines.\n\n2. The novelty of devlelopment of Multi-Label Edit is quite low, why not consider memory-based methods or EMA methods to alleviate the forgetting of previous knowledge stored in LLMs when you are certain that these knowledge fact are all accurate.\n\n3. For Knowledge Conflict, it will also be a problem in retrieval-based LLMs, any thoughts to solve this problem according to your expiermental results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2526/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734566625,
            "cdate": 1698734566625,
            "tmdate": 1699636189155,
            "mdate": 1699636189155,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gju9SBgUxu",
                "forum": "fNktD3ib16",
                "replyto": "1nhtaL1BmX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2526/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2526/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XF6v (Part 1/2)"
                    },
                    "comment": {
                        "value": "We would like to thank you very much for the detailed feedback and valuable suggestions. we hope the following comments could address your questions.\n\n---\n\n> **The Essence of Knowledge Distortion**\n\nThe results obtained from evaluating the impact of knowledge editing methods on unrelated samples using the locality\u2191 metric after editing a single piece of knowledge with the current knowledge editing methods are as follows [1]:\n\n| Model (Dataset) | FT | MEND | ROME | MEMIT |\n| --- | --- | --- | --- | --- |\n| **GPT-J (ZsRE)** | 37.24 | 97.39 | 99.19 | 99.62 |\n| **GPT-J (CounterFact)** | 1.02 | 93.75 | 93.61 | 97.17 |\n\nBased on the results from this table, we can see that the current knowledge editing methods differ from fine-tuning, as editing a single knowledge has a relatively minor impact on irrelevant knowledge within the model.\n\n**The Knowledge Distortion issue proposed in this paper, unlike the influence on irrelevant knowledge mentioned above, essentially aims to explore the degradation of the original knowledge structure due to the imbalance in samples under specific conditions.**\n\nIn investigating the Knowledge Distortion problem, **we utilized the Round-Edit strategy to differentiate it from the normal generalization process of knowledge editing.** According to the definition of knowledge editing generalization, when applying the edited sample \"Joe Biden was born in California \u2192 Vienna,\" the answer to the question \"What country was Joe Biden born in?\" would change from the USA to Austria. In this example, our focus shouldn't be on maintaining knowledge structure related to California but rather on emphasizing the knowledge structure related to Vienna. Therefore, evaluating the Knowledge Distortion problem becomes challenging. Through Round-Edit, we completely disregard generalization concerns and can assess the impact of knowledge editing on the model's knowledge structure.\n\n---\n\n> **I believe that the solution to this issue is to develop more reasonable retrieval-based LLMs\u2026In such consideration, I suppose the contribution of this paper is limited, and it will be better to include evaluation of retrieval-based LLMs as baselines.**\n\nAs you said, retrieval-based knowledge editing methods do indeed possess stronger interpretability in many settings. **However, existing retrieval-based knowledge editing methods like SERAC [2] and IKE [3] currently lack support for generating Knowledge Conflict and settings conducive to studying Knowledge Distortion in this paper.** Our focus is on the interaction of multiple edits. SERAC, due to limitations in its training process, can only handle single edit retrieval scenarios. On the other hand, the Memory+IKE (Retrieval-based LLMs) approach incorporates multiple edit samples as in-context inputs into the model, but strictly speaking, this cannot be considered as the impact of editing methods on the model's internal structure. We believe that both of these are unable to offer additional reference value while regarded as baselines."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2526/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408416184,
                "cdate": 1700408416184,
                "tmdate": 1700408416184,
                "mdate": 1700408416184,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zH74TAGNra",
                "forum": "fNktD3ib16",
                "replyto": "1nhtaL1BmX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2526/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2526/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XF6v (Part 2/2)"
                    },
                    "comment": {
                        "value": "> **The novelty of devlelopment of Multi-Label Edit is quite low, why not consider memory-based methods or EMA methods to alleviate the forgetting of previous knowledge stored in LLMs when you are certain that these knowledge fact are all accurate.**\n\n1. The retrieval-based knowledge editing method offers stronger interpretability, while fine-tuning-based editing doesn\u2019t require additional parameters or entail reasoning costs, thus holding significant practical value. However, in our actual usage, we haven't effectively integrated the advantages of retrieval-based and fine-tuning-based editing methods to address the Knowledge Distortion issues mentioned in this paper.\n\n2. The Multi-Label Editing (MLE) method we propose emperically alleviates knowledge distortion problems. However, we can't precisely define under a specific editing sample what needs generalization transfer and what should be preserved. **Nevertheless, MLE offers an approach to tackle this issue. For instance, in the example \"Joe Biden was born in California \u2192 Vienna,\" we can consider the differences in the knowledge structures of California and Vienna, recall their related semantics within the model [4], and select the semantics that need to be retained.**\n\n---\n\n> **For Knowledge Conflict, it will also be a problem in retrieval-based LLMs, any thoughts to solve this problem according to your experimental results?**\n\nIt's important to note that the Knowledge Conflict discussed in retrieval-based Language Model (LLMs) primarily revolves around conflicting facts retrieved by the model. This differs slightly in concept from the knowledge conflicts resulting from inadequate model knowledge updates discussed in this paper. **From the experimental results presented in this paper, we glean some insights. For instance, during the retrieval process, employing logical rules to identify factors that might lead to uncertainty of model predictions or hallucination could be beneficial.**\n\nHopefully, these insights could address your questions.\n\n---\n\n*[1] Editing Large Language Models: Problems, Methods, and Opportunities (EMNLP 2023)*\n \n*[2] Memory-Based Model Editing at Scale (ICML 2022)*\n\n*[3] Can We Edit Factual Knowledge by In-Context Learning?*\n\n*[4] Dissecting Recall of Factual Associations in Auto-Regressive Language Models (EMNLP 2023)*"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2526/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408446698,
                "cdate": 1700408446698,
                "tmdate": 1700587260938,
                "mdate": 1700587260938,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3T6PHnerUs",
                "forum": "fNktD3ib16",
                "replyto": "1nhtaL1BmX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2526/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2526/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle Reminder and Appreciation for Continued Participation in Manuscript Review Discussion to Reviewer XF6v"
                    },
                    "comment": {
                        "value": "Dear Reviewer XF6v,\n\nWe would like to express our profound gratitude for your insightful review. Following your valuable suggestions, we have replied to your issues and updated our submission.\n\nCould we kindly enquire if our responses and adjustments have adequately resolved your concerns? We are more than happy to answer any further queries or concerns you may have. Thank you once again.\n\nBest Regards,\n\nAuthors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2526/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722334314,
                "cdate": 1700722334314,
                "tmdate": 1700722334314,
                "mdate": 1700722334314,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cLPUarr1lX",
            "forum": "fNktD3ib16",
            "replyto": "fNktD3ib16",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2526/Reviewer_ihfd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2526/Reviewer_ihfd"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the potential pitfalls of knowledge editing for Large Language Models (LLMs). It introduces new benchmark datasets and evaluation metrics to investigate the issues of knowledge conflict and knowledge distortion. The results demonstrate that knowledge editing can lead to unintended consequences and inconsistencies in LLMs. The paper also presents potential solutions and challenges for knowledge editing in LLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The information provides insights into different knowledge editing methods and their performance in various setups.\n2. The paper discusses the concept of knowledge distortion and its impact on language models.\n3. The paper introduces the idea of conflict detection technologies to address potential knowledge discrepancies."
                },
                "weaknesses": {
                    "value": "1. The information provided is quite technical and may be difficult for non-experts to understand.\n2. Some sentences are poorly structured and difficult to comprehend."
                },
                "questions": {
                    "value": "1. What do you think is the fundamental reason for knowledge distortion during knowledge editing for LLMs? How to handle cases of one-to-many knowledge editing?\n2. How do the knowledge editing methods compare to each other in terms of their effectiveness and efficiency, what is the takeaway in method selection?\n3. In Multi-label Edit, how to guarantee the overall conceptual hierarchy among labels?\n\nMinor Issues:\n1. \"Emperically\" should be \"Empirically.\"\n2. \"ROME is effective in both GPT-XL and GPT-J\" should be \"ROME is effective in both GPT2-XL and GPT-J.\"\n3. \"This motivates us to employ conflict detection technologies\" should be \"This motivates the employment of conflict detection technologies.\"\n4. \"Knowledge Conflict has reflected\" should be \"Knowledge Conflict reflects.\"\n5. \"However, it is undesirable for a robust editing method to weaken the preference\" should be \"However, weakening the preference is undesirable for a robust editing method.\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2526/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739097080,
            "cdate": 1698739097080,
            "tmdate": 1699636189043,
            "mdate": 1699636189043,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ev9j5QtzNE",
                "forum": "fNktD3ib16",
                "replyto": "cLPUarr1lX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2526/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2526/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ihfd"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the time and effort you dedicated to reviewing our work. Your constructive comments are much appreciated, and we would like to address each of the questions you raised below.\n\n---\n\n> **What do you think is the fundamental reason for knowledge distortion during knowledge editing for LLMs? How to handle cases of one-to-many knowledge editing?**\n\n1. The primary reason lies in the reliablity and generalization target of traditional knowledge editing evaluation methods, which only ensure the expected performance of the model on edited and similar samples but **overlook the issue of sample imbalance introduced by editing a single sample**. \n**Our proposed Multi-Label Editing (MLE) offers insights to address this issue in editing scenario of one-to-many relations.** Experimental results of MLE demonstrate empirically that by initially mining knowledge structures related to the edited sample (or some relevant tokens) from the original model, knowledge editing can alleviate distortion.\n\n2. **However, our efforts in addressing Knowledge Distortion are not yet sufficient to propose an application scheme coherent with generalization metric for knowledge editing.** According to the definition of generalization, after applying the edit sample \"Joe Biden was born in California \u2192 Vienna,\" the answer to the question \"What country was Joe Biden born in?\" would change from the USA to Austria. In this example, rather than preserving knowledge structures related to California, knowledge structures related to Vienna become more crucial. In summary, knowledge editing methods must determine the extent of generalization on specific edit samples.\n\n---\n\n> **How do the knowledge editing methods compare to each other in terms of their effectiveness and efficiency, what is the takeaway in method selection?**\n\n1. Generally, **the effectiveness of knowledge editing methods can be evaluated through the Reliability metric**. We only need to evaluate the model's performance on edited samples before and after editing. If the probability of predicting the edited target increases after editing, it signifies successful editing procedure. **Efficiency can be evaluated based on the consume for training, average time taken to edit a single sample, and inference time after editing.**\n\n2. In selecting editing methods, we not only consider effectiveness and efficiency but also pay attention to factors like **Locality (whether it affects predictions on other unrelated samples)** and **the Robustness proposed in this paper (whether it introduces inconsistencies in knowledge and disrupts knowledge structures)**.\n\n---\n\n> **In Multi-label Edit, how to guarantee the overall conceptual hierarchy among labels?**\n\nApologies, we currently do not have a viable solution to maintain the conceptual hierarchy among labels. Achieving this likely requires training strategies supporting structured inputs, and there are challenges in evaluating this structurally. We hope to address this issue in future research. \n\nWe hope these responses could address your questions. **Additionally, in the rebuttal revision, we will incorporate the minor issues as you proposed.** Once again, thank you for your comments and suggestions\uff01"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2526/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408285283,
                "cdate": 1700408285283,
                "tmdate": 1700586970691,
                "mdate": 1700586970691,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pGrs5j6YNp",
            "forum": "fNktD3ib16",
            "replyto": "fNktD3ib16",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2526/Reviewer_mRJP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2526/Reviewer_mRJP"
            ],
            "content": {
                "summary": {
                    "value": "This paper comprehensively explores the side effects of knowledge editing for large language models (LLMs), highlighting potential risks in real-world use cases. To facilitate a rigorous evaluation, the researchers introduce two innovative datasets specifically crafted to highlight the unintended consequences of knowledge editing. The study offers solutions for knowledge conflicts and introduces the MLE method to mitigate distortion risks. It also discusses implementation challenges and prospects of knowledge editing for LLMs."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The authors assess the risks associated with current knowledge editing methodologies for LLMs, and introduce two datasets for the purposes of finding potential drawbacks of LLMs.\n\nThis paper presents the MLE method as a straightforward solution to mitigate knowledge distortion risks and address potential knowledge conflicts.\n\nThe challenges and prospects of implementing knowledge editing for LLMs are discussed."
                },
                "weaknesses": {
                    "value": "The paper's scope is limited to factual knowledge editing. \n\nHowever, the presence or absence of knowledge conflicts or distortions in other types of knowledge editing remains unexplored.\n\nThe authors should supplement this part of the paper to make it more comprehensive."
                },
                "questions": {
                    "value": "please refer to the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2526/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698742065186,
            "cdate": 1698742065186,
            "tmdate": 1699636188969,
            "mdate": 1699636188969,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cy5LzI22El",
                "forum": "fNktD3ib16",
                "replyto": "pGrs5j6YNp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2526/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2526/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to review our paper thoroughly and for providing valuable comments and feedback. In response to your concerns, below are the detailed answers.\n\n---\n\n> **The paper's scope is limited to factual knowledge editing, However, the presence or absence of knowledge conflicts or distortions in other types of knowledge editing remains unexplored.**\n\nIn our paper, we only consider specific types of knowledge editing techniques. For other known types of knowledge editing, our considerations are as follows:\n\n- **Unstructured Knowledge Editing:** Methods such as SERAC [1], MEND, and CaliNet [2] support unstructured knowledge editing, which can be achieved through QA-based model editing. The robustness issues of knowledge editing proposed in this paper depend on determining the editing scope and mining knowledge structure. In unstructured knowledge editing, for instance, \u201cWhat is the highest mountain in the world? Mauna Kea \u2192 Mount Everest\u201d, even if no (s, r, o)-formatted triplets can be extracted, traditional metrics used to evaluate individual knowledge edits (e.g., generalization and locality) are easily calculable. However, considering multiple edits as discussed in this paper, as most edit samples may lack any connection, **interference among multiple edits will be challenging to explore in unstructured knowledge editing**. Additionally, issues with the knowledge structure within the model also require exploration using accurate, structured knowledge bases like Wikidata to facilitate further research into the interpretability of knowledge editing and its reliable practical applications in LLMs.\n\n- **Common-sense Editing[3]:** There's limited research on the logical and reasoning chains of common-sense knowledge, **lacking a theoretical basis in defining the core problems explored in this paper** and constructing benchmarks.\n\n---\n\n*[1] SERAC: Memory-based Model Editing at Scale (ICML 2022)*\n\n*[2] CaliNet: Calibrating Factual Knowledge in Pretrained Language Models (EMNLP 2022)*\n\n*[3] Editing Common Sense in Transformers (EMNLP 2023)*"
                    },
                    "title": {
                        "value": "Response to Reviewer mRJP"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2526/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408128958,
                "cdate": 1700408128958,
                "tmdate": 1700408234440,
                "mdate": 1700408234440,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]