[
    {
        "title": "How do skip connections affect Graph Convolutional  networks  with graph sampling? A theoretical analysis on generalization"
    },
    {
        "review": {
            "id": "s1542y8WuA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8606/Reviewer_YYVW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8606/Reviewer_YYVW"
            ],
            "forum": "J2pMoN2pon",
            "replyto": "J2pMoN2pon",
            "content": {
                "summary": {
                    "value": "This paper delves into the generalization of a two-layer GNN that takes both the residual connection and sampling into consideration. This paper is very hard to digest due to writting issue."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The orginal motivation of this paper is very interesting and important. But the theoritical part needs improvment."
                },
                "weaknesses": {
                    "value": "The paper looks fine until Section 3.3. \n\nHowever, beyond Section 3.3, it gives the impression that the authors may not have fully understand Allen-Zhu & Li's work before extending it from a residual-connected MLP to a residual-connected GCN. Consequently, the notations, proofs, and overall presentation become challenging to digest:\n\n1. The presentation lacks clarity, with numerous notations introduced without proper explanation. For instance, in the paragraph surrounding Equations 6 and 7, the authors transform the original 2-layer residual-connected GCN into a new format, but it remains unclear how these two formulations correspond. Questions arise regarding the meaning of symbols like $p\\_\\mathcal{F}$ and $p\\_\\mathcal{G}$. Additionally, it's unclear what $\\sum_{i\\in [p\\_\\mathcal{F}]}$ and $\\sum_{i\\in [p\\_\\mathcal{G}]}$ represent in the context of GCN. It's also unclear whether $w_{r,i}^\\star$ refers to the i-th or r-th row/column of a weight matrix. This section is not well organized, making it exceptionally challenging to comprehend.\n\n2. The notation on Page 6, particularly at the top under the algorithm section, is even more convoluted. The authors introduce numerous notations, but their purpose and connection to GCN are unclear. I have no idea how these notations relate to the GCN.\n\n3. There appear to be errors in the paper. Detailed questions and concerns have been raised, which should be addressed for clarity and accuracy.\n\n4. 2-layer GNN is not deep enough comparing to existing works. Especially the authors argue existing works' considered model is not deep ..."
                },
                "questions": {
                    "value": "1. On Page 5, at the top, we have this $d_1 \\sqrt{d_i/d_j}$. Where does this $d_1$ originate from? Why does the node sampling probability always depend on this value?\n\n2. According to your sampling method, neither $A^\\star$ nor the \"expectation of the sampled adjacency matrices\" is identical to the original adjacency matrix $A$. Won't this introduce bias during training? In other words, your objective differs from the original objective function. In this case, how can we ensure generalization?\n\n3. What does the first term on page 6, in the first line, represent? Without a clear explanation, I cannot grasp the impact of $A^\\star$ on the $\\epsilon_0$ of Thm3.2.\n\n4. Why C is not trainable in Eq. 1-4? Since this is linear-regression using square loss, cannot we just think this C as identity matrix?\n\n5. What is the $r_w$ and $r_v$ in Eq. 11-12?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8606/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697236748649,
            "cdate": 1697236748649,
            "tmdate": 1699637076889,
            "mdate": 1699637076889,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pusMumsRKx",
                "forum": "J2pMoN2pon",
                "replyto": "s1542y8WuA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Weakness 1: \n The presentation lacks clarity, with numerous notations introduced without proper explanation. For instance, in the paragraph surrounding Equations 6 and 7, the authors transform the original 2-layer residual-connected GCN into a new format, but it remains unclear how these two formulations correspond. Questions arise regarding the meaning of symbols like $p_\\mathcal{F}$ and $p_\\mathcal{G}$. Additionally, it's unclear what $\\sum_{i \\in [p_\\mathcal{F}]}$ and $\\sum_{i \\in [p_\\mathcal{G}]}$ represent in the context of GCN. It's also unclear whether $w_{r,i}^{*}$ refers to the i-th or r-th row/column of a weight matrix. This section is not well organized, making it exceptionally challenging to comprehend.\n\n## Answer Weakness 1: \n**The role of equations (6) and (7)**. We first need to clarify that we do not transform the learner network, which is 2-layer residual-connected GCN, to equations (6) and (7). The concept class in (6) and (7) is introduced for the theoretical generalization analysis only. We first define many target functions in the form of (6) and (7) to predict the labels. These functions have the same form but different parameters of $a_{\\mathcal{F}, r, i}^*$, $a_{\\mathcal{G}, r, i}^*$, $w^*_{r,i}$, $v_{r,i}^*$. Then OPT is defined as the smallest label prediction error of the best target function  (over the choices of parameters) in the concept class. OPT decreases as the concept class becomes more complex, such as increasing the integers $p_\\mathcal{F}, p_\\mathcal{G}$ and making activations  $\\mathcal{F}$ and $\\mathcal{G}$ more complex.  Note that the definition of OPT itself is irrelevant to the GCN learning, it is purely defined from an approximation perspective.\n\n  Then when we analyze learning 2-layer GCN, we show that the label prediction error by the learned 2-layer GCN model is close to $10$ OPT. Intuitively, that means the learned GCN model can  approximate almost the best target function in the form of (6) and (7). \n\n  This approach, i.e., first defining target functions to approximate labels and then quantifying generalization performance with respect to  the best target function,  follows from those in xxxxxx. (cite Allen Zhu's and Hongkang's paper).\n\n**The meaning of  $p_\\mathcal{F}, p_\\mathcal{G}$, $\\sum_{i \\in [p_\\mathcal{F}]}$**\n$p_\\mathcal{F}, p_\\mathcal{G}$ are two positive integers. We want to say $F_{A^*}$ and $G_{A^*}$ are sum of  $p_\\mathcal{F}$ and $p_\\mathcal{G}$ functions respectively.  We realized that the notation of $\\sum_{i \\in [p_\\mathcal{F}]}$ $\\sum_{i \\in [p_\\mathcal{G}]}$can be confusing.\nWe revised them to $\\sum_{i=1}^{p_\\mathcal{F}}$ and $\\sum_{i=1}^{p_\\mathcal{G}}$.\n\nIn $a_{\\mathcal{F}, r, i}^*, a_{\\mathcal{G}, r, i}^*$, $w_{r, i}^*$, $v_{r, i}^*$, the subscript $r, i$ are not indices for vectors or matrices. We use $r,i$ to indicate that for every function $i$ and the $r$th row, there are different sets of coefficients $a_{\\mathcal{F}}$, $a_{\\mathcal{G}}$, vectors $w^*$ and $v^*$. For example, for ever $r$ and $i$, $w_{r, i}^*$ is a vector in  $\\mathbb{R}^{d}$,  and  $v_{r, i}^*$ is a vector in $\\mathbb{R}^{k}$. We revised in the paper that accordingly that \n \n  ``Given $r$, $i$, the scalers $a_{\\mathcal{F}, r, i}^*, a_{\\mathcal{G}, r, i}^*$ are in $[-1,1]$, the vectors     $w_{r, i}^*$ are in  $\\mathbb{R}^{d}$,  and  $v_{r, i}^*$ are in $\\mathbb{R}^{k} $. For simplicity, we assume $\\left \\| w_{r, i}^* \\right \\|=\\left \\| v_{r, i}^* \\right \\| =\\frac{1}{\\sqrt{2} } $ for all $r$, $i$.  ''\n We have revised this section, with major changes highlighted.\n\n##  Weakness 2:\nThe notation on Page 6, particularly at the top under the algorithm section, is even more convoluted. The authors introduce numerous notations, but their purpose and connection to GCN are unclear. I have no idea how these notations relate to the GCN.\n\n##  Answer Weakness 2:\nWe realized that some notations are not necessary or can be simplified for presenting the main theorem. We removed some unnecessary notations and added the formal definition of model complexity constants $C_{\\varepsilon}$ and sample complexity constants $C_{\\mathfrak{s}}$ in (8) and (9). We also simplified the assumptions of Theorem 3.2 with necessary notations only. \n\n## Weakness 3:\nThere appear to be errors in the paper. Detailed questions and concerns have been raised, which should be addressed for clarity and accuracy.\n\n## Answer Weakness 3:\nWe have addressed the specific comments in the following questions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711553416,
                "cdate": 1700711553416,
                "tmdate": 1700731517463,
                "mdate": 1700731517463,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cAE06gSexg",
                "forum": "J2pMoN2pon",
                "replyto": "s1542y8WuA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Weakness 4:\n2-layer GNN is not deep enough comparing to existing works. Especially the authors argue existing works' considered model is not deep ...\n\n## Answer Weakness 4:\nWe understand that the reviewer would like to see a theoretical analysis of a network model that is more complicated than the current one. We agree that this is a valid point, and we would like to explore that direction in the future.  However, we first need to stress that **the simple model we consider in this paper is already advancing the state-of-the-art model for the generalization analysis for graph neural networks**. For example, recent works [2, 6] are published in ICML and NIPS that build the theoretical analysis on two-layer neural networks. In fact, the work [1] only considers one-layer GCNs.  Only [5] consider deeper networks, but they typically do not include non-linear activation functions.\n\n1. To the best of our knowledge, the only approach that considers graph neural networks, or even the general neural networks with more than two layers theoretically, is the neural tangent kernel (NTK) approach. However, the  NTK approach considers the scenario that the neural network stays close to the linearization around the initialization and does not characterize the nonlinear activations. \nTherefore, there is a performance gap between practical neural networks and the NTK approach, as shown in [3, 4]. Thus, we do not follow this line. In contrast, we directly analyze the interactions of nonlinear activations across layers.\n\n2. We need to emphasize that **our paper makes novel theoretical contributions**, despite the two-layer model. Specifically,   our paper provides the first theoretical analysis of GCNs with skip connection using graph sampling.  We discover that skip connections necessitate varying sampling requirements across different layers. This finding challenges the conventional sampling approach that is uniform to different layers and suggests the need for layer-specific sampling considerations.\n\n3. **We have verified our theoretical insights in deep GCNs on large datasets.** For example, in Figure 2(a), we train an 8-layer Jumping Knowledge Network   GCN with concatenation layer aggregation on the  Ogbn-Arxiv dataset.  We verify in this deep GCN that graph sampling in shallow layers has a more significant impact than graph sampling in deeper layers.\n\n## Reference\n1. Zhang, Shuai, et al. \"How unlabeled data improve generalization in self-training? A one-hidden-layer theoretical analysis.\" International Conference on Learning Representations. 2021.\n2. Hongkang Li, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. Generalization guarantee of\ntraining graph convolutional networks with graph topology sampling. In International Conference\non Machine Learning (ICML), pp. 13014\u201313051. PMLR, 2022a.\n3. Chen, Shuxiao, Hangfeng He, and Weijie Su. \"Label-aware neural tangent kernel: Toward better generalization and local elasticity.\" Advances in Neural Information Processing Systems 33 (2020): 15847-15858.\n4. Arora, Sanjeev, et al. \"Harnessing the power of infinitely wide deep nets on small-data tasks.\" arXiv preprint arXiv:1910.01663 (2019).\n5. Keyulu Xu, Mengshi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural\nnetworks: Implicit acceleration by skip connections and more depth. In International Conference\non Machine Learning (ICML). PMLR, 2021.\n6. Cao, Yuan, et al. \"Benign overfitting in two-layer convolutional neural networks.\" Advances in neural information processing systems 35 (2022): 25237-25250."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718539558,
                "cdate": 1700718539558,
                "tmdate": 1700731543993,
                "mdate": 1700731543993,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QiP8q5AO2v",
                "forum": "J2pMoN2pon",
                "replyto": "s1542y8WuA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Question 1:\nOn Page 5, at the top, we have this $d_1 \\sqrt{\\frac{d_i}{d_j}}$. Where does this $d_1$ originate from? Why does the node sampling probability always depend on this value?\n\n## Answer Question 1:\n1. We follow the same assumption on node degrees as that in [1]. Specifically, the node\ndegrees can be divided into L   groups, with each group having $N_l$ nodes.   The degrees of all these $N_l$ nodes in group $l$ are in the order of $d_l$, i.e., between $cd_l$ and $Cd_l$ for\nsome constants $c \\leq  C$. $d_l$ is order-wise smaller than $d_{l+1}$, i.e., $d_l = o(d_{l+1})$. Then $d_1$ is the smallest degree (order-wise) of these degree groups.\n\n2. Our core sampling concept revolves around sampling edges between nodes of lower degrees with higher probability. Because the edges between lower-degree nodes correspond to larger entries in $A$, we want to sample larger entries with a higher probability and smaller entries with a lower probability. In our sampling strategy, we just divide entries in $A$ into two parts, the parts with larger values are sampled with $1-p_{ij}^k$, and the group with smaller values is sampled with $p_{ij}^k$, where $p_{ij}^k$. The size of the larger part, which is selected as $d_1\\sqrt{d_i/d_j}$, is selected to ensure that $ \\left \\| A^* \\right \\| _1 $ is $O(1)$, which is required in our generalization analysis. It does not have to the exact value of $d_1\\sqrt{d_i/d_j}$, any value in the order of $d_1\\sqrt{d_i/d_j}$ will not change our order-wise analysis.\n\n3. We want to emphasize that although we set this value to simplify our theoretical analysis, **the main idea of  \nsampling edges between nodes of lower degrees with higher probability are preserved in our sampling strategy.**\n\n## Question 2:\nAccording to your sampling method, neither $A^*$ nor the \"expectation of the sampled adjacency matrices\" is identical to the original adjacency matrix $A$. Won't this introduce bias during training? In other words, your objective differs from the original objective function. In this case, how can we ensure generalization?\n\n## Answer Question 2:\n1. We agree with the statement that neither $A^*$ nor the \"expectation of the sampled adjacency matrices\" is identical to the original adjacency matrix $A$.'' That is indeed one of our main messages that graph sampling does not have to get an unbiased estimator of $A$. We only need to make sure the resulting $A^*$ can accurately characterize the data correlation. $A^*$ can be sparse than $A$. Because the original adjacency matrix $A$ of the graph often contains redundant information, learning using   $ A^* $ can perform comparable to, or even better than $ A $. That explains why graph sampling can achieve desirable generalization.  \n\n2. We also verified the redundancy and the effectiveness of $A^*$ empirically.  In  Figure 2a,  when both $ q_1 $ and $ q_2 $ are above 0.6, which corresponds to sparse effective adjacency matrix $ A^* $, the test error aligns closely with results using the original $ A $ (where $ q_1 $ and $ q_2 $ are 1). This implies that our model, utilizing roughly 60\\% of the graph, performs comparably to using the full graph. \n\n## Question 3:\nWhat does the first term on page 6, in the first line, represent? Without a clear explanation, I cannot grasp the impact of $A^*$ on the $\\epsilon_0$ of Thm3.2.\n\n## Answer Question 3:\n1. We included the definition of the model complexity constants $C_{\\varepsilon}$ and sample complexity constants $C_\\mathfrak{s}$ in (8) and (9). The model complexity constants $\\mathfrak{C}_\\varepsilon$ and sample complexity constants $\\mathfrak{C}_s$ those in    \\cite{li2022generalization} (Section 1.2) and  \\cite{NEURIPS2019_5857d68c} (Section 4). They represent the required number of model parameters and training samples to learn $\\phi$ up to $\\epsilon$ error, respectively.\n\n2.  When $\\left \\| A^* \\right \\|_1 $ increases, both the model complexity constants and sample complexity constants of functions $\\mathcal{F}$ and $\\mathcal{G}$ incease. Then $\\epsilon_0$ increases, indicating a larger generalization error. Theorem 3.2 also indicates the model complexity $M_0$ (the number of neurons)  and the sample complexity $N_0$ (the number of labeled nodes) both increase. We added this discussion to the paper.\n\n## Reference\n1. Li, Hongkang, et al. \"Generalization guarantee of training graph convolutional networks with graph topology sampling.\" International Conference on Machine Learning. PMLR, 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721822435,
                "cdate": 1700721822435,
                "tmdate": 1700731604674,
                "mdate": 1700731604674,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XkoR3jiM8L",
                "forum": "J2pMoN2pon",
                "replyto": "s1542y8WuA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Question 4:\nWhy $C$ is not trainable in Eq. 1-4? Since this is linear-regression using square loss, cannot we just think this $C$ as identity matrix?\n\n## Answer Question 4:\n1. We would like to clarify that an untrained output is a typical setting for theoretical analysis for generalization.  Existing works [1, 2, 3] on top conferences also made this assumption in their analyses.  \n\n2. In our study, the decision to sample $C_{i,j}$ from $\\mathcal{N}(0, 1/m)$ is primarily to simplify the analysis, ensuring that the norm $\\left \\| C \\right \\| \\leq 1$, as detailed in Appendix section B.1. \nFurthermore, it is indeed feasible to consider $C$ as an identity matrix scaled by $1/m$. This alternative representation would effectively function as a mean pooling layer, providing a similar effect while maintaining the simplicity and boundedness required for theoretical analysis.\n\n## Question 5:\nWhat is the $r_w$ and $r_v$ in Eq. 11-12?\n\n## Answer Question 5:\nI think you mean  $\\tau_w$ and $ \\tau_v $. We prove that $ \\|W_t\\|_2 $ and $ \\|V_t\\|_2 $  are bounded by $\\tau_w$ and $\\tau_v$ in Appendix section B.3. Because $\\tau_w$ and $\\tau_v$ can be bounded by model complexity and sample complexity constants to simply the presentation of the paper, we removed $\\tau_w$ and $\\tau_v$ from the main text and replace them with their bounds using $ C_F  $ and $ C_G  $.\n\n## Reference\n1. Allen-Zhu, Zeyuan, and Yuanzhi Li. \"What can resnet learn efficiently, going beyond kernels?.\" Advances in Neural Information Processing Systems 32 (2019).\n2. Li, Hongkang, et al. \"Generalization guarantee of training graph convolutional networks with graph topology sampling.\" International Conference on Machine Learning. PMLR, 2022.\n3. Zhang, Shuai, et al. \"Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural Networks.\" In International Conference on Learning (ICLR), 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723366039,
                "cdate": 1700723366039,
                "tmdate": 1700731652684,
                "mdate": 1700731652684,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TqrvTvTowG",
            "forum": "J2pMoN2pon",
            "replyto": "J2pMoN2pon",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8606/Reviewer_cdgo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8606/Reviewer_cdgo"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors try to figure the relationship between graph sampling and skip-connections in GCN. Based on this motivation, many solid theories have been given. Also they validate the theoretical results on benchmark datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper gives a different theoretical perspective on the relationship between graph sampling and different layers of GCNs.\n\n2. It provides very solid theoretical analysis. It is convincing."
                },
                "weaknesses": {
                    "value": "This work is far from the real setting of graph neural networks.\n1. In this paper, they assume all with perfectly homophilous graphs. Actually, it is not going to happen in heterophilous graphs. If we have perfectly homophilous graphs, the stationary point will make the generalization happen. \n\n2. The setting of two-layer skip connections is oversimple. Two graph convolutions only access two-hop neighborhoods. Thus, I cannot imagine how these conclusions can inspire this community (Graph Neural Network).\n\n3. The dense graph is not a usual condition. Even the transformer generates an implicit graph from batch data (then it is sparse from the global point.). If the graph is dense and perfectly homophilous, then the graph convolution basically equals making every node its' class center."
                },
                "questions": {
                    "value": "I have some concerns about how this work can help graph neural network (or transformer) community."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8606/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802045135,
            "cdate": 1698802045135,
            "tmdate": 1699637076774,
            "mdate": 1699637076774,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sw4WL4ADZe",
                "forum": "J2pMoN2pon",
                "replyto": "TqrvTvTowG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Weakness 1: \nIn this paper, they assume all with perfectly homophilous graphs. Actually, it is not going to happen in heterophilous graphs. If we have perfectly homophilous graphs, the stationary point will make the generalization happen.\n## Answer Weakness 1: \n1. We need to emphasize that **reaching a stationary point of the training optimization problem does not automatically lead to generalization**. Overfitting is a typical counterexample. If a model is overly complex or trained with insufficient sample complexity, it might overfit the training data to achieve zero training loss but fail to generalize well to new data.\n2. **We do not assume homophilous graphs in this paper, and our theoretical results apply to both homophilous graphs and heterophilous graphs**. Our main result Theorem 3.2 indicates that the learned model can predict labels with a risk about 10 OPT, where OPT is the smallest approximation error of the target function in the form of (6) and (7). It is possible that, when everything else is the same,  OPT is smaller if the graph is homophilous rather than heterophilous, but our Theorem 3.2 holds despite whether OPT is small or large. Moreover, our insight in Lemma 3.1 that graph sampling in different layers contributes to the output approximation differently. This sight applies to both homophilous graphs and heterophilous graphs. \n3. We agree with the reviewer that GCNs have been mostly employed in homophilous graphs in the literature, maybe because they have better generalization performance in homophilous graphs. However, our results do not require the assumptions of homophilous graphs and can characterize the GCN generalization performance (no matter it is good or bad) in both homophilous graphs and heterophilous graphs, as discussed in the previous point. \n\n## Weakness 3: \nThe dense graph is not a usual condition. Even the transformer generates an implicit graph from batch data (then it is sparse from the global point.). If the graph is dense and perfectly homophilous, then the graph convolution basically equals making every node its' class center.\n## Answer Weakness 3: \nWe do not assume the graph is dense. We realize this confusion may result from our statement in the original submission that \n\"This research illustrates that the target functions rely on $A^*$ instead of the original graph's adjacency matrix $A$.\" What we should have said is that $A^*$ can be sparser than $A$. We did not assume that the graph represented by $A$ is inherently dense or sparse. Consequently, we have revised the original sentence to state that \u201c$A^*$ can be sparser than $A$.\u201d"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718568547,
                "cdate": 1700718568547,
                "tmdate": 1700731347171,
                "mdate": 1700731347171,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BiGnZMtHuF",
                "forum": "J2pMoN2pon",
                "replyto": "TqrvTvTowG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Weakness 2:\nThe setting of two-layer skip connections is oversimple. Two graph convolutions only access two-hop neighborhoods. Thus, I cannot imagine how these conclusions can inspire this community (Graph Neural Network).\n## Answer Weakness 2:\nWe understand that the reviewer would like to see a theoretical analysis of a network model that is more complicated than the current one. We agree that this is a valid point, and we would like to explore that direction in the future.  However, we first need to stress that **the simple model we consider in this paper is already advancing the state-of-the-art model for the generalization analysis for graph neural networks**. For example, recent works [2, 6] are published in ICML and NIPS that build the theoretical analysis on two-layer neural networks. In fact, the work [1] only considers one-layer GCNs.  Only [5] consider deeper networks, but they typically do not include non-linear activation functions.\n\n1. To the best of our knowledge, the only approach that considers graph neural networks, or even the general neural networks with more than two layers theoretically, is the neural tangent kernel (NTK) approach. However, the  NTK approach considers the scenario that the neural network stays close to the linearization around the initialization and does not characterize the nonlinear activations. \nTherefore, there is a performance gap between practical neural networks and the NTK approach, as shown in [3, 4]. Thus, we do not follow this line. In contrast, we directly analyze the interactions of nonlinear activations across layers.\n\n2. We need to emphasize that **our paper makes novel theoretical contributions**, despite the two-layer model. Specifically,   our paper provides the first theoretical analysis of GCNs with skip connection using graph sampling.  We discover that skip connections necessitate varying sampling requirements across different layers. This finding challenges the conventional sampling approach that is uniform to different layers and suggests the need for layer-specific sampling considerations.\n\n3. **We have verified our theoretical insights in deep GCNs on large datasets.** For example, in Figure 2(a), we train an 8-layer Jumping Knowledge Network   GCN with concatenation layer aggregation on the  Ogbn-Arxiv dataset.  We verify in this deep GCN that graph sampling in shallow layers has a more significant impact than graph sampling in deeper layers.\n\n## Reference\n1. Zhang, Shuai, et al. \"How unlabeled data improve generalization in self-training? A one-hidden-layer theoretical analysis.\" International Conference on Learning Representations. 2021.\n2. Hongkang Li, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. Generalization guarantee of\ntraining graph convolutional networks with graph topology sampling. In International Conference\non Machine Learning (ICML), pp. 13014\u201313051. PMLR, 2022a.\n3. Chen, Shuxiao, Hangfeng He, and Weijie Su. \"Label-aware neural tangent kernel: Toward better generalization and local elasticity.\" Advances in Neural Information Processing Systems 33 (2020): 15847-15858.\n4. Arora, Sanjeev, et al. \"Harnessing the power of infinitely wide deep nets on small-data tasks.\" arXiv preprint arXiv:1910.01663 (2019).\n5. Keyulu Xu, Mengshi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural\nnetworks: Implicit acceleration by skip connections and more depth. In International Conference\non Machine Learning (ICML). PMLR, 2021.\n6. Cao, Yuan, et al. \"Benign overfitting in two-layer convolutional neural networks.\" Advances in neural information processing systems 35 (2022): 25237-25250.\n\n## Answer Question 1:\nI think we can answer this question in Weakness 2."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723724433,
                "cdate": 1700723724433,
                "tmdate": 1700731371118,
                "mdate": 1700731371118,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CgTS6bkxkJ",
            "forum": "J2pMoN2pon",
            "replyto": "J2pMoN2pon",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8606/Reviewer_8ATu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8606/Reviewer_8ATu"
            ],
            "content": {
                "summary": {
                    "value": "The authors analyze the effect of subsampling the symmetrically normalized adjacency matrix in node regression tasks for two layered GCNs with a single skip connection. Arguing that the second layer is less affected by a \"bad\" sampling than the first layer, the authors propose an algorithm that samples different matrices for different layers. The sampling strategy is based on sampling edges with higher probability from low-degree nodes. Finally, the authors provide theoretical results for GCNs trained with SGD that show sample complexity bounds to achieve near-optimal performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The analyzed problem itself is highly interesting; While many papers focus on expressivity, this papers gives generalization and complexity bounds for GNNs, which is a hard and interesting problem.\n- The experimental results seem to match the theory well\n- Proofs backup theoretical results"
                },
                "weaknesses": {
                    "value": "- The authors tend to overstate their results, for example, it is often mentioned that the work would analyze a large class of graph learning models, while only two-layer GCNs are analyzed, where the final prediction layer is not trained. This is not a realistic scenario. While frequently, the GCN layers are not trained the final layer is, up to the Reviewers' knowledge, always trained to be able to linearly separate classes.\n- Another example is in Section 3.1, where the authors highlight their own work in comparison to others by mentioning that other works only accommodate \"shallow GCNs\". However, the cited works also analyze two-layered GCNs.\n- Some of the assumptions seem highly restrictive: For example in section 3.3. the function $\\mathcal{F}$ and $\\mathcal{G}$ are assumed to be smooth functions on $\\mathbb{R}^{d \\times N} \\times \\mathbb{R}^{N \\times N}$. However, as the domain is the graph domain, it is not clear whether this is a reasonable assumption as this could break the permutation equivariance. \n- Another example is the final (untrained) layer in Equation 5, which is simply a matrix multiplication. Thus, does not satisfy universal approximation properties.\n- Many notations and definitions are missing, which leads to confusion. For example, Section 3.4 is unclear. \n- The work lacks clarity, and often explanations and intuitions are not given. For example, in their main results, Lemma 3.1 and Theorem 3.2 the assumptions of the results are not clear. The results are also not well-presented."
                },
                "questions": {
                    "value": "While the work analyzes an interesting theoretical question, the writing and presentation lack clarity and mathematical preciseness. Which are necessary to be able to value the presented results. I would recommend the authors to go over their work again, and make sure that every notion is well-defined and intuitions are given. \n\nSome more Questions:\n-  What is OPT in Equation 13? $\\mathcal{H}_{n,A^*}$ is defined as the target function, while $y_n$ is the label of node $n$. How is it possible that $OPT$ is non-zero?\n-  In Lemma 3.1 and Theorem 3.2: It is not clear with respect to which event the probability is taken.\n- In Theorem 3.2: Could the authors present the assumptions better or give more intuitions?\n- Why do the authors average in Equation 14 over all iterations of the SGD steps?\n- How is $(X,y_n)$ sampled, and how is $\\mathcal{D}$ defined?\n- Could you elaborate on many assumptions, e.g., Section 3.4: it doesn't seem clear that the norms of the learned weight matrices are uniformly bounded."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8606/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8606/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8606/Reviewer_8ATu"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8606/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699367410143,
            "cdate": 1699367410143,
            "tmdate": 1699637076631,
            "mdate": 1699637076631,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xqa02slosk",
                "forum": "J2pMoN2pon",
                "replyto": "CgTS6bkxkJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Weakness 1:\nThe authors tend to overstate their results, for example, it is often mentioned that the work would analyze a large class of graph learning models, while only two-layer GCNs are analyzed, where the final prediction layer is not trained. This is not a realistic scenario. While frequently, the GCN layers are not trained the final layer is, up to the Reviewers' knowledge, always trained to be able to linearly separate classes.\n## Answer Weakness 1:\nWe would like to clarify that an untrained output is a typical setting for theoretical analysis for generalization. Existing works [1, 2, 3, 4, 5] on top conferences also made this assumption in their analyses. **Meanwhile, although we make such an assumption, the problem we solve is still challenging and significant.** \n1. Existing works on the generalization of GNNs cannot characterize the influence of graph sampling with skip connections. \n2. Our paper provides the first theoretical analysis of GCNs with skip connection using graph sampling. We discover that skip connections necessitate varying sampling requirements across different layers. This finding challenges the conventional sampling approach that is uniform to different layers and suggests the need for layer-specific sampling considerations.\n\n## Reference\n1. Allen-Zhu, Zeyuan, Yuanzhi Li, and Yingyu Liang. \"Learning and generalization in overparameterized neural networks, going beyond two layers.\" Advances in neural information processing systems 32 (2019).\n2. Allen-Zhu, Zeyuan, Yuanzhi Li, and Zhao Song. \"A convergence theory for deep learning via over-parameterization.\" International conference on machine learning. PMLR, 2019.\n3. Cao, Yuan, et al. \"Benign overfitting in two-layer convolutional neural networks.\" Advances in neural information processing systems 35 (2022): 25237-25250.\n4. Li, Hongkang, et al. \"Generalization guarantee of training graph convolutional networks with graph topology sampling.\" International Conference on Machine Learning. PMLR, 2022.\n5. Zhang, Shuai, et al. \"Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural Networks.\" In International Conference on Learning (ICLR), 2023.\n\n\n## Weakness 3:\nSome of the assumptions seem highly restrictive: For example in section 3.3, the function $\\mathcal{F}$ and $\\mathcal{G}$ are assumed to be smooth functions on $\\mathbb{R}^{d \\times N} \\times \\mathbb{R}^{N \\times N}$. However, as the domain is the graph domain, it is not clear whether this is a reasonable assumption as this could break the permutation equivariance.\n\n\n## Answer Weakness 3:\nWe want to clarify that our concept class of target functions satisfies the permutation equivariance property. That is because when we permutate the node indices, the corresponding adolescences matrix is also permuted. To see this, consider a toy example of a two-node graph. }\nLet $N=2$, given the feature matrix ($X\\in\\mathbb{R}^{d \\times 2}$ ) and adjacency matrix ( $A\\in\\mathbb{R}^{2 \\times 2}$ ):\n$$\nX = \\left[ \\begin{array}{cc}\nx_1 & x_2\n\\end{array} \\right], \nA = \\left[ \\begin{array}{cc}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{array} \\right] \n$$\n\n\n\nApplying the function \\( f \\) to the product \\( XA \\) gives:\n$$\n f(XA) = \\left[ \\begin{array}{cc}\nf(a_{11}x_1 + a_{12}x_2), & f(a_{21}x_1 + a_{22}x_2)\n\\end{array} \\right]\n$$\nwhere the first entry of $f(XA)$ corresponds to the output of $x_1$, and the first entry of $f(XA)$ corresponds to the output of $x_2$.\n\nIf we swap the node indices of these two nodes such that $x_1=x_2$ and $x_2=x_1$, then the feature matrix and the new adjacency matrix become \n$$\n X' = \\left[ \\begin{array}{cc}\nx_2, & x_1\n\\end{array} \\right], \nA' = \\left[ \\begin{array}{cc}\na_{22}, & a_{21} \\\\\na_{12}, & a_{11}\n\\end{array} \\right] \n$$\nThe function $ f $  applied to the product $ X'A' $  results in:\n$$ \nf(X'A') = \\left[ \\begin{array}{cc}\nf(a_{12}x_1 + a_{22}x_2), & f(a_{11}x_1 + a_{21}x_2)\n\\end{array} \\right] \n$$ \nwhere the first entry of $f(X'A')$ corresponds to the output of $x'_1$, which equals $x_2$, and the first entry of $f(X'A')$ corresponds to the output of $x'_2$, which is $x_1$.\nThis demonstrates that $ f $ is permutation equivariant."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725715284,
                "cdate": 1700725715284,
                "tmdate": 1700730813110,
                "mdate": 1700730813110,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iN2OLplMS4",
                "forum": "J2pMoN2pon",
                "replyto": "CgTS6bkxkJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Weakness 4:\nAnother example is the final (untrained) layer in Equation 5, which is simply a matrix multiplication. Thus, does not satisfy universal approximation properties.\n## Answer Weakness 4:\nAs we said in Weaknesses 1, we would like to clarify that an untrained output is a typical setting for theoretical analysis for generalization. Existing works on top conferences also made this assumption in their analyses.\n\nThe universal approximation property [1, 2] refers to the expressive power of a multi-layer feedforward neural network that the network can approximate any real-valued continuous function to any desired degree of accuracy. We agree that the network in Equation 5 may not be able to approximate all the possible continuous functions. **However, our work focuses on analyzing the convergence and generalization of GNN with skip connections, which is a different aspect from function approximation.** Existing works that study the neural network using the universal approximation property usually do not involve the analysis of convergence and generalization. Our analysis shows that learning a two-hidden-layer GNN with a skip connection using graph sampling can achieve a label prediction error similar to the best prediction error among a concept class of target functions.\n\n## Reference\n1. Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. \"Multilayer feedforward networks are universal approximators.\" Neural networks 2.5 (1989): 359-366.\n2. Lu, Zhou, et al. \"The expressive power of neural networks: A view from the width.\" Advances in neural information processing systems 30 (2017).\n\n## Weakness 5:\nMany notations and definitions are missing, which leads to confusion. For example, Section 3.4 is unclear.\n## Answer Weakness 5:\nWe have already simplified our notations and definitions and deleted some complex functions. \n\n## Weakness 6:\nThe work lacks clarity, and often explanations and intuitions are not given. For example, in their main results, Lemma 3.1 and Theorem 3.2 the assumptions of the results are not clear. The results are also not well-presented.\n\n## Answer Weakness 6:\nWe have simplified the assumptions in Lemma 3.1 and Theorem 3.2 to improve the presentation.  We update  Lemma 3.1 and Theorem 3.2.\n\nMoreover, to clarify the assumption of Lemma 3.1, we added the following to the paper\n``Note that $\\widetilde{\\Theta}\\left(\\alpha  C_\\mathfrak{s}(\\mathcal{G})\\right)<1$, then the upper bound for $p_{ij}^2$ is higher than that for $p_{ij}^1$ in the assumption. That means the sampling for the first hidden layer must focus more on low-degree edges, while such a requirement is  relaxed in the second layer.''\nTo better interpret Theorem 3.2, we added the following discussion,\n\"Moreover, when $\\|A^*\\|_1$ increases,  and  $C_s$, and $\\epsilon_0$ all increase.  Theorem 3.2  indicates the model complexity $M_0$,  $N_0$, and the generalization error $\\epsilon$ all increasing, indicating worse prediction performance.\""
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726006013,
                "cdate": 1700726006013,
                "tmdate": 1700730927542,
                "mdate": 1700730927542,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fTHgsypBZC",
                "forum": "J2pMoN2pon",
                "replyto": "CgTS6bkxkJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Weakness 2:\nAnother example is in Section 3.1, where the authors highlight their own work in comparison to others by mentioning that other works only accommodate \"shallow GCNs\". However, the cited works also analyze two-layered GCNs.\n## Answer Weakness 2:\nWe understand that the reviewer would like to see a theoretical analysis of a network model that is more complicated than the current one. We agree that this is a valid point, and we would like to explore that direction in the future.  However, we first need to stress that **the simple model we consider in this paper is already advancing the state-of-the-art model for the generalization analysis for graph neural networks**. For example, recent works [2, 6] are published in ICML and NIPS that build the theoretical analysis on two-layer neural networks. In fact, the work [1] only considers one-layer GCNs.  Only [5] consider deeper networks, but they typically do not include non-linear activation functions.\n\n1. To the best of our knowledge, the only approach that considers graph neural networks, or even the general neural networks with more than two layers theoretically, is the neural tangent kernel (NTK) approach. However, the  NTK approach considers the scenario that the neural network stays close to the linearization around the initialization and does not characterize the nonlinear activations. \nTherefore, there is a performance gap between practical neural networks and the NTK approach, as shown in [3, 4]. Thus, we do not follow this line. In contrast, we directly analyze the interactions of nonlinear activations across layers.\n\n2. We need to emphasize that **our paper makes novel theoretical contributions**, despite the two-layer model. Specifically,   our paper provides the first theoretical analysis of GCNs with skip connection using graph sampling.  We discover that skip connections necessitate varying sampling requirements across different layers. This finding challenges the conventional sampling approach that is uniform to different layers and suggests the need for layer-specific sampling considerations.\n\n3. **We have verified our theoretical insights in deep GCNs on large datasets.** For example, in Figure 2(a), we train an 8-layer Jumping Knowledge Network   GCN with concatenation layer aggregation on the  Ogbn-Arxiv dataset.  We verify in this deep GCN that graph sampling in shallow layers has a more significant impact than graph sampling in deeper layers.\n\n## Reference\n1. Zhang, Shuai, et al. \"How unlabeled data improve generalization in self-training? A one-hidden-layer theoretical analysis.\" International Conference on Learning Representations. 2021.\n2. Hongkang Li, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. Generalization guarantee of\ntraining graph convolutional networks with graph topology sampling. In International Conference\non Machine Learning (ICML), pp. 13014\u201313051. PMLR, 2022a.\n3. Chen, Shuxiao, Hangfeng He, and Weijie Su. \"Label-aware neural tangent kernel: Toward better generalization and local elasticity.\" Advances in Neural Information Processing Systems 33 (2020): 15847-15858.\n4. Arora, Sanjeev, et al. \"Harnessing the power of infinitely wide deep nets on small-data tasks.\" arXiv preprint arXiv:1910.01663 (2019).\n5. Keyulu Xu, Mengshi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural\nnetworks: Implicit acceleration by skip connections and more depth. In International Conference\non Machine Learning (ICML). PMLR, 2021.\n6. Cao, Yuan, et al. \"Benign overfitting in two-layer convolutional neural networks.\" Advances in neural information processing systems 35 (2022): 25237-25250."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727338647,
                "cdate": 1700727338647,
                "tmdate": 1700730988860,
                "mdate": 1700730988860,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ODyD5dJHCx",
                "forum": "J2pMoN2pon",
                "replyto": "CgTS6bkxkJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Question 1:\nWhat is OPT in Equation 13? $ \\mathcal{H}_{n, A^*} $ is defined as the target function, while $ y_n $ is the label of node $ n $. How is it possible that OPT is non-zero?\n\n## Answer Question 1:\n1. We revised the  OPT definition, which is in equation (11) of the revision  to resolve the confusion. \n  We want to clarify that OPT is NOT the optimal value of the training problem. Instead, OPT is the smallest label prediction error of the best target function  (over the choices of parameters  $a_{\\mathcal{F}, r, i}^*$, $a_{\\mathcal{G}, r, i}^*$, $w^*_{r,i}$, $v_{r,i}^*$) in the concept class. OPT decreases as the concept class becomes more complex, such as increasing the integers $p_\\mathcal{F}, p_\\mathcal{G}$ and making activations  $\\mathcal{F}$ and $\\mathcal{G}$ more complex. \n\n2. The definition of OPT does not consider learning, we only discuss the approximation accuracy of using some target function to approximate the labels. We will later use OPT to measure the generalization performance of our learned model.   We realized the original presentation sequence of defining OPT after introducing the training algorithm may lead to confusion. We adjusted the sequence to define OPT immediately after the concept class and revised the definition based on the above discussion. \n\n## Question 2:\nIn Lemma 3.1 and Theorem 3.2: It is not clear with respect to which event the probability is taken.\n## Answer Question 2:\nHere the probability is with respect to the randomness in the SGD algorithm. The terminology is commonly used in theoretical generalization analysis, such as  Theorem 3.1 in [1], and Theorem 1 in [2].\n\n## Question 3:\nIn Theorem 3.2: Could the authors present the assumptions better or give more intuitions?\n## Answer Question 3:\nAs shown in Weakness 6, we have simplified the assumptions in  Theorem 3.2 to improve the presentation and we write a Proof overview in Appendix section C.1 to give more intuitions.\n\n## Question 4:\nWhy do the authors average in Equation 14 over all iterations of the SGD steps?\n## Answer Question 4:\nThis is a typical way to characterize the learning performance of SGD. Please see Theorem 1 in section 3.1 of paper [1] and Theorem 1 in section 6 of paper [2] also characterize the average performance of all SGD iterations. The intuition is that because the average performance is already good, considering that the models in the initial few iterations do not perform well, then the models learned at the end must have a desirable generalization.\n\n## Question 5:\nHow is $ (X, y_n) $ sampled, and how is $ D $ defined?\n## Answer Question 5:\nNote that our results are distribution-free. The setup of $(X, y_n)$ and $\\mathcal{D}$ are exactly the same as those in [3] see the first paragraph of page 7 of that paper. Specifically, in our paper, let $\\mathcal{D}_{{\\tilde{x}_n}}$ and\n$D_y$ denote the distribution from which the feature and label of node $n$ are drawn, respectively. \nLet $\\mathcal{D}$ denote the concatenation of these distributions.\n\nThen the given feature matrix $X$ and partial labels in   $\\Omega$    can be viewed as  $|\\Omega|$    identically distributed but correlated samples  $(X, y_n)$ from $\\mathcal{D}$. The correlation results from the fact that the label of node $i$ depends on not only the feature of node $i$ but also neighboring features. We added this discussion after equation (10) in the revision.\n\n## Question 6:\nCould you elaborate on many assumptions, e.g., Section 3.4: it doesn't seem clear that the norms of the learned weight matrices are uniformly bounded.\n## Answer Question 6:\nWe prove that $ \\|W_t\\|_2 $ and $ \\|V_t\\|_2 $  are bounded by $\\tau_w$ and $\\tau_v$ in Appendix section B.3. Because $\\tau_w$ and $\\tau_v$ can be bounded by model complexity and sample complexity constants to simply the presentation of the paper, we removed $\\tau_w$ and $\\tau_v$ from the main text and replace them with their bounds using $ C_F  $ and $ C_G  $.\n\n## Reference\n1. Allen-Zhu, Zeyuan, Yuanzhi Li, and Yingyu Liang. \"Learning and generalization in overparameterized neural networks, going beyond two layers.\" Advances in neural information processing systems 32 (2019).\n2. Allen-Zhu, Zeyuan, and Yuanzhi Li. \"What can resnet learn efficiently, going beyond kernels?.\" Advances in Neural Information Processing Systems 32 (2019).\n3. Hongkang Li, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. Generalization guarantee of training graph convolutional networks with graph topology sampling. In International Conference on Machine Learning (ICML), pp. 13014\u201313051. PMLR, 2022a."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731128901,
                "cdate": 1700731128901,
                "tmdate": 1700731290051,
                "mdate": 1700731290051,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fwSpjVJxu0",
            "forum": "J2pMoN2pon",
            "replyto": "J2pMoN2pon",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8606/Reviewer_urgn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8606/Reviewer_urgn"
            ],
            "content": {
                "summary": {
                    "value": "The paper analyzes the generalization errors of a 2-layer graph convolutional network (GCN) that incorporates skip connections and independently performs edge sampling at two different layers. Based on the developed theorems, the paper also presents a list of practical insights. Furthermore, the paper includes experimental evaluations on synthetic datasets as well as two real-world datasets, with the experimental results aligning with the theoretical findings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The analyses focus on GCN structures with skip connections, utilizing edge sampling as the sampling strategy, which distinguishes them as novel aspects compared to previous generalization analyses on GCNs.\n\n2. Skip connections and edge sampling are two commonly adopted design elements in contemporary GCNs. The theoretical discoveries offer valuable practical insights for the development of GCN architectures.\n\n3. The experiments are conducted on both synthetic datasets and real-world datasets, results from both experiments support the theoretical findings."
                },
                "weaknesses": {
                    "value": "$\\newcommand{\\sB}{\\mathcal{B}}$\n$\\newcommand{\\sF}{\\mathcal{F}}$\n$\\newcommand{\\sG}{\\mathcal{G}}$\n$\\newcommand{\\mW}{\\mathbf{W}}$\n$\\newcommand{\\mV}{\\mathbf{V}}$\n\n\nFirstly, I want to acknowledge that I understand the challenges associated with presenting mathematically intensive theoretical analyses, and the paper's overall structure is well-constructed. The following suggestions represent some \"nice-to-have\" additions that could enhance the logical flow and improve reader comprehension.\n\n1. I recommend adding an explanation for the choice of the value $d_1\\sqrt{\\frac{d_i}{d_j}}$ and the rationale behind differentiating the sampling strategies based on the cases where $i > j$ and $i \\leq j$\n\n2. Some conclusions are presented but not utilized within the main paper, such as the bounds on $\\sB_{\\sF \\circ \\sG}$, $||\\mW_t||_2$ and $||\\mV_t||_2$. This may lead to confusion regarding their initial inclusion.\n\n3. I recommend separating the proof for Lemma 3.1 from the proof for Theorem 3.2 and integrating them within the main paper. This adjustment is essential as it contributes to one of the key insights of the paper."
                },
                "questions": {
                    "value": "$\\newcommand{\\sL}{\\mathcal{L}}$\n$\\newcommand{\\sC}{\\mathcal{C}}$\n$\\newcommand{\\sS}{\\mathcal{S}}$\n\n1. Could the authors kindly provide a brief proof for the bound on $\\sL_\\sG$? I am particularly interested in the steps which introduce $\\sB_{\\sF}$ into the final expression.\n\n2. The upper bound for the combination factor $\\alpha$ is $O(\\frac{1}{kp_\\sG \\sC_\\sS(\\sG, \\sB_{\\sF}||A^*||_1)})$. I am curious about the order of magnitude of this value. The concern arises when this value becomes exceedingly small in practice, which can result in the target function degrading to $\\sF(A^*,x)$ and thus diminishing the potential impact of $\\sG(A^*, x)$ in reducing the error. This can also lead to minimal constraints on $A^2$. However, in such cases, it deviates significantly from the concept of hierarchical learning, rendering it a trivial situation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8606/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699591122740,
            "cdate": 1699591122740,
            "tmdate": 1699637076517,
            "mdate": 1699637076517,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QsSyTcTxj7",
                "forum": "J2pMoN2pon",
                "replyto": "fwSpjVJxu0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Weakness 1:\nThe analyses focus on GCN structures with skip connections, utilizing edge sampling as the sampling strategy, which distinguishes them as novel aspects compared to previous generalization analyses on GCNs.\n## Answer Weakness 1:\nOur core sampling concept revolves around sampling edges between nodes of lower degrees with higher probability. Because the edges between lower-degree nodes correspond to larger entries in $A$, we want to sample larger entries with a higher probability and smaller entries with a lower probability. In our sampling strategy, we just divide entries in $A$ into two parts, the parts with larger values are sampled with $1-p_{ij}^k$, and the group with smaller values is sampled with $p_{ij}^k$, where $p_{ij}^k$. The size of the larger part, which is selected as $d_1\\sqrt{d_i/d_j}$, is selected to ensure that $ \\left \\| A^* \\right \\| _1 $ is $O(1)$, which is required in our generalization analysis. It does not have to the exact value of $d_1\\sqrt{d_i/d_j}$, any value in the order of $d_1\\sqrt{d_i/d_j}$ will not change our order-wise analysis.\n\nWe want to emphasize that although we set these value to simplify our theoretical analysis, the main idea of  \nsampling edges between nodes of lower degrees with higher probability are preserved in our sampling strategy. We added a footnote about this in the paper. \n\n## Weakness 2: \nSome conclusions are presented but not utilized within the main paper. This may lead to confusion regarding their initial inclusion.\n## Answer Weakness 2:\nWe have removed them in the main paper to simplify presentation.\n\n## Weakness 3: \nI recommend separating the proof for Lemma 3.1 from the proof for Theorem 3.2 and integrating them within the main paper. This adjustment is essential as it contributes to one of the key insights of the paper.\n## Answer Weakness 3:\nWe separated the proof for Lemma 3.1 We also added a proof overview of Theorem 3.1 and Lemma 3.1. Due to the space limit of the main text, we put it in Appendix Section B.1.  \n\n## Question 1:  \nCould the authors kindly provide a brief proof for the bound on $ \\mathcal{L}_G $? I am particularly interested in the steps which introduce $ \\mathcal{B}_F $ into the final expression.\n\n## Answer Question 1:\nWe add the proof in Appendix Section A.\n\n## Question 2: \nThe upper bound for the combination factor $\\alpha$ is $O\\left(\\frac{1}{k_{pg}C_s(\\mathcal{G},\\mathcal{F}\\|A^*\\|_1)}\\right)$. I am curious about the order of magnitude of this value. The concern arises when this value becomes exceedingly small in practice, which can result in the target function degrading to $\\mathcal{F}(A^*, x)$ and thus diminishing the potential impact of $\\mathcal{G}(A^*, x)$ in reducing the error. This can also lead to minimal constraints on $A^2$. However, in such cases, it deviates significantly from the concept of hierarchical learning, rendering it a trivial situation.\n\n\n## Answer Question 2:\nWe completely understand the reviewer's concern that our analysis only specifies the order but not the magnitude, and  a small magnitude of $\\alpha$ can make the second term much smaller than the first term in the target function. } We agree that the result would be stronger if we could specify the magnitude of $\\alpha$ and prove it to be large. However, we need to emphasize that this is almost impossible because of the high complexity of the analysis in this paper, and we highly doubt if it is possible at all. In fact, the generalization analysis for a two-layer ResNet in [1] also only specifies the order of $\\alpha$ (See Theorem 1 in Section 6). Moreover, even if $\\alpha$ is small such that the base function $\\mathcal{F}$ can predict the labels with reasonable accuracy, adding the additional component $\\alpha \\mathcal{G}(\\mathcal{F})$ can still improve the accuracy, and our result provides the theoretical guarantee of the hierarchical learn these two functions. \n\nFurthermore, in our experiments in Section 4.1, we choose $\\alpha=0.5$. This value is deliberately chosen to be non-negligible to preserve the hierarchical structure that is central to our model's learning process. A significant $\\alpha$ ensures that the contribution of the function $\\| \\mathcal{F} \\|_2$ is on the same order of magnitude as $\\| \\alpha \\mathcal{G}(\\mathcal{F}) \\|_2)$. \n\n## Reference\n1. Allen-Zhu, Zeyuan, and Yuanzhi Li. \"What can resnet learn efficiently, going beyond kernels?.\" Advances in Neural Information Processing Systems 32 (2019)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729937766,
                "cdate": 1700729937766,
                "tmdate": 1700730596446,
                "mdate": 1700730596446,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]