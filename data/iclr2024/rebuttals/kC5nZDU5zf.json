[
    {
        "title": "Selective Visual Representations Improve Convergence and Generalization for Embodied AI"
    },
    {
        "review": {
            "id": "DMmnUJLRUD",
            "forum": "kC5nZDU5zf",
            "replyto": "kC5nZDU5zf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8895/Reviewer_NmEd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8895/Reviewer_NmEd"
            ],
            "content": {
                "summary": {
                    "value": "This work starts from a very attractive motivation: redundant information will blind people to making correct actions. The authors utilize a simple yet effective trick, i.e., adding a parameter-efficient codebook, and expecting it to filter out the unnecessary information in the embeddings. The method significantly boosts the performance of the baseline EmbCLIP on several benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I am quite appealed by the introduction, which I believe illustrates a very interesting motivation and will inspire the community. The method is also very simple and effective. Simply incorporating a codebook module is able to improve the baseline EmbCLIP by a large margin on several benchmarks. Overall, I believe this paper makes a good contribution, especially in terms of motivations and methods."
                },
                "weaknesses": {
                    "value": "One of the weaknesses is also related to its strengths, i.e., motivation. I was quite attracted by the example of Figure 1, in which the authors describe the situation that the keys would only lie on a flattened surface, not a corner or somewhere else. I do like the motivation and expect the authors to incorporate such a human prior into the method yet I did not find it in the paper. I would suggest the authors consider incorporating a text prompt on the CLIP text model such as \"the chair is usually on the floor\" etc. Not necessarily for all the benchmarks but I think it would be interesting to see whether several cases can be improved.\n\nThe second weakness is about codebook collapse, this is a good motivation and interesting problem that is also related to interpreting what codebook really learns. I would expect whether the current method exists codebook collapse without dropout during training. This means the authors may need to conduct ablation studies and provide illustrations about collapse or not."
                },
                "questions": {
                    "value": "Should adding a skip connection between \\hat{E} and E help improve the current version, i.e., \\hat{E} = theta(E)+Code Module(E)? I think bottleneck representations such as ResNet etc will usually do so.\n\nFor the other questions please see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8895/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812121528,
            "cdate": 1698812121528,
            "tmdate": 1699637119605,
            "mdate": 1699637119605,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0FVqHZk9TF",
                "forum": "kC5nZDU5zf",
                "replyto": "DMmnUJLRUD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8895/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your valuable feedback. In the following, we use W for Weaknesses and Q for Questions.\n\n\n**W1) Incorporating text prompts as a human prior into the models**\n\nThat\u2019s a very interesting suggestion! It can definitely be applied as a future work of our approach. In our current proposed method, we anticipate the automatic extraction of such common-sense knowledge and human priors from the observations through the bottleneck. Integrating such priors in the form of language could indeed be a compelling direction for future work.\n\n\n**W2) Codebook collapse ablations**\n\nThanks for the insightful comment. We included an ablation study in section **A.7**. Codebook collapse is a prevalent issue defined as the assignment of high probabilities to a specific subset of latent codes in the codebook, causing the majority of codes to be underutilized. While it\u2019s important to note that our codebook module doesn\u2019t experience a complete collapse, incorporating regularization into the codebook enhances overall performance by promoting a more balanced utilization of all the latent codes. While various solutions have been suggested, we have determined that applying a simple dropout on top of the codebook probabilities is the most effective and straightforward approach. In **Fig. 10**, we compare the average codebook probability distributions between two models\u2014one trained with a 0.1 dropout probability and the other without any dropout. The figure shows that this regularization leads to a more uniform average usage of the latent codes. **Table 8** presents the corresponding evaluation results. Clearly, introducing such regularization to the codebook module results in an improved performance. We additionally tried the **Linde-Buzo-Gray[1]** splitting algorithm as an alternative approach to balance the codebook usage. More specifically, during the training process, if there is a code vector that is underutilized, we perform a split on the most frequently used latent code, creating two equal embeddings and replacing the unused one. The corresponding training curve is depicted in **Fig. 11**, which shows a severe imbalance and eventual collapse during the training.\n\n[1] Yoseph Linde, Andres Buzo, and Robert Gray. An algorithm for vector quantizer design. In IEEE Transactions on communications, 1980.\n\n\n**Q1) What\u2019s the effect of adding a skip connection?**\n\nWe observed that incorporating a skip connection into our architecture leads to poorer performance, likely due to overfitting. There is a possibility that our existing hyperparameters may need adjustment to better suit the architecture with the skip connection, but despite our efforts, we were unable to identify the optimal hyperparameters, given the time constraints of the discussion period. We believe this requires further investigation.  Nonetheless, based on our current findings, it appears that employing a full bottleneck yields better results than introducing a skip connection. Table below shows the results for 300 million steps of training.\n\n| **Benchmark**             | **Model**                          | **SR(%)\u2191** | **EL\u2193** | **SPL\u2191** |\n| ------------------------- | ---------------------------------- | ---------- | ------- | -------- |\n| **ProcTHOR-10k (test)**   | EmbCLIP-Codebook                   | 73.40      | 124.00  | 49.84    |\n|                           | EmbCLIP-Codebook + Skip Connection | 44.58      | 170.00  | 26.11    |\n| **ArchitecTHOR (0-shot)** | EmbCLIP-Codebook                   | 55.67      | 171.00  | 34.49    |\n|                           | EmbCLIP-Codebook + Skip Connection | 36.80      | 169.00  | 21.20    |\n| **AI2-iTHOR (0-shot)**    | EmbCLIP-Codebook                   | 72.67      | 93.00   | 54.82    |\n|                           | EmbCLIP-Codebook + Skip Connection | 49.60      | 119.00   | 33.75    |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613490355,
                "cdate": 1700613490355,
                "tmdate": 1700613490355,
                "mdate": 1700613490355,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VVaqiNiSyU",
                "forum": "kC5nZDU5zf",
                "replyto": "0FVqHZk9TF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8895/Reviewer_NmEd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8895/Reviewer_NmEd"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for the reply. The authors address my main concerns about collapse and skip connection. It is a good paper but I will keep the score because I wish the authors could address my largest concern about integrating human priors, which reflects the motivation of the paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714243825,
                "cdate": 1700714243825,
                "tmdate": 1700714243825,
                "mdate": 1700714243825,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "75pE5eUMGb",
            "forum": "kC5nZDU5zf",
            "replyto": "kC5nZDU5zf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8895/Reviewer_vxKH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8895/Reviewer_vxKH"
            ],
            "content": {
                "summary": {
                    "value": "Visual encoders like CLIP capture general purpose scene information which includes details not relevant to the task. The paper proposes to learn a codebook module that selectively filters information from visual representations specific for a task. They demonstrate the approach on a wide range of Embodied AI tasks across several large-scale benchmarks. Additionally, they present qualitative analysis showing that the codebook module better encodes task information which lends to more robust navigational policies."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Approach is simple, easy to follow, and well motivated. \n\nCodebook representations and policy are learned end-to-end.\n\nResults are shown across a diverse set of tasks and benchmarks. Improvement over EmbCLIP seems significant. Also report a comprehensive set of metrics including a new metric Success Weighted by Episode Length which accounts for actions like changing viewing angles that also requires time and effort. \n\nInteresting experiments showing that with minimal finetuning of the Adaptation Module with a frozen codebook can help transfer to new visual domains.\n\nSaliency visualizations are informative of where the model is paying attention to and highlights relevant tasks objects.\n\nNice set of ablation studies on codebook latent size and linear probes."
                },
                "weaknesses": {
                    "value": "Does the codebook work for other types of general purpose visual encoders? This work seems to only show the codebook module applied to EmbCLIP."
                },
                "questions": {
                    "value": "Could the codebook be learning to localize the goal information in the scene? The input to the codebook contains both the visual observation and a language description of the goal. It could be possible that the codebook is simply acting as a bridge between the two input modalities.\n\nWhat are some of the failure cases of the EmbCLIP + codebook method? Are there instances where codebook fails to identify task objects?\n\nAre there cases where two objects of the same class are present in the scene, but the task requires the model to pick one of the objects."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8895/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827075435,
            "cdate": 1698827075435,
            "tmdate": 1699637119481,
            "mdate": 1699637119481,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IsnCRhh2fx",
                "forum": "kC5nZDU5zf",
                "replyto": "75pE5eUMGb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8895/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your valuable feedback. In the following, we use W for Weaknesses and Q for Questions.\n\n\n**W1) Applying the codebook to other pretrained visual encoders**\n\nThanks a lot for your great comment! We included section **A.2** in the appendix which shows the codebook module is representation-agnostic and can be applicable to other pretrained visual encoders as well. We used pretrained **DINOv2**[1] visual features and used the codebook to bottleneck the new goal-conditioned representations. We use the frozen **DINOv2 ViT-S/14** model to encode RGB images into a 384x7x7 tensor. We fuse this tensor with a 32-dimensional goal embedding and the previous action embedding and flatten the result to obtain a 1574-dimensional goal-conditioned observation embedding. We employed a codebook with similar dimensions, K = 256 and Dc = 10, to bottleneck the goal-conditioned representations. As shown in **Table 5** (shown in below as well), our approach outperforms the DINOv2 baseline models across a variety of Object Navigation metrics in various benchmarks. This experiment underscores the capability of our codebook module in effectively bottlenecking other visual features for embodied-AI tasks.\n\n| Benchmark                   | Model              | SR(%)\u2191 | EL\u2193   | Curvature\u2193 | SPL\u2191  | SEL\u2191  |\n|-----------------------------|--------------------|-------:|------:|-----------:|------:|------:|\n| **ProcTHOR-10k (test)**     | DINOv2             | 74.25  | 151.00| 0.24       | 49.53 | 43.20 |\n|                             | +Codebook (Ours)   | **76.31** | **129.00**| **0.12**   | **50.26** | **44.70** |\n| **ArchitecTHOR (0-shot)**   | DINOv2             | 57.25  | 218.00| 0.25       | 36.83 | 29.00 |\n|                             | +Codebook (Ours)   | **59.75** | **194.00**| **0.11**   | **36.00** | **31.70** |\n| **AI2-iTHOR (0-shot)**      | DINOv2             | 74.67  | 97.00 | 0.19       | 59.45 | 26.50 |\n|                             | +Codebook (Ours)   | **76.93** | **68.00** | **0.07**   | **60.14** | **28.30** |\n| **RoboTHOR (0-shot)**       | DINOv2             | 60.54  | -     | -          | **29.36** | -     |\n|                             | +Codebook (Ours)   | **61.03** | -     | -          | 28.01 | -     |\n\n[1] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023\n\n**Q1) Codebook as a bridge between modalities?**\n\n\nYes, our model encodes goal related information beyond just the appearance of the goal (location, context, places, etc). Our model bridges between task-related information and visual information. Our tasks are represented as an embedding and our codebook connects the two. Figure 8 shows the encoded information in different latent codes. While some codes are directly responsible for goal-related information such as goal visibility and proximity to the agent, others encode other information required for the object navigation task such as walkable areas, corners, etc.\n\n\n**Q2) What are the failure cases of the EmbCLIP-Codebook?**\n\n\nWe included section **A.5** which analyzes the failure cases for EmbCLIP-Codebook. **Fig. 9** illustrates examples of two modes of failure in our EmbCLIP-Codebook agent: perception and exploration. Although the codebook module enables the agent to effectively filter out distractions and concentrate on the target object, the agent\u2019s performance remains constrained by the perceptual capabilities of the pretrained visual encoder. So there exists instances where the codebook fails to identify target objects.\nThe top row of **Fig. 9** showcases examples of failures related to perception. These instances predominantly occur when the goal object is either too small or challenging to identify (the baseball bat on the table in the top left example). In these scenarios, although the agent traverses past the object, it fails to accurately locate the target. The second row of the figure presents additional instances of failure, wherein the agent fails to explore specific areas of the environment where the target object is located.\n\n\n\n**Q3) Are there cases where two objects of the same class are present in the scene?**\n\n\nYes, that is indeed possible. Take, for example, a scenario where the target object is a 'chair.' In a typical scene, multiple chairs are often found around dining tables. The objective of Object Goal Navigation is to find any one instance of the target object type \u2013 in this case, any chair in the scene. Therefore, the task requires the agent to find any single instance of the specified object type."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613417057,
                "cdate": 1700613417057,
                "tmdate": 1700685568806,
                "mdate": 1700685568806,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "w5PLgEg5nQ",
            "forum": "kC5nZDU5zf",
            "replyto": "kC5nZDU5zf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8895/Reviewer_YjyH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8895/Reviewer_YjyH"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a parameter efficient approach to filter out task-irrelevant information encoded by visual encoders, such as CLIP, in embodied AI tasks. This approach leverages a compact, learnable codebook module to establish a task-specific filter for visual observations. The codebook is trained to optimize task performance, serving as a filter that directs the agent's attention toward task-relevant visual cues. The experimental results demonstrate performance improvement in object goal navigation and object displacement tasks across various benchmarks. Qualitative analysis illustrates that agents become more proficient in exploring their surroundings, retaining task-relevant information, and disregarding irrelevant visual details."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The paper proposes a simple parameter efficient approach for adapting representations which leads to consistent performance improvement on multiple benchmarks. \n2) Because the model learns to ignore the irrelevant parts of the visual inputs, it is able to do more efficient exploration and navigation.. \n3) The proposed finetuning approach of the codebook module is clever."
                },
                "weaknesses": {
                    "value": "1) Given that the proposed approach is about the codebook module, it would have been good to see if this approach could be applied to other pretrained visual encoders, for eg ViT based models. \n2) It's not clear how their approach compares against a full-scale visual encoder fine tuning baseline? Finetuning of the visual encoder should also allow it to forget information that is not relevant to the task. While finetuning is computationally expensive, it will still be interesting to see how EmbCLIP-codebook performs with respect to it. \n3) It is also not clear how important is the use of the codebook vectors compared to just the introduction of a bottleneck in the architecture? To test this, I recommend the authors try using their proposed bottleneck architecture without the codebook vectors."
                },
                "questions": {
                    "value": "I have listed my main concerns in the weaknesses section. I will be happy to increase the score if the authors can answer those with relevant experiments. Other than that, I have some other questions and suggestions that I list below:\n1) What is meant by \u201cSamplers\u201d, which is mentioned in the experimental details?\n2) Why is RobotTHOR missing some metrics in Table 1?\nTypos:\nPage 6: HMD semantics -> HM3D Semantics. Also missing citation for HM3D Semantics\t\nPage 5: temperture -> temperature\nPage 2: codebook better encodes -> codebook encodes better\nPage 17: gent\u2019s -> agent\u2019s"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8895/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8895/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8895/Reviewer_YjyH"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8895/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833667911,
            "cdate": 1698833667911,
            "tmdate": 1699637119344,
            "mdate": 1699637119344,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sdZCBn04Mf",
                "forum": "kC5nZDU5zf",
                "replyto": "w5PLgEg5nQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8895/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your valuable feedback. In the following, we use W for Weaknesses and Q for Questions.\n\n**W1) Applying the codebook to other pretrained visual encoders**\n\n\nThanks for the great feedback! We added section **A.2** in the appendix. In order to show the applicability of the codebook to other visual encoders, we used pretrained **DINOv2**[1] visual features and applied the codebook to bottleneck the new goal-conditioned representations. We use the frozen **DINOv2 ViT-S/14** model to encode RGB images into a 384x7x7 tensor. We fuse this tensor with a 32-dimensional goal embedding and the previous action embedding and flatten the result to obtain a 1574-dimensional goal-conditioned observation embedding. We employed a codebook with similar dimensions, K = 256 and Dc = 10, to bottleneck the goal-conditioned representations. As shown in **Table 5** (shown in below as well), our approach outperforms the DINOv2 baseline models across a variety of Object Navigation metrics in various benchmarks. This experiment underscores the capability of our codebook module in effectively bottlenecking other visual features for embodied-AI tasks.\n\n| Benchmark                   | Model              | SR(%)\u2191 | EL\u2193   | Curvature\u2193 | SPL\u2191  | SEL\u2191  |\n|-----------------------------|--------------------|-------:|------:|-----------:|------:|------:|\n| **ProcTHOR-10k (test)**     | DINOv2             | 74.25  | 151.00| 0.24       | 49.53 | 43.20 |\n|                             | +Codebook (Ours)   | **76.31** | **129.00**| **0.12**   | **50.26** | **44.70** |\n| **ArchitecTHOR (0-shot)**   | DINOv2             | 57.25  | 218.00| 0.25       | 36.83 | 29.00 |\n|                             | +Codebook (Ours)   | **59.75** | **194.00**| **0.11**   | **36.00** | **31.70** |\n| **AI2-iTHOR (0-shot)**      | DINOv2             | 74.67  | 97.00 | 0.19       | 59.45 | 26.50 |\n|                             | +Codebook (Ours)   | **76.93** | **68.00** | **0.07**   | **60.14** | **28.30** |\n| **RoboTHOR (0-shot)**       | DINOv2             | 60.54  | -     | -          | **29.36** | -     |\n|                             | +Codebook (Ours)   | **61.03** | -     | -          | 28.01 | -     |\n\n\n[1] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613260707,
                "cdate": 1700613260707,
                "tmdate": 1700614690869,
                "mdate": 1700614690869,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g7w4bEFnd2",
                "forum": "kC5nZDU5zf",
                "replyto": "w5PLgEg5nQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8895/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W2) Full-scale visual encoder fine-tuning as a baseline**\n\n\nThank you for the suggestion. To explore the impact of full-scale fine-tuning, we fine-tuned the entire policy, including the visual encoder, for both EmbCLIP and EmbCLIP-Codebook models. We used the best checkpoints reported in **Table 1** and fine-tuned the entire policy, including the ResNet-50 visual encoder, for an additional 30 million steps. Details of the implementation, including multi-node training, large batch size, and small learning rate, can be found in **Appendix 6**. The results are presented in the following table (also included in the updated paper as **Table 7**).\n\n| Benchmark                   | Model                                    | SR(%)\u2191 | EL\u2193 | SPL\u2191 |\n|-----------------------------|------------------------------------------|-------:|----:|-----:|\n| **ProcTHOR-10k (test)**     | EmbCLIP                                  | 67.70  | 182.00 | 49.00 |\n|                             | EmbCLIP-Codebook                         | 73.72  | 136.00 | 48.37 |\n|                             | EmbCLIP + CLIP ResNet fine-tuning        | 74.37  | 138.00 | 52.46 |\n|                             | EmbCLIP-Codebook + CLIP ResNet fine-tuning | **79.38** | **120.00** | **52.67** |\n| **ArchitecTHOR (0-shot)**   | EmbCLIP                                  | 55.80  | 222.00 | 38.30 |\n|                             | EmbCLIP-Codebook                         | 58.33  | 174.00 | 35.57 |\n|                             | EmbCLIP + CLIP ResNet fine-tuning        | 59.00  | 182.00 | **38.92** |\n|                             | EmbCLIP-Codebook + CLIP ResNet fine-tuning | **62.58** | **168.00** | 37.10 |\n| **AI2-iTHOR (0-shot)**      | EmbCLIP                                  | 70.00  | 121.00 | 57.10 |\n|                             | EmbCLIP-Codebook                         | **78.40** | **86.00** | 54.39 |\n|                             | EmbCLIP + CLIP ResNet fine-tuning        | 72.53  | 88.00 | 59.28 |\n|                             | EmbCLIP-Codebook + CLIP ResNet fine-tuning | 77.75 | 99.00 | **59.33** |\n\nAs the table shows, while there is a substantial improvement from fine-tuning the entire policy model, including the visual encoder, compared to using the frozen one, a noticeable gap remains between the fine-tuned EmbCLIP and fine-tuned EmbCLIP-Codebook. Notably, with further end-to-end fine-tuning, EmbCLIP-Codebook achieves significantly superior results compared to all other models.\n\n\n**W3) Bottlenecked architecture without the codebook module as a baseline**\n\n\nThanks for the insightful comment. We included a new section, **A.4**, in the appendix. **Table 6** compares our proposed bottleneck architecture (EmbCLIP-Codebook) with 2 other information bottleneck baselines on 3 Object Navigation benchmarks. **EmbCLIP-AE** utilizes an auto-encoder on the goal-conditioned embedding E. This autoencoder comprises a series of linear layers, mapping the representation to $\\mathcal{P} \\in \\mathcal{R}^{256}$ and $h \\in \\mathcal{R}^{10}$ before resizing back to $\\hat{E} \\in \\mathcal{R}^{1574}$. The results in Table 6 demonstrate the superior performance of our method compared to EmbCLIP-AE across all 3 benchmarks which underscores the importance of the codebook module in the proposed architecture.\n\n**Q1) What is meant by \u201cSamplers\u201d mentioned in experiment details?**\n\nThe concept of a Sampler is defined in AllenAct [1]. The primary function of a Sampler is to sample an episode (or a Task) for the agent. Each episode specifies the action space, success criteria, and the rewards that the environment returns to the agent. After the agent completes an episode, the Sampler selects a new episode for the agent. Consequently, a greater number of Samplers indicates that a higher number of episodes are being processed by the agent in parallel. In other words, more Samples imply a larger batch size for training a policy model.\n\n\n[1] Luca Weihs and Jordi Salvador and Klemen Kotar and Unnat Jain and Kuo-Hao Zeng and Roozbeh Mottaghi and Aniruddha Kembhavi, \u201cAllenAct: A Framework for Embodied AI Research\u201d, Arxiv 2020, url: https://allenact.org/.\n\n**Q2) Why is RoboTHOR missing some metrics?**\n\n\nRoboTHOR test results are only available through submissions to the **leaderboard** ([link](https://leaderboard.allenai.org/robothor_objectnav/submission/ckaa7vjb33j953fg5h0g)). We also included the link to the submission in **a footnote on page 6**. The leaderboard metrics only include **Success Rate** and **SPL**."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613318408,
                "cdate": 1700613318408,
                "tmdate": 1700640088078,
                "mdate": 1700640088078,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ybUMTA0odN",
            "forum": "kC5nZDU5zf",
            "replyto": "kC5nZDU5zf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8895/Reviewer_X3Z2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8895/Reviewer_X3Z2"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an adaptive feature modulation module for embodied AI.  The core idea is to create a task-conditioned representation bottleneck, based on a learnable codebook, to select useful features for each robotic task.  The codebook is jointly trained to optimize the task reward.  This paper demonstrates superior efficacy of the proposed method, and conduct thorough ablation study / visualization of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Good ablation study.  Table 3 clearly shows the selected information of the codebook. Grad-CAM visualization also makes sense.\n\n2. The paper is well written.  It's easy to follow especially that readers are primed with some high-level ideas from human vision.\n\n3. The performance gain is significant compared to the baseline."
                },
                "weaknesses": {
                    "value": "1. Learning codebook as information bottleneck helps discriminative mapping of observations to task outputs.  Meanwhile, throwing away information means that there's little knowledge shared among different task.  It'd be interesting to see if the performance comparison on zero-shot/few-shot learning of novel tasks.  My guess is that EmbCLIP would adapt toe the novel task faster and better than the proposed codebook method.\n\n\n2. The paper does not provide comparison to other bottleneck-based baselines.  For example, one can learn a self-attention modules atop CLIP feature maps.  Also, learning an auto-encoder, where the bottleneck is low-dimensional latent feature, seems to be another reasonable baseline.  Both self-attention modules and the auto-encoder will condition on the goal task description and previous action."
                },
                "questions": {
                    "value": "1. What's the performance of fine-tuning entire model on Habitat and adaptation module on HMD semantics (table 2)?  Also, what's the before-finetuning results of EmbCLIP+codebook on both benchmark?  Right now the comparison is conducted over different dataset, not apple-to-apple.\n\n2. The nearest neighbor probably should be done by treating learned codes as query.  In stead of using the pooled/weighted sum features, the authors can use a one-hot vector to select each code and upsample it to 1568 dimension.  It'd be more interesting to see what are learned in the codebook and what do the retrievals look like for each task.\n\n3. Perhaps the authors should compare with other information-bottleneck baselines, e.g. self-attention / autoencoder.  Such results could clarify if codebook / information bottleneck is the key."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8895/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8895/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8895/Reviewer_X3Z2"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8895/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698857018743,
            "cdate": 1698857018743,
            "tmdate": 1700692830642,
            "mdate": 1700692830642,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5GDFwCxY83",
                "forum": "kC5nZDU5zf",
                "replyto": "ybUMTA0odN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8895/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for the valuable feedback. In the following, we use W for Weaknesses and Q for Questions.\n\n\n**W1) Throwing away information using the codebook might result in worse performance in zero-shot/few-shot learning of novel tasks**\n\nBottlenecking and discarding information does not necessarily lead to poorer generalization in novel scenarios. An illustrative example of this is found in regularization techniques such as dropout, wherein they effectively prevent overfitting, resulting in simpler and more generalizable models. Furthermore, our codebook module is specifically designed to be **task-specific**. Our main goal is to bottleneck the information to best suit the task in hand. Therefore, performing a new task requires training another task-specific codebook module. It is however expected of our proposed approach to be more generalizable when applied to the **same task in new domains**. \nAs demonstrated in **Section 4.2** of the paper, our method significantly outperforms non-bottlenecked architectures in adapting to new visual domains through the lightweight fine-tuning of a small adaptation module.\n\n\n**W2, Q3) Comparison with other information bottleneck baselines**\n\nThanks a lot for this insightful feedback. We included a new section, **A.4**, in the appendix. **Table 6** compares our proposed bottleneck architecture (EmbCLIP-Codebook) with 2 other information bottleneck baselines on 3 Object Navigation benchmarks. **EmbCLIP-AE** utilizes an auto-encoder on the goal-conditioned embedding E, while **EmbCLIP-SelfAttn** applies self-attention to the goal-conditioned CLIP feature maps. The results in Table 6 demonstrate the superior performance of our method compared to the other information bottleneck baselines across all 3 benchmarks.\n\n\n\n**Q1) Habitat zero-shot results and fine-tuning results on the same benchmark**\n\nSorry for the confusion. All results are originally evaluated on the same benchmark (HM3D Semantics dataset) used in Habitat 2022 ObjectNav Challenge. We have updated **Table 2** to clarify our fine-tuning results. \nWe\u2019re also reporting the zero-shot results for both models here. The models are evaluated at 220M and 420M training steps. We\u2019re not including the results in the paper as the 0-shot results are very low compared to the fine-tuned versions.\n\n\n\n\n| Benchmark              | Model            | Training Step | SR(%)\u2191 | EL\u2193 | SPL\u2191      |\n| ---------------------- | ---------------- | ------------- | ------------ | -------------- | -------- |\n|**Habitat Challenge 2022** (zero-shot) | EmbCLIP          | 220M          | **11.20**    | 313            | **6.50** |\n| **Habitat Challenge 2022** (zero-shot) | EmbCLIP-Codebook | 220M          | 8.15         | **253**        | 4.71     |\n|**Habitat Challenge 2022** (zero-shot) | EmbCLIP          | 420M          | 8.50         | 362            | 4.85     |\n|**Habitat Challenge 2022** (zero-shot) | EmbCLIP-Codebook | 420M          | **9.85**     | **345**        | **5.23** |\n\n\n\n**Q2) Nearest-neighbor analysis using learned codes as queries**\n\nThanks for the interesting suggestion! We added this analysis in section **A.3** in the appendix. Using a set of 10k ProcTHOR frames each accompanied by a corresponding goal specification that may or may not be visible in the frame, we input these frames and their associated goals into our pretrained EmbCLIP-Codebook model. We save their corresponding 10-dim hidden compact representations h which are convex combinations of the latent codes. To visually assess the information encoded in each individual learnable code, we perform nearest neighbor retrieval on the dataset using each 10-dim codebook entries as queries. **Fig. 8** illustrates the 8 nearest neighbors for 9 different codes. The figure reveals that certain codes primarily contribute to goal visibility, determined by their proximity to the agent, while others capture various geometric and semantic details of scene layouts, such as walkable areas, corners, doors, etc."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612860596,
                "cdate": 1700612860596,
                "tmdate": 1700614300285,
                "mdate": 1700614300285,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2pgUOf9Zaf",
                "forum": "kC5nZDU5zf",
                "replyto": "5GDFwCxY83",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8895/Reviewer_X3Z2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8895/Reviewer_X3Z2"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response.  My questions are mostly answered and I'm happy to increase my ratings."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692803024,
                "cdate": 1700692803024,
                "tmdate": 1700692803024,
                "mdate": 1700692803024,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]