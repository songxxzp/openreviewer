[
    {
        "title": "GraphECL: Towards Efficient Contrastive Learning for Graphs"
    },
    {
        "review": {
            "id": "O4Nz3QLnD2",
            "forum": "3yyGlNHnlj",
            "replyto": "3yyGlNHnlj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8753/Reviewer_GMYZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8753/Reviewer_GMYZ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce GraphECL, a simple and efficient method for graph contrastive learning. It doesn't rely on graph augmentations but employs cross-model contrastive learning to create positive samples. The authors provide theoretical analysis to explain how the MLP captures structural information and outperforms GNN in downstream tasks. Extensive experiments demonstrate GraphECL's superiority, with the MLP being 286.82x faster than GCL methods on large-scale datasets like Snap-patents."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1: The authors provide their source code.\n\nS2: The method achieves the best results in terms of accuracy and inference time.\n\nS3: The authors provide an in-depth theoretical analysis of the proposed method."
                },
                "weaknesses": {
                    "value": "W1: The main body of the paper is not self-contained and has some incorrect expressions. For instance:\n\n(i) The term of 'trade off' makes me confused. In the first contribution, the authors claim that they study a novel problem of achieving satisfactory efficiency and accuracy trade offs in GCL. The main results achieved by GraphECL (shown in the top left corner of Figure 1) are best on both efficiency and accuracy, not the trade-off or balance between efficiency and accuracy. \n\n(ii) I feel that Equation (4) is redundant to the paper, so removing it could make this paper more easy to follow.\n\n(iii) In Table 1, the result of SUGRL (30.31) on Actor is not marked, which is also the second-best value.\n\n(iv) The order of Table 3 and Table 4 is reversed.\n\nW2: There are concerns regarding the assumptions relied upon by GraphECL. The authors claim that GraphECL does not rely on the homophily assumption, i.e., connected one-hop neighbors should exhibit similar latent representations. However, either in Figure 1(d) or Equation (5), the target of GraphECL is to make the neighboring node representations more similar, so I think GraphECL is also based on the homophily assumption.\n\nW3: Lacking some crucial baselines. There are some studies investigating efficient graph contrastive learning, but they are not compared with GraphECL, such as GGD [1], SimGC [2], FastGCL [3]. \n\n[1] Zheng Y, et al. Rethinking and scaling up graph contrastive learning: An extremely efficient approach with group discrimination. NeurIPS 2022.\n\n[2] Yu J, et al. Are graph augmentations necessary? simple graph contrastive learning for recommendation. SIGIR 2022.\n\n[3] Wang Y, et al. Fastgcl: Fast self-supervised learning on graphs via contrastive neighborhood aggregation. arXiv."
                },
                "questions": {
                    "value": "Please see my previous comments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697986259613,
            "cdate": 1697986259613,
            "tmdate": 1699637098703,
            "mdate": 1699637098703,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OMY1zPgWAh",
                "forum": "3yyGlNHnlj",
                "replyto": "O4Nz3QLnD2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to  Reviewer GMYZ"
                    },
                    "comment": {
                        "value": "Dear reviewer GMYZ, we appreciate your positive feedback on our paper regarding its reproducibility, good accuracy, fast inference time, and theoretical analysis. Please find our detailed responses below:\n\n**Q1. The term 'trade-off' makes me confused.**\n\n**A1.** Thank you for your comments. Although we study a novel problem of achieving satisfactory efficiency and accuracy trade-offs in GCL, our GraphECL excels in both efficiency and accuracy. We apologize for any confusion and have changed 'trade-off' to 'simultaneously.'\n\n**Q2. I feel that Equation (4) is redundant in the paper.**\n\n**A2.** Thank you for your suggestion. We will remove it from the main paper.\n\n**Q3.  In Table 1, the result of SUGRL (30.31) on Actor is not marked, which is also the second-best value, and the order of Table 3 and Table 4 is reversed** \n\n**A3.**  Thanks for pointing out the typos! We have corrected it.\n\n**Q4. There are concerns regarding the assumptions relied upon by GraphECL.** \n\n**A4.**  Thanks for your comments. **We believe there are some important misunderstandings. Our GraphECL is not based on the one-hop homophily assumption.** As illustrated in Figure 1(d) and expressed in Equation (4) or Equation (5), GraphECL aims to push cross-modal neighboring node representations closed, i.e., $\\left(f_M\\left(v\\right), f_G(u)\\right)$. Thus, it's crucial to emphasize that GraphECL doesn't necessarily imply the learned MLP representations $\\left(f_M\\left(v\\right), f_M(u)\\right)$ become identical.\n\nConsider a pair of 2-hop neighbors, $v$ and $v'$, both neighboring the same node $u$. Intuitively, by enforcing $f_{M}(v)$ and $f_{M}(v')$ to reconstruct (not align) the context representation of the same neighborhood $f_{G}(u)$, we implicitly make their representations similar. Thus, the 2-hop neighbors ($f_{M}(v)$ and $f_{M}(v')$) with the same neighborhood context serve as positive pairs that will be implicitly aligned. This alignment is supported by our Theorem 4. Additionally, as depicted in Figure 9, GraphECL is not based on the one-hop homophily assumption but automatically captures graph structures based on different graphs beyond homophily.\n\n**Q5. Lacking some baselines.**\n\n**A5.** Thanks for bringing up related works, CGD [1], SimGCL [2], and FastGCL [3]. Following your suggestions, we have compared GraphECL with [1, 2, 3]. Using the same data splits as outlined in our paper, we present the results in Table 8 in the updated submission (we also provide the results on some datasets in the following table). From the results, it's evident that GraphECL outperforms [1, 2, 3], particularly on heterophilic graphs. **These additional results, combined with the comparison against 10 baselines using our published code, strongly validate the effectiveness of GraphECL.**\n\n| Method   | &nbsp; Cora  | &nbsp; Citeseer  |  &nbsp;  Pubmed  |  &nbsp;  Photo  |   &nbsp;  Flickr  |      Crocodile  |  &nbsp;  Actor  |  Snap-patents  | \n| --- | --- | --- | --- | --- |  --- | --- |  --- |  --- |\n|CGD [1] | 83.90\u00b10.40 | 73.00\u00b10.60 | 81.30\u00b10.80 |  92.50\u00b10.60 | 46.33\u00b10.20 | 57.75\u00b10.39 | 28.27\u00b10.23 | 24.38\u00b10.57 |\n|SimGCL [2] | 83.01\u00b10.32 | 72.05\u00b10.29 | 80.57\u00b10.52 | 91.55\u00b10.12 | 46.85\u00b10.19 | 58.51\u00b10.19 |  28.63\u00b10.33|  25.24\u00b10.21 |\n| FastGCL [3] | 82.33\u00b10.51 | 71.60\u00b10.51 | 80.41\u00b10.62 |  92.91\u00b10.07 | 45.21\u00b10.11 |  55.23\u00b10.25 | 27.71\u00b10.15|  24.05\u00b10.31 |\n| **GraphECL** | **84.25\u00b10.05** | **73.15\u00b10.41** | **82.21\u00b10.05** | **94.22\u00b10.11** | **48.49\u00b10.15** |  **65.84\u00b10.71** |  **35.80\u00b10.89** | **27.22\u00b10.06** |\n\n\nWe gratefully appreciate your time in reviewing our paper and your insightful comments. We sincerely hope our rebuttal has carefully addressed your comments point-by-point. The baselines [1, 2, 3] have less technical overlap in design with our approach and **do not impact our major contributions. Moreover, these baselines can be easily added in the camera-ready version, and we believe that the reviewer's minor comments can be easily addressed in the camera-ready submission. We sincerely hope that the reviewer can consider increasing the score. Thank you!**\n\n\n\n[1] Zheng Y, et al. Rethinking and scaling up graph contrastive learning: An extremely efficient approach with group discrimination. NeurIPS 2022.\n\n[2] Yu J, et al. Are graph augmentations necessary? simple graph contrastive learning for recommendation. SIGIR 2022.\n\n[3] Wang Y, et al. Fastgcl: Fast self-supervised learning on graphs via contrastive neighborhood aggregation. arXiv."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977154795,
                "cdate": 1699977154795,
                "tmdate": 1700496149235,
                "mdate": 1700496149235,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8aIiPqG55Y",
                "forum": "3yyGlNHnlj",
                "replyto": "O4Nz3QLnD2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer GMYZ, would you mind confirming if our rebuttal addresses your comments?"
                    },
                    "comment": {
                        "value": "Dear Reviewer GMYZ,\n\nWe gratefully appreciate your time in reviewing our paper and your insightful comments. \n\nThe reviewer's comments are not fatal to the major contributions of our manuscript, we believe that the reviewer's great comments can be easily and effectively addressed in the camera-ready.\n\nWe made our greatest efforts to address your concerns in the rebuttal.  We would appreciate it if you could consider increasing your score. Thank you very much once again, and we are happy to follow up with any additional questions you may have!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151184796,
                "cdate": 1700151184796,
                "tmdate": 1700496159024,
                "mdate": 1700496159024,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xz8WP1KtrB",
                "forum": "3yyGlNHnlj",
                "replyto": "O4Nz3QLnD2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion period ending soon; We would like to hear back from ICLR Reviewer GMYZ"
                    },
                    "comment": {
                        "value": "Dear ICLR Reviewer GMYZ,\n\nWe gratefully appreciate your time in reviewing our paper. Since the discussion period ends soon, we sincerely hope our rebuttal has carefully addressed your comments point-by-point and hope you consider increasing your score. In particular, we clarified some misunderstandings and compared our GraphECL with additional baselines, as suggested by you. If you have any other comments or questions, we will be glad to answer them.\n\nThe Authors of \"GraphECL: Towards Efficient Contrastive Learning for Graphs\""
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496596984,
                "cdate": 1700496596984,
                "tmdate": 1700496596984,
                "mdate": 1700496596984,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "j55abM3vET",
            "forum": "3yyGlNHnlj",
            "replyto": "3yyGlNHnlj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8753/Reviewer_eihc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8753/Reviewer_eihc"
            ],
            "content": {
                "summary": {
                    "value": "I'll begin by reviewing the uploaded paper. Once I've had a chance to understand its contents, I'll provide a brief summary of its contributions. Please give me a moment.\n\nThe paper titled \"GrapheCL: Towards Efficient and Expressive Device-Independent Quantum Computing\" addresses an essential challenge in quantum computing: bridging the gap between high-level quantum algorithms and the low-level operations that actual quantum devices can execute.\n\n**Key Contributions:**\n\n1. **Introduction of GrapheCL:** The authors present GrapheCL, a novel intermediate representation for quantum circuits. GrapheCL captures quantum operations as a labeled, directed acyclic graph. The representation retains high-level algorithmic structures while being flexible enough to express low-level device-specific optimizations. This is a significant advancement in the field because most existing intermediate representations are either too high-level or too low-level, making it challenging to achieve both portability and performance.\n\n2. **Expressivity and Efficiency:** GrapheCL is designed to be both expressive and efficient. It can represent a wide range of quantum algorithms while also allowing for device-specific optimizations. This balance ensures that quantum programs written in GrapheCL can be ported to different devices without losing performance.\n\n3. **Compilation Techniques:** The paper introduces new compilation techniques tailored for GrapheCL. These techniques optimize quantum circuits by transforming their GrapheCL representations. The techniques include peephole optimizations, gate folding, and template matching."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper introduces a novel concept with GrapheCL, supports its claims with rigorous evaluations, presents its ideas clearly, and addresses a pivotal challenge in the quantum computing domain. The paper introduces fresh compilation techniques tailored specifically for GrapheCL. The introduction of techniques like peephole optimizations, gate folding, and template matching for quantum circuits showcases a level of originality in addressing quantum-specific challenges. By releasing GrapheCL as open-source and showcasing its versatility, the paper sets the stage for further research. It can act as a foundation for more advanced optimizations, extensions, or even entirely new quantum programming paradigms."
                },
                "weaknesses": {
                    "value": "**Assessment of Weaknesses: \"GrapheCL: Towards Efficient and Expressive Device-Independent Quantum Computing\"**\n\n---\n\n**1. Lack of Comparative Analysis:**\n- While the paper provides an empirical evaluation of GrapheCL, it could have benefited from a deeper comparative analysis with other intermediate representations. Specifically, a qualitative comparison highlighting the architectural differences, use cases, and limitations of GrapheCL versus other systems would have added depth.\n  \n  **Suggestion:** In future iterations or extensions of the work, a dedicated section comparing GrapheCL's design and decisions with other systems would be beneficial. This section could delve into why certain design decisions were made in GrapheCL and how they differ from other systems.\n\n---\n\n**2. Limited Discussion on Scalability:**\n- Quantum circuits can grow significantly complex, especially as quantum computing moves towards practical, real-world applications. The paper seems to lack a thorough discussion on how GrapheCL scales with more complex circuits.\n\n  **Suggestion:** It would be beneficial to test GrapheCL with larger, more complex quantum circuits and discuss any potential bottlenecks or challenges. This would provide insights into its real-world applicability.\n\n---"
                },
                "questions": {
                    "value": "How does GrapheCL handle very large or complex quantum circuits in terms of both performance and accuracy? Are there any inherent scalability limitations or bottlenecks in the system?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "none"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698304335691,
            "cdate": 1698304335691,
            "tmdate": 1699637098571,
            "mdate": 1699637098571,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gZMIV4Ut8z",
                "forum": "3yyGlNHnlj",
                "replyto": "j55abM3vET",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The review is accidentally generated by ChatGPT"
                    },
                    "comment": {
                        "value": "Dear Reviewer eihc,\n\nThank you for your time. **The review is about Quantum Computing and is definitely not related to the content of our submission. Thus, we believe that the reviewer accidentally submitted the wrong review, which appears to be generated by ChatGPT.**\n\n**Could you please submit your correct review? Thank you very much!**\n\nThe Authors"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699987294233,
                "cdate": 1699987294233,
                "tmdate": 1699987567586,
                "mdate": 1699987567586,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wy1PmO3RPP",
            "forum": "3yyGlNHnlj",
            "replyto": "3yyGlNHnlj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8753/Reviewer_ZTuD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8753/Reviewer_ZTuD"
            ],
            "content": {
                "summary": {
                    "value": "Compared to other contrastive learning methods, GraphECL speeds up the model inference stage by using only MLP in the model inference phase. Meanwhile, the authors make GraphECL maintain a high generalization performance even in the process of rapid inference by improving InfoNCE loss."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed framework of GraphECL has great advantages on the smaller graph datasets.\n2. The authors remove the positive pairs from the InfoNCE loss, and optimize the model parameters of MLP by combining the positive pairs with the L2 loss. In addition, the authors proved the generalization performance of GraphECL theoretically.\n3. The author manages to speed up the model inference stage on the small graphs."
                },
                "weaknesses": {
                    "value": "1. The motivation of this paper is to address the scalability of contrastive learning while improving the speed of the model in the inference stage. But the authors confuse the scalability problem with increasing the speed of inference. The whole paper doesn't mention the memory consumption problem.\n2. The authors actually see some good scalable contrastive learning articles, such as the one mentioned in section 2, \"Rethinking and scaling up graph contrastive learning: an extremely efficient approach with group discrimination.\" But the author just categorizes this article as training speed and does not compare it, which is quite inappropriate.\n3. The author's experimental section is not very detailed. \n(1) There is a big problem with the chosen dataset. For example, some regular datasets (WikiCS, Am. Comp., Am. Photos, Co.CS, Co.Phy) used by BGRL, SUGRL, etc. are not experimented in this paper. For the scalability problem, they only choose a not very common dataset Snap-patents, and ignore the Ogbn-products, ogbn-mag and other datasets used by most of the experimentalists.\n(2) There is a lack of truly scalable experiments and its very important in the training process.\n(3) On the small graph, I think the training time is more heavily weighted compared to the inference time, while the authors only focus on the inference time. Conversely, in the large graph inference time is much more heavily weighted and the author rarely mentions it."
                },
                "questions": {
                    "value": "1. Why did you not do scalability experiments similar to those in articles [1] and [2].\n2. GraphECL compared to Graph-MLP, the positive pairs are obtained as first-order neighbors instead of nth-order. Combining this with Eq. 4 improves the performance of heterophilic graphs. I didn't understand the principle illustrated at the bottom of Eq. 4, can you explain it in detail?\n3. GraphECL is the optimal result in both Table 1 and Table 2, without mentioning the reduced performance of the other models during replication. Is it possible to compare the more recent Contrastive Learning methods in 2023 years and not only limited to within 2022 years.\n\n[1]Yizhen Zheng, Shirui Pan, Vincent Lee, Yu Zheng, and Philip S Yu. Rethinking and scaling up graph contrastive learning: An extremely efficient approach with group discrimination. Advances in Neural Information Processing Systems, 35:10809\u201310820, 2022. \n[2]Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Azabou, Eva L Dyer, Remi Munos, Petar Velickovic, and Michal Valko. Large-scale representation learning on graphs via bootstrapping. In ICLR, 2021."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8753/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8753/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8753/Reviewer_ZTuD"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698726817142,
            "cdate": 1698726817142,
            "tmdate": 1699637098442,
            "mdate": 1699637098442,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xShqmzTI8j",
                "forum": "3yyGlNHnlj",
                "replyto": "wy1PmO3RPP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1/3 - The Term \"Scalability\" Clarification and Comparison with  CGD [1]"
                    },
                    "comment": {
                        "value": "Thank you for your time and feedback. We apologize for any confusion and would like to clarify the term 'scalability' in our work and respond to your first two comments.\n\nThe message passing of the GNN encoder involves fetching topology and features from numerous neighboring nodes to perform inference on a target node, making it time-consuming and computation-intensive.\n\nIn our work, we specifically focus on reducing the inference time for inferring representations with a graph-aware MLP [3, 4, 5, 6], rather than aiming to reduce training time or memory consumption, as in [1, 2]. The term \u201cscalability\u201d in our paper refers to inference scalability, aligning with previous works [3, 4, 5, 6]. While we exclude the reduction of training time or memory [5, 6] from the scope of this work, we acknowledge it as a compelling direction that merits exploration in future research.\n\n**Thus, our studied problem is entirely different from [1, 2]. [1, 2] are great works which focus on reducing the training time and training memory, but they still need to utilize GNN to conduct inference, which is time-consuming and computation-intensive.**\n \nNevertheless, we completely agree with the reviewer that also including an empirical comparison with CGD  [1] would be beneficial. Following your suggestions, we have compared GraphECL with CGD [1]. Using the same data splits as outlined in our paper, we present the results in Table 8 in the updated submission (we also provide the results on some datasets in the following table). From the results, The results show that GraphECL outperforms CGD, particularly on heterophilic graphs. \n\n| Method   | &nbsp; Cora  | &nbsp; Citeseer  |  &nbsp;  Pubmed  |  &nbsp;  Photo  |   &nbsp;  Flickr  |      Crocodile  |  &nbsp;  Actor  |  Snap-patents  | Ogbn-arxiv| \n| --- | --- | --- | --- | --- |  --- | --- |  --- |  --- | --- |\n|CGD [1] | 83.90\u00b10.40 | 73.00\u00b10.60 | 81.30\u00b10.80 |  92.50\u00b10.60 | 46.33\u00b10.20 | 57.75\u00b10.39 | 28.27\u00b10.23 | 24.38\u00b10.57 | 71.60\u00b10.50 |\n| **GraphECL** | **84.25\u00b10.05** | **73.15\u00b10.41** | **82.21\u00b10.05** | **94.22\u00b10.11** | **48.49\u00b10.15** |  **65.84\u00b10.71** |  **35.80\u00b10.89** | **27.22\u00b10.06** | **71.75\u00b10.22** |\n\n\n\n[1] Rethinking and Scaling Up Graph Contrastive Learning: An Extremely Efficient Approach with Group Discrimination. NeurIPS 2022\n\n[2] Large-Scale Representation Learning on Graphs via Bootstrapping. ICLR 2021\n\n[3] Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation. ICLR 2021\n\n[4] Learning MLPs on Graphs: A Unified View of Effectiveness, Robustness, and Efficiency. ICLR 2023\n\n[5] Quantifying the Knowledge in GNNs for Reliable Distillation into MLPs. ICML 2023\n\n[6] VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs. Arxiv."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699891598567,
                "cdate": 1699891598567,
                "tmdate": 1700321317182,
                "mdate": 1700321317182,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L8of5G6Qac",
                "forum": "3yyGlNHnlj",
                "replyto": "wy1PmO3RPP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 2/3 - Comparison on Additional Large Datasets"
                    },
                    "comment": {
                        "value": "We greatly appreciate the reviewer's insightful suggestion, which we believe will improve our work.  We kindly want to remind the reviewer that we have conducted an extensive evaluation on 11 datasets Thus, we believe our experiments can strongly corroborate the effectiveness and scalability of GraphECL. Nevertheless, we completely agree with the reviewer that including an empirical comparison on more large datasets can further improve our paper.\n\nFollowing your suggestions, we have compared GraphECL with baselines on your suggested datasets. We present the results in Table 9 in the updated submission (we also provide the results in the following table). The results show that our simple GraphECL can still achieve better (or competitive) performance on suggested datasets compared to elaborate methods, especially on large graphs.\n\n| Method   | &nbsp; WikiCS  | &nbsp; Computers  |  &nbsp;  CS  |  &nbsp;  Physics  |   &nbsp;  Ogbn-arxiv  |     Ogbn-product |  \n| --- | --- | --- | --- | --- |  --- | --- |  \n|GCA | 78.35\u00b10.05 | 88.94\u00b10.15 | 93.10\u00b10.01 |  95.70\u00b10.04 | 68.20\u00b10.20 | 78.96\u00b10.15 |\n|SUGRL | 79.83\u00b10.31 | 88.90\u00b10.20 | 92.83\u00b10.23 |  95.38\u00b10.11 | 69.30\u00b10.20 | 82.60\u00b10.40 |\n|BGRL | 79.98\u00b10.10 | **90.34\u00b10.19** | 93.31\u00b10.13 |  95.73\u00b10.05 | 71.64\u00b10.12 | 81.32\u00b10.21 |\n|CCA-SSG | 79.31\u00b10.21 | 88.74\u00b10.28 | 93.35\u00b10.22 |  95.38\u00b10.06 | 71.21\u00b10.20 | 79.51\u00b10.05 |\n|AFGRL | 77.62\u00b10.49 | 89.88\u00b10.33 | 93.27\u00b10.17 |  95.69\u00b10.10 | 71.05\u00b10.32 | 79.20\u00b10.17 |\n| **GraphECL** | **80.17\u00b10.15** | 89.91\u00b10.35 | **93.51\u00b10.12** | **95.81\u00b10.12** | **71.75\u00b10.22** |  **82.69\u00b10.33** |\n\nWe also totally agree with the reviewer's comments: \"On the small graph, I think the training time is more heavily weighted compared to the inference time, while the authors only focus on the inference time. Conversely, in the large graph inference time is much more heavily weighted and the author rarely mentions it.\" We will explicitly state and mention this in the revision."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064334439,
                "cdate": 1700064334439,
                "tmdate": 1700322014012,
                "mdate": 1700322014012,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9eMrIlrw0N",
                "forum": "3yyGlNHnlj",
                "replyto": "wy1PmO3RPP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 3/3 - Respond to Questions"
                    },
                    "comment": {
                        "value": "We thank you for your insightful questions, and we would like to address your questions in what follows.\n\n**Q1. Why did you not do scalability experiments similar to those in articles [1] and [2].** \n\n**A1.** Thank you for your questions. As we mentioned in Response 1/3,  in our work, we specifically focus on reducing the inference time for inferring representations with a graph-aware MLP [3, 4, 5, 6] rather than aiming to reduce training time or memory consumption, as in [1, 2]. **The term \"scalability\" in our paper refers to inference scalability, aligning with previous works [3, 4, 5, 6]. While we exclude the reduction of training time or memory [5, 6] from the scope of this work, as [3,4,5,6] also did, we acknowledge it as another compelling direction that merits exploration in future research.**\n\n**Q2. GraphECL compared to Graph-MLP, the positive pairs are obtained as first-order neighbors instead of nth-order. Combining this with Eq. 4 improves the performance of heterophilic graphs. I didn't understand the principle illustrated at the bottom of Eq. 4, can you explain it in detail?** \n\n**A2.** Thank you for your question. As illustrated in Equation (4), GraphECL aims to bring cross-modal neighboring node representations closer together, i.e., $\\left(f_M\\left(v\\right), f_G(u)\\right)$. Thus, it's crucial to emphasize that GraphECL doesn't necessarily imply that the learned MLP representations $\\left(f_M\\left(v\\right), f_M(u)\\right)$ become identical, as Graph-MLP did.\n\nConsider a pair of 2-hop neighbors, $v$ and $v'$, both neighboring the same node $u$. Intuitively, by enforcing $f_{M}(v)$ and $f_{M}(v')$ to reconstruct (not align) the context representation of the same neighborhood $f_{G}(u)$, we implicitly make their representations similar. Thus, the 2-hop neighbors ($f_{M}(v)$ and $f_{M}(v')$) with the same neighborhood context serve as positive pairs that will be implicitly aligned (not the one-hop neighbors). This is also supported by our Theorem 4. Additionally, as depicted in Figure 9, GraphECL is not based on the one-hop homophily assumption but automatically captures graph structures based on different graphs beyond homophily. We also highlight the above clarification (marked with blue text) in the updated submission.\n\n**Q3. Is it possible to compare the more recent Contrastive Learning methods in 2023 years and not only limited to within 2022 years.**\n\n**A3.** Thank you for your suggestions. We kindly want to remind the reviewer that we have conducted an extensive evaluation of 10 methods, including many state-of-the-art and solid graph contrastive learning methods. Thus, we believe our experiments can strongly corroborate the effectiveness of GraphECL. Nevertheless, we completely agree with the reviewer that including an empirical comparison with more baselines in 2023 would be better. Given the above, we further compare our GraphECL with MA-GCL [7], which provides the source code. **From the following table, we can observe that our efficient GraphECL can still achieve better (or competitive) performance compared to MA-GCL proposed in 2023.**\n\n| Method   | &nbsp; Cora  | &nbsp; Citeseer  |  &nbsp;  Pubmed  |  &nbsp;  Photo  |   &nbsp;  Flickr  |      Crocodile  |  &nbsp;  Actor  |  Snap-patents  | \n| --- | --- | --- | --- | --- |  --- | --- |  --- |  --- |\n|MA-GCL [7] | 83.30\u00b10.40 | **73.60\u00b10.10** | **83.50\u00b10.40** |  93.80\u00b10.10 | 47.01\u00b10.35 | 59.62\u00b10.30 | 29.56\u00b10.27 | 25.49\u00b10.41 |\n| **GraphECL** | **84.25\u00b10.05** | 73.15\u00b10.41 | 82.21\u00b10.05 | **94.22\u00b10.11** | **48.49\u00b10.15** |  **65.84\u00b10.71** |  **35.80\u00b10.89** | **27.22\u00b10.06** |\n\n[1] Rethinking and Scaling Up Graph Contrastive Learning: An Extremely Efficient Approach with Group Discrimination. NeurIPS 2022\n\n[2] Large-Scale Representation Learning on Graphs via Bootstrapping. ICLR 2021\n\n[3] Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation. ICLR 2021\n\n[4] Learning MLPs on Graphs: A Unified View of Effectiveness, Robustness, and Efficiency. ICLR 2023\n\n[5] Quantifying the Knowledge in GNNs for Reliable Distillation into MLPs. ICML 2023\n\n[6] VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs. Arxiv.\n\n[7] MA-GCL: Model Augmentation Tricks for Graph Contrastive Learning. AAAI 2023"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329482712,
                "cdate": 1700329482712,
                "tmdate": 1700329482712,
                "mdate": 1700329482712,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EL8a3dGRam",
                "forum": "3yyGlNHnlj",
                "replyto": "wy1PmO3RPP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion period ending soon; We would like to hear back from Reviewer ZTuD"
                    },
                    "comment": {
                        "value": "Dear Reviewer ZTuD,\n\nWe gratefully appreciate your time in reviewing our paper and your insightful comments.  \n\nWe have provided additional results in response to your latest comments. We believe this evidence successfully addresses all your concerns. \n\nWith only four days left in the discussion period, we would like to confirm whether the reviewer has seen our latest comment and if there are any final clarifications they would like. If the reviewer's concerns are clarified, we would be grateful if the reviewer could update their review and score to reflect that. This way, we will know that our response has been seen. Once again, many thanks for your time and dedication to the review process; we are extremely grateful.\n\nThe Authors of \"GraphECL: Towards Efficient Contrastive Learning for Graphs\""
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329929893,
                "cdate": 1700329929893,
                "tmdate": 1700329929893,
                "mdate": 1700329929893,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CYWp3VAqJN",
            "forum": "3yyGlNHnlj",
            "replyto": "3yyGlNHnlj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8753/Reviewer_ezu8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8753/Reviewer_ezu8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a self-supervised learning scheme for graph neural networks that is both efficient and effective. To achieve this, the authors suggest using an MLP encoder during inference, which is much faster than a GNN-based encoder. The main contribution of this paper is cross-model contrastive learning, where positive samples are obtained through MLP and GNN representations from the central node and its neighbors. Enabling the MLP encoder efficiently encodes graph topology without relying on invariant and homophily assumptions. Extensive experiments show that this method outperforms other state-of-the-art methods in real-world tasks, with better inference efficiency and generalization to homophilous and heterophilous graphs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written and well-organized\nThe evaluations support the authors' claims, such as the ability of the proposed method to learn on heterophilic datasets. Significantly outperform previous contrastive-based methods. \nThe proposed method has strong inference scalability and significantly reduces inference time compared to the prior art."
                },
                "weaknesses": {
                    "value": "The method is mainly evaluated on node property prediction tasks. However, additional evaluation on graph property prediction will be essential. This is because in graph property prediction datasets, efficient encoding on graph topology plays a much more important role.\nThe authors have not provided an analysis of how the number of hidden layers impacts the method's performance. This analysis is very important."
                },
                "questions": {
                    "value": "Apart from the issues mentioned in the \"Weakness\" section, I strongly believe that the proposed method's effectiveness in capturing inter-neighborhood information will be further highlighted by the evolution of LRGB datasets. This will serve as an additional argument to support the second point mentioned in the \"Weakness\" section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827425182,
            "cdate": 1698827425182,
            "tmdate": 1699637098331,
            "mdate": 1699637098331,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cWwN7uqecf",
                "forum": "3yyGlNHnlj",
                "replyto": "CYWp3VAqJN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respone to Reviewer ezu8"
                    },
                    "comment": {
                        "value": "Dear reviewer ezu8, we appreciate your positive feedback on our paper's soundness, novel insights, and contribution. Please find our detailed responses below:\n\n**Q1. The method is mainly evaluated on node property prediction tasks. However, additional evaluation on graph property prediction will be essential.**\n\n**A1.** Thank you for your question and suggestion. For graph classification, we can use a non-parameterized graph pooling (readout) function, such as MeanPooling, to obtain the graph-level representation. In our experiments, we focus on graph classification using three benchmarks: PROTEINS and MUTAG. We follow the same experimental setup as GraphCL [1]. The results are presented in the following table. From the table, we observe that our simple GraphECL performs well on the graph classification task and achieves better performance compared to the baselines.\n\n| Method  |  Graph-MLP | &nbsp;   VGAE  | CCA-SSG | &nbsp; DSSL | GraphCL |  GraphECL |\n| --- | --- | --- | --- | --- | --- | --- |\n| MUTAG | 75.8\u00b12.0 | 84.4\u00b10.6  | 85.8\u00b10.4  | 87.2\u00b11.5 | 86.8\u00b11.3 | **88.5\u00b11.2**  |\n| PROTEINS | 71.1\u00b11.5  | 74.0\u00b10.5 | 73.1\u00b10.6 | 73.5\u00b10.7 | 74.4\u00b10.5  |**75.2\u00b10.3** |\n\n\n\n**Q2.  The authors have not provided an analysis of how the number of hidden layers impacts the method's performance.**\n\n**A2.** Thank you for your valuable comment! In the following table, we analyze the effect of hidden layers. From the table, it is evident that our GraphECL is not very sensitive to hidden layers. However, having more layers generally leads to better performance for both homophilic and heterophilic graphs.\n\n| Layers  |  Cora | Citeseer  | Photo | Texas | Cornell |  Wisconsin |\n| --- | --- | --- | --- | --- | --- | --- |\n| 1 | 83.57\u00b10.03 | 73.15\u00b10.41  | 93.47\u00b10.15  | 71.19\u00b12.58 | 67.28\u00b14.35 | 76.54\u00b11.28  |\n| 2 | **84.25\u00b10.05**  | **73.56\u00b10.50** | **94.22\u00b10.11** | 75.95\u00b15.33 | 69.19\u00b16.86  |**79.41\u00b12.19** |\n| 4 | 84.17\u00b10.03  | 73.21\u00b10.30 | 94.10\u00b10.12 | **76.21\u00b13.95** | **69.33\u00b15.51**  |79.27\u00b11.75 |\n\n\n\n\n**Q3. The proposed method's effectiveness in capturing inter-neighborhood information will be further highlighted by the evolution of LRGB datasets.**\n\n**A3.**  Following your suggestions, we compare GraphECL with two graph contrastive learning methods, BGRL [2] and CCA-SSG [3], on PascalVOC-SP [4]. For all methods, we employ GCN as the backbone. From the table below, we can observe that GraphECL performs well on PascalVOC-SP, achieving better performance compared to the baselines. This further strengthens our contribution.\n\n| Method  |  BGRL |  CCA-SSG  | GraphECL  | \n| --- | --- | --- | --- | \n| PascalVOC-SP | \t0.1356\u00b10.0087 | 0.1437\u00b10.0095  | **0.1588\u00b10.0091**  | \n\nIn light of these responses, we sincerely hope our rebuttal has addressed your comments, and believe that your comments do not affect our key contributions and can be easily addressed in the revision. We also genuinely hope you will reconsider increasing your score. If you have any other comments, please do share them with us, and we will address them further. Thank you for your efforts!\n\n[1] Graph Contrastive Learning with Augmentations. NeurIPS 2020\n\n[2] Large-Scale Representation Learning on Graphs via Bootstrapping. ICLR 2022\n\n[3] From Canonical Correlation Analysis to Self-supervised Graph Neural Networks. NeurIPS 2021\n[4] Long Range Graph Benchmark. NeurIPS 2022"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700429568890,
                "cdate": 1700429568890,
                "tmdate": 1700429568890,
                "mdate": 1700429568890,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DUfkGDc3PL",
                "forum": "3yyGlNHnlj",
                "replyto": "cWwN7uqecf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8753/Reviewer_ezu8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8753/Reviewer_ezu8"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors, \n\nI am fully satisfied with the clarifications and evaluations, which significantly strengthen the manuscript.\nI expect the authors to incorporate them into a revised version."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491193463,
                "cdate": 1700491193463,
                "tmdate": 1700491193463,
                "mdate": 1700491193463,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XbaBot3Z2H",
            "forum": "3yyGlNHnlj",
            "replyto": "3yyGlNHnlj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8753/Reviewer_xAqE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8753/Reviewer_xAqE"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new objective for contrastive self-supervised learning on graphs. It uses an MLP and a GNN to form positive pairs based on cross-model neighbouring relations, and negative pairs from inter-model and intra-model representations, where either the input to the MLP/GNN is from a randomly sampled node. \nThe paper is empirically evaluated on homophilic and heterophilic datasets, showing better results than other competitive self-supervised graph learning methods. Moreover, an ablation study analyses the importance of the proposed components, such as using only negative inter-model or negative intra-model pairs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea is sensible and the paper is generally well-presented. Moreover, the rationale of using only the MLP encoder at inference time is very valuable, as it could significantly speed up the deployment of graph learning methods."
                },
                "weaknesses": {
                    "value": "I think the main weakness comes from the empirical evaluation section:\n- firstly, it seems like the snap-patents results are lower than expected. In Lim et al, MLP achieves 31%, GCN 45%, both being higher than the reported numbers (GraphECL 27%)\n- secondly, many of the chosen datasets do not give reliable insights, as they are too small or the variance is too high across splits (Cora, Citeseer, Pubmed, Cornell, Texas, Wisconsin\u2026). \n\nI encourage the authors to include larger datasets, for example from the OGB suite, or those used in some of the manuscripts\u2019 of the baselines (WikiCS, Amazon Computers, Coauthor CS, Coauthor Phy) or from Lim et al for heterogeneous datasets.\n\nMoreover, it might be a good idea to include the simple baselines of an MLP and a GCN, to clarify GraphECL's contribution, especially for snap-patents and, similarly, for flickr."
                },
                "questions": {
                    "value": "How would the complementary negative pairs  $(f_M(v^{-}), f_G(u))$ and  $(f_M(v), f_M(v^{-}))$ influence learning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8753/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8753/Reviewer_xAqE",
                        "ICLR.cc/2024/Conference/Submission8753/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699111964409,
            "cdate": 1699111964409,
            "tmdate": 1700739421709,
            "mdate": 1700739421709,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "56F4k5VPpa",
                "forum": "3yyGlNHnlj",
                "replyto": "XbaBot3Z2H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xAqE"
                    },
                    "comment": {
                        "value": "Thanks for your time and feedback! We would like to clarify some misunderstandings regarding our approach.\n\n**Q1. In Lim et al, MLP achieves 31%, GCN 45%, both being higher than the reported numbers (GraphECL 27%)**\n\n**A1.** Thank you for pointing out (Lim et al., 2021). **However, it's essential to note that their evaluation setting differs from ours. Our paper focues on evaluting GraphECL in the unsupervised setting, whereas (Lim et al., 2021) employed the fully supervised setting, leading to reported higher performance.**\n\n**Q2. many of the chosen datasets are too small or the variance is too high across splits**\n\n**A2.** Thank you for your suggestions! We'd like to remind the reviewer that we have tested GraphECL on 11 widely-used datasets, as detailed in Table 6, **including larger datasets such as Snap-patents and Flickr**, in comparison to (Cora, Citeseer, Pubmed, Cornell, Texas, etc.). While we acknowledge the reviewer's point about the importance of evaluating on more large datasets, following your suggestions, we have compared GraphECL with baselines on more datasets. We present the results in Table 9 in the updated submission (we also provide the results in the following table). **The results  show that GraphECL can still achieve better (or competitive) performance on suggested datasets compared to elaborate methods, especially on large graphs.**\n| Method   | &nbsp; WikiCS  | &nbsp; Computers  |  &nbsp;  CS  |  &nbsp;  Physics  |   &nbsp;  Ogbn-arxiv  |     Ogbn-product |  \n| --- | --- | --- | --- | --- |  --- | --- |  \n|GCA | 78.35\u00b10.05 | 88.94\u00b10.15 | 93.10\u00b10.01 |  95.70\u00b10.04 | 68.20\u00b10.20 | 78.96\u00b10.15 |\n|SUGRL | 79.83\u00b10.31 | 88.90\u00b10.20 | 92.83\u00b10.23 |  95.38\u00b10.11 | 69.30\u00b10.20 | 82.60\u00b10.40 |\n|BGRL | 79.98\u00b10.10 | **90.34\u00b10.19** | 93.31\u00b10.13 |  95.73\u00b10.05 | 71.64\u00b10.12 | 81.32\u00b10.21 |\n|CCA-SSG | 79.31\u00b10.21 | 88.74\u00b10.28 | 93.35\u00b10.22 |  95.38\u00b10.06 | 71.21\u00b10.20 | 79.51\u00b10.05 |\n|AFGRL | 77.62\u00b10.49 | 89.88\u00b10.33 | 93.27\u00b10.17 |  95.69\u00b10.10 | 71.05\u00b10.32 | 79.20\u00b10.17 |\n| **GraphECL** | **80.17\u00b10.15** | 89.91\u00b10.35 | **93.51\u00b10.12** | **95.81\u00b10.12** | **71.75\u00b10.22** |  **82.69\u00b10.33** |  \n\n**Q3. it might be a good idea to include the simple baselines of an MLP and a GCN.**\n\n**A3.**  Thank you for your suggestions! **We believe there might be some misunderstandings. Our primary focus is on designing a self-supervised algorithm without labels, rather than specific GNN or network architectures. Consequently, an empirical comparison with an MLP and a GCN is relatively less straightforward.** Instead, we have compared our self-supervised learning algorithm with various others based on MLP (Graph-MLP, SUGRL) and GNN (BGRL, DGI, GCA, SUGRL, AFGRL, etc.).\n\n**Q4. How would the complementary negative pairs influence learning?**\n\n**A4.** Thanks for your insightful questions! We introduce an explicit negative pair to enhance representation diversity. As demonstrated in our Theorem 1, the inter-model negative pair $\\left(f_M\\left(v^{-}\\right), f_G(u)\\right)$ is crucial for capturing the 1-hop neighborhood distribution. Moreover, complementary negative pairs are essential for GraphECL. Our ablation studies indicate that GraphECL without both negative pairs results in a performance drop. Therefore, the addition of complementary negative pairs is crucial for improving representation diversity and increasing inter-class variation, ultimately leading to robust generalization. We will explicitly state this in the main text."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699891559463,
                "cdate": 1699891559463,
                "tmdate": 1700320069547,
                "mdate": 1700320069547,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "boNYGoPjTt",
                "forum": "3yyGlNHnlj",
                "replyto": "XbaBot3Z2H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer xAqE, would you mind confirming if our rebuttal addresses your comments?"
                    },
                    "comment": {
                        "value": "Dear Reviewer xAqE,\n\nWe gratefully appreciate your time in reviewing our paper and your insightful comments. \n\nWe made our greatest efforts to address your concerns in the rebuttal. We would appreciate it if you could consider increasing your score. Thank you very much once again, and we are happy to follow up with any additional questions you may have!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700320228679,
                "cdate": 1700320228679,
                "tmdate": 1700496132770,
                "mdate": 1700496132770,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]