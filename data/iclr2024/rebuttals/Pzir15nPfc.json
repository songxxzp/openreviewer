[
    {
        "title": "Contextual Vision Transformers for Robust Representation Learning"
    },
    {
        "review": {
            "id": "7xx05cH1Fu",
            "forum": "Pzir15nPfc",
            "replyto": "Pzir15nPfc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5863/Reviewer_Y1Xv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5863/Reviewer_Y1Xv"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Contextual Vision Transformers (ContextViT), a method to address structured variations and distribution shifts in image datasets. It leverages context tokens and token inference models to enable robust feature representation learning across groups with shared characteristics. The paper provides evidence of ContextViT's effectiveness through experiments in gene perturbation classification and pathology image classification."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper introduces a novel method, ContextViT, to address structured variations and distribution shifts in image datasets. It brings a unique perspective to the problem of improving feature representations for vision transformers.\n- The paper is well-written and provides clear explanations of the methodology, experiments, and results.\n- ContextViT is extensively evaluated in different tasks, showcasing its effectiveness in improving out-of-distribution generalization and resilience to batch effects."
                },
                "weaknesses": {
                    "value": "- How to chose and define the \"in-context\" prompt is unclear.\n- While the paper is well-structured and well-written, it would be beneficial to include more detailed comparisons with related work to highlight the novelty of the proposed approach.\n- In the \"Out-of-Distribution Generalization (Pathology Images)\" section, it's not entirely clear what \"linear probing accuracy\" means and how it relates to out-of-distribution generalization. A more in-depth explanation of this metric would improve the clarity of the paper."
                },
                "questions": {
                    "value": "- Are there any specific use cases or domains where ContextViT is particularly well-suited, and are there any limitations or scenarios where it may not perform as effectively?\n- Could the authors provide more insights into how ContextViT's approach to handling structured variations and distribution shifts could be applied in practical applications outside of the ones discussed in the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5863/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698648431725,
            "cdate": 1698648431725,
            "tmdate": 1699636621015,
            "mdate": 1699636621015,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m1WeG5vcuP",
                "forum": "Pzir15nPfc",
                "replyto": "7xx05cH1Fu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5863/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed review and constructive feedback.\n\n**Context Choice and Definition:** We carefully select the group membership variable, as depicted in Figure 2, ensuring datasets share a conditional dependency structure with their environment. For instance, pathology images can vary based on the staining procedures or machines used by different hospitals. To address this, we chose the hospital identity indicator as the context variable, which has shown to significantly improve model performance in our experiments. \n\n**Comparisons with Related Work:** Different from previous work, ContextViT explores token conditioning based on the group membership of the data. ContextViT introduces two novel technical enhancements:\n+ Context inference network that maps example sets from the same group into a context token;\n+ Layer-wise context conditioning that allows the ViT to integrate context at various network depths, enhancing generalization. \n+ We also establish a principled mathematical link from ContextViT to in-context learning for distribution shifts. \n\nWe have updated our related work section to better highlight these differences.\n\n**Linear probing:** Linear probing, a common protocol for evaluating self-supervised learning [1], involves training a linear classifier on top of fixed pre-trained ViT and ContextViT backbones to predict a target label, such as the presence of cancer. In the Cameylon17 dataset, the classifier is trained on images from three training hospitals and tested on a fourth, unseen hospital. Successful generalization to the held-out hospital indicates aligned feature distributions. We have expanded our discussion on this evaluation metric to provide further clarity.\n\n**Use Cases and Limitations:**\n+ ContextViT is particularly well-suited for imaging applications where data are grouped or batched. \n+ However, if the group membership variables are mis-specified and fail to capture the environmental context, ContextViT's performance may not exceed that of a regular ViT. For example, a randomly assigned context variable would not improve the model's performance. The effectiveness of ContextViT relies on the user's understanding of the dataset to accurately specify group memberships. \n+ We appreciate the reviewer's insight on this matter and have included a discussion on the potential complexities and dependencies that could affect generalization in our paper.\n\n**Practical Applications and Structured Variations:**\n+ This links very well to the previous question. In practice, users want to consider invariant factors of variation per group that systematically shift across groups. For example, shared environments over datasets or global variables in the plated context (see Figure 2 for a graphical model interpretation).\n+ We also note that oftentimes this can be an exploratory process where people need to train specific models with different contexts and see whether the resulting models explain away the distributions shifts when there is no obvious choice of the context variable.\n+ Lastly, the question interfaces with our desire for a more formal framework in future work based on statistical tests on top of ViT embeddings to detect biases over various covariates and suggest useful context,  which would be a useful tool and would link this work more to causal inference.\n\nWe trust that this rebuttal has addressed the points raised by the reviewer. We are thankful for your time and effort and are open to any further questions.\n\n**References:**\n1. Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" International conference on machine learning. PMLR, 2020."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665006275,
                "cdate": 1700665006275,
                "tmdate": 1700665006275,
                "mdate": 1700665006275,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Vqkd072sYX",
            "forum": "Pzir15nPfc",
            "replyto": "Pzir15nPfc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5863/Reviewer_hyoq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5863/Reviewer_hyoq"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a Contextual Vision Transformers (ContextViT) based on ViT. ContextViT is designed for adapting ViTs to OOD data with varying latent factors. This work is inspired by in-context learning and prepends tokens to input sequences for alleviating model performance. This paper finds out that standard context tokens might not be able to generalize to unseen domains, therefore it proposes a context inference network that estimates context tokens from input images. The proposed method is evaluated with cell-imaging and histopathology datasets and achieves performance improvements under distribution shifts.\n\nPros:\n\n- This paper is well-written and easy to follow.\n- Figure 1 is well drawn to illustrate the overall idea of this work.\n- Layer-wise context conditioning is well-motivated and makes sense.\n\nCons:\n\nThe novelty of this work is limited.\n- The intrinsic difference between this work and visual prompting [1] is unclear. It seems that visual prompting can also fit this OOD scenario. \n- The key idea of this work is similar to [2], which also uses a network to predict the context/domain tokens.\n- The comparison in experiment section is insufficient.\n- Lack of visualization of the learned context token, which shows the difference of context tokens of different groups.\n\nThe paper is simple and effective, but its novelty is unfortunately limited, and analysis for the insight of this approach is absent.\n\n[1] Jia, Menglin, et al. \"Visual prompt tuning.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n[2] Zhang, Xin, et al. \"Domain Prompt Learning for Efficiently Adapting CLIP to Unseen Domains.\" arXiv preprint arXiv:2111.12853 (2021)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper is well-written and easy to follow.\n- Figure 1 is well drawn to illustrate the overall idea of this work.\n- Layer-wise context conditioning is well-motivated and makes sense."
                },
                "weaknesses": {
                    "value": "The novelty of this work is limited.\n- The intrinsic difference between this work and visual prompting [1] is unclear. It seems that visual prompting can also fit this OOD scenario. \n- The key idea of this work is similar to [2], which also uses a network to predict the context/domain tokens.\n- The comparison in experiment section is insufficient.\n- Lack of visualization of the learned context token, which shows the difference of context tokens of different groups.\n\nThe paper is simple and effective, but its novelty is unfortunately limited, and analysis for the insight of this approach is absent."
                },
                "questions": {
                    "value": "-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5863/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5863/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5863/Reviewer_hyoq"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5863/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698668871432,
            "cdate": 1698668871432,
            "tmdate": 1699636620915,
            "mdate": 1699636620915,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gr8e1cbS2V",
                "forum": "Pzir15nPfc",
                "replyto": "Vqkd072sYX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5863/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed review and constructive feedback.\n\n**Related work:** We are grateful for the additional related work highlighted by the reviewer. We have incorporated these references into our paper to provide a more comprehensive context for our contributions.\n+ Visual prompt tuning: Unlike visual prompt tuning, which learns extra tokens for each downstream task, our work defines the extra token based on the group membership of the data. This distinction is crucial for enabling our model to adapt to new distributions without task-specific tokens.\n+ Domain prompt learning: Similar to visual prompt tuning, domain prompt learning generates a consistent output prompt for each task, as the text label set remains unchanged. In our work, we don\u2019t consider text inputs as they are not meaningful in cross-distribution generalization. For instance, the CLIP text encoder would not produce meaningful representations using the group name such as  \u201cplate BR00116991\u201d and \u201cplate BR00116993\u201d or \u201chospital A\u201d and \u201chospital B\u201d. Moreover, ContextViT's extra token is dynamically derived from the group membership, allowing for generalization beyond the scope of fixed tasks.\n\n**Novelty:** In addition to our unique perspective on token conditioning based on the data groups, ContextViT is complemented by two novel technical enhancements:\n+ Context inference network maps sets of examples from the same group into a context token, enabling ContextViT to generalize to unseen groups during training.\n+ Layer-wise context conditioning enables the ViT to integrate context at various depths within the network, not just at the input layer, which significantly improves generalization.\n+ Finally, we provide a principled mathematical linking from ContextViT to in-context learning for distribution shift.\n\n**Comparison in the experiment section:**\n+ ContextViT has been evaluated as a plug-and-play enhancement for three established pre-trained ViTs (DINO, SWAG, CLIP) and has consistently outperformed recent fine-tuning baselines in generalization tasks.\n+ In self-supervised representation learning, ContextViT has set a new state-of-the-art on the Camelyon17-WILDS dataset, outperforming 26 other baselines. The full comparison is available on the WILDS leaderboard (https://wilds.stanford.edu/leaderboard).\n\n**Analysis of the model:**\n+ We have conducted a thorough comparison of ContextViT with its non-context-conditioned ViT counterparts (Tables 1, 2, and 3).\n+ Our ablation studies and runtime comparisons (Table 4) carefully examine the impact of layer-wise context conditioning and various implementations of the context inference network.\n+ Figures 5 (in the appendix)  explores ContextViT's effectiveness with limited data for context token inference.\n+ Figure 7 (in the appendix)  provides a visualization of the context token. We observe that the similarity between the context tokens are closely related to the similarity of the input images.\n+ Figure 6 (in the appendix) provides a visual representation of the attention heatmap produced by ContextViT, illustrating its interpretability.\n\nWe trust that this rebuttal has addressed the points raised by the reviewer. We are thankful for your time and effort and are open to any further questions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664879339,
                "cdate": 1700664879339,
                "tmdate": 1700664879339,
                "mdate": 1700664879339,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZJxLfFrgQv",
            "forum": "Pzir15nPfc",
            "replyto": "Pzir15nPfc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5863/Reviewer_4Dw1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5863/Reviewer_4Dw1"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes ContextViT to address the distribution shift between different datasets. ContextViT uses a context inference model taking the dataset as input to get a context embedding for the dataset, and predicts the label conditioned on the context embedding (token). It also makes this process layer-wise to capture different-scale distribution shift."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper presents a method to mitigate the distribution gap between different datasets. Based on their experimental results, the proposed method, ContextViT, has the ability to improve the performance under distribution shift."
                },
                "weaknesses": {
                    "value": "- The paper mentioned that the proposed method applies the concept of in-context learning in vision transformer. However, in my opinion, in-context learning is a kind of few-shot learning, which predicts based on the (data, label) pair of a few samples, unlike the usage of all the dataset-c data (or a batch of the data) in this paper. The method looks like a summarization of the dataset information and then makes the prediction based on that summarization.\n\n- The method requires a lot of distribution-c data at the inference stage and increases the inference overhead. \n\n- The oracle-context model is very similar to some prompt tuning works, like Visual Prompt Tuning & Prompt Learning for Vision-Language Models, but these works are not discussed in the paper."
                },
                "questions": {
                    "value": "Please see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5863/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815668878,
            "cdate": 1698815668878,
            "tmdate": 1699636620798,
            "mdate": 1699636620798,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CNQq8MsYIl",
                "forum": "Pzir15nPfc",
                "replyto": "ZJxLfFrgQv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5863/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed review and constructive feedback.\n\n**Concept of in-context learning:** We appreciate your input on this matter. To clarify, we have updated our methods section to distinguish between data conditioning and data+label conditioning. In-context learning is a wide catch term for various types of data conditioning. We consider the case of in-context learning for generalization under group membership of the data and derive our method under that assumption. This is for instance when we condition the model on different environments, a very common assumption. This consideration is also driven by the reality that labels for test distributions are often unavailable, whereas data is plentiful. The context inference network is designed to summarize dataset information, enabling the vision transformer to adapt its representation of the input image accordingly.\n\n**Requires a lot of distribution-c data at the inference stage:**\n+ While in an ideal world we would need all the distribution-c data to derive the context token, we empirically tested the data hunger of the model. In Figure 5, we demonstrate ContextViT's performance across a range of testing batch sizes, from 1 to 512. The model's ability to infer context improves with more examples; however, it still achieves state-of-the-art performance with as few as 8 examples, outperforming all existing benchmarks on the WILDS leaderboard (https://wilds.stanford.edu/leaderboard).\n+ We also highlight that in many real-world settings, such as new hospitals, devices, users, or biological experiments, testing data is naturally batched. ContextViT is well-suited for these practical applications.\n\n**Related work:** We are grateful for the identification of missing references. These have now been incorporated into our revised paper. Our approach is distinct in its use of group membership to define the context token, coupled with two innovative technical contributions:\n+ Context inference network maps sets of examples from the same group into a context token, allowing ContextViT to generalize to new groups unseen during training.\n+ Layer-wise context conditioning enables the ViT to integrate context at various network depths, not just at the input layer, which significantly improves generalization.\n+ Finally, we provide a principled mathematical linking from ContextViT to in-context learning for distribution shift.\n\nWe believe this rebuttal has addressed the points raised by the reviewer. We are thankful for your time and effort and are open to any further questions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664811092,
                "cdate": 1700664811092,
                "tmdate": 1700664811092,
                "mdate": 1700664811092,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "y6HZ20MhzI",
            "forum": "Pzir15nPfc",
            "replyto": "Pzir15nPfc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5863/Reviewer_kTos"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5863/Reviewer_kTos"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an improved ViT where some group-specific context information from the sub-groups in datasets is collected and generated from those images in a group. The network generates the context token from those images and appends them to image patch embeddings. Their experiments show some improvement of this ViT on some group-specific datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of capturing context information from the datasets is interesting.\nThe writing of this method is clear and easy to follow.\nThe experiments demonstrate the efficiency of their proposed framework on both the dataset with the same distribution and other datasets with different distributions."
                },
                "weaknesses": {
                    "value": "The view and impact of this paper are limited. It seems the method focuses on improving the performance of the datasets that contain several distinct groups. Although the authors demonstrate improvements on some specific datasets, the improvement in general image tasks is still unclear. It is suggested to widely evaluate their framework on other popular datasets and tasks or extend related techniques to improve the capability of transfer learning from one task to some other tasks. It should also be compared with more related works.\n\n\nDespite the proposed contextual learning paradigm, the technical contributions in this paper are limited and not novel enough.\n\n\nSome unclear presentations:\n\n1. Figure 1 is unclear and somehow misleading. The source of the context (where those images come from) and the function (input, output) of the inference model should be labeled. I strongly suggest redoing this figure.\n\n2. The end of page 5 is missing.\n\n3. Table 2 looks messy and should be redesigned."
                },
                "questions": {
                    "value": "What would the performance be if we want to apply this framework to a large dataset that was combined with several small datasets?\n\nIf we don't know the sub groups of the data, is there anyway to benefit from the proposed framework?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5863/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698862685115,
            "cdate": 1698862685115,
            "tmdate": 1699636620699,
            "mdate": 1699636620699,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0EQVsLNEFh",
                "forum": "Pzir15nPfc",
                "replyto": "y6HZ20MhzI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5863/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed review and constructive feedback.\n\n**Scope of the paper:** We agree that ContextViT is tailored for scenarios with grouped datasets, which is a common occurrence in real-world applications where data often arrives in batches or groups. The focus of this paper is on enhancing robustness across distribution shifts. We show that instead of having to learn one model per distribution, we can share the parameters across the tasks and utilize the context inference network to inform the model of the specific distribution. We separately show that by doing so and learning a context inference network we can also learn to generalize to new test distributions effectively. \n\n**Improvement in general image tasks:**\n+ In this paper, we have included three widely-recognized imaging benchmarks from WILDS and a recently released cell imaging benchmark released by the Broad Institute.\n+ The reviewer asks us to focus on general image tasks. We highlight that instead of going for artificial benchmarks, we went for real world datasets that exhibit specific problem structure that is not tackled by more general machinery. We would ask the reviewer to see the value of overcoming unsolved problems rather than iterating on known problems.\n+ To be more specific, in many practical applications (especially scientific domains such as biological discovery), being able to deal with batch effects is a key hindrance to the successful application of representation learning. This is exemplified by our results on the recent JUMP-CP cell painting dataset released by the Broad institute of Havard and MIT, for which ContextViT consistently improves the generalization across different data distributions. We also extensively tested ContextViT on pathology imaging such as Camelyon17-WILD. This is a key problem as real world applications like medical imaging often suffer from distribution shifts in batches due to the employment of different staining procedures or machines across hospitals. We hope we have demonstrated the importance of the task that we tackle to the reviewer.\n\n**Evaluation and baselines:**\n+ ContextViT has been tested as a plug-and-play enhancement for three well-established pre-trained ViTs (DINO, SWAG, CLIP) in a supervised learning context. It has consistently outperformed two recent fine-tuning baselines in terms of generalization.\n+ In the realm of self-supervised representation learning, we have considered both the in-distribution generalization and out-of-distribution generalization. In the first, ContextViT consistently outperformed ViT across all testing configurations. In the second, ContextViT has achieved a new state-of-the-art on the Camelyon17-WILDS dataset, surpassing 26 other baselines. Due to space constraints, we only included the top baselines in Table 3. The complete list of baselines is available on the WILDS leaderboard (https://wilds.stanford.edu/leaderboard).\n\n**Technical contributions:** We recognize prior work on conditioning ViTs with extra tokens in our related work section. Our approach uniquely leverages group membership to define the context token and introduces two novel technical enhancements:\n+ The context inference network effectively maps sets of examples from the same group into a context token. This enables ContextViT to generalize on new groups that it hasn\u2019t seen during training, as demonstrated by our empirical studies. \n+ Layer-wise context conditioning allows the ViT to utilize context at multiple levels of the network, rather than just the input layer, enhancing generalization capabilities.\n+ Finally, we provide a principled mathematical linking from ContextViT to in-context learning for distribution shift.\n\n**Clarity:**\n  + Based on your suggestions, we have revised the figure to enhance its clarity and readability.\n  + The end of page 5 continues on page 6 (under Table 1).\n\n**Application to large datasets with small sub datasets:** The JUMP-CP benchmark, comprising multiple data plates from various perturbations, serves as an example of combining \"small datasets\".\n\n**Without knowledge of the group membership, is there any way to benefit from the proposed framework?**\n+ For this paper, we assume that the indicator function of the group membership is known during training, a realistic assumption given that data is often generated in group fashion in the real world. In future work, one might strive to infer latent group membership from the data, but we don\u2019t tackle that in this submission.\n+ In Figure 5 (in the appendix), we explored the performance of ContextViT with a testing batch size of one. This would be equivalent to the situations where we don\u2019t assume knowledge of the group membership at test time. We have demonstrated that this self-conditioning still improves the performance on top of the standard ViT.\n\nWe trust that this rebuttal addresses the concerns raised. We are grateful for your time and effort and welcome any further questions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664753391,
                "cdate": 1700664753391,
                "tmdate": 1700664753391,
                "mdate": 1700664753391,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]