[
    {
        "title": "Space-Time Attention with Shifted Non-Local Search"
    },
    {
        "review": {
            "id": "YBO97zxsWn",
            "forum": "Mqukp3Lsnt",
            "replyto": "Mqukp3Lsnt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4156/Reviewer_CKa5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4156/Reviewer_CKa5"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a dense offset field (like optical flow) by using attention, and shows demonstrations of video frame alignment (Fig7) and video denoising (Fig8).\n\nThe paper claims to propose a kind of a combination of NATTAN (non-local search) and GDA (shift or displacement prediction) as shown in Figure 1. It first finds a large displacement somehow, then refines it using attention, as demonstrated in Fig3. Thus the proposed method is called \"shifted\" \"non-local search\".\n\nThe core of the paper is section 3.1, which shows the whole process using top-k search with attention, and section 3.2 introduces the case when shift F is given.\n\nSection 3.3 justifies that the refinement of optical flow leads to a better offset estimation, and section 3.4 simply states that the method is implemented on CUDA (which is called in-place).\n\nExperiments show that the proposed method gives a better quality, less memory usage (Fig9) and faster computation (Fig10)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This approach is somewhat on the hardware side and is thus very advantageous in terms of speed and memory consumption over other methods. The method is implemented \"in-place\", whatever it means as no details are disclosed, so fewer memory consumption is very attractive compared to recent memory-hungry large models.\n\nDenoising performance is better than others as shown in Tables 1 and 2, and table 2 shows that the proposed method has a good trade-off between computation time and gpu memory."
                },
                "weaknesses": {
                    "value": "Patch-based offset correction: as long as reading section 3.1, similarities for search are computed to each \"reference locations\" and \"search locations\", depending on strides S_Q and S_K. Given the predicted offset F, which is floating point coordinates, corrected coordinates reside in the integer grid. In experiments strides were set to 2 (probably, as it is now shown), however it is slower as shown in Table 10. There are no experiments on denoising and alignment with stride 2, it is difficult to expect the proposed method to work as better as stride 1.\n\nExperiments: Results show that the proposed method works as expected, but so many details are not explained and hence it is hard to see how the method really works and how it behaves for different hyper-parameters under ablation studies. For example, patch size P, feature extractor for patches, details predicting offset F, K of top-K, \n\nInsights: This paper is on \"space-time attention\" and top-k patches are used for the search over T frames. It should be shown that how these top-k patches are selected, how patches attend to where/when, and how corrections are improved, because such insights would be a great help for understanding the method and prompting potential following works.\n\n\n\nOther comments:\n\nOrganization and writing: Symbols and concepts defined in section 3.1 are not well connected to the following sections and explanations, which makes the logical flow hard to understand.\n\nOffset instead of alignment/denoising: Experimental results demonstrate how the method works in applications, however, directly investigating the learnt offset value F would be helpful for evaluating the proposed method.\n\n\nTerms:\n\n- \"STAN\" appears first in p5, but explained and spelled in p8.\n- N_Q and N_K for Reference locations and Search locations do not match as Q is for query and K for key."
                },
                "questions": {
                    "value": "see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698224126815,
            "cdate": 1698224126815,
            "tmdate": 1699636381420,
            "mdate": 1699636381420,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LRCnZik5Sh",
                "forum": "Mqukp3Lsnt",
                "replyto": "YBO97zxsWn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4156/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "- We hope the updates to Sections 3.1 and 3.2 better explain our method, and the meaning of \u201cin-place computation\u201d. By \u201cin-place\u201d, we simply mean no input video data is copied [[wiki](https://en.wikipedia.org/wiki/In-place_algorithm)]. The indexing described in Section 3.2 is directly used within our code.\n\n- We have updated the terms \u201creference/search\u201d strides to \u201cquery/key\u201d strides, respectively.\n\n- We have updated Figure 7 in Section 4.1 to address your interest regarding how parameters (such as patch size and query stride) impact the search quality. We also gently note in the supplemental Section 6.2, an experiment uses a patch size of 7 and query stride of 4. Additionally, Sections 6.3 and 6.4 include ablations experiments regarding the key stride and space-time window sizes.\n\n- Directly investigating the offsets, $\\mathbf{F}$, is important. This is a motivation for Figure 3 in Section 3.3. However, directly comparing offsets to evaluate the quality of a search module will be misleading. This is because attention modules act as a kernel function on the input video. Thus different offsets may produce similar outputs, depending on the content of the value video ($\\mathbf{V}$).  Written agin, when the two offsets are not equal, $\\mathbf{F}^{\\text{Ideal}}  \\neq \\mathbf{F}^{\\text{Shifted-NLS}}$, the final output might be similar, $\\mathbf{X}^{\\text{out,Ideal}} \\approx \\mathbf{X}^{\\text{out,Shifted-NLS}}$. It may also be true that when the two offsets are similar (but not equal), the final output might be dramatically different. This ambiguity is resolved by comparing offsets through their impact on the attention module's final output. The Video Alignment experiment in Section 4.1 provides an interpretable output target for the attention module. We thank you for pointing out this gap in our explanation and we have updated the text in Section 4.1 to clarify this point."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700079599257,
                "cdate": 1700079599257,
                "tmdate": 1700081034961,
                "mdate": 1700081034961,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1j3fGIEKPb",
            "forum": "Mqukp3Lsnt",
            "replyto": "Mqukp3Lsnt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4156/Reviewer_UHjC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4156/Reviewer_UHjC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Shifted Non-Local Search for frame-wise alignment. Specifically, the query points are searched in the windows which is shifted by the predicted optical flows. The top-k locations are then aggregated with Guided Deformable Attention and 3D convolution."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors demonstrate that optical flow requires only minor spatial corrections for frame-wise alignment.\n\n2. The authors introduce In-Place Computation, which significantly reduces the memory working set and consequently enhances speed.\n\n3. The proposed method achieves state-of-the-art results on video denosing task."
                },
                "weaknesses": {
                    "value": "1. The way authors show that optical \ufb02ow only needs small spatial corrections is from the results of Sintel-Clean benchmark, however, this setting is far from the real-world dataset, where blur and degradation could happens. Moreover, these results are from methods with high computational cost, which is not feasible for the online setting.\n\n2. the idea is already explored in video enhancement task, such as BasicVSR++ [1] RVRT [2], where the deformable convolutions/attentions' offsets are computed on top of the SpyNet predicted optical flow. Moreover,  IART [3] propose an cross-attention scheme by searching around the OF-shifted window.\n\n3. This paper is the mixture of \n\n[1] BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment (CVPR2022)\n\n[2] Recurrent Video Restoration Transformer with Guided Deformable Attention (NeurlPS2022)\n\n[3] An Implicit Alignment for Video Super-Resolution (arXiv:2305.00163)"
                },
                "questions": {
                    "value": "1. the reproduced results for RVRT 39.29 seems deviate a lot from the original paper 40.57 for DAVIS sigma=10, why this is the case? Moreover, I think RVRT already implement offsets on top of predicted optical flow, which I think is the same method with the proposed one.\n\n2. Is there direct comparison on STAN alignment with guided deformable attention? (i.e. comparing results by only changing the alignment module, keep the original backbone unchanged for RVRT.)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4156/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4156/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4156/Reviewer_UHjC"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674045227,
            "cdate": 1698674045227,
            "tmdate": 1699636381327,
            "mdate": 1699636381327,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ik1hQeOxeh",
                "forum": "Mqukp3Lsnt",
                "replyto": "1j3fGIEKPb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4156/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Not a Mixture.** Our paper is not a mixture of the three listed papers. \n- First, BasicVSR++ [1] is not considered in this paper as our method is inspired from non-local denoising methods, such as Video Non-Local Bayes [4]. \n- Second, RVRT [2] uses Predicted Offsets while our Shifted Non-Local Search outperforms this method by 3 - 7 dB on video alignment in Figures 6 and 7 (Sec 4.1). When replacing RVRT\u2019s Predicted Offsets with our Shifted Non-Local Search, RVRT\u2019s denoising quality improves by 0.30 dB in Table 1 (Sec 4.2).\n- Third, IART [3] is an unpublished paper that provides less functionality than the Shifted Non-Local Search and requires 81 times the memory ([relevant code link](https://github.com/kai422/IART/blob/05ee7d57fcc2c7d028e8389c330b50b5c797aa41/archs/implicit_alignment.py#L144)) when searching a window of size 9x9.\n\n[4] Pablo Arias and Jean-Michel Morel. Video denoising via empirical bayesian estimation of space-time patches. Journal of Mathematical Imaging and Vision, 2018.\n\n**IART.** IART [3] is an unpublished paper that is (1) computationally more expensive and (2) provides less functionality than this paper\u2019s Shifted Non-Local Search. Firstly, IART requires a significant spike in memory consumption. When searching a spatial window of size $(W_s,W_s)$, IART must copy the input video $W_s^2$ times ([relevant code link](https://github.com/kai422/IART/blob/05ee7d57fcc2c7d028e8389c330b50b5c797aa41/archs/implicit_alignment.py#L144)). For a 9x9 search (as in Section 4.1), IART requires the input video to be copied 81 times, while the in-place Shifted Non-Local requires no copying. Second, IART\u2019s use of Pytorch\u2019s grid_sample function restricts the method to use image patches of size 1, while our method uses image patches of size 3 and 7 in Sections 4.1 and 6.2, respectively. In the first and third rows of Figure 7, there is an approximate 3 - 4 dB difference between a patch size of 1 and 3.\n\n**RVRT uses Predicted Offsets.** We agree that RVRT does indeed use an auxiliary network to predict offsets (named \u201cPredicted Offset\u201d in the paper) with optical flow and video features as inputs. In Table 1 from Section 4.2, we replace this small network with our Shifted Non-Local Search and observe the video denoising quality improve. We have updated the text in Section 4.2 to clarify this point.\n\n**Small Optical Flow Errors on the Sintel-Clean Benchmark.** We have updated our text in Section 3.3 to clarify our point regarding the Sintel-Clean dataset. We agree with you: the results from the Sintel-Clean benchmark will be overly optimistic. Our discussion in Section 3.3 is to illustrate that even recent, large-scale deep networks still report an end-point-error of about 1 even on this \u201cfar from real-world\u201d dataset. Yet, using OpenCV\u2019s implementation of Farnback\u2019s 2003 optical flow method (a computationally \u201ccheap\u201d method) combined with a large grid search of 41x41 pixels, the most similar pixels (i.e. the pixels used for attention) exist in a small radius surrounding the offset values. We further acknowledge this radius may grow as noise increases, which is a motivation for the Video Alignment experiment in Section 4.1.\n\n**Q1. Reproducing RVRT.** One reason our reproduced RVRT network yields lower quantitative results may be due to our computational setup. We use a stand-by queue on SLURM, which requires training in 4-hour increments. While existing open-source packages claim to fully support this functionality, in reality, this may be unreliable [examples of previous issues: [1](https://github.com/Lightning-AI/lightning/issues/18588), [2](https://github.com/Lightning-AI/lightning/issues/12812)]. Still, the STAN network is trained in the same fashion and yields significantly better results. Training time may also be a factor, which we discuss in more detail in Supplement Section 6.4. \n\n**Q2. Comparing STAN and RVRT.** Since STAN processes multiple frames in parallel and RVRT does not, we are unable to directly compare the two networks. The updated text at the start of Section 4 hopefully clarifies this point. Notably, in Section 4.2 we do directly compare our proposed grid search against the auxiliary network used within the Guided Deformable Attention module (within RVRT). Table 1 shows the direct comparison between offsets computed from an auxiliary network (\u201cPredicted Offsets\u201d ) and our grid search (\u201cShifted Non-Local Search\u201d). We also include ablation experiments for both RVRT and STAN in Sections 6.3 and 6.4, respectively."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700078487917,
                "cdate": 1700078487917,
                "tmdate": 1700326742947,
                "mdate": 1700326742947,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yBC4WcXUI9",
                "forum": "Mqukp3Lsnt",
                "replyto": "1j3fGIEKPb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4156/Reviewer_UHjC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4156/Reviewer_UHjC"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the responses and would like to keep my original evaluation."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708568646,
                "cdate": 1700708568646,
                "tmdate": 1700708568646,
                "mdate": 1700708568646,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3SQLUMK4ma",
            "forum": "Mqukp3Lsnt",
            "replyto": "Mqukp3Lsnt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4156/Reviewer_U4AC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4156/Reviewer_U4AC"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses non-local search (ie, dense point tracking) from temporal sequences. The authors propose a two stages approach, where at first a local displacement is estimated, the followed by a local search. This framework improves upon previous work which either estimate a point-wise large scale shift, or compute a a local displacement/correlation. The framework is validated in the context of space-time attention, for application such as denoising on Davis dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) Non global image matching/search is a hard problem. Applications related to video analysis (object tracking, denoising) are important. \n\n2) The proposed approach (first predicting an off-set, then refinining the estimation in a local search window) is intuitive. The merits of the approach are shown experimentally (frame alignment, space-time attention for video denoising)."
                },
                "weaknesses": {
                    "value": "1) The technical explanations of the implementation of the approach are difficult to follow. Section 3.1 and 3.2 could certainly be clarified and simplified. For example,  specify the meaning of the indices, use different letters for different variables (what is the difference between I and \\tilde_I?; if K_v is the variable for the Keys then do not use K again to denote the number of neighbor, etc)\n\n2) The results section is not clear to me. I suggest the authors to start the experiments section by summarizing how they will attempt to demonstrate what are the advantages of their approach through several specific applications using different datasets and different evaluation metrics."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698850294735,
            "cdate": 1698850294735,
            "tmdate": 1699636381232,
            "mdate": 1699636381232,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hCO2tL14ji",
                "forum": "Mqukp3Lsnt",
                "replyto": "3SQLUMK4ma",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4156/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the comments on our presentation. We have updated Sections 3.1 and 3.2 to improve the readability of the method. In Sections 4.1 and 4.2, we have added more details to explain the intention of the experiments.\n\n- [Sections 3.1 and 3.2]: We have changed our presentation to guide the reader through increasingly sophisticated search methods within attention modules. I.e. Global Attention -> Neighborhood Attention -> Non-Local Search -> Shifted Non-Local Search.\n\n- [Section 4]: We have added a paragraph summarizing our experiment section.\n\n- [Section 4.1]: We add motivation for using the Video Alignment task as our evaluation of the different search methods.\n\n- [Section 4.2]: We add more detail to explain how RVRT\u2019s auxiliary network (\u201cPredicted Offsets\u201d) are replaced with our grid search (\u201cShifted-NLS\u201d)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700077784454,
                "cdate": 1700077784454,
                "tmdate": 1700078584051,
                "mdate": 1700078584051,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mIhzz2yLtp",
                "forum": "Mqukp3Lsnt",
                "replyto": "hCO2tL14ji",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4156/Reviewer_U4AC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4156/Reviewer_U4AC"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for the revised version of the manuscript (which as significantly improved). The paper presents a smart implementation of existing concepts combined together (cf comments of reviewer UHjC). The novelty of the current work still needs to be better exposed."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730210342,
                "cdate": 1700730210342,
                "tmdate": 1700730210342,
                "mdate": 1700730210342,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]