[
    {
        "title": "Hypothesis Search: Inductive Reasoning with Language Models"
    },
    {
        "review": {
            "id": "Nsv06MhRwM",
            "forum": "G7UtIGQmjm",
            "replyto": "G7UtIGQmjm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3169/Reviewer_e7wF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3169/Reviewer_e7wF"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose to use large language models (LLM) to generate hypothesis for abstraction and reasoning corpus (ARC). \nGiven a task in ARC, the LLM first propose a set of hypothesis, then either a language model or a human in the loop can select a subset hypothesis for generating a program that satisfy the hypothesis as the specification. \nThe automated pipeline which uses the LLM to perform the selections has 27.5% accuracy, and with human in the loop has 37.5%."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: 5/5 \nThe idea of using the LLM to generate hypotheses and then synthesizing the downstream Python program is novel and interesting. The experimental result gives positive feedback that the natural language is capable of representing human intuition in this low data in-context learning environment. \n\nQuality: 3/5\nThe experimental result shows promising improvement in the methodology. However, it seems still quite expensive and not reliable enough to generate 64 different hypotheses for the language model by setting the temperature to 1.0. It would be nice to have a chart on the GPT-4 query number against the rate where it hit the correct hypothesis. \n\nClarity: 3/5\nThere are quite a lot of details that are necessary to help understand the work in the supplementary material, for example, the GPT-4 prompts. \n\nSignificance: 4/5\nThis work is important to the program synthesizing community in how to synthesize a natural and intuitive program, instead of synthesizing a functionally correct but not necessarily generalizable program."
                },
                "weaknesses": {
                    "value": "See strength."
                },
                "questions": {
                    "value": "It would be nice if there a statistical analysis on the failure case analysis."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3169/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820048631,
            "cdate": 1698820048631,
            "tmdate": 1699636264657,
            "mdate": 1699636264657,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H9nA5QSdDO",
                "forum": "G7UtIGQmjm",
                "replyto": "Nsv06MhRwM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3169/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer e7wF"
                    },
                    "comment": {
                        "value": "Thank you for your encouraging comments and insightful questions!\n\n> It would be nice to have a chart on the GPT-4 query number against the rate where it hit the correct hypothesis.\n\nGreat idea; we\u2019ve added this to Appendix B.1. The number of tasks with correct hypotheses increases consistently as we sample more hypotheses from GPT-4.\n\n> It would be nice if there a statistical analysis on the failure case analysis.\n\nThanks for the excellent suggestion! For the human-selected evaluation, we\u2019ve now added a simple failure case analysis - namely, we were curious about the following: for what subset of the questions did the model fail because it failed to generate a correct hypothesis, and for what subset did it fail because it failed to implement a correct hypothesis? We looked at the 100 sampled tasks and the human-selected correct generated hypotheses (though, we reiterate here that \u201ccorrect\u201d is both subjective and somewhat arbitrary without experiments in line with those conducted in LARC). For 49% of them, at least one correct hypothesis was identified as correct. Of those selected, 33 (67%) led to correct programs. However, when given all correct language (i.e., the Human-Written Hypothesis evaluation), the model successfully implements 45% correctly. Of these, only one task is solved by the human-selected pipeline that isn\u2019t solved by the human-written pipeline. This may suggest that there is a relationship between the tasks that the model can propose a hypothesis for and the tasks for which it can implement that hypothesis.\n\n\n> However, it seems still quite expensive and not reliable enough to generate 64 different hypotheses for the language model by setting the temperature to 1.0.\n\nWe strongly agree with this in principle, and have now highlighted this in our limitations section! However, we note that over time the best large language models do get better, cheaper, and faster, which will make our method more effective. Less than a month ago, a version of GPT-4 was announced that appears to be broadly improved, faster, is over two times less expensive, and (ostensibly) supports a 128k context window."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546495828,
                "cdate": 1700546495828,
                "tmdate": 1700546495828,
                "mdate": 1700546495828,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HqOP4S5bJ1",
            "forum": "G7UtIGQmjm",
            "replyto": "G7UtIGQmjm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3169/Reviewer_7fhN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3169/Reviewer_7fhN"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new program synthesis framework for solving inductive reasoning problems based on large language models and prompting techniques. The idea is to first generate hypotheses based on the training samples, and then select a few hypotheses to realize their implementations. The implementations are verified on the training samples and the best implementation is selected to perform inference on the test samples. Experiments on ARC and 1D-ARC verify the effectiveness of the proposed method, while the proposed method doesn't outperform direct prompting on SyGuS."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper shows that large language models can generate natural language hypotheses based on the training samples. The generated hypothesis can improve the performance of program synthesis on inductive reasoning benchmarks.\nThe paper conducts experiments on ARC, which is a challenging benchmark for inductive reasoning."
                },
                "weaknesses": {
                    "value": "The overall prompting framework in this paper is very similar to self-debug[1] , except that self-debug focuses on iterative refinement, while this paper emphasizes hypothesis search. If this is the point, the authors should provide a deeper analysis of the generated hypotheses. Algorithm 1 has a similar high-level idea of Figure 3 from the self-debug paper. So this paper is more like revisiting self-debug from a different perspective, which limits its novelty and contribution. This paper also misses an important citation[2].\nExperiments results are not sufficient to justify the significance of the method. Of the 3 datasets used in the paper, the proposed method only works on ARC and 1D-ARC, which are very similar. Besides, it only uses 40 samples for inference and the variance of the performance is not reported. It is likely the observation in this paper may be overestimated due to variance in performance and model selection.\nThe contribution of this paper is not very clear. From the intro, it looks like the authors try to solve the inductive reasoning problem. From the experiments, there is no comparison with non-LLM baselines, and it looks more like an ablation study of using natural language hypotheses in program synthesis.\n\n[1] Chen, et al. Teaching large language models to self-debug. arXiv 2023.\n[2] Austin and Odena, et al. Program synthesis with large language models. arXiv 2021."
                },
                "questions": {
                    "value": "Questions:\nIs there any deeper connection between Hypothesis Search and the Bayesian learner mentioned in the introduction?\nSec. 2.4. \u201ca lower bound\u201d -> It is not very clear to me why it is a lower bound before I read the experiment section. May rewrite the last sentence.\nSec. 3.1. \u201cIt contains\u201d -> incomplete sentence.\nSec. 3.2.1. perf -> per\nSec. 3.2.1. Human-Selected Hypotheses. Why do you use 3 rounds of execution feedback here? The other experiments are based on 2 rounds.\nSec. 3.2.3. How about the ability of GPT3.5 in generating hypotheses? Why is there no table for this section?\nSec. 3.4. Why is there no table for this section? Also the last sentence is an overclaim. It\u2019s the improvement of GPT-4 over CrossBeam, not the proposed prompting technique."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3169/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698957369112,
            "cdate": 1698957369112,
            "tmdate": 1699636264565,
            "mdate": 1699636264565,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lCia99mFdF",
                "forum": "G7UtIGQmjm",
                "replyto": "HqOP4S5bJ1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3169/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 7fhN"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments. We appreciate the chance to clear up any points of confusion, but please do not hesitate to let us know if any further clarifications are needed.\n\n> while the proposed method doesn't outperform direct prompting on SyGuS.\n\nWe want to clarify that there is no direct prompting baseline in the SyGuS experiment. because there are no testing examples in SyGuS. Our method performs slightly worse than our program-only hypothesis. We note that on a new evaluation using gpt-4-0613, our full pipeline obtains the same performance with the program only hypothesis.\n\n> The overall prompting framework in this paper is very similar to self-debug[1] , except that self-debug focuses on iterative refinement, while this paper emphasizes hypothesis search.\n\nFirst, we apologize for the misunderstanding. Self-debug does not explore inductive reasoning (that is, taking a collection of examples and identifying their underlying rule). Not only does it not \u201cemphasize hypothesis search\u201d - indeed, hypothesis search is not part of their method, and is our primary contribution.\n\nIn particular, we are (to our knowledge) the first to demonstrate that language models can effectively generate a collection of hypotheses to solve an inductive reasoning problem, and then select the right hypothesis by evaluating its ability to explain the inductive reasoning problem\u2019s examples. We have added a discussion to the Introduction section to help clarify this.\n\nWhile our paper does use the language model to revise its solutions in code, we also strongly agree that this idea is not one of our contributions. We have updated the paper to more clearly emphasize that our primary contribution is programmatic inductive reasoning pipeline via hypothesis generation, and have added additional citations to prior work on automated code repair [1,2,3,4] to help contextualize this work.\n\n[1] \"A Bidirectional LSTM Language Model for Code Evaluation and Repair\" Rahman et al 2021  \n[2] \"Neural Program Repair by Jointly Learning to Localize and Repair\" Vasic et al 2019  \n[3] \"Deep Learning for Code Repair\" Pang 2018  \n[4] \"Automated program repair through the evolution of assembly code\" Schulte 2010  \n\n> From the intro, it looks like the authors try to solve the inductive reasoning problem. From the experiments, there is no comparison with non -LLM baselines, and it looks more like an ablation study of using natural language hypotheses in program synthesis.\n\nThank you for highlighting this aspect of our work. First we want to note that on SyGuS, we have reported the accuracy of a non-LLM baseline CrossBeam [5], which is outperformed by our methods. In our paper, we specifically concentrate on exploring inductive reasoning through the lens of language models, as indicated in our title. This focus is intentionally chosen to better understand the capabilities of LLMs in this context. Our main focus was on understanding the impact of generating various kinds of intermediate hypotheses when solving inductive reasoning tasks with language models, a type of reasoning that they have been known to struggle with. For some context, we\u2019ve also now highlighted the special-purpose DSL-based state-of-the-art on ARC (according to [6]) in our text, which performs marginally worse than our results with human-written hypotheses, solving 43 of our 100 sampled tasks. \n\n>  Experiments results are not sufficient to justify the significance of the method. Of the 3 datasets used in the paper, the proposed method only works on ARC and 1D-ARC, which are very similar.\n\nCould you please clarify this? Our results indicate a significant improvement over prior work on all of the datasets. Note that both the program-only and the full hypothesis search methods show significant improvement over prior work on SyGuS. We have now added the cognitive-science-inspired inductive reasoning dataset, List Functions, from \u201cThe Child as Hacker\u201d [7] which is fairly different from any of the current three datasets. As on the ARC datasets, this shows a clear advantage with the natural language hypotheses. \n\n| Method           | 64 hypotheses   |\n|------------------|-----------------|\n| Direct Prompting | 31%             |\n| Program Only     | 59%             |\n| Full             | 69%             |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545578797,
                "cdate": 1700545578797,
                "tmdate": 1700547386357,
                "mdate": 1700547386357,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vzDmL3voTn",
                "forum": "G7UtIGQmjm",
                "replyto": "HqOP4S5bJ1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3169/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 7fhN (Cont')"
                    },
                    "comment": {
                        "value": "Other points:\n> This paper also misses an important citation.\n\n Thank you, we\u2019ve added [8] to the related works. Note that this work focuses on solving program synthesis given natural language descriptions, while our work focuses on inductive reasoning tasks where we only observe a few input-output examples as the specification for programs. \n\n> Besides, it only uses 40 samples for inference and the variance of the performance is not reported. \n\nThanks for this great point. Originally, scaling this up was prohibitively expensive. However, we have now been able to access additional resources, allowing us to extend these results to 100 tasks for ARC, 1D-ARC and the new List Functions datasets. All the conclusions remain consistent with the previous results, which demonstrate the effectiveness of our proposed pipeline.\n\n> The contribution of this paper is not very clear.\n\nThank you, we\u2019ve added a paragraph to the introduction to help clarify.\n\n> Is there any deeper connection between Hypothesis Search and the Bayesian learner mentioned in the introduction? \n\nUnder the assumption of a low false-positive rate, which we empirically observed, our method is approximately the same as importance sampling in a hierarchical Bayesian model, where the importance distribution is that of the language model conditioned on the examples. This could potentially guide future work toward more efficient algorithms -- we\u2019ve added a note about this to future work.\n\n\n\n\n> Why do you use 3 rounds of execution feedback here? \n\nWe have updated experiments to uniformly use 3 rounds of feedback.\n\n> How about the ability of GPT3.5 in generating hypotheses? \n\nFor ARC we noted that we observed GPT-3.5 to mostly generate meaningless hypotheses. However, motivated by this question, we ran additional experiments to understand the performance of GPT3.5-generated hypotheses on 1D-ARC, SyGuS and List Functions when using GPT3.5-generated hypotheses. Notably, on all of the datasets we evaluated besides ARC, natural language hypothesis search improved performance over both a direct prompting baseline, as well as a program-only hypothesis search. Surprisingly, in the case of 1D-ARC, the program-only GPT-3.5 results were actually slightly worse than its direct prompting results, while the full hypothesis search pipeline was better than both. We have added the following table to Appendix B.\n\n|                    | 1D-ARC | SyGuS | List Func |\n|--------------------|--------|-------|-----------|\n| Direct             | 25.9   | N/A   | 16        |\n| Program Only (Ours)| 23.1   | 80.9  | 46        |\n| Full (Ours)        | 26.9   | 86.5  | 57        |\n\n\n\n>Sec. 3.4. Why is there no table for this section? \n\nWe have updated the paper to summarize the results in a table.\n\n> Also the last sentence is an overclaim. It\u2019s the improvement of GPT-4 over CrossBeam, not the proposed prompting technique.\n\nWe agree that the results are not directly comparable. We have revised the text accordingly. We provide the number for CrossBeam as for a reference of the state-of-the-art DSL-based method.\n\n[5] \"Crossbeam: Learning to search in bottom-up program synthesis,\" Shi et al. 2022  \n[6] \"Large Language Models as General Pattern Machines,\" Mirchandani, et al. 2022  \n[7] \"The Child as Hacker,\" Rule et al. 2020  \n[8] \"Program synthesis with large language models.\" Austin, et al. 2021"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545695006,
                "cdate": 1700545695006,
                "tmdate": 1700547186096,
                "mdate": 1700547186096,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GEZ8MfiuHT",
            "forum": "G7UtIGQmjm",
            "replyto": "G7UtIGQmjm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3169/Reviewer_Bu26"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3169/Reviewer_Bu26"
            ],
            "content": {
                "summary": {
                    "value": "This paper prompts LLMs to generate Python programs to solve symbolic pattern recognition problems. This may be better than letting the model directly predict answers. On Abstraction and Reasoning Corpus (ARC) where the inputs are 2D or 1D pixel grids, letting GPT-4 generate natural language hypotheses to then guide program generation improves the result. Hypothesis generation slightly harms the performance on SyGuS where the inputs are strings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper proposed to let GPT-4 generate programs to solve symbolic pattern recognition tasks. It shows that using natural language hypotheses to guide the program generation can be helpful on ARC, on which letting the model directly generate programs results in bad programs.\n1. The paper reports the limitation that on SyGuS where the model can directly generate programs, hypothesis guidance is not helpful.\n1. The presentation is clear and several findings are interesting."
                },
                "weaknesses": {
                    "value": "1. The technical novelty is limited and the main challenge of generating high-quality programs is largely unsolved.\n2. The effectiveness of the proposed method of using hypotheses to guide program generation has unclear applicability. (1) GPT-3.5 fails to generate meaningful hypotheses. (2) GPT-4 hypotheses are not helpful on SyGuS where GPT-4 can directly generate good programs. (3) GPT-4 hypotheses are helpful on ARC, but ARC results are still only 37.5 with the hypotheses. Practitioners will have to develop alternative models that can better understand 2D geometry to solve the task and then natural language hypotheses may no longer be helpful as in SyGuS. (4) Model-generated hypotheses hurt the performance of Parsel, a compositional program generation method that can significantly  improve the performance when model-generated hypotheses are not used.\n3. Multiple questions need to be clarified; some requires experimental results. Please refer to Questions.\n4. Typo: Sec 3.1 \"It contains Although simpler...\""
                },
                "questions": {
                    "value": "1. Sec 3.2.2 says summarized hypotheses can often become vague and ambiguous. Will the hypotheses used to guide program generation be of higher quality if you let the model rank the hypotheses? You could analyze the recall@k, i.e., whether top k hypotheses contain a correct one.\n1. ARC: In Table 2, using human written hypotheses only has 37.5 accuracy. Does that mean LLM fails to write programs based on correct hypotheses? The statement at the end of page 5 that \"GPT-4 is pretty good at both generating hypotheses and realizing them as programs\" requires some more evidence or explanation.\n1. ARC: In Table 2, the accuracy with human-selected and human-written hypotheses are both 37.5. Does this mean model-generated hypotheses for each task almost always contain a correct one? Or is it the case that model-generated hypotheses sometimes have mistakes but, when correct, leads to better programs, and thus both 37.5? Can you evaluate the recall of model-generated hypotheses, either by some automatic metric or human evaluation?\n1. For ARC, why do you only consider top-1 accuracy but not top-3 as in the official evaluation? Can you compare your method with state-of-the-art methods on the task?\n1. What are the types of tasks that (1) program generation and (2) hypotheses search can be helpful? Can you summarize the features of such tasks? \"Inductive reasoning tasks\" is too general and abstract. To begin with, is it true that the method is applicable only to symbolic pattern recognition tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3169/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699164755402,
            "cdate": 1699164755402,
            "tmdate": 1699636264502,
            "mdate": 1699636264502,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HXg95MiUQC",
                "forum": "G7UtIGQmjm",
                "replyto": "GEZ8MfiuHT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3169/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Bu26"
                    },
                    "comment": {
                        "value": "Thank you for the thorough and very helpful questions!\n\n> Hypothesis generation slightly harms the performance on SyGuS where the inputs are strings\u2026 \n\nThank you for this point! We\u2019d like to clarify that we don\u2019t consider the SyGuS results as negative: indeed, our pipelines (both Program Only and Full) both perform significantly better than prior work, and we\u2019ve added a table to Sec. 3.4 to help highlight this. However, we do find that the natural language hypotheses are not a useful additional abstraction in this dataset. This is likely because the SyGuS tasks are relatively simple so the Program Only ablation already achieves saturated performance. \n\nHowever, we also introduce a new dataset to demonstrate the usefulness of natural language hypotheses on non-ARC tasks. Specifically, we now investigate the cognitive-science-inspired inductive reasoning dataset, List Functions, from \u201cThe Child as Hacker\u201d [1] which is fairly different from any of the current three datasets. As on the ARC datasets, this shows a clear advantage with the natural language hypotheses. \n\n| Method           | 64 hypotheses   |\n|------------------|-----------------|\n| Direct Prompting | 31%             |\n| Program Only     | 59%             |\n| Full             | 69%             |\n\nDue to a change in our available resources, we also updated our results with a slightly newer version of gpt-4, gpt-4-0613. With the updated model, both pipelines correctly solve 94.3% of the examples.\n\n[1] \u201cThe Child as a Hacker,\u201d Rule et al. 2020  \n\n> The main challenge of generating high-quality programs is largely unsolved.\n\nWe agree that there is still substantial work to be done before these difficult inductive learning tasks can be considered complete. On the other hand, our methods achieve large improvement over four datasets, which we believe is substantial. Our main contribution is showing how to approach inductive learning using program generation as a component, instead of tacking the program generation problem. Future improvements on generating high-quality programs can be expected to benefit our approach to induction reasoning tasks.\n\n>The technical novelty is limited\n\nNovelty is, of course, a subjective quality. Some technical novelty derives from the extensive mathematical reasoning required. Other novelty derives from simple ideas which haven\u2019t been previously noticed and yet have powerful implications. We believe our paper is an example of the latter. The ability to do inductive learning from a few examples has been a cornerstone ability of LLMs in recent years, and their failure in more complex inductive learning tasks (e.g. ARC) has been taken by a substantial subset of computer scientists as a condemnation of the enterprise. Thus finding that a structured approach to induction yields very large improvements, and that this depends on the right staging between natural and formal languages, is an important finding. To those who expected LLMs to make no progress on hard problems of induction it is a surprising, we\u2019d say novel, one."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546159211,
                "cdate": 1700546159211,
                "tmdate": 1700546159211,
                "mdate": 1700546159211,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hgsUKXwF0r",
                "forum": "G7UtIGQmjm",
                "replyto": "GEZ8MfiuHT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3169/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Bu26 (Cont')"
                    },
                    "comment": {
                        "value": "> ARC: In Table 2, using human written hypotheses only has 37.5 accuracy. Does that mean LLM fails to write programs based on correct hypotheses? The statement at the end of page 5 that \"GPT-4 is pretty good at both generating hypotheses and realizing them as programs\" requires some more evidence or explanation.\n\nFirst, we agree: we have removed this sentence. For some context, this was intended as a relative statement, given the poor performance of the LLM-based alternatives. We have also added a more explicit we\u2019ve now added a failure case analysis - namely, we were curious about the following: for what subset of the questions did the model fail because it failed to generate a correct hypothesis, and for what subset did it fail because it failed to implement a correct hypothesis? We looked at the 100 sampled tasks and the human-selected correct generated hypotheses (though, we reiterate here that \u201ccorrect\u201d is both subjective and somewhat arbitrary without experiments in line with those conducted in LARC). For 49% of them, at least one correct hypothesis was identified as correct. Of those selected, 33 (67%) led to correct programs. However, when given all correct language (i.e., the Human-Written Hypothesis evaluation), the model successfully implements 45% correctly. Of these, only one task is solved by the human-selected pipeline that isn\u2019t solved by the human-written pipeline. This may suggest that there is a relationship between the tasks that the model can propose a hypothesis for and the tasks for which it can implement that hypothesis.\n\n\n>  Can you evaluate the recall of model-generated hypotheses, either by some automatic metric or human evaluation?\n\nWe have included additional experiments in Appendix B.1 where we evaluate the recall of model-generated hypotheses on multiple datasets. On ARC, we measure the percentage of tasks with correct hypotheses (judged by human) when we increase the number of hypotheses. On 1D-ARC and List Funcctions, we measure the accuracy of our pipeline as we increase the number of hypotheses. Both experiments show that the recall steadily increases as the number of hypotheses increases.\n\n> For ARC, why do you only consider top-1 accuracy but not top-3 as in the official evaluation? Can you compare your method with state-of-the-art methods on the task?\n\nAlthough top-3 is indeed technically the \u201cofficial\u201d evaluation metric, top-1 has been the often-unstated convention in LLM-based papers (e.g. [3,4,5]). In general, we would argue that pass@1 makes more sense since we care about evaluating generalization. For some context, we\u2019ve also now highlighted the special-purpose DSL-based state-of-the-art (according to [4]) in our text, which performs marginally worse than our results with human-written hypotheses, solving 43 of our 100 sampled tasks.\n\n> What are the types of tasks that (1) program generation and (2) hypotheses search can be helpful? Can you summarize the features of such tasks? \"Inductive reasoning tasks\" is too general and abstract. To begin with, is it true that the method is applicable only to symbolic pattern recognition tasks?\n\nIn principle, we would argue this framework should be applicable for any inductive reasoning task that can be represented computationally \u2014 in practice, there are a few types of inductive reasoning tasks that are not currently supported in our framework. \n\nFor example, many more complex visual inductive reasoning tasks will require extending this implementation, such as using a compositional visual programming language to represent the program (e.g., VISPROG [6]). Moreover, if a task is truly unprogrammable, then we lose a core value of this work, namely the inductive bias and generalization that comes with a programmatic representation. Some challenges may also arise for models with a higher false-positive rate which we\u2019ve now noted in the limitations. \n\nIn terms of tasks that cannot be solved by Python programs, in discussing future work, we\u2019ve added discussion around more open-ended tasks like summarization where we might want to optimize a \u201csoft\u201d score (e.g., ROUGE [7]). In short, these problems are still approachable with Python but may require the language model to leverage another machine learning model.\n\n\n> Typo: Sec 3.1 \"It contains Although simpler...\"\nThank you, this is now fixed! \n\n\n[2] \u201cCoder reviewer reranking for code generation,\u201d Zhang et al. 2023\n[3] \u201cLLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations,\u201d Xu et al. 2023   \n[4] \u201cLarge Language Models as General Pattern Machines,\u201d Mirchandani et al. 2023   \n[5] \u201cLarge Language Models Are Not Strong Abstract Reasoners,\u201d Gendron et al. 2023  \n[6] \u201cVisual Programming: Compositional visual reasoning without training,\u201d Gupta and Kembhavi 2022  \n[7] \u201cROUGE: A package for automatic evaluation of summaries,\u201d Lin 2004"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547013048,
                "cdate": 1700547013048,
                "tmdate": 1700547026015,
                "mdate": 1700547026015,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rNfYkBo65M",
            "forum": "G7UtIGQmjm",
            "replyto": "G7UtIGQmjm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3169/Reviewer_PFzt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3169/Reviewer_PFzt"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents the hypothesis search approach for inductive reasoning. Specifically, hypothesis search first generates multiple hypotheses on the shared transformation rule for the given input-output pairs. Afterward, a subset of hypotheses is selected by humans, or summarized by the LLM. Finally, the LLM generates the Python program given a hypothesis, and the program is executed on the input-output pairs to verify the correctness. They evaluate their approach on ARC, 1D-ARC and SyGuS. Using GPT-4, their approach outperforms the baselines that directly generate the answer or the Python program without hypothesis generation. In particular, they demonstrate that using hypotheses generated by GPT-4 achieves the same performance as using human-written hypotheses."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Inductive reasoning is an important and challenging problem. This work achieves a notable improvement on ARC and 1D-ARC, showing that combining both abstract hypothesis and concrete code is beneficial.\n\n2. The approach of hypothesis summarization is interesting. Also, it is an interesting finding that using GPT4-generated hypotheses achieves the same performance as using human-written hypotheses, demonstrating the promise of LLMs for generating high-quality hypotheses for inductive reasoning."
                },
                "weaknesses": {
                    "value": "While the overall results are promising, a lot of important ablations and details are missing in the draft.\n\n1. What is the performance with different number of hypotheses? Specifically, in Table 1, it is important to know the performance with fewer number of initial generated hypotheses, such as 8. Comparing hypothesis summarization with directly generating 8 initial hypotheses can validate the importance of the hypothesis summarization stage.\n\n2. In Table 1, the comparison of sample size and token size among different methods is unclear. Specifically, for hypothesis summarization, it is better to uniformly require the model to generate 8 programs for each of the 8 hypotheses for all problems, instead of only applying to 21 tasks, so that the sampling size is more comparable to the program prompting. Similarly, for human-selected hypotheses, it is unclear how many hypotheses are kept after filtering. It is better to always keep 8 hypotheses after filtering. In addition, it is unclear why the number of execution rounds varies for different methods. It is better to unify the setup for a fair comparison.\n\n3. From Table 2, it is interesting to see that the final performance of GPT-3.5 is comparable to GPT-4. Have you tried gpt-3.5-turbo-16k, which has a longer context length? The performance may further improve.\n\n4. The findings on SyGuS are divergent from the main evaluation, as the best result is achieved with purely code generation.\n\n5. Please provide a quantitative analysis on the failure mode; i.e., the percentage of error cases where none of the hypothesis is correct, and the percentage of error cases caused by the wrong generated programs.\n\n6. Please provide the full prompt including the few-shot demonstrations. The appendix only contains the zero-shot prompt. What is the performance of zero-shot prompting? How much does adding 1 or 2 problems in the prompt affects the performance?\n\n7. The evaluation sets of ARC and 1D-ARC are too small. It is better to include at least 100 tasks."
                },
                "questions": {
                    "value": "1. What is the performance with different number of hypotheses?\n\n2. Make the comparison of sample size and token size among different methods clearer. Specifically, for hypothesis summarization, it is better to uniformly require the model to generate 8 programs for each of the 8 hypotheses for all problems, instead of only applying to 21 tasks, so that the sampling size is more comparable to the program prompting. Similarly, for human-selected hypotheses, it is unclear how many hypotheses are kept after filtering. It is better to always keep 8 hypotheses after filtering. In addition, it is unclear why the number of execution rounds varies for different methods. It is better to unify the setup for a fair comparison.\n\n3. For Table 2, have you tried gpt-3.5-turbo-16k, which has a longer context length? The performance may further improve.\n\n4. Please provide a quantitative analysis on the failure mode; i.e., the percentage of error cases where none of the hypothesis is correct, and the percentage of error cases caused by the wrong generated programs.\n\n5. Please provide the full prompt including the few-shot demonstrations. The appendix only contains the zero-shot prompt. What is the performance of zero-shot prompting? How much does adding 1 or 2 problems in the prompt affects the performance?\n\n6. The evaluation sets of ARC and 1D-ARC are too small. It is better to include at least 100 tasks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3169/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699179327930,
            "cdate": 1699179327930,
            "tmdate": 1699636264434,
            "mdate": 1699636264434,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DNwLHKoSli",
                "forum": "G7UtIGQmjm",
                "replyto": "rNfYkBo65M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3169/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer PFzt"
                    },
                    "comment": {
                        "value": "We sincerely thank you for this very in-depth and constructive review!\n\n> What is the performance with different number of hypotheses? \n\nThanks for this excellent question! We\u2019ve performed additional experiments in Appendix B.1 to investigate the performance as the number of considered hypotheses changes and found that the accuracy of models increases consistently with more hypotheses from GPT-4.\n\n> In Table 1, the comparison of sample size and token size among different methods is unclear. Specifically, for hypothesis summarization, it is better to uniformly require the model to generate 8 programs for each of the 8 hypotheses for all problems, instead of only applying to 21 tasks, so that the sampling size is more comparable to the program prompting. \n\nFor hypothesis summarization, we agree that it is better and more rigorous to evaluate on all the tasks instead of a subset. Originally, a subset was used due to cost constraints, but we have now been able to access additional resources. We are currently running this experiment on the full 100 tasks and expect to have results before the end of the discussion period \u2013 please note, however, that we do not expect the final results on hypothesis summarization to meaningfully change.\n\n> Similarly, for human-selected hypotheses, it is unclear how many hypotheses are kept after filtering. It is better to always keep 8 hypotheses after filtering. \n\nWe will only keep the first correct hypothesis found for each task. So we are testing one hypothesis for each task in the human-selected experiment. We agree that selecting 8 hypotheses after filtering would be better for consistency; there are likely other clever oracles that one might use for filtering for this (e.g., using GPT-4 to find the most similar hypotheses to the ground truth), and we believe this would be a valuable exploration for future work.\n\n> In addition, it is unclear why the number of execution rounds varies for different methods. It is better to unify the setup for a fair comparison.\n\nGreat point! We have unified all of the experiments to have three feedback rounds.\n\n> From Table 2, it is interesting to see that the final performance of GPT-3.5 is comparable to GPT-4. Have you tried gpt-3.5-turbo-16k, which has a longer context length? The performance may further improve.\n\nFirst, we apologize for the confusion - the table is actually showing GPT-4 results, but the caption was ambiguous. We have run an additional experiment using gpt-3.5-turbo-16k to generate 128 programs with human-written hypotheses on the 100 sampled ARC tasks and achieved an accuracy of 30%, which is a little bit higher than the accuracy of gpt-3.5-turbo (27%).\n\n> The findings on SyGuS are divergent from the main evaluation, as the best result is achieved with purely code generation.\n\nThank you for this point! We\u2019d like to clarify that we don\u2019t consider the SyGuS results as contradicting the overall result: indeed, our pipelines (both Program Only and Full) both perform significantly better than prior work. However, we do find that the natural language hypotheses are not a useful additional abstraction in this dataset. This is likely because the SyGuS problems are relatively easy.\n\nWe have also introduced a new dataset to demonstrate the usefulness of natural language hypotheses on non-ARC tasks. Specifically, we now investigate the cognitive-science-inspired inductive reasoning dataset, List Functions, from \u201cThe Child as Hacker\u201d [1] which is fairly different from any of the current three datasets. As on the ARC datasets, this shows a clear advantage with the natural language hypotheses. \n\n| Method           | 64 hypotheses   |\n|------------------|-----------------|\n| Direct Prompting | 31%             |\n| Program Only     | 59%             |\n| Full             | 69%             |\n\n[1] \u201cThe Child as a Hacker,\u201d Rule et al. 2020"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545777293,
                "cdate": 1700545777293,
                "tmdate": 1700545777293,
                "mdate": 1700545777293,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HxJn49jiWF",
                "forum": "G7UtIGQmjm",
                "replyto": "rNfYkBo65M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3169/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer PFzt (Evaluating All Summarized Hypotheses)"
                    },
                    "comment": {
                        "value": "Hi! We appreciate your patience -- we've now also completed the experiment where we evaluated all of the summarized hypotheses, not just the ones where the tasks had human-selected hypotheses. Including the problems without human-selected hypotheses solved two additional problems, bringing the summarized score up to 30% (from 28%). Notably, this suggests that the success rate for problems with a correct hypothesis before summarization was roughly 57% (28/49), while the success rate for the others was about 4% (2/51). We've updated all of the corresponding parts of the paper (Tables 1 and 2, the abstract, and Section 3.2).\n\nFor one of the newly solved problems, the summarized hypothesis is correct despite all of its corresponding hypotheses having mistakes. In this case, the summarization removes some incorrect details from the original hypotheses. For the other problem, the model solved it despite not having a correct hypothesis.\n\nThanks again for the great suggestions, and please let us know if you have any remaining questions!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599556664,
                "cdate": 1700599556664,
                "tmdate": 1700599556664,
                "mdate": 1700599556664,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AezMgMjCMI",
            "forum": "G7UtIGQmjm",
            "replyto": "G7UtIGQmjm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3169/Reviewer_SPJ6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3169/Reviewer_SPJ6"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the challenge of inductive reasoning in large language models (LLMs). Directly prompting by in-context learning may not be able to solve complex tasks. The authors propose a novel approach inspired by the Bayesian rule that involves generating explicit hypotheses in natural language and then translating them into concrete Python programs, which can be verified. This approach, tested on tasks like ARC, 1D-ARC, and SyGuS, significantly improves LLMs' performance. By combining abstract reasoning with programmatic logic, and filtering hypotheses through LLM summaries or human annotators, the method demonstrates substantial improvements, achieving up to 37.5% accuracy on ARC, compared to a 12.5% baseline. The paper highlights the synergy between natural language processing and programmatic approaches in enhancing LLM inductive reasoning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper introduces a novel method of enhancing inductive reasoning in LLMs by generating explicit hypotheses and translating them into Python programs. This approach creatively combines the strengths of natural language processing and programmatic logic, offering a unique solution to the challenge of inductive reasoning in complex tasks.\n2. The paper stands out for its robust methodology and the quality of its experimental results. The authors thoroughly test their approach on challenging datasets like ARC, demonstrating significant improvements in LLM performance. The ablation studies further substantiate the quality of the research, clarifying the contributions of each component of the proposed method.\n3. The presentation is good with clarity, presenting complex ideas and methodologies in a comprehensible manner. This clarity enhances the paper's accessibility to a broad audience, which is crucial for disseminating innovative ideas."
                },
                "weaknesses": {
                    "value": "1. While the method of generating and implementing hypotheses as Python programs is innovative, it may pose scalability challenges. For instance, generating a large number of hypotheses for complex problems could be computationally intensive and time-consuming. Moreover, the filtering process\u2014whether automated or human-assisted\u2014might not efficiently narrow down to the most effective hypotheses. To improve, the authors could explore more sophisticated algorithms for hypothesis generation that prioritize efficiency and scalability, possibly through more advanced heuristics or machine learning techniques.\n2. The paper demonstrates success in specific datasets like ARC, 1D-ARC, and SyGuS, but it's unclear how well this method generalizes to other types of inductive reasoning tasks, particularly those with differing structures or complexity levels, or even cannot be solved with python programs. Also, the baselines are limited only with direct prompting and ablated baselines, with no baselines from related works. In other words, the range of experimental tasks presented is somewhat limited, potentially restricting the scope of the paper\u2019s conclusions.\n3. The hypothesis proposal and selection process is essentially a search problem. The proposed iterative sampling and verification process is costly and inefficient from the perspective of search. The authors could consider more advanced search methods, such as DFS/BFS/MSTC, etc. Get some inspiration from the recent tree search prompting literature, like Tree-of-thoughts, reasoning-via-planning, etc."
                },
                "questions": {
                    "value": "- Is there potential for the proposed method to be generalized across a broader array of tasks beyond those presented in the paper?\n- How might this method perform tasks that are inherently difficult or perhaps impossible to encapsulate within a programmable framework?\n- Could the authors clarify the missing elements in the appendix that might be pertinent to the paper's methodology or findings?\n- Regarding the ARC tasks, what is the average duration, and why do most exceed the 4096 token limit imposed by many LLMs?\n- The Direct Prompting baseline, is it just few-shot prompting or Chain-of-thought prompting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3169/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3169/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3169/Reviewer_SPJ6"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3169/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699211226583,
            "cdate": 1699211226583,
            "tmdate": 1699636264374,
            "mdate": 1699636264374,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gbLkPreeRu",
                "forum": "G7UtIGQmjm",
                "replyto": "AezMgMjCMI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3169/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer SPJ6"
                    },
                    "comment": {
                        "value": "Thank you for your detailed and insightful comments. We're happy to have the chance to clear up these points, but please let us know if anything is still unclear.\n\n> While the method of generating and implementing hypotheses as Python programs is innovative, it may pose scalability challenges.\n\nThis is an excellent point - indeed, we emphasize the high cost of this approach throughout the paper, as well as some strategies that we took to attempt to mitigate it (e.g., since the primary cost comes from implementing rather than generating hypotheses, our summarization approach substantially reduces the necessary costs). We highlight multiple approaches to improve the scalability by focusing on selecting or extracting a smaller set of hypotheses with the language model; on the other hand, our method can also provide a flexible trade-off between performance and cost by varying the number of hypotheses and programs to search for. This is unachievable by the baseline method that directly prompts the answer. \nRecent work makes the reduced performance from the current search-space-reduction approaches less surprising, emphasizing that even advanced language models struggle to evaluate their own generations without external feedback [1].\nFurthermore, we would like to highlight the flexibility of Python code from a program synthesis perspective. Before LLMs, program synthesis typically required operating over a small DSL with a very restrictive grammar, e.g. regular expressions, string manipulations, or 3D graphics [2,3,4,5]. However, we agree that this is an important point. To emphasize it, we have added a section on limitations and future work, where we discuss this.\n> it's unclear how well this method generalizes to other types of inductive reasoning tasks, particularly those with differing structures or complexity levels, or even cannot be solved with python programs\n\nThanks for this point! In response, we\u2019ve added a new cognitive-science-inspired inductive reasoning dataset, List Functions, from \u201cThe Child as Hacker\u201d [6] which is fairly different from any of the current three datasets. As on the ARC datasets, this shows a clear advantage with the natural language hypotheses.\n\n| Method           | 64 hypotheses   |\n|------------------|-----------------|\n| Direct Prompting | 31%             |\n| Program Only     | 59%             |\n| Full             | 69%             |\n\nWe also agree that there are always more datasets that we could include. We\u2019d like to note that datasets like 1D-ARC are designed to vary the complexity and structure of ARC, and SyGus is a very differently structured task. In terms of tasks that cannot be solved by Python programs, in discussing future work, we\u2019ve added discussion around more open-ended tasks like summarization where we might want to optimize a \u201csoft\u201d score (e.g., ROUGE [7]). In short, these problems are still approachable with Python but may require the language model to leverage another machine learning model. \n\n> Also, the baselines are limited only with direct prompting and ablated baselines, with no baselines from related works\n\nThanks for raising this point. One of the reasons that we were excited to work on this direction is that there is not an extensive body of work that has explored inductive reasoning with language models. We will highlight that certain ablations can indeed be seen as corresponding to applying related works to the inductive reasoning task. For example, the program-only baseline without revisions can be seen as somewhat analogous to the \u201cProgram of Thoughts\u201d prompting if applied to the inductive reasoning domain [8]. We\u2019ve also added a language-only baseline that can be seen as analogous to a \u201cChain of Thoughts\u201d baseline when applied to inductive reasoning [9]. We have also now modified our main text to incorporate this. We\u2019ve also run the state-of-the-art DSL-based approach for ARC on our 100-task subset, finding that it performs slightly below the human-written score, solving 43 of the problems correctly. We\u2019ve added this context in Section 3.2."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545389026,
                "cdate": 1700545389026,
                "tmdate": 1700545865810,
                "mdate": 1700545865810,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "t8tn2VO2W6",
            "forum": "G7UtIGQmjm",
            "replyto": "G7UtIGQmjm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3169/Reviewer_qtss"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3169/Reviewer_qtss"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a pipeline to solve abstraction and reasoning tasks. The pipeline prompts LLMs to propose hypothesis about the problem, convert the hypothesis into executable programs, which is later validated against the ground truth outputs given inputs. Experiments on ARC, 1D-ARC, and SyGus demonstrates that the proposed pipeline is effective."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The ablation studies are quite extensive. The authors dissect the effect of each component in the pipeline by, for example, skipping the program generation, skipping generation of natural language hypothesis. The performance improvement of the full pipeline is also clear.\n- The abundant technical details contribute to the reproducibility of the work."
                },
                "weaknesses": {
                    "value": "- Which part of the pipeline is novel is not quite clear from the paper writing\n\nThe paper introduces every part the proposed pipeline in intensive details - but it is not quite clear which part of the pipeline is novel. I feel compared to earlier works like program-of-thoughts, the novel part is generating natural language hypothesis before program generation and a verification step to verify the correctness of hypothesis. I suggest adding a paragraph in introduction to highlight which parts are novel and the contributions of the work.\n\n\n- I feel some experiments, such as comparing the performance of GPT 3.5 and GPT 4 is not relevant to the main contribution of the paper. The numbers of these experiments can be moved to appendix to avoid distraction."
                },
                "questions": {
                    "value": "- In Table 3, why are the names of the methods different from Table 1. Does \"Full\" in Table correspond to any method in Table 1?\n- For negative results presented in Sec. 3.4, I suggest to summarize them in a table as well."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3169/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699600898856,
            "cdate": 1699600898856,
            "tmdate": 1699636264306,
            "mdate": 1699636264306,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4pau6eXcxc",
                "forum": "G7UtIGQmjm",
                "replyto": "t8tn2VO2W6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3169/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer qtss"
                    },
                    "comment": {
                        "value": "Thank you for your excellent questions!\n\n> The paper introduces every part the proposed pipeline in intensive details - but it is not quite clear which part of the pipeline is novel. I feel compared to earlier works like program-of-thoughts, the novel part is generating natural language hypothesis before program generation and a verification step to verify the correctness of hypothesis.\n\nThank you for this great point! We\u2019ve added a short list of contributions to the introduction.\n\n> I feel some experiments, such as comparing the performance of GPT 3.5 and GPT 4 is not relevant to the main contribution of the paper. The numbers of these experiments can be moved to appendix to avoid distraction.\n\nThanks again for the suggestion - we\u2019ve now moved the GPT-3.5 experiments to the appendix.\n\n> In Table 3, why are the names of the methods different from Table 1. Does \"Full\" in Table correspond to any method in Table 1?\n\nThis is a great question. Because the problem sizes (in terms of the number of tokens necessary) and the tasks are easier for 1D-ARC, for the \u201cFull\u201d method, we evaluate all of the generated hypotheses. In contrast, a \u201cFull\u201d evaluation of all of the generated hypotheses for ARC would be prohibitively infeasible for us at the current cost of GPT-4. We\u2019ve now clarified this in the text of the corresponding subsection. For the direct prompting method, we report the results from [2] for a direct comparison.\n\n> For negative results presented in Sec. 3.4, I suggest to summarize them in a table as well.\n\nThank you for this suggestion! We\u2019ve added a table to Sec. 3.4, but we\u2019d like to clarify that we don\u2019t consider the SyGuS results as negative. Indeed, both the Program-Only and Full hypothesis search pipelines perform significantly better than prior work. However, we do find that the natural language hypotheses are not a useful additional abstraction in this dataset. Likely because the SyGuS tasks are simply easier. We also introduce a new dataset to demonstrate the usefulness of natural language hypotheses on non-ARC tasks. Specifically, we now investigate the cognitive-science-inspired inductive reasoning dataset, List Functions, from \u201cThe Child as Hacker\u201d [1] which is fairly different from any of the current three datasets. As on the ARC datasets, this shows a clear advantage with the natural language hypotheses. \n\n| Method           | 64 hypotheses   |\n|------------------|-----------------|\n| Direct Prompting | 31%             |\n| Program Only     | 59%             |\n| Full             | 69%             |\n\n  \n[1] \u201cThe Child as a Hacker,\u201d Rule et al. 2020  \n[2] LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations. Xu et al. 2023"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546601527,
                "cdate": 1700546601527,
                "tmdate": 1700546601527,
                "mdate": 1700546601527,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]