[
    {
        "title": "Compositional Generative Inverse Design"
    },
    {
        "review": {
            "id": "eGQp7RPlXv",
            "forum": "wmX0CqFSd7",
            "replyto": "wmX0CqFSd7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1627/Reviewer_WWyF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1627/Reviewer_WWyF"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel approach to tackle inverse design problems commonly found in engineering fields by optimizing energy functions within a diffusion model, instead of using traditional optimization techniques over learned dynamic models. This method addresses the challenge of adversarial modes encountered in the optimization process, improving design performance. By employing a compositional design system, the paper illustrates the potential to merge multiple diffusion models representing different subsystems, enhancing the complexity and specificity of the design.  The introduced Compositional Inverse Design with Diffusion Models (CinDM) method is highlighted for its capability to address out-of-distribution and more intricate design inputs beyond the training data, demonstrating promising advancements in the field of inverse design."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- A novel approach in addressing an interesting problem in the field of neural inverse design.\n- The results seem to be well enough, showcasing the benefits of this novel approach. However, further evaluation with the state-of-the-art counterpart neural inversion methods is required."
                },
                "weaknesses": {
                    "value": "The manuscript lacks discussion on some pivotal related works, notably the contributions by Ren et al. [1] and Ansari et al [2]. Ren et al. elucidated various neural inversion methodologies and assessed their efficacy and accuracy encompassing Neural Adjoint, Tandem, Invertible Neural Networks, among other techniques. Additionally, they proposed a regularization scheme to mitigate the occurrence of out-of-distribution solutions.\n\nOn the other hand, Ansari et al. put forth a method wherein uncertainty information is integrated during the neural network inversion process. They asserted a multitude of benefits for this tactic, such as avoiding out-of-distribution solutions and erroneous local minima, alongside diminishing the model's susceptibility to initialization conditions.\n\nThese inversion methods should be mentioned and where possible compared with the proposed approach. In the cases where a comparison is not possible, sufficient explanation is required.\n\n[1]  Ren, Simiao, Willie Padilla, and Jordan Malof. \"Benchmarking deep inverse models over time, and the neural-adjoint method.\" Advances in Neural Information Processing Systems 33 (2020): 38-48.\n\n[2] Ansari, Navid, et al. \"Autoinverse: Uncertainty aware inversion of neural networks.\" Advances in Neural Information Processing Systems 35 (2022): 8675-8686.\n\n- A proper discussion over the limitations is missing.\n- The code and dataset is missing."
                },
                "questions": {
                    "value": "- How other neural inversion methods perform in the context of the proposed experiments?\n- How sensitive is this inversion method to hyperparameters and initialization?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1627/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1627/Reviewer_WWyF",
                        "ICLR.cc/2024/Conference/Submission1627/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1627/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698121263075,
            "cdate": 1698121263075,
            "tmdate": 1700643696968,
            "mdate": 1700643696968,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u317vPkJ86",
                "forum": "wmX0CqFSd7",
                "replyto": "eGQp7RPlXv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer WWyF (1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thoughtful comments. In the following, we address the points the reviewer raised about baseline comparison and answer the questions.\n\n>__Comment 1__: The manuscript lacks a discussion on some pivotal related works, notably the contributions by Ren et al. [1] and Ansari et al [2]. Ren et al. elucidated various neural inversion methodologies and assessed their efficacy and accuracy encompassing Neural Adjoint, Tandem, and Invertible Neural Networks, among other techniques. Additionally, they proposed a regularization scheme to mitigate the occurrence of out-of-distribution solutions.\n\n>On the other hand, Ansari et al. put forth a method wherein uncertainty information is integrated during the neural network inversion process. They asserted a multitude of benefits for this tactic, such as avoiding out-of-distribution solutions and erroneous local minima, alongside diminishing the model's susceptibility to initialization conditions.\n\n>These inversion methods should be mentioned and where possible compared with the proposed approach. In the cases where a comparison is not possible, sufficient explanation is required.\n\n>[1] Ren, Simiao, Willie Padilla, and Jordan Malof. \"Benchmarking deep inverse models over time, and the neural-adjoint method.\" Advances in Neural Information Processing Systems 33 (2020): 38-48.\n\n>[2] Ansari, Navid, et al. \"Autoinverse: Uncertainty aware inversion of neural networks.\" Advances in Neural Information Processing Systems 35 (2022): 8675-8686.\n\n**Answer**: Thank you for the valuable feedback and the suggestion of the literature. In the updated manuscript, we have added in the \u201cRelated Work\u201d section the citation to the paper \"Benchmarking deep inverse models over time, and the neural-adjoint method\" [1] and\u00a0 \"Autoinverse: Uncertainty aware inversion of neural networks\" [2] as well as some related works \"Design-bench: Benchmarks for data-driven offline model-based optimization\" and \"Bidirectional learning for offline infinite-width model-based optimization\". While both of the literature [1, 2] perform inverse design for a wide range of applications, the literature has notable differences from our work (which makes it not feasible to compare within the experiment in the main text.) The methods in [1, 2] aim to avoid out-of-distribution solutions, while CinDM aims to generalize to out-of-distribution and more complex design inputs than seen in training. Neural Adjoint method in [1] relies on the boundary loss function defined by statistics of *training data* such as confidence interval and mean in the inverse design task, which in principle means once the design gets far from training data, the design variable is forced back to in-distribution domain. INN in [1, 2] also cannot generalize to a longer temporal scale because the temporal scale for the invertible neural network function is fixed. On the other hand, CinDM achieves generalization to **_out-of-distribution_** in terms of both spatial and temporal scales. The key idea that allows CinDM to generalize to out-of-distribution is to view the inverse design problem from an energy optimization perspective, where local constraints of the simulation model are implicitly captured through learning the generative energy function, and spatially and/or temporally compose multiple generative energy functions to govern global constraints. With this compositional feature, CinDM may design sequences of outputs that are significantly longer than the ones seen in training and design systems with many more objects and more complex shapes than those seen in training. Namely, our model is trained over trajectories with two bodies (in the N-body task) and a single boundary (in the 2D airfoils task) and is tested on larger numbers of bodies and boundaries for longer time horizons. Despite the difference, we acknowledge the importance of comparing our method to the representative baselines in [1, 2]. Thus we evaluated these baselines. The implementation and results are detailed in the response to the next comment."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1627/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625825620,
                "cdate": 1700625825620,
                "tmdate": 1700628746836,
                "mdate": 1700628746836,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FQM6cWuHSg",
                "forum": "wmX0CqFSd7",
                "replyto": "eGQp7RPlXv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer WWyF (2)"
                    },
                    "comment": {
                        "value": ">__Comment 2__: How other neural inversion methods perform in the context of the proposed experiments?\n\n**Answer**: Thank you for the suggestions. In the updated manuscript, we have evaluated two additional baselines: neural adjoint method + boundary loss function (NABL) and conditional invertible neural network (cINN) method for both N-body and 2D airfoils experiments (they are added to Appendix H). Since it has been shown in\u00a0 [1] that cINN outperforms INN, we adopt cINN instead of INN for comparison. We note that tandem in [1] is unfeasible in our problem settings because the output dimension is smaller than the dimension of the input of our models.\n\nWe implement NABL on top of our previous baseline architectures, namely FNO and LE-PDE in the 2D airfoils task and U-Net in the N-body task, named \u201cNABL, FNO\u201d, \u201cNABL, LE-PDE\u201d and \u201cNABL, U-Net\u201d, respectively. These new NABL baselines additionally use the boundary loss defined by the mean value and 95% significance radius of the training dataset [1], which aims to mitigate the occurrence of out-of-distribution solutions.\n\nFor cINN, we use the implementation from [https://github.com/BensonRen/BDIMNNA](https://github.com/BensonRen/BDIMNNA), which is suggested by [1]. cINN is not applicable to compositional design because the input scale for the invertible neural network function is fixed. Therefore, in the N-body time composition task, we trained 4 cINN models, each for one of the time steps: 24, 34, 44, and 54. These models differ only in the input size. The input $x$ to cINN is a vector of size\u00a0 2 * 4 * number_steps. The condition y is set to 0, the minimal distance to the target point. For cINN for 2D airfoil design, we adopt as the input 2D coordinates of 40 boundary points, which are spanned into an 80-dimensional vector, since the invertible constraint on the cINN model hardly accepts image-like inputs adopted in the main experiments. Therefore we evaluate cINN only in the single airfoil design task. The condition $y$ is set as the minimal value of drag - lift drag in the training trajectories. In both tasks, the random variable $z$ has a dimension of dim($x$) - dim($y$). It is drawn from a Gaussian distribution and then input to the INN for inference. We also adjust the hyperparameters, such as hidden size and the number of reversible blocks, to make the number of parameters in cINN close to ours for a fair comparison. The results of NABL and cINN are shown in Table 10 and Table 11 below. Due to limitation of the number of words, we present analysis of results in the next section of response."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1627/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626059150,
                "cdate": 1700626059150,
                "tmdate": 1700628607142,
                "mdate": 1700628607142,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PSXzzpCTJm",
                "forum": "wmX0CqFSd7",
                "replyto": "eGQp7RPlXv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer WWyF (3)"
                    },
                    "comment": {
                        "value": "From Table 10 and Table 11, we see that CinDM significantly outperforms the new baselines in both experiments. Even compared to the original baselines without the boundary loss function, the NABL baselines in both N-body and 2D airfoils tasks do not show an improvement in the objective for out-of-distribution data. These results show that our method generalizes to out-of-distribution while the original and new baselines struggle to generalize to out-of-distribution samples.\n\nOur method also outperforms cINN by a large margin in both the time composition and airfoil design tasks. Despite the quantities, we also find that airfoil boundaries generated by cINN have little variation in shape, and the orientation is not as desired, which could incur high drag force in simulation. These results may be caused by the limitation of the model architecture of cINN, which utilizes fully connected layers as building blocks and thus has an obvious disadvantage in capturing inductive bias of spatial-temporal features. We think it is necessary to extend cINN to convolutional networks if we expect cINN to deal with such high-resolution design problems. However, this appears challenging when the invertible requirement is imposed.\n\nIn summary, our method outperforms both NABL and cINN in both experiments. Furthermore, our method could be used for flexible compositional design. We use only one trained model to generate samples lying in a much larger state space than in training during inference, which is a unique advantage of our method beyond these baselines.\n\nThese results and analysis of NABL and cINN baselines are presented in Appendix H in the updated manuscript.\n\nTable 10. Comparison to baseline NABL and cINN for the 2D airfoils inverse design task.\n\n| Methods      | #Parameters(million) | obj(1 airfoil) | lift/drag(1 airfoil) | obj(2 airfoils) | lift/drag(2 airfoils) |\n|--------------|----------------------|:--------------:|:--------------------:|:--------------:|:--------------------:|\n| NABL, FNO    |         3.29         |     0.03232    |        1.3169        |     0.3071     |        0.9541        |\n| NABL, LE-PDE |         3.13         |     0.1010     |        1.3104        |     0.0891     |        0.9860        |\n| cINN         |         3.07         |     1.1745     |        0.7556        |        -       |           -          |\n| CinDM        |         3.11         |     0.0797     |         2.177        |     0.1986     |        1.4216        |\n\nTable 11. Comparison to baseline NABL and cINN for the N-body inverse design task.\n\n| Methods     | design_obj\uff082-body 24 steps\uff09 | MAE\uff082-body 24 steps\uff09 | design_obj\uff082-body 34 steps\uff09 | MAE\uff082-body 34 steps\uff09 | design_obj\uff082-body 44 steps\uff09 | MAE\uff082-body 44 steps\uff09 | design_obj\uff082-body 54 steps\uff09 | MAE\uff082-body 54 steps\uff09 |\n|-------------|-------------------------------|------------------------|-------------------------------|------------------------|-------------------------------|------------------------|-------------------------------|------------------------|\n| NABL\uff0cU-net | 0.1174 \u00b1 0.0071               | 0.01650 \u00b1 0.00807      | 0.1425 \u00b1 0.0106               | 0.01511 \u00b1 0.00197      | 0.1788 \u00b1 0.0192               | 0.01185 \u00b1 0.00511      | 0.2606 \u00b1 0.0064               | 0.02042 \u00b1 0.00378      |\n| cINN        | 0.3235 \u00b1 0.0188               | 0.11704 \u00b1 0.01848      | 0.3085 \u00b1 0.0349               | 0.18015 \u00b1 0.02608      | 0.3478 \u00b1 0.0680               | 0.18322 \u00b1 0.02740       | 0.3372 \u00b1 0.0233               | 0.19296 \u00b1 0.01825      |\n| CinDM(ours) | 0.1143 \u00b1 0.0047               | 0.01202 \u00b1 0.00114      | 0.1251 \u00b1 0.0071               | 0.00763 \u00b1 0.00069      | 0.1326 \u00b1 0.0087               | 0.00695 \u00b1 0.00067      | 0.1533 \u00b1 0.0140               | 0.00870 \u00b1 0.00150      |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1627/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626130925,
                "cdate": 1700626130925,
                "tmdate": 1700629021456,
                "mdate": 1700629021456,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zgpgKzhsge",
                "forum": "wmX0CqFSd7",
                "replyto": "eGQp7RPlXv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer WWyF (4)"
                    },
                    "comment": {
                        "value": ">__Comment 3__: A proper discussion over the limitations is missing.\n\n**Answer**: In the original manuscript, we have already discussed the limitations of our method in Appendix G and referenced in the \u201cSection 5. Conclusion\u201d section. In the updated manuscript, the discussions are improved and are presented in the Appendix J, which we elaborate below:\n\n(1) In terms of design quality and exploration space, we need to strike a balance between different objectives to avoid getting stuck in local optima, especially when dealing with complex, nonlinear systems in the real world. We also need to ensure that the designed samples adhere to complex multi-scale physical constraints.\n\n(2) Furthermore, achieving interpretability in the samples designed by deep learning models is challenging for inverse design applications.\n\n(3) From a cost perspective, training diffusion models requires large datasets and intensive computational resources. The complexity of calculations also hinders the speed of our model design.\n\n>__Comment 4__: The code and dataset is missing.\n\n**Answer**: We release our code to reviewers via the anonymous link: https://anonymous.4open.science/r/CinDM_anonymous-0CAF, inside which we also present downloading link to the example datasets and trained model checkpoints in README.md file. The current datasets contain the full 2-body dataset and part of the 2D airfoils dataset. We have in total 30,000 airfoil trajectories, which is about 400GB, and we release 1,00 trajectories among them due to space limitations. Reviewers can follow the instructions to install the environment and run the code on the example datasets. Our code will be public once our paper is accepted."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1627/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626382921,
                "cdate": 1700626382921,
                "tmdate": 1700636586318,
                "mdate": 1700636586318,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lDtK6gofJi",
                "forum": "wmX0CqFSd7",
                "replyto": "eGQp7RPlXv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer WWyF (5)"
                    },
                    "comment": {
                        "value": ">__Comment 5__: How sensitive is this inversion method to hyperparameters and initialization?\n\n**Answer**:\n\nThe key hyperparameter in our method is $\\lambda$, a tradeoff between the (compositional) energy model and design objective. Its influence is also concerned by Reviewer 4Mfs and Reviewer 7GzX. We have performed additional experiments to test the influence of $\\lambda$ for both N-body and 2D airfoils inverse design. The results are listed in the following Table 12 and Table 13. From these tables, we can see that our method is robust and has steady performance in a wide range of $\\lambda$. If $\\lambda$ is set too small (<=0.0001 in the 2D airfoils task, or <0.01 in the N-body task), then the design results are inferior because only little objective guidance is added. If $\\lambda$ is set too large (>0.01 in the 2D airfoils task, or >1.0 in the N-body task), then it is prone to enter a poor likelihood region, and physical consistency is not well preserved. In practice, $\\lambda$ could be set as 0.01 to 1.0 for the N-body task and 0.0002 to 0.02 for the 2D airfoils task.\n\nThe evaluation results and analysis of $\\lambda$ are presented in Appendix I.1 in the updated manuscript.\n\nTable 12. Effect of $\\lambda$ in 2 airfoils inverse design.\n\n| $\\lambda$ | obj | lift/drag |\n| --- | --- | --- |\n| 0.05 | 0.7628\u00b10.1892 | 1.0150\u00b10.2008 |\n| 0.02 | 0.3849\u00b10.0632 | 1.0794\u00b10.1165 |\n| 0.01 | 0.2292\u00b10.0408 | 1.2860\u00b10.1402 |\n| 0.005 | 0.2061\u00b10.0388 | 1.2378\u00b10.1414 |\n| 0.002 | 0.2170\u00b10.0427 | 1.2429\u00b10.1243 |\n| 0.001 | 0.2277\u00b10.0451 | 1.2608\u00b10.1469 |\n| 0.0005 | 0.2465\u00b10.0473 | 1.4102\u00b10.1771 |\n| **0.0002** | **0.1986\u00b10.0431** | **1.4216\u00b10.1607** |\n| 0.0001 | 0.2710\u00b10.0577 | 1.1962\u00b10.1284 |\n\nTable 13. Effect of $\\lambda$ in N-body time composition inverse design.\n\n| $\\lambda$ | design_obj(2-body 24 steps) | MAE(2-body 24 steps) | design_obj(2-body 34 steps) | MAE(2-body 34 steps) | design_obj(2-body 44 steps) | MAE(2-body 44 steps) | design_obj(2-body 54 steps) | MAE(2-body 54 steps) |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0.0001 | 0.3032 \u00b1 0.0243 | 0.00269 \u00b1 0.00047 | 0.2954 \u00b1 0.0212 | 0.00413 \u00b1 0.00155 | 0.3091 \u00b1 0.0223 | 0.00394 \u00b1 0.00076 | 0.2996 \u00b1 0.0201 | 0.01046 \u00b1 0.00859 |\n| 0.001 | 0.2531 \u00b1 0.0185 | 0.00385 \u00b1 0.00183 | 0.2937 \u00b1 0.0213 | 0.00336 \u00b1 0.00115 | 0.2797 \u00b1 0.0190 | 0.00412 \u00b1 0.00105 | 0.2927 \u00b1 0.0219 | 0.00521 \u00b1 0.00103 |\n| 0.01 | 0.1200 \u00b1 0.0069 | 0.00483 \u00b1 0.00096 | 0.1535 \u00b1 0.0135 | 0.00435 \u00b1 0.00100 | 0.1624 \u00b1 0.0137 | 0.00416 \u00b1 0.00059 | 0.1734 \u00b1 0.0154 | 0.00658 \u00b1 0.00267 |\n| 0.1 | 0.1201 \u00b1 0.0046 | 0.01173 \u00b1 0.00150 | 0.1340 \u00b1 0.0107 | 0.00772 \u00b1 0.00099 | 0.1379 \u00b1 0.0088 | 0.00816 \u00b1 0.00149 | 0.1662 \u00b1 0.0180 | 0.01141 \u00b1 0.00473 |\n| 0.2 | 0.1283 \u00b1 0.0141 | 0.01313 \u00b1 0.00312 | 0.1392 \u00b1 0.0119 | 0.00836 \u00b1 0.00216 | 0.1529 \u00b1 0.0130 | 0.01019 \u00b1 0.00584 | 0.1513 \u00b1 0.0131 | 0.00801 \u00b1 0.00172 |\n| **0.4** | **0.1143 \u00b1 0.0047** | **0.01202 \u00b1 0.00114** | **0.1251 \u00b1 0.0071** | **0.00763 \u00b1 0.00069** | **0.1326 \u00b1 0.0087** | **0.00695 \u00b1 0.00067** | **0.1533 \u00b1 0.0140** | **0.00870 \u00b1 0.00150** |\n| 0.6 | 0.1259 \u00b1 0.0100 | 0.01382 \u00b1 0.00115 | 0.1326 \u00b1 0.0126 | 0.01171 \u00b1 0.00595 | 0.1592 \u00b1 0.0151 | 0.01140 \u00b1 0.00355 | 0.1670 \u00b1 0.0177 | 0.00991 \u00b1 0.00287 |\n| 0.8 | 0.1217 \u00b1 0.0073 | 0.01596 \u00b1 0.00127 | 0.1385 \u00b1 0.0120 | 0.01095 \u00b1 0.00337 | 0.1573 \u00b1 0.0116 | 0.00893 \u00b1 0.00113 | 0.1715 \u00b1 0.0181 | 0.01026 \u00b1 0.00239 |\n| 1 | 0.1330 \u00b1 0.0063 | 0.01679 \u00b1 0.00139 | 0.1428 \u00b1 0.0112 | 0.01087 \u00b1 0.00149 | 0.1634 \u00b1 0.0119 | 0.00968 \u00b1 0.00079 | 0.1789 \u00b1 0.0164 | 0.01102 \u00b1 0.00185 |\n| 2 | 0.1513 \u00b1 0.0079 | 0.02654 \u00b1 0.00160 | 0.1795 \u00b1 0.0129 | 0.01765 \u00b1 0.00193 | 0.1779 \u00b1 0.0121 | 0.01707 \u00b1 0.00474 | 0.2113 \u00b1 0.0161 | 0.01447 \u00b1 0.00130 |\n| 10 | 0.2821 \u00b1 0.0197 | 0.21153 \u00b1 0.01037 | 0.2210 \u00b1 0.0149 | 0.09715 \u00b1 0.00236 | 0.2273 \u00b1 0.0133 | 0.07781 \u00b1 0.00232 | 0.2269 \u00b1 0.0175 | 0.06538 \u00b1 0.00210 |\n\nDue to limitation of the number of words, we present analysis of sensitivity of initialization in the next section of response."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1627/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626517560,
                "cdate": 1700626517560,
                "tmdate": 1700628519650,
                "mdate": 1700628519650,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a8u3COy5BK",
                "forum": "wmX0CqFSd7",
                "replyto": "eGQp7RPlXv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer WWyF (6)"
                    },
                    "comment": {
                        "value": "For sensitivity of initialization, we take a similar analysis approach presented in [1]. We view the \u201cre-simulation\u201d error $r$ of a target objective $y$ as a function of $T$, where T is the number of samplings and each sampling starts from a Gaussian initialization $z$. For each $x$ from the $T$ design results given the target $y$, we use the simulator to obtain its output $\\hat{y}$ and compute the \u201cre-simulation\u201d error $L(\\hat{y}, y)$, then we compute the least error among a batch of $T$ design results. For each $T$, we take an average over $N$ such batches to get the mean least error $r_T$.\n\nResults for the N-body inverse design task are presented in Table 14 below, where we take $T={10,20,\\cdots,100}$ and $N=10$. The target $y$ is set as 0, the distance to a fixed target point. We can see that $r_T$ decreases very slowly with increasing $T$ in the 24-step design, which means that the $x$-space is well explored and most solutions could be retrieved, even if a small sampling number is used. This shows that our method is efficient in generation. What\u2019s more, when time composition is performed in 34, 44, and 54 steps, we can get a similar kind of observation. This reflects the effectiveness of our time composition, which could capture long-time-range physical dependency well and perform efficient generation in a larger $x$-space.\n\nThe target $y$ of the 2D airfoils inverse design task is slightly different from the N-body task. The model output (drag - lift force) is expected to be as small as possible. Therefore, in the 2D airfoils task, we adopt the \u201cre-simulation\u201d performance metric, i.e. lift/drag, rather than a \u201cre-simulation\u201d error as in the N-body task, to evaluate sensitivity to initialization. For each $T$, lift/drag is chosen as the **highest** one over the simulation results of a batch of $T$ designed boundaries (or boundary pairs for 2 airfoils design). Note that we remove invalid design results like overlapping airfoil pairs in 2 airfoil designs from the $T$ results first and then compute maximal lift/drag over residue results. Finally, we average over $N$ such batches for each $T$ as the reported numbers.\n\nThe results for the 2D airfoil design task are shown in Table 15 below. In the 1 airfoil design column, we can see that the lift/drag performance is roughly steady with respect to $T$ when $T\\ge 20$. For $T= 10$, lift/drag is relatively low, which implies that the $x-$space is not sufficiently explored because its dimension is as high as 64x64x3 in our boundary mask and offsets representation. In the 2 airfoils design column, the lift/drag increases along with $T$. This is due to the higher dimensional and much more complex design space, compared to the single airfoil design task. Specifically, more strict constraints on boundary pairs, such as non-overlapping, induce many complex infeasible regions in the design space. Random initialization may end in these infeasible regions after a reverse diffusion process and the design results would be invalid. The increase of lift/drag becomes slow when $T \\geq 30$, which is an indicator that a majority of solutions have been explored. Considering that the training data only contain a single airfoil boundary, lying in a relatively low dimensional and simple design space, our model shows a strong ability of generalization and efficient generation for this very challenging 2 body compositional design problem. We hope our explanation could address the reviewer's concern.\n\nThese evaluation results and analysis of the influence of initialization are presented in Appendix I.2 in the updated manuscript.\n\nTable 14. Influence of initialization: $r_T$ with respect to $T$ for N-body inverse design task. Each number is an average over $N$=10 batches.\n\n| T | 2-body 24 steps | 2-body 34 steps | 2-body 44 steps | 2-body 54 steps |\n| --- | --- | --- | --- | --- |\n| 10 | 0.10123 | 0.10226 | 0.10542 | 0.11228 |\n| 20 | 0.10051 | 0.10123 | 0.10262 | 0.10555 |\n| 30 | 0.09951 | 0.10107 | 0.10221 | 0.10408 |\n| 40 | 0.09929 | 0.10066 | 0.10174 | 0.10409 |\n| 50 | 0.09795 | 0.10024 | 0.10169 | 0.10463 |\n| 60 | 0.09877 | 0.09997 | 0.10106 | 0.10257 |\n| 70 | 0.09858 | 0.09979 | 0.10124 | 0.10180 |\n| 80 | 0.09810 | 0.09973 | 0.10061 | 0.10203 |\n| 90 | 0.09809 | 0.09942 | 0.10109 | 0.10121 |\n| 100 | 0.09734 | 0.09913 | 0.10056 | 0.10135 |\n\nTable 15. Influence of initialization: lift/drag with respect to $T$ for 2D airfoils inverse design task. Each number is an average over $N$=10 batches.\n\n| T | 1 airfoil | 2 airfoils |\n| --- | --- | --- |\n| 10 | 1.4505 | 0.8246 |\n| 20 | 2.2725 | 0.7178 |\n| 30 | 2.2049 | 1.3862 |\n| 40 | 2.6506 | 1.5781 |\n| 50 | 2.1355 | 1.6055 |"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1627/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626604644,
                "cdate": 1700626604644,
                "tmdate": 1700626604644,
                "mdate": 1700626604644,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bM8rb54DzN",
                "forum": "wmX0CqFSd7",
                "replyto": "eGQp7RPlXv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1627/Reviewer_WWyF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1627/Reviewer_WWyF"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors for their thorough response. At this point, all my concerns are addressed, and I will change my evaluation accordingly."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1627/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643626500,
                "cdate": 1700643626500,
                "tmdate": 1700643807228,
                "mdate": 1700643807228,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "f25irp84mI",
            "forum": "wmX0CqFSd7",
            "replyto": "wmX0CqFSd7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1627/Reviewer_obHR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1627/Reviewer_obHR"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new approach to inverse design by optimizing over the energy function learned by a diffusion model combined with the target function instead of backpropagating through (surrogate) dynamics. The \"compositional\" comes from the fact that the energy functions/diffusion models are learned over overlapping slices across time and state space. Experiments are performed on fluid dynamics and n-body dynamics tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality:\n\n- the paper adopts or re-invents various tricks I've seen across the literature (unrolling across time steps and jointly diffusing, using a diffusion model as a smoothed ODE effectively) but does so in a clever combination\n- novelty: I'm not aware of any similar work, although conditional policydiffusion or codefusion might come close, and adding noise to FNO etc. is standard practice\n- clarity: overall clear presentation, especially on hyperparameters (kudos!), some questions (see below)\n- significance: difficult to judge in a still rather niche topic, but I think the general idea (learning sliced energy models to perform inverse design on) has promise to have high impact"
                },
                "weaknesses": {
                    "value": "- maybe I missed it, but page 7, I don't think $M$ is ever defined. How exactly do you train $M$ beyond the range of timesteps in training?\n- I would question the compositionality of the method and call it a \"piecewise\" or \"mixture\" approach? Given that you simply partition the spaces required into overlapping pieces (unless I misunderstood something)\n- Were the numbers of parameters matched for the different baselines? Given that you a partitioned energy functions, there might be potential for unfairness here?"
                },
                "questions": {
                    "value": "- Did you try training a single shared network across the overlapping chunks? I was kind of expecting something like this (maybe with different degrees of subsampling to give long and short range dynamics)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1627/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698699115004,
            "cdate": 1698699115004,
            "tmdate": 1699636091427,
            "mdate": 1699636091427,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KTmbhKemkq",
                "forum": "wmX0CqFSd7",
                "replyto": "f25irp84mI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer obHR (1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed feedback and helpful suggestions. We are glad that the reviewer recognizes the novelty, clarity, and potential impact of our work. Below, we address the reviewer\u2019s questions one by one.\n\n>__Comment 1__: maybe I missed it, but on page 7, I don't think M is ever defined. How exactly do you train M beyond the range of timesteps in training?\n\n**Answer**: $M$ is exactly the same as $N$ in Eq. (7), which means the number of time trunks for time composition. Actually, M is a hyperparameter and could not be \u201ctrained\u201d. By using $M \\geq 2$, our model could generalize to more time steps than training data in inverse design. In the experiment, we tested the performance of time composition on $M=1, 2, 3, 4$ for the N-body inverse design task. The results are shown in Table 1 of the manuscript.\n\nWe replaced M with N to avoid such confusion in the updated manuscript.\n\n>__Comment 2__: I would question the compositionality of the method and call it a \"piecewise\" or \"mixture\" approach? Given that you simply partition the spaces required into overlapping pieces (unless I misunderstood something)\n\n**Answer**: Although our method looks like a \u201cmixture\u201d approach, there is a substantial difference. Because the energy function can be viewed as the (unnormalized) negative log-probability by the relation $E_{\\theta}(x) \\propto -\\log p(x)$, our summation of energy functions of several components in compositional design corresponds to taking products of probability of each design component, which is an approximation of the joint distribution of the involved components. Thus the compositional energy function essentially models the *****_***concurrence***_***** of those components in a probabilistic approach, which could naturally govern their physics. It is quite different to a simple mixture over partitioned space with overlapping pieces, which lacks an effective global physical constraint in the whole space and thus may produce design results that fail to preserve physical consistency across subspaces. Our compositional energy approach is similar to past work [1] and it has been shown to be an effective way for compositional tasks. We hope our explanation could address the reviewer's concern.\n\n[1] Yilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum, Sander Dieleman, Rob Fer- gus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc. arXiv preprint arXiv:2302.11552, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1627/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625542665,
                "cdate": 1700625542665,
                "tmdate": 1700628864451,
                "mdate": 1700628864451,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cmvO5x2DwA",
                "forum": "wmX0CqFSd7",
                "replyto": "f25irp84mI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer obHR (2)"
                    },
                    "comment": {
                        "value": "> __Comment 3__: Were the numbers of parameters matched for the different baselines? Given that you a partitioned energy functions, there might be potential for unfairness here?\n\n**Answer**: Thanks for your reminder of the model size. This is an important problem in fair comparison. We count the number of parameters in all the baselines and our method. The results are shown in the following Table 6 and Table 7.\n\nFor the airfoils inverse design task, we see from Table 6 that the model size of CinDM is between the two baselines FNO and LEPDE. To make a fair comparison, we adjust the model size of all the baselines to align with our CinDM. \u201cCEM, FNO\u201d and \u201cBackprop, FNO\u201d share the same model architecture, so do \u201cCEM, LE-PDE\u201d and \u201cBackprop, LE-PDE\u201d. Note that we could not make the model size of baselines exactly the same as ours because the number of parameters is determined by hyperparameters of model architectures, i.e. hidden layer numbers and latent embedding size. Baselines with the updated number of parameters are trained and evaluated again. Furthermore, to make the simulation more accurate and convincing, we use a 128 x 128 resolution of flow field, instead of 64 x 64 as we did in the initial submission, for evaluation of the 2D airfoils design task. The new results are shown in the following Table 8. The results confirm that our CinDM still outperforms all the baselines significantly regarding lift/drag metric under comparable model size.\n\nFor the N-body inverse design task, Table 7 reveals that CinDM and U-Net have comparable model sizes, both larger than GNS. The small number of parameters in GNS is due to its utilization of a semi-implicit Euler integration for updating node features, leveraging existing physical prior knowledge. Consequently, GNS does not require a large number of parameters. GNS restricts its input features solely to the current time step, making it ineffective to employ a larger model.\n\nTable 6. Comparison of the number of parameters for different methods in 2D airfoils inverse design task.\n\n| model | #Parameters (million) |\n| --- | --- |\n| FNO | 7.39 |\n| LE-PDE | 1.83 |\n| CinDM | 3.11 |\n\nTable 7. Comparison of the number of parameters for different methods in N-body inverse design task.\n\n| model | #Parameters (million) (23-steps) | #Parameters (million) (1-step) |\n| --- | --- | --- |\n| GNS | 0.22 | 0.21 |\n| U-Net | 20.45 | 19.88 |\n| CinDM | 20.76 | 20.19 |\n\nTable 8. Comparison of CinDM with baselines under comparable model size for the 2D airfoils inverse design task.\n\n\n| Methods          | #Parameters(million) | obj(1 airfoil) | lift/drag(1 airfoil) | obj(2 airfoils) | lift/drag(2 airfoils) |\n|------------------|----------------------|:--------------:|:--------------------:|:--------------:|:--------------------:|\n| CEM, FNO         |         3.29         |     0.0932     |        1.4005        |     0.3890     |        1.0914        |\n| CEM, LE-PDE      |         3.13         |     0.0794     |        1.4340        |     0.1691     |        1.0568        |\n| Backprop, FNO    |         3.29         |     0.0281     |        1.3300        |     0.1837     |        0.9722        |\n| Backprop, LE-PDE |         3.13         |     0.1072     |        1.3203        |     0.0891     |        0.9866        |\n| CinDM            |         3.11         |     0.0797     |         2.177        |     0.1986     |        1.4216        |\n\nThe new results on comparison with baselines under the comparable number of parameters for the 2D airfoils inverse design task are presented in Table 3 of Section 4.3 in the updated manuscript."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1627/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625612523,
                "cdate": 1700625612523,
                "tmdate": 1700628835991,
                "mdate": 1700628835991,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7EQvGeV3mz",
                "forum": "wmX0CqFSd7",
                "replyto": "f25irp84mI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer obHR (3)"
                    },
                    "comment": {
                        "value": ">__Comment 4__: Did you try training a single shared network across the overlapping chunks? I was kind of expecting something like this (maybe with different degrees of subsampling to give long and short-range dynamics)\n\n**Answer**: Thanks for suggesting such an important baseline. To address this question, we train two diffusion models with 44 steps for the N-body task, aiming to perform inverse design directly without time composition. Their parameter sizes are roughly equivalent to CinDM and twice as large. \u00a0All other training configurations are kept identical to those used in CinDM. The results are presented in the following Table 9. It clearly shows the new baseline design results are not superior to those achieved by CinDM. Due to the long-range dependencies among different time steps, capturing the features of trajectories across 44 steps simultaneously using a single model is challenging. In this case, a 24-step diffusion model is more suitable. Therefore, for designs involving more time steps, employing time composition is proven to be a more effective approach, with lower cost and better performance.\n\nTable 9. Comparison to the baseline: directly diffuse 44 steps without time composition in an N-body inverse design task.\n\n| Methods                   | #Parameters(million) | design_obj(2-body 44 steps) | MAE(2-body 44 steps) | design_obj(4-body 44 steps) | MAE(4-body 44 steps) |\n|---------------------------|:--------------------:|-----------------------------|----------------------|-----------------------------|----------------------|\n| Our method                | 20.76M               | 0.1326 \u00b1 0.0087             | 0.00695 \u00b1 0.00067    | 0.2281 \u00b1 0.0145           | 0.03195 \u00b1 0.00705    |\n| Directly diffuse 44 steps | 44.92M               | 0.2779 \u00b1 0.0197             |  0.00810 \u00b1 0.00200   | 0.2986 \u00b1 0.0148            | 0.05166 \u00b1 0.01218    |\n\nThe evaluation results and analysis of this new baseline for N-body time composition inverse design task are presented in Appendix C in the updated manuscript."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1627/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625677367,
                "cdate": 1700625677367,
                "tmdate": 1700628814618,
                "mdate": 1700628814618,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ATN7J57ApM",
                "forum": "wmX0CqFSd7",
                "replyto": "f25irp84mI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1627/Reviewer_obHR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1627/Reviewer_obHR"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification. So to check my understanding, paraphrasing:\n\n0. You don't have a mixture-of-experts ($p(x;\\mathbf{\\Theta})=\\sum_i \\alpha_i p_i(x;\\theta_i)$ ), but  a product of experts ($p(x;\\mathbf{\\Theta})=\\prod_i p_i(x;\\theta_i)=\\exp\\sum_i \\log p_i(x;\\theta_i)$ ) over your compositions\n1. The hyperparameter $N$ is chosen as to segment the training trajectories, and each sub-model then learns its component dynamics energy function\n2. To use an intuitive analogy, one component might specialize on a specific band of of high frequencies, the other on a specific band of low frequencies, and if both are \"active\" then both components will be favoured in sampling\n3. By using more of these time components with overlaps, we can expect each expert to learn different dynamics, which then explains the extrapolation - even if a specific dynamic is not encountered, it's components might, in a different compositional\n4. In the best case, $N$ components will allow you to learn $2^N$ \"dynamics settings\", assuming each component non-redundant, is identified correctly during training and its expert submodel generalises to the new \"dynamics setting\" \n\nI.e., relevant works to look at would be http://proceedings.mlr.press/v119/cohen20b/cohen20b.pdf  and https://www.cs.toronto.edu/~hinton/absps/nccd.pdf (in particluar the latter to ground product of experts)  , maybe https://arxiv.org/abs/2310.09397 (which if applicable, would give your method favourable statistical theory grounding, since *in principle* very few observations should suffice)  and maybe https://link.springer.com/article/10.1007/s10898-023-01333-5  (since the generalisation you observe might be understood through the lense of heteroscedascity of the  time series?)\n\nIf I understood you correctly, I think creating the links to the previous literature (in particular \"product of experts\" seems like a useful keywords for readers) would be appropriate, and if done would result in me raising my score in combination with the new evaluations.\n\nHowever, could you please also perform a significance check (e.g. using the logic of https://www.jmlr.org/papers/volume7/demsar06a/demsar06a.pdf or another suitable statistical check) across multiple evaluations (e.g. using k-fold validation) and mark the statistically significant differences in your tables (e.g. with a star)? This would help judge the effect of the compositional modelling in the cases where things are close."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1627/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658303886,
                "cdate": 1700658303886,
                "tmdate": 1700658565628,
                "mdate": 1700658565628,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6b6179ngCI",
                "forum": "wmX0CqFSd7",
                "replyto": "f25irp84mI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Official comment by Reviewer obHR"
                    },
                    "comment": {
                        "value": "Thanks for the detailed clarification questions and query! Firstly, these are the answers to your clarification questions:\n\n0. You are correct that our method is similar to product of experts instead of mixture-of-experts. But our method differs from typical product of experts in one important aspect: the different experts in our method focus on different subspace $x_i$ of the input $x$ instead of sharing the same input space $x$, i.e., in our CinDM, suppose that we have in total $K$ experts, and we decompose the input space $x$ into $N$ components, then we have $p(x;\\Theta)=\\Pi_{i=1}^{N} p(x_i;\\theta_{i_k})=\\text{exp}\\ \\Sigma_{i=1}^N\\ \\text{log}\\ p(x_i;\\theta_{i_k})$, where $x_i\\in \\mathcal{X}_i, x\\in \\mathcal{X}$,  such that $\\cup_i \\mathcal{X}_i = \\mathcal{X}$. $i_k\\in\\{1,2,...K\\}$. In other words, in inference, the input space $\\mathcal{X}$ can be decomposed into many subspaces $\\mathcal{X}_i$ (potentially with overlapping), and each subspace is assigned one expert $i_k$ for learning the distribution within its subspace. In this paper, we provide three important examples of such decomposition (depicted in Fig. 1 in the paper):  (1) along the time dimension, in terms of overlapping time chunks, (2) on the state dimension, decomposing N-body interaction into $\\frac{N(N-1)}{2}$ 2-body interactions, or (3) on the boundary dimension, decomposing the interaction between fluid and a full shape into interaction of fluid and its many parts.\n\n1. You are essentially correct. One small clarification, the hyperparameter N is chosen as to segment (or decompose) the *inference* trajectories, and such decomposition can be in different aspects: (1) along time dimension, (2) along the state dimension, or (3) along the boundary dimension, as detailed in the answer to question 0 above.\n\n2. Yes, that is correct.\n\n3. Yes, you are essentially correct. The way our method can generalize is that in inference, each \"expert\" generates a distribution $p(x_i)$ in a subspace $\\mathcal{X}_i$ that is *in-distribution*, but as a whole, the combined distribution $p(x)$ is *out-of-distribution*, since different subspaces can compose in a myriad of ways.\n\n4. As stated in the answer to question 0, in our method, there is a distinction between the \"subspace\" where we decompose the input space, and the number of experts. If we have trained $K$ experts, we can generalize to much more than $2^K$ \"dynamical settings\", due to the additional flexibility that we can decompose the input $x$ into different subspaces and each assign a relevant expert, and the combination can much more than $2^K$. Take the decomposition of N-body interaction into 2-body interaction (the second example in the answer to question 0) as an example. Suppose that we have learned $K=3$ experts, each of which can account for one kind of interaction between 2 bodies. Now suppose that we have N bodies with pairwise interactions, so we have $N(N-1)/2$ interactions, each of which can choose one expert to model. Therefore, there are $3^{N(N-1)/2}$ combinations, which can be much more than $2^K=2^3=8$ dynamical settings.\n\nSecondly, we have added the four references that are related to the product of experts to the related work section, and have added the explanation of its difference to our method at the end of the method section. We have again updated the manuscript to reflect this change.\n\nThirdly, thanks for the suggestion of performing a significance check. Due to the fact that we have only one day left in the rebuttal, we do not have time to perform it, but we promise that we will do it in the camera-ready version of the paper.\n\nAgain, we really appreciate your detailed questions and suggestions!"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1627/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666410800,
                "cdate": 1700666410800,
                "tmdate": 1700666527555,
                "mdate": 1700666527555,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zusTeBLDgd",
                "forum": "wmX0CqFSd7",
                "replyto": "6b6179ngCI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1627/Reviewer_obHR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1627/Reviewer_obHR"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the additional elaboration. Conditional on this\n\n>Thirdly, thanks for the suggestion of performing a significance check. Due to the fact that we have only one day left in the rebuttal, we do not have time to perform it, but we promise that we will do it in the camera-ready version of the paper.\n\nI feel good about improving my score given the improved evaluation and contextualization."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1627/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685110477,
                "cdate": 1700685110477,
                "tmdate": 1700685110477,
                "mdate": 1700685110477,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XfgfF8ScQD",
            "forum": "wmX0CqFSd7",
            "replyto": "wmX0CqFSd7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1627/Reviewer_7GzX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1627/Reviewer_7GzX"
            ],
            "content": {
                "summary": {
                    "value": "This works investigates inverse desigs in dynamic systems. The authors look into inverse design while avoiding adversarial samples in order to improve efficiency. The authors proposed a new formulation for inverse design by energy optimization, and introduced the Compositional Inverse Design with Diffusion Models (CinDM), which is able to branch out and generate further designs than observed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The generative optimization structure containing both the energy-based model and the design objective is quite unique and novel. It enables the optimization problem for design to be more readily approached via the joint learning procedure.\n2. The experiments conducted in Section 4 are complete which explains well the questions raised at the beginning of the section.\nOverall, the ability shown in the work to generalize is quite impressive and seems promising with potential to be applied to more applications."
                },
                "weaknesses": {
                    "value": "1. This is more of a question. On the joint optimization, it is trying to minimize the energy component which is calculated from the trajectories and the boundary, and minimizing the design objective as well. It is proposed to achieve this by optimizing the design and the trajectory at the same time. In the joint optimization formulation as in Eqn.(3), the design objective function is weighted by $\\lambda$. I am curious how this hyperparameter is estimated/configured, and how sensitive the optimization results are to the change in $\\lambda$."
                },
                "questions": {
                    "value": "Additionally, I wonder whether changing its value will lead to different results in the experiments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1627/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1627/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1627/Reviewer_7GzX"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1627/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821339646,
            "cdate": 1698821339646,
            "tmdate": 1699636091355,
            "mdate": 1699636091355,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zbFcTsLaKO",
                "forum": "wmX0CqFSd7",
                "replyto": "XfgfF8ScQD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer 7GzX"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback. We appreciate that the reviewer thinks our work is novel in method, complete in experiments, and promising in more applications. Below, we address the reviewer\u2019s concern on the hyperparameter $\\lambda$.\n\n>__Comment 1__:\u00a0 I am curious how this hyperparameter (lambda) is estimated/configured, and how sensitive the optimization results are to the change in (lambda).\n\n**Answer**: This is a very important question for the evaluation performance of our method. It is also concerned by Reviewer 4Mfs and Reviewer WWyF. We have performed additional experiments to test the influence of $\\lambda$ for both N-body and 2D airfoils inverse design. The results are listed in the following Table 4 and Table 5. From these tables, we can see that our method is robust and has steady performance in a wide range of $\\lambda$. If $\\lambda$ is set too small ($\\leq 0.0001$ in the 2D airfoils task, or $< 0.01$ in the N-body task), then the design results are inferior because only little objective guidance is added. If $\\lambda$ is set too large ($> 0.01$ in the 2D airfoils task, or $> 1.0$ in the N-body task), then it is prone to enter a poor likelihood region, and physical consistency is not well preserved. In practice, $\\lambda$ could be set as 0.01 to 1.0 for the N-body task and 0.0002 to 0.02 for the 2D airfoils task. In our paper, we choose $\\lambda$ based on the best evaluation performance, namely we set $\\lambda$ as 0.4 for the N-body task and 0.0002 for the 2D airfoils task.\n\nThe evaluation results and analysis of $\\lambda$ are presented in Appendix I.1 in the updated manuscript.\n\nTable 4. Effect of $\\lambda$ in 2 airfoils inverse design.\n\n| $\\lambda$ | obj | lift/drag |\n| --- | --- | --- |\n| 0.05 | 0.7628\u00b10.1892 | 1.0150\u00b10.2008 |\n| 0.02 | 0.3849\u00b10.0632 | 1.0794\u00b10.1165 |\n| 0.01 | 0.2292\u00b10.0408 | 1.2860\u00b10.1402 |\n| 0.005 | 0.2061\u00b10.0388 | 1.2378\u00b10.1414 |\n| 0.002 | 0.2170\u00b10.0427 | 1.2429\u00b10.1243 |\n| 0.001 | 0.2277\u00b10.0451 | 1.2608\u00b10.1469 |\n| 0.0005 | 0.2465\u00b10.0473 | 1.4102\u00b10.1771 |\n| **0.0002** | **0.1986\u00b10.0431** | **1.4216\u00b10.1607** |\n| 0.0001 | 0.2710\u00b10.0577 | 1.1962\u00b10.1284 |\n\nTable 5. Effect of $\\lambda$ in N-body time composition inverse design.\n\n| $\\lambda$ | design_obj(2-body 24 steps) | MAE(2-body 24 steps) | design_obj(2-body 34 steps) | MAE(2-body 34 steps) | design_obj(2-body 44 steps) | MAE(2-body 44 steps) | design_obj(2-body 54 steps) | MAE(2-body 54 steps) |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0.0001 | 0.3032 \u00b1 0.0243 | 0.00269 \u00b1 0.00047 | 0.2954 \u00b1 0.0212 | 0.00413 \u00b1 0.00155 | 0.3091 \u00b1 0.0223 | 0.00394 \u00b1 0.00076 | 0.2996 \u00b1 0.0201 | 0.01046 \u00b1 0.00859 |\n| 0.001 | 0.2531 \u00b1 0.0185 | 0.00385 \u00b1 0.00183 | 0.2937 \u00b1 0.0213 | 0.00336 \u00b1 0.00115 | 0.2797 \u00b1 0.0190 | 0.00412 \u00b1 0.00105 | 0.2927 \u00b1 0.0219 | 0.00521 \u00b1 0.00103 |\n| 0.01 | 0.1200 \u00b1 0.0069 | 0.00483 \u00b1 0.00096 | 0.1535 \u00b1 0.0135 | 0.00435 \u00b1 0.00100 | 0.1624 \u00b1 0.0137 | 0.00416 \u00b1 0.00059 | 0.1734 \u00b1 0.0154 | 0.00658 \u00b1 0.00267 |\n| 0.1 | 0.1201 \u00b1 0.0046 | 0.01173 \u00b1 0.00150 | 0.1340 \u00b1 0.0107 | 0.00772 \u00b1 0.00099 | 0.1379 \u00b1 0.0088 | 0.00816 \u00b1 0.00149 | 0.1662 \u00b1 0.0180 | 0.01141 \u00b1 0.00473 |\n| 0.2 | 0.1283 \u00b1 0.0141 | 0.01313 \u00b1 0.00312 | 0.1392 \u00b1 0.0119 | 0.00836 \u00b1 0.00216 | 0.1529 \u00b1 0.0130 | 0.01019 \u00b1 0.00584 | 0.1513 \u00b1 0.0131 | 0.00801 \u00b1 0.00172 |\n| **0.4** | **0.1143 \u00b1 0.0047** | **0.01202 \u00b1 0.00114** | **0.1251 \u00b1 0.0071** | **0.00763 \u00b1 0.00069** | **0.1326 \u00b1 0.0087** | **0.00695 \u00b1 0.00067** | **0.1533 \u00b1 0.0140** | **0.00870 \u00b1 0.00150** |\n| 0.6 | 0.1259 \u00b1 0.0100 | 0.01382 \u00b1 0.00115 | 0.1326 \u00b1 0.0126 | 0.01171 \u00b1 0.00595 | 0.1592 \u00b1 0.0151 | 0.01140 \u00b1 0.00355 | 0.1670 \u00b1 0.0177 | 0.00991 \u00b1 0.00287 |\n| 0.8 | 0.1217 \u00b1 0.0073 | 0.01596 \u00b1 0.00127 | 0.1385 \u00b1 0.0120 | 0.01095 \u00b1 0.00337 | 0.1573 \u00b1 0.0116 | 0.00893 \u00b1 0.00113 | 0.1715 \u00b1 0.0181 | 0.01026 \u00b1 0.00239 |\n| 1 | 0.1330 \u00b1 0.0063 | 0.01679 \u00b1 0.00139 | 0.1428 \u00b1 0.0112 | 0.01087 \u00b1 0.00149 | 0.1634 \u00b1 0.0119 | 0.00968 \u00b1 0.00079 | 0.1789 \u00b1 0.0164 | 0.01102 \u00b1 0.00185 |\n| 2 | 0.1513 \u00b1 0.0079 | 0.02654 \u00b1 0.00160 | 0.1795 \u00b1 0.0129 | 0.01765 \u00b1 0.00193 | 0.1779 \u00b1 0.0121 | 0.01707 \u00b1 0.00474 | 0.2113 \u00b1 0.0161 | 0.01447 \u00b1 0.00130 |\n| 10 | 0.2821 \u00b1 0.0197 | 0.21153 \u00b1 0.01037 | 0.2210 \u00b1 0.0149 | 0.09715 \u00b1 0.00236 | 0.2273 \u00b1 0.0133 | 0.07781 \u00b1 0.00232 | 0.2269 \u00b1 0.0175 | 0.06538 \u00b1 0.00210 |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1627/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625442657,
                "cdate": 1700625442657,
                "tmdate": 1700628534632,
                "mdate": 1700628534632,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zc1Ke535MC",
            "forum": "wmX0CqFSd7",
            "replyto": "wmX0CqFSd7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1627/Reviewer_4Mfs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1627/Reviewer_4Mfs"
            ],
            "content": {
                "summary": {
                    "value": "The authors address the complex task of inverse design with what I believe is a rather novel approach. \nA first line of work optimizes over the forward process using an optimization procedure (CEM, gradient based optimization), this suffers from falling into adversarial local optima and potentially poor likelihood of the generated solution.\nTo fight such a behavior, the others propose to optimize over a linear combination of an EBM, accounting for the generation of likely condition, and the design objective.\nIn addition, the authors propose to estimate the EBM in a compositional fashion to simplify learning. \nThe proposed framework is tested through two main sets of experiments: N-body problem and airfold optimization"
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors approach is very interesting.\nThe paper is straightforward and aims at directly addressing the problem it uses. \nIt is clear and fairly well-written. The experiments provided by the authors seem to confirm the validity of the proposed method."
                },
                "weaknesses": {
                    "value": "I personally found the experiments slightly harder to read compared to the rest of the paper. For other remarks see questions."
                },
                "questions": {
                    "value": "1. Can the authors describe the role of $\\alpha$ line 12, Alg.1 ?\n2. Can the authors comment on the choice of the energy function for the airfold design ? How do we compare to training data ?\n3. Can the authors comment on how to balance $\\lambda$ during the optimization ? Could the optimization end up in a poor likelihood region ?\n3Bis. Can other forward / optimization steps be considered for such a task ?\n4. What is the influence of the number of optimization steps  ?\n5. For the airfold design: what is the relationship between the initial objective function and the reported ratio  ? Which quantity is actually at stake here ? \n6. Can the authors think of any limitation when applying a compositional energy approach ? For instance is it computationally efficient to learn \u201csmaller models\u201d vs one big EBM ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1627/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699262618056,
            "cdate": 1699262618056,
            "tmdate": 1699636091284,
            "mdate": 1699636091284,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mfOzAyTRM1",
                "forum": "wmX0CqFSd7",
                "replyto": "zc1Ke535MC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer 4Mfs  (1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive and detailed feedback. We are glad that the reviewer recognizes the significance, clarity, and experimental strengths of our work. Below, we address the reviewer\u2019s questions one by one.\n\n>__Comment 1__: Can the authors describe the role of $\\alpha$ in line 12, Alg.1?\n\n**Answer**: This is a typo. $\\alpha$ should be removed.\n\nIt is fixed in the updated version.\n\n>__Comment 2__: Can the authors comment on the choice of the energy function for the airfoil design? How do we compare to training data ?\n\n**Answer**: The energy function is used to model the joint distribution of trajectory and boundary data in our physical evolution. In airfoil design, it is very hard to explicitly compute this energy function due to high dimensional spatial-temporal dependency and complex gas-solid coupling. So we take an implicit way to characterize the energy function: we use a stochastic process to learn the gradient of the energy function from observed data and take an opposite direction to reach the low energy region during inference. By introducing a design objective term, we can generate samples that achieve better objective values than training data while maintaining physical consistency as training data. Furthermore, this implicit energy function makes it easy for compositional design, which could generate samples that are substantially different from training data, like the compositional design of formation flying of 2 airfoils. We think it is really challenging to think of other choices of energy function for airfoil design without considering stochastic processes, especially when one wants compositional airfoil design."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1627/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625203857,
                "cdate": 1700625203857,
                "tmdate": 1700628602742,
                "mdate": 1700628602742,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B7Z7o4dTlv",
                "forum": "wmX0CqFSd7",
                "replyto": "zc1Ke535MC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer 4Mfs (2)"
                    },
                    "comment": {
                        "value": ">__Comment 3__: Can the authors comment on how to balance $\\lambda$ during the optimization? Could the optimization end up in a poor likelihood region ? 3Bis. Can other forward / optimization steps be considered for such a task?\n\n**Answer**: We have performed additional experiments to test the influence of $\\lambda$ for both N-body and 2D airfoils inverse design. The results are listed in the following Table 1 and Table 2. From these tables, we can see that our method is robust and has steady performance in a wide range of $\\lambda$. If $\\lambda$ is set too small ($\\le 0.0001$ in the 2D airfoils task, or $<0.01$ in the N-body task), then the design results are inferior because only little objective guidance is added. If $\\lambda$ is set too large ($>0.01$ in the 2D airfoils task, or $>1.0$ in the N-body task), then it is prone to fall into a poor likelihood region, and physical consistency is not well preserved. In practice, $\\lambda$ could be set as 0.01 to 1.0 for the N-body task and 0.0002 to 0.02 for the 2D airfoils task. In our paper, we choose $\\lambda$ based on the best evaluation performance, namely we set $\\lambda$ as 0.4 for the N-body task and 0.0002 for the 2D airfoils task.\n\nFor a forward/optimization strategy, there are some other choices. In forward, our network predicts the noise of each step, perhaps we can instead predict $z_0$ in Eq. (6) of Section 3.2, which contains clean boundary and state variables, in each forward step. This approach has been shown to be effective in some generation tasks. In inference, we can use optimization techniques like DDIM [1] to speed up the procedure of generation. For details about the evaluation of DDIM on our method, please refer to our response to the next question.\n\nThe evaluation results and analysis of $\\lambda$ are presented in Appendix I.1 in the updated manuscript.\n\n[1] Song, J., Meng, C., & Ermon, S. (2020). Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502.\n\nTable 1. Effect of $\\lambda$ in 2 airfoils inverse design.\n\n| $\\lambda$ | obj | lift/drag |\n| --- | --- | --- |\n| 0.05 | 0.7628\u00b10.1892 | 1.0150\u00b10.2008 |\n| 0.02 | 0.3849\u00b10.0632 | 1.0794\u00b10.1165 |\n| 0.01 | 0.2292\u00b10.0408 | 1.2860\u00b10.1402 |\n| 0.005 | 0.2061\u00b10.0388 | 1.2378\u00b10.1414 |\n| 0.002 | 0.2170\u00b10.0427 | 1.2429\u00b10.1243 |\n| 0.001 | 0.2277\u00b10.0451 | 1.2608\u00b10.1469 |\n| 0.0005 | 0.2465\u00b10.0473 | 1.4102\u00b10.1771 |\n| **0.0002** | **0.1986\u00b10.0431** | **1.4216\u00b10.1607** |\n| 0.0001 | 0.2710\u00b10.0577 | 1.1962\u00b10.1284 |\n\nTable 2. Effect of $\\lambda$ in N-body time composition inverse design.\n\n| $\\lambda$ | design_obj(2-body 24 steps) | MAE(2-body 24 steps) | design_obj(2-body 34 steps) | MAE(2-body 34 steps) | design_obj(2-body 44 steps) | MAE(2-body 44 steps) | design_obj(2-body 54 steps) | MAE(2-body 54 steps) |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0.0001 | 0.3032 \u00b1 0.0243 | 0.00269 \u00b1 0.00047 | 0.2954 \u00b1 0.0212 | 0.00413 \u00b1 0.00155 | 0.3091 \u00b1 0.0223 | 0.00394 \u00b1 0.00076 | 0.2996 \u00b1 0.0201 | 0.01046 \u00b1 0.00859 |\n| 0.001 | 0.2531 \u00b1 0.0185 | 0.00385 \u00b1 0.00183 | 0.2937 \u00b1 0.0213 | 0.00336 \u00b1 0.00115 | 0.2797 \u00b1 0.0190 | 0.00412 \u00b1 0.00105 | 0.2927 \u00b1 0.0219 | 0.00521 \u00b1 0.00103 |\n| 0.01 | 0.1200 \u00b1 0.0069 | 0.00483 \u00b1 0.00096 | 0.1535 \u00b1 0.0135 | 0.00435 \u00b1 0.00100 | 0.1624 \u00b1 0.0137 | 0.00416 \u00b1 0.00059 | 0.1734 \u00b1 0.0154 | 0.00658 \u00b1 0.00267 |\n| 0.1 | 0.1201 \u00b1 0.0046 | 0.01173 \u00b1 0.00150 | 0.1340 \u00b1 0.0107 | 0.00772 \u00b1 0.00099 | 0.1379 \u00b1 0.0088 | 0.00816 \u00b1 0.00149 | 0.1662 \u00b1 0.0180 | 0.01141 \u00b1 0.00473 |\n| 0.2 | 0.1283 \u00b1 0.0141 | 0.01313 \u00b1 0.00312 | 0.1392 \u00b1 0.0119 | 0.00836 \u00b1 0.00216 | 0.1529 \u00b1 0.0130 | 0.01019 \u00b1 0.00584 | 0.1513 \u00b1 0.0131 | 0.00801 \u00b1 0.00172 |\n| **0.4** | **0.1143 \u00b1 0.0047** | **0.01202 \u00b1 0.00114** | **0.1251 \u00b1 0.0071** | **0.00763 \u00b1 0.00069** | **0.1326 \u00b1 0.0087** | **0.00695 \u00b1 0.00067** | **0.1533 \u00b1 0.0140** | **0.00870 \u00b1 0.00150** |\n| 0.6 | 0.1259 \u00b1 0.0100 | 0.01382 \u00b1 0.00115 | 0.1326 \u00b1 0.0126 | 0.01171 \u00b1 0.00595 | 0.1592 \u00b1 0.0151 | 0.01140 \u00b1 0.00355 | 0.1670 \u00b1 0.0177 | 0.00991 \u00b1 0.00287 |\n| 0.8 | 0.1217 \u00b1 0.0073 | 0.01596 \u00b1 0.00127 | 0.1385 \u00b1 0.0120 | 0.01095 \u00b1 0.00337 | 0.1573 \u00b1 0.0116 | 0.00893 \u00b1 0.00113 | 0.1715 \u00b1 0.0181 | 0.01026 \u00b1 0.00239 |\n| 1 | 0.1330 \u00b1 0.0063 | 0.01679 \u00b1 0.00139 | 0.1428 \u00b1 0.0112 | 0.01087 \u00b1 0.00149 | 0.1634 \u00b1 0.0119 | 0.00968 \u00b1 0.00079 | 0.1789 \u00b1 0.0164 | 0.01102 \u00b1 0.00185 |\n| 2 | 0.1513 \u00b1 0.0079 | 0.02654 \u00b1 0.00160 | 0.1795 \u00b1 0.0129 | 0.01765 \u00b1 0.00193 | 0.1779 \u00b1 0.0121 | 0.01707 \u00b1 0.00474 | 0.2113 \u00b1 0.0161 | 0.01447 \u00b1 0.00130 |\n| 10 | 0.2821 \u00b1 0.0197 | 0.21153 \u00b1 0.01037 | 0.2210 \u00b1 0.0149 | 0.09715 \u00b1 0.00236 | 0.2273 \u00b1 0.0133 | 0.07781 \u00b1 0.00232 | 0.2269 \u00b1 0.0175 | 0.06538 \u00b1 0.00210 |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1627/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625298600,
                "cdate": 1700625298600,
                "tmdate": 1700628639720,
                "mdate": 1700628639720,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AUmJdZY3yo",
                "forum": "wmX0CqFSd7",
                "replyto": "zc1Ke535MC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1627/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer 4Mfs (3)"
                    },
                    "comment": {
                        "value": ">__Comment 4__: What is the influence of the number of optimization steps ?\n\n**Answer**: We use DDIM sampling methods to evaluate the effect of different optimization steps on the design performance of the N-body task of our methods. As the results shown in Table 3, which are visualized in Fig 16 and Fig 17 of Appendix I.3, it is evident that an augmentation in the number of sampling time steps corresponds to a gradual reduction in the design objective. Conversely, MAE exhibits fluctuations within a narrow range, occasionally experiencing increments.\n\nThis observed phenomenon can be rationalized as follows: With an increase in the number of sampling steps, the design objective becomes intricately involved in the diffusion process. Consequently, the designs improve and closely align with the design objective, resulting in a decrease in the design objective value. However, as the number of sampling steps increases, the MAE also rises. This is attributed to a concentration of trajectories within a narrow range when there are a small number of sampling steps, causing some designed samples to have very low initial velocities. As a result, both the true trajectory and the diffused trajectory become highly concentrated, leading to a small calculated MAE value. Through a meticulous analysis of the sensitivity of the design objective and MAE to different sampling steps, we can confidently assert that CinDM achieves desired design outcomes aligned with the design objectives and physical constraints. This is accomplished by judiciously selecting an appropriate sampling step size during the inverse design process.\n\nThe evaluation results and analysis of the number of optimization steps in inference are presented in Appendix I.3 in the updated manuscript.\n\nTable 3 The performance of different optimization (sample) steps in N-body time composition inverse design of CinDM.\n\n| Sample steps | design obj | MAE |\n| --- | --- | --- |\n| 50 | 0.2339 | 0.00429 |\n| 100 | 0.1778 | 0.00338 |\n| 150 | 0.1767 | 0.00408 |\n| 200 | 0.1510 | 0.00465 |\n| 250 | 0.1468 | 0.00648 |\n| 300 | 0.1359 | 0.00676 |\n| 350 | 0.1321 | 0.00766 |\n| 400 | 0.1235 | 0.00710 |\n| 450 | 0.1329 | 0.00862 |\n| 500 | 0.1192 | 0.00738 |\n| 550 | 0.1185 | 0.01060 |\n| 600 | 0.1250 | 0.01195 |\n| 650 | 0.1273 | 0.01323 |\n| 700 | 0.1159 | 0.01041 |\n| 750 | 0.1176 | 0.01117 |\n| 800 | 0.1217 | 0.01221 |\n| 850 | 0.1193 | 0.01043 |\n| 900 | 0.1179 | 0.01236 |\n| 950 | 0.1189 | 0.01350 |\n| 1000 | 0.1143 | 0.01224 |\n\n>__Comment 5__: For the airfoil design: what is the relationship between the initial objective function and the reported ratio? Which quantity is actually at stake here?\n\n**Answer**: Lift-to-drag ratio (https://www.sciencedirect.com/topics/engineering/lift-to-drag-ratio) is a standard evaluation metric in aerodynamics. We adopt the objective function, instead of lift/drag, as guidance during inference because it performs more steadily than lift/drag during optimization. Therefore, lift-to-drag ratio is a more practical metric and more attention should be paid when evaluating design results. We also show the objective as a metric because it is an indicator of the optimization performance.\n\n> __Comment 6__: Can the authors think of any limitations when applying a compositional energy approach? For instance is it computationally efficient to learn \u201csmaller models\u201d vs one big EBM?\n\n**Answer**: It is indeed more computationally efficient to learn individual components since they are simpler functions. However, the composition may not fully accurately capture the full distribution as it assumes independence in different components. The advantage of the compositional energy approach is that it provides an implicit way to model the energy landscape of global physical states, thus producing design results closer to physical reality on compositional tasks."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1627/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625332841,
                "cdate": 1700625332841,
                "tmdate": 1700628743728,
                "mdate": 1700628743728,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]