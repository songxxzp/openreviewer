[
    {
        "title": "Posterior Sampling via Langevin Monte Carlo for Offline Reinforcement Learning"
    },
    {
        "review": {
            "id": "d1RZWRjC59",
            "forum": "WwCirclMvl",
            "replyto": "WwCirclMvl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8841/Reviewer_VjxR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8841/Reviewer_VjxR"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a model-free posterior sampling approach for offline RL using Langevin Monte Carlo (LMC) for posterior approximation. They introduce practical algorithms in an episodic setting for both linear low-rank MDPs, and general MDPs (with over-parameterized neural networks for value function approximation, alongside an auxiliary linear model for LMC). Notably, the paper establishes frequentist sub-optimal bounds both cases. Empirical evaluations on linear MDP and non-linear contextual bandits support the proposed algorithms' effectiveness."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I believe the most important strength is that the paper offers an insightful advancement in offline RL through a Bayesian lens. While the value-based variation to classical PSRL and the employment of LMC for posterior approximation are not novelties in isolation, their integration within offline RL is both meaningful and aptly executed. \n\nThe implicit pessimism by posterior sampling with the proof of a frequentist bound is also a non-trivial contribution, and provides a fresh perspective to ongoing discussions in this domain."
                },
                "weaknesses": {
                    "value": "While the paper makes significant theoretical advancements, it would further solidify its applicability if the proposed algorithms were tested on well-regarded benchmarks, such as the MuJoCo tasks from the D4RL suite. Additional experiments with model-based approaches would offer a comprehensive perspective on the approach's effectiveness.\n\nThe presented approach captures pessimism through posterior sampling. While innovative, one could question whether this form of pessimism adequately represents the complex nature of uncertainties found in the offline dataset, particularly given the non-stationary distributions that can arise from varied data collection policies."
                },
                "questions": {
                    "value": "Please refer to the concerns in the weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8841/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698588258126,
            "cdate": 1698588258126,
            "tmdate": 1699637112078,
            "mdate": 1699637112078,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7rXShb2krM",
                "forum": "WwCirclMvl",
                "replyto": "d1RZWRjC59",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8841/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8841/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for acknowledging our contributions and for the insightful questions. \n\n> The presented approach captures pessimism through posterior sampling. While innovative, one could question whether this form of pessimism adequately represents the complex nature of uncertainties found in the offline dataset, particularly given the non-stationary distributions that can arise from varied data collection policies. \n\nThis is indeed a very insightful question. The short answer is we do not know yet for sure. We speculate that this form of pessimism in our algorithms is likely not tightly capturing the uncertainties in the offline dataset as there is still a gap of $\\sqrt{d}$ of our bounds for linear MDPs in the worst-case scenarios (please see the \"Interpolating Bounds\" paragraph on page 7). Closing this gap of approximate posterior sampling (perhaps with better algorithms that can tightly capture sufficient pessimism for offline RL, per your suggestion) is left as a future direction."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8841/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503293226,
                "cdate": 1700503293226,
                "tmdate": 1700503293226,
                "mdate": 1700503293226,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "r5cryazQhd",
            "forum": "WwCirclMvl",
            "replyto": "WwCirclMvl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8841/Reviewer_jpdm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8841/Reviewer_jpdm"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores convergence of posterior sampling via Langevin Monte Carlo for offline RL."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The study sounds solid, although I did not go through each step of the proof."
                },
                "weaknesses": {
                    "value": "The paper did not clearly explain the fundamental difference between the convergence of Langevin Monte Carlo and the convergence of the RL posterior sampling under the offline setting."
                },
                "questions": {
                    "value": "1. What is the difference between the convergence of Langevin Monte Carlo and the convergence of the RL posterior sampling under the offline setting? Will the former lead to the latter? \n\n2. Why is LMC, instead of SGLD, used in Algorithms 2 and 3?  Can mini-batch data be used in simulations of the proposed algorithm?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8841/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698629057871,
            "cdate": 1698629057871,
            "tmdate": 1699637111902,
            "mdate": 1699637111902,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oxTGdq5nWx",
                "forum": "WwCirclMvl",
                "replyto": "r5cryazQhd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8841/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8841/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the positive reviews. \n\n---\n> What is the difference between the convergence of Langevin Monte Carlo and the convergence of the RL posterior sampling under the offline setting? Will the former lead to the latter? \n\nTo clarify the context, we assume that you are questioning the relationship between the Langevin Monte Carlo (LMC-PPS) way in Algorithm 2 and the pessimistic posterior sampling way in Algorithm 1. Note standard posterior sampling in [US21] does not have any frequentist guarantee. Please let us know if you are implying other stuff.  We are happy to elaborate on that more.\n\nResponding to the relation between the two algorithms, we discussed this at length in the second paragraph of Section 6. For the reviewer\u2019s convenience, we summarize it here. In our settings, LMC-PPS and exact posterior sampling (PPS with exact posterior samples, not with approximate samples via LMC) achieve the same guarantees. This shows a promising benefit of LMC-PPS, where we can employ first-order sampling methods (e.g., LMC) without compromising the statistical guarantees. This is immensely meaningful as, in many cases, exact posterior sampling is much more expensive to obtain (in our settings, as formally discussed in Section 6) or even intractable, while LMC is simply a noisy gradient-based method that can efficiently apply to any differentiable model. \n\n---\n\n> Why is LMC, instead of SGLD, used in Algorithms 2 and 3? Can mini-batch data be used in simulations of the proposed algorithm? \n\nWe can replace LMC with SGLD, where the use of stochastic gradients introduces additional complexities that are typically managed by the standard tools in LMC [1]. In essence, we only need to account for the additional source of stochasticity stemming from the random batch selection in the analysis. However, it's important to note that this point is ancillary to our main argument. Consequently, we introduce LMC for the sake of clarity in exposition.\n\nIn our simulations, we did, in fact, utilize mini-batch data (SGLD). Moreover, we discuss in Section D.2 in our appendix for a thorough description of the experimental setup and training particulars. We will enhance the elucidation of the relationship between LMC and SGLD in the final version.\n\nReference: \n\n[1] AS Dalalyan, A Karagulyan. User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient. Stochastic Processes and their Applications, 2019"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8841/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502757384,
                "cdate": 1700502757384,
                "tmdate": 1700515719471,
                "mdate": 1700515719471,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BWcL0qphf7",
                "forum": "WwCirclMvl",
                "replyto": "r5cryazQhd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8841/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8841/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up comments"
                    },
                    "comment": {
                        "value": "Dear reviewer jpdm, \n\nAs the author-reviewer discussion period will end soon, we will appreciate it if you could check our response to your review comments. We are confident that we have addressed your concern in the rebuttal. We haven't hear from you, could you let us know if your concerns have been addressed? If our response resolves your concerns, we kindly ask you to consider raising the rating of our work. If not, we are very happy to provide further explanations. Thank you very much for your time and efforts!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8841/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681465659,
                "cdate": 1700681465659,
                "tmdate": 1700681731940,
                "mdate": 1700681731940,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u4MZfo08d5",
            "forum": "WwCirclMvl",
            "replyto": "WwCirclMvl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8841/Reviewer_Cabd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8841/Reviewer_Cabd"
            ],
            "content": {
                "summary": {
                    "value": "The submission studies a Bayesian method for offline RL. The proposed method is quite simple (which I see as a pro), simply do the noisy gradient descent on the regression objective. Analysis with improved bounds is the main contribution of the paper."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method is simple and seems implementable in practice. \n\n- The bounds are improved from previous work. \n\n- Analysis of the NTK regime is performed, which might be of independent interest."
                },
                "weaknesses": {
                    "value": "- I do not see particularly new ideas from the submission, either in the algorithm or in the analysis. Thus, the novelty of the paper is limited. \n\n- The work in Uehara and Sun [US21] considers the setting where the representation of state-action $\\phi(s,a)$ is unknown, whereas the submission assumes the feature representation function is known. I think it is not fair to claim the improvement from [US21].\n\nIn general, even though this is a technical paper, the submission is a bit hard to parse. \n\n- How is the regression objective related to posterior sampling? How is this a \"Langevin\" Monte Carlo method? It would be good to be introductory to Langevin Monte Carlo methods, and how the concepts are attached to the actual algorithm presented. \n\n- I had to understand the linear (or low-rank) MDP part very clearly before paying attention to the NTK function approximation part. I do not see any particular contribution from the NTK part to reinforcement learning theory. It is a good add-on result though."
                },
                "questions": {
                    "value": "- I wonder how this method performs on some offline deep-RL benchmarks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8841/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817616617,
            "cdate": 1698817616617,
            "tmdate": 1699637111777,
            "mdate": 1699637111777,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zKhRNDp3Lj",
                "forum": "WwCirclMvl",
                "replyto": "u4MZfo08d5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8841/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8841/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the positive reviews and the constructive feedback. We will revise our paper accordingly based on your suggestions. \n\n---\n> I do not see particularly new ideas from the submission, either in the algorithm or in the analysis. Thus, the novelty of the paper is limited.\n\nFor algorithms, Lin-LMC-PPS and Neural-LMC-PPS in our paper are both novel for offline RL. Especially, the use of linear auxiliary models in our Neural-LMC-PPS is non-standard. For analysis, we did not claim to invent any new technical tools and we built upon some of the existing techniques used for linear MDP [JYW21] and Neural MDP [NTA23]  to analyze our problem settings. However, the fact that our algorithms are completely different from their algorithms by nature and that our bounds improve upon the bounds of [JYW21] and [NTA23] in the respective MDP models shows that we cannot use their techniques ``as is'' and that we need a new analysis treatment.  The key technical challenge that is absent from both [JYW21] and [NTA23] is to show whether we can obtain pessimism only from **approximate** posterior samples. To address this challenge (and obtain such improvement consequently), the key idea is to separate the distributional shift problem from the estimation problem (Lemma A.3 in our appendix) and leverage the (near-)linear structures of linear MDP and neural MDPs to control the noises propagated from **approximate** posterior samples. \n\n---\n> The work in Uehara and Sun [US21] considers the setting where the representation of state-action is unknown, whereas the submission assumes the feature representation function is known. I think it is not fair to claim the improvement from [US21].\n\nWe apologize for the confusion.  We compared only with Section 8 of [US21], which studies the model-based Posterior Sampling for offline RL where there is no representation learning (the rest of [US21] is not related to posterior sampling). We will revise our paper accordingly. \n\n---\n> How is the regression objective related to posterior sampling? How is this a \"Langevin\" Monte Carlo method?\n\nThe regression objective is the negative log-likelihood function of the posterior (see footnote 7 in our paper). If one simply samples from $\\theta\\sim \\exp(-L_h(\\theta))$, then it corresponds to the standard posterior sampling (with uninformative prior). However, direct sampling from the general $\\exp(-L_h(\\theta))$ could be intractable, so the alternative method would be adding noise to the gradient decent step of the negative log-likelihood function, which is essentially Langevin Monte Carlo (also check eqn (4) of [WT11]).\n\n---\n>  I do not see any particular contribution from the NTK part to reinforcement learning theory. It is a good add-on result though. \n\nWe study the statistical benefits of Posterior Sampling via Langevin Monte Carlo for offline RL in neural MDPs using NTK. Linear MDPs and neural MDPs (with NTK) are two different models that cover two different aspects of scenarios we might encounter. The guarantees in linear MDPs do not imply any guarantees in neural MDPs (in fact, roughly speaking, the bound for neural MDPs is worse than that for linear MDP by a factor of $\\sqrt{H}$); thus, considering both settings gives two orthogonal views of the landscape of  Posterior Sampling via Langevin Monte Carlo for offline RL.\n\n---\n> I wonder how this method performs on some offline deep-RL benchmarks.\n\nWhile the current scope of our paper focuses only on the theoretical understanding of the offline LMC algorithm, the empirical performances of our algorithm in large-scale scenarios remain an interesting and non-trivial question that we leave as future work."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8841/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502201752,
                "cdate": 1700502201752,
                "tmdate": 1700503568201,
                "mdate": 1700503568201,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]