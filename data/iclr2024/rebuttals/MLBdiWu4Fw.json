[
    {
        "title": "InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation"
    },
    {
        "review": {
            "id": "H1HxdqN6Qp",
            "forum": "MLBdiWu4Fw",
            "replyto": "MLBdiWu4Fw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4917/Reviewer_1UqW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4917/Reviewer_1UqW"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents InternVid, an large-scaled video-language dataset, consisting of a collection of 7M videos, 234M video clips and corresponding textual descriptions. They consider diversity and data quality by sourcing YouTube contents from various countries and languages. This new dataset enables pretraining a video-language model, named ViCLIP, exhibiting state-of-the-art scores in action recognition and video retrieval tasks. It also demonstrates a credible generative capability in text-to-video generation, supported by qualitative and quantitative assessment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Clarity in writing\n\n- The paper is well-written and easy to understand.\n\n2. Scale and impact\n\n- InternVid includes large-scaled 7.1M videos, making it a significant contribution to the video-language pretraining. Referring to table 1, InternVID comprises 234M 720p video clips with LLM generated captions which shows outperforming accuracy in description. The dataset\u2019s substantial size can potentially have a profound impact on the field.\n\n3. Diversity and Quality\n\n- The effort to collect videos from various countries and languages is commendable. This diversity in data collection may address the issue of bias, a crucial aspect of large-scale datasets, as it mitigates the risk of training biased models.\n\n4. High performance\n\n- The suggested model, referred as ViCLIP, achieve the state-of-the-art in video-language benchmarks such as  action recognition and text-to-video retrieval.\n- InternVID improves text-to-video generation by empowering the previous models with its large scale video-text dataset. It shows quantitative and qualitative (Figure 12) improvement in generation quality."
                },
                "weaknesses": {
                    "value": "1. Lack of definitions, explanations\n\n- Definition of UMT-SIM is absent. In table 2, justification of the design choice of InternVid-10M-FLT is not explained. (At glance, the result of performance improvement after filtering the dataset with a high UMT-SIM score seems trivial.)\n- Figure 5 introduced 3 schemes of interleaving video clips, text, and ASR text. However, comparison between the methods or further analysis is not given.\n- It is doubtful that CLIP is a suitable choice for comparing their method in action recognition tasks. There are likely more appropriate video models to use in the benchmark for a fairer comparison.\n\n2. Lack of novelty\n\n- The proposed method for generating the dataset is not novel. It simply uses BLIP to generate captions for video frames, and exploits LLM to generate a summarized caption for the video. Also, description of the used LLM is not given.\n- ViCLIP is not a novel architecture and inherits the CLIP model without significant modifications. The paper predominantly focuses on introducing the dataset, which leaves room for exploring further architectural improvement and training techniques.\n\n3. Unclear model selection process\n\n- The paper lacks information on the criteria used to select the image captioning and language models.\n\n4. Absence of ablation study and potential impact\n\n- In contrast to previous models which are benchmarked in datasets mostly composed of English, InternVID consists of clips with diverse languages. Further analysis about the potential impact caused by this aspect should be considered.\n\n5. Minor comments on Figure 2\n\n- Similar colors like green and dark green are used, but this may cause confusion. Using more distinct and discrete colors, such as red and blue, could enhance the clarity."
                },
                "questions": {
                    "value": "1. Consideration with Table 6\n\n- Comment of VideoCrafter, VideoFusion is absent. Does InternVID also improve VideoCrafter, VideoFusion in IS, FID, FVD, and CLIPSIM? Can you provide further analysis about the choice of InternVID-Aes-18M rather than exploiting the full InternVID-200M? \n\n2. Consideration with Section 5.2 and Figure 13~17\n\n- To insist that InternVID provides more powerful video-text aligned representations for video-centric dialogue systems, authors must compare the results from VideoChat-ViCLIP with that of vanilla VideoChat. Can you provide quantitative and qualitative comparisons, showing improvement of the results after replacement of the previous visual encoder with ViCLIP?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns",
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The data source is YouTube, where the content often includes personal recordings, potentially raising privacy concerns. The collectors should carefully address these issues and rigorous data selection criteria should be put in place to ensure strict adherence to privacy and other relevant guidelines."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4917/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4917/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4917/Reviewer_1UqW"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4917/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697874413041,
            "cdate": 1697874413041,
            "tmdate": 1700553891753,
            "mdate": 1700553891753,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GnTrqKygqr",
                "forum": "MLBdiWu4Fw",
                "replyto": "H1HxdqN6Qp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4917/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4917/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review of Submission4917 by Reviewer 1UqW: Q1"
                    },
                    "comment": {
                        "value": "**Q1: Lack of definitions, explanations. Definition of UMT-SIM is absent. In table 2, justification of the design choice of InternVid-10M-FLT is not explained. (At glance, the result of performance improvement after filtering the dataset with a high UMT-SIM score seems trivial.) Figure 5 introduced 3 schemes of interleaving video clips, text, and ASR text. However, comparison between the methods or further analysis is not given. It is doubtful that CLIP is a suitable choice for comparing their method in action recognition tasks. There are likely more appropriate video models to use in the benchmark for a fairer comparison.**\n\n1. UMT-SIM: In our work, UMT-SIM refers to the use of Unmasked Teacher (UMT) [R1] to compute the similarity score between a given video clip and the accompanying text. UMT is a video-language model that performs similarly to CLIP in terms of video-text similarity computation, but is trained in different data and task settings. Specifically, we extract the normalized video embedding of the given clip using UMT's video encoder and the normalized text embedding of the text using UMT's text encoder, then we compute their cosine similarity.\n\n2. For InternVid-10M-FLT, we apply filtering strategies to video data in addition to DIV sampling. The employed steps as given as:\na) Removing video clips shorter than 1s (approximately 23.15% of the total) or longer than 120s (around 0.84% of the total).\nb) Computing CLIPScore for each video clip using a randomly sampled frame from the clip with OpenAI's CLIP-ViT-L/14, then selecting clips within the top 30% of CLIPScores.\nc) Sampling 10M out of the remaining clips using DIV sampling.\n  For DIV (diversity sampling), we aimed to sample video clips from all long videos available to maximize data diversity. This was done by counting the frequencies of long videos in the segmented clip pool and sampling clips with probabilities inverse to these frequencies. Here is a pseudocode example of this process:\n\n```python \n    from collections import Counter\n    import json\n    import random\n    import numpy as np\n\n    data = json.load(open(\"/path/to/to_sample\"))\n    video_id = set([x[\"video\"].split(\"/\")[-1][:11] for x in data])\n    video_id_counter = Counter([x[\"video\"].split(\"/\")[-1][:11] for x in data])\n    sampling_weights = [1.0 / video_id_counter[x[\"video\"].split(\"/\")[-1][:11]] for x in data]\n    np.random.seed(42)\n    sampling_weights = np.array(sampling_weights)\n    sampling_weights = sampling_weights / sampling_weights.sum()\n    sampled_index = np.random.choice(len(data), 10647458, replace=False, p=sampling_weights)\n    data = [data[i] for i in sampled_index]\n    json.dump(data, open(\"/path/to/sampled\", \"w\"))  \n```\n\n3. Interleaving Video Clip Schemes: We introduced three potential schemes without claiming them as our contributions. The intention was to illustrate possible applications of the dataset.\n\n4. Benchmark for Action Recognition Tasks: We acknowledge that CLIP might not be the most suitable model for action recognition. Therefore, we also compared our method with other specialized video models for action recognition (see Table A below). Notably, ViCLIP trained only using contrastive loss and without additional learnable parameters on InternVid outperforms both CLIP-ViP and TVTSv2, where the latter two approaches are also pretrained on large-scale video datasets, demonstrating the effectiveness of our dataset.\n\n  ### Table A. Zero-shot action recognition results on Kinetics 400/600/700.\n  |Method | Training Data | K400 |  | K600 |  | K700 |  |\n  |:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n  | | |top-1 | AVG | top-1 | AVG | top-1 | AVG |\n  |CLIP-ViP-B [R2] | +HD-VILA-100M(sub+cap)+Img-text data | 37.6 | - | - | - | - | - |\n  |TVTSv2-B [R3] | +YT-Temporal-180M+WebVid-2M | 54.3 | - | - | - | - | - |\n  |ViCLIP-B | +InternVid-200M | 56.58 | 69.20 | 53.57 | 66.20 | 45.82 | 58.28 |\n  |ViCLIP-B | +InternVid-10M-FLT | 58.52 | 71.11 | 55.37 | 68.27 | 47.09 | 59.98 |\n  |ViCLIP-L | +InternVid-200M | 59.80 | 71.09 | 57.80 | 69.34 | 49.30 | 61.25 |\n  |ViCLIP-L | +InternVid-10M-FLT | 64.80 | 75.70 | 62.20 | 73.53 | 54.30 | 66.38 |\n  |TVTSv2-H [R3] | +YT-Temporal-180M+WebVid-2M | 59.6 | - | - | - | - | - |\n\nReferences:\n\n[R1] Unmasked teacher: Towards training-efficient video foundation models. In ICCV. 2023.\n\n[R2] CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment. In ICLR. 2023.\n\n[R3] TVTSv2: Learning Out-of-the-box Spatiotemporal Visual Representations at Scale. In arXiv. 2023."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509699226,
                "cdate": 1700509699226,
                "tmdate": 1700509719029,
                "mdate": 1700509719029,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L2UBts3qNA",
                "forum": "MLBdiWu4Fw",
                "replyto": "H1HxdqN6Qp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4917/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4917/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review of Submission4917 by Reviewer 1UqW: Q4 - Q6"
                    },
                    "comment": {
                        "value": "**Q4: Absence of ablation study and potential impact. In contrast to previous models which are benchmarked in datasets mostly composed of English, InternVID consists of clips with diverse languages. Further analysis about the potential impact caused by this aspect should be considered.**\n\nUnlike previous models that are benchmarked mostly on English-based datasets, InternVID encompasses clips from a variety of languages. This necessitates a further analysis to determine the potential impact this diversity might have.\nCurrently, we hypothesize that the language of the video may not significantly impact the generated captions as our deployed image caption models generate English descriptions based purely on input frames. However, in terms of video distributions, there may exist differences (such as in behaviors, activities, and events) between videos stemming from different countries due to varied cultural backgrounds.\n\nTo examine this hypothesis, we selected two 2 million subsets of InternVid: one consisting of only English videos (InternVid-2M-EN) and another with only Chinese videos (InternVid-2M-CN). It's important to note that whether the videos are in English or Chinese, we generate captions in English.\n\nOur ViCLIP-B model was pretrained on these subsets, and we conducted zero-shot experiments as described below. Due to resource constraints, we trained the ViCLIP-B with a batchsize of 4096 using 8 A100 GPUs with a mask ratio set to 0.9. The remaining training settings were kept consistent with those outlined in the paper.\n\nWe found that the model pretrained with InternVid-2M-EN outperformed that with InternVid-2M-CN notably in both zero-shot action recognition on K400/600/700 and video retrieval. This result can be attributed to the fact that InternVid-2M-EN has a data distribution much closer to downstream task data than InternVid-2M-CN, as all used task videos are sourced from English sources.\n\n### Table B. Zero-shot action recognition results of ViCLIP using different pretraining sources on Kinetics 400/600/700.\n|Method | Training Data | K400 |  | K600 |  | K700 |  |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| | |top-1 | AVG | top-1 | AVG | top-1 | AVG |\n|ViCLIP | +InternVid-2M-EN | 40.20 | 53.68 | 37.40 | 51.00 | 29.60 | 42.06 |\n|ViCLIP | +InternVid-2M-CN | 35.9 | 49.73 | 33.70 | 47.05 | 26.90 | 39.02 |\n\n### Table C. Results of zero-shot video retrieval of ViCLIP using different pretraining sources on MSR-VTT, LSMDC, DiDeMo, MSVD, and ActivityNet.\n|Method | Data | MSR-VTT | | LSMDC | | DiDeMo | | MSVD | | ANet | |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| | |T2V | V2T | T2V | V2T | T2V | V2T | T2V | V2T | T2V | V2T |\n|ViCLIP | +InternVid-2M-EN | 24.1 | 24.1 | 9.8 | 9.1 | 10.3 | 15.6 | 31.4 | 50.5 | 7.1 | 12.0 |\n|ViCLIP | +InternVid-2M-CN | 22.2 | 22.2 | 8.4 | 8.7 | 9.9 | 15.1 | 29.6 | 48.5 | 5.7 | 9.7 |\n\n**Q5: Minor comments on Figure 2. Similar colors like green and dark green are used, but this may cause confusion. Using more distinct and discrete colors, such as red and blue, could enhance the clarity.**\n\nSure. We will update Figure 2 and check other ones for better clarity.\n\n**Q6: Consideration with Table 6. Comment of VideoCrafter, VideoFusion is absent. Does InternVID also improve VideoCrafter, VideoFusion in IS, FID, FVD, and CLIPSIM? Can you provide further analysis about the choice of InternVID-Aes-18M rather than exploiting the full InternVID-200M?**\n\nThe lack of full training code and details from VideoCrafter and VideoFusion poses a challenge for reproducing their results and further training with additional data.\n\nThe decision to use InternVID-Aes-18M instead of the full InternVID-200M is driven by techniques from studies [R4]. These studies suggested that image aesthetic quality significantly impacts text-to-image generation, even more than the sheer quantity of images. We hypothesize that this principle can be extrapolated to video generation as well. Therefore, we decided to curate a subset based on video aesthetics for text-to-video generation.\n\nThe process to compute video aesthetics involved calculating the image aesthetic scores of four randomly sampled frames from each video clip. The tool used for this task was the LAION aesthetic predictor. The highest score among the four frames was then chosen to represent the aesthetic score of that particular video.\n\nThis approach ensured that our model was trained on aesthetically pleasing videos, potentially improving the overall quality of the generated outputs. However, note that this approach has not been explicitly tested on VideoCrafter or VideoFusion. Therefore, it remains uncertain whether InternVid would result in improved IS, FID, FVD, and CLIPSIM metrics for these models. Further experiments would be needed to confirm this hypothesis.\n\n[R4] Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack. In arxiv. 2023."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510248890,
                "cdate": 1700510248890,
                "tmdate": 1700510354973,
                "mdate": 1700510354973,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KUZ5lutRzf",
                "forum": "MLBdiWu4Fw",
                "replyto": "kWYYw6Zg44",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4917/Reviewer_1UqW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4917/Reviewer_1UqW"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed answers. We appreciate the answers regarding the comparison between T5 and Llama, further analysis between InternVid-2M-EN and InternVid-2M-CN considering language/ content diversity, and quantitative comparison between vanilla VideoChat and VideoChat-ViCLIP. I would like to see the inclusion of these analyses in the final version, which resolved our main concerns, so we raise the score to 6."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553855381,
                "cdate": 1700553855381,
                "tmdate": 1700553855381,
                "mdate": 1700553855381,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tT8nT4UOl8",
            "forum": "MLBdiWu4Fw",
            "replyto": "MLBdiWu4Fw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4917/Reviewer_6e9v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4917/Reviewer_6e9v"
            ],
            "content": {
                "summary": {
                    "value": "- This paper introduces a web-scale video language dataset InternVid comprised of 234M clips and 760K hours.\n- It introduces ViCLIP, a video-language model based on ViT-L, pretrained with contrastive learning (like CLIP) and masked autoencoder (like VideoMAE, MAE).\n- This work expands the utility of their dataset to video understanding tasks like recognition and retrieval, and video generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- It introduces a web-scale video-language dataset bridging the gap of lack of such datasets, unlike image-text domains.\n- It evaluates on multiple benchmarks in both finetuned and zero-shot setups.\n- The dataset curation is fairly detailed and I also like the hierarchical video caption generation strategy."
                },
                "weaknesses": {
                    "value": "- The majority of the comparisons are with CLIP which was pretrained with image-text pairs and not video-text; I would like to see how ViCLIP performs compared to other video or video-language models (e.g., pretrained on HD-VILA, HT100M, YT100M), I think that would be a fair comparison than comparing with image-text models.\n- I would encourage authors to dig deeper and find a more concrete argument/explanation: why does pretraining with InternVid-10M-FLT or InternVid-10M-DIV perform better in **zero-shot** than InternVid-200M, but not when **finetuned**. \n- In fig. 7 and 8, the experiments are done only on scaling the dataset size, it would be interesting to see the effect of model scaling in addition to the dataset scaling. I would encourage you to add such experiments in the final version.\n- I would be interested to see linear evaluation (i.e., a single FC layer, or use linear SVM) performance on the downstream benchmarks."
                },
                "questions": {
                    "value": "- Will you share the pretrained and finetuned models with the supporting code base (e.g., data processing, pretraining, finetuning, generation)? \n- Could you please share the processed clips, even processing the data by individuals (typically for academic researchers) would be a difficult task considering its massive size. Additionally, we all know the unavailability of videos due to location constraints, permission issues, etc., so even if the full data is not possible to share at least share the 3 10M versions.\n- I suggest releasing the fixed embeddings of the datasets from the trained (e.g., pretrained, finetuned) models.\n- Did you investigate if the InternVid has any sort of bias in the curated clips, could you please share a report with such details? Bias could be of many forms e.g., location/race/gender per action category. A suggested reference: https://arxiv.org/abs/1505.01257\n- Did you investigate, if ViCLIP is robust against some of the OOD setups, some of the popular benchmarks are Mimetics, RareAct etc. For more details please see: https://arxiv.org/abs/2306.02014"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4917/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4917/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4917/Reviewer_6e9v"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4917/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698680389582,
            "cdate": 1698680389582,
            "tmdate": 1699636477339,
            "mdate": 1699636477339,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H8IkLb261a",
                "forum": "MLBdiWu4Fw",
                "replyto": "tT8nT4UOl8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4917/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4917/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review of Submission4917 by Reviewer 6e9v - Q1 & Q2"
                    },
                    "comment": {
                        "value": "**Q1: The majority of the comparisons are with CLIP which was pretrained with image-text pairs and not video-text; I would like to see how ViCLIP performs compared to other video or video-language models (e.g., pretrained on HD-VILA, HT100M, YT100M), I think that would be a fair comparison than comparing with image-text models.**\n\nWe understand your concern about the comparison with CLIP, which was pretrained on image-text pairs. Indeed, to provide a more fair comparison, we included results against CLIP-ViP, a version of CLIP pretrained on HD-VILA with coarse synthetic captions (one frame caption used as the video caption) and ASR text. However, please note that this comparison still has limitations as CLIP-ViP only offers a ViT-base version and some of its variants are trained with different settings. \n\nTurning our attention to Table A, when evaluated on MSR-VTT, our model, ViCLIP-B, trained on InternVid-200M/-10M-FLT outperforms CLIP-ViP-B, trained on HD-VILA-100M with subtitles, achieving a higher R@1 score (50.7%/49.0% vs. 47.7%). CLIP-ViP-B reaches similar performance (49.6%) only when using a combination of captions and subtitles from HD-VILA-100M. It's worth noting that CLIP-ViP-B uses more frames for pretraining than ours (12 vs. 8).\n\n### Table A. Fine-tuned video retrieval on MSR-VTT.\n|Method | Data | #Frames in Train | T2V | V2T |\n|:---:|:---:|:---:|:---:|:---:|\n|CLIP-ViP-B | +HD-VILA-100M | 12 | 47.7 |  |\n|CLIP-ViP-B | +HD-VILA-100M(sub+cap) | 12 | 49.6 |  |\n|CLIP-ViP-B | +HD-VILA-100M(sub)+Im-text data | 12 | 49.1 |  |\n|ViCLIP-B | +InternVid-200M | 8 | 50.7 | 49.4 |\n|ViCLIP-B | +InternVid-10M-FLT | 8 | 49.0 | 49.2 |\n\n\n**Q2: I would encourage authors to dig deeper and find a more concrete argument/explanation: why does pretraining with InternVid-10M-FLT or InternVid-10M-DIV perform better in zero-shot than InternVid-200M, but not when finetuned.**\n\nThe varying performance of ViCLIP (trained on InternVid-10M-FLT/DIV and InternVid-200M) in zero-shot vs. fine-tuned action recognition can be attributed to the differing demands of these tasks. Zero-shot action recognition primarily depends on the alignment between video and text representations, while fine-tuned action recognition focuses on discriminating video representation as we fine-tune the video encoder using supervised video-label data and discard the text encoder.\nSo it's important to note that there's no theoretical guarantee for correlation between a model's zero-shot and fine-tuned capabilities.\n\nYou correctly noted that InternVid-10M-DIV/FLT performs better in zero-shot action recognition (Table 2), but not in the fine-tuned setting (Table 3), when compared to InternVid-200M. Our interpretation is as follows:\n1. ViCLIP trained on InternVid-200M likely has a more robust video representation due to a larger quantity and diversity of videos.\n2. On the other hand, ViCLIP trained on InternVid-10M-FLT/DIV might excel in video-text alignment\u2014made possible by limiting the number of clips drawn from identical videos. This results in more diverse training data compared to InternVid-200M\u2019s, thereby facilitating contrastive learning, enhancing video-text representation, and driving better outcomes for zero-shot action recognition and tasks like retrieval."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700123828830,
                "cdate": 1700123828830,
                "tmdate": 1700124024962,
                "mdate": 1700124024962,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hRiFs23CRc",
                "forum": "MLBdiWu4Fw",
                "replyto": "tT8nT4UOl8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4917/Reviewer_6e9v"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4917/Reviewer_6e9v"
                ],
                "content": {
                    "title": {
                        "value": "Post rebuttal comment"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for responding to my comments. I hope the authors would make the said changes in their final version as promised. I am happy to stick to my original rating in favor of acceptance."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185016334,
                "cdate": 1700185016334,
                "tmdate": 1700185085501,
                "mdate": 1700185085501,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xt7EedelKQ",
            "forum": "MLBdiWu4Fw",
            "replyto": "MLBdiWu4Fw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4917/Reviewer_G48V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4917/Reviewer_G48V"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new video and language dataset, INTERNVID, which includes 234M video-text pairs lasting 760K hours. In addition to the dataset, this paper also introduce a baseline model, ViCLIP, demonstrating its performance on various downstream applications after pre-training on INTERNVID dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well written, and each section is easy to follow. \n\n2. The proposed large-scaled video-text dataset can contribute to the community, scaling up the current model and largely improving the video and language feature representation learning. \n\n3. This paper provides very detailed statistic for the dataset, and also reveal the method for data curation.\n\n4. By leveraging the new dataset, this paper demonstrates extensive experiments on many downstream applications and achieving promising results on many benchmarks."
                },
                "weaknesses": {
                    "value": "1. There are not many analyses and design justification for the proposed ViCLIP. If ViCLIP is claimed as one of the contributions of this paper. The proposed architecture is not novel and the masking idea needs further justification. For example, the efficiency gain vs. the performance drop, and the ablation over the masking ratio.\n\n2. The video caption is generated by language model from frame-level captions. In this case, will this reduce the number of motion-related words that need to be captured from video-based understanding?"
                },
                "questions": {
                    "value": "Given 10M pretrained data, the ViCLIP receives better zero-shot action recognition results from WebVid and better fine-tune action recognition results in Table 2 and 3. And the gain from 50M, 200M INTERNVID pretraining is minor. Does it mean the pretraining data is not the more the better? The performance of vision and language model will be saturated when the pretraining data reach to a certain scale? Could you please provide more insights here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4917/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790159010,
            "cdate": 1698790159010,
            "tmdate": 1699636477084,
            "mdate": 1699636477084,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MKx8dtWxdh",
                "forum": "MLBdiWu4Fw",
                "replyto": "xt7EedelKQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4917/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4917/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review of Submission4917 by Reviewer G48V"
                    },
                    "comment": {
                        "value": "**Q1: There are not many analyses and design justifications for the proposed ViCLIP. If ViCLIP is claimed as one of the contributions of this paper. The proposed architecture is not novel and the masking idea needs further justification. For example, the efficiency gain vs. the performance drop, and the ablation over the masking ratio.**\n\nThe primary aim of applying ViCLIP on InternVid is to validate the efficacy of our proposed dataset in video understanding tasks. We concur that the architecture and training methodology for ViCLIP are not novel; it follows the design principles of CLIP, with the addition of mask modeling for training efficiency.\n\nThe masking scheme, specifically the choice of a 0.9 masking ratio from VideoMAE, was adopted to balance computational efficiency and performance. Below, we present Table A, which lists the trade-off between efficiency gains and performance drops at different masking ratios for ViCLIP-B. As can be seen, decreasing the masking ratio improves performance but increases GPU memory usage significantly. The masking ratio of 0.9 was chosen because it provides the highest efficiency with a tolerable performance drop on downstream tasks.\n\n- Table A. Fine-tuned video retrieval on MSR-VTT from ViCLIP with different masking ratios. ``Mem\" denotes single GPU memory usage with per GPU batch size 128.\n|Method | Data | Masking ratio | T2V | Mem/G|\n|:---:|:---:|:---:|:---:|:---:|\n|ViCLIP-B | +InternVid-10M | 0.7 | 48.0 | 27.9 |\n|ViCLIP-B | +InternVid-10M | 0.8 | 47.7 | 19.2 |\n|ViCLIP-B | +InternVid-10M | 0.9 | 47.4 | 12.1 |\n\n**Q2: The video caption is generated by language model from frame-level captions. In this case, will this reduce the number of motion-related words that need to be captured from video-based understanding?**\n\nFrom a statistical perspective, generating video captions from frame-level captions using a language model has a **negligible** effect on the number of motion-related words captured for video-based understanding.\n\nTo illustrate this, we counted the unique verbs (using nltk package) in the captions from a 10m subset of InternVid under two settings:\n\n1. In the first setting, the captions are video captions generated by the language model.\n\n2. In the second setting, the captions are frame-wise ones from BLIP2 and tag2text.\n\nWe found that the number of unique verbs in the video captions is **109,859**, whereas for the frame-wise captions it is slightly higher at **109,895**. This **small discrepancy** suggests that almost no motion-related words are lost during the caption generation process by LM. Therefore, we believe our approach maintains most of the important motion-related information needed for video understanding.\n\n**Q3: Given 10M pretrained data, the ViCLIP receives better zero-shot action recognition results from WebVid and better fine-tune action recognition results in Table 2 and 3. And the gain from 50M, 200M INTERNVID pretraining is minor. Does it mean the pretraining data is not the more the better? The performance of vision and language model will be saturated when the pretraining data reach to a certain scale? Could you please provide more insights here?**\n\nFor the subset of 10M pretraining data, ViCLIP performs better in both zero-shot and fine-tuned action recognition when utilized with InternVid-10M-FLT/DIV compared to WebVid. This performance differences between models using InternVid-10M-FLT/DIV and InternVid-10M underscores the importance of sampling unique clips from videos - a hypothesis supported by our initial explorations.\n\nOur findings from Tables 2 and 5 along with Figures 7 and 8 demonstrate a consistent performance improvement in zero-shot action recognition and fine-tuned video retrieval as the pretraining data size escalates. However, the enhancement in zero-shot video retrieval is only marginal, which we believe stems from the limited increase in video-text diversity during pretraining. In particular, the more clips sampled from the same video, the higher tendency they have to share similar semantics. Furthermore, while a generative captioning method aids in video descriptions, the limited capacity of the captioning model restricts the diversity of produced captions, making them somewhat inferior to human annotations. These findings motivate us to explore scaling InternVid-10M-FLT/DIV and improving video captions through advanced captioning methods, language modeling, and human feedback techniques.\n\nFrom this standpoint, our current experimental data does not conclusively suggest a saturation point for vision and language model performance as pretraining data scales up. We can only affirm that zero-shot video retrieval performance will reach a plateau at a certain scale of pretraining data when further diversification of the data used becomes unavailable."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699976096136,
                "cdate": 1699976096136,
                "tmdate": 1699976096136,
                "mdate": 1699976096136,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "shewN2v2tz",
                "forum": "MLBdiWu4Fw",
                "replyto": "xt7EedelKQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4917/Reviewer_G48V"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4917/Reviewer_G48V"
                ],
                "content": {
                    "title": {
                        "value": "Post rebuttal comment"
                    },
                    "comment": {
                        "value": "Thanks all authors for their effort in the rebuttal. \n\nThe rebuttal address most of my concerns, so I keep my rating suggesting acceptance of this work.\n\nThanks"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590660020,
                "cdate": 1700590660020,
                "tmdate": 1700590660020,
                "mdate": 1700590660020,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "73u7sK1sWS",
            "forum": "MLBdiWu4Fw",
            "replyto": "MLBdiWu4Fw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4917/Reviewer_gWXX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4917/Reviewer_gWXX"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a large-scale video-text dataset for video representation learning. Considering the noise and low correlation in ASR transcripts, this paper utilizes captioning models and a pre-trained language model to generate video descriptions. The authors pre-train the ViCLIP model on the collected dataset and the model shows strong performance on action recognition and text-video retrieval tasks. They also explore potential applications of this dataset on text-to-video generation and video-centric dialogue systems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The dataset is large-scale and diverse, and the generated descriptions have better relevance with videos.\n- The pre-trained model shows good performance.\n- The experiments are extensive."
                },
                "weaknesses": {
                    "value": "- Some details are not presented clearly. 1) How many descriptions are generated at the coarser level? Are all descriptions used for training the final model at the finer level? 2) Which pre-trained language model is used for processing captions? 3) The details about DIV and FLT are not introduced.\n- A closely related work, CLIP-ViP[R1], is not included in related works and compared. It also adopts a caption model to generate descriptions for video clips. It is corresponding to the coarser level in this paper.\n- The performance of InterVid-10M-DIV, InterVid-10M-FLT is weird. It shows much better performance in zero-shot action recognition in Table 2, but poor in the fine-tuned setting of Table 3. The authors give reasons for false negatives, but what is the training batch size, given the much larger training data, I think the possibility of the same video clip appearing in the same batch is low.\n\n[R1]: CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment, ICLR 2023."
                },
                "questions": {
                    "value": "Refering to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4917/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4917/Reviewer_gWXX",
                        "ICLR.cc/2024/Conference/Submission4917/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4917/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823157889,
            "cdate": 1698823157889,
            "tmdate": 1700530836872,
            "mdate": 1700530836872,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9kG86tWpS5",
                "forum": "MLBdiWu4Fw",
                "replyto": "73u7sK1sWS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4917/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4917/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review of Submission4917 by Reviewer gWXX - Q1 & Q2"
                    },
                    "comment": {
                        "value": "**Q1: Some details are not presented clearly. 1) How many descriptions are generated at the coarser level? Are all descriptions used for training the final model at the finer level? 2) Which pre-trained language model is used for processing captions? 3) The details about DIV and FLT are not introduced.**\n\n1) We generated both coarse and fine descriptions for **all** of the collected and segmented video clips, totaling 234 million. However, not all descriptions at the finer level were used in training the final model. For our largest experimental setting conducted on InternVid-200M, we trained ViCILP on a random selection of 200 million clips with synthetic video captions derived from both coarse and fine-level captions.\n\n2) The language model predominantly used for processing captions was T5-summary, accessible at [this URL](https://huggingface.co/mrm8488/flan-t5-large-finetuned-openai-summarize_from_feedback). A small proportion of conversions were handled by Vicuna. We found that, when compared to other options like LLM (7b or 13b), T5 offered significantly faster processing speeds and did not generate hallucinations. On average, T5 completed each summary within 14.96ms while Llama2-7b took longer at 81.51ms, both using A100-80G.\n\n3) For DIV (diversity sampling), we aimed to sample video clips from all long videos available to maximize data diversity. This was done by counting the frequencies of long videos in the segmented clip pool and sampling clips with probabilities inverse to these frequencies. Here is a pseudocode example of this process:\n\n```python \n    from collections import Counter\n    import json\n    import random\n    import numpy as np\n\n    data = json.load(open(\"/path/to/to_sample\"))\n    video_id = set([x[\"video\"].split(\"/\")[-1][:11] for x in data])\n    video_id_counter = Counter([x[\"video\"].split(\"/\")[-1][:11] for x in data])\n    sampling_weights = [1.0 / video_id_counter[x[\"video\"].split(\"/\")[-1][:11]] for x in data]\n    np.random.seed(42)\n    sampling_weights = np.array(sampling_weights)\n    sampling_weights = sampling_weights / sampling_weights.sum()\n    sampled_index = np.random.choice(len(data), 10647458, replace=False, p=sampling_weights)\n    data = [data[i] for i in sampled_index]\n    json.dump(data, open(\"/path/to/sampled\", \"w\"))  \n```\n\nFor FLT (filtering), we applied a series of filtering strategies to video data alongside DIV sampling. These included:\n\na) Removing video clips shorter than 1s (approximately 23.15% of the total) or longer than 120s (around 0.84% of the total).\n\nb) Computing CLIPScore for each video clip using a randomly sampled frame from the clip with OpenAI's CLIP-ViT-L/14, then selecting clips within the top 30% of CLIPScores.\n\nc) Sampling 10M out of the remaining clips using DIV sampling.\n\n**Q2: A closely related work, CLIP-ViP[R1], is not included in related works and compared. It also adopts a caption model to generate descriptions for video clips. It is corresponding to the coarser level in this paper.**\n\nWe acknowledge the omission of CLIP-ViP[R1] in the related works. This will be rectified in the final manuscript where a thorough discussion on CLIP-ViP and its relation to our work will be included.\n\nIn response to your request for a comparison with CLIP-ViP, we have provided preliminary results in Table X. Please note, these results are not strictly like-for-like due to differing parameters such as number of sampled frames during training, training epochs, and resource constraints that limited our ability to retrain all models.\n\nAs seen in Table A, when evaluated on MSR-VTT, our model (ViCLIP-B used with InternVid-200M/-10M-FLT) outperforms CLIP-ViP-B used with HD-VILA-100M, giving a better R@1 score (50.7%/49.0% vs. 47.7%). However, when captions and subtitles from HD-VILA-100M are combined, CLIP-ViP-B achieves comparable result of 49.6%. An important note is that CLIP-ViP-B uses more frames for pretraining than ours (12 vs. 8). \n\nThis suggests that synthetic video captions from our InternVid dataset can compete effectively against HD-VILA-100M's combined subtitles and captions.\n\n- Table A. Fine-tuned video retrieval on MSR-VTT.\n|Method | Data | #Frames in Train | T2V | V2T |\n|:---:|:---:|:---:|:---:|:---:|\n|CLIP-ViP-B | +HD-VILA-100M | 12 | 47.7 |  |\n|CLIP-ViP-B | +HD-VILA-100M(sub+cap) | 12 | 49.6 |  |\n|CLIP-ViP-B | +HD-VILA-100M(sub)+Im-text data | 12 | 49.1 |  |\n|ViCLIP-B | +InternVid-200M | 8 | 50.7 | 49.4 |\n|ViCLIP-B | +InternVid-10M-FLT | 8 | 49.0 | 49.2 |\n\n[R1]: CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment, ICLR 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699975528001,
                "cdate": 1699975528001,
                "tmdate": 1699976196403,
                "mdate": 1699976196403,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n0hTlLZJFl",
                "forum": "MLBdiWu4Fw",
                "replyto": "73u7sK1sWS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4917/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4917/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review of Submission4917 by Reviewer gWXX - Q3"
                    },
                    "comment": {
                        "value": "**Q3: The performance of InterVid-10M-DIV, InterVid-10M-FLT is weird. It shows much better performance in zero-shot action recognition in Table 2, but poor in the fine-tuned setting of Table 3. The authors give reasons for false negatives, but what is the training batch size, given the much larger training data, I think the possibility of the same video clip appearing in the same batch is low.**\n\nWe reason that the contrasting performance of ViCLIP (trained on InternVid-10M-FLT/DIV and InternVid-200M) in zero-shot vs. fine-tuned action recognition comes from differing task demands. Zero-shot performance largely hinges on aligning video and text representations, while fine-tuned action recognition focuses more on the discriminative abilities of the video representation. As such, there is no guaranteed correlation between a model's capabilities in these two tasks.\n\nRegarding your observation about InternVid-10M-DIV/FLT, we agree it performs better in zero-shot action recognition (Table 2), yet lacks in the fine-tuned setting (Table 3). Our explanation is that ViCLIP trained on InternVid-200M has a more robust video representation due to a higher quantity and diversity of videos. Conversely, the version trained on InternVid-10M-FLT/DIV likely excels in video-text alignment: a result of reducing the number of clips sourced from identical videos. This approach makes their training data more distinct than InternVid-200M\u2019s, which promotes contrastive learning to enhance video-text representation\u2014yielding superior outcomes for zero-shot action recognition and video-text tasks such as retrieval.\n\nIn response to the query about false negatives and batch sizes, you're right that the same video clip appearing in the same batch is relatively rare given the extensive size of the training data. However, we consider an alternative explanation for the false negatives: the higher video diversity in InternVid-10M-FLT/DIV and its improved video-text correlation (sorted by CLIP) result in a more effective video-text representation compared to InternVid-10M."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699975592632,
                "cdate": 1699975592632,
                "tmdate": 1699975785537,
                "mdate": 1699975785537,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cv0o6PcpKM",
                "forum": "MLBdiWu4Fw",
                "replyto": "9kG86tWpS5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4917/Reviewer_gWXX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4917/Reviewer_gWXX"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my concerns; I have gained some clarity on certain concerns. The use of coarse-level captions indeed seems similar to that in CLIP-ViP, but the dataset that provides fine-level captions appears to be a significant contribution. However, I have some questions:\n1. What is the mixing strategy for coarse and fine-level captions when training InternVid-200M? What motivated the inclusion of coarse-level captions? I would like to see an ablation study that uses coarse-level captions, fine-level captions, and both, possibly at a scale of 10M.\n2. I would appreciate more examples from the dataset, especially those that illustrate the contrast between coarse and fine-level captions.\n3. Regarding the T5-summary and Vicuna models mentioned, were they fine-tuned for processing captions? I suspect that these models are not capable enough for zero-shot handling of captions. Perhaps GPT-3.5-Turbo or GPT-4 might be suitable."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700018269341,
                "cdate": 1700018269341,
                "tmdate": 1700018269341,
                "mdate": 1700018269341,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "njPV5WXCCL",
                "forum": "MLBdiWu4Fw",
                "replyto": "9MiP0Tst55",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4917/Reviewer_gWXX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4917/Reviewer_gWXX"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's detailed responses, most of my questions have been answered. Now I would like to see this paper accepted. I hope the author will include the rebuttal content in the final version."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530778811,
                "cdate": 1700530778811,
                "tmdate": 1700530778811,
                "mdate": 1700530778811,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]