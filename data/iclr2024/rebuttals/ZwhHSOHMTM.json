[
    {
        "title": "Learning dynamic representations of the functional connectome in neurobiological networks"
    },
    {
        "review": {
            "id": "UzXgshKWwo",
            "forum": "ZwhHSOHMTM",
            "replyto": "ZwhHSOHMTM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3970/Reviewer_nAoq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3970/Reviewer_nAoq"
            ],
            "content": {
                "summary": {
                    "value": "The goal of this work is to infer community structure in neural networks (specifically that of _C. elegans_) based on functional data. To do this, the authors 1) define a pairwise affinity score computable from single-neuron time series; 2) use tensor factorization to group these pairs into dynamical motifs; 3) feed tensor components from these motifs into a community detection algorithm. When applied to data recorded from individually identified _C. elegans_ neurons, this produces networks that are tested in validation experiments, which find that perturbation of key nodes does affect network structure and behavior related to, e.g., salt avoidance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The problem of organizing functional data from neurons into more interpretable submodules that can be investigated in causal experiments is an important one.\n- The approach is reasonable and makes use of established techniques (tensor factorization, community detection).\n- Use of real neural data for both algorithm validation and suggesting perturbative experiments is a huge plus."
                },
                "weaknesses": {
                    "value": "- I found the affinity score a bit _ad hoc_. I can understand the intuition, but it seems like there should be a more principled way to get at this information. Related, but along the opposite direction: why not include strong _anticorrelations_ in the affinity score? Shouldn't two neurons whose activity moves opposite to one another at most times be considered related/coupled?\n- The tensor factorization will tend to treat affinities independently of one another, though the $N(N-1)/2$ affinities result from only $N$ neuronal time series. That is, the tensor factorization does not respect the underlying geometry of the problem. It's unclear to me how big an issue this is in practice, but it might lead to issues with the method.\n- While the experimental data are a definite plus, it's always unclear how strongly they should be taken as validation of a particular data analysis method. In a strongly coupled network, ablating any one neuron is likely to have an effect, and it's not shown that that the method proposed here would necessarily outperform others for selecting which perturbations to apply."
                },
                "questions": {
                    "value": "- How sensitive are the results presented to the particular choice of affinity score? Would, e.g., a Spearman correlation between the two time series yield qualitatively similar results?\n- I might have missed this, but how did the authors decide how many tensor components to retain?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3970/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3970/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3970/Reviewer_nAoq"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3970/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759912693,
            "cdate": 1698759912693,
            "tmdate": 1699636358101,
            "mdate": 1699636358101,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2h8fM7O9KN",
                "forum": "ZwhHSOHMTM",
                "replyto": "UzXgshKWwo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3970/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your time and your valuable comments and questions.\n\n**I found the affinity score a bit ad hoc. I can understand the intuition, but it seems like there should be a more principled way to get at this information. Related, but along the opposite direction: why not include strong anticorrelations in the affinity score? Shouldn't two neurons whose activity moves opposite to one another at most times be considered related/coupled?**\n\nYou are absolutely correct in your intuition, and in fact that is precisely the motivation behind our proposed affinity score computation. Firstly, we use first derivatives to eliminate the influence of the raw activity trace values (which are based on the calcium fluorescence signal and may not be a fair comparison across different neurons \u2013 see Sec. 2.1, 2nd paragraph). Secondly, we use absolute values specifically to allow for neurons with opposite changes in activity to have strong affinity regardless of the derivative sign. As stated in Appendix A.1: *\"Notice that the areas are unsigned (\u2026) This is motivated by the fact that two neurons with very similar but opposite sign derivatives are still likely to be interacting by means of some inhibitory mechanism. This further contributes to making the affinities interpretable as indicating the likelihood of interactions between pairs of neurons, regardless of the specific physiological connectivity mechanisms involved.\"*\n\nThis feature is further mentioned in the caption of Fig. 5 (last line):\n\n*\"Notice that, because the affinities are computed from absolute derivatives, neurons will be strongly connected even when their traces have opposite signs.\"*\n\nIn summary, every decision behind our affinity score has been carefully examined and extensively tested to arrive at a value that gives a reasonable measure of the likelihood that two neurons are interacting. Our experimental results further confirm the validity of our choices.\n\nAlthough the affinity derivation is developed in detail in Appendix A.1, based on your comments we now realize that this particular feature of our method should have been more clearly stated in the main text. So we have updated the manuscript to include in Sec. 2.1, 4th paragraph (new text in bold):\n\n*\"Affinities show how similar two curves are, locally, in terms of their absolute derivatives, __which is motivated by the fact that two neurons with very similar but opposite sign derivatives are still likely to be interacting by means of some inhibitory mechanism.__ As a result, they can be interpreted as how likely it is for two neurons to be interacting\"*\n\n**The tensor factorization will tend to treat affinities independently of one another, though the N(N\u22121)/2 affinities result from only N neuronal time series.**\n\nAlthough they originate from N time series, every affinity is computed independently for each pair of neurons. This is important for a number of reasons. For example a neuron $i$ might have an activity that is moderately similar to two other neurons, $j$ and $k$, while at the same time $j$ and $k$ have little affinity to each other. Here is an example to illustrate this subtlety, drawn directly from our experience with these neural data.  Imagine a brief time window during which $i$'s activity decreases and soon after increases, whereas $j$ decreases in tandem with $i$ but does not increase afterwards. In contrast, $k$ does not decrease initially, but does increase together with $i$. Then both $j$ and $k$ are somewhat similar to $i$, but not to each other.  This illustrates how, allowing the affinities between pairs of neurons to be treated independently, leads to a more appropriate measure of the likelihood of instantaneous interactions between them.\n\n**How sensitive are the results presented to the particular choice of affinity score? Would, e.g., a Spearman correlation between the two time series yield qualitatively similar results?**\n\nIn prior work we tried many of the standard approaches and found that none of them agreed with our natural, intuitive assessment of how two traces should relate to one another. This qualitative mismatch was, in fact, the main motivation for our developing a novel affinity score. We mentioned, in Sec. 2, second paragraph, that previous studies were limited to traditional statistics and were not able to reveal the same relationships we found between neurons. Moreover, from Sec. 2.1, first paragraph: Affinities should thus be a function of time, rather than the more traditional method of computing a global-time measure such as correlation.\n\nSpearman correlation, in particular, computes an average across all values, so it could not be used to produce a continuous similarity value over time. Our affinity method, on the other hand, proposes a principled way to define local time windows in which the rates of change can be compared (in absolute value, as explained above)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700103355677,
                "cdate": 1700103355677,
                "tmdate": 1700103355677,
                "mdate": 1700103355677,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CPv7MEecgo",
                "forum": "ZwhHSOHMTM",
                "replyto": "2h8fM7O9KN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3970/Reviewer_nAoq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3970/Reviewer_nAoq"
                ],
                "content": {
                    "comment": {
                        "value": "> **The tensor factorization will tend to treat affinities independently of one another, though the N(N\u22121)/2 affinities result from only N neuronal time series.**\n\nYes, this is what I was asking about, but I'm still a bit confused as to why treating these affinities as independent -- which ignores  some key structure in the data that generated them -- should be preferred. Clearly, all the information is extracted from the underlying data, and the authors have shown that the empirical performance is good. I'm just not entirely sure why, but this is a minor point.\n\nI appreciate the authors' replies to my other questions. I think this is nice work that still seems (to me) fairly tightly tuned to this particular application, so I will be maintaining my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515268880,
                "cdate": 1700515268880,
                "tmdate": 1700515268880,
                "mdate": 1700515268880,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QWFupi6rtL",
            "forum": "ZwhHSOHMTM",
            "replyto": "ZwhHSOHMTM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3970/Reviewer_6VUY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3970/Reviewer_6VUY"
            ],
            "content": {
                "summary": {
                    "value": "This study presents an unsupervised method to identify the dynamic interactions between neurons. The approach has two main steps: first, based on calcium activity, the neural traces are organized to identify groups of neurons likely interacting over specific time intervals. Then, a generative model is applied to detect weighted communities in the functional patterns."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written and easy to follow. It is well-organized with illustrative figures. Also, the proposed method is simple, yet potentially effective for different tasks and domains. Moreover, compared to existing studies that use step-by-step statistical methods, this paper designs a method that allows considering the full system of similarities and animals across time. Also, presenting extensive experimental results supports the claim and the potential of the approach."
                },
                "weaknesses": {
                    "value": "1. The main weakness of this paper is the lack of novelty in the model design. In fact, the proposed approach is a simple combination of existing methods and I cannot see a novel insight or a novel contribution from the machine learning side. \n\n2. It would be better to include more related baselines. In the literature, there are several learning methods that learn the structure of the brain networks. Based on the current set of baselines, the proposed method shows superior performance, but existing baselines are general graph learning methods and do not use special properties of the brain. Therefore, I suggest adding additional brain network-based graph learning approaches as baselines. \n\n3. There is a lack of discussion and experimental results about the scalability of the method and its efficiency. It would be great if the authors could provide more information about the efficiency of the method."
                },
                "questions": {
                    "value": "Please see the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3970/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3970/Reviewer_6VUY",
                        "ICLR.cc/2024/Conference/Submission3970/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3970/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811449130,
            "cdate": 1698811449130,
            "tmdate": 1700586210999,
            "mdate": 1700586210999,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uYgiHs0iCS",
                "forum": "ZwhHSOHMTM",
                "replyto": "QWFupi6rtL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3970/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and for their insightful comments and suggestions for the paper.\n\n**The main weakness of this paper is the lack of novelty in the model design. In fact, the proposed approach is a simple combination of existing methods and I cannot see a novel insight or a novel contribution from the machine learning side.**\n\n\nAlthough our algorithm does employ two well-understood components, we stress the novel insight is in how they are used. While non-negative tensor factorization (NTF) has been used in analyzing neuronal responses (e.g., [1], [2]), no one has previously used it successfully to define neural communities. Our critical contribution is the tensor dimension: *we seek to organize the affinities between pairs of neurons directly*. It is these neural pairs that lead, through the pipeline, to dynamic neural communities. Attempts to build tensor dimensions on the neurons directly could not accomplish this level of community organization.\n\nTo elaborate, instead of implicitly computing similarities between the traces within the NTF computation (which, under the hood, would cause their comparison to be based entirely on linear dot products), we explicitly pre-compute our nonlinear differential affinities. If we had worked with the activities directly it would have yielded tensor factorizations based on how similar the individual traces were; our approach goes one order higher and organizes the affinities directly. Working with individual neurons would produce factors based on how similar the raw traces were. \n\nThere is another advantage to our approach to teasing apart the functional connectome in neural networks. We use a novel nonlinear differential affinity, which enables (i) a differential comparison between neural activity traces that is not dependent on the actual calcium fluorescence values, and (ii)  accounts for both positive and negative relationships. Other recent studies have been limited to using traditional correlation-like measures of similarity between activity traces, and failed to find the neural interactions that we reveal in this paper (e.g., [3], [4], [5], [6]).\nBecause most of the affinity computation is developed in Appendix A.1, we are adding a sentence to the main text of the manuscript to highlight this novelty.\n\n[1] Williams, A. H., et al. (2018). Unsupervised discovery of demixed, low-dimensional neural dynamics across multiple timescales through tensor component analysis. Neuron, 98(6), 1099-1115.\\\n[2] Dyballa, L., et al. (2023). Population encoding of stimulus features along the visual hierarchy. bioRxiv.\\\n[3] Susoy, V., et al. (2021). Natural sensory context drives diverse brain-wide activity during C. elegans mating. Cell, 184(20), 5122-5137.\\\n[4] Randi, F., et al. (2022). Neural Signal Propagation Atlas of C. Elegans. arXiv preprint arXiv:2208.04790.\\\n[5] Yemini, E., et al. (2021). NeuroPAL: a multicolor atlas for whole-brain neuronal identification in C. elegans. Cell, 184(1), 272-288.\\\n[6] Kato, S., et al. (2015). Global brain dynamics embed the motor command sequence of Caenorhabditis elegans. Cell, 163(3), 656-669.\\\n\n**It would be great if the authors could provide more information about the efficiency of the method.**\n\nThank you for pointing this out. We realize this information is highly relevant in a context where experiments can collect increasing amounts of data. One of the reasons behind our choice of algorithm for NTF (HALS) was precisely its efficiency; it has been thoroughly benchmarked against other popular algorithms in [1], using a variety of real-world datasets.\n\nWe have modified the manuscript to include this information (Sec. 2.2, 4th paragraph). For reference, our own experiments with running HALS on our dataset took about 35 s on a 3.1 GHz Dual-Core Intel Core i5 MaxBookPro laptop with 16GB of memory.\n\n[1] A.-H. Phan and A. Cichocki. Multi-way nonnegative tensor factorization using fast hierarchical alternating least squares algorithm (HALS). In Proc. of The 2008 International Symposium on Nonlinear Theory and its Applications, Budapest, Hungary, 2008."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700103259785,
                "cdate": 1700103259785,
                "tmdate": 1700103259785,
                "mdate": 1700103259785,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gH4C6caIoJ",
                "forum": "ZwhHSOHMTM",
                "replyto": "QWFupi6rtL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3970/Reviewer_6VUY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3970/Reviewer_6VUY"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the new results and for answering my questions. I have raised my score to 6.\n\n\nWhile I appreciate the contributions of the paper from the neuroimaging and application side, I still believe that the novelty from the machine learning side is limited, which is the reason that I didn\u2019t raise my score to 8."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586193239,
                "cdate": 1700586193239,
                "tmdate": 1700586296169,
                "mdate": 1700586296169,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2qnQXrY48L",
            "forum": "ZwhHSOHMTM",
            "replyto": "ZwhHSOHMTM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3970/Reviewer_vGem"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3970/Reviewer_vGem"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel analysis method to infer time-varying functional connectomes from neuronal data in general and calcium imaging data in C. elegans in particular. The method is based on a three-step procedure. First, time-varying affinities between pairs of neurons are computed based on concurrent changes of the neuronal signal. Second, a non-negative tensor decomposition is employed to identify neuronal-temporal motifs of affinity across animals. Thirdly, community structure is inferred from the motifs using a stochastic block model. Taken together, these steps enable an interesting visualization of dynamical functional connectomes. The authors apply their method to experimental data recorded in C. elegans during a stimulus avoidance / attraction paradigm and identify a neuron previously not implicated in salt sensing. They then experimentally test its role by exposing worms with / without the neuron silenced to a salt stimulus and find that indeed the worms respond behaviorally as predicted."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors identify and address a highly relevant problem, i.e., the issue that relationships between neurons are highly dynamic yet few algorithms are able to infer dynamical functional connectomes. Their new methods enables a new visualization of this complex, high-dimensional data and can be used to derive experimentally testable predictions on the time-dependent involvement of neurons in behaviorally relevant neuronal ensembles. The authors further test and validate one specific hypothesis experimentally in a behavioral experiment. Another strength is that the manuscript is very well written and easy to read."
                },
                "weaknesses": {
                    "value": "Some of the algorithmic choices appear rather ad-hoc without a rigorous theoretical or neurophysiological justification. In particular, it is unclear to me why the problems in constructing a time-varying similarity measure, that the authors discuss in the second paragraph of Section 2.1, does not also apply to the derivatives of the calcium traces? Since the derivatives represent the influx/outflux of calcium, and are thus likely a better representation of the neurons' firing rates, I would think that similar problems persist? Also, it is not clear to me why the local differential affinities should be non-negative? One could argue that two neurons also form a network if one inhibits the other, which in my understanding would lead to a negative affinity?\n\nA further (and significant) weakness is that no link to code is provided in the manuscript. I believe that making all code publicly available is absolutely essential for reproducibility."
                },
                "questions": {
                    "value": "My most relevant question is regarding code availability -- why has the code not been made available, and how do the authors intend to remedy that situation?\n\nFurther questions are minor ones:\n\n* Since the affinity matrices are symmetric, vectorizing these (and using Euclidean norms) does not seem to be the right choice here? Have you looked into proper distance metrics for symmetric matrices [1]?\n* What does \"CP\" stand for on page 4? That abbreviation is not introduced?\n* Why was the set of neurons restricted to sensory and inter-neurons? Did the results change when using all neurons?\n* Would the method also work on other neuronal data modalities, e.g., spiking data?\n\n1. Vemulapalli, Raviteja, and David W. Jacobs. \"Riemannian metric learning for symmetric positive definite matrices.\" arXiv preprint arXiv:1501.02393 (2015)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3970/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699553629528,
            "cdate": 1699553629528,
            "tmdate": 1699636357943,
            "mdate": 1699636357943,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "st4fFfJJaE",
                "forum": "ZwhHSOHMTM",
                "replyto": "2qnQXrY48L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3970/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the attentive reading, your comments, questions, and suggestions.\n\n**why has the code not been made available, and how do the authors intend to remedy that situation?**\n\nWe agree with the reviewer that the code should be fully available. Our plan is to make it available on github, so it may become a standard technique in computational neuroscience; we did not include a link due to the anonymity clause. To illustrate our intention, we now attach a zip file to the submission containing the source code for our method.\n\n**it is unclear to me why the problems in constructing a time-varying similarity measure (...) does not also apply to the derivatives of the calcium traces? Since the derivatives represent the influx/outflux of calcium (...) I would think that similar problems persist?**\n\nYou are correct in that if the derivatives were to be compared using standard approaches such as Euclidean distance or cosine similarity, the same issues would persist. However, as discussed in detail in Appendix A.1, the comparison between derivatives is highly nonlinear:\n\nFirst, discrete time windows within which to compute local affinities are restricted to periods during which a neuron's derivative has constant sign, i.e., periods of monotonic influx or outflux of calcium. Then, the affinity is computed as the normalized intersection between the integrated calcium influx/outflux of each neuron (areas under the neurons' derivative curves). This approach acts to effectively eliminate the pitfalls presented in the second paragraph of Sec. 2.1, as has been empirically demonstrated.\n\n**it is not clear to me why the local differential affinities should be non-negative? One could argue that two neurons also form a network if one inhibits the other, which in my understanding would lead to a negative affinity?**\n\nYes: negative affinities are not only possible, they indeed frequently occur in our data. That is precisely the reason why we use absolute values: our goal is for the affinity to represent a (non-negative) \u201cprobability\u201d that two neurons interact, regardless of the qualitative nature of that interaction (excitation or inhibition).\n\nAlthough the affinity derivation is developed in detail in Appendix A.1, based on your comments we now realize that this particular feature of our method should be more clearly stated in the main text. So we have updated the manuscript to include in Sec. 2.1, 4th paragraph (new text in bold):\n\n*\"Affinities show how similar two curves are, locally, in terms of their absolute derivatives, __which is motivated by the fact that two neurons with very similar but opposite sign derivatives are still likely to be interacting by means of some inhibitory mechanism.__ As a result, they can be interpreted as how likely it is for two neurons to be interacting\"*\n\n\n**Since the affinity matrices are symmetric, vectorizing these (and using Euclidean norms) does not seem to be the right choice here?**\n\nThanks for the suggestion as well as the interesting reference. Note, however, that in tensor factorization we do not compare affinity matrices as a whole. Rather, because tensor factorization is based on an alternating least-squares (ALS) approach, at each iteration the algorithm solves a problem of the form:\n\n$\\mathbf{A} \\leftarrow \\textup{arg} \\min_{\\mathbf{A}} \\left \\|\\mathbf{X}_{(1)} - (\\mathbf{C}\\odot \\mathbf{B}) \\mathbf{A}^{T}  \\right \\|$\n\n$\\mathbf{B} \\leftarrow \\textup{arg} \\min_{\\mathbf{B}} \\left \\|\\mathbf{X}_{(2)} - (\\mathbf{C}\\odot \\mathbf{A}) \\mathbf{B}^{T}  \\right \\|$\n\n$\\mathbf{C} \\leftarrow \\textup{arg} \\min_{\\mathbf{C}} \\left \\|\\mathbf{X}_{(3)} - (\\mathbf{B}\\odot \\mathbf{A}) \\mathbf{C}^{T}  \\right \\|$\n\nwhere $\\mathbf{X}_{(i)}$ is the mode-$i$ matricization of the tensor $\\mathbf{X}$ (following the notation from [1]). Therefore, the Euclidean distance here is never actually computed with respect to any of the symmetric affinity matrices (slices of $\\mathbf{X}$), but rather to non-square, matricized versions of the entire tensor. It would therefore be non-trivial to adapt these techniques to our case. Of course, it is possible in the future that we will find an alternative metric which is better, but for now the standard approach suffices.\n\n[1] Rabanser, S., Shchur, O., & G\u00fcnnemann, S. (2017). Introduction to tensor decompositions and their applications in machine learning. arXiv preprint arXiv:1711.10781.\n\n[1] Kolda, T. G., & Bader, B. W. (2009). Tensor decompositions and applications. SIAM review, 51(3), 455-500."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700102884567,
                "cdate": 1700102884567,
                "tmdate": 1700102884567,
                "mdate": 1700102884567,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TgL2jU1QLp",
                "forum": "ZwhHSOHMTM",
                "replyto": "st4fFfJJaE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3970/Reviewer_vGem"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3970/Reviewer_vGem"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifications. They have reinforced my assessment that this is very solid and interesting work that is likely to be influential in neurobiology. I do acknowledge the criticism of one of the other reviewers that the ML/AI novelty is limited. But I do think that (in line with the Call for Papers) \"applications to neuroscience & cognitive science\" have a place at ICLR."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669846764,
                "cdate": 1700669846764,
                "tmdate": 1700669846764,
                "mdate": 1700669846764,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]