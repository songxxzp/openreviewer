[
    {
        "title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment"
    },
    {
        "review": {
            "id": "3VCBcueAyR",
            "forum": "uMAujpVi9m",
            "replyto": "uMAujpVi9m",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3164/Reviewer_bxUv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3164/Reviewer_bxUv"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a novel self-supervised pretraining approach called ProFSA to learn effective pocket representations by leveraging protein-only data. The key idea is to extract pseudo-ligand-pocket pairs from proteins by segmenting structures into fragments and designating the surroundings as pockets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors present a novel pairwise data synthesis pipeline by extracting pseudo-ligand-pocket pairs from protein data\n2. The authors develop large-scale datasets and new pretraining methods to exploit the full potential of pocket representation learning, emphasizing the interactions between pockets and ligands.\n3. ProFSA achieves significant performance gains in a variety of downstream tasks."
                },
                "weaknesses": {
                    "value": "1. The evidence for the construction of the pseudo-ligand is not clear.\n2. Ablation studies evaluating the impact of critical design choices like fragment sizes, distance thresholds for pockets would provide useful insights.\n3. While terminal corrections are applied to address biases from breaking peptide bonds, the pseudo-ligands may still exhibit substantial discrepancies from real drug-like ligands."
                },
                "questions": {
                    "value": "1. Why do the authors choose peptides to replace small molecules, and is this choice reliable? Have the authors considered other potential ways to further close the gap between pseudo-ligands and real ligands, either through data processing strategies or by fine-tuning on downstream tasks?\n2. Section 3.1, second paragraph, line 4, what do the N Terminal and C Terminal refer to?\n3. Why fixed the molecule encoder in contrastive learning, i.e., the encoder that encodes the pseudo-ligand.\n4. Could ProFSA be extended to other tasks like protein-protein interaction prediction? How might the pipeline and contrastive approach need to be adapted?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3164/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730663384,
            "cdate": 1698730663384,
            "tmdate": 1699636263882,
            "mdate": 1699636263882,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pA613JNjrY",
                "forum": "uMAujpVi9m",
                "replyto": "3VCBcueAyR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3164/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3164/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Response 1/2\n\n## Response to evidence for the construction of the pseudo-ligand and using peptides to replace small molecules \n\nAs we discussed in the paper, pseudo-ligands share similar sizes as real ligands, and they also make similar non-covalent interactions with the pocket. To further support our proposal, we would like to cite another paper **\"A defined structural unit enables de novo design of small-molecule\u2013binding proteins\"**[1], published in Science 2020, which supports that **intra-protein interactions are similar to protein-ligand interactions.** That is the reason we use peptides to **represent** small molecules to let the pocket encoder learn the interaction information. \n\nAlso, we'd like to clarify that our intention is not to **\"replace\"** small molecules with peptides. It's a pretraining framework that leverages abundant protein-only data and uses peptides to **simulate** pocket-small molecule interactions for enhanced pocket representations. Following this pretraining phase, **the model can be finetuned with datasets that include real small molecules, ensuring its effectiveness in practical applications**.\n\nTo demonstrate the validity of constructing pseudo-ligands, we provide visualizations in **Appendix F** showing that interactions between pockets and our pseudo-ligands are similar to those with true ligands. These figures reveal shared interaction types like **hydrogen bonding**, **$\\pi-\\pi$ stacking**, and **salt bridge**, depicted with color-coded dashed lines for each interaction type. This should help clarify the rationale and evidence supporting our approach.\n\n[1] Polizzi, Nicholas F., and William F. DeGrado. \"A defined structural unit enables de novo design of small-molecule\u2013binding proteins.\" Science 369, no. 6508 (2020)\n\n## Response to ablation studies evaluating the impact of critical design choices\n\nThanks for your advice on adding more ablation studies on design choices. We did ablation studies on the effectiveness of distributional alignment, where the major difference is that fragment sizes are modified. We found that aligned fragment sizes with real ligands could provide the best performance. The result is shown in **Table 5**.\n\nAs for the distance thresholds, we define the pocket following the UniMol setup. We also found our model can also adapt to an 8\u00c5 setup in the ToughM1 experiment. Following your advice, we did an ablation study on the distance thresholds. We tested our method with three different thresholds: 4\u00c5, 6\u00c5, and 8\u00c5. The result is shown below:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| 4\u00c5 | 0.7062 | 0.7549 | 0.1240 | 0.1095 | 28.29 | 13.07 |\n| 6\u00c5 | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| 8\u00c5 | 0.8322 | 0.8292 | 0.1256 | 0.1125 | 34.83 | 12.92 |\n\nSince the 8\u00c5 threshold corresponds with the pocket definition in the Kahraman and Tough M1 datasets, it leads to optimal results. Our decision to use a 6\u00c5 threshold was made to align with the methodology of pretraining data creation by Uni-Mol, facilitating a fair comparison. Notably, even with the 6\u00c5 threshold, we achieved strong results in the pocket-matching task, which serves as a testament to the effectiveness of our approach.\n\n## Response to substantial discrepancies between pseudo and real ligands\n\nThank you for pointing out the discrepancies between pseudo and real ligands. We understand and acknowledge your concern. To mitigate the impact of this discrepancy, we have made several efforts. First, we performed **distribution alignment** to make the data distribution of pseudo-ligands more similar to that of true ligands in PDBbind. Additionally, we **fixed the molecule encoder during pretraining** to prevent it from being misled by the discrepancy. Our ablation studies have shown that these strategies are effective:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nThe table above shows that lacking distribution alignment and not fixing the molecular encoder during pretraining both lead to lower performance, underscoring the effectiveness of our methods in reducing discrepancies.\n\nTheorem 3.1 theoretically supports our approach's efficacy, even with discrepancies between pseudo and real ligands. Empirically, our method outperforms other pretraining techniques, confirming its effectiveness despite these discrepancies."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235690165,
                "cdate": 1700235690165,
                "tmdate": 1700235690165,
                "mdate": 1700235690165,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ROjDyy9bas",
                "forum": "uMAujpVi9m",
                "replyto": "3VCBcueAyR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3164/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3164/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Explanation of the newly revised paper"
                    },
                    "comment": {
                        "value": "In our revised paper, we have incorporated changes following feedback from another reviewer, which we believe also addresses your concerns and queries.\n\nRegarding your question on the evidence supporting our approach, particularly the use of peptides to simulate small molecules, we've enriched **Section 3.1**. A new paragraph there now provides an in-depth explanation of non-covalent interactions. To aid understanding, **Figure 2** has been updated with visualizations that include both real and pseudo receptor-ligand pairs, illustrating three types of interactions. Furthermore, we delve into the details of **how intra-protein interactions mirror protein-ligand interactions**, involving specific types of amino acids.\n\nTo better evaluate our design choices, the ablation studies have been extended. Alongside the existing studies in **Table 4** (different pretrained molecular encoders), **Table 5** (distribution alignment), and **Figure 5** (data scale). we've added a study on the impact of using a fixed molecular encoder in **Table 5** in the main text to address your question on the fixed molecular encoder. We've also included an ablation study on varying **cutoff values for pocket definition**, placed in the **Appendix C.4** due to its more focused scope. We can move it to the main part of the paper in the future if needed.\n\nWe acknowledge your concern about the **differences between true and pseudo ligands**, so we've expanded our discussion on the distinct properties of our curated dataset versus the PDBBind dataset in **Figure 9** and **Section 3.2**. The ablation study in **Table 5** demonstrates our methods and their effectiveness in minimizing these discrepancies. Notably, the performance significantly drops without our distribution alignment or fixed molecular encoder, highlighting our efforts to mitigate these issues. The updated **Figure 2** further supports this, showing that while the properties of true and pseudo ligands may differ, **the interaction types are consistent.** We hope these results can ease your concerns about our synthetic dataset, and they also respond to your query about **why we kept the molecular encoder constant during pocket pretraining.**\n\nWe appreciate the constructive critiques and thoughtful engagement from you. Your feedback has undeniably contributed to the refinement and strengthening of our paper. We sincerely value the time and expertise invested in the review process, and we look forward to any further suggestions you may have."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560380064,
                "cdate": 1700560380064,
                "tmdate": 1700560380064,
                "mdate": 1700560380064,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Jv6QGKyDWD",
            "forum": "uMAujpVi9m",
            "replyto": "uMAujpVi9m",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3164/Reviewer_kXw3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3164/Reviewer_kXw3"
            ],
            "content": {
                "summary": {
                    "value": "This paper enhances protein pocket pretraining by introducing a new large pseudo ligand-pocket dataset. The dataset is constructed by segmenting a fragment from a protein and treating the neighboring area of the fragment as a pocket. Several important strategies are adopted to make the generated fragment-pocket pairs more like real ligand-pocket pairs. This results a dataset with 5.5 million pseudo ligand-pocket pairs. Contrastive learning is conducted using the generated dataset, in which a pretrained small molecular encoder is used to extract features for the fragments to align with a pocket encoder to be pretrained. Experiments are conducted on both pocket-only tasks and a pocket-molecule task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.The strategy of constructing pseudo ligand-pocket dataset is novel and has the potential to be extended to construct larger datasets. \n\n2.Effective strategies are introduced to make the pesudo ligand-pocket pairs effective to mimic real ones and a practical contrastive learning strategy is adopted to address the difference between the segmented fragments from real ligands."
                },
                "weaknesses": {
                    "value": "1. One weakness is that the proposed method is only evaluated on limited tasks. \n\n2. The baselines in the  experiment are quite old, with the latest method published in 2020 except Uni-Mol."
                },
                "questions": {
                    "value": "1. Will the proposed method work on other tasks, such as protein-ligand binding pose prediction?\n2. Is there any new methods on the POCKET MATCHING task? If so, please include them in comparison."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3164/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752480728,
            "cdate": 1698752480728,
            "tmdate": 1699636263752,
            "mdate": 1699636263752,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RYuWPf2mV2",
                "forum": "uMAujpVi9m",
                "replyto": "Jv6QGKyDWD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3164/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3164/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Response to limited tasks and potential in protein-ligand binding pose prediction\n\nIn our paper, the primary objective is to assess the effectiveness of our pretraining method and the quality of the trained pocket representations. To achieve this, we focused on two types of downstream tasks: pocket-only tasks (pocket property prediction and pocket matching) and pocket-ligand interaction tasks (ligand binding affinity prediction).\n\nWe appreciate the suggestion to apply our method to the protein-ligand binding pose prediction task. However, it's important to note that our current evaluation framework is designed to specifically assess **pocket representations**. For the tasks we chose, the architecture is straightforward: either a **zero-shot** evaluation or a **simple MLP** for mapping embeddings to predictions. In contrast, protein-ligand binding pose prediction often involves **complex methodologies**. For instance, state-of-the-art methods like DiffDock[1] require training a diffusion generative model and a separate confidence model, while other approaches like EDM-Dock[2], rely on reconstructing ligand conformations from predicted distance maps. These methods are not end-to-end and do not directly align with our objective of evaluating pretrained pocket representations.\n\nNevertheless, we recognize the potential of our method in enhancing existing binding pose prediction techniques. To integrate our approach, we would need to modify our framework. Using the same data creation strategy, we could train a binding pose prediction model with our preprocessed data, which could then be further fine-tuned using real pocket-ligand pair data from sources like PDBbind. Additionally, our data could be used to train a side-chain packing model, allowing for side-chain flexibility during docking. Thank you again for your advice, and we will leave protein-ligand binding pose prediction as a separate future work for our method.\n\nIn response to the need for evaluating our method on a broader range of tasks, we have extended our analysis to include two additional downstream tasks: LEP (Ligand Efficacy Prediction), and PPA (Protein-Protein Affinity Prediction).\n\nresult for LEP:\n\n| Method      | AUROC $\\uparrow$ | AUPRC $\\uparrow$ |\n|-------------|--------------|--------------|\n| ATOM3D-GNN  | 0.681        | 0.598        |\n| GeoSSL      | 0.776\u00b10.03   | 0.694\u00b10.06   |\n| Uni-Mol     | 0.782\u00b10.02   | 0.695\u00b10.07   |\n| ProFSA      | 0.840\u00b10.04   | 0.806\u00b10.04   |\n\nGeoSSL and Uni-Mol, both pretraining methods, yield comparable results. However, ProFSA outperforms these methods, demonstrating the advantage of our pocket pretraining approach.\n\nresult for PPA:\n\n| Method            | Spearman $\\uparrow$  |\n|-------------------|----------------------|\n| SchNet            | 0.072 \u00b1 0.021        |\n| DimeNet++         | 0.171 \u00b1 0.054        |\n| EGNN              | 0.080 \u00b1 0.038        |\n| TorchMD           | 0.117 \u00b1 0.008        |\n| GET               | **0.363 \u00b1 0.017**    |\n| ProFSA(zero shot) | _0.248_              |\n\nAs a **zero-shot** method, ProFSA is able to outperform other **supervised learning** models except for GET, a newly proposed unified model. This demonstrates our approach is able to capture protein-protein interaction information despite it being designed for protein-ligand interaction modeling.\n\nYou can find detailed experiment settings and results in **Appendix C5 and C6**.\n\n[1] Corso et al., \"Diffdock: Diffusion steps, twists, and turns for molecular docking.\", ICLR 2023.\n[2] Masters et al., \"Deep learning model for efficient protein\u2013ligand docking with implicit side-chain flexibility.\" Journal of Chemical Information and Modeling 63, no. 6 (2023).\n\n## Response to the issue of lack of latest baselines\n\nFor the pocket matching task, CoSP is a newly proposed baseline which is published in ECML PKDD 2023. Alongside CoSP and Uni-Mol, we selected some of the most effective baseline results presented in the CoSP paper. We also tried to evaluate recent methods like PocketAnchor (Li et al., Cell Systems 2022) on our own since they are not tested on the pocket matching task. However, since the result didn't outperform other baseline machine learning methods(Uni-Mol, CoSP, and DeeplyTough), we decided not to include it in our final baseline comparison. We can include it in the camera-ready version if needed.\n\nFor the ligand binding affinity task, a lot of newly proposed baselines are included, i.e. ProNet (Wang et al., 2022b); as well as pretraining methods such as GeoSSL (Liu et al., 2023), EGNN-PLM (Wu et al., 2022), DeepAffinity (Karimi et al., 2019) and Uni-Mol (Zhou et al., 2023)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235364633,
                "cdate": 1700235364633,
                "tmdate": 1700235364633,
                "mdate": 1700235364633,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HI1xwE05Mh",
                "forum": "uMAujpVi9m",
                "replyto": "Jv6QGKyDWD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3164/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3164/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Anticipating your response"
                    },
                    "comment": {
                        "value": "Thank you for your insightful feedback. In response to your concerns regarding the scope of our tasks, we have expanded our research in the revised paper. We now include additional tasks such as **ligand efficiency prediction** and **protein-protein affinity prediction**, detailed in **Appendix C5 and C6**. These tasks were incorporated to enhance the robustness of our method. We acknowledge that they are currently in the appendix due to space constraints, but we are open to relocating them to the main text if it would be beneficial.\n\nRegarding your concern about the lack of recent baselines, we provided a detailed explanation in our previous response. We want to reiterate that our benchmarks do include the latest baselines, with some as recent as 2023.\n\nWe greatly appreciate the time and effort you have invested in reviewing our work. Your feedback has been invaluable in refining our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656823113,
                "cdate": 1700656823113,
                "tmdate": 1700656823113,
                "mdate": 1700656823113,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LbKCNmwdYS",
            "forum": "uMAujpVi9m",
            "replyto": "uMAujpVi9m",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3164/Reviewer_FNxz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3164/Reviewer_FNxz"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel approach called ProFSA for pretraining pocket representations based on the guided fragment-surroundings contrastive learning. Furthermore, a novel scalable pairwise data synthesis pipeline is designed to extract pseudo-ligand-pocket pairs from protein-only data. Extensive experiments demonstrate the potential of ProFSA as a powerful tool in the field of drug discovery."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-structured and clearly articulates the research methodology and findings. The overall presentation is easy for readers to grasp the key ideas of this paper.\n\n2. By utilizing pseudo-ligand construction and pocket construction, authors develop an innovative strategy for mining extensive protein-only data from the PDB repository, which can effectively alleviate the scarcity of experimentally determined pocket-ligand pairs.\n\n3. A contrastive learning approach in the protein-fragment space is introduced to attain ligand-aware pocket representations. By sampling negative samples from protein pockets and pseudo-ligands, the pocket encoder can learn to identify the true positive sample when given the other one.\n\n4. Extensive experiments demonstrate the potential of ProFSA as a powerful tool in the drug discovery field."
                },
                "weaknesses": {
                    "value": "1. I'm not fully satisfied with the Related Work section. More work should be presented, such as [1], [2] and [3].\n\n2. Why is COSP introduced as a component of the pocket pretraining method in Section 2.2, but not included as a baseline in Table 3?\n\n3. In section 3.2, the authors mention that the \"the first loss is to differentiate the corresponding ligand fragment from a pool of candidates for a given pocket.\" The first loss is constructed by sampling negative samples from protein pocket. Therefore, I think the purpose of the first loss is to identify the true protein pocket when given a pseudo-ligand.\n\n4. I am confused about how ProFSA works without the distributional alignment  mechanism. In this context, what determines the length of the pocket representation?\n\n[1] Liu S, Guo H, Tang J. Molecular geometry pretraining with se (3)-invariant denoising distance matching[J]. arXiv preprint arXiv:2206.13602, 2022.\n[2] Wu F, Li S, Wu L, et al. Discovering the representation bottleneck of graph neural networks from multi-order interactions[J]. arXiv preprint arXiv:2205.07266, 2022.\n[3] Karimi M, Wu D, Wang Z, et al. DeepAffinity: interpretable deep learning of compound\u2013protein affinity through unified recurrent and convolutional neural networks[J]. Bioinformatics, 2019, 35(18): 3329-3338."
                },
                "questions": {
                    "value": "Please see the questions in weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3164/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3164/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3164/Reviewer_FNxz"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3164/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803235306,
            "cdate": 1698803235306,
            "tmdate": 1699636263688,
            "mdate": 1699636263688,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rK6Us6OcGE",
                "forum": "uMAujpVi9m",
                "replyto": "LbKCNmwdYS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3164/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3164/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Response to lack of related works\n\nThanks for providing us with more insightful related works that could support our arguments. We actually have cited these three papers in the section on ligand binding affinity experiments, but we are also happy to add these in Related Works section 2.2. You can find the change in pdfdiff.\n\n## Response to lack of COSP as a baseline in Table 3\n\nBecause best to our knowledge, they did not release their code and they did not test their method on the ligand binding affinity dataset. \n\n## Response to the question on the loss terms\n\nThank you for your advice. We apologize that we accidentally mentioned loss1 and loss2 in reverse, and we are sorry for the confusing statement. You are correct that the first loss is to identify the true protein pocket when given a pseudo-ligand. We revise the original statement to: \"The primary purpose of the first loss is to identify the true protein pocket from a batch of samples when given a pseudo-ligand. Similarly, the second loss seeks to identify the corresponding ligand fragment for a given pocket.\" You can also find the change in pdfdiff.\n\n## Response to the question on the length of the pocket representation without alignment\n\nWith or without distributional alignment, pockets are always defined by the given protein fragment with a fixed distance cutoff (6\u00c5 in our works, following the UniMol setup). The distributional alignment process merely samples these pairs to match the sizes of real ligands and pockets. Without distributional alignment, fragments are uniformly sampled from 1 to 7 residues. Pockets are similarly determined as with residues within the range of 6\u00c5 around peptide fragments."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235009258,
                "cdate": 1700235009258,
                "tmdate": 1700235009258,
                "mdate": 1700235009258,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QWt3dHhACG",
                "forum": "uMAujpVi9m",
                "replyto": "LbKCNmwdYS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3164/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3164/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Anticipating your response"
                    },
                    "comment": {
                        "value": "Thank you for your invaluable feedback on our paper. In response to your insightful suggestions, we have made several modifications in our revised paper. \n\nWe have cited additional papers in Section 2.2 following your advice. The description of loss terms in Section 3.2 has been corrected.  COSP was not included as a baseline for the Ligand Binding Affinity (LBA)  task because it is not open-sourced, and its performance on LBA was not evaluated in their published work. We used it as a baseline in the pocket-matching task.\n\nIn Section 3, we've expanded our discussion to answer your question on **ProFSA's effectiveness without distributional alignment**. This section now offers a detailed explanation of the foundational concepts and justifications for our method. We draw attention to the parallels between ligand-protein and intra-protein non-covalent interactions, as shown in **Figure 2**. This comparison supports our strategy of using peptides as stand-ins for actual ligands to mimic pocket-ligand interactions. Consequently, **even in the absence of distributional alignment, the types of interactions remain comparably relevant**, which explains why ProFSA continues to function, albeit with reduced effectiveness, as demonstrated in Table 5. We also want to clarify that distributional alignment is only applied to fragment-pocket complex as a whole, and **it would not change the definition of the pocket with a given ligand**. Without this alignment, these fragments are chosen uniformly, with lengths varying from 1 to 7 residues, and pockets are consistently identified based on residues within a 6\u00c5 radius of these peptide fragments.\n\nWe are grateful for the time and effort you have dedicated to reviewing our work. Your thorough and constructive feedback has significantly contributed to the refinement of our research. Please let us know if there are any other aspects of our paper that you would like us to address or clarify. We look forward to your further feedback."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659534597,
                "cdate": 1700659534597,
                "tmdate": 1700659534597,
                "mdate": 1700659534597,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9ap4s1SZlZ",
            "forum": "uMAujpVi9m",
            "replyto": "uMAujpVi9m",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3164/Reviewer_yfM3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3164/Reviewer_yfM3"
            ],
            "content": {
                "summary": {
                    "value": "This paper primarily aims to enhance the pocket pretraining method, as existing approaches only consider pockets during pretraining. There are two main contributions in this paper: (1) The authors introduce a novel method, ProFSA, for pocket pretraining, which extracts additional information from corresponding ligands. However, the number of pocket-ligand complex structures is quite limited in existing datasets. (2) To address this issue, the authors generate over 5 million complexes by segmenting fragments and their corresponding pockets in protein structures. By aligning features of fragments and pockets, the pocket encoder learns the interaction between fragments and pockets. The authors design downstream tasks such as pocket druggability prediction, pocket matching, and ligand binding affinity prediction to demonstrate the effectiveness of ProFSA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors propose a new perspective of pretraining pockets and construct a large-scale dataset, which data distribution is also considered, to make the efficient pre-training possible.\n\nThe results are competitive, especially for zero-shot settings.\n\nAbundant experiments and ablation study support the argument and result of the authors."
                },
                "weaknesses": {
                    "value": "1. The technical novelty is limited.\n  - The pocket encoder is borrowed from Uni-Mol.\n  - The contrastive loss is the vanilla form of classical contrastive learning.\n\n2. The bound of Theorem 3.1 is trivial. The authors claim that the bound naturally exists for these representations extracted by pretrained molecule models. However, it's a bit counterintuitive, because many models not pretrained on molecule datasets also fulfill this prior. So, can these models be used for this task? **I strongly suggest removing this part from the paper**.\n\n3. Some issues about dataset creation:\n - 3.1. The authors consider the distribution of ligand size and pocket size when designing the dataset. However, molecules possess more properties that can also lead to imbalance. It would be better to, at least, add some discussion about this issue.\n - 3.2. In the second stage of the data construction process, the approach to defining pockets needs further explanation or an ablation study.\n\n4. Experiments: It would be better to add some biological justification or visualization of the results.\n\nFor this paper, one fact is that the technical novelty is below the bar of ICLR. However, I admire the simple but effective model for the right question. It's a struggle for me to make a decision. I will maintain a neutral attitude and make my final decision after the discussion."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3164/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3164/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3164/Reviewer_yfM3"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3164/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835886192,
            "cdate": 1698835886192,
            "tmdate": 1700545194816,
            "mdate": 1700545194816,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VI2DnaxHOD",
                "forum": "uMAujpVi9m",
                "replyto": "9ap4s1SZlZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3164/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3164/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Response 1/3\nWe appreciate the time and effort you have dedicated to reviewing our paper, and we are grateful for your constructive feedback and thoughtful evaluation of our work. We would like to address your comments about the technical novelty of our work and provide additional clarification on certain aspects of our paper.\n\n## Response to technical novelty\n\nFirstly, we acknowledge your observation that we didn't use any fancy models compared with typical **model-centric** works. While we respect your assessment, we would like to highlight that our primary focus in this paper is a **data-centric** pretraining method to introduce groundbreaking improvements in the field of protein pocket pretraining. Our work aims to address a critical challenge of data scarcity in the field of protein pocket representation by constructing large-scale synthetic data that facilitates the pretraining of models, ultimately enhancing the accuracy and robustness of protein pocket representations. We believe that the research community can conduct fast following-ups on our **released dataset** with more sophisticated models, and even extend our pipelines to other tasks like docking or drug design. We also would like to point out that we have provided a novel method to **distill knowledge from well-trained molecule models** to protein models. Though employing the same contrastive loss, our approach is different from existing contrastive learning models since it uses a fixed molecular encoder. The motivation here is to use a well-trained molecular encoder on a relatively larger dataset to **guide** the training of the protein encoder. For example, some quantum-chemistry properties that are difficult to compute for large systems like proteins could be distilled from molecules, by using some quantum-chemistry-aware molecular encoders such as Frad. Though our framework seems to be simple, it is non-trivial to make it work. We have made several efforts to solve the unavoidable discrepancies between true ligands and pseudo ligands. The ablation study shows that our efforts are effective and necessary:\n\n|  | Kahraman$\\uparrow$ | Tough M1$\\uparrow$ | Fpocket$\\downarrow$ | Druggability$\\downarrow$ | Total SASA$\\downarrow$ | Hydrophobicity$\\downarrow$|\n| --- | --- | --- | --- | --- | --- | --- |\n| ProFSA | 0.7870 | 0.8178 | 0.1238 | 0.1090 | 31.17 | 12.01 |\n| w/o alignment | 0.7614 | 0.7589 | 0.1265 | 0.1108 | 34.79 | 14.86 |\n| w/o fix mol encoder | 0.6905 | 0.7337 | 0.1247 | 0.1094 | 32.17 | 12.20 |\n\nRegarding your positive acknowledgment of our \"simple but effective model for the right question,\" we are pleased to hear that our approach resonates with the objective we set out to achieve. We designed our model with **simplicity** in mind, prioritizing effectiveness and practical utility for the specific problem domain. This deliberate choice aligns with the notion that sometimes the most impactful solutions are elegantly straightforward. Also, we intentionally borrowed a pocket encoder from Uni-Mol to make a **fair comparison** with it, which strongly supports the power of our dataset. Similar to the molecule encoder, our pocket encoder can be changed to any other model due to our flexible framework. Notably, **we didn't load the pretrained weights of Uni-Mol pocket encoder**. We only use the same backbone architecture and the pretraining was completely done on our processed data with our training strategy. \n\nNotably, many simple but effective approaches have been recognized by top-tier conferences and journals. A prime example of this is in the field of protein language models, like the ESM series and ProtTrans. These studies adapted the Transformer architecture and masked language modeling techniques from natural language processing to protein sequences. While they didn't introduce groundbreaking techniques, their substantial contributions to protein modeling are evident, with publications in prestigious conferences and journals like NeurIPS, ICML, TPAMI, PNAS, and Science. Another example is the widely acclaimed CLIP paper presented at ICML 2021. CLIP, while not employing novel techniques, stands as a hallmark of data-centric deep learning. Its use of contrastive learning enabled training on expansive web-sourced text-image datasets, moving beyond the constraints of meticulously curated databases like MS-COCO. This aligns with our approach to addressing **data scarcity challenges** by facilitating training on large-scale datasets, mirroring our strategy for overcoming similar hurdles in the protein pocket modeling domain.\n\nWe appreciate your efforts to remain neutral and understand the challenges in decision-making. We believe further discussion will highlight our paper's contributions. Your feedback and insights in the upcoming discussion will be invaluable for refining our work."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234069409,
                "cdate": 1700234069409,
                "tmdate": 1700234069409,
                "mdate": 1700234069409,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bZRbDIhMlK",
                "forum": "uMAujpVi9m",
                "replyto": "Ox288MESWo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3164/Reviewer_yfM3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3164/Reviewer_yfM3"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response."
                    },
                    "comment": {
                        "value": "Sorry for the confusion. Regarding the theorem part, I mean it is not necessary to involve Thm. 3.1 to decorate your models, simply removing it should be fine."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323230097,
                "cdate": 1700323230097,
                "tmdate": 1700323230097,
                "mdate": 1700323230097,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K4kU1AOJva",
                "forum": "uMAujpVi9m",
                "replyto": "l6fUzfcFu7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3164/Reviewer_yfM3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3164/Reviewer_yfM3"
                ],
                "content": {
                    "title": {
                        "value": "Biological justification"
                    },
                    "comment": {
                        "value": "For the visualization, I mean you need include some examples in the main part, not just in the appendix. If there are some cases where your model performs well while the other models do not, that would be more convincing."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323489301,
                "cdate": 1700323489301,
                "tmdate": 1700323489301,
                "mdate": 1700323489301,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5tUGm4AwfD",
                "forum": "uMAujpVi9m",
                "replyto": "9ap4s1SZlZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3164/Reviewer_yfM3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3164/Reviewer_yfM3"
                ],
                "content": {
                    "title": {
                        "value": "About the revisions."
                    },
                    "comment": {
                        "value": "By now, I haven't found your revised version of your paper (Correct me if I'm wrong). \nIf in the revision, you can focus more on the problem & data rather than the model & theorem and provide more visualization cases that can help the machine learning community better understand the pocket-ligand prediction problem. I would raise my score.\n\nAdditionally, since your model involves the pretrained mol encoder,  the existing pretraining approaches on molecules should be extensively discussed in the related works."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323943014,
                "cdate": 1700323943014,
                "tmdate": 1700324450982,
                "mdate": 1700324450982,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]