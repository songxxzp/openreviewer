[
    {
        "title": "Privileged Sensing Scaffolds Reinforcement Learning"
    },
    {
        "review": {
            "id": "G3h8JS8rxP",
            "forum": "EpVe8jAjdx",
            "replyto": "EpVe8jAjdx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6525/Reviewer_Sbzi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6525/Reviewer_Sbzi"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose Scaffolder, a model-based RL method that can leverage privileged information at training time. Here, priviledged information means an MDP where:\n\n* There is some true state $s$ with observations $o^+$, where $o^+$ includes privileged info we do not want to assume is available at inference time. (i.e. ground truth object state). This could be used to train a privileged policy $\\pi^+$ that cannot be used as-is for inference.\n* There is an observation $o^-$ for unprivileged / impoverished target observations, which our final policy $\\pi^-$ will depend on.\n* We would like the best $\\pi^-$ possible while leveraging information in $o^+$.\n\nThis problem has been studied in a number of recent works, often in a model free manner. This paper aims to leverage privileged information in a model-based manner.\n\nTo do so, the authors train 2 worlds models. The world model subroutine used is DreamerV3. One models privileged information $o^+$ and the other models target information $o^-$. In this summary I'll call them WM+ and WM-.\n\nA \"latent translator\" is learned to translate WM+ into an observation that WM- can use in its rollouts. Specifically: WM+ has internal latent state $z^+$. We fit a prediction model $p(e^-|z^+)$, where $e^- \\approx emb(o^-)$. Part of DreamerV3 is learning a posterior $q(z_{t+1}|z_t,a_t,e_{t+1}=emb(o_{t+1}))$ that infers latent state from history and current observation. By replacing the impoverised $e^- = emb(o^-)$ with a prediction driven by privileged latent $z^+$, we can channel some privileged information into the rollout of $z^-$, assuming that privileged information is eventually observable in unprivileged information.\n\nThis latent translator lets us use $\\pi^-(a|z^-)$ to rollout both WM+ and WM-, giving a sequence of latents $(z^+,z^-)$ from both world models. The learned reward function is then defined as $R(z^+,z^-)$ to allow observing privileged information in the critic.\n\nWe additionally fit a $\\pi^+$ directly in the privileged world model, using this solely to generate additional exploratory data (it is possible that some exploration behaviors are easier to learn or discover from privileged information). Last, a decoder is trained to map $z^-$ to $o^+$. To me this seems the least motivated, in that not all parts of $o^+$ should be predictable from $z^-$ in the first place, but it seems to empirically be effective.\n\nThe evaluation of Scaffolder is done in a variety of \"sensory blindfold\" tasks, mostly robotics based, where some sensors are defined as privileged and some are not. The method is compared to DreamerV3 on just target information, a few variants of DreamerV3 based on only fitting one world model with decoding of privileged information, and some model free baselines like slowly decaying use of privileged information, asymmetric actor critic, or using BC to fit an unprivileged policy to a privileged one."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper provides a good overview of previous methods for handling privileged information, proposes an evaluation suite for studying the problem of privileged information, and proposes a modification of DreamerV3 that handles the information better than Informed Dreamer. There is a significant amount of machinery around Scaffolder, but it's mostly clear why the added components ought to be helpful for better world modeling and policy exploration. The model-free baselines used are pretty reasonable, and it is shown that Scaffolder still outperforms these model free methods even when the model free methods are given significantly more steps.\n\nFinally, the evaluation suite covers a wide range of interesting robot behaviors and the qualitative exploratory methods discovered to handle the limited state (i.e. spiraling exploration behavior) are quite interesting. The S3 suite looks like a promising testbed for future privileged MDP work, separate from the algorithmic results of the paper."
                },
                "weaknesses": {
                    "value": "In some sense, Scaffolder requires doing 2x the world model fitting, as both the z^- and z^+ models need to be fit for the approach to work. In general, this is \"fair\" for model-based RL, which is usually judged in number of environment interactions rather than number of gradient steps, but it very definitely is a more complex system and this can introduce instability.\n\nA common actor-critic criticism is that the rate of learning between the actor and critic needs to be carefully controlled such that neither overfits too much to the other. Scaffolders seems to take this and add another dimension for the rate of learning between the privileged world model and unprivileged one, as well as the learning speed of the privileged exploratory actor $\\pi^+$ and target actor $\\pi^-$."
                },
                "questions": {
                    "value": "In general, the paper focuses on the online, tabula rasa case where we are in an entirely unfamiliar environment. How adaptable is this method to either the offline case, or the finetuning case where we have an existing policy / world model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6525/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6525/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6525/Reviewer_Sbzi"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6525/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698374961848,
            "cdate": 1698374961848,
            "tmdate": 1699636733822,
            "mdate": 1699636733822,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aoW5d90Xdv",
                "forum": "EpVe8jAjdx",
                "replyto": "G3h8JS8rxP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6525/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6525/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for your review!\n\n> Scaffolder requires doing 2x the world model fitting, as both the z^- and z^+ models need to be fit for the approach to work. In general, this is \"fair\" for model-based RL, which is usually judged in number of environment interactions rather than number of gradient steps, but it very definitely is a more complex system and this can introduce instability. A common actor-critic criticism is that the rate of learning between the actor and critic needs to be carefully controlled such that neither overfits too much to the other. Scaffolders seems to take this and add another dimension for the rate of learning between the privileged world model and unprivileged one, as well as the learning speed of the privileged exploratory actor \n\nWe found the hyperparameter tuning process for Scaffolder, and DreamerV3 in general, to be short and easy. On a completely new environment and task, we found that DreamerV3 only needs tuning for two hyperparameters, the model size and the update to data (UTD) ratio. We follow an easy guideline for tuning these - more complicated dynamics generally require larger models, and tasks with harder exploration require more data and fewer updates (low UTD). \n\nWe did not tune these two DreamerV3 hyperparameters towards Scaffolder - rather, when we used model sizes and UTD ratios from when we ran DreamerV3 with privileged inputs as a reference method used for computing the upper bound scores. These same settings are then used for all DreamerV3 methods (Scaffolder, Informed Dreamer, DreamerV3+BC, Guided Observability) to be consistent.\n\nAs a result, we found hyperparameter settings that work for tasks with similar properties:\n\n- Model / UTD\n- Small / 512: Blind Pick, Blind Locomotion, Wrist Pick-Place, Occluded Pick-Place (Simple Dynamics, Easy Exploration)\n- Large / 512: Blind Deaf Piano (Complex dynamics, Easy Exploration)\n- Large / 16: Noisy Monkey, Blind Pen, Blind Cube, RGB Pen, RGB Cube (Complex Dynamics, Hard Exploration)\n\nNo tuning was needed for the actor-critic learning rate, the world model learning rates, and scaffolded / target policy learning rates.\n\n> In general, the paper focuses on the online, tabula rasa case where we are in an entirely unfamiliar environment. How adaptable is this method to either the offline case, or the finetuning case where we have an existing policy / world model?\n\nThank you for the interesting question. Indeed, we have thought about this too, and believe that Scaffolder may be well-suited for learning from offline data. Many large offline trajectory datasets, such as RT-X [1], have additional privileged information, in the form of metadata, captions, additional modalities, etc. that can be exploited by Scaffolder as privileged sensors to better train the target policy. Training Dreamer style models entirely offline may be challenging, but it may be possible to extend current offline model-based RL frameworks [2,3]  to achieve this.\n\nScaffolder may also be an excellent fit for finetuning policies in settings where we have some prior knowledge. Scaffolder has explicit components to model dynamics models, policies, value functions, exploration policies etc. This  permits injecting varied types of prior knowledge: for example, the privileged dynamics model could be initialized based on prior knowledge and finetuned. Residual learning methods [4], shown to have success in RL settings before, may also be combined with Scaffolder for further improved finetuning performance.\"\n\n\n[1] Padalkar, Abhishek, et al. \"Open x-embodiment: Robotic learning datasets and rt-x models.\" arXiv preprint arXiv:2310.08864 (2023).\n\n[2] Kidambi, Rahul, et al. \"Morel: Model-based offline reinforcement learning.\" Advances in neural information processing systems 33 (2020): 21810-21823.\n\n[3] Yu, Tianhe, et al. \"Mopo: Model-based offline policy optimization.\" Advances in Neural Information Processing Systems 33 (2020): 14129-14142.\n\n[4] Haldar, Siddhant, et al. \"Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations.\" arXiv preprint arXiv:2303.01497 (2023)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6525/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176960982,
                "cdate": 1700176960982,
                "tmdate": 1700176960982,
                "mdate": 1700176960982,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LocwyzTRe6",
            "forum": "EpVe8jAjdx",
            "replyto": "EpVe8jAjdx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6525/Reviewer_bAbn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6525/Reviewer_bAbn"
            ],
            "content": {
                "summary": {
                    "value": "The learning process known as \"sensory scaffolding\" involves novice learners using more sensory inputs than experts. This principle has been applied in this study to train artificial agents. The researchers propose \"Scaffolder,\" a reinforcement learning method that utilizes privileged information during training to optimize the agent's performance.\n\nTo evaluate this approach, the researchers developed a new \"S3\" suite of ten diverse simulated robotic tasks that require the use of privileged sensing. The results indicate that Scaffolder surpasses previous methods and frequently matches the performance of strategies with continuous access to the privileged sensors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper delves into a critical question within the field of reinforcement learning: how can we effectively use privileged information as a 'scaffold' during training, while ensuring the target observation remains accessible during evaluation? This question takes on an added significance in robotic learning, where simulation is a major data source.\n\nWhile there has been considerable research in this area, as detailed in the related work, this paper adds value to the existing body of knowledge, even without introducing novel methods. The proposed method may not be groundbreaking, but it offers a comprehensive examination of this issue from four perspectives: model, value, representation, and exploration.\n\nThis research serves as a valuable resource for those looking to deepen their understanding of the field. The excellent writing and presentation of this paper further enhance its contribution. Overall, despite the lack of methodological novelty, the paper is worthy of acceptance due to its systematic exploration and clear articulation of the subject matter."
                },
                "weaknesses": {
                    "value": "1. Increasing the clarity around the Posterior and detailing how it is used to transition from the privileged latent state to the non-privileged latent state would greatly enhance understanding of the method.\n   \n2. The related work section could be expanded to include research papers that leverage privileged simulation reset to improve policy. These works also seem to align with the scaffolding concept presented in this paper. Papers such as [1][2] could be added for reference.\n\n3. In the experimental design, the wrist camera and touch features don't appear to be excessively privileged or substantially different from the target observations. It would be beneficial to experiment with more oracle-like elements in the simulator as privileged sensory inputs. For instance, the oracle contact buffer or geodesic distance could be considered.\n\n\n[1] DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills\n\n[2] Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation"
                },
                "questions": {
                    "value": "1. More clarification on the posterior and embedding component in the method part. \n2. More clarification of other scaffolding methods in the related work, no need for any experiments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6525/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698618084426,
            "cdate": 1698618084426,
            "tmdate": 1699636733701,
            "mdate": 1699636733701,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qHEORlX5rA",
                "forum": "EpVe8jAjdx",
                "replyto": "LocwyzTRe6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6525/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6525/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for your review!\n\n> Increasing the clarity around the Posterior and detailing how it is used to transition from the privileged latent state to the non-privileged latent state would greatly enhance understanding of the method.\n\nWe noticed several reviewers also had questions about the methodology, in particular the Nested Latent Imagination where we unroll the scaffolded dynamics model with the target policy and posterior. The reviewer may be interested in our response to Reviewer xL3j on the motivation behind learning separate scaffolded and target representations. \n\nWe have added additional details in Appendix A.3 explaining the posterior and its usage in Nested Latent Imagination (NLI) to the appendix, along with pseudocode and a side-by-side figure comparing Nested Latent Imagination with the standard Latent Imagination procedure from DreamerV3 which only uses the impoverished target world model to generate trajectories.\n\n> Related work section could be expanded to include research papers that leverage privileged simulation reset to improve policy.\n\nThat is an interesting point! While we mainly considered privileged training in the form of privileged sensors (hence the name sensory scaffolding), resets could be seen as another privileged training mechanism to scaffold policy learning. We have incorporated this discussion into the related work (changed text in red), citing the papers suggested.\n\n> In the experimental design, the wrist camera and touch features don't appear to be excessively privileged or substantially different from the target observations. It would be beneficial to experiment with more oracle-like elements in the simulator as privileged sensory inputs. For instance, the oracle contact buffer or geodesic distance could be considered.\n\nScaffolder can be used for sim-to-real, with simulator states acting as the privileged sensors. In this scenario, it makes sense to use oracle-like elements in the simulator. Some tasks in our S3 suite do indeed resemble such a setting: for example, the dexterous manipulation tasks use ground truth object poses as privileged sensors.\n\nHowever, it\u2019s important to note that Scaffolder generalizes privileged sensors from simulator state, to noisy, high-dimensional observations - enabling privileged training in the real world. Some of the S3 tasks are designed to emulate a real world learning setup and as a result use realistic privileged sensors such as wrist cameras or touch sensors.\n\n> While there has been considerable research in this area, as detailed in the related work, this paper adds value to the existing body of knowledge, even without introducing novel methods. The proposed method may not be groundbreaking, but it offers a comprehensive examination of this issue from four perspectives: model, value, representation, and exploration.\n\nWhile this may be subjective, we respectfully disagree with the assertion that this paper does not introduce novel methods. Scaffolder is a novel method that permits privileged information to influence and improve policy learning through a comprehensive suite of routes: world model, value, exploration, and representation, as acknowledged by  reviewers V3Vw and Sbzi. Firstly, it is not a priori clear how to achieve this, but furthermore, Scaffolder is more substantial than simply combining various existing methods. A particular salient example is Nested Latent Imagination, which frugally achieves the goals of generating synthetic experience for an impoverished target policy, from a world model powered by privileged sensory information. Note that others have also considered this problem and arrived at different solutions: see our baseline Informed Dreamer, which Scaffolder outperforms comprehensively.\n\nFinally, beyond the method, this paper is also novel in considering learning from privileged information beyond just simulator state: Scaffolder can use high dimensional and noisy privileged observations like RGB images to train target policies."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6525/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176165299,
                "cdate": 1700176165299,
                "tmdate": 1700176165299,
                "mdate": 1700176165299,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ujrmyVssPP",
                "forum": "EpVe8jAjdx",
                "replyto": "qHEORlX5rA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6525/Reviewer_bAbn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6525/Reviewer_bAbn"
                ],
                "content": {
                    "title": {
                        "value": "Replay to Author Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the detailed explanation. The idea of enable real world training with high-dimensional observations for scaffolding is fantastic. I really hope this paper can be accepted and so keep my original rating of strong accept."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6525/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631491115,
                "cdate": 1700631491115,
                "tmdate": 1700631491115,
                "mdate": 1700631491115,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VLWdgtgeLN",
            "forum": "EpVe8jAjdx",
            "replyto": "EpVe8jAjdx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6525/Reviewer_V3Vw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6525/Reviewer_V3Vw"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes to utilize privileged sensory information to improve every component of model-based reinforcement learning, including world model, exploration policy, critic, and representation as well. This work provides extensive evaluation over 10 environments including different kinds of sensory data, showing the proposed method outperform all representative baselines. This work also provide detailed ablation study over all environments showing the"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This work provides systematic analysis over different components in the \u201csensory scaffolding\u201d setting, and proposes corresponding scaffolding counterparts  of every component in MBRL, except the policy during deployment. \n2. This work provides a promising evaluation comparison with multiple representative baselines, demonstrating that with the proposed pipeline, privilege information improves the sample efficiency as well as the final performance over wide-range of tasks.\n3. Through ablation study, this work shows different components in the system boost the performance in a different way, providing additional insight on how privileged information can be used in the future work.\n4. Experiment details are well presented in the Appendix, including runtime and resource comparison over different methods on different environments.\n5. The overall presentation of the work is good, considering the complexity of the system and amount of information delivered."
                },
                "weaknesses": {
                    "value": "1. For scaffolded TD error comparison, it\u2019s not clear why the comparison is conducted on Blind pick environment, since the gap between the proposed method and the version without scaffolded critic is much larger (at least in terms of relative gap) on Blind Cube Rotation environment. Also it would be great to see whether the estimate is close for tasks like Blind Locomotion (since the gap is small on that task). It seems there is some obvious pattern in the Figure 9, that the scaffolded TD is worse at 5, 10, 15 epoch and performs best on 7, 12, 18 epoch, it would be great to have some explanation for that.\n2. For some claims made in the paper, it\u2019s actually not quite convincing. For \u201cIn other words, much of the gap between the observations o\u2212 and o+ might lie not in whether they support the same behaviors, but in whether they support learning them.\u201d, some additional visualization like trajectory visualization might be helpful to strengthen the claim, since the similar reward score does not necessarily result in similar behavior. \n3. For runtime comparison, since the speed of given GPUs varies a lot, it might be better to compare the wall-time with similar system configuration, assuming the wall-time is consistent across different seeds."
                },
                "questions": {
                    "value": "1. Refer to weakness. \n2. Regarding some technical details, is a bit confusing:\n* In section C1, it says \u201cWe launch 4-10 seeds for each method\u201d, what\u2019s the exact meaning of using different number seeds across methods or across environments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6525/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822113737,
            "cdate": 1698822113737,
            "tmdate": 1699636733556,
            "mdate": 1699636733556,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Gxr2Q37dz7",
                "forum": "EpVe8jAjdx",
                "replyto": "VLWdgtgeLN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6525/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6525/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response Part 1 of 2"
                    },
                    "comment": {
                        "value": "Thank you for your review!\n> For scaffolded TD error comparison, it\u2019s not clear why the comparison is conducted on Blind pick environment, since the gap between the proposed method and the version without scaffolded critic is much larger (at least in terms of relative gap) on Blind Cube Rotation environment. Also it would be great to see whether the estimate is close for tasks like Blind Locomotion (since the gap is small on that task). \n\nThank you, we believe this comment arises from a misunderstanding of the analysis experiment reported in Fig 9. Our exposition here grew rather terse for space, and we have updated the experiment description in Section 4.3 to improve it, following the clarifications below. \n\nFig 9 *does not* represent a comparison between Scaffolder and No Scaffolder Critic. Instead, it compares the policy return estimated with all scaffolded components (specifically dynamics, reward, critic, and continue heads, as seen in Eq 3) with the policy return estimated using *no* scaffolded components. Recall that the gradient of this estimated policy return determines the policy updates. \n\nWe ran this analysis on Blind Pick because, as Fig 8 shows, \u201cNo-Scaff. WM\u201d performs very poorly here compared to full Scaffolder (this is also true for Blind Locomotion). We wanted to trace this performance gap all the way back to differences in the computed policy returns when not using scaffolded WM (including dynamics, reward, continue heads) and critic.  \n\nWe also agree with the reviewer that reporting these results for tasks other than Blind Pick may be interesting, and plan to report them, hopefully within the rebuttal period. \n\n\n> For some claims made in the paper, it\u2019s actually not quite convincing. For \u201cIn other words, much of the gap between the observations o\u2212 and o+ might lie not in whether they support the same behaviors, but in whether they support learning them.\u201d, some additional visualization like trajectory visualization might be helpful to strengthen the claim, since the similar reward score does not necessarily result in similar behavior.\n\nThank you, we recognize now that this statement was not clearly worded. Target and privileged policies often behave very differently in our settings, of necessity: for example, consider Blind Pick, where the target policy must first grasp around the table to find an object (see [videos on the website](https://sites.google.com/view/sensory-scaffolding#h.qlmom6baw90)), whereas the privileged policy sees the object and can directly pick it up. We do not of course claim that such wildly different behaviors are the same, just that they might achieve similar task metrics (e.g. task success rates). We will reword to: \u201cIn other words, much of the gap between the observations o\u2212 and o+ might lie not in whether they support achieving similar task performance metrics, but in whether they support _learning_ them.\"\n\n> For runtime comparison, since the speed of given GPUs varies a lot, it might be better to compare the wall-time with similar system configuration, assuming the wall-time is consistent across different seeds.\n\nWe will update the appendix with wall times recorded on a similar system configuration. \n\n> In section C1, it says \u201cWe launch 4-10 seeds for each method\u201d, what\u2019s the exact meaning of using different number seeds across methods or across environments?\n\nDue to computational limitations, it was expensive to run 10 seeds for all methods across all environments. We respectfully ask the reviewer to consider our large evaluation setup - 10 tasks and 5 model-based methods each taking a GPU and up to a day to run would result in 500 GPU days worth of compute.  \n\nWe have recorded the current number of seeds for each task below.\n|  | Blind Pick | Blind Locomotion | Blind Deaf Piano | Blind Numb Pen | Blind Numb Cube | Noisy Monkey | Wrist Pick-Place | Occluded Pick-Place | RGB Pen | RGB Cube |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Scaffolder | 8 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 |\n| DreamerV3 | 10 | 10 | 5 | 5 | 5 | 5 | 5 | 5 | 4 | 4 |\n| Informed Dreamer | 8 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 |\n| DreamerV3 + BC | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 4 | 4 |\n| Guided Observability | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 |\n| RMA+ (100M) | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| AAC (100M) | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n\nNote that our baselines RMA+ and AAC are run for 100M environment steps (20-400x the normal step budget), at which point we expect them to have overcome variations across initialization seeds. As a result, we just reported 1 seed for RMA+ and AMC.\n\nWe are running more seeds, and will update the results with 10 seeds for all tasks when ready, although this will likely happen after the rebuttal period. We do not anticipate any major changes in trends."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6525/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700175567345,
                "cdate": 1700175567345,
                "tmdate": 1700180059193,
                "mdate": 1700180059193,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hVdUBzyOmL",
                "forum": "EpVe8jAjdx",
                "replyto": "hyNOzcOXis",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6525/Reviewer_V3Vw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6525/Reviewer_V3Vw"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you authors for clarification,\nI'll keep my original evaluation."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6525/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700628750066,
                "cdate": 1700628750066,
                "tmdate": 1700628750066,
                "mdate": 1700628750066,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5TmC2kNvL2",
                "forum": "EpVe8jAjdx",
                "replyto": "VLWdgtgeLN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6525/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6525/Authors"
                ],
                "content": {
                    "title": {
                        "value": "TD analysis on another environment for V3Vw (11/22/23)"
                    },
                    "comment": {
                        "value": "We have now performed the TD($\\lambda$) analysis experiment on another environment, Blind Locomotion, as you had suggested. We find similar trends where the Scaffolded TD($\\lambda$) return estimate is more accurate than the target TD($\\lambda$) estimate. This provides further evidence that the training signals from Scaffolder are more accurate and lead to better policy improvement. Note that this experiment requires training Scaffolder from scratch with additional components (namely the target critic) to permit computing the target TD($\\lambda$) estimate in addition to the scaffolded TD($\\lambda$) estimate, so we had reported it only for Blind Pick (Fig 9) in the main paper. We added this Blind Locomotion result to Appendix D as additional evidence, alongside the Gridworld results already reported there.\n\n\nWe have provided some walltime learning curves, in response to xL3j, that you may also be interested in. Due to time constraints, we didn\u2019t have time to rerun all methods on a single machine, but we will do so after the rebuttal."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6525/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710489165,
                "cdate": 1700710489165,
                "tmdate": 1700713921579,
                "mdate": 1700713921579,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "a23ywWAMgM",
            "forum": "EpVe8jAjdx",
            "replyto": "EpVe8jAjdx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6525/Reviewer_xL3j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6525/Reviewer_xL3j"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Scaffolder, a MBRL method that extends DreamerV3 with privileged information in its modules. Scaffolder uses privileged world models and exploration policies to roll-out trajectories to train a better target policy. To ensure consistency between target and privileged latent, Scaffolder proposes to predict target latent from privileged latent, bottlenecked by target observation. Scaffolder outperforms baselines on the newly proposed S3 benchmark."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper is well written and motivated. The presentation is clear.\n+ Strong empirical performance."
                },
                "weaknesses": {
                    "value": "- I agree that it makes sense to evaluate the proposed method on the newly proposed benchmark, for motivations mentioned in the paper. However, the paper would still benefit from evaluating extra existing benchmarks, just for reference. \n- One major benefit of privileged information reinforcement learning is to train the target policy with privileged information in simulation, and deploy it in the real world where there is no privileged information. However, all experiments in the paper are purely in sim. Can the authors comment more on how well the presented approach will work in real-world applications?\n- In addition to the number of frames being the x-axis for figure 6, please also include one where x-axis is the wall-clock time. This way the community will have a better understanding of how the proposed method and baselines work on this particular environment set."
                },
                "questions": {
                    "value": "- Real-world applications. Please see details in the weaknesses section above.\n- I am curious about one particular design choice. Why do the authors choose to predict target latent from privileged latent, bottlenecked by target observation. Why not usethe same latent, shared by both the privileged and target modules?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6525/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6525/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6525/Reviewer_xL3j"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6525/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699294036110,
            "cdate": 1699294036110,
            "tmdate": 1700776977361,
            "mdate": 1700776977361,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RkbL6vXFnG",
                "forum": "EpVe8jAjdx",
                "replyto": "a23ywWAMgM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6525/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6525/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response Part 1 of 2 (9/16/23)"
                    },
                    "comment": {
                        "value": "Thank you for your review! \n> Sim-to-real scaffolder for real-world applications?\n\nYes, Scaffolder can easily be used as a sim-to-real approach, where simulator state serves as the privileged observation and sensory inputs (e.g. camera images) can serve as the target observation. Among our experimental settings, RGB Cube and RGB Pen involving privileged object pose and target visual observations follows a similar template.  \n\nFurthermore, note that we evaluate Scaffolder in more general settings that suggest a more direct route to real-world applications: even the privileged observations in our settings often come from additional sensors, rather than directly reading the simulator state. (e.g. privileged cameras in the Blind Pick environment) \u2014 this can be replicated in real-world learning settings such as the autonomous car setup described in the introduction. Scaffolder is also sample-efficient enough to reasonably permit this: it inherits and improves the sample-efficiency of Dreamer, which has already been applied to real-world robot learning [1].\n\nFinally, you may also be interested in our response to reviewer Sbzi, where we suggest some practical ways to fine-tune and use offline data for Scaffolder. \n\n[1] Wu, Philipp, et al. \"Daydreamer: World models for physical robot learning.\" Conference on Robot Learning. PMLR, 2023.\n\n> Performance vs training wall-clock time?\n\nAppendix B in the submission already reports the total training time for Scaffolder and baselines, showing that Scaffolder trains in similar time to other baselines. The model-free baselines RMA and AAC both need much longer to train than the model-based approaches like Scaffolder. Scaffolder needs on average 30-40% more wall clock time per update step than base Dreamer v3, due to having to train an additional world model but generally makes up for this by training much more sample-efficiently. We will expand this section to provide plots like Fig 6 but with wall clock time on the x-axis.\n\nFinally, note that training time costs in most real-world applications would be dominated by the cost of collecting environment steps (\u201cframes\u201d), such as with a robot moving in the real-world at each step. As such, the number of environment steps is often the most relevant cost to consider, which is why research in this area usually reports performance vs. steps as in our Fig 6.\n\n> Design choice: why predict target latent from privileged latent, bottlenecked by target observation,  why not use the same latent, shared by both the privileged and target modules?\n\nThank you for this question. We notice that several reviewers have had questions about this part of our approach, and will aim to expand and improve the exposition of Nested Latent Imagination, following the description below.\n\nScaffolder tries to simultaneously achieve the following two things: (1) it trains a scaffolded world model operating on scaffolded observations $o^+$ (represented into a \u201cscaffolded latent\u201d $z^+$) to more faithfully model the environment and generate better synthetic training experiences, (2) it trains a target policy, which in our setting, is restricted to operate only on information from target observations $o^-$, represented in a target latent $z^-$. In other words, $z^-$ cannot contain information that cannot be inferred from target observations alone. \n\nA shared representation $z^+=z^-$, as used in some prior approaches like RMA and Informed Dreamer, would violate these requirements: if it contained information from privileged sensors to enable better world model learning as required in (1), it would not be fully inferable from $o^-$ alone, as required by (2). Consider, for example, S3 tasks like Blind Picking or Blind Locomotion, which have target observations (e.g. proprioception) that are not predictive of privileged observations (images of objects). \n\nScaffolder proposes a relatively straightforward solution to achieve both (1) and (2). It keeps $z^+$ and $z^-$ separate, but learns a target embedding translator $z^+ \\rightarrow e^-$. This allows a mapping from $z^+ \\rightarrow e^- \\rightarrow z^-$ via the target posterior, so that the synthetic experiences generated in the scaffolded WM can still be used to train target policies. Note that the scaffolded observations $o^+=[o^-, o^p]$ include and extend the target observations, so it is reasonable to learn a unique mapping from $z^+$ to $e^-$."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6525/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174899921,
                "cdate": 1700174899921,
                "tmdate": 1700177211963,
                "mdate": 1700177211963,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RYsTMFiTVZ",
                "forum": "EpVe8jAjdx",
                "replyto": "zrGjv02wAR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6525/Reviewer_xL3j"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6525/Reviewer_xL3j"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for addressing my questions. My concerns are resolved, and I am raising my scores accordingly."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6525/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513226828,
                "cdate": 1700513226828,
                "tmdate": 1700513226828,
                "mdate": 1700513226828,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]