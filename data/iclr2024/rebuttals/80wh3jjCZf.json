[
    {
        "title": "Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces"
    },
    {
        "review": {
            "id": "Mia9e1242o",
            "forum": "80wh3jjCZf",
            "replyto": "80wh3jjCZf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3247/Reviewer_kH6M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3247/Reviewer_kH6M"
            ],
            "content": {
                "summary": {
                    "value": "The authors present an approach for learning in environments with large, discrete action spaces. Specifically, they learn policies in a continuous space that can be mapped to the discrete actions. The mapping the propose leverages structure in the discrete space in which it performs a localized search to efficiently perform the mapping."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originality:** The work presented in this paper is novel as far as I know. They propose an approach that has not been done before that attempts to mitigates drawbacks exhibited by other algorithms.\n\n**Quality:** The problem is well-motivated and the drawbacks of current SOTA approaches for the problem are identified and the authors describe sufficiently how their approach attempts to tackle one of these issues. While I have not delved thoroughly into the appendix to check every bit of the theory, the sketched out lemmas seem sound. The experiments conducted were reasonable and thorough enough (I appreciate the ablation study performed.)\n\n**Clarity:** Easy to read and understand.\n\n**Significance:** I believe this work could be impactful to others in the area."
                },
                "weaknesses": {
                    "value": "My one issue is with the first set of experiments in the maze. The results do not really demonstrate that DNC actually provides any advantage over the other baselines. While the second set of experiments do, I think maybe adding another domain may support performance claims by the authors. Alternatively, if the argument is that the performance of DNC is competitive to other SOTA, what other benefits may it provide (better wall-clock time, memory usage, etc.)?"
                },
                "questions": {
                    "value": "Tiny nitpick: The second sentence in Problem Definition --- it would read better to describe the states and actions in the order you introduce them."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3247/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3247/Reviewer_kH6M",
                        "ICLR.cc/2024/Conference/Submission3247/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3247/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698706241368,
            "cdate": 1698706241368,
            "tmdate": 1700695513140,
            "mdate": 1700695513140,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "avqJJ2vp23",
                "forum": "80wh3jjCZf",
                "replyto": "Mia9e1242o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3247/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments!"
                    },
                    "comment": {
                        "value": "Thank you for providing highly useful feedback and valuing our paper\u2019s originality, the quality of our\nproposed approach, the clarity of our write-up, and our paper\u2019s significance.\n\n**Weakness:** We agree that the maze environment is not ideal to benchmark our DNC method, as also mentioned by the other reviewers. However, we included the maze environment as it was used like this in prior works on large discrete action spaces [cf. 1, 2]. Accordingly, we decided to study the same environment to contextualize our results. Its purpose is to show that our approach does not fall behind the quality of other approaches in established environments. \n\nThe motivation for DNC are contextual multi-stage stochastic optimization problems that arise in real-world applications. We followed your suggestion and added another environment from those problem domains, specifically a dynamic job-shop scheduling problem [cf. 3, 4]. In this  extensively studied problem a set of $G$ jobs needs to be allocated to $N$ machines that can serve up to $L$ jobs each, yielding an action space of size $|\\mathcal{A}|=L^N$. We consider a load-balancing variant of this problem, for which jobs need to be balanced over heterogeneous machines. The action space in our experimental design amounts to $10^{10}$ actions, hence only DNC, DNC w/o SA, and MinMax can learn policies at all. The results show that DNC significantly outperforms DNC w/o SA and MinMax. For a more thorough results discussion as well as the updated convergence graphs in Figure 3, we refer to the updated paper. To keep the results section concise, we replaced the maze environment with 28 actuators by the job-shop scheduling results.\n\nMoreover, we improved the manuscript according to your suggestions and now discuss DNC's memory advantage. DNC exhibits a consistent memory usage even as the action space size increases, highlighting its practical benefits. Our results, presented in Table 3 in Appendix H (see also pasted below), show that while the memory requirements for kNN, LAR, and VAC rise substantially when scaling from a maze of size $2^{12}$ to a job-shop environment with $10^{10}$ actions, the memory consumption for DNC remains stable at 68 to 73 MB. The memory consumption in the case of kNN, LAR, and VAC is driven by the need to setup a matrix containing all discrete actions. For example, in the case of the job-shop scheduling problem this would be a matrix with 5 rows and $10^{10}$ columns. We obtained the consumed memory using the Python profiler guppy. The stability of DNC is vital for applications with limited resources or when scaling for larger datasets without a significant increase in resource demands. Its constant memory footprint allows it to handle larger action spaces efficiently without performance loss due to memory constraints. This scalability is critical in real-world applications with finite computational resources. DNC's ability to maintain consistent memory usage, regardless of the action space size, makes it an essential tool in machine learning, particularly when resources are limited.\n\n| **Algorithm** | Maze ($2^{12}$) | Maze ($2^{20}$) | Job-shop ($10^{10}$) |\n|--------------------|-------------------|------------------|----------------------|\n| DNC               | 68 MB           | 68 MB           |               73 MB |\n| DNC w/o SA   | 68 MB           | 68 MB           |               73 MB |\n| MinMax          | 68 MB           | 68 MB           |               73 MB |\n| kNN                | 68 MB           | 252 MB          |     373,000 MB |\n| LAR                | 68 MB           | 252 MB          |     373,000 MB |\n| VAC                | 68 MB           | 252 MB          |     373,000 MB |\n\n\n\n**Questions:** Thank you for the suggestion. We have altered the order in this sentence.\n\nIf you are satisfied with our improvements, we would kindly ask you to consider raising the score for\nour manuscript."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669880672,
                "cdate": 1700669880672,
                "tmdate": 1700669880672,
                "mdate": 1700669880672,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8O0UxL529R",
                "forum": "80wh3jjCZf",
                "replyto": "avqJJ2vp23",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3247/Reviewer_kH6M"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3247/Reviewer_kH6M"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their responses and additions to the paper. I believe it will make the paper a lot stronger. My score, though, will remain the same."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695605473,
                "cdate": 1700695605473,
                "tmdate": 1700695605473,
                "mdate": 1700695605473,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7Xxkeila4S",
            "forum": "80wh3jjCZf",
            "replyto": "80wh3jjCZf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3247/Reviewer_x32f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3247/Reviewer_x32f"
            ],
            "content": {
                "summary": {
                    "value": "Given the need in fields like inventory management, an efficient method for RL with structured large discrete action spaces (n-dimensional lattices) is proposed. First a continuous action is generated and discretized, then through simulated annealing with perturbation defined by other actions on the lattice with low hamming distance, a final proposed discrete action is obtained. This avoids the necessity of constructing extremely large nearest neighbor graphs, or solving a linear program on every forward pass. The convergence for the action selection process is proven, and the method is evaluated in two simulated domains with strong results over baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The method is well-motivated\n- Results are strong over baselines in Figure 3\n- Components are ablated"
                },
                "weaknesses": {
                    "value": "I should first couch my review by noting I'm not the most familiar with the literature in this area, so this is an educated guess:\n\n- The maze domain seems a bit too trivial to be useful, it is informative for a demonstration of the method, but unlike the inventory management environment, I'm not sure what the real-world justification might be. I would prefer to have at least one more grounded and complex environment (perhaps a video game with many different possible actions, perhaps a multi-agent setting? I'm not the most familiar with the domains in this subfield).\n- Perhaps this is my unfamiliarity with the domain, but sometimes the text is quite hard to follow. I think part of this is the reliance on lots of acronyms (LDAS, SLDAS, MILP, SA), but also the claim in Section 2 \"we formulate the task of finding a Q-value maximizing a as a mixed-integer linear program\" was a bit confusing on first read, as really the authors are proposing an approximate solution, so saying this upfront might be a bit more clear.\n- Would be nice to see MILP results in Figure 5 against the proposed method and other baselines in Figure 3, and the discussion at the end of Appendix B is informative and it may be nice to have a note in the text"
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3247/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3247/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3247/Reviewer_x32f"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3247/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698720075722,
            "cdate": 1698720075722,
            "tmdate": 1699636273110,
            "mdate": 1699636273110,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "15J7LR5lxb",
                "forum": "80wh3jjCZf",
                "replyto": "7Xxkeila4S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3247/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments"
                    },
                    "comment": {
                        "value": "Thank you for the valuable feedback. We appreciate the compliments on our paper's motivation and results. Please find our detailed answers to all questions and comments below.\n\n**W1:**\nWe appreciate your feedback regarding the choice of the maze domain and the suggestion for more complex environments. We included the maze environment as it was used like this in prior works on large discrete action spaces [1,2]. Accordingly, we decided to study the same environment to contextualize our results. Its purpose is to show that our approach does not fall behind the quality of other approaches in established environments.\n\nThe motivation for DNC are contextual multi-stage stochastic optimization problems that arise in real-world applications. We followed your suggestion and added another environment from those problem domains, specifically a dynamic job-shop scheduling problem [cf. 3,4]. In this  extensively studied problem a set of $G$ jobs needs to be allocated to $N$ machines, that can serve up to $L$ jobs each, yielding an action space of size $|\\mathcal{A}|=L^N$. We consider a load-balancing variant of this problem, for which jobs need to be balanced over heterogeneous machines. The action space in our experimental design amounts to $10^{10}$ actions, hence only DNC, DNC w/o SA, and MinMax can learn policies at all. The results show that DNC significantly outperforms DNC w/o SA and MinMax. For a more thorough results discussion as well as the updated convergence graphs in Figure 3, we refer to the updated paper. To keep the results section concise, we replaced the maze environment with 28 actuators by the job-shop scheduling results.\n\n**W2:**\nThank you for your suggestions. We understand that the abbreviations, while aimed at maintaining conciseness to adhere to the page limit, may have impacted the readability to some extent. Because of the page limit, we decided to keep the abbreviations in the main document. We made several textual changes to enhance the clarity of the paper. \n\nWe have improved the sentence related to the MILP, highlighting that we only use the MILP to formalize the problem but aim for an efficient algorithm to solve it.\n\n**W3:**\nThank you for pointing this out. We added the remaining results in Figure 5 and added a note referring to Appendix B in the main text.\n\nIf you are satisfied with our improvements, we would kindly ask you to consider raising the score for our manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670001392,
                "cdate": 1700670001392,
                "tmdate": 1700670001392,
                "mdate": 1700670001392,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Pt20j9uko6",
                "forum": "80wh3jjCZf",
                "replyto": "7Xxkeila4S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3247/Reviewer_x32f"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3247/Reviewer_x32f"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response, I realize some of my phrasing was quite poor on Weakness 3: I wanted the results for MILP and Hybrid to be presented in Figure 3 in the main text, not for results on many baselines from Figure 3 to be added to Figure 5. Barring this change, all of my concerns are answered and I would be happy to raise my score, pending discussion with other reviewers."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690843504,
                "cdate": 1700690843504,
                "tmdate": 1700692626223,
                "mdate": 1700692626223,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wSF2qWO5OS",
            "forum": "80wh3jjCZf",
            "replyto": "80wh3jjCZf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3247/Reviewer_ZSBT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3247/Reviewer_ZSBT"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of reinforcement learning in large discrete action spaces, which vanilla techniques typically cannot handle, and require adaptations. The authors propose a technique that, similarly to other works, outputs a continuous action, which is then mapped to a valid discrete action that is executed in the environment. Their proposal applies for action spaces that are regularly structured (e.g., an equally-spaced grid), which allows the method to circumvent needing to store the valid actions in memory. Instead, to map to a discrete action, a simulated annealing search is proposed, that is shown (via reasonable assumption and a standard analysis technique) to yield actions that improve the (estimated) Q-value. The method is evaluated in environments with an exponential number of actions, showing similar performance with prior methods in regimes with less actions, and better performance in regimes with more actions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**S1**. The proposed technique is simple, elegant, and rigorously analyzed. It is also shown to work well empirically in environments that satisfy the required conditions.\n\n**S2**. The writing of the paper is excellent and clear."
                },
                "weaknesses": {
                    "value": "**W1**. My primary criticism is that the considered environments and scenarios where the technique applies are somewhat contrived. If one is faced with such a huge decision space, we would typically expect to spend the time in the modelling stage of the problem to design an action space that is substantially more manageable. For example, in the maze environment, we could design a discrete set of actions (move left, etc.), with the actual execution being handled by lower-level control primitives. Indeed, this needs to be performed for each individual problem (as the authors do mention).\n\n**W2**. I also have some concerns about the evaluation:\n\n- Firstly, for the maze environments, the variability of the techniques is such that the confidence intervals overlap substantially, and it is not possible to draw reliable conclusions from these figures alone. Would this still happen with substantially more seeds (e.g. 50+)? Alternatively, would another procedure (e.g., running many episodes with randomly initialised positions) yield more reliable (i.e., with non-overlapping CIs)?\n- The simulated annealing search procedure must introduce a computational overhead compared to picking the best action greedily (e.g., as in DNC w/o SA). This is not mentioned, and the impact on runtimes is not analyzed or discussed. This needs to be addressed, accompanied by measurements."
                },
                "questions": {
                    "value": "**C1**. The paper refers in several places to approaches for \"unstructured\" LDAS (abstract, intro, figure 1, etc.). This is somewhat of a misnomer since these problems *do* exhibit structure (i.e., the action space is such that actions that are close in embedding space yield similar Q-values and lead to similar outcomes when actuated in the environment). I think a better term would be *irregularly* structured. Similarly, the proposed method applies to *regularly* structured action spaces.\n\n**C2**. Related to the second point of W2 above, the paper mentions the use of \"SA to efficiently search across different and potentially better neighborhoods\" (Discussion on p6). While this statement potentially applies for the memory efficiency, it certainly doesn't in terms of time, and should be qualified. SA in general is not computationally efficient.\n\n**C3**. Small nitpick: \"Constraints 2\" -> \"Constraint 2\", etc. in the text at the end of Section 2."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3247/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3247/Reviewer_ZSBT",
                        "ICLR.cc/2024/Conference/Submission3247/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3247/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837793084,
            "cdate": 1698837793084,
            "tmdate": 1700736374554,
            "mdate": 1700736374554,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FrqE5cFS3X",
                "forum": "80wh3jjCZf",
                "replyto": "wSF2qWO5OS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3247/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments"
                    },
                    "comment": {
                        "value": "Thank you for providing valuable feedback and appreciating our paper's writing, the simplicity of our approach, and our rigorous analysis. Below we answer to all questions and comments.\n\n**W1:** We agree that the huge action space in the maze could be avoided by refined modeling. We included the maze as it was used like this in prior works on large discrete action spaces [1,2]. Accordingly, we decided to study the same environment to contextualize our results. Its purpose is to show that our approach does not fall behind the quality of other approaches in established environments.\n\nWe believe using DNC and a refined modeling effort are not necessarily mutually exclusive. For example, one can use DNC to create benchmarks to verify the performance of the modeling approach. The main motivation of our work does not relate to problems handled by refined modeling, but stems from contextual stochastic multi-stage optimization settings grounded in real-world applications. Such settings, e.g., the inventory and job-shop environments, often lack room for action space simplification, as it would infer a significant loss in solution quality.\n\n**W2.1:** We re-ran the maze with 12 actuators and with teleporting tiles (50 seeds). For statistical significance and consistency we also ran 50 seeds for the other environments. To reduce the computational burden and save space in the main text, we excluded the results on the maze with 28 actuators. We report the results in Fig 3.\n\n* For the maze with 12 actuators, CIs are narrower than in the original submission. However, CIs still overlap and the avg. performances of the different approaches do not differ significantly from the results in the original paper.\n\n* For the maze with teleporting tiles, we reconfirm previous observations. The CI of DNC overlaps with MinMax's CI only slightly, showing DNC's advantage over MinMax in this environment. DNC is therefore able to provide high quality policies in more complex environments, comparable to kNN. This result is noteworthy, as MinMax is the only SOTA method able to scale to very large SLDAS such as the inventory environment. To disclose the CI overlap of DNC and MinMax, we included Fig 8 in Ap. H and summarized the findings in the main paper.\n\nIn addition, we added boxplots in the appendix, displaying the avg. performance across the maze in the final training episode (see Fig 9 in Ap. H) and summarized the findings in the main paper. The results suggest that convergence in the maze is much more stable than perceived (cf. Fig 3). The wide CIs can be attributed to the inherent reward variance of the environment. We refer to Ap. H, Fig 10 for a more thorough discussion.\n\nWe addressed your alternative suggestion to run several episodes with random initial positions. The resulting outcomes and discussion are detailed in Fig 11 in Ap. H.\n\n**W2.2:** We measured the avg. duration of obtaining a decision from the policy and applying it to the environment, i.e., step time, within an episode during testing. We apply DNC during both testing and training, so the reported step times also apply during training.  We have added a discussion and Fig 7 in Ap. H, wherein we compare avg. step times (50 seeds) for the benchmarks. We added a reference in the main paper.  Our findings suggest that while SA increases training time, its benefit in terms of search quality may outweigh these costs, especially if the action space is so large that other methods fail to learn at all due to memory restrictions (kNN) or learn less performant policies (MinMax, DNC w/o SA). During training, we consider longer computational times to be acceptable as the focus is on model accuracy, not speed. In practical implementation, DNC offers step times in the range of milliseconds, remaining practical for real-time decision-making.\n\n**C1:** We added the proposed terms and made a clarification in the beginning of paragraph 2. In the rest of the paper we kept the original submission's \"structured\" and \"unstructured\" for conciseness.\n\n**C2:** We have adjusted wording on page 6 to more accurately reflect the characteristics of SA. SA's time impact within DNC remains controllable, see Fig 7 in Ap. H. The max. no. SA iterations is limited by DNC's hyperparameters. The max. no. SA search iterations needed to achieve good performance is limited to 10 SA search iterations in the inventory environment (cf. Table 2, Ap. F). During a single SA iteration, DNC evaluates multiple neighbors in parallel through our dynamic neighborhood construction procedure, see Section 3. Hence, evaluating a larger neighborhood does not necessarily require more SA search iterations.\n\n**C3:** Thank you for the comment. We use the plural to emphasize that we refer to a set of constraints. For example, Equation (4) has to hold for all neurons $d\\in \\mathcal{D}_2$.\n\nIf you are satisfied with our improvements, we would kindly ask you to consider raising the score for our manuscript."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669099843,
                "cdate": 1700669099843,
                "tmdate": 1700669099843,
                "mdate": 1700669099843,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9AoLVhmjbU",
                "forum": "80wh3jjCZf",
                "replyto": "FrqE5cFS3X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3247/Reviewer_ZSBT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3247/Reviewer_ZSBT"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal response to authors"
                    },
                    "comment": {
                        "value": "Thanks for engaging with my suggestions! I think the additional results have substantially improved the robustness of the findings. Given this, I am increasing my score to an 8."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736358728,
                "cdate": 1700736358728,
                "tmdate": 1700736358728,
                "mdate": 1700736358728,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]