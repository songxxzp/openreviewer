[
    {
        "title": "Generative Human Motion Stylization in Latent Space"
    },
    {
        "review": {
            "id": "R2AujaEnFc",
            "forum": "daEqXJ0yZo",
            "replyto": "daEqXJ0yZo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4166/Reviewer_fyTQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4166/Reviewer_fyTQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the problem of stylizing 3D human motions. To this end, the authors first train an autoencoder to bring the motion sequence into latent space and propose to decouple the style and content of the motion in latent space. Specifically, the latent code is mapped to content code and style space via two encoders and then mapped back to the original latent space via a generator equiped with adaptive instance normalization (controled by the style code).  During training, several losses are applied to ensure the disentanglement between the content and style including  the Homo-style Alignment loss, swap and cycle reconstruction loss."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The presentation is clear and easy to follow.\n- The overall pipeline is sound and the authors provide comprehensive experimental analysis on the performance of the proposed method."
                },
                "weaknesses": {
                    "value": "- In terms of the pipeline itself, there are not much novelties. Every component in the pipeline is straightforward and proposed by previous methods for example the adaptive instance normalization, the cycle consistent reconstuction. Although the proposed homo-style alignment loss is new, it is quite simple.\n- More details about the \"Ours w/o latent\" model is needed.\n- It seems all the inline references used in the paper are not in parenthesis. I believe the authors miss used \\citet when there should be \\citep."
                },
                "questions": {
                    "value": "- It seems the whole model can be trained end-to-end. Why the motion autoencoder is pretrained? What will happend when every components are trained end-to-end?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4166/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4166/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4166/Reviewer_fyTQ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4166/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698131308609,
            "cdate": 1698131308609,
            "tmdate": 1699636382498,
            "mdate": 1699636382498,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "W2i7WcnTdz",
                "forum": "daEqXJ0yZo",
                "replyto": "R2AujaEnFc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4166/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4166/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer fyTQ"
                    },
                    "comment": {
                        "value": "**We appreciate your time and effort spent on reviewing our paper. Below, we answer the raised questions one by one.**\n\n***Q1**: In terms of the pipeline itself, there are not much novelties. Every component in the pipeline is straightforward and proposed by previous methods for example the adaptive instance normalization, the cycle consistent reconstuction. Although the proposed homo-style alignment loss is new, it is quite simple.*\n\n**A1**: We would like hightlight the following technical distinctions of our approach from existing works:\n*  **Latent Stylization**: We propose latent stylization which use the latent features from autoencoders instead of raw poses (as in previous works) for stylization. This compact stylization representation contains more discriminative features, and empirically enhances the performance on stylization accuracy, content preservation, and generalization ability on unseen datasets. It also encourages better style space learning as shown in Figure 7.\n* **Probabilistic Style Encoding**: Unlike previous works using deterministic codes, we propose probabilistic style encoding, modeling style as a Gaussian distribution for each motion input. During training, we encourage all the style Gaussian distribution to be close to N(**0**, **I**). This lead to a smooth and dense style space, supporting novel and diverse style sampling. Benefiting from the probabilistic style, our method exhibits notable flexibility that previous works didn't have, accommodating diverse stylization, various style inputs, and different training settings as shown in Table 1.\n\n* **Homo-style alignment** and **global motion prediction**.  We introduce *homo-style alignment* to provide extra strong supervision signals of style cues which is essentially important in unsupervised setting, and the *post-global motion prediction* to address practical root motion transfer issues.\n\nThese technical designs, while not involving computationally intensive operations, achieve state-of-the-art performance while being the most efficient during use. More details are available in our ablation study (Appendix A.), highlighting the performance improvements brought about by these design choices.\n\n---\n\n***Q2**:  More details about the \"Ours w/o latent\" model is needed.*  \n\n**A2**: Thanks for the suggestion. We have upload more ablation comparison regarding \"ours w/o latent\". Please refer to the revised submission of supplementary videos, with file name \"7_additional_results(rebuttal).mp4\". Or you could easily access through this [anonymous link](https://drive.google.com/file/d/1JcceadXfFwLgDwnlLf0yZ-L_tQ02ZShh/view?usp=drive_link).\n\n---\n***Q3**: It seems all the inline references used in the paper are not in parenthesis. I believe the authors miss used \\citet when there should be \\citep.*\n\n**A3**: We thank the reviewer for pointing this out. We have corrected the reference format in the submitted revision.\n\n---\n***Q4**: It seems the whole model can be trained end-to-end. Why the motion autoencoder is pretrained? What will happend when every components are trained end-to-end?*\n\n**A4**: While it is true that the entire model can be trained end-to-end, we found it challenging for the model to simultaneously learn a meaningful latent representation and figure out how to transfer style traits in these latent-based representations. During end-to-end training, we observed the loss quickly getting stuck at a high range. Conversely, training the autoencoders and stylization models stage-by-stage essentially decomposes the whole task into two simpler sub-tasks, allowing each model to address its corresponding aspect. This strategy significantly eases the learning process.\n\nWe conducted an ablation experiment to study this choice of training strategy, reporting results on Aberman(*) and Xia(#) datasets in supervised motion-based stylization. The experimental results align with our observation, indicating that stylization accuracy is merely around 15% on both datasets in the end-to-end training scenario, compared to the accuracy of 92% achieved by stage-by-stage training.\n\n| Training Strategy | Style Acc*| Style FID*|  Geo Dist*| Style Acc#| Content Acc# | Content FID#| Geo Dist#| \n| ----  |----  | ----  | ----  | ----  |----  | ----  | ----  | \n| Seperately|**0.945**|**0.020**|**0.344**|**0.926**|**0.674**|**0.189**|**0.680**|\n|End-to-end|0.125|1.521|0.577|0.174|0.293|1.417|0.700|\n---\n\nThis table and related discussion has been incoporated in the revised manuscript, as in Table 8.\n\n---\n*Thanks again for your value time and effort. Please let us know if you have further concerns.*"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411144078,
                "cdate": 1700411144078,
                "tmdate": 1700411144078,
                "mdate": 1700411144078,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HNqRI2v2iY",
                "forum": "daEqXJ0yZo",
                "replyto": "R2AujaEnFc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4166/Reviewer_fyTQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4166/Reviewer_fyTQ"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the rebuttal"
                    },
                    "comment": {
                        "value": "Although I am still not fully convinced by the novelty of the proposed pipeline, the rebuttal addressed some of my concerns and I now have a clearer view of the paper.  I'd much appreciate it if my further comments can be addressed.\n\n- It would be better to include more technical details of the baseline model \"Ours w/o latent\" not only the qualitative results. Otherwise, it is hard for readers to understand why it is worse than the proposed model.\n\n- Since the pre-trained auto-encoder cannot be perfect, it is better to report the reconstuction results of the auto-encoder and analyse how the reconstuction quality will affect the style transfer. For example,  what if the model is given an out-of-distribution motion, how will it peform?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559312894,
                "cdate": 1700559312894,
                "tmdate": 1700559326558,
                "mdate": 1700559326558,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GeF2UVtNrL",
                "forum": "daEqXJ0yZo",
                "replyto": "R2AujaEnFc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4166/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4166/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your follow-up comments"
                    },
                    "comment": {
                        "value": "Dear Reviewer  fyTQ,\n\n**Thanks for your timely response. We hope the following could clarify your concerns.**\n\n***Q1**: It would be better to include more technical details of the baseline model \"Ours w/o latent\" not only the qualitative results. Otherwise, it is hard for readers to understand why it is worse than the proposed model.*\n\n**A1**: Thanks for your suggestion on this issue. We agree that this part need further clarification. **Ours w/o latent** employs the identical architecture as our full model, as illustrated in Figure 2 (a), without the steps of pretraining or training the motion encoder $\\mathcal{E}$ and decoder $\\mathcal{D}$ as autoencoders. Although it maintains the same number of model parameters, it directly learns style transfer on poses, allowing us to assess the impact of our proposed latent stylization. \n\nWe also have revised the content in the section 4.1 of updated manuscript.\n\n---\n\n***Q2**: Since the pre-trained auto-encoder cannot be perfect, it is better to report the reconstuction results of the auto-encoder and analyse how the reconstuction quality will affect the style transfer. For example, what if the model is given an out-of-distribution motion, how will it peform?*\n\n**A2**: We quantitatively evaluated the reconstruction results of our VAE and AE models using the MPJPE metric in millimeters, obtaining values of 39.4 for AE and 32.5 for VAE, respectively. To analyze the impact of reconstruction quality on stylization, we experimented with different hyperparameters, including $\\lambda_{l1}$ and $\\lambda_{sms}$ for the AE model. Specifically, $\\lambda_{l1}$ encourages sparsity in latent features, while $\\lambda_{sms}$ enforces temporal feature smoothness. Results on Xia et al (#) and Aberman et al (*) datasets in the supervised motion-based stylization setting are reported in Table 12 of the revised manuscript.\n\n|$\\lambda_{l1}$|$\\lambda_{sms}$|MPJPE(*)|Style Acc(*)|Style FID(*)|MPJPE(#)|Style Acc(#)|Content Acc(#)| Content FID(#) |\n|---|---|---|---|---|---|---|---|---|\n|0.001|0.001|**39.4**|**0.945**|**0.020**|**62.5**|**0.926**|**0.674**|**0.189**| \n|0.1|0.1|360.1|0.862|0.041|431.8|0.804|0.589|0.276| \n|0.01|0.01|180.4|*0.873*|*0.041*|250.5|0.830|0.656|0.244| \n|0.0001|0.0001|*77.6*|0.857|0.042|*130.9*|*0.901*|*0.661*|*0.239*|\n---\nExcessive penalties on smoothness and sparsity proved detrimental, resulting in lower reconstruction quality. We observed a substantial correlation between reconstruction and stylization performance, with better reconstruction often translating to improved stylization, especially on the unseen dataset (i.e, Xia et al.)\nFor qualitative results, we introduced a new section 'reconstruction' in the supplementary videos, showcasing reconstructed results from our VAE and AE models.  You may check the newly submitted the supplementary videos, or access this [anonymous link](https://drive.google.com/file/d/1JcceadXfFwLgDwnlLf0yZ-L_tQ02ZShh/view), at the end of the video.\n\nFor out-of-distribution motions, we would like to point out that we have provided many such cases in our supplementary videos. Note the motions from CMU Mocap, text2motion results, and the dataset of Xia et al. are all out-of-domain data for our stylization models. To highlight this, in the updated supplemantary videos, we explicitly indicate the motions come from out-of-distributions in the corresponding cases. In additional, for content that are severely different from the training data, such as backflip, breaking dance, the model may fail, as indicated in our discussion of failure cases.\n\n---\n\n*Thanks again for your comment. We sincerely hope our response have properly addressed your concerns.*"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640903614,
                "cdate": 1700640903614,
                "tmdate": 1700678445246,
                "mdate": 1700678445246,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XvQwyZaT4M",
                "forum": "daEqXJ0yZo",
                "replyto": "natOZ3uN60",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4166/Reviewer_fyTQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4166/Reviewer_fyTQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the responses. I have no further questions."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698533043,
                "cdate": 1700698533043,
                "tmdate": 1700698533043,
                "mdate": 1700698533043,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "O5LYeNKsP7",
            "forum": "daEqXJ0yZo",
            "replyto": "daEqXJ0yZo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4166/Reviewer_bi3M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4166/Reviewer_bi3M"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a generative method for motion generation. The key idea is using the motion latent code as a compact and expressive representation. The motion code is enforced to decompose into a deterministic content code and a probabilistic style code. The style code is expressed in a certain formula of distribution. And the generation is performed by an auto-encoder. The method can generate stylized motion in different schemes: unconditional or conditional to certain style label or exemplar motion. Also, the benchmarking results also suggest the good effectiveness and the time efficiency of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method can support two modes of motion generation: conditional to label of style and the by sampling from the prior distribution. This enables a diverse potential downstream applications.\n- The latent code of motion is shorter than the motion itself, making the leveraging of motion descriptor in the neural network more efficient.\n- The stylization and the content information can be disentangled from the input motion code, supporting versatile application of motion generation.\n- I find the idea of using three groups of input is interesting that requires no additional style label but can encourage disentanglement. Also, the cycle-consistency idea is also interesting."
                },
                "weaknesses": {
                    "value": "- Probably not a \u201cweakness\u201d, but this is a unconvincing design to me that a small 1D convolutional network can predict the global motion (say root position) from local joint motion. I would ask for more details and evidence to support this design. The results shown in Table 7 only suggest relatively marginal influence, especially in the supervised settings. Is it possible to also have more results on other datasets?\n- Given multiple components and modules are proposed to construct the proposed method, a complete ablation study is expected to validate their effectiveness. The ablation of loss weights shown in Table 6 is probably not enough to convince readers that all proposed components are contributing to the final improvement of the observed benchmarking results.\n- Question: The results are focused on motion generation in the skeleton representations. Can the authors also propose some visualization with SMPL LBS? With the mesh, sometimes it can be more intuitive to estimate the plausibility of the joint angles."
                },
                "questions": {
                    "value": "Please see my questions above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4166/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698581955986,
            "cdate": 1698581955986,
            "tmdate": 1699636382428,
            "mdate": 1699636382428,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rxjTB7mRPz",
                "forum": "daEqXJ0yZo",
                "replyto": "O5LYeNKsP7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4166/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4166/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer bi3M"
                    },
                    "comment": {
                        "value": "**We thank you for your encouraging comments. The following responses aim to address your concerns point by point.**\n\n***Q1**: Probably not a \u201cweakness\u201d, but this is a unconvincing design to me that a small 1D convolutional network can predict the global motion (say root position) from local joint motion. I would ask for more details and evidence to support this design. The results shown in Table 7 only suggest relatively marginal influence, especially in the supervised settings. Is it possible to also have more results on other datasets?*  \n\n**A1**: **Technical Details**:\n\"In this work, our pose representation is a 260-dimensional vector, encompassing rotation, position, and velocity information. The global motion predictor (GMP) focuses on regressing the 2D root velocity on the XY-plane, representing the global motion, based on the remaining 258-dimensional features. All features are Z-normalized. The GMP comprises three 1D convolutional layers with a kernel size of 3, no downsampling, and channel numbers of 512, 256, and 260 at each layer. Training utilizes mean absolute error, with added random Gaussian noise for robustness (scale from 0 to 0.5, probability 0.5).\n\n**More results:** To address your concern, we conducted two experiments to evaluate the GMP. Firstly, we assessed prediction accuracy on three test sets, demonstrating reasonable performance even on unseen datasets (Table 10 in the revised content). We extended the evaluation of GMP to all three datasets (Aberman(*), CMU Mocap(#), and Xia(@)) in motion-based stylization, covering both supervised (S) and unsupervised (U) settings.\n\n|Aberman et al. | CMU Mocap| Xia et al.|\n|---|---|---|\n|46.2|48.7|57.7|\n---\n\nWe further evaluate the combined impact of GMP on the final stylization, and extend the original evaluation to all the three datasets, specifically on Aberman(*), CMU Mocap(#) and Xia(@). This experiment is conducted in motion-based stylization, on both supervised (S) and unsupervised setting (U).\n\n|Method| Style Acc*|Foot Skating*| Style Acc#|Foot Skating#|Style Acc@|Foot Skating@|\n|---|---|---|---|---|---|---|\n|Ours (S)| **0.945** | **0.130**| 0.918| **0.140**|**0.926**|**0.263**|\n|Ours w/o GMP (S)|0.942|0.141|**0.920**|0.160|0.882|0.331|\n|Ours (U)|**0.840**|**0.102**|**0.828**|**0.099**|**0.860**|**0.179**|\n|Ours w/o GMP (U)| 0.817|0.116|0.820|0.122|0.777|0.307|\n---\n\nWe introduced a foot skating metric to assess foot sliding artifacts, observing that our proposed GMP effectively mitigates this issue. While the root motion's impact on style classification results is limited due to its 2D nature in the 260-D pose vector, it improves stylization accuracy on the Xia dataset by 9%. Emphasizing the primary objective of GMP, we illustrate in our 3rd and 4th supplementary videos how it facilitates adaptive pacing for diverse motion contents and styles, showcasing different stylization outcomes (label-based and motion-based) for the same content.\n\nThese tables and discussions have been incoporated in the revised manuscript, as in Appendix A., and table 9 and table 10.\n\n---\n***Q2**: Given multiple components and modules are proposed to construct the proposed method, a complete ablation study is expected to validate their effectiveness.*\n\n**A2**: We appreciate the reviewer's suggestion and have conducted a comprehensive ablation study on all relevant designs in our model. Below are partial results of motion-based stylization in the supervised setting, and the full comparisons are available in Table 6 of the revised version. Note that symbols * and # represent the Aberman and Xia datasets, respectively.\n\nMethods | Style Acc* |Style FID*|Geo Dist*|Style Acc#|Content Acc#|Content FID#|Geo Dist#\n---|---|---|---|---|---|---|---\nOurs| **0.945**|**0.020**|**0.344**|**0.926**|**0.674**|**0.189**|**0.680**\nw/o latent|0.932 |0.022|0.463|0.851|0.654|0.258|0.707\nw/o homo-style|0.883|0.032|0.507|0.851|0.537|0.232|0.760\nw/o cycle-recon|0.917|0.021|0.385|0.872|0.627|0.208|0.699\n---\n***Q3**: The results are focused on motion generation in the skeleton representations. Can the authors also propose some visualization with SMPL LBS? With the mesh, sometimes it can be more intuitive to estimate the plausibility of the joint angles.*\n\n**A3**: We would like to clarify that SMPL visualization is outside the research scope of this work. For character animation, we have provided visualization examples using clothed 3D avatars in our submitted videos under the name '2_label_based_stylization.' Or you could easily access through this [anonymous link](https://drive.google.com/file/d/1XrpkLLzWHrGHFtJJuLSUELIXBoENdHEM/view?usp=drive_link). Additionally, fitting SMPL shape and poses is not easy in our cases due to the structural differences between our skeleton and SMPL.\n\n---\n*We sincerely hope that we have properly addressed your concerns. If not, we are happy to open further discussions.*"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407417392,
                "cdate": 1700407417392,
                "tmdate": 1700407417392,
                "mdate": 1700407417392,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iY5ALZTBnu",
                "forum": "daEqXJ0yZo",
                "replyto": "O5LYeNKsP7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4166/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4166/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional Response to Reviewer bi3M"
                    },
                    "comment": {
                        "value": "Dear Reviewer bi3M,\n\nThanks for your time and effort on reviewing our work, and especially your suggestions on additiona analysis of global motion prediction and ablation analysis. The author-reviewer period will be end in two days, we are eager to know if our response adequately addressed your concerns. If you have additional questions, please feel free to let us know.\n\nBest,\n\nPaper 4166 Authors."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600502196,
                "cdate": 1700600502196,
                "tmdate": 1700600502196,
                "mdate": 1700600502196,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3dTl2msgES",
                "forum": "daEqXJ0yZo",
                "replyto": "rxjTB7mRPz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4166/Reviewer_bi3M"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4166/Reviewer_bi3M"
                ],
                "content": {
                    "title": {
                        "value": "Feedback"
                    },
                    "comment": {
                        "value": "Thanks for the reply from the authors to address my concerns.\n\nThe added experiment results under A1 are great and my concern about the expressiveness of the vector representation has been relieved by the results. I keep my positive rating to this submission."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617730700,
                "cdate": 1700617730700,
                "tmdate": 1700617730700,
                "mdate": 1700617730700,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rU79OMbLIV",
            "forum": "daEqXJ0yZo",
            "replyto": "daEqXJ0yZo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4166/Reviewer_rCKX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4166/Reviewer_rCKX"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new human motion stylization method based on a latent editing approach. The approach is based on a latent motion space which can differentiate the content and style of the motion. Based on the label or prior information of the style, the proposed approach can generate stylized motion results. The proposed approach is validated on several motion benchmarks like Aberman and CMU Mocap. Also, an user study is reported to further validate the performance of the approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of generate stylized motion sequence is interesting and helpful in many industry applications. \n2. The proposed approach is well validated in several motion benchmark quantitatively. Also, a user study is provided to further validated the performance of the approach."
                },
                "weaknesses": {
                    "value": "1. How to differentiate between the content and style of motion is not a trivial work. Usually, these two content is interleaved. The proposed approach seems to implicitly to decouple these two factors. Is it possible to provide a validation of the proposed approach can decouple these two content?\n\n2. For the experiments, the metrics may not well validate the performance of the proposed approach. For example, the style accuracy is based on a style classifier, which may be very accurate to capture the styleness of the proposed algorithm. Also, for case with a styled motion input, the transfer result based on the stylized input may not be easily verified. \n\n3. In the attribute edting literature, scholars are actively involved in the style edting, for example, to interpolate two style codes. Is it possible to generalize the generative ability of the proposed approach for more editing operation based on the latent space. \n\n4. How about the performance of the proposed approach on the long motion sequence? Is it possible to maintain the style for the whole motion sequence rather than a short period."
                },
                "questions": {
                    "value": "Please well address the questions in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4166/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4166/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4166/Reviewer_rCKX"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4166/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698654565942,
            "cdate": 1698654565942,
            "tmdate": 1700654252903,
            "mdate": 1700654252903,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OLNcDF4SAK",
                "forum": "daEqXJ0yZo",
                "replyto": "rU79OMbLIV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4166/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4166/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer rCKX (part 1/2)"
                    },
                    "comment": {
                        "value": "**We thank the reviewer for the comments, and hope the following response properly address your concerns.**\n\n***Q1**: The proposed approach seems to implicitly to decouple these two factors. Is it possible to provide a validation of the proposed approach can decouple these two content?*\n\n**A1**: Explicitly demonstrating the disentanglement of style and content can be challenging due to the absence of a ground truth for these representations. Nevertheless, we recognize the importance of addressing this question and have taken steps to enhance confidence in our model.\n\nTo offer insights into the disentanglement, we visualized the extracted content code and style code separately. The reviewer can refer to Figure 8 in the revised manuscript or visit this [anonymous link](https://drive.google.com/file/d/15vnVDOvggH3jyyCVHQRMGO-mA45gA3Ue/view?usp=drive_link). This analysis is conducted using our unsupervised stylization model.\n\n\nFor content code visualization, we extracted content codes from the Xia et al. dataset, which includes both content and style labels for each motion sequence. The temporal content codes are summed along the time dimension and projected to 2D space using t-SNE. In subfigure (a), we color the content samples based on their content labels (left) and style labels (right). The left figure demonstrates a strong association between content code clusters and their content labels (e.g., kicking, punching, jumping), while the right figure shows even distribution of style labels within each cluster. This indicates that the content code encapsulates semantic information while removing style identities.\n\nFor style code visualization, we extracted style codes from the Aberman dataset, which includes style label annotations. The style codes are similarly projected to 2D space using t-SNE and colored by their corresponding style labels. Notably, these style code clusters align with their labels, even though they are learned unsupervisedly and implicitly. This demonstrates the capability of our style encoder captures the style characteristics from the motion corpus. Unfortunately, the Xia dataset is not suitable for this case due to the shorter durations of its motions compared to the 6s input required for our style encoder.\n\nThese analysis and content have been added in our revised manuscript.\n\n---\n\n***Q2**: For the experiments, the metrics may not well validate the performance of the proposed approach. For example, the style accuracy is based on a style classifier, which may be very accurate to capture the styleness of the proposed algorithm.*\n\n**A2**: We understand the concern about using a style classifier for stylization accuracy. However, it's important to note that in real-world scenarios, the perception of style is subjective and may not have a universally accepted defintiion. Our use of a style classifier is a practical attempt to quantify and measure style in a consistent manner, even if it may not capture all nuances. It is also widely adopted in the prior works [1, 2, 3, 4, 5], even when dealing with inputting style motions[1,2, 4,5]. \n\nMoreover, we acknowledge that relying on a single style metric may have limitations. We have made our best efforts to ensure a fair and comprehensive evaluation. This includes extending existing metrics such as content accuracy, content/style FID, geodesic distance and inference cost. Additionally, we conducted subjective user studies to gather qualitative feedback. We are also open to other ideas if the reviewer think it necessary.\n\n---\n\n***Q3**: In the attribute edting literature, scholars are actively involved in the style edting, for example, to interpolate two style codes. Is it possible to generalize the generative ability of the proposed approach for more editing operation based on the latent space.*\n\n**A3**: We would like to remind the reviewer that we have included visualizations of both cross-style and homo-style interpolations in our supplementary videos, specifically in the file named \"5_interpolation.mp4.\" Or you could visit through this [anonymous link](https://drive.google.com/file/d/1ygkcFROeHOb2R8Sog34IAYMMQqnsCk-_/view?usp=drive_link\n). These visualizations demonstrate the smoothness and compactness of the learned style space.\n\n---\n\n[1] Unpaired Motion Style Transfer from Video to Animation, ACM Transactions on Graphics (TOG), 2020.  \n[2] Motion Puzzle: Arbitrary Motion Style Transfer by Body Part, ACM Transactions on Graphics (TOG), 2022.  \n[3] Style-ERD: Responsive and Coherent Online Motion Style Transfer, CVPR, 2022.  \n[4] Autoregressive Stylized Motion Synthesis with Generative Flow, CVPR 2021.  \n[5] Diverse Motion Stylization for Multiple Style Domains via Spatial-Temporal Graph-Based Generative Model, SCA, 2021."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700399284035,
                "cdate": 1700399284035,
                "tmdate": 1700399800825,
                "mdate": 1700399800825,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UKAZyr94gd",
                "forum": "daEqXJ0yZo",
                "replyto": "rU79OMbLIV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4166/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4166/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer rCKX (part 2/2)"
                    },
                    "comment": {
                        "value": "***Q4**: How about the performance of the proposed approach on the long motion sequence? Is it possible to maintain the style for the whole motion sequence rather than a short period.*\n\n**A4**: Our model is designed to stylize motions with arbitrary length, as the content encoder utilizes a 1D CNN window sliding along the temporal dimension. In our submitted videos, you can find many motions of variable length, such as stylized text2motion. Additionally, we have included two animations specifically showcasing long motion stylization in the revised supplementary videos, labeled as \"7_additional_results(rebuttal).mp4.\" Or you could visit through this [anonymous link](https://drive.google.com/file/d/1JcceadXfFwLgDwnlLf0yZ-L_tQ02ZShh/view?usp=drive_link).\n\n---\n*Hope our reply satisfactorily address your concerns. Otherwise, we will be happy to further discuss.*"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700399751936,
                "cdate": 1700399751936,
                "tmdate": 1700399751936,
                "mdate": 1700399751936,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OwnIHCC153",
                "forum": "daEqXJ0yZo",
                "replyto": "rU79OMbLIV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4166/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4166/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional Response to Reviewer rCKX"
                    },
                    "comment": {
                        "value": "Dear Reviewer rCKX,\n\nWe sincerely thank you for reviewing our work. This is a kind reminder that the author-reviewer period will be over in two days. Please feel free to let us know if you have any other questions.\n\nBest,\n\nPaper 4166 Authors."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600259849,
                "cdate": 1700600259849,
                "tmdate": 1700600259849,
                "mdate": 1700600259849,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ee9NAUG5jA",
                "forum": "daEqXJ0yZo",
                "replyto": "OLNcDF4SAK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4166/Reviewer_rCKX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4166/Reviewer_rCKX"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses. The authors have well addressed my concerns in last round. I will shift my rating to the positive side."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654221665,
                "cdate": 1700654221665,
                "tmdate": 1700654221665,
                "mdate": 1700654221665,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pXgDXPxULJ",
            "forum": "daEqXJ0yZo",
            "replyto": "daEqXJ0yZo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4166/Reviewer_rvJ1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4166/Reviewer_rvJ1"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an innovative framework for human motion stylization. The method breaks down the motion latent code into a content code and a style code. It leverages AdaIN to infuse style information into the semantic content, dictated by the content code. Furthermore, the authors introduce a new learning approach that encompasses reconstruction, cycle consistency, and homostyle alignment."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper demonstrates impressive quantitative and qualitative results, establishing a new state-of-the-art with a substantial margin.\n\n2. The proposed framework is novel. The associated conclusions and experiments make significant contributions to the research community.\n\n3. The paper is well-written, ensuring that its content is easily understandable for readers."
                },
                "weaknesses": {
                    "value": "My main concern primarily revolves around the selection between probabilistic and deterministic methods when extracting the content code and style code. The authors should incorporate further discussion and analysis in the following areas:\n\n1. The authors should provide more intuitive explanations for why deterministic encoding is used for the content code while probabilistic encoding is used for the style code.\n\n2. The authors need to conduct an ablation study to validate their conclusions regarding this design aspect."
                },
                "questions": {
                    "value": "Please kindly refer to the weaknesses mentioned above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4166/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830657330,
            "cdate": 1698830657330,
            "tmdate": 1699636382264,
            "mdate": 1699636382264,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5qxeZyztoc",
                "forum": "daEqXJ0yZo",
                "replyto": "pXgDXPxULJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4166/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4166/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer rvJ1"
                    },
                    "comment": {
                        "value": "**Thank you for your encouraging comments and constructive suggestions. They are indeed important to improve the clarity of our work. Meanwhile, your questions are answered as below.**\n\n***Q1**: The authors should provide more intuitive explanations for why deterministic encoding is used for the content code while probabilistic encoding is used for the style code.*\n\n**A1**: The main reason of using probabilistic encodeing for style code is to deriving generative ability for diverse or novel style sampling. While for content code, we instead use deterministic encoding for two reasons. Firstly, We do not expect generative ability from temporal content space (it is also difficulty to have). Secondly, unlike styles, which often represent global features of motions, content features exhibit more locality and determinism. These features correlate with precise details in local contexts, making deterministic encoding more appropriate and alleviating the learning burden.\n\nWe agree this could be important as it's part of our model design. We have added the content in sec 3.2.1.\n\n---\n***Q2**:  The authors need to conduct an ablation study to validate their conclusions regarding this design aspect.*\n\n**A1**: Thanks for the sugguestion. For the analysis of this selection, we conduct an ablation experiment on the supervised motion-based stylization setting.  We report the performance on Aberman et al(*) dataset and Xia et al (#) dataset. Note Xia dataset is unseen to model, aiming to test the generalization ability of models. We denote probabilistic encoder as P, and deterministic encoding as D. Firstly, we can see that probabilistic encoding of styles (second row) outperforms the deterministic encoding (first row) on both dataset with significant margins. For instance, the content accuracy achieved by the probabilistic style encoding on the Xia et al. dataset is 15% higher than that of the deterministic counterpart. Expanding our analysis to the content space, modeling it in a similar probabilistic manner (third row) yields nuanced performance gains in stylization accuracy. However, we observed a challenge in generalization to the Xia et al. dataset, resulting in a sharp performance drop. Meanwhile, the performance of probabilistic content encoding on *content presevation* (as indicated in Geo Dist, Content Acc, etc.) is also suboptimal, which also agrees with the assumption we made above. We also have added this table and related dicussion on the result in Table 7 in the Appendix. A.\n\n| Content Space   | Style Space | Style Acc*| Style FID*|  Geo Dist*| Style Acc#| Content Acc# | Content FID#| Geo Dist#| \n| ----  | ----  |----  | ----  | ----  | ----  |----  | ----  | ----  | \n| D | D|0.913|0.022|0.509|0.870|0.524|0.249|0.767|\n|D|P|0.945|0.020|**0.344**|**0.926**|**0.674**|**0.189**|**0.680**|\n|P|P|**0.947**|**0.017**|0.489|0.891|0.417|0.322|0.758|\n\n---\n\n**We sincerely hope our response addresses all your concerns. Please feel free to let us know if you have further questions.**"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700393602169,
                "cdate": 1700393602169,
                "tmdate": 1700393602169,
                "mdate": 1700393602169,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]