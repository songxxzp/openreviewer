[
    {
        "title": "One-Hot Encoding Strikes Back: Fully Orthogonal Coordinate-Aligned Class Representations"
    },
    {
        "review": {
            "id": "xG1Zigxsxs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1992/Reviewer_vbSf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1992/Reviewer_vbSf"
            ],
            "forum": "UQNSe4Da29",
            "replyto": "UQNSe4Da29",
            "content": {
                "summary": {
                    "value": "This paper proposes two techniques ICR and DCR for transforming existing class embeddings to be orthogonal and axis aligned for interpretability and better performance.\n\nThe brevity of the review doesn't stand for the quality of the review or of the paper, it is solely because of the prime questions I have about the paper."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The motivation for the problem setup (within in its assumptions) makes sense for the modern-day end-to-end learned representations.\n\nThe math behind the formulations and algorithms checks out and gives us a projection matrix that helps in improving the interpretability and accuracy in certain multi-class settings.\n\nThe applications to OOD also make quite a bit of sense."
                },
                "weaknesses": {
                    "value": "I found the paper's fundamental question confusing. If I understand correctly, authors want to take the \"spherical\" learned embeddings of the images from the penultimate layer -- which often are not disentangled. Then the authors want to compute the class prototype by the class mean which will not be orthogonal to other classes (by design). The goal is to transform these to be orthogonal using techniques like OPL and CIDER and then make them axis-aligned and binary through ICR and DCR. Please correct me if I am wrong in this.\n\nNow the questions are\n\n1) I do not see why we need orthogonal class representations -- because semantically I would like to have a substantial weight in the tail of similar but not the same classes. In case I do not want the semantic similarity between class prototypes to be smooth, one can normalize with the appropriate temperature. \n\n2) The second question is more about why we even need these transformations. If you learn a one-vs-all multi-class classifier for all these data points and classes you will end up generating a \"one-hot\" vector of dimensionality = number of classes. \n\nThis is the regular linear classifier with softmax after all the multi-class image classification networks we learn today. The classifier itself is the projection to ensure you obtain an orthogonal axis-aligned vector for each datapoint and in turn each class. Leaving it at softmax and not thresholing gives your semantically meaningful embeddings that can further be used for class embeddings. \n\nIt would be great if these questions could be resolved and the paper heavily leans on OPL and CIDER at times and would be good to have a short background section on the math behind them.\n\nI also point the authors to error-correcting output code line of work (Dietterich & Bakiri, 1995) and probably a concise survey (Kusupati et al 2021) spanning interpretable and learned ECOCs. They aim at learning sub-linear cost class representations in binary space (not necessarily orthogonal, but can be made and also made interpretable in attribute space). This helps result in axis-aligned attribute interpretable binary codes for classes.  This also deals with some notions of OOD detection. \n\n\n\nDuring the rebuttal, if I could get an understanding of this, that would help me be convinced to accept the paper."
                },
                "questions": {
                    "value": "see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1992/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697323367114,
            "cdate": 1697323367114,
            "tmdate": 1699636130980,
            "mdate": 1699636130980,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MOurL9dmms",
                "forum": "UQNSe4Da29",
                "replyto": "xG1Zigxsxs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1992/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1992/Authors"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "In addition to making class means aligned to axis, we actually make them orthogonal.  Although OPL and CIDR attempted to do this, we show that they came up significantly short of this goal.  Our method resolves this issue in making class means completely orthogonal.  \n\n\nQ1.  Why should two classes which are distinct (say apple and orange) be associated?  A fruit cannot be an apple and an orange?  Note that our representation does not stop a new image (not in labeled training set) from have a representation that thinks it might be an apple or an orange.  A data points can have associations with two classes, but the distinct classes do not need correlation with each other.  \n\nQ2.  If you just learn classifiers then, either:\n   - these can have the same unnecessary associations discussed in Q1.  If something is clearly an apple, that does not mean a classifier should also guess that it might be an orange.  Note this is indeed what happens without these techniques.  \n   - or these classifications are not part of the embedding layer.  So they do not aid to the interpretability.  Our goal is to put as much structure and utility into this one embedding layer.  There are many shifts in industrial machine learning to try to store the results of training as vectors (check out the new company \"pinecone\").  This aim of this is so that these learned representations capture the main desired properties of the data, and are also more interpretable.  \n\nWe believe that OPL and CIDR are two examples of ways to initially train a near-orthogonal embedding.  In principal they are somewhat interchangeable (although for SOTA in certain tasks, sometimes one is much better than there others for reasons probably beyond the scope of this paper).  The key part, we believe, is our method which finally achieves full orthogonality, and uncorrelated the representations of distinct labeled classes.  \n\nThe code book methods seem to have as an aim to compress learned representations.  Our goal is to work within the space of generic vectorized representations, but add enforced structure that do not detract from performance while improving interpretability."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1992/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231619180,
                "cdate": 1700231619180,
                "tmdate": 1700231619180,
                "mdate": 1700231619180,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3vPk2ssyeG",
                "forum": "UQNSe4Da29",
                "replyto": "MOurL9dmms",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1992/Reviewer_vbSf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1992/Reviewer_vbSf"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the rebuttal. \n\nI get that class prototypes being orthogonal does not mean the images will not have an ordering of the classes based on the inner product. However, it is sample inefficient to pack information that similar classes are as far apart as drastically different classes. I also understand that leveraging them for classification does not affect anything as long as the new basis preserves the orderings. \n\nI do not understand you point on the classifiers. A linear layer after embeddings later is still OK with interpretability and the probabilities essentially map to the class prototypes. I am familiar with vector databases, however, I still do not see what this brings to table, over the one hot vector for each classes, when each instance is still a real vector. \n\nI think you are missing the point of the codebook methods, your PoV is correct that they might be compressed representations, but they are ideally also solving what you need. \n\nI still am not convinced after the rebuttal and willing to discuss further with authors."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1992/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700252233110,
                "cdate": 1700252233110,
                "tmdate": 1700252233110,
                "mdate": 1700252233110,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zIQLOpjUgh",
            "forum": "UQNSe4Da29",
            "replyto": "UQNSe4Da29",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1992/Reviewer_E1pC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1992/Reviewer_E1pC"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a post-processing step to the training phase of a learned embedding in the context of  multi-class image classification. The main objective of the step is to obtain orthogonal class means while preserving linearly separable classes at the last layer of the network.\nTwo algorithms are proposed : Iterative Class Rectification and Discontinuous Class Rectification. Theoretical guarantees of the convergence of both methods are given and numerous experiments are carried out to demonstrate the orthogonality obtained and the preservation of performance in a classification context."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method is simple, its internal objectives are well described, and the theoretical part seems sound. \n- Numerous experiments are carried out"
                },
                "weaknesses": {
                    "value": "- The orthogonality objective of the method (although achieved) is not linked to a specific performance improvement in the experimental context,  which makes  its claim somewhat arbitrary. For example, the use of the post-processing step for Out-of-Distribution detection leads to a performance degradation.\n\n- In general, the choice of a smaller ResNet architecture (ResNet-9) does not allow to obtain experimental results equivalent to those provided by the mentioned methods for baseline comparison, although complete results and numerous details are provided.\n\n- The method does not seem to perform consistently depending on the evaluation criteria for image classification. This affects the evaluation of the performance in the case of classification.  Since classification performance does not seem to be the main objective of the post-processing steps : other experiments could have been tried, such as robustness to label noise or robustness to adversarial attacks, as for example in the OPL method paper.\n\n- From a broader point of view, it seems that the goal of orthogonalising the latent space has something in common with  the orthogonal classifier setting, which could provide further experiments and theoretical approaches (See for examples \u00ab Controling directions orthogonal to a classifier \u00bb, ICLR\u201922).\n\n- As ICR is an extension of ISR (Aboagaye et al.),  the scope of the contribution is limited, despite the scaling capacity of the proposed algorithms."
                },
                "questions": {
                    "value": "-What query related experiments in the SOTA could be used to evaluate the validity of the method ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1992/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1992/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1992/Reviewer_E1pC"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1992/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698745932050,
            "cdate": 1698745932050,
            "tmdate": 1699636130909,
            "mdate": 1699636130909,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V6jLoKLPjJ",
                "forum": "UQNSe4Da29",
                "replyto": "zIQLOpjUgh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1992/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1992/Authors"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "The main goals of the proposed methods are:\n  1. Make embeddings more interpretable by having some coordinates associated with class means, and so these aspects are completely orthogonal\n  2. Basically preserve the state of the art.  \n\nWe achieve these goals.  This method makes no attempt to directly further optimize performance metrics on any tasks, so we should not expect improvement -- although it sometimes happens."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1992/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231529459,
                "cdate": 1700231529459,
                "tmdate": 1700231529459,
                "mdate": 1700231529459,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C2cZDJSlhF",
                "forum": "UQNSe4Da29",
                "replyto": "V6jLoKLPjJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1992/Reviewer_E1pC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1992/Reviewer_E1pC"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the rebuttal. \nI think that the equivalence claim between the orthogonality goal and embedding explainability is still insufficiently grounded from a theoretical and experimental perspective."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1992/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562219697,
                "cdate": 1700562219697,
                "tmdate": 1700562219697,
                "mdate": 1700562219697,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dHgKshgV9V",
            "forum": "UQNSe4Da29",
            "replyto": "UQNSe4Da29",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1992/Reviewer_j8xu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1992/Reviewer_j8xu"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript presents a mechanism that takes state-of-the-art learned representations and modifies them to assign specific meanings to some of the coordinates. The method makes the representation of each class orthogonal to the others, and then changes the basis to be on coordinate axes. This adjustment aims to improve the interpretability of the representations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper delves into an interesting research direction to  leverage pre-trained encoders \n- The authors provide theoretical proofs that underpin the orthogonalization achieved by the proposed methods, granted certain assumptions are met"
                },
                "weaknesses": {
                    "value": "### Major weaknesses:\n\n- The experiments, conducted only on a Resnet-9 and limited to CIFAR10 and CIFAR100 datasets, lack the breadth needed to prove the generality of the method.\n \n- The assumption that class means should be entirely orthogonal raises questions. While it makes sense for entirely independent classes, it doesn't account for cases where classes share features or have relationships. The logic behind making every pair of classes orthogonal, especially when some classes naturally have similarities (e.g., apple and orange), remains unclear.\n\n- Despite emphasizing the method's potential for enhanced embedding interpretability, by orthogonalizing them, the paper does not provide empirical evidence that assesses this claim of improved interpretability.\n\n### Weaknesses:\n- The method primary objective and results appear straightforward, yet the paper presents the method in a very convoluted manner.\n\n- While the paper suggests possible advantages in downstream tasks by 'ignoring' particular classes or concept, there are not any experiment supporting this claim.\n\n### Minor: \n- The paper does not discuss the relationship with prototypical networks, leaving a potentially relevant connection unaddressed.\n\n- The paper claims that \"if the representations are successful, then for direct tasks only a simple classifier is required afterwards,\", however, it is arguable if a linear layer is always enough after a learned representation \n- Typo: Roccio algorithm"
                },
                "questions": {
                    "value": "- Why is it necessary for classes to be orthogonal before projecting them into the new basis? Wouldn't it be possible to simply compute the coefficients and execute a change of basis, e.g., using a least squares approach? The rationale behind the choice to orthogonalize vectors first and then execute a change of basis using an orthogonal matrix remains ambiguous. Why not directly learn a non-orthogonal matrix for this transformation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1992/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1992/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1992/Reviewer_j8xu"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1992/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783257984,
            "cdate": 1698783257984,
            "tmdate": 1699636130811,
            "mdate": 1699636130811,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0ZbAFg9oH7",
                "forum": "UQNSe4Da29",
                "replyto": "dHgKshgV9V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1992/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1992/Authors"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "Weakness #2:   We feel that distinct classes should indeed be uncorrelated.  Although apples and oranges are both fruits they are completely different fruits, and a labeled image is in exactly one such class.  For an unknown object (not labeled in the training set), it is still possible for it to have association with both classes if it is not clear which it is.  However, if it is clearly an apple, that should *not* imply that it is associated with the class orange.  Our orthogonal representation allows this.  \n\nWeakness #3:  We believe the example uses in Figure 2,3 provide clear evidence of the interpretability. \n\nWeakness #4:  We have updated the presentation of the method a bit based on another reviewers request.  However, we feel that although the objective is simple to state, it does not have a simple solution (we looked for ones, and prior simpler approaches did not achieve orthogonality).  If you have a simpler approach in mind, that would be great!  \n\nQuestion:  Attempts to learn how to make classes orthogonal (OPL and CIDR) did not fully orthogonalize class means.  We tried other variants which also did not achieve close to full orthogonality.  Our ISR method does.  \nWhat you are describing sounds similar to running one step of ISR; as we show running a few more iterations is required so that the class means of the embedded data are actually orthogonal.  Transformations that orthogonalize the vectors v1 and v2 at the current class means apply to the actual data non-uniformly, so if one recomputes the class means again afterwards, they are no longer orthogonal."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1992/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231486776,
                "cdate": 1700231486776,
                "tmdate": 1700231486776,
                "mdate": 1700231486776,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "enxfKlGxNb",
                "forum": "UQNSe4Da29",
                "replyto": "0ZbAFg9oH7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1992/Reviewer_j8xu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1992/Reviewer_j8xu"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the rebuttal.\n\n- (Weakness #2): I am still not convinced that they should be orthogonal, in general. Altough they indeed are completely different fruits, they share many features -- e.g. the shape. An orthogonal representation does not allow to capture shared features, and in general does not allow to learn any meaningful metric between samples.\n\n- (Weakness #3): While Figure 2,3 provide examples of interpretability in two specific samples, given the focus on this aspect a deeper qualitative or quantitative analysis would have greatly enhanced the work.\n\nI still am not convinced after the rebuttal and willing to discuss further with authors."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1992/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471028795,
                "cdate": 1700471028795,
                "tmdate": 1700471028795,
                "mdate": 1700471028795,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6ha6RcMs3F",
            "forum": "UQNSe4Da29",
            "replyto": "UQNSe4Da29",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1992/Reviewer_EbUE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1992/Reviewer_EbUE"
            ],
            "content": {
                "summary": {
                    "value": "While representations in deep learning have become more expressive over time, their interpretability has deteriorated over time as well. This paper advocates to recover the interpretability of the features produced by a deep learning model, by building on top of previous works that encourage sparse and compact representations, and proposing two post-hoc methods that further orthogonalizes these representations. Empirical results show that representations obtained with the proposed methods are indeed orthogonal and axis-aligned, while retaining the classification performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The motivation of the work is really well-driven in the introduction section, and it is really appealing.\n- Theoretical guarantees for both proposed algorithms are provided.\n- Empirical results show the effectiveness of the proposed approaches, as well as the problems of previous approaches.\n- I can easily see this work being leveraged by practitioners to improve the interpretability of their features."
                },
                "weaknesses": {
                    "value": "- W1. The introduction of the proposed methods is rather convoluted, short, and unclear. Besides, it relies too much on the reader having previous knowledge of how ISR works. The authors should work on providing more context and explanations to the reader. This is the biggest concern I have with the current state of the manuscript.\n- W2. Citations should be fixed, as well as references in the bibliography (e.g., some of them have no venues).\n- W3. The explanation of why we cannot simply run Gram-Schmidt (GS) is unclear to me and, in any case, traditional orthogonalization methods (GS, Householder transformations, etc.) should be added as baselines in the experimental section.\n- W4. I find the back-and-forth between using OPL vs. CIDER in sections 3.2 and 3.3 a bit too confusing\n- W5. The presentation of the results needs a bit more polishing. For example, no statistical results (e.g. standard deviations) are presented, and there is no point in having 3 decimals in Table 4 as the least accuracy is $100/10.000 = 0.01$."
                },
                "questions": {
                    "value": "- Q1. What does it mean that DCR uses a \"discontinuous operation\"? Discontinuous wrt what exactly?\n- Q2. Why is it sensible to normalize the class means? That is, why is it a good idea to completely disregard the magnitude of the class means (since $\\arg\\min_j D(x, v_j) \\neq \\arg\\min_j D(x, v_j / ||v_j||)$ in general)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1992/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831301071,
            "cdate": 1698831301071,
            "tmdate": 1699636130712,
            "mdate": 1699636130712,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1WPF42vXIP",
                "forum": "UQNSe4Da29",
                "replyto": "6ha6RcMs3F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1992/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1992/Authors"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "Thank you for the feedback on the writing of the paper, we have updated things to try to address these issues.  \n\nW3:  Gram-Schmidt can by applied to a pair of vectors (e.g., the class means), but we need an operation that applies to every embedded point, including ones were we may not know the label.  In short, it is not even clear what a baseline of Gram-Schmidt would be.  If there is something we are missing, please let us know what you have in mind.  \n\nW4:  We compared against the state-of-the-art in each task.  \n\nQ1:  DCR segments the embedding space, and applies a Gram-Schmidt-driven rotation to part of the embedding space, leaving the rest fixed.  The boundary between the transformed space and the fixed space causes a discontinuity in how the update step affects the embedding.  \n\nQ2:  The goal is interpretability of the embeddings.  We believe a normalized value will be more interpretable since the length now has some meaning.  \nIn particular, when the class means are all normalized, then the relative relationship for an evaluation point between classes can be easily understood by directly comparing their coordinates."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1992/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231458701,
                "cdate": 1700231458701,
                "tmdate": 1700231458701,
                "mdate": 1700231458701,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UHLabx78Vr",
                "forum": "UQNSe4Da29",
                "replyto": "1WPF42vXIP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1992/Reviewer_EbUE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1992/Reviewer_EbUE"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, sorry for the late reply, and thanks for your answer. I do not have any further questions. Again, I appreciate the clarifications."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1992/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646596990,
                "cdate": 1700646596990,
                "tmdate": 1700646596990,
                "mdate": 1700646596990,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]