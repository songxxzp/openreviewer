[
    {
        "title": "Multi-Agent Interpolated Policy Gradients"
    },
    {
        "review": {
            "id": "T5DHOik5mM",
            "forum": "mPONXmVmZ6",
            "replyto": "mPONXmVmZ6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4339/Reviewer_4F8p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4339/Reviewer_4F8p"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce an approach to interpolate between using a joint Q function and factorised Q function in order to find a better balance in the variance bias trade-off. The idea is supported by some theoretical results that study the bias produced by the new objective in comparison with the original MARL objective."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well written and the idea, though being intuitive, seems novel and an effective tool for tackling an important problem in MARL. The authors have put effort into studying the properties of their proposed method with the theoretical analysis offering some useful insights.\n\nThe empirical results show that the method does deliver improvements in performance."
                },
                "weaknesses": {
                    "value": "Despite the fact that the idea is novel, it is hard to fully evaluate the benefits of the contribution given some relevant work has seemingly been missed by the authors; specifically, these works seem highly relevant:\n\n[1] Kuba, Jakub Grudzien, et al. \"Settling the variance of multi-agent policy gradients.\" Advances in Neural Information Processing Systems 34 (2021): 13458-13470.\n\n[2] Rashid, Tabish, et al. \"Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning.\" Advances in neural information processing systems 33 (2020): 10199-10210.\n\n[3] Mguni, David Henry, et al. \"MANSA: learning fast and slow in multi-agent systems.\" International Conference on Machine Learning. PMLR, 2023.\n\nThough [2] tackles value-based methods I think it is still worth discussing. Similarly, the method in [3] learns the best set of states to switch from using a decentralized critic to centralized critic. I caveat this with the fact that the specific algorithms involved in [3] are value-based methods but given that it is plug & play, the method seemingly captures gradient methods.Without having included a discussion on [3], it is hard to know how much this approach could be useful since the authors' approach has a fixed weighting parameter for all states whereas the approach in [3] can be viewed as a weighting variable whose optimal value {0,1} is learned for each state. \n\n\nThe theoretical analysis though insightful didn't allow me to fully grasp an improvement in the performance of the proposed method with regard to variance and bias. I was expecting to see some results that indicate that for a given variance, the method achieves a reduced level of bias and similarly, for given level of bias the method would achieve a lower variance as compared to the standard objective. This has been shown nicely in the ablation study but I didn't see the corresponding analytic statements for this. \n\nIt is not clear if there are situations where this method would under-perform. For example, I can see a potential for choosing greedily over either $Q^\\pi$ or $Q^\\nu$ yielding locally optimal actions but doing the same over their convex combination yielding a poor action. I would like to have seen a discussion on whether this is possible and under what conditions. \n\nMinor\n\nOn page 6 it is written that the assumption of the Lipschitz smoothness of the gradient is reasonable since \"Q functions tend to be smooth in most environments\". I think this is slightly problematic since in many RL environments, the reward is sparse in these environments, so even the Q function is not so smooth. Besides, the smoothness of the Q functions does not suggest smoothness of its gradient."
                },
                "questions": {
                    "value": "* What are the benefits/weaknesses as compared to [3]?\n\n* For a given variance can it be shown analytically that the method achieves a reduced level of bias and similarly, for given level of bias does the method achieve a lower variance as compared to the standard objective?\n\n* Can the authors discuss how this method would perform in situations, if they exist, where $Q^\\pi$ and $Q^\\nu$ may have different maxima. A coordination game such as the stag-hunt may be one such situation.\n\n\n\n[3] Mguni, David Henry, et al. \"MANSA: learning fast and slow in multi-agent systems.\" International Conference on Machine Learning. PMLR, 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4339/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4339/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4339/Reviewer_4F8p"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4339/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838330676,
            "cdate": 1698838330676,
            "tmdate": 1700391031255,
            "mdate": 1700391031255,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZkbFIP2Z32",
                "forum": "mPONXmVmZ6",
                "replyto": "T5DHOik5mM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4339/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We express our sincere gratitude to Reviewer 4F8p for their invaluable insights, which have significantly enhanced the robustness of our work. In response to the specific concerns raised, we present a thorough clarification of each point.\n\n1. **Related works**.\n\nIn the revised version of our paper, we have incorporated a discussion on references [a][b][c]. \n\nReference [a] is particularly relevant to our work as it addresses variance, and we emphasize that there is no conflict between [a] and our method. It is possible to integrate an optimal baseline from [a] into our method by replacing the advantage estimator in Equation 10 with Equation 12 from [a]. However, this modification requires the estimation of joint Q-functions, and GAE is not compatible, potentially impacting the performance of PPO-based gradients.\n\nReference [b], a milestone work on value factorization, shares similar ideas when analyzing the Bellman operator of a factorized function class, and we have included it in the related works.\n\nReference [c] proposes a promising approach using switch control to dynamically switch from independent learning to centralized learning. We believe similar methods could be applied to dynamically adjust the weight parameter $\\nu$ in our approach. However, achieving this requires further efforts in both theoretical and experimental aspects, as discussed in the related and future works section of our revised paper.\n\n[a] Kuba, Jakub Grudzien, et al. \"Settling the variance of multi-agent policy gradients.\" Advances in Neural Information Processing Systems 34 (2021): 13458-13470.\n\n[b] Rashid, Tabish, et al. \"Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning.\" Advances in neural information processing systems 33 (2020): 10199-10210.\n\n[c] Mguni, David Henry, et al. \"MANSA: learning fast and slow in multi-agent systems.\" International Conference on Machine Learning. PMLR, 2023.\n\n2. **Analytical Comparison of Bias and Variance**.\n\nRegarding the question on achieving reduced bias and variance, our method intentionally introduces bias to reduce variance compared to the standard unbiased objective. \n\nIn Equation 10, the first term, requiring a sample from the joint action space, contributes to high variance, while the second term does not. Consequently, in contrast to the standard objective ($\\nu=0$), our method reduces the variance of the high variance term by a factor of $1-\\nu$, simultaneously introducing a low variance term with a coefficient of $\\nu$. For a more comprehensive understanding, refer to Equation 13, where the variance of the high variance term is explicitly reduced by $(1-\\nu)^2$.\n\nIt's important to note that, as the standard objective is unbiased, our method does not achieve reduced bias compared to it. The term \"low bias\" in our context reflects that the additional bias introduced by the factorized function is comparatively modest. This acknowledgment is particularly pertinent considering that a factorized function has the potential to introduce significant bias.\n\n\n3. **Performance with Different Maxima of $Q^\\pi$ and $Q^\\mu$**.\n\nWhile we do not directly use $Q^\\pi$, we understand the concern about the potential impact on policy performance if $Q^\\pi$ and $Q^\\mu$ have different maxima. Although combining two value functions with different maxima could lead to a suboptimal policy, we argue that such a scenario is unlikely to occur and is rarely observed. To demonstrate this, we conducted a matrix game, the details of which are provided in Appendix D3 of the revised paper. The nature of learning Q functions and our choice of on-policy implementation contribute to mitigating the likelihood of disparate maxima between $Q^\\pi$ and $Q^\\mu$."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700049602827,
                "cdate": 1700049602827,
                "tmdate": 1700049602827,
                "mdate": 1700049602827,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o27lRBg1um",
                "forum": "mPONXmVmZ6",
                "replyto": "T5DHOik5mM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4339/Reviewer_4F8p"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4339/Reviewer_4F8p"
                ],
                "content": {
                    "title": {
                        "value": "Re:"
                    },
                    "comment": {
                        "value": "Thanks. \n\n1/Related works. \n\nMy main concern remains as it seems that reference [c] seems to do something similar  to solving this problem but allows the value of $\\nu$ to vary at each state      which seems to be a more powerful approach. In light of this I would like to have seen some statements as to why this paper is a useful contribution given [c]. \n\n2 + 3/bias \n\nMy concern here is that the method introduced here seems to lead to biased solutions (and maybe even in the asymptotic training regime) - using this method it seems we lose convergence guarantees to any useful stable point. If so, because in general the stable points/equilibra of multi-agent systems are extremely sensitive to the objective parameters we could end up converging to a point a long way from any sort of local optimum (with arbitrarily bad solutions). This seems to be a significant issue. \n\nI would like to see at the very least some analysis on the conditions when this bias would be relatively small. Alternatively, if the authors could show that the bias is small in a vast number of randomly generated games that could help."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147600844,
                "cdate": 1700147600844,
                "tmdate": 1700148675000,
                "mdate": 1700148675000,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XghjLJcstR",
                "forum": "mPONXmVmZ6",
                "replyto": "T5DHOik5mM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4339/Reviewer_4F8p"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4339/Reviewer_4F8p"
                ],
                "content": {
                    "title": {
                        "value": "Re:"
                    },
                    "comment": {
                        "value": "Thanks. These responses have allayed my concerns.\n\nIf the authors could incorporate their responses, particularly, precise technical statements for point 2 in the script, I would be happy to raise my original score.\n\nApart from that, I believe that the empirical support for the authors' claim in point 2 would be greatly strengthened if the paper included supporting experiments across a range of (say, randomly generated) normal form games (for which standard methods converge)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700253604946,
                "cdate": 1700253604946,
                "tmdate": 1700253881338,
                "mdate": 1700253881338,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3osh6VNHJB",
            "forum": "mPONXmVmZ6",
            "replyto": "mPONXmVmZ6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4339/Reviewer_ZGxu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4339/Reviewer_ZGxu"
            ],
            "content": {
                "summary": {
                    "value": "In order to explore the bias-variance trade-off of the policy gradient in MARL, this paper considers a convex combination of the joint Q-function (with coefficient $1- \\nu$) and a factorized Q-function (with coefficient $\\nu$) and applies the policy gradient to this new function. They then establish some bounds on the bias of this function ($\\propto \\nu$) and the variance of its gradient ($\\propto (1 - \\nu)^2$). Finally, they provide some experiments for different values of $v$ to support their results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) The paper investigates the idea of using this convex combination for the multi-agent case, which, based on their claims, was only done before for the single-agent case.\n\n(2) There is a good variety of experiments that support the arguments made in the paper and also show the algorithm is flexible in the sense that it can employ different value factorization methods."
                },
                "weaknesses": {
                    "value": "(1) Although this paper is for the multi-agent case and also employs a different approach in the implementation (on-policy instead of off-policy), the idea and the bounds are too similar to that of the single-agent paper referenced in section 5. In this review's opinion, this makes the result incremental and not novel and significant enough for consideration at this venue. \n\n(2) A number of inconsistencies with the notations that make it hard for the average reader to precisely follow the claims of the paper. For instance, the notations $\\nabla J$ and $\\nabla \\hat{J}$ are used interchangeably whereas they are not the same (look at and compare equations (1), (10), and (12)). Also, $\\hat{Q}$ is used in equation (12) despite it not being properly defined until equation (14). Plus, sometimes the notations $\\hat{Q}$ and $Q^\\mu$ are used interchangeably even though they are clearly different when $Q^\\mu$ is not in the function class $\\mathcal{Q}$. \n\n(3) There seems to be a mistake in the proof of Proposition 4. The upper bounds $\\frac{1}{2} L \\sigma^2$ and $L \\sqrt{mn}$ are supposed to be on the absolute value of the difference of $Q^\\mu (s,a)$ and its first order estimate (initialed at $\\mu(s)$); however, they are apparently used as upper bounds on $(Q^\\mu (s,a) - \\overline{Q}(s,a))^2$. So it seems the result should have been something like $c_1 L^2 \\sigma^4 + L^2 mn \\sigma^2$ instead of $c_1 L \\sigma^2 + L \\sqrt{mn} \\sigma^2$ which would change the final bound as well. \nOn another note, the absolute value should be outside the expectation for the second term in the last inequality of equation (27), and that is what makes the last argument of the proof (Proposition 4) possible."
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4339/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4339/Reviewer_ZGxu",
                        "ICLR.cc/2024/Conference/Submission4339/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4339/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699399574078,
            "cdate": 1699399574078,
            "tmdate": 1700638907025,
            "mdate": 1700638907025,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gtL8JSAsa4",
                "forum": "mPONXmVmZ6",
                "replyto": "3osh6VNHJB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4339/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We extend our sincere appreciation to Reviewer ZGxu for the thorough evaluation of our manuscript. We are grateful for the time and dedication you have invested in providing constructive feedback. Below, we address each of the reviewers' concerns individually.\n\n1.**The Incremental Nature of Results**.\n\nWe would like to clarify certain aspects where there might be a misunderstanding regarding the novelty and significance of our work.\n\nFirstly, our approach significantly deviates from single-agent papers referenced in Section 5. The integration of a joint Q-function and factorized function in our work is motivated by a distinct trade-off between expressive ability and learning difficulty. As detailed in Appendix A.1, our exploration of a convex combination highlighted its unsuitability for value-based methods, leading us to focus on policy-based methods. \nAlthough the resulted policy gradients share some similarity with IPG, our motivation is fundamentally different from any single-agent paper referenced.\n\nSecondly, our theoretical results, particularly Proposition 4, provide distinctive insights into bounding the objective function. The analysis of bias induced by a factorized Q function, discussed in Section 4.2, represents a novel contribution absent in single-agent settings. \nAdditionally, in Section 4.3, we introduced the concept of compatible function approximation in multi-agent reinforcement learning, exploring its relationship with value factorization.\n\nThirdly, the \u201cdifferent approach in the implementation (on-policy instead of off-policy)\u201d is not an arbitrary choice. It aligns seamlessly with our theoretical findings, as used in Equation 14 and later on, and serves to prevent poor performance, as detailed in Appendix F. This approach is a deliberate and well-justified decision based on our theoretical framework.\n\nIn summary, while there may be similarities in addressing the bias-variance trade-off, our work represents a distinct and valuable contribution to the field. \nThe strong relationship between the idea and bounds to value factorizations in the multi-agent settings sets our paper apart.\nOur implementation choices are grounded in the theory and have been carefully considered to yield consistent and meaningful experimental results. We believe these aspects collectively establish the novelty and significance of our proposed method.\n\n2. **Inconsistent Notations**.\n\nWe appreciate the reviewer's observation regarding inconsistent notations, and we would like to provide further clarification. In our paper, $J$ denotes the standard unbiased objective for policy gradient, while $\\hat{J}$ represents the objective of our proposed method. Additionally, $\\hat{Q}$ is introduced in the third line of the second paragraph of section 3.2, serving as an estimation of $Q^\\mu$. To enhance consistency, we have updated the notations in the paper. Specifically, in Equation 12, we replaced $J$ with $\\hat{J}$, and in Equation 10, $Q^\\mu$ has been replaced by $\\hat{Q}$.\n\n3. **Mistake in the Proof of Proposition 4**.\n\nCertainly, we acknowledge the oversight in Proposition 4. In response, we have revised both the proposition and its proof in the paper. Notably, we have refined the definition of $\\sigma$ for conciseness."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700049135365,
                "cdate": 1700049135365,
                "tmdate": 1700622982693,
                "mdate": 1700622982693,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s3wcmwzT2h",
                "forum": "mPONXmVmZ6",
                "replyto": "gtL8JSAsa4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4339/Reviewer_ZGxu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4339/Reviewer_ZGxu"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification and the changes made in the revision. I understand the novelty of the work better now.\n\nThe paper is much more readable now due to fixing the notations, and the proof is also taken care of. I am happy to change my original score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638762340,
                "cdate": 1700638762340,
                "tmdate": 1700638762340,
                "mdate": 1700638762340,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cVfwYfP05r",
            "forum": "mPONXmVmZ6",
            "replyto": "mPONXmVmZ6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4339/Reviewer_JyH2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4339/Reviewer_JyH2"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the setting of multi-agent reinforcement learning and proposes the method Multi-Agent Interpolated Policy Gradient which allows for trade-off between variance and bias. Theoretical analysis of the proposed method gives an expression for the variance of the gradient and shows the effectiveness of the method in reducing variance. An upper bound on the bias introduced by incorporating a factorized Q function is also given and it is shown how by tuning a parameter bias and variance can be balanced. Finally empirical results are presented that compare the performance of the proposed method with other baselines and also ablation studies are conducted that study the effect of various design choices."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method uses a control variate, in this case factorized Q function, to reduce the variance of the policy gradient. The idea has been explored in the setting of single agent reinforcement learning and the paper extends it to multi-agent setting. Although the idea behind the main method is not original it is still a solid one and the extension of the same to multi-agent setting is significant. \n\nThe main body of the paper is presented clearly for most part and the flow of the contents is also natural."
                },
                "weaknesses": {
                    "value": "The empirical results for the proposed method are not convincing. Although in GRF domain the proposed method gives better performance as compared to baselines the same is not true for the SMAC domain. If results on another benchmark could be provided it would make the empirical results stronger."
                },
                "questions": {
                    "value": "1. In Algorithm 1, you mention that you use recurrent neural networks in policy networks $\\pi_\\theta$, state-value networks $V^\\varphi$ and action-value networks $Q^\\psi$. Given that in your setting each agent observes the whole state I don't see why you need it? Also, how would the performance be affected by its absence?\n\n2. In proof of Proposition 2, you prove equation 19 first and then use that to rewrite eq. 10. What are the intermediate steps involved in this?\n\n3. Effect of bias on the performance of the algorithm: Does a low value of $\\nu$ give a better performance in most of the scenarios? I know that the effect of $\\nu$ is investigated in ablation studies but since the number of experiments there is small I was wondering if the behavior seen in ablation studies holds in general or not.\n\n4. For the GRF domain, QMIX was not included in the baselines. Is there a reason for this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4339/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4339/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4339/Reviewer_JyH2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4339/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699493481816,
            "cdate": 1699493481816,
            "tmdate": 1699636403925,
            "mdate": 1699636403925,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YYDNQCaC6f",
                "forum": "mPONXmVmZ6",
                "replyto": "cVfwYfP05r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4339/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We express our gratitude to Reviewer JyH2 for recognizing the strength of our work. We now address the concerns raised by the reviewer:\n\n1. **Concerns about the Empirical Results**.\n\nWhile our method outperforms other baselines, it is essential to address the QMIX's comparable performance in several SMAC maps. Notably, QMIX is an off-policy method, incurring higher computational intensity and operating at about one-third the speed of our method. We have incorporated an additional plot illustrating compute time on the x-axis for a comprehensive comparison, as shown in Figure 8 of the revision of our paper.\n\n2. **Rationale for Utilizing Recurrent Neural Networks**.\n\nAs indicated in the fourth line of the preliminary section, our approach is designed for Dec-POMDP, and the choice of full observation is made to simplify theoretical analysis. Given that SMAC is partially observable, the integration of RNNs proves beneficial for enhancing performance.\n\n3. **Clarification on Intermediate Steps from Equation 19 to Equation 10**.\n\nWe can directly define\n $\\mu(s) := E_{\\pi(\\xi)}[f(s,\\xi)]$\n , then \n $E_{\\pi(a,\\xi|s)}$$[\\nabla_\\theta f_\\theta(s,\\xi)$$\\nabla_aQ(s,a)]= \nE_{\\pi(a|s)}[\\nabla_\\theta \\mu(s)\\nabla_aQ(s,a)]$.\nIn other words, in the context of a Gaussian policy, where $a= f_\\theta(s,\\xi)=\\mu_\\theta(s)+\\Sigma_\\theta(s)^{1/2}\\xi$ and $\\xi\\sim\\mathcal{N}(0,1)$, \nwe have\n$E_{\\pi(\\xi)}[f(s,\\xi)] = \\mu_\\theta(s)$.\n\n4. **Impact of $\\mu$ Value on Performance Across Scenarios**.\n\nIn our experiments, we observed that a $\\mu$ value of 0.3 in SMAC and 0.5 in GRF yielded optimal performance, which can hardly be considered \"low\". However, in tasks with unnormalized reward, the choice of $\\mu$ depends on the reward scale. Notably, PPO employs normalized advantage, resulting in $Q^\\mu$ values in our methods that may be one order of magnitude larger than $A^\\pi$, necessitating a smaller $\\mu$. For instance, in MPE, we set $\\mu=0.05$.\n\n5. **Exclusion of QMIX in GRF**.\n\nAddressing the absence of QMIX in GRF, as explained in the third paragraph of section 6.1, QMIX was not included due to its reported inferior performance in the MAPPO's paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700048363653,
                "cdate": 1700048363653,
                "tmdate": 1700622915096,
                "mdate": 1700622915096,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rkbltU6U0F",
                "forum": "mPONXmVmZ6",
                "replyto": "YYDNQCaC6f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4339/Reviewer_JyH2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4339/Reviewer_JyH2"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications.\n\n3/Intermediate steps: In equation 10, there is advantage function $\\hat{A}$ but when you rewrite it the advantage function is replaced with $Q^\\pi$. Is this a simple replacement or is there something more to it?\n\nAdditionally, could you expand on the motivation for compatible function approximation."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700357850914,
                "cdate": 1700357850914,
                "tmdate": 1700357850914,
                "mdate": 1700357850914,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]