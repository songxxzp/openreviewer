[
    {
        "title": "Adaptive Softmax Trees for many-class classification"
    },
    {
        "review": {
            "id": "WgxjdOeIzN",
            "forum": "oqDoAMYbgA",
            "replyto": "oqDoAMYbgA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1530/Reviewer_P37h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1530/Reviewer_P37h"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents Adaptive Softmax Trees, an extension of Softmax trees, designed for many-class classification tasks. Similar to Softmax trees, the method is using Tree Alternating Optimization (TAO) to learn decision trees with sparse hyperplane splits and small Softmax classifiers in the leaves. Different to Softmax trees, the current method does not assume a complete tree. Instead, it grows the tree iteratively and therefore is able to learn deeper trees that are not complete but adapted to the data distribution."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n- Extension of the recently proposed Softmax trees that is designed for many-class classification tasks (typically in NLP) and is able to reduce the inference time and model size.\n- Experiments show the proposed approach leads to significantly shorter inference time and better generalization (lower testing error).\n- The paper is clear, written well, and easy to follow."
                },
                "weaknesses": {
                    "value": "Weaknesses:\nThe main weakness is the somewhat limited technical novelty: this is a relatively straightforward extension of Softmax trees, combining it with iterative expansion/growing of leaves. The key contribution is described in Section 5, and the majority of the technical content is an extensive summary of related previous  works (primarily, Tree Alternating Optimization and Softmax Trees). The rest of the paper is dedicated to different experiments that, as noted in Strengths, shows clear empirical improvement over previous work, however there is no significant technical insight beyond that."
                },
                "questions": {
                    "value": "The paper is clear and I do not have any questions"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1530/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698607580162,
            "cdate": 1698607580162,
            "tmdate": 1699636081649,
            "mdate": 1699636081649,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "N7u1BCkB0g",
            "forum": "oqDoAMYbgA",
            "replyto": "oqDoAMYbgA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1530/Reviewer_qCCg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1530/Reviewer_qCCg"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new algorithm to train a softmax tree of arbitrary structure to reduce the training and inference time for softmax layer. The tree structure is learned optimally by interleaving steps that grow the structure with steps that optimize the parameters of the current structure. The resulting softmax tree improves considerably the predictive accuracy while reducing the model size and inference time even further, as demonstrated in datasets with thousands of classes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper provided detailed literature research and it also did detailed empirical comparison with baseline models.\n\nThe new model has much better training time and inference time."
                },
                "weaknesses": {
                    "value": "In the result tables, number of parameters in these models are missing. So it's not clear if the new model has less parameters."
                },
                "questions": {
                    "value": "Is GPU used in the model training and evaluation? How will the model might perform when using the GPU?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1530/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815286959,
            "cdate": 1698815286959,
            "tmdate": 1699636081550,
            "mdate": 1699636081550,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mVLWwXkWjL",
                "forum": "oqDoAMYbgA",
                "replyto": "N7u1BCkB0g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1530/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1530/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "W1. \n\nSince AST produces hard decisions in each decision node, an instance passes through a single root to leave path and gets classified in the leaf softmax. We provide the number of floating point operations to indicate that AST learns trees that classify samples (on average) faster and more efficiently.\n\nQ1.\n\nThe current version of AST is implemented on the CPU (with parallelism)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1530/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550965576,
                "cdate": 1700550965576,
                "tmdate": 1700550965576,
                "mdate": 1700550965576,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WIhIFcAK3M",
            "forum": "oqDoAMYbgA",
            "replyto": "oqDoAMYbgA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1530/Reviewer_u3Fd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1530/Reviewer_u3Fd"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a method of adaptively building tree structure for Softmax Tree. The method performs top-down tree growing: starting from a shallow tree, it grows the leaves with small subtrees in a BFS manner. After growing all leaves at one level, it performs joint optimization of the tree parameters. The method is compared with static tree structures for Softmax Trees, and with other tree-based methods for multiclass classification, over various datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Determining an optimal tree structure for multi-class and multi-label tree-based methods is a challenging and important problem. The introduced method would only expand the leaf when the total loss doesn\u2019t degrade (allowing a slight margin for degradation), which gives some confidence on the obtained tree. The experimental part shows that the learned tree structure improves over the static structure."
                },
                "weaknesses": {
                    "value": "1. The experimental study is limited: the comparisons with other methods are provided only on a single Wiki-small dataset. From that, it\u2019s not enough to judge on the comparison with other baselines.\n\n2. The training time seems to be the main bottleneck of the method, its training is slower than for almost any other tree method (as reported in the paper). Probably because of that, applying the method on bigger datasets becomes infeasible. (Fair to say, that the same shortcoming applies for the original Softmax Tree, and the presented method seems to double the training time).\n\n3. The method seems to be quite sensitive to hyperparameters, so in order to apply it method for a new problem, one has to perform some careful hyperparameter search to find a proper $\\alpha$."
                },
                "questions": {
                    "value": "1. LOMTree is mentioned in the paper as one of the baselines but I didn\u2019t see any results for it.\n\n2. Can the method produce degraded trees, when only a single tree path is getting expanded? Is there any guardrail for it?\n\n3. Is the Wiki-small actually a multi-label dataset (the instances can have more than 1 label)? Was it somehow transformed to a multi-class problem?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1530/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1530/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1530/Reviewer_u3Fd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1530/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699393224459,
            "cdate": 1699393224459,
            "tmdate": 1699636081472,
            "mdate": 1699636081472,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FTMWDs0Oi7",
                "forum": "oqDoAMYbgA",
                "replyto": "WIhIFcAK3M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1530/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1530/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "W1.\n\nAt this work we show that learning structure of a tree using AST produces better results then initializing ST with complete tree randomly or using k-means. We provide a comparison on 4 datasets: Letter, ALOI, LSHTC1, Wiki-Small for classification and PTB dataset for language modeling on which AST consistently outperforms ST. Results of Zharmagambetov et al., 2021 shows that ST outperforms other baseline methods.\n\n\nW2.\n\nThe goal of the proposed approach is to learn once an accurate model with very fast inference. The training time will be longer because of the more exploration and optimization steps but the significant gain in accuracy and inference time justifies the approach.\n\nTo train AST efficiently we use warm starting in decision nodes and in leaf nodes. Furthermore, each expansion step and regular step requires up to 5 - 10 TAO iterations (depending on the dataset), while ST is typically trained for much longer. While training time is slower, it is comparable to other methods such as ST and MACH, and much faster compared to softmax or one-vs-all.\n\nW3.\n\nWe provide an extensive study of hyperparameters of both ST and AST. For most datasets AST outperforms ST with default parameters ($\\alpha = 0.5$, $\\rho = 1.0$), while tuning $\\alpha$ even further increase the gap between two methods. Values of $\\lambda$ and $\\mu$ can be tuned in the same way as for ST, by setting $\\lambda = \\mu$. What is more, good choice of $\\alpha$ can compensate for not optimal choice of $\\mu$.\n\n\nQ2.\n\nYes, in principle it is possible for the algorithm to expand a tree only through one path, and thus produce such ``lopsided'' trees. But we think that there is nothing inherently wrong with such trees in that the underlying true distribution of points can favor such structure, if for example, a certain group of classes can be easily separated from the rest in each decision node. Having said that, we do not observe such behavior in our experiments, and since especially our random initialization (based on random direction and median split) has some bias towards balanced trees.\n\nQ3. \n\nWe take this dataset from (Joshi et al. NIPS2017). The authors there convert this multi-label dataset to multi-class format by replicating the instances belonging to different class labels."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1530/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550863312,
                "cdate": 1700550863312,
                "tmdate": 1700550863312,
                "mdate": 1700550863312,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wk1IDktoF5",
                "forum": "oqDoAMYbgA",
                "replyto": "FTMWDs0Oi7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1530/Reviewer_u3Fd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1530/Reviewer_u3Fd"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply; a couple of follow up questions:\n\nW1: Have you performed the comparison of AST vs ST on ODP dataset as in [Zharmagambetov et al., 2021]?\n\nW3: In the result tables I couldn't see any supports for the claim that the AST outperforms ST when \\rho=1. The only such comparison I could find is in the Table 2, showing that AST is not better."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1530/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667447737,
                "cdate": 1700667447737,
                "tmdate": 1700667447737,
                "mdate": 1700667447737,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7XrraKCUlR",
            "forum": "oqDoAMYbgA",
            "replyto": "oqDoAMYbgA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1530/Reviewer_FA55"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1530/Reviewer_FA55"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Adaptive Softmax Trees (ASTs), an extension of Softmax Trees. The original Softmax Tree is a mix of hard and soft decision tree algorithms. The internal nodes (here called decision nodes) contain hard routers and leaf nodes softmax estimators. Originally, Softmax Tree has a predefined tree structure and uses the Tree Alternating Optimization (TAO) algorithm for training its node parameters. The idea behind the proposed extension is to build the tree in an iterative manner, starting from the shallow predefined tree (trained with TAO) and in each step by trying to expand a leaf node into new subtrees, trained using the same TAO algorithm, that is added to the tree if it yields the improvement in the optimized objective. If the subtree is added, the new tree is again retrained using the TAO algorithm. The attractiveness of the proposed approach is evaluated on text classification and language modeling task and compared against a few baselines that also use linear classifiers. The results confirm the superiority of AST over the baselines and original ST in terms of predictive performance and inference times."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The empirical comparison seems to prove the attractiveness of the proposed approach."
                },
                "weaknesses": {
                    "value": "1. The novelty of the paper is very limited as the proposed extension is quite simple and of a heuristic nature, as there is also no theoretical contribution accompanying it.\n\n2. Section 4 copies a lot of text from the paper of Zharmagambetov et al., 2021, which itself is not such a big issue for me, but unfortunately, I find both explanations hard to follow and missing important details. The biggest issues, in my opinion, are:\n    - $\\mathbf{y}_n$ is not defined,\n    - output of the $\\tau(\\mathbf{x}_n; \\Theta)$ is also not clearly defined,\n    - it's not clear what is a tree structure, I understand that it's given since it's parameterized by $\\Delta$ and $k$, it seems that classes in the leaves can be redundant. I understood that the class assignment is either random or obtained by some k-means clustering.\n\n3. It seems that while the ST provides very fast inference due to hard tree routing, it is costly to train, especially the proposed AST variant that repeats TAO training multiple for each and after each expansion of the tree.\n\n4. > Training HSM-based language models is efficient (usually logarithmic in vocabulary size), but it leads to no speedup at inference time: during prediction, an input instance is propagated to all the leave\n\n     This statement is not true, HSM structure allows efficient retrieval of top-k classes or all the classes about the given threshold of marginal probability $P(y | \\boldsymbol{x})$ by applying a proper tree search algorithm.\n\n5. The strength of softmax and hierarchical softmax is that they are fully differentiable and can be easily used as a part of more complex architectures. They also aim to provide calibrated estimates of conditional class probabilities $P(y | \\boldsymbol{x})$. Hierarchical softmax also speeds up both training and inference. As in the case of ST/AST, performance/speed-up trade-off can be easily controlled by selecting the proper tree structure. The ST/AST, while providing superior predictive performance, seem not to allow end-to-end training and do not aim to provide accurate probability estimates, which severely limits their applications. I belive the relevance of this work.\n\n6. I got the impression that the AST required a lot of hyperparameter tuning before it achieved better results than ST.\n\nNITs:\n1. There are some related works that authors might consider discussing:\n    - A quite recent algorithm that also mixes hard trees and soft trees (could serve as another baseline): *Sun, W., Beygelzimer, A., Iii, H. D., Langford, J., and Mineiro, P. (2019). Contextual memory trees.*\n    - Variant of HSM that builds tree structure online: *Beygelzimer, A., Langford, J., Lifshits, Y., Sorkin, G. B., and Strehl, A. L. (2009). Conditional probability tree estimation analysis and algorithms*\n    - Generalization of HSM from multi-class to the multi-label case: *Wydmuch, M., Jasinska, K., Kuznetsov, M., Busa-Fekete, R., and Dembczynski, K. (2018). A no-regret generalization of hierarchical softmax to extreme multi-label classification*\n\n2. Why not include hierarchical softmax in the empirical comparison for text classification? What variant is used on PTB task? Many variants are possible, e.g., the most computationally performant with a hamming tree, or popular in neural networks, two-level hierarchical softmax, which provides less speed-up in terms of complexity but is GPU-friendly and usually very close to flat softmax in terms of predictive performance.\n3. No citation for Penn Treebank (PTB) dataset"
                },
                "questions": {
                    "value": "I would be happy to see the authors respond to my critique from the weaknesses section.\n\nAdditional questions:\n- How the structure of the new subtree is decided? \n- Is reported training time, a clock time, or CPU time (does it take into parallelism account)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1530/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699439833342,
            "cdate": 1699439833342,
            "tmdate": 1699636081411,
            "mdate": 1699636081411,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0RZ7mAPyTd",
                "forum": "oqDoAMYbgA",
                "replyto": "7XrraKCUlR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1530/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1530/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "W1.\n\nThe proposed extension fills an important gap in the original ST model. Since the original ST assumes an initial tree structure of a given depth, it cannot explore deeper tree structures. It is also sensitive to initial model parameters, necessitating the use of various heuristic-based initialization. Our proposed approach effectively addresses these two shortcomings by cleverly expanding the tree where needed and globally reoptimizing the whole model. Each step of our algorithm has a guarantee of monotonically decreasing the objective function value.\n\nW2.\n\nBecause the algorithm is not widely popular we choose to review the algorithm in more detail. Therefore, our explanation is similar (but not a copy) to the one in (Zharmagambetov et al., EMNLP 2021)\n\nIn Section 4, we define a dataset $\\{(x_n, y_n)\\}_{n=1}^N \\subset R^D \\times \\{1,...,K\\}$, where $y_n$ is ground truth target label.\n\nThe output of a Softmax tree $\\tau(x; \\theta)$ is a predicted label for an instance $x$.\n\nTo obtain initial Softmax Trees in our algorithm we use random initialization, without doing any heuristic-based $k$-means unlike in (Zharmagambetov et al, 2021).\nThe number of classes in each leaf softmax is at most $k$. If the set of training points reaching a given leaf has smaller number of classes than $k$, then the corresponding leaf softmax adjusts to this.\n\n\nW3.\n\nThe goal of the proposed approach is to learn once an accurate model with very fast inference. The training time will be longer because of the more exploration and optimization steps but the significant gain in accuracy and inference time justifies the approach.\n\nTo train an AST efficiently we use warm starting in decision nodes and in leaf nodes. Furthermore, each expansion step and regular step requires up to 5 - 10 TAO iterations (depending on the dataset), while the original ST is typically trained for much longer. While training time is slower, it is comparable to other methods such as ST and MACH, and much faster compared to softmax or one-vs-all.\n\nW4.\n\nWhile it is possible to prune the search space in HSMs when finding for a maximum probability leaf, many nodes/leaves has still be explored. This is unlike in our approach where one and only one leaf is reached during inference.\n\nW5.\n\nYes, it is true that an HSM has the advantage of being differentiable and the support for end-to-end learning. But we do not propose ASTs as a drop-in replacement for HSMs. We view ASTs as standalone models for many-class classification problems with good enough accuracy and very fast inference. ASTs can also be trained on neural network (NN) features, and so be used as a last classifier layer in NNs (as we do in our language modeling experiments). And ASTs output correctly the probabilities, in that it is nonzero for a small subset of classes (for the classes in the reached leaf) and exactly zero for the rest.\n\nW6.\n\nFor most datasets AST outperforms ST with default parameters ($\\alpha = 0.5$, $\\rho = 1.0$), while tuning $\\alpha$ even further increase the gap between two methods. Values of $\\lambda$ and $\\mu$ can be tuned in the same way as for ST, by setting $\\lambda = \\mu$. What is more, good choice of $\\alpha$ can compensate for not optimal choice of $\\mu$.\n\n\nQ1.\n\nTo expand we use a subtree of depth 1. We mention that depth 2 can be used to grow tree faster, however training each subtree requires more iterations.\n\nQ2.\n\nWe report clock time taking into account parallelism."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1530/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550651099,
                "cdate": 1700550651099,
                "tmdate": 1700550651099,
                "mdate": 1700550651099,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qka4U4H6oH",
                "forum": "oqDoAMYbgA",
                "replyto": "0RZ7mAPyTd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1530/Reviewer_FA55"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1530/Reviewer_FA55"
                ],
                "content": {
                    "title": {
                        "value": "Re: Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Thank you for your comments, which clarified for me some parts of the submission; however, I still have doubts about a few aspects:\n\n> The output of a Softmax tree $\\tau(x; \\theta)$ is a predicted label for an instance $x$.\n\nSo in Equation (1) and (3), **bold** $\\boldsymbol{y}_n$ should be normal $y_n$, or is there some reason it's bolded there?\n\n> And ASTs output correctly the probabilities, in that it is nonzero for a small subset of classes (for the classes in the reached leaf) and exactly zero for the rest.\n\nIf I understand the ST algorithm correctly, the softmax in a single leaf is trained using only subsets of examples. So it seems to me that while indeed the value is some kind of probability, it is a biased estimate of $P(y | \\boldsymbol{x})$.\n\n> We report clock time taking into account parallelism.\n\nSo, is the training time reported in Table 2 a clock time when using multiple cores/threads? If yes, then I'm missing detailed information about the experimental setup that ensures that all the methods can use the same level of parallelism for fair comparison. If this is not possible, I would recommend reporting CPU time instead. I'm also missing the information about the hardware that was used to run these experiments.\n\n---\n\nOverall, the authors' response does not change my opinion that their work is a simple algorithmic extension of ST, which itself has narrow applications, and because of that, both the contribution and relevance of this work are limited. Additionally, I believe the presentation of the paper could be improved. I keep my score unchanged."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1530/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700329591,
                "cdate": 1700700329591,
                "tmdate": 1700700329591,
                "mdate": 1700700329591,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]