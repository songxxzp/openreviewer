[
    {
        "title": "Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis"
    },
    {
        "review": {
            "id": "JUozWVTxI0",
            "forum": "p4eG8rCa0b",
            "replyto": "p4eG8rCa0b",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4652/Reviewer_FnjA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4652/Reviewer_FnjA"
            ],
            "content": {
                "summary": {
                    "value": "This article proposes two methods to develop a diffusion model that merges the capability to manipulate the three-dimensional positioning of objects with the application of disentangled global stylistic semantics from exemplar images onto the objects.\n\nSpecifically, the paper introduces depth disentanglement training, which makes the model realize the  3D relative positioning of multiple objects by disentangling the salient object depth and the background object depth for the fusion of the condition during training.  In the meantime, this work presents a technique called soft guidance, which imposes the mask information into cross-attention mechanism to facilitate apply global semantics onto targeted regions without specific local localization cues."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is written in a clear and coherent style, presenting ideas in a manner that is easily comprehensible. Additionally, most figures in the paper effectively visualize and reinforce the concepts discussed.\n\n2. As stated in the paper, this work is the first to leverage the disentaglement of images to salient object depth and impainted (unseen) background depth map for training the relative depth aware diffusion model.  The soft guidance technique is also novel for applying the global semantics to specific localizations.\n\n3. The adequate experiment supports the effectiveness of the model. Both the qualitative and quantitative comparisons demonstrate that the model can control the relative placement of the objects and the effectively prevent the concept bleeding."
                },
                "weaknesses": {
                    "value": "1. Correct me if I have misunderstood. I'm confused about the details of the soft guidance. I understand that the work wants to leverage the mask map to selectively impose the foreground embedding and background embedding for the cross-attention mechanism. However, I'm uncertain as to whether this process should take place during the computation of similarity or subsequent to it. I would appreciate it if the author could elucidate the specific dimensions and computational details related to both the cross-attention mechanism and the soft guidance technique, to enhance reader comprehension. Please refer to the questions for additional context on my confusion.\n\n2. Apart from the standard metrics used for evaluating generative models, I wonder if there exist specific metrics that can accurately assess the model's capability to control the three-dimensional placement of objects and localize global semantics, as these are the primary objectives of this study. While the Mean Absolute Error (MAE) between the ground truth depth map and the depth maps derived from the generated images may offer some insight into the model\u2019s proficiency in 3D object placement, I am curious about how we might effectively gauge its ability to localize global semantics. Could there be other metrics or methods of evaluation that address this second capability?"
                },
                "questions": {
                    "value": "1. In my understanding, the size of S is $i\\times j$, where i is the number of queries, and j is the number of keys; the size of $W_K \\dot y_{full}$ is $j \\times C$ and the size of $W_Q \\dot z_t$ is $ i \\times C$. However, I am puzzled as to why $j$ needs to be greater than $2N$.\n\n2. Additionally, I am uncertain about whether the mask should be applied along the dimension of $C$. It perplexes me that the mask is utilized on the calculated similarity rather than during the actual computation of similarity."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4652/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4652/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4652/Reviewer_FnjA"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4652/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698612300166,
            "cdate": 1698612300166,
            "tmdate": 1699636445621,
            "mdate": 1699636445621,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6eL9SPTVkJ",
                "forum": "p4eG8rCa0b",
                "replyto": "JUozWVTxI0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4652/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4652/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for acknowledging the contributions of our paper, mainly:\n- Our proposed DDT is the first method to leverage image triplets to distill the relative depth positions of objects in a diffusion model.\n- Our proposed soft guidance is a novel method to localize global semantics.\n- Our experiments demonstrate the model's capabilities of relative placement of objects and its effectiveness in preventing concept bleeding.\n\n> [W1, Q1, Q2] Correct me if I have misunderstood. I'm confused about the details of the soft guidance. I understand that the work wants to leverage the mask map to selectively impose the foreground embedding and background embedding for the cross-attention mechanism. However, I'm uncertain as to whether this process should take place during the computation of similarity or subsequent to it. I would appreciate it if the author could elucidate the specific dimensions and computational details related to both the cross-attention mechanism and the soft guidance technique, to enhance reader comprehension. Please refer to the questions for additional context on my confusion.\n> 1. In my understanding, the size of $S$ is\u00a0$i\\times j$, where $i$ is the number of queries, and $j$ is the number of keys; the size of $W_K \\cdot y_{full}$\u00a0is\u00a0$j \\times C$\u00a0and the size of\u00a0$W_Q \\cdot z_t$\u00a0is\u00a0$i \\times C$. However, I am puzzled as to why\u00a0$j$ needs to be greater than\u00a0$2N$.\n> 2. Additionally, I am uncertain about whether the mask should be applied along the dimension of\u00a0$C$. It perplexes me that the mask is utilized on the calculated similarity rather than during the actual computation of similarity.\n\n[W1] Firstly, we would like to address your questions on soft guidance and why the mask is utilized on the calculated similarity rather than during the actual computation of similarity. Assuming \u201cduring the computation of similarity\u201d means **before** $S$ is rescaled with softmax and multiplied with $V$ (eg. $S'=(QK^T/\\sqrt{d})\\otimes M'$), and \u201ccalculated similarity\u201d means **after** S is rescaled with softmax and multiplied with $V$ (eg. $S'=\\operatorname{softmax}(QK^T/\\sqrt{d})\\otimes M'$), we would like to clarify that soft guidance is **indeed applied during the computation of similarity, or before the softmax operation.** It seems that this confusion stems from us omitting the softmax operation in the paper, where we state that \u201c\u2026the attention operation $S' \\cdot V$ is completed.\u201d, and whereas we intended $S$ to be understood as the raw similarity scores, readers would assume that $S$ means the scaled similarity score (or attention score, depending on sources). We thank you for your detailed insight on this matter. We\u2019ve updated the equations to be clear that soft guidance is indeed applied during the similarity computation, and the reshaped, flattened, repeated boolean mask $\\varphi(M) \\in \\{0,1\\}^{i \\times N}$ to $\\varphi(M) \\in \\mathbb{B}^{i \\times N}$ to avoid confusion.\n\n[W1] Secondly, we would like to illuminate the specific dimensions used in the soft guidance technique. We bring up an exact example of how soft guidance is applied in the first cross-attention layer latents pass through, when generating an image of size $512 \\times 512$.\nIn our implementation, we use 8 attention heads, and set $N=4$, or the number of tokens $\\mathbf{y}\\_\\text{fg}$, $\\mathbf{y}\\_\\text{bg}$ are projected to. Since we use the CLIP text encoder, the given text prompt is encoded into $77$ tokens with a dimension of $768$. By $\\mathbf{y}\\_\\text{full} = \\operatorname{concat}(\\mathbf{y}\\_\\text{text}, \\lambda\\_\\text{fg}\\mathbf{y}\\_\\text{fg},\\lambda\\_\\text{bg}\\mathbf{y}\\_\\text{bg})$, $\\mathbf{y}\\_\\text{full}$ becomes a tensor of shape `[1, 85, 768]`. The latent $\\mathbf{z}\\_t$ is a tensor of shape `[1, 4096, 320]` because it has been processed by the subsequent self attention layer. $\\mathbf{z}\\_t$ and $\\mathbf{y}\\_\\text{full}$ are then projected into $Q$, $K$, $V$, with respective dimensions of `Q=[8, 4096, 40]` , `K=[8, 85, 40]`, and `V=[8, 85, 40]`. The raw similarity scores are calculated by $S=QK^T/\\sqrt{d}$, resulting in `S=[8, 4096, 85]`. We then reshape, flatten, repeat the mask $M$ used for soft guidance (with an initial dimension of `[1, 512, 512]`, to a shape of `fg_mask=[1, 4096, 4]`, which serves as the mask for the foreground token part of $\\mathbf{y}\\_\\text{full}$. We then take the inverse of the mask, `bg_mask=[1, 4096, 4]`, as the mask for the background token part of $\\mathbf{y}\\_\\text{full}$. We then create a base mask of only ones `base_mask=[1, 4096, 77]`, and concat the three masks to create $M'$ of shape `[1, 4096, 85]`. Then the operation of $S\\otimes M'$ is conducted across all attention heads (eg. masking), to get $S'$. Then, the rest of the attention operation, $\\operatorname{softmax}(S')\\cdot V$ is performed."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700294453491,
                "cdate": 1700294453491,
                "tmdate": 1700294453491,
                "mdate": 1700294453491,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pu4aemcpGu",
                "forum": "p4eG8rCa0b",
                "replyto": "ydLdj5L5BC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4652/Reviewer_FnjA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4652/Reviewer_FnjA"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the author's detailed response, they addressed my concerns."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699787074,
                "cdate": 1700699787074,
                "tmdate": 1700699787074,
                "mdate": 1700699787074,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eXlL6m7tJ3",
            "forum": "p4eG8rCa0b",
            "replyto": "p4eG8rCa0b",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4652/Reviewer_iDhm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4652/Reviewer_iDhm"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method for controllable text-to-image generation. The method learns auxiliary modules on top of a pre-trained Stable diffusion model, and introduces a novel training scheme to facilitate compositional image synthesis given two depth images that represent the foreground and background. Further, foreground and background styles are controlled by separate images thanks to a localized cross-attention mechanism. The qualitative and quantitative experiments demonstrate that the proposed method outperforms several baselines in terms of image quality, image-text alignment and foreground-background disentanglement."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper studies the composition of control signals in controllable text-to-image diffusion. Unlike previous approach which takes a single control image, the proposed method allows the conditioning on two depth images. This provides a means to separately control foreground and background image content. The method also enables localized control of image styles using exemplar images. To the best of my knowledge, compositional image generation remains a challenging problem, and this paper demonstrates one feasible solution to the problem by engineering pre-trained diffusion models.\n\n- The paper presents a novel training scheme to instill depth awareness into the diffusion model. Training of the method relies on RGB images and their foreground / background depth maps which are not readily available. To this end, the paper introduces a simple strategy to create synthetic training data from single-view image datasets. This is key to the success of the proposed method and may be of interest to the image synthesis community in a broader context.\n\n- The method allows localized control of image styles using exemplar images. The key idea is to limit the extent of cross-attention so that tokens representing an exemplar image only contribute to a local region in the image. Many works have used attention maps to localize objects or control their shapes and appearance. This paper for the first time uses attention maps to control (local) image styles.\n\n- The experiments demonstrate superior qualitative and quantitative results. The proposed method outperforms several strong baselines in image quality and image-text alignment while supporting broader applications."
                },
                "weaknesses": {
                    "value": "- Calling the model \"depth-aware\" is misleading. I would rather say it learns to compose two spatial layouts and generate a coherent image. Using the teaser figure as an example, the cat can appear either in front of or behind the cake given the same depth maps, and similarly, the castle can either occlude or be occluded by the mountain. In other words, the exact ordering of objects is not induced by the depth maps. This phenomenon is likely because the depth produced by MiDaS is scale and shift invariant (i.e., it is not metric depth, and background can appear closer than foreground).\n\n- Since all that matters is generating a coherent image, I would imagine that other types of spatial conditions (e.g., segmentation masks for both foreground and background) can work equally well if used for training. I encourage the authors to test this hypothesis, and design additional ablation experiments to fully reveal the behavior of their model."
                },
                "questions": {
                    "value": "- The illustration of soft guidance in Figure 2 is confusing. I personally prefer color coding of the attention maps to highlight the regions influenced by different tokens.\n\n- Details about the reconstruction experiments (Figure 6) are lacking. It is unclear from the text what is the exact evaluation procedure. Also the MAE values reported in Table 2 is not meaningful, again because MiDaS does not predict metric depth. Please include qualitative comparison between the input and reconstructed depth maps."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4652/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698695826964,
            "cdate": 1698695826964,
            "tmdate": 1699636445525,
            "mdate": 1699636445525,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3xtXrCL1cf",
                "forum": "p4eG8rCa0b",
                "replyto": "eXlL6m7tJ3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4652/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4652/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for acknowledging the main contributions of our work, stating that:\n- Our method poses as a feasible solution in the challenging problem of compositional image generation.\n- Our novel training scheme may be of interest to the community in a broader context.\n- Our experiments demonstrate superior qualitative and quantitative results.\n\n> [W1] Calling the model \"depth-aware\" is misleading. I would rather say it learns to compose two spatial layouts and generate a coherent image. Using the teaser figure as an example, the cat can appear either in front of or behind the cake given the same depth maps, and similarly, the castle can either occlude or be occluded by the mountain. In other words, the exact ordering of objects is not induced by the depth maps. This phenomenon is likely because the depth produced by MiDaS is scale and shift invariant (i.e., it is not metric depth, and background can appear closer than foreground).\n\n[W1] We agree on your insight stating that the exact ordering of objects are not induced by the depth maps. The ability to order objects into their successive foreground/background stems from the proposed depth disentanglement training (DDT), where information about the otherwise occluded background is retrieved via our image triplets and distilled into our local fuser. The depth maps produced by MiDaS are indeed scale/shift invariant, but they do hold attractive properties that lead us to choosing them as our local condition, instead of other representations (eg. segmentation maps). Because depth maps are single channel representation of an object's shape and holds the relative depth representation of an object in the scope of the image, they hold an inductive bias that we sought attractive to leverage. To summarize, the \"depth-aware\" property of our model doesn't stem from the depth maps themselves (although we do leverage its inductive biases), but its ability to process each depth map in their respective foreground/background stream in our local fuser and place them accordingly.\n\n\n> [W2] Since all that matters is generating a coherent image, I would imagine that other types of spatial conditions (e.g., segmentation masks for both foreground and background) can work equally well if used for training.\n\n[W2] We agree that different spatial conditions can work equally well during training. We have added an additional experiment that elucidates this case in our revised paper, section [A.4] Ablation Study, of the appendix. We explore the use of canny edges instead of depth maps, due to the fact that canny edges hold very different inductive biases to that of depth maps. Canny edges are inherently binary, and they hold much more fine grained detail about an object while trading off the ability to represent the relative depth of a pixel. The added figure 13 of page 18 showcases the results, where we find that our model generalizes well to canny edges. As mentioned above, this ability to generalize well stems from our depth disentanglement training, where information about the background is distilled to the model, even if the representation used doesn't hold inductive biases about the relative depth of an object. However, the inductive biases of each representation does come into play when generating images. In figure 13 (a) and (c), depth maps generate images that capture the sense of depth present in the conditioned images, while canny edges result in the generated images often looking flat. The opposite holds, for in figure 13 (b) and (d), it can be seen that canny edges generate better fidelity samples to that of depth maps when conditioned on flat, illustration based images. This leads to the conclusion that while DDT is successful in distilling information of what object to place in front of and cover, our model leverages the inductive biases of the representations used. If the representation inherently holds information about the relative depth of an image, DDT also distills this information into our model, resulting in the ability to compose images in a depth aware fashion."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292704032,
                "cdate": 1700292704032,
                "tmdate": 1700292704032,
                "mdate": 1700292704032,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i1QRHWN4dq",
                "forum": "p4eG8rCa0b",
                "replyto": "jkQnNwupao",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4652/Reviewer_iDhm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4652/Reviewer_iDhm"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for addressing my comments. The revised draft looks good to me, and I would like to keep my rating."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708754138,
                "cdate": 1700708754138,
                "tmdate": 1700708754138,
                "mdate": 1700708754138,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bWwlBMZynd",
            "forum": "p4eG8rCa0b",
            "replyto": "p4eG8rCa0b",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4652/Reviewer_taqR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4652/Reviewer_taqR"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose the Compose and Conquer (CnC) network that achieves 3D object placement and successfully integrates global styles and local conditions. To this end, the authors first propose depth disentanglement training (DDT) which disentangles the foreground and background depth and processes them with independent layers before fusing them together. Moreover, the paper also involves a novel soft guidance block that efficiently combines global and local conditions. Thorough qualitative and quantitative evaluations demonstrate the design and the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The strengths of the proposed paper can be summarized as:\n1. The paper is well-written and easy to follow\n2. The proposed DDT and soft guidance modules are novel and effective, demonstrated by both qualitative and quantitative results\n3. Evaluations are comprehensive and showcase better results than existing SOTA methods"
                },
                "weaknesses": {
                    "value": "The weaknesses of the proposed paper can be summarized as:\n1. Type of conditions. (1) Is the model capable of applying different types of conditions? (2) Is the model capable of applying two different conditions simultaneously while recognizing the 3D relations? I am rather interested in these different situations especially considering that the authors only employ depth in the submitted paper. More examples or scenarios would be appreciated.\n2. Image triplets. There is no visualization of the prepared image triplets for training. I am curious regarding the quality of foreground image, background image and foreground mask. It's also especially important to analyze the inpainted background image and how it would negatively affect the training and final outcomes.\n3. Qualitative results. (1) What are the prompts for examples in Figure 3? (2) Through the visualization in Figure 3, 4 and 5, it's interesting to see that the final generated images do not fully reflect the foreground depth condition. Meanwhile, the background depth map is often ignored through the qualitative results.\n4. No limitations and societal impacts are discussed in the submission."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4652/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4652/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4652/Reviewer_taqR"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4652/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699425680654,
            "cdate": 1699425680654,
            "tmdate": 1700613307013,
            "mdate": 1700613307013,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jcoZXW0OWf",
                "forum": "p4eG8rCa0b",
                "replyto": "bWwlBMZynd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4652/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4652/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for acknowledging the strengths of our paper in that:\n- The methods we propose, DDT and soft guidance are novel and effective in enhancing the capabilities of diffusion models.\n- The evaluations are comprehensive and quantitative results showcase better results than existing SOTA methods.\n\n> [W1-1] Type of conditions. Is the model capable of applying different types of conditions? [W1-2] Is the model capable of applying two different conditions simultaneously while recognizing the 3D relations?\n\n[W1-1] Yes, our model indeed is capable of applying different types of conditions. We have added a section in our revised paper, section [A.4] Ablation Study of the appendix. We explore the usage of canny edges as an alternative to depth maps, due to the fact that while canny edges are spatial representations of an image, it trades off the ability to represent depth with more fine grained details. We include qualitative samples comparing generated images from canny edges and depth maps, in Figure 13. It can be seen that our model generalizes well to canny edges, a trait that stems from our depth disentanglement training where information about the salient object, and what's behind said object is distilled into our local fuser. However, canny edges hold inherently different inductive biases to that of depth maps. This can be seen playing a role in the generated samples, where canny edges yield better results for \"flat\" images (such as  graphics), but often fail in capturing a sense of depth. The opposite holds for depth maps, so we find that special cases might call for variants of our model trained on different conditions. We have also included a quantitative evaluation in the same section, where we compare the canny edge variant of our model with other baseline models, and find the results to be comparable. [W1-2] Because our model generalizes well to different conditions, we find no reason for our model to fail in generalizing different conditions for the foreground and background. While due to limitations in our computational resources at the moment, we plan to explore this possibility, starting with canny edges for the foreground and depth maps for the background. This initial choice comes from our newly found insights while experimenting with the canny edge variant of our model, where we find that canny edges provide more control over the intricate details of an object. We hypothesize that leveraging depth maps as background representations with canny edges as foreground representations might give us the best of both worlds; fine grained control over the salient object while additionally providing a sense of depth.\n\n\n> [W2] Image triplets. There is no visualization of the prepared image triplets for training. I am curious regarding the quality of foreground image, background image and foreground mask. It's also especially important to analyze the inpainted background image and how it would negatively affect the training and final outcomes.\n\n[W2] We kindly refer the reviewer to section [A.2] of the appendix of our paper, where we detail the process and provide visualizations of preparing the image triplets. We agree that the quality of the synthetic image triplets does influence the quality of the outcomes, especially the background image. Figure 9 of section [A.2], Details on CnC, of our appendix elucidates this factor about the image triplets, where we initially try out [1]LaMa, a widely utilized inpainting model. We show that the depth maps of the background images extracted by LaMa holds artifacts that don't provide information of what is behind the salient object, whereas background images extracted by SD don't. For depth disentanglement training to be fully effective, the background image has to distill information that is initially occluded by the salient object. If the quality of the background image were to be on par with the samples extracted by LaMa as in figure 9 (i.e. doesn't provide any information about the initially occluded background), it would hold the same effect of training our model with the same depth maps, as shown in figure 3 of our paper.\n\nWe also find the process of binary dilation to be crucial in inpainting with SD, because the masks $M$ extracted by the Salient Object Detection model we utilize are usually pixel-perfect. SD's inpainting module tends to leave certain edge artifacts when the masks just barely cover the targeted object, so we additionally use binary dilation to expand the mask. These extra steps while utilizing a much computationally demanding module (SD over LaMA) are taken to ensure the quality of our image triplets, for without it would negatively effect the quality of final outcomes, as you've pointed out.\n\n[1]Suvorov, Roman, et al. \"Resolution-robust large mask inpainting with fourier convolutions.\"\u00a0_Proceedings of the IEEE/CVF winter conference on applications of computer vision_. 2022."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292614679,
                "cdate": 1700292614679,
                "tmdate": 1700292614679,
                "mdate": 1700292614679,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aUC3hluWvo",
                "forum": "p4eG8rCa0b",
                "replyto": "bWwlBMZynd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4652/Reviewer_taqR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4652/Reviewer_taqR"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' detailed responses, which effectively addressed most of my preliminary concerns. Hence, I would like to raise my rating. The authors may want to carefully revise the manuscript and incorporate the materials in the rebuttal."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613288364,
                "cdate": 1700613288364,
                "tmdate": 1700613288364,
                "mdate": 1700613288364,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]