[
    {
        "title": "Uni3D: Exploring Unified 3D Representation at Scale"
    },
    {
        "review": {
            "id": "Wiege48bHx",
            "forum": "wcaE4Dfgt8",
            "replyto": "wcaE4Dfgt8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1339/Reviewer_1Xtq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1339/Reviewer_1Xtq"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents Uni3D, a general 3D foundation model to explore the \"unified\" 3D representation at scale. Given the point cloud at input, it uses a 2D initialized ViT to align 3D / 2D features. It scales up to 1B (which is very large for 3D tasks) and achieves good performance on a broad range of tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Simple and effective structure to scale up the model capacity to 1B. This is very impressive in the 3D point cloud domain.\n\n+ Good performance in a wide span of tasks with detailed experimental results."
                },
                "weaknesses": {
                    "value": "- The exact insight as to why the proposed method succeeds in 3D domain is not fully stated or analyzed. Please see the questions below."
                },
                "questions": {
                    "value": "1. The successful scaling up to 1B parameters is a very key contribution from this work. It works well on multiple downstream tasks. Does the gain come from the 2D pretrained models? Table 7 seems to give some ablations and yet this is not clearly stated in the introduction. I was wondering this since it would tell people whether to focus more on 2D data/pretraining to resolve 3D problems, or 3D pretraining is essential. If I am getting this right, using none 2D initialized weights as shown in Table 7, the gain is not too much obvious (44.8 vs 45.8).\n\n2. Uni3D is verified on a wide variety of tasks and benchmarks. This is motivating. Do you plan to try some challenging and realistic settings, eg. autonomous driving settings with point clouds? That would strength the proposed approach to great extent.\n\n3. The last row in Table 1 shows the result of models separately trained on each benchmark. The unified approach of Uni3D is on par with them, demonstrating the generalization or universality. Does Uni3D potentially could surpass the performance of the model trained each on one particular benchmark? I was wondering the nessecity of training a universal model.\n\n---\nMinor:\n- Typo in Figure 1, \"ensambled\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1339/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757986231,
            "cdate": 1698757986231,
            "tmdate": 1699636061076,
            "mdate": 1699636061076,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xcOYC3ZufU",
                "forum": "wcaE4Dfgt8",
                "replyto": "Wiege48bHx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1339/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 1Xtq (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate Reviewer 1Xtq for the acknowledgment of our work and constructive feedback. We will respond comprehensively to the questions as follows.\n\n**Q1: Where does the gain of scaling up Uni3D to billion-scale parameters come from.**\n\nThe performance gain of Uni3D in scaling up model sizes mainly comes from two parts: \n\n**1. The well-explored scaling up strategies from the unified 2D ViT model.** We directly leverage the vanilla transformer structurally equivalent to ViT, which can naturally solve the difficulties in scaling designs in 3D domain by simply scaling up the model size with the well-studied unified 2D/NLP scaling-up strategies. The effectiveness and efficiency of our scaling-up strategy are fully demonstrated by the comprehensive exploration of scaling up ViT in the 2D vision domain. As shown in Fig. 1 and Tab. 5, we observe continuous performance improvements as the scaling of model sizes under the flexible and unified framework.\n\n**2. The sharable 2D prior from large pre-trained 2D ViT.**\nUni3D directly leverages the vanilla transformer structurally equivalent to ViT as the 3D backbone, which brings a new perspective of introducing pretrained priors. Specifically, we can naturally adopt the pretrained large models in other modalities which share the same vanilla transformer as ours to initialize Uni3D. These pretrained models are trained in datasets consisting of billions of images and texts, which already learn rich underlying representational abilities for Transformer and have the potential to enhance and stabilize the learning of large-scale 3D representations.\n\nTo demonstrate the key role of the large pre-trained 2D ViT for initializing Uni3D, we conduct extensive ablation studies under different scales of Uni3D from 6 M (Tiny), 23 M (Small), 88 M (Base), 307 M (Large) to 1 B (giant). We keep the default data setting the same as the ablation studies of the main paper, i.e., \u201cEnsembled (no-LVIS)\u201d. We list the results below:\n \n ***Table A : Ablation studies on the effect of initializing Uni3D with pre-trained 2D ViT.***\n| Model Size | From scratch | 2d pretrained ViT init |\n| --- | --- | --- |\n| Tiny | 42.8 | 43.5 |\n| Small | 43.7 | 44.8 |\n| Base | 44.8 | 45.8 |\n| Large | 45.0 | 46.2 |\n| giant | 26.6\uff08broken\uff09 | 47.2 |\n\nAs shown, the initialization from large pre-trained 2D ViT improves the performance at all model sizes. Specifically, the training of the model with extremely large scales of parameters (e.g. 1 B of giant) is **broken** when optimizating from scratch, while the continuous improvement is further achieved by introducing 2D priors from pre-trained large vision models. This justifies that the powerful initialization from 2D pretrained models is necessary for achieving performance gain in scaling up Uni3D model to extremly large parameters (e.g. 1B).\n\nWe further refer the review to Tab. 10 of the main paper where we conducted ablation studies under two-modal alignment. The results show that the optimization crashed without EVA initialization in the difficult situation where only images are available (40.1->20.7) or only texts are available (26.3->12.4). The results in Tab. 10 of the main paper are reported under the Uni3D-Base model. We additionally supplement this ablation with different model scales, i.e. from 6 M (Tiny), 23 M (Small) to 88 M (Base), under the difficult text-point cloud alignment situation. We report the results below:\n\n  ***Table B : Learning text-point cloud alignment from scratch or initializing with pre-trained 2D ViT.***\n| Model Size | From scratch | 2d pretrained ViT init |\n| --- | --- | --- |\n| Tiny | 37.0 | 38.3 |\n| Small | 37.8 | 39.6 |\n| Base | 20.7\uff08broken\uff09 | 40.1 |\n\n\nWe observe the same experimental phenomena as Table A, where the large pre-trained 2D ViT improves the performance at all model sizes. More specifically, the performance of models trained from scratch improves normally from Tiny to Small (37.0 -> 37.8), but crashes when further scaling up from Small to Base (37.8->20.7). While models trained with 2D priors of pre-trained large vision models obtain continuous improvements from Tiny, Small, to Base (38.3->39.6->40.1). \n\nThese two ablation studies demonstrate the necessity of introducing 2D priors from large pre-trained 2D ViT for initializing Uni3D. The 2D priors play the key role in improving and stablizing the optimization of Uni3D models, especially when scaling up to large parameters, where the optimization crashed when training from scratch. Moreover, we observe that for the more difficult situation, such as text-point cloud alignment, the crashment comes eailer during the model size scaling. And the 2D prior stabilizes all the situations and achieves continuous improvements in all the settings."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502735799,
                "cdate": 1700502735799,
                "tmdate": 1700502735799,
                "mdate": 1700502735799,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NbZayXWwPx",
                "forum": "wcaE4Dfgt8",
                "replyto": "Wiege48bHx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1339/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 1Xtq (2/2)"
                    },
                    "comment": {
                        "value": "**Q2: Expending Uni3D to more challenging settings, e.g., autonomous driving.**\n\nWe believe that Uni3D, which has powerful and universial 3D perceptions, has great potentials in some challenging and realistic settings, eg. autonomous driving settings with point clouds, VR/AR and robotics. We consider these as the exciting furture work for Uni3D and will be committed to exploring the application prospects of Uni3D.\n\n**Q3: The performance of training a universal Uni3D model.**\n\nWe justify that all the experiment results on Tab. 1 are tested in a zero-shot way with the universial Uni3D model trained under two data settings, i.e., Ensembled (no LVIS) and Emsembled. The last row reports the best results achieved on different benchmarks under different convergences of optimization. Specifically, the best performance of ModelNet and ScanObjNN is achieved with the early stopped Uni3D model since those two benchmarks are quite easier with few categories and samples, where more training iterations will lead to performance degeneration. While the best performance of Objaverse-LVIS is achieved with the fully converged Uni3D model since Objaverse-LVIS is much more difficult with 1,156 categories containing some out-of-the-distribution samples, which requires more extensive training."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502915777,
                "cdate": 1700502915777,
                "tmdate": 1700502915777,
                "mdate": 1700502915777,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wA6FKkMkHk",
            "forum": "wcaE4Dfgt8",
            "replyto": "wcaE4Dfgt8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1339/Reviewer_8Neg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1339/Reviewer_8Neg"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a 3D foundation model, dubbed Uni3D, which uses a 3D point tokenizer and ViT to align 3D features with CLIP features (images and texts). Uni3D is trained with the triplet contrastive loss OpenShape used. By scaling up the model size of ViT as a point encoder to a billion-scale, Uni3D shows impressive performance on zero-shot and few-shot 3D perception tasks including classification and semantic segmentation. Although the experiment results are impressive, the technical novelty of the proposed is limited, and a few analyses (as described in the weakness section) seem necessary to strengthen the paper."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "[Method] Training a 3D foundation model is a timely topic and the proposed method Uni3D showed impressive results on various benchmarks.\n\n[Experiments] The paper provides not only common benchmarks for open-world understanding but also interesting analysis (e.g., point cloud painting). Especially, the point cloud coloring analysis shows that the trained 3D encoder can encode the color of a 3D point cloud, which is aligned with text features.\n\n[Detailed explanation] The paper provides implementation details to help readers understand the proposed method well. For example, I could understand that Uni3D used a PointNet++-based upsampling strategy from the details related to part segmentation experiments, as shown in the Appendix."
                },
                "weaknesses": {
                    "value": "[Novelty] The proposed method mainly follows the previous work, OpenShape, in terms of the training data construction (Sec 3.2) and the training objective (Eq. (1)). From my understanding, the only difference is that Uni3D uses PointBERT\u2019s 3D tokenizer + ViT as a 3D encoder while OpenShape uses PointBERT. Although OpenShape is still in arXiv, I recommend the authors clarify what differentiates Uni3D from OpenShape since OpenShape is the most relevant baseline and high similarity to this work.\n\n[Experiments] Although the paper provides text-to-3D retrieval and image-to-3D retrieval, I think 3D-to-text (captioning) and 3D-to-image (generation) experiments need to be included in the paper to show the good alignment of 3D, image, and text.\n\n[Analysis] As shown in Figure 1, scaling up OpenShape does not improve its zero-shot accuracy, unlike Uni3D. Why can Uni3D have such consistent improvement while it has a similar architecture to OpenShape? Does this improvement come from the initialization of the large pre-trained 2D ViT? I recommend the authors provide an analysis of this to make the paper stronger."
                },
                "questions": {
                    "value": "Please refer to the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1339/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1339/Reviewer_8Neg",
                        "ICLR.cc/2024/Conference/Submission1339/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1339/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699104057535,
            "cdate": 1699104057535,
            "tmdate": 1700714627880,
            "mdate": 1700714627880,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "p3cjc4joua",
                "forum": "wcaE4Dfgt8",
                "replyto": "wA6FKkMkHk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1339/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 8Neg (1/2)"
                    },
                    "comment": {
                        "value": "We deeply appreciate the reviewer 8Neg for the thoughtful feedback and time invested in evaluating our work. We respond to each question below.\n\n**Q1: The contributions of Uni3D and differences to OpenShape.**\n\nWe justify that the training objective to align text-image-point cloud representations is not considered as a technical contribution of Uni3D, nor is it for OpenShape. The objective was first introduced in ULIP [1], and both OpenShape and Uni3D follow the training objective and focus on scaling up 3D representations. While OpenShape focuses on the data scaling to leverage the large open-source Objaverse dataset with specific efforts on ensembling multiple datasets and data augmentation to filter and enrich text descriptions. We go beyond OpenShape and propose to focus on the scaling up on model sizes. We achieve this by designing unified 3D representation to directly adopt the vanilla transformer structurally equivalent to ViT as the 3D backbone, which leverages abundant 2D pretrained models as initialization. This unlocks the great potential of 2D models and scaling-up strategies to the 3D world, which plays the key role in scaling up Uni3D to billion-scale parameters.\n\nWe summarize the main difference between Uni3D and OpenShape as below:\n\n1. We design the first billion-scale 3D foundation model, and demonstrate a billion-scale 3D representation model can transfer well to various downstream tasks and scenarios. This is acheived by introducing the sharable 2D prior and scaling up strategies from large pre-trained 2D ViT as we will discuss in **Q3**. On the contrary, OpenShape struggles to scale up models to large parameters, which shows degradation during the model size scaling up (e.g. 72 M in Fig. 2).\n2. We unify the 3D representation backbone to a 2D ViT. This enables Uni3D to explore the great potential of pretrained priors and scaling-up strategies of large vision models to the 3D world. On the contrary, OpenShape adopts the specifically designed point cloud backbones, e.g., SparseConv, PointBERT, PointMLP, etc. OpenShape achieved inconsistent performance with different backbones. For instance, selecting PointBERT as the 3D backbone performs the best on Objaverse-LVIS, while leveraging SparseConv as the backbone significantly outperforms PointBERT under ScanObjectNN. Uni3D achieves significant improvement under all benchmarks consistently with the unified model.\n3. We explore more representative 3D tasks and introduce more applications with the aligned point cloud-text-image representations from Uni3D, such as point cloud painting, open-world understanding and the difficult dense prediction task of part segmentation.\n\n\n**Q2: More experiments on the applications of Uni3D.**\n\nWe provide the extra application of Uni3D for 3D captioning in Sec.F in the Appendix of the revised paper. More applications (e.g. point cloud-based image generation) will be further included in the next revision.\n\n[1] Xue L, Gao M, Xing C, et al. ULIP: Learning a unified representation of language, images, and point clouds for 3D understanding. CVPR 2023"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501733950,
                "cdate": 1700501733950,
                "tmdate": 1700501733950,
                "mdate": 1700501733950,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wkq26l4p81",
                "forum": "wcaE4Dfgt8",
                "replyto": "wA6FKkMkHk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1339/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 8Neg (2/2)"
                    },
                    "comment": {
                        "value": "**Q3: How does Uni3D successfully scale up to billion-scale parameters.**\n\nThe success of Uni3D in scaling up model sizes mainly comes from two parts: \n\n**1. The well-explored scaling up strategies from the unified 2D ViT model.** We directly leverage the vanilla transformer structurally equivalent to ViT, which can naturally solve the difficulties in scaling designs in 3D domain by simply scaling up the model size with the well-studied unified 2D/NLP scaling-up strategies. The effectiveness and efficiency of our scaling-up strategy are fully demonstrated by the comprehensive exploration of scaling up ViT in the 2D vision domain. As shown in Fig. 1 and Tab. 5, we observe continuous performance improvements as the scaling of model sizes under the flexible and unified framework.\n\n**2. The sharable 2D prior from large pre-trained 2D ViT.**\nUni3D directly leverages the vanilla transformer structurally equivalent to ViT as the 3D backbone, which brings a new perspective of introducing pretrained priors. Specifically, we can naturally adopt the pretrained large models in other modalities which share the same vanilla transformer as ours to initialize Uni3D. These pretrained models are trained in datasets consisting of billions of images and texts, which already learn rich underlying representational abilities for Transformer and have the potential to enhance and stabilize the learning of large-scale 3D representations.\n\nTo demonstrate the key role of the large pre-trained 2D ViT for initializing Uni3D, we conduct extensive ablation studies under different scales of Uni3D from 6 M (Tiny), 23 M (Small), 88 M (Base), 307 M (Large) to 1 B (giant). We keep the default data setting the same as the ablation studies of the main paper, i.e., \u201cEnsembled (no-LVIS)\u201d. We list the results below:\n \n ***Table A : Ablation studies on the effect of initializing Uni3D with pre-trained 2D ViT.***\n| Model Size | From scratch | 2d pretrained ViT init |\n| --- | --- | --- |\n| Tiny | 42.8 | 43.5 |\n| Small | 43.7 | 44.8 |\n| Base | 44.8 | 45.8 |\n| Large | 45.0 | 46.2 |\n| giant | 26.6\uff08broken\uff09 | 47.2 |\n\nAs shown, the initialization from large pre-trained 2D ViT improves the performance at all model sizes. Specifically, the training of the model with extremely large scales of parameters (e.g. 1 B of giant) is **broken** when optimizating from scratch, while the continuous improvement is further achieved by introducing 2D priors from pre-trained large vision models. This justifies that the powerful initialization from 2D pretrained models is necessary for achieving performance gain in scaling up Uni3D model to extremly large parameters (e.g. 1B).\n\nWe further refer the review to Tab. 10 of the main paper where we conducted ablation studies under two-modal alignment. The results show that the optimization crashed without EVA initialization in the difficult situation where only images are available (40.1->20.7) or only texts are available (26.3->12.4). The results in Tab. 10 of the main paper are reported under the Uni3D-Base model. We additionally supplement this ablation with different model scales, i.e. from 6 M (Tiny), 23 M (Small) to 88 M (Base), under the difficult text-point cloud alignment situation. We report the results below:\n\n  ***Table B : Learning text-point cloud alignment from scratch or initializing with pre-trained 2D ViT.***\n| Model Size | From scratch | 2d pretrained ViT init |\n| --- | --- | --- |\n| Tiny | 37.0 | 38.3 |\n| Small | 37.8 | 39.6 |\n| Base | 20.7\uff08broken\uff09 | 40.1 |\n\n\nWe observe the same experimental phenomena as Table A, where the large pre-trained 2D ViT improves the performance at all model sizes. More specifically, the performance of models trained from scratch improves normally from Tiny to Small (37.0 -> 37.8), but crashes when further scaling up from Small to Base (37.8->20.7). While models trained with 2D priors of pre-trained large vision models obtain continuous improvements from Tiny, Small, to Base (38.3->39.6->40.1). \n\nThese two ablation studies demonstrate the necessity of introducing 2D priors from large pre-trained 2D ViT for initializing Uni3D. The 2D priors play the key role in improving and stablizing the optimization of Uni3D models, especially when scaling up to large parameters, where the optimization crashed when training from scratch. Moreover, we observe that for the more difficult situation, such as text-point cloud alignment, the crashment comes eailer during the model size scaling. And the 2D prior stabilizes all the situations and achieves continuous improvements in all the settings."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502515002,
                "cdate": 1700502515002,
                "tmdate": 1700502515002,
                "mdate": 1700502515002,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0YMCmkf04j",
                "forum": "wcaE4Dfgt8",
                "replyto": "wkq26l4p81",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1339/Reviewer_8Neg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1339/Reviewer_8Neg"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the responses."
                    },
                    "comment": {
                        "value": "I appreciate the detailed answers as well as the additional experiments. I am more convinced about the value of the paper. Therefore, I calibrate my rating to \"accept\"."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578014048,
                "cdate": 1700578014048,
                "tmdate": 1700578014048,
                "mdate": 1700578014048,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "d4jCCohfeb",
            "forum": "wcaE4Dfgt8",
            "replyto": "wcaE4Dfgt8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1339/Reviewer_7VZo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1339/Reviewer_7VZo"
            ],
            "content": {
                "summary": {
                    "value": "**Note: this is an emergency review**\n\nThis paper proposes Uni3D, a 3D point cloud foundation model for open-world understanding that achieves state-of-the-art performance on zero-shot shape classification. The Uni3D architecture, drawing inspirations from the scalability of ViT, is composed of a network that encodes local point cloud patches into features, akin to the image patch features in ViT. Subsequently, the transformer layers of ViT process these patch features and output a final global feature. These transformers can be loaded from pretrained 2D visual encoders such as the ones in DINO or EVA-CLIP. Besides demonstrating the superior open-world performance of Uni3D, the authors also showcase applications such as point cloud painting and cross-modal retrieval."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is generally well-written. \n- Authors plan to release all the code and pretrained models, which will greatly facilitate future research efforts.\n- The proposed ViT-like architecture is intuitive and well-motivated. The architecture also achieves quite a significant performance gain over existing models in e.g., OpenShape."
                },
                "weaknesses": {
                    "value": "- Further exploration and analysis of Uni3D's behaviors could provide additional insights and understanding. Please refer to the \"Questions\" section below for more details.\n- (New concern on Nov 22) Prior works like OpenShape have been trained on a much smaller batch size than Uni3D (OpenShape is trained using a batch size of 200 on a single A100-80G, while Uni3D is trained using a batch size of 1152 on 24x A100-40G). My question is, if authors train Uni3D using the same batch size as OpenShape, or train OpenShape using the same batch size as Uni3D, does Uni3D still outperform OpenShape (when both models have similar numbers of parameters)? This experiment will reveal whether the better training setting or the proposed architecture plays a bigger role in the superior performance of Uni3D. Though, regardless of the final findings from this experiment, it won't affect my positive view of the paper."
                },
                "questions": {
                    "value": "- It would be helpful to include a comparison of the inference speed of Uni3D compared to prior work on open-world 3D understanding.\n- The main source of performance gains of Uni3D over existing work seems to come from the proposed architecture, instead of how the architecture is initialized (according to authors' reply to Reviewer 1Xtq, whether to initialize the architecture from pretrained ViT only leads to about 1% performance difference). This observation might be attributed to the extensive size of the pretraining dataset, which encompasses 1 million 3D shapes. This raises a question: If a smaller pretraining dataset were used, would initializing with a pretrained ViT have a more significant impact on open-world generalization?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1339/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1339/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1339/Reviewer_7VZo"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1339/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700639513708,
            "cdate": 1700639513708,
            "tmdate": 1700935263398,
            "mdate": 1700935263398,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IQHZpZopuG",
                "forum": "wcaE4Dfgt8",
                "replyto": "d4jCCohfeb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1339/Reviewer_7VZo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1339/Reviewer_7VZo"
                ],
                "content": {
                    "comment": {
                        "value": "(Nov 22 update) Hi authors, I have added an additional question in the updated review. Thanks!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689204257,
                "cdate": 1700689204257,
                "tmdate": 1700689204257,
                "mdate": 1700689204257,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]