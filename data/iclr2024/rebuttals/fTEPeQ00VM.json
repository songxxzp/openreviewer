[
    {
        "title": "TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications"
    },
    {
        "review": {
            "id": "i1F5eG5NSq",
            "forum": "fTEPeQ00VM",
            "replyto": "fTEPeQ00VM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3335/Reviewer_EeeT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3335/Reviewer_EeeT"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes TabRepo -- a dataset of predictions and metrics for 1206 hyperparameter configurations of 6 models (i.e. 201 configuration per model) on 200 classification and regression tasks (to clarify, these 200 tasks are taken from existing public benchmarks).\n\nThe paper demonstrates that TabRepo can be useful for:\n- analyzing whether hyperparameter tuning and ensembling can help traditional models outperform modern AutoML systems;\n- analyzing ensembling strategies by using the published model predictions without retraining these models (*\"at no cost\"*);\n- performing *\"transfer learning\"* (in this paper, this term describes the usage of results obtained on some tasks to inform the choice of models/hyperparameters/etc. on other tasks); as an example of this, the paper shows that portfolio learning outperforms existing AutoML approaches."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is easy to follow.\n- The training, evaluation, ensembling and hyperparameter tuning protocols are clear and transparent.\n- The evaluation of the \"Portfolio learning\" technique and its positive results is interesting.\n- TabRepo is the largest dataset in its niche.\n- Many modern AutoML algorithms are covered."
                },
                "weaknesses": {
                    "value": "*(a quick comment on the selected confidence level: I am not a big expert in the whole landspace of AutoML papers, though I am familiar with this type of methods; as for all other aspects, I am fully familiar with them)*\n\n\n**(A) Regarding datasets, in my opinion, the \"quality vs. quantity\" balance should be improved.** I believe that, for the field of tabular data, it is time to raise the bar in terms of dataset quality and to compose benchmarks that will have more chances to generalize to real world problems. After a quick review, I noticed the following datasets that can make the benchmark biased in ways that a hypothetical practitioner would not approve:\n- volcanoes-{a2,a3,a4,b1,b2,b5,b6,d1,d4,e1} -- the real world is not 10x biased towards tabular datasets about volcanoes with 3 features and 5 classes, but the benchmark is biased in this way.\n- wine-quality-{red,white} -- see the previous bullet.\n- fri_c0_1000_5, fri_c0_500_5 and 8 more similar tasks (10 in total) -- also seems to be a set of closely related problems as in previous bullets.\n- optdigits -- I think that computer vision problems should not be included in general tabular benchmarks, or should be presented in a separate group.\n- kr-vs-k -- I think that deterministic game-based problems (here, chess) should not be included in general tabular benchmarks, or should be presented in a separate group.\n- etc.\n\nPerhaps, works like `[1]` can be a source of more realistic datasets (worth mentioning: `[1]` is a bit limited in terms of dataset sizes, so other works like `[2]` may also be worth considering).\n\n**(B) I think that models should be more diverse.** The current set of models is strongly biased towards:\n- tree-based models (all models except for MLP)\n- ensemble-like models (all models except for MLP)\n\nI am afraid that this may limit the potential of TabRepo in terms of what kind of analysis it allows conducting and what results it allows uncovering. I suggest considering the following:\n- Adding one linear model.\n- Adding one non-parametric model (e.g. kNN or modern kNN-like models), at least on datasets where it is possible.\n- Adding one modern parametric DL model (note that competitive parametric DL models are not necessarily heavy transformers `[3]`).\n- Adding one modern non-parametric DL model.\n- Keeping no more than two gradient boostings (personally, I would prefer just one, again, to reduce tge bias, given that there is also RandomForest).\n- Excluding ExtraTrees.\n\nAlso, I appreciate that there are various opinions on whether tabular DL models are worth attention. However, if DL models are not well presented, then the benchmark should be positioned as Classic-ML-only benchmark, but not as a general benchmark. Otherwise, some readers may have wrong expectations from the title and the abstract.\n\n**(C) In my opinion, the paper may need bigger stories (bigger than Section 4 and Section 5) to support the proposed dataset.** My understanding is that it is (implicitly) suggested that TabRepo will help others to uncover and tell big/novel/non-trivial stories. However, compared to mainstream dataset-oriented works like `[1]` (where it is easy to imagine a wide target audience and a potential range of works based on the proposed benchmark), TabRepo seems to be more niche, and, to me, it is not immediately obvious how TabRepo can be used to obtain novel results. This is why, in this specific case, I expect the proposed dataset to be supported by at least one strong self-sufficient finding.\n\nI would like to add that:\n- I appreciate the stories told in Section 4 and Section 5, there is nothing wrong with them. However, they are not positioned as founding elements of the paper and, indeed, it may be too early to position them as such.\n- If (A) and (B) are perfectly addressed, then (C) will not be a blocker, at least not for me.\n\n**(D) Other things:**\n- The story in the introduction may be a bit polarizing. I mean things like \"their performance has saturated and state-of-the-art methods now leverage AutoML techniques\", *\"AutoML solutions currently dominate tabular prediction benchmarks\"*, etc. I embraced the suggested perspective for the review, but overall, I don't share it, and I can imagine how this can trigger big discussions.\n- Not a big issue, but personally, I find the \"at no cost\" wording a bit controversial, I would probably avoid it or somehow make it softer. The reported findings have non-zero cost, and the ability to use a public dataset at no cost is a usual property of public datasets. Perhaps, I am missing something here, but sharing this impression just in case.\n\n**References**\n\n- `[1]` \"Why do tree-based models still outperform deep learning on tabular data?\" Grinsztajn et al.\n- `[2]` \"TabR: Tabular Deep Learning Meets Nearest Neighbors in 2023\" Gorishniy et al.\n- `[3]` \"On Embeddings for Numerical Features in Tabular Deep Learning\" Gorishniy et al."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3335/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3335/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3335/Reviewer_EeeT"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3335/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698586298340,
            "cdate": 1698586298340,
            "tmdate": 1699636282964,
            "mdate": 1699636282964,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Vy4AvMra6s",
                "forum": "fTEPeQ00VM",
                "replyto": "i1F5eG5NSq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3335/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3335/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer"
                    },
                    "comment": {
                        "value": "Thank you very much for your constructive and thorough review.\n\n```\n(A) Regarding datasets, in my opinion, the \"quality vs. quantity\" balance should be improved.\n```\nWe took the union of the collection from the AutoMLBenchmark (Gijsbers et al., 2022) and from the Auto-Sklearn 2 paper (Feurer et al., 2020), the exact rationale for dataset selection is currently detailed in Appendix C. \nOur motivation for extending those suites was that both works are prominent in the field of AutoML and in particular Gijsbers 2022 is the current most extensive empirical evaluation of AutoML methods.\nWe agree that the study [1] is also interesting but we believe the current collection covers sufficient variety (it contains the majority of datasets from [1] for instance).\n\n```\n(B) I think that models should be more diverse. The current set of models is strongly biased towards: tree-based models (all models except for MLP) ensemble-like models (all models except for MLP). I am afraid that this may limit the potential of TabRepo\n```\n\nWe agree that more methods are worth adding, in particular DL methods, to increase model diversity and the usefulness of TabRepo.\nFollowing your suggestion, we added: one linear model, one non-parametric model (KNN), one modern parametric DL model (FT-transformer), one modern non-parametric DL model (TabPFN). \nThe results are described in the main response with a table. They follow the observations done in prior work [1,3], e.g. \"modern\" DL models are in between the performance of the best boosted trees methods and MLPs.\n\nWe do not think having only one gradient boosting method would be better for the dataset given that we have 3 DL methods now and boosted trees are performing better on average (the performance of the portfolio degrades significantly when removing CatBoost or LightGBM). We believe the methods are quite balanced between DL and trees with those additions.\n\nRegarding excluding extra-trees, we also do not think it would make the dataset better. The method is useful in itself in particular given that it is extremely fast and can be useful in case one wants to find solution with very low fitting time (as can be seen in Fig 1).\n\n```\n(C) My understanding is that it is (implicitly) suggested that TabRepo will help others to uncover and tell big/novel/non-trivial stories. However, compared to mainstream dataset-oriented works like [1] (where it is easy to imagine a wide target audience and a potential range of works based on the proposed benchmark), TabRepo seems to be more niche, and, to me, it is not immediately obvious how TabRepo can be used to obtain novel results.\n``` \nWe believe we demonstrated at least one impactful use-case for TabRepo which is reaching or outperforming the state-of-the-art of tabular predictions by simply considering ensemble of portfolio configurations. We agree that [1] provides valuable insight on the performance of DL versus tree-based methods. However, it serves a different purpose: it does not compare with SOTA tabular prediction methods (such as AutoGluon or AutoSklearn2 which use ensembling between different model families) and the dataset cannot be used to reach performance close to SOTA without spending significant compute given that all model would have to be retrained to evaluate the performance of ensembles. We believe our paper is serving more than a niche given that reaching SOTA performance at low cost is very important for research.\n\n```\n(D1) \"The story in the introduction may be a bit polarizing. I mean things like \"their performance has saturated and state-of-the-art methods now leverage AutoML techniques\", \"AutoML solutions currently dominate tabular prediction benchmarks\", etc. I embraced the suggested perspective for the review, but overall, I don't share it.\"\n```\nThank you for pointing this in a constructive fashion. Our intent was not to be polarizing and we highly appreciate your feedback. \nWe understand that the sentences may have been polarizing as it opposes base models with AutoML systems whereas both work together (AutoML systems become better with any base models improvement). We reformulated the sentences and removed the term \"saturated\", \"dominate\" which we see as indeed potentially polarizing, we hope the paragraph now reads better.\n\n```\n(D2) \"I find the \"at no cost\" wording a bit controversial\".\n```\nWe agree with your point that \"at no cost\" is potentially misleading and we reworded the sentence to \"at marginal cost\". We believe that \"at marginal cost\" is suited given that running the experiments of the paper takes 2 hours of compute with TabRepo instead of 27K hours if models are retrained from scratch."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3335/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509831162,
                "cdate": 1700509831162,
                "tmdate": 1700509831162,
                "mdate": 1700509831162,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VE4ssYRdxU",
            "forum": "fTEPeQ00VM",
            "replyto": "fTEPeQ00VM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3335/Reviewer_XQfD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3335/Reviewer_XQfD"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a big dataset of tabular model predictions. It showcases a few use cases of such dataset like: offline evaluation of hyperparameter tuning, hyperparameter transfer (portfolio learning). It shows that a simple portfolio learning method using this dataset outperforms state-of-the-art AutoML system on a standard benchmark."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is very well written.\n- The experimental setup is sound: proper baselines, standard and relevant benchmark.\n- The idea of sharing a large set of model evaluations is interesting, potentially practical and extensible. \n- Performing on par with SoTA AutoML systems with a simple model selection technique from a proposed dataset.\n- The investigation into how much data is needed for efficient transfer is insightful."
                },
                "weaknesses": {
                    "value": "- A set of models is rather small. One potentially interesting\n  extension could be adding more mainstream DL techniques (besides\n  transformers, which are considerably slower, as correctly noted in\n  the limitations section). Extending MLPs with regularization\n  techniques from `[1]` could make the results more \"modern\" in the DL\n  part of tabular models. MLPs with embeddings for continuous features\n  from `[2]` is another potential candidate for a more \"modern\" but\n  still fast DL method.\n- It is unclear how the results would transfer to a more\n  out-of-distribution datasets. Would portfolio transfer work as well\n  as AutoML or hyperparameter tuning on datasets that differ from the\n  datasets present in the benchmark in some aspects. Seeing\n  performance on disregarded larger datasets (discussed in appendix)\n  could shed more light on practical applicability of TabRepo\n- One limitation that should be discussed is the tradeoff between\n  computational efficency and memory. TabRepo is a large dataset,\n  this could introduce problems in practice and make AutoML systems preferable.\n\n**References**:\n- `[1]` Kadra, Arlind, et al. \"Well-tuned simple nets excel on tabular datasets.\" Advances in neural information processing systems 34 (2021): 23928-23941.\n- `[2]` Gorishniy, Yury, Ivan Rubachev, and Artem Babenko. \"On embeddings for numerical features in tabular deep learning.\" Advances in Neural Information Processing Systems 35 (2022): 24991-25004."
                },
                "questions": {
                    "value": "- Are the portfolios (selected models+hyperparameters) interpretable? (In a rough sense: there is a large MLP, small GBDT, heavily regularized MLP, etc.), or the portfolios are mostly random and change for different subsets (of datasets)?\n- Could TabRepo results be used for a new, potentially OOD datasets (for example larger tabular datasets than present in the benchmark). How does zero shot portfolio transfer compares to AutoML and hyperparameter tuning on OOD datasets?\n\nMinor remarks (mostly stylistic or notation):\n- In the model bagging section there might be a slight misuse of the $[n] = \\{1,...,n\\}$ notation introduced earlier, where it is used as an index in $(X^{(\\mathrm{train})}[b], y^{(\\mathrm{train})}[b]), (X^{(\\mathrm{val})}[b], y^{(\\mathrm{val})}[b])$.\n- In the same model bagging section and the next (Datasets, predictions and evaluation) you say that models are fitted by minimizing the losses, in case of binary classification it's AUC, is it directly optimized for all models, or is it just used as a metric (and the terms loss and metric are used interchangeably)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3335/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3335/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3335/Reviewer_XQfD"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3335/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819225900,
            "cdate": 1698819225900,
            "tmdate": 1699636282881,
            "mdate": 1699636282881,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5ND8bz4aNp",
                "forum": "fTEPeQ00VM",
                "replyto": "VE4ssYRdxU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3335/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3335/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer"
                    },
                    "comment": {
                        "value": "We thank you for the review. \n\n```\nOne potentially interesting extension could be adding more mainstream DL techniques...\n```\n\nWe added two more \"modern\" DL models, FT-transformer from [2] and TabPFN, as it was also pointed out by Reviewer EeeT that this family was under-represented. The methods performs a bit better than MLPs and are a bit worse that tree-based methods on the studied datasets which is consistent with previous findings in the literature.\n\n\n```\nIt is unclear how the results would transfer to a more out-of-distribution datasets.\n```\nRegarding the generalization to out-of-distribution datasets, we evaluate portfolios on one unseen dataset via leave-one-dataset-out cross-validation. Of-course, the unseen dataset is still drawn from the same \"distribution\" of OpenML datasets however this gives a diverse distribution (as can be seen in Table 4-5-6 in the appendix). \n\n```\nCould TabRepo results be used for a new, potentially OOD datasets (for example larger tabular datasets than present in the benchmark). How does zero shot portfolio transfer compares to AutoML and hyperparameter tuning on OOD datasets?\n```\nYes, we initially ran our evaluations with all datasets including the larger ones but observed the same results. Consequently, we decided to filter them as one obtained the same conclusion at lower compute cost. With larger datasets, we observed the same behavior e.g. that portfolio were outperforming AutoML systems which were outperforming ensembles of models from single families.\n\nThat being said, we believe one could improve over simple portfolio approaches which are agnostic to the dataset at hand by engineering dataset features. We chose to not include this approach in order to propose a simple baseline as we thought having a simple method perform on-par or better than current SOTA systems is a more interesting finding.\n\n```\nOne limitation that should be discussed is the tradeoff between computational efficency and memory. TabRepo is a large dataset, this could introduce problems in practice and make AutoML systems preferable.\n```\nThank for pointing this, we agree the tradeoff between efficiency and memory is important and we added a discussion of this point in the paper.\nAs pointed out in the appendix TabRepo takes 107GB as such having to store it in memory would impose expensive hardware. We bypassed this issue by using a memmap datastructure which allows to load the model predictions on the fly from disk to memory and allows to run the experiments on commodity hardware (only ~20GB of memory are needed). We added a mention of this point in appendix A).\n\nNote that loading TabRepo is only required to perform portfolio selection, once those are learned an AutoML system can just store the list of models and re-use them on unseen tasks. \n\n```\nAre the portfolios (selected models+hyperparameters) interpretable? (In a rough sense: there is a large MLP, small GBDT, heavily regularized MLP, etc.), or the portfolios are mostly random and change for different subsets (of datasets)?\n```\nYes indeed, the portfolios are interpretable and consistent when computed for different hold-out test dataset.\nThis consistency is expected as each portfolio shares 198 datasets in common with one-another (due to leave-one-dataset-out). \n\nEvery portfolio contains the exact same first two models from CatBoost and MLP families. The third model is always from the LightGBM model family although it is not always exactly the same configuration. This portfolio pick order is somehow expected as tree models and neural networks ensemble well due to their lower correlation (which can be seen in Fig 1) and thus a MLP configuration is picked despite being a weaker model individually than other options. The early models all have low learning rates and are above average in model size (ex: number of leaves, depth, layers), but otherwise have hyperparameters that avoid the extremes of the search spaces. \n\nInterestingly, we observe that all 6 model families are picked at least once within the first 12 models in the portfolios which indicates that the model diversity is beneficial."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3335/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509707283,
                "cdate": 1700509707283,
                "tmdate": 1700509707283,
                "mdate": 1700509707283,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MFxvAVSD8x",
            "forum": "fTEPeQ00VM",
            "replyto": "fTEPeQ00VM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3335/Reviewer_PZnZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3335/Reviewer_PZnZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides a collection of predictions from a wide range of models over an extensive benchmark of 200 regression and classification datasets. The paper draws some conclusions about model performance in different families, and how the predictions can be used for post-hoc ensembling."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I agree with the paper that extensive benchmarking is quite expensive, and the AutoML benchmark in particular is expensive to run. \nThe paper is quite clearly written, and easy to follow."
                },
                "weaknesses": {
                    "value": "It's unclear to me how the problem of expensive benchmarks is solved by the proposed repository; at best it can be a benchmark for ensemble strategies. Something similar has been done in \"CMA-ES for Post Hoc Ensembling in AutoML: A Great Success and Salvageable Failure\" by Purucker, though with much more involved ensembling methods.\n\nThis paper only uses a simple greedy method, similar to what is used in Autosklearn, or \"Mining Robust Default Configurations for Resource-constrained AutoML\" (Flaml zero shot) or \"Learning Multiple Defaults for Machine Learning Algorithms\" or \"Learning hyperparameter optimization initializations\".\n\nFuthermore, the distinction and benefit over OpenML is not entirely clear. Figure 1, for example, could have been generated with the runs on OpenML, which contains 10M runs, compared to 200k runs in this paper (with the disclaimer that the runs are not a cross-product of models and datasets, i.e. not all models are evaluated on all datasets, though there is several \"studies\" that do exactly that).\n\nThe main reason that OpenML does not store predictions or probabilities is that this would be very storage intensive and there is no funding for it. Most works that do portfolio building have computed all of these metrics, though they are usually not shared since the storage overhead seems daunting."
                },
                "questions": {
                    "value": "How large is TabRepo in GB?\nDo you intent for TabRepo to have new models dynamically added, or do you want to fix thecurrent models?\nWhat future uses do you see for TabRepo?\nHow does your work compare to \"CMA-ES for Post Hoc Ensembling in AutoML\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3335/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698870333084,
            "cdate": 1698870333084,
            "tmdate": 1699636282807,
            "mdate": 1699636282807,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ohFONXsDYD",
                "forum": "fTEPeQ00VM",
                "replyto": "MFxvAVSD8x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3335/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3335/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer"
                    },
                    "comment": {
                        "value": "We thank you for your review. \n\n```\n* It's unclear to me how the problem of expensive benchmarks is solved by the proposed repository; at best it can be a benchmark for ensemble strategies. Something similar has been done in \"CMA-ES for Post Hoc Ensembling in AutoML: A Great Success and Salvageable Failure\" by Purucker, though with much more involved ensembling methods.\n* This paper only uses a simple greedy method, similar to what is used in Autosklearn, or \"Mining Robust Default Configurations for Resource-constrained AutoML\" (Flaml zero shot) or \"Learning Multiple Defaults for Machine Learning Algorithms\" or \"Learning hyperparameter optimization initializations\".\n```\nWe agree that the ensembling method proposed in Purucker is very interesting. However, we chose to report only the performance of simple known methods (Caruana ensemble on top of greedy portfolio) because the the main point of our paper is to introduce a dataset and not a new method. In particular, we show that those simple known methods outperform or matche the performance of all AutoML systems and methods which we believe illustrate some of the benefit of the proposed dataset. That being said, we believe the work is very relevant to provide further improvement and added a reference in our paper.\n\n```\n* The main reason that OpenML does not store predictions or probabilities is that this would be very storage intensive and there is no funding for it. Most works that do portfolio building have computed all of these metrics, though they are usually not shared since the storage overhead seems daunting.\n* Furthermore, the distinction and benefit over OpenML is not entirely clear. Figure 1, could have been generated with the runs on OpenML ...\n\n```\nThe storage is ~107GB (mentioned in the appendix C and F3) which is tractable. As you pointed out, most work does not store predictions and does not allow to efficiently measure the performance of ensemble configurations, this is a key contribution of our paper and dataset.\n\nRegarding the difference with OpenML, we agree that the first two figures could have been generated with current results of OpenML. However, the key difference of TabRepo is to also expose the model predictions. This is a critical aspect as it allows researchers to consider ensembles and consequently to simulate solutions that outperforms SOTA AutoML systems at little cost (just recomputing metrics after having retrieved existing predictions). \n\nWe believe that the key findings of the paper that the performance of SOTA tabular methods can be matched with simple techniques (as you said using only simple greedy methods) is a valuable finding for the community. We also think that providing a way to reach SOTA at low compute cost can be highly valuable for future research.\n\n```\nHow large is TabRepo in GB? Do you intent for TabRepo to have new models dynamically added, or do you want to fix the current models? What future uses do you see for TabRepo? How does your work compare to \"CMA-ES for Post Hoc Ensembling in AutoML\"?\n```\n\nRegarding your last question, we plan to add landmark models over time. As future use, we believe any work considering ensembling can leverage TabRepo, in particular we believe work considering multi-fidelity (e.g. increase the number of folds as the fidelity) or cross hyperparameter/model tuning (CASH) are both interesting research directions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3335/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509533274,
                "cdate": 1700509533274,
                "tmdate": 1700509533274,
                "mdate": 1700509533274,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mYf9ceRsMD",
                "forum": "fTEPeQ00VM",
                "replyto": "ohFONXsDYD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3335/Reviewer_PZnZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3335/Reviewer_PZnZ"
                ],
                "content": {
                    "title": {
                        "value": "Re Purucker"
                    },
                    "comment": {
                        "value": "My point in bringing up Purucker was mostly that the kind of research that you want to enable with TabRepo is already being done, and that someone interested in this research could simply reach out to Purucker and ask for the dataset they used.\nSimilar is likely true for the other methods I mentioned. The reason that Purucker did not make their data immediately available is likely not because they want to keep it private, but because they didn't consider it a valuable research contribution.\nThe fact that none of the previous publications didn't bother to make the intermediate data available doesn't seem a strong enough reason to publish this one."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3335/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667011697,
                "cdate": 1700667011697,
                "tmdate": 1700667011697,
                "mdate": 1700667011697,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1Pl5rvHIpo",
            "forum": "fTEPeQ00VM",
            "replyto": "fTEPeQ00VM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3335/Reviewer_kTk5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3335/Reviewer_kTk5"
            ],
            "content": {
                "summary": {
                    "value": "Authors introduce a large dataset of tabular model evaluations on a large set of models as well as datasets. The prediction outputs of the considered models are also provided for efficient analysis without having to reevaluate the models. Authors demonstrate the utility of their dataset by 1. comparing hpo methods and auto-ml systems, 2. demonstrating ensembling, portfolio-selection and 3. transfer learning capabilities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The evaluation is extensive, with all the models constructed through bagging on multiple cross-validation folds and initialization for each dataset. \n- Utility of TabRepo is demonstrated by analyzing the cost of tuning and the performance obtained for various auto-ml methods.\n- Model portfolio  construction and transfer learning is shown to be effective using already-computed predictions."
                },
                "weaknesses": {
                    "value": "As a dataset of tabular-model evaluations, the work is sound. However, I am not entirely convinced about the utility of the analysis provided in this work. For ex, various autoML methods and their performance comparisons (Fig 2) are already provided as part of AutoGluon. It would be helpful if the authors could illustrate a few more cases which potentially could benefit from TabRepo."
                },
                "questions": {
                    "value": "Could you suggest few more potential use-cases that benifit from including prediction outputs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3335/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699054636531,
            "cdate": 1699054636531,
            "tmdate": 1699636282737,
            "mdate": 1699636282737,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8lygfXl6XK",
                "forum": "fTEPeQ00VM",
                "replyto": "1Pl5rvHIpo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3335/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3335/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer"
                    },
                    "comment": {
                        "value": "Thank you for your review. \n\n```\n* As a dataset of tabular-model evaluations, the work is sound. However, I am not entirely convinced about the utility of the analysis provided in this work. For ex, various autoML methods and their performance comparisons (Fig 2) are already provided as part of AutoGluon. It would be helpful if the authors could illustrate a few more cases which potentially could benefit from TabRepo.\n* Could you suggest few more potential use-cases that benifit from including prediction outputs?\n```\nWe agree that Fig 2 is already provided as part of AutoGluon's original paper, however it is expensive to reproduce and evaluate ensembles of methods given it requires fitting them on a large collection of datasets. The key contribution of this paper is to make this process much cheaper, by just requiring loading model predictions from disk instead of fitting models.\n\nRegarding use-cases for TabRepo, we believe that the performance of the portfolio ensemble in section 5 shows an important result for the community, namely that a simple ensemble of models can beat all state-of-the-art current AutoML systems. We believe this is an important result as it leads to methods with much lower latency and cost.\n\nRegarding more potential use-cases, we see a lot of potential work as our dataset allow to use many methods while considering ensembling without requiring to fit any model. This is critical since ensembling is almost always required to reach state-of-the-art performance in AutoML systems. For instance potential use-cases could include tuning hyperparameters and models (CASH) or applying early-stopping/multi-fidelity techniques that evaluate configurations partially and only let the top ones run with larger budget."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3335/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509622263,
                "cdate": 1700509622263,
                "tmdate": 1700509622263,
                "mdate": 1700509622263,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]