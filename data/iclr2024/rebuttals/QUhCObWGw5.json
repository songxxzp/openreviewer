[
    {
        "title": "PATHS: Parameter-wise Adaptive Two-Stage Training Harnessing Scene Transition Mask Adapters for Video Retrieval"
    },
    {
        "review": {
            "id": "yg1UyHXEBB",
            "forum": "QUhCObWGw5",
            "replyto": "QUhCObWGw5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2484/Reviewer_hfBT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2484/Reviewer_hfBT"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose a 2 stage method \u201cPATHS\u201d to address the main weakness of the current text-to-video retrieval CLIP-based models, i.e., overfitting. The proposed method includes 2 stages: 1) select best params by monitoring the fluctuations of params, which is much cheaper than e.g. per 50 step eval; 2) train an adapter module STMA with CLIP params frozen.\n\nThe proposed method is generic enough to be applicable to any existing CLIP-based models. The result achieves SOTA in the common text-to-video retrieval tasks: LSMDC and MSRVTT."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The method is relatively simple and the result is strong with several SOTA.\n\nThe experiments are extensive with many baselines; STMA is applied to a wide range of models to show its effectiveness.\n\nThe paper is well written and very readable.\n\nThe code is open-sourced."
                },
                "weaknesses": {
                    "value": "5.4 ablates the param selection strategies, which is great; also it shows stage-1\u2019s importance in Tab 4. However, it\u2019s still unclear what the performance would be like if we apply stage-2 ONLY to the common param selection strategies (i.e. skip stage-1). This might make it clearer how important stage 1 and 2 is respectively?"
                },
                "questions": {
                    "value": "Appendix seems not uploaded?\n\nIIUC, the red curve of Fig 3 is comparable with the green curve of Fig 2 (both are X-CLIP eval), but it seems there\u2019s some difference (e.g. in the end of epoch 5)? If my understanding is correct, maybe it would be clearer if we plot the green curve of Fig 2 in Fig 3 as well?\n\nIn 4.1 \u201cTwo-stage Process\u201d: in the 2nd stage, if params are frozen, why do we still need to \u201cload these parameters back into the model at the end of each epoch to perform the pivoting\u201d? And could you please explain a bit what \u201cperform the pivoting\u201d exactly means?\n\nmild comments:\ntypo in 2.2: \u201cadapters have been *unsed* for progressive learning\u2026\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2484/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2484/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2484/Reviewer_hfBT"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2484/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698702812382,
            "cdate": 1698702812382,
            "tmdate": 1699636184957,
            "mdate": 1699636184957,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Szp08HMual",
                "forum": "QUhCObWGw5",
                "replyto": "yg1UyHXEBB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2484/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2484/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hfBT"
                    },
                    "comment": {
                        "value": "First of all, please accept our apologies for submitting the appendix through supplementary material only, which might have been missed as it was not appended to the main draft.\n\nWe rectified this error by updating the submission to include the appendix within the manuscript. We are deeply sorry for any inconvenience this has caused and would be grateful if you could take a look at the appendix with reconsideration.\n\nWe sincerely thank you for the constructive comments. We address the concerns below.\n\n---\n\n>5.4 ablates the param selection strategies, which is great; also it shows stage-1\u2019s importance in Tab 4. However, it\u2019s still unclear what the performance would be like if we apply stage-2 ONLY to the common param selection strategies (i.e. skip stage-1). This might make it clearer how important stage 1 and 2 is respectively?\n\nThank you for your remarks regarding the clarity of our two-stage method's performance and the potential concerns of overfitting.\n\nRegarding the ablation of parameter selection strategies in Section 5.4, we have conducted experiments using X-CLIP and CLIP4Clip as backbones to assess the isolated impact of the second stage without the first (parameter-wise method). Unfortunately, due to space constraints and the structure of Table 4, these results were not included in the manuscript. When the second stage of our method was applied without the parameter-wise initial stage (with initialization done epoch-wise), we observed a performance of Table below. We recognize the importance of this information and will include these details in the appendix of our revised manuscript for a comprehensive understanding.\n\n| Model | R@1 | R@5 | MeanR |\n|-------|--------|-------|--------|\n| X-CLIP | 47.7 | 73.8 | 13.4 |\n|CLIP4Clip| 43.9 | 72.2 | 15.1 |\n\n\n---\n\n>Appendix seems not uploaded?\n\nThank you for informing us regarding the appendix. The appendix was initially uploaded only through the supplementary material. We would like to append the appendix at the end of main draft for better readability. Once again, we appreciate your attention to this matter and assistance in improving our submission.\n\n---\n\n>IIUC, the red curve of Fig 3 is comparable with the green curve of Fig 2 (both are X-CLIP eval), but it seems there\u2019s some difference (e.g. in the end of epoch 5)? If my understanding is correct, maybe it would be clearer if we plot the green curve of Fig 2 in Fig 3 as well?\n\nWe appreciate your observation regarding the comparison between the red curve in Figure 3 and the green curve in Figure 2, both of which represent evaluations of the X-CLIP model. The red curve in Figure 3 illustrates the performance at points selected by our parameter-wise method. As such, it does exhibit differences from the green curve in Figure 2, which represents evaluations conducted at fixed intervals (per 50 epochs).\n\nThe x-axis in Figure 2 corresponds to these fixed intervals, whereas the x-axis in Figure 3 corresponds to evaluations conducted five times per epoch, with K set to 5. This difference in evaluation frequency makes a direct overlay of the two curves challenging. However, we recognize the need for clearer comparative visualization to facilitate reader understanding. We will explore ways to integrate these two plots more effectively and update the figures accordingly in our revised manuscript.\n\n---\n\n>In 4.1 \u201cTwo-stage Process\u201d: in the 2nd stage, if params are frozen, why do we still need to \u201cload these parameters back into the model at the end of each epoch to perform the pivoting\u201d? And could you please explain a bit what \u201cperform the pivoting\u201d exactly means?\n\nThank you for inquiring about the process described in Section 4.1 of our manuscript. During the second stage, while the original parameters of the model are indeed frozen, we continue to train the adapter. In this stage, we monitor and pivot the ranking of relative changes within the adapter's internal group of parameters.\n\nPivoting, in this context, refers to restoring the model's parameters to the state with the highest recorded performance at the end of each epoch. This is achieved by reloading the saved parameters, allowing the model to revert to the optimal point before continuing the training process. This technique helps to fine-tune the model by leveraging the most influential parameter settings discovered during the training epochs.\n\nWe will ensure that this explanation is clarified within our manuscript to assist readers in understanding the nuanced training dynamics of our proposed two-stage process.\n\n---\n\n>mild comments: typo in 2.2: \u201cadapters have been\u00a0unsed\u00a0for progressive learning\u2026\u201d\n\nWe are deeply thankful for your astute attention to detail concerning the typographical error found in our manuscript. This has been immediately corrected in the text.\n\n---\n\nThank you for your time and consideration.\n\nBest regards,"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2484/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699852786176,
                "cdate": 1699852786176,
                "tmdate": 1699853715525,
                "mdate": 1699853715525,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QTeLwL2eYm",
                "forum": "QUhCObWGw5",
                "replyto": "yg1UyHXEBB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2484/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2484/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer hfBT,\n\nWe are grateful for your constructive feedback.\n\nWe would appreciate informing us whether your concerns have been adequately addressed.\n\nYour insights are highly valued, and we would like to let you know that we would happily answer any further questions if any.\n\nBest regards, The Authors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2484/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585542138,
                "cdate": 1700585542138,
                "tmdate": 1700585542138,
                "mdate": 1700585542138,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KZPVkspxS5",
                "forum": "QUhCObWGw5",
                "replyto": "QTeLwL2eYm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2484/Reviewer_hfBT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2484/Reviewer_hfBT"
                ],
                "content": {
                    "comment": {
                        "value": "Most of my concerns have been addressed. I would like to thank authors for the explanation."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2484/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686840275,
                "cdate": 1700686840275,
                "tmdate": 1700686840275,
                "mdate": 1700686840275,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aUCc5FYB5t",
            "forum": "QUhCObWGw5",
            "replyto": "QUhCObWGw5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2484/Reviewer_zzzT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2484/Reviewer_zzzT"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the author proposes a two-stage training method (PATHS) to improve the results of video-retrieval tasks. In the first stage, it selects 5 candidates according to the parameter ranking. And the results of these candidates are better than those epoch candidates. In the second stage, it uses the previous best checkpoint for initialization and adds an adapter for further fine-tuning. Further experiments demonstrate its effectiveness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The motivation is clear and the method is simple to reproduce."
                },
                "weaknesses": {
                    "value": "- The paper is not well organized. \n  - The PRELIMINARIES section seems to be redundant, and the `3.1` and `3.1.1` seem to stage for a single section (subsection). \n  - Some details are not clear. For example, how does the ranking work? How does a two-stage scheme w/o STMA work (no adapter and freeze the backbone)? How to split scenes in STMA?\n- The method seems to be tricky, which overfits a specific test set.\n- The Figures 2 and 3 are in low resolution and hard to read."
                },
                "questions": {
                    "value": "- In Page 5, `Main Idea` part, `In accordance with Figure 1, we denote end of each epoch with dotted line` should be `Figure 2`.\n- In Page 5, `Motivation` part, `When the model starts to diverge after passing the optimum point, the model parameter values exhibit strong fluctuations. This often involves rearranging parameters in terms of importance (or the value of parameters)`, how can the author get the conclusion?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2484/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811977721,
            "cdate": 1698811977721,
            "tmdate": 1699636184851,
            "mdate": 1699636184851,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F5VxyY5aVK",
                "forum": "QUhCObWGw5",
                "replyto": "aUCc5FYB5t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2484/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2484/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zzzT"
                    },
                    "comment": {
                        "value": "First of all, please accept our apologies for submitting the appendix through supplementary material only, which might have been missed as it was not appended to the main draft. We rectified this error by updating the submission to include the appendix within the manuscript. We are deeply sorry for any inconvenience this has caused and would be grateful if you could take a look at the appendix with reconsideration.\n\nWe sincerely thank you for the constructive comments. We address the concerns below.\n\n---\n\n>The PRELIMINARIES section seems to be redundant, and the\u00a03.1\u00a0and\u00a03.1.1\u00a0seem to stage for a single section (subsection).\n\nWe acknowledge your concerns regarding the structure of our PRELIMINARIES section and the segmentation between sections 3.1 and 3.1.1. We are considering restructuring these sections into a cohesive single section or revising the PRELIMINARIES section to eliminate redundancy and effectively address your concerns. We are grateful for your precise feedback and will refine our manuscript accordingly.\n\n---\n\n>Some details are not clear. For example, how does the ranking work? How does a two-stage scheme w/o STMA work? How to split scenes in STMA?\n\nThank you for highlighting areas in our manuscript where further clarity is needed. Regarding the parameter group ranking, it operates as outlined in Section 4.1: Quantifying Dynamics subsection. We monitor the magnitude of change within each parameter group and compare it to the changes in other groups. In other words, Ranking works by comparing the amount of change in the \"parameter group\". Based on these observations, we use three methods\u2014$BP$, $SP$, and $USP$\u2014to determine the final evaluation points. This process allows us to assess and rank the parameter groups effectively. \n\nIn the case of operating without the Scene Transition Mask Adapter (STMA), our two-stage scheme would not freeze the original parameters but would instead continue additional training. We acknowledge that this was not sufficiently detailed in our initial description and will make the necessary amendments to ensure this is clearly articulated in our revised manuscript. \n\nAs for scene splitting within STMA, this is elaborated in Appendix A.3. We calculate the similarity between video frames and identify where the largest difference in similarity occur, using these points to segment the video. This method allows the STMA to process and learn from the inherent structure of the video content effectively. We will incorporate these clarifications into the revised version of our manuscript to ensure these processes are transparent and comprehensible for all readers.\n\n---\n\n>The method seems to be tricky, which overfits a specific test set.\n\nThank you for your comment on the potential overfitting of our method to a specific test set. We want to clarify that at no point during the training phase do we utilize the test set. Our methodology strictly follows the experimental protocol. That is to say, we strictly adheres to using validation datasets across all datasets, and the parameters selected for final evaluation are those that yield the highest performance on the validation set. The results reported are derived from assessments conducted on the test dataset using these optimally tuned parameters. This approach ensures that our method is evaluated fairly and without bias towards the test set.\n\n---\n\n>In Page 5,\u00a0Main Idea\u00a0part,\u00a0In accordance with Figure 1, we denote end of each epoch with dotted line\u00a0should be\u00a0Figure 2.\n\nWe are truly grateful for your keen observation regarding the typographical error in our manuscript. We have promptly addressed and corrected this in the manuscript.\n\n---\n\n>In Page 5,\u00a0Motivation\u00a0part,\u00a0When the model starts to diverge after passing the optimum point, the model parameter values exhibit strong fluctuations. This often involves rearranging parameters in terms of importance, how can the author get the conclusion?\n\nWe apologize for the confusion, we originally meant to use \"fluctuation\" to describe \"rank fluctuation\".\n\nAs elaborated in the Motivation section on page 5 of our manuscript, the statement *\"When the model starts to diverge after passing the optimum point, the model parameter values exhibit strong fluctuations. This often involves rearranging parameters in terms of importance\"* is predicated on the previous sentences on manuscript: *\"The solution we proposed is based on our key observation. When the model converges, parameters in the neural network exhibit stable behavior. Here, we focus on the other way, i.e., when the model diverges.\"*\n\nOur hypothesis is grounded in the observation that parameters in a neural network stabilize as the model converges. Conversely, we posit that significant parameter\u2019s rank fluctuations suggest divergence, which indicates instability. This premise has shaped our development of the PATHS and guided our empirical efforts to substantiate its effectiveness.\n\nThank you for your time and consideration.\n\nBest regards,"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2484/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699852561287,
                "cdate": 1699852561287,
                "tmdate": 1699854287363,
                "mdate": 1699854287363,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hUz5Y63jIg",
                "forum": "QUhCObWGw5",
                "replyto": "aUCc5FYB5t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2484/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2484/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer zzzT,\n\nWe are grateful for your constructive feedback.\n\nWe would appreciate informing us whether your concerns have been adequately addressed.\n\nYour insights are highly valued, and we would like to let you know that we would happily answer any further questions if any.\n\nBest regards, The Authors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2484/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585490873,
                "cdate": 1700585490873,
                "tmdate": 1700585490873,
                "mdate": 1700585490873,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r9CgFobvrv",
                "forum": "QUhCObWGw5",
                "replyto": "hUz5Y63jIg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2484/Reviewer_zzzT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2484/Reviewer_zzzT"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "I appreciate the authors' response, but I maintain my initial assessment and am inclined to reject the paper.\n\nIn my view, the key concept remains somewhat `tricky`. Moreover, regarding the issue of `overfitting`, I harbor concerns that such a strategy could potentially compromise robustness in practical applications. For instance, if we were to implement this strategy during pretraining with the aim of selecting the optimal checkpoint.\n\nAs for the manuscript, it necessitates significant polishing and improved presentation:\n\n(1) The diagrams suffer from low resolution and their color schemes make them difficult to interpret.\n\n(2) The font sizes used within the tables lack consistency; furthermore, references are conspicuously absent in Tables 3 & 4. Vital context-related claims should also be incorporated into the captions.\n\n(3) The manuscript's structure should be reorganized to better highlight the authors' methodology. For example, visualizing the BP, SP and USP pipeline would be beneficial for readers unfamiliar with the ranking system. Moreover, Figure 6, which pertains to STMA, would ideally be included within Figure 4 to enhance readability.\n\nLooking forward, I hope the authors will invest significant effort in refining their paper. From my perspective, the core idea could be neatly partitioned into two segments: architecture and training. I recommend the use of more engaging visuals in lieu of excessive text to depict the motivation and processing pipeline. Providing more ablation studies on the architecture and training, emphasizing efficiency and effectiveness (i.e., performance vs. training time) would also add value. A demonstration of robustness in pretraining contexts, perhaps added to current efficient CLIP video post-pretraining methods, would contribute further strength to the paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2484/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724927637,
                "cdate": 1700724927637,
                "tmdate": 1700724927637,
                "mdate": 1700724927637,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U6M5XeXbmv",
            "forum": "QUhCObWGw5",
            "replyto": "QUhCObWGw5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2484/Reviewer_Cekr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2484/Reviewer_Cekr"
            ],
            "content": {
                "summary": {
                    "value": "The paper tries to address the weight corruption problem arising when extending pre-trained CLIP weights to tasks in the video domain.\n\nIt proposes a new learning strategy named \"Parameter-wise Adaptive Two-stage training Harnessing Scene transition mask adapter\" (PATHS), which involves a two-phase learning process. The first phase focuses on determining the optimal weights by monitoring parameter fluctuations, while the second phase concentrates on understanding scenes using an adapter module. \n\nThe paper demonstrates the effectiveness of their approach by achieving leading performances on major text-video benchmark datasets such as MSRVTT and LSMDC."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method does not require frequent evaluations at every N step, which is distinct from recent approaches that incur extra computational overhead.\n2. PATHS can be applied to strong baselines in a plug-and-play manner and has shown consistent performance improvements."
                },
                "weaknesses": {
                    "value": "1. The elaboration in Section 4.2 on the proposed **Co-Attention** module in STMA is not clear enough. The sentence '*the co-attention layer takes different queries, keys, and values to enable the learning and updating of two pieces of information regarding each other*' is confusing. What are the settings of QKV in your Co-Attention? Considering this module is part of the core designs, more formulation or illustration is needed for better understanding.\n2. If the **Alignment Attention** in Figure 4 is the so-called *'attention layer' utilized to identify the crucial parts of the video*, the authors should clarify its module name in the paper. Is **Alignment Attention** in STMA a vanilla attention module? How does the attention layer *identify the most crucial part of the video throughout the entire video and each scene*? More explanation and evidence are needed to support this.\n3. According to the paper, the authors set the hyperparameter $K$ as 5. How do the authors choose this value? More ablation studies on $K$ should be conducted.\n4. What if in some certain samples, there does not exist a scene transition or contains more than one transition? Can the proposed STMA be applied to all possible conditions?"
                },
                "questions": {
                    "value": "Please see the Weaknesses mentioned above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2484/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698897972668,
            "cdate": 1698897972668,
            "tmdate": 1699636184784,
            "mdate": 1699636184784,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E4CHxOj0YK",
                "forum": "QUhCObWGw5",
                "replyto": "U6M5XeXbmv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2484/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2484/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Cekr (1/2)"
                    },
                    "comment": {
                        "value": "First of all, please accept our apologies for submitting the appendix through supplementary material only, which might have been missed as it was not appended to the main draft. We rectified this error by updating the submission to include the appendix within the manuscript. We are deeply sorry for any inconvenience this has caused and would be grateful if you could take a look at the appendix with reconsideration.\n\nWe sincerely thank you for the constructive comments. We address the concerns below.\n\n---\n\n>The elaboration in Section 4.2 on the proposed\u00a0Co-Attention\u00a0module in STMA is not clear enough. The sentence 'the co-attention layer takes different queries, keys, and values to enable the learning and updating of two pieces of information regarding each other' is confusing. What are the settings of QKV in your Co-Attention?\n\nWe appreciate the feedback regarding the description of the Co-Attention module. To clarify, our Co-Attention module is designed to update the values of one modality by taking it as a query and using the keys from another modality. Specifically, when we have two scenes, say $A$ and $B$, scene $A$ serves as the query, and the keys from scene $B$ are utilized to update the values corresponding to scene $B$. In light of your comment, we recognize the need for a more detailed explanation and will update the manuscript to include a clearer description of the QKV settings in our Co-Attention module. \n\n---\n\n>If the\u00a0Alignment Attention\u00a0in Figure 4 is the so-called\u00a0'attention layer' utilized to identify the crucial parts of the video, the authors should clarify its module name in the paper. Is\u00a0Alignment Attention\u00a0in STMA a vanilla attention module? How does the attention layer\u00a0identify the most crucial part of the video throughout the entire video and each scene?\n\nThank you for your inquiry regarding the Alignment Attention. Yes, the Alignment Attention Layer within our system is indeed a vanilla attention module. It functions by taking the original video, denoted as $A$, and the scenes $B$ and $C$, which are derived from $A$ and updated through Co-Attention, as inputs. These inputs are then concatenated. The Alignment Attention operates on this concatenated information to compute additional attention scores across the elements, thereby extracting the most salient parts of the video. This process effectively allows the model to identify and focus on the most informative segments throughout the entire video and within each scene. You can find a figure of this in A.3 in the appendix. We acknowledge the need for a more comprehensive explanation in our manuscript and will enhance the manuscript with additional details to better support the functioning and utility of the Alignment Attention.\n\n---\n\n>According to the paper, the authors set the hyperparameter\u00a0$K$ as $5$. How do the authors choose this value? More ablation studies on K should be conducted.\n\nThank you for your question regarding selecting our study's hyperparameter value set to 5. As mentioned in comment 1 of reviewer 1, We acknowledge the trade-off between the upper bound of performance and the computational overhead that comes with increasing the value of this $K$. We chose $5$ to maintain computational efficiency while capturing a significant portion of the performance gains. We posit that a higher evaluation frequency could further enhance performance in cases with larger training datasets and more learning steps. However, for the scope of our experiments, we found that setting this $K$ to $5$ provided a suitable balance between performance and computational resource management. We appreciate the suggestion for additional ablation studies on this $K$ and will consider this for future research to further elucidate its impact. \n\nThe suggested ablation experiments (different K) are currently underway.  We wanted to respond to other questions first in a timely manner, and would like to come back once we get the results.\n\n---\nBelow, we continue our response with the results.\n\nWe are very thankful for your constructive suggestion. As indicated in the table, there is a trend that performance improves as the size of K increases. However, our objective was not to find the ideal value of K but to identify a value that minimizes computational overhead and can increase the potential of the model. This was the rationale for fixing K at 5 across all experiments. We genuinely appreciate your precise observation and will ensure this point is addressed in our manuscript to aid our readers\u2019 understanding. Thank you.\n\n| Model |Num K| R@1 | R@5 | MeanR |\n|-------|-------|--------|-------|-------|\n| X-CLIP | Baseline | 47.2 | 73.5 | 13.8 | \n| |K = 3| 48.1 | 73.2 | 13.5 |\n| |K = 5 | 48.4 | 73.7 | 13.2 |\n||K = 10 | 48.4 | 73.3 | 13.3 |\n|CLIP4Clip| Baseline| 43.0 | 71.9 | 15.6|\n||K = 3| 44.3 | 72.2 | 14.6 |\n||K = 5|44.5 | 72.2 | 14.6 |\n||K = 10| 45.0 | 72.3 | 15.0 |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2484/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699851896657,
                "cdate": 1699851896657,
                "tmdate": 1699976971655,
                "mdate": 1699976971655,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0KN9UMzn07",
                "forum": "QUhCObWGw5",
                "replyto": "U6M5XeXbmv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2484/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2484/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer Cekr,\n\nWe are grateful for your constructive feedback.\n\nWe would appreciate informing us whether your concerns have been adequately addressed.\n\nYour insights are highly valued, and we would like to let you know that we would happily answer any further questions if any.\n\nBest regards, The Authors"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2484/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585466870,
                "cdate": 1700585466870,
                "tmdate": 1700585466870,
                "mdate": 1700585466870,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SegimeK2xu",
            "forum": "QUhCObWGw5",
            "replyto": "QUhCObWGw5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2484/Reviewer_tWWe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2484/Reviewer_tWWe"
            ],
            "content": {
                "summary": {
                    "value": "This work tackles the problem of text-to-video retrieval and mainly focuses on the methods based on the pretrained CLIP. To mitigate the overfitting onto a target video dataset, this work proposes a two-stage training method where the first stage optimizes the image-to-video weight transfer, and the second stage introduces an adaptor to further improve the video understanding. The proposed method PATHS is a plug-and-play module and can be added to other existing methods such as CLIP4Clip, X-CLIP, DiCoSA. When tested on MSVD, LSMDC and MSRVTT retrieval benchmarks, PATHS improves over different baseline methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The proposed PATHS method is a plug-and-play model that can be added to other existing methods and consistently improves the retrieval performances.\n* The code is available which helps the reproducibility of the method."
                },
                "weaknesses": {
                    "value": "* As the method requires more frequent evaluation than the standard per-epoch evaluation, the method still introduces computation overhead and leave the frequency as a hyperparameter which will potentially vary for different datasets.\n* The contribution of the STMA adaptor seems marginal. For example in Table 1, the gain from STMA is only 0.2 point.\n* In the ablation section, all BP, SP, USP methods exhibit very similar results. It is hard to tell if one quantifying strategy is better than others, raising a question whether the quantifying strategy is a main component of the proposed method.\n* Typo \"raking\" --> \"ranking\" in section 5.4"
                },
                "questions": {
                    "value": "* While the proposed method focuses on the retrieval task, would PATHS method also applicable to other video-text tasks such as video captioning or video QA?\n* Please see the weaknesses section for other questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2484/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698959299293,
            "cdate": 1698959299293,
            "tmdate": 1699636184723,
            "mdate": 1699636184723,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Cm2cxdgzDt",
                "forum": "QUhCObWGw5",
                "replyto": "SegimeK2xu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2484/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2484/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tWWe"
                    },
                    "comment": {
                        "value": "First of all, please accept our apologies for submitting the appendix through supplementary material only, which might have been missed as it was not appended to the main draft.\n\nWe rectified this error by updating the submission to include the appendix within the manuscript. We are deeply sorry for any inconvenience this has caused and would be grateful if you could take a look at the appendix with reconsideration.\n\nWe sincerely thank you for the constructive comments. We address the concerns below.\n\n---\n\n>As the method requires more frequent evaluation than the standard per-epoch evaluation, the method still introduces computation overhead and leave the frequency as a hyperparameter which will potentially vary for different datasets.\n\nYour concern regarding the computational overhead due to more frequent evaluations is indeed valid. However, our perspective is to address the limitation of conventional epoch-wise evaluation, which may not fully use the potential of \u201cexisting\u201d models. Our goal is to achieve higher performance while keeping the computational overhead minimized. We believe that while the ideal scenario would be to obtain optimal results with equal or reduced computational costs compared to epoch-wise evaluation, our method demonstrates enhanced efficiency against the current alternative ($N-step$ evaluation). We aim to emphasize that our approach is designed to strike a balance between uncovering the potential of models and maintaining computational feasibility. \n\nWhile the frequency ($K$) is a hyperparameter, we have fixed it to $K=5$ across all the datasets in our experiments without any further tuning. We claim that the reported results are not achieved through extra parameter tuning. \n\n---\n\n>The contribution of the STMA adaptor seems marginal. For example in Table 1, the gain from STMA is only $0.2$ point.\n\nWe appreciate the reviewer\u2019s observation regarding the perceived marginal contribution of the STMA adapter, as reflected in Table 1. However, it is basically designed as an adapter for transfer learning. If only STMA is applied in stage 1, the STMA module may not seem relatively important. This is because the complexity of the model increases by adding an adapter while the model is not optimized. However, it is different in stage 2. For $w/o STMA$ in Table 1 at stage 2, $R@1$ performs well, but you can see that $R@5$, $R@10$, and $MeanR$ are overfitted compared to the baseline. Our parameter-wise two-stage method pivots based on the $R@1$ performance on the validation set during the second stage, which could indeed lead to overfitting on $R@5$, $R@10$, and $MeanR$. To counteract this, we integrate the \u201cscene\u201d information into the model learning process, which not only mitigates the risk of overfitting but also contributes to an incremental increase in $R@1$.\n\n---\n\n>In the ablation section, all BP, SP, USP methods exhibit very similar results. It is hard to tell if one quantifying strategy is better than others, raising a question whether the quantifying strategy is a main component of the proposed method.\n\nThank you for your insightful comments regarding the ablation study and the performance of $BP$, $SP$, and $USP$ methods. It's worth noting that these methods are employed to determine the parameter ranking within our framework. We do not explicitly confirm one strategy over others, as we intend to provide a flexible approach that can accommodate various quantifying methods. We believe many viable methods could be utilized effectively in this context. Our presentation of these methods aims to highlight the empirical observation that monitoring the extent of parameter rank fluctuations can indicate performance improvements at most rank fluctuation points. By showcasing these strategies, we suggest a direction for future research to quantify parameter changes to understand and optimize model performance. This part of our work offers a groundwork for further strategies to develop and refine.\n\n---\n\n>Typo \"raking\" --> \"ranking\" in section 5.4\n\nThank you for pointing out the typographical error. Your attention to detail is greatly appreciated, and we will ensure the correction is made in the revised manuscript.\n\n---\n\n>While the proposed metod focuses on the retrieval task, would PATHS method also applicable to other video-text tasks such as video captioning or video QA?\n\nWe thank the reviewer for brining this up; PATHS can be applied to other tasks, where we have tested for video QA in Section A.1 in the appendix. We regret that the appendix wasn't appended at the end of main draft, but uploaded separately through supplementary material.  We apologize for any inconvenience.\n\n---\n\nThank you for your time and consideration. \n\nBest regards,"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2484/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699851473678,
                "cdate": 1699851473678,
                "tmdate": 1699851473678,
                "mdate": 1699851473678,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L36Gw9zTIB",
                "forum": "QUhCObWGw5",
                "replyto": "SegimeK2xu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2484/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2484/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer tWWe,\n\nWe are grateful for your constructive feedback.\n\nWe would appreciate informing us whether your concerns have been adequately addressed.\n\nYour insights are highly valued, and we would like to let you know that we would happily answer any further questions if any.\n\n\nBest regards,\nThe Authors"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2484/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585423013,
                "cdate": 1700585423013,
                "tmdate": 1700585423013,
                "mdate": 1700585423013,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]