[
    {
        "title": "SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding"
    },
    {
        "review": {
            "id": "2nnSuwEoy2",
            "forum": "GKau1ekOtH",
            "replyto": "GKau1ekOtH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6056/Reviewer_Unxf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6056/Reviewer_Unxf"
            ],
            "content": {
                "summary": {
                    "value": "This work aims to unify CLIP and SAM \u2013 two powerful vision foundation models (VFMs) \u2013 to enable a single set of parameters that are capable of retraining the advantages of both VFMs. The authors treat such model merging as a continual learning problem, where, given a pretrained VFM, the knowledge of a second VFM is merged without forgetting the initial knowledge.\n\nThe proposed model, SAM-CLIP, assumes access to a small part of pretraining data or its surrogates to be replayed during the merging process. The SAM model is used as the base VFM during the distillation, where CLIP is regarded as the auxiliary VFM and its knowledge is distilled via a cosine distillation loss. To avoid the catastrophic forgetting of SAM\u2019s original capabilities, the authors propose a rehearsal-based multi-task distillation loss to gradually distill the external knowledge to the base VFM.\n\nThe resulting trained SAM-CLIP is able to perform zero-shot classification, image-text retrieval, instance segmentation, and semantic segmentation. Across several benchmark datasets, the authors show that SAM-CLIP can achieve state-of-the-art performance in a single-stage inference setup."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper endeavors to create a comprehensive model by merging pre-trained vision foundation models, aligning with the contemporary trends in computer vision.\n- The contributed SAM-CLIP stems from a continual learning perspective, which is intuitive. As a result, SAM-CLIP is capable of conducting multiple visual understanding tasks in a zero-shot manner."
                },
                "weaknesses": {
                    "value": "- A glaring omission in the paper is the technical detail surrounding the cross-VFM distillation. A deeper dive into the methodology, choices of operations, and potential effects of the framework is necessary.\n- The paper's structure and presentation could use refinement. The disproportionate emphasis on background and literature review, coupled with scant technical details, detracts from its overall coherence and depth.\n- Benchmarking SAM-CLIP against prior models, particularly those based on SAM, would offer a more rounded perspective on its performance and advantages."
                },
                "questions": {
                    "value": "- **Q1:** The efficiency of SAM-CLIP on edge devices is emphasized multiple times throughout the manuscript, particularly in the \u201cAbstract\u201d and \u201cIntroduction\u201d sections. However, the empirical evidence supporting SAM-CLIP's performance on such devices seems absent. Could the authors elucidate the specifics of the claim regarding SAM-CLIP\u2019s suitability for edge devices? The reviewer would like to know what the claim means by \u201capt for edge device applications\u201d.\n\n---\n\n- **Q2:** When assessing zero-shot semantic segmentation, SAM-CLIP is exclusively juxtaposed with CLIP-based models. How does SAM-CLIP fare when contrasted with SAM-centric models, notably Semantic-SAM [R1] and SEEM [R2]?\n\n---\n\n- **Q3:** The \u201cProposed Approach\u201d section might benefit from more detailed explanations regarding the design and implementation. In particular, how do you perform the joint training between head probing and multi-task distillation?  how is the balance between head probing and multi-task distillation maintained during joint training? What metrics or criteria guide the selection of appropriate stopping points for training?\n\n---\n\n- **Q4:** The \u201cBackground\u201d section contains a profusion of general literature introductions. A more succinct and discerning review that delves into comparative analyses would greatly enhance its value.\n\n---\n\n- **Q5:** Notable typos appeared in the current illustration of this paper, which should be revised accordingly. For example:\n  - Page 1, the last paragraph: there should be a space between \u201ctasks\u201d and \u201cFifty et al., 2021\u201d.\n  - Page 2, the first paragraph: \u201cconsuming massive amount \u2026\u201d should be \u201cconsuming a massive amount \u2026\u201d.\n  - Page 2, the first paragraph: \u201chow to access to \u2026\u201d should be how to access \u2026\u201d.\n  - Page 2, the second paragraph: \u201cgeneralization to diverse set of tasks\u201d should be \u201cgeneralization to diverse sets of tasks\u201d.\n  - Page 2, the third paragraph: \u201cwe allow access to small part of \u2026\u201d should be \u201cwe allow access to a small part of \u2026\u201d.\n  - Page 3, the first paragraph: \u201cWith compromise of a negligible drop \u2026\u201d should be \u201cWith a compromise of a negligible drop \u2026\u201d.\n  - Page 3, the second paragraph: \u201cenable additional zero-shot capabilities\u201d should be \u201cenabled additional zero-shot capabilities\u201d.\n  - Page 3, the second paragraph: \u201con-top of \u2026\u201d should be \u201con top of \u2026\u201d.\n  - Page 3, the third paragraph: \u201ca model, and training recipe \u2026\u201d should be \u201ca model, and a training recipe \u2026\u201d.\n  - Page 3, the third paragraph: \u201cand produce high-resolution segmentation mask\u201d should be \u201cand produces a high-resolution segmentation mask\u201d\n  - Page 3, the third paragraph: \u201cbut has not released \u2026\u201d should be \u201cbut have not released \u2026\u201d.\n  - Page 3, the fourth paragraph: \u201cThey show transfer of the same \u2026\u201d should be \u201cThey show the transfer of the same \u2026\u201d.\n  - Page 3, the fourth paragraph: \u201cand demonstrate transfer of different zero-shot capabilities\u201d should be \u201cand demonstrate the transfer of different zero-shot capabilities\u201d.\n  - Page 3, the fourth paragraph: \u201cas well as emergence of new zero-shot capability\u201d should be \u201cas well as the emergence of new zero-shot capability\u201d.\n  - Page 3, the fifth paragraph: \u201creferring to loss of previously learned knowledge due to \u2026\u201d should be \u201creferring to a loss of previously learned knowledge due to \u2026\u201d.\n  - Page 4, the third paragraph: \u201cto obtain segmentation mask\u201d should be \u201cto obtain segmentation masks\u201d.\n  - Page 4, the third paragraph: \u201cand many forward passes, make their deployment \u2026\u201d should be \u201cand many forward passes, making their deployment \u2026\u201d.\n  - Page 4, the fourth paragraph: \u201cthe optimization algorithm is exploring the parameter space \u2026\u201d should be \u201cthe optimization algorithm explores the parameter space \u2026\u201d.\n  - Page 5, the second paragraph: \u201cand inherits its \u2026\u201d should be \u201cand inherit its \u2026\u201d.\n  - Page 5, the sixth paragraph: \u201cwhich is the case of our experiment of \u2026\u201d should be \u201cwhich is the case in our experiment of \u2026\u201d.\n\n---\n\n- **Q6:** How does SAM-CLIP perform under out-of-distribution or data corruption cases?\n\n---\n\nReferences\n\n- [R1] F. Li, et al. \u201cSemantic-SAM: Segment and Recognize Anything at Any Granularity.\u201d arXiv preprint arXiv 2307.04767.\n\n- [R2] X. Zou, et al. \u201cSegment Everything Everywhere All at Once.\u201d arXiv preprint arXiv  2304.06718."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concern or only a minor ethics concern is observed."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6056/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698573416920,
            "cdate": 1698573416920,
            "tmdate": 1699636651705,
            "mdate": 1699636651705,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Pz5RIBQMPB",
                "forum": "GKau1ekOtH",
                "replyto": "2nnSuwEoy2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6056/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6056/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Unxf [Part 1]"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their constructive feedback and comments. Below are our itemized responses. We hope that our responses address all questions and concerns and clarify our contributions.\n\n1. [**Writing Suggestions**] We appreciate the reviewer's suggestions regarding the refinement of the writing and structure of the paper. These suggestions will be incorporated in the next revision.\n\n2. [**Comparison with SAM-based Models**] Our primary goal with SAM-CLIP is to integrate the abilities of CLIP into the SAM model to achieve enhanced semantic understanding. We demonstrate that SAM-CLIP exhibits superior zero-shot semantic segmentation performance compared to previous methods. To our knowledge, as of the submission date of this paper, the state-of-the-art zero-shot semantic segmentation models were all CLIP-based; thus, we did not have SAM-based models for comparison in this task. It is important to note that the Semantic-SAM and SEEM models, as mentioned by the reviewer, are trained on image datasets with semantic segmentation annotations. Models trained with semantic segmentation annotations are not considered *zero-shot* in the literature. Therefore, these SAM-based models cannot be compared for the zero-shot semantic segmentation task. We thank the reviewer for bringing Semantic-SAM and SEEM to our attention. Relevant discussion will be included in the revision.\n\n3. [**Suitability for Edge Devices**] \n   To run an application requiring both SAM and CLIP abilities, one would need to store and deploy both models on the device. However, by merging SAM and CLIP into a single ViT-B model, only one ViT model is required to complete tasks that previously needed two models. Moreover, for processing each image on the edge device, SAM-CLIP only requires one forward pass through the backbone, whereas using two separate models would need 2x forward passes (doubling the compute and memory footprint costs). Therefore, SAM-CLIP is advantageous in terms of storage, memory, and compute costs for edge devices.\n\n   Please note that we are not introducing a new architecture in this work, but proposing to merge existing ViTs into a single one. Hence, existing benchmarks on different devices are valid (e.g., the memory and runtime performance of SAM-CLIP on any device is identical to that of SAM with ViT-B on such a device).\n\n4. [**More Details**] \n   We thank the reviewer for suggesting elaborating more on the training technical details and distillation background review. These points will be addressed in the next revision.\n   In general, all our design choices (mentioned in Sections 3, 4.1, and Appendix A) are based on the trade-off between forgetting SAM\u2019s original capabilities and learning CLIP\u2019s capabilities. Instance Segmentation (SAM\u2019s capability), zero-shot classification (CLIP\u2019s capability), and zero-shot semantic segmentation (a new joint capability) are the metrics we used to guide our choices.\n\n   Our training process consists of two stages: i) CLIP-head probing and ii) joint training of the SAM-head, CLIP-head, and the ViT backbone using a multi-task distillation loss. In the first stage, only the CLIP-head is trainable, and it is trained using a single CLIP distillation loss (cosine distance between embeddings). At this stage, all image batches come from only $D_{CLIP}$. This stage does not involve early stopping (we train for 20 epochs). The motivation for this step is to have a warm start for the CLIP-head in the next stage where we also allow modifying the backbone, similar to [1].\n\n   In the second stage, the SAM-head and the ViT backbone become trainable, and we have a multi-task objective: CLIP Distillation (Eq. 1) and SAM self-distillation (Eq. 2). The balance between the losses is determined by the coefficient $\\lambda$, which we picked to optimize the aforementioned trade-off. Each training step for this stage is performed as follows:\n   - Pick a batch of 2048 images from $D_{CLIP}$, run the forward pass, and compute gradients backward from the CLIP Distillation loss (note that only parameters of the CLIP-head and ViT backbone will get gradients after this step).\n   - Pick a batch of 32 images from $D_{SAM}$, run the forward pass, and compute gradients backward from the SAM self-distillation loss (note that only parameters of the SAM-head and ViT backbone will get gradients after this step).\n   - Apply one optimization step (note that at this point, the parameters of the ViT backbone have accumulated gradients from the above two steps).\n\n   We early-stop after 16 epochs (out of a full training length of 20 epochs) as we observed more severe forgetting (as measured by instance segmentation performance on COCO) after the 16th epoch."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6056/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699858029926,
                "cdate": 1699858029926,
                "tmdate": 1699858029926,
                "mdate": 1699858029926,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IasEFlqTE1",
            "forum": "GKau1ekOtH",
            "replyto": "GKau1ekOtH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6056/Reviewer_a39d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6056/Reviewer_a39d"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes SAM-CLIP to build a unified model with both the strengths of SAM and CLIP.  SAM and CLIP is employed to share the same image encoder with two separate heads. Two phased are adopted during the KD process: 1) Head probing 2) Multi-task distillation. Also 40.8M images are used in the distillation process. The results are validated on zero-shot instance segmentation, semantic segmentation and classification benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper has a good motivation on merging two visual foundation models, i.e., SAM and CLIP, into a unified model, such that the distilled model can obtain both semantic and spatial understanding.\n\n2. The paper is well organized and easy to understand.\n\n3. The experiments in Figure 1 and the experiment section show the distilled model retains both good zero-shot ability from SAM and CLIP."
                },
                "weaknesses": {
                    "value": "1. When evaluating zero-shot semantic segmentation, as in Figure 3, the paper proposes a two-stage process to first using clip head for coarse masks predictions and taking it as input to SAM for refinement. Is the predicted masks by SegCLIP in Table 2 also refined by SAM? Can the authors also provide the zero-shot semantic segmentation without using geometric prompts?\n\n2. When evaluating zero-shot instance segmentation, the performance decrease on LVIS is not negligible. This suggests that the ability of SAM is decreasing after the distillation process. Can the authors also provide comparison to HQ-SAM on zero-shot instance segmentation with the same bounding box as prompt? HQ-SAM [a] is also designed for minimal forgetting and efficient tuning for SAM but without discussion in related works or results comparison. Also, the paper misses MobileSAM in the related work section, which also uses knowledge distillation.\n\n[a] Segment Anything in High Quality. NeurIPS, 2023.\n[b] Faster Segment Anything: Towards Lightweight SAM for Mobile Applications. arXiv:2306.14289.\n\n3. Since the paper mentions edge device applications in the abstract, what are the model size, speed and memory consumption of the proposed sam-clip comparing to SAM/CLIP?\n\n4. What is the influence of the dataset scale in Merged-41M, for example reducing images by half or further increasing the image number? How does the paper decide the respective data percentage for CLIP and SAM training? Also, how to decide the distillation loss value scales for the sam head and clip head, like 1:10?"
                },
                "questions": {
                    "value": "Can the method deal with the instance segmentation not using bbox as prompt but using the semantics from CLIP? Overall I am positive about this paper and willing to raise scores if my concerns in the weakness can be well addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6056/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698662957101,
            "cdate": 1698662957101,
            "tmdate": 1699636651604,
            "mdate": 1699636651604,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rYAbyeKzDJ",
                "forum": "GKau1ekOtH",
                "replyto": "IasEFlqTE1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6056/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6056/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer a39d"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their positive and constructive feedback. Below are our itemized responses. We hope that our responses address all questions and concerns and clarify our contributions.\n\n1. [**Clarification on Semantic Segmentation Results**] The performance of SAM-CLIP reported in Table 2 is obtained using **only the CLIP-head**, **without** any SAM-head refinement -- for a fair comparison, the input images to SAM-CLIP, similar to those in the baseline methods (shown in Table 2), are resized to 448px. \n\n    Figure 3 illustrates that combining the CLIP-head and SAM-head can further enhance performance, although the results in Table 2 do not utilize the SAM-head. The performance of SAM-CLIP with SAM-head enhancement is showcased in Table 5. For example, on the Pascal-VOC dataset, SegCLIP achieves 52.6 mIoU, while our SAM-CLIP, using only the CLIP-head without any refinement, attains 60.6 mIoU (as shown in Table 2). If both CLIP and SAM heads are combined, our SAM-CLIP's performance can further improve to 66.0 mIoU (as shown in Table 5).\n\n2. [**Difference from HQ-SAM**] SAM-CLIP's training integrates the abilities of CLIP into the SAM model, resulting in a multi-task zero-shot model. In contrast, HQ-SAM focuses on enhancing SAM's original ability and improves its instance segmentation by fine-tuning on high-quality instance segmentation datasets. It is important to note that SAM-CLIP has a completely different goal from HQ-SAM: we aim to make the SAM model multi-task, while HQ-SAM focuses on improving its original task. Consequently, HQ-SAM shows improvement in instance segmentation (e.g., COCO and LVIS), while ours experiences some performance drop, as our goal is not to enhance SAM's instance segmentation ability. We thank the reviewer for bringing the HQ-SAM and MobileSAM papers to our attention. We will include the related discussion in the next revision.\n\n3. [**Model Size**] We use the SAM ViT-B version as our base model architecture. After the merging process, a single ViT-B image encoder of SAM-CLIP can serve to perform original SAM tasks (prompt-based segmentation), CLIP tasks (prompt-based classification and retrieval), and new tasks of text-to-segmentation (i.e., zero-shot/text-prompted semantic segmentation). Note that all these tasks can be done with a single inference of ViT-B and require loading only one image encoder. We should note that for CLIP-related tasks, SAM-CLIP has an additional light-head (3 transformer layers) which adds 25% to the memory footprint compared to a stand-alone CLIP with ViT-B (12 transformer layers). Thanks for the suggestion. We will clarify this in the next revision.\n\n4. [**Dataset Choice and Loss Coefficient**] In selecting the dataset, we primarily followed the approach of EVA-CLIP[1], which also distilled a CLIP model, using CC3M, CC12M, and ImageNet-21k. We additionally included YFCC-15M, a subset of the 400M data used to train the original OpenAI\u2019s CLIP model.\n\n   The loss coefficient ratio of 1:10 for the CLIP and SAM heads was empirically determined from three options: 1:1, 1:10, 1:100. We found that 1:10 offers the best trade-off between mitigating the forgetting of SAM\u2019s ability and learning CLIP\u2019s ability. We will clarify this process in the next revision.\n\n5. [**Instance Segmentation Without Bounding Box as Prompt**] If we understand your question correctly, you are referring to zero-shot semantic segmentation. This task involves providing an image and a set of candidate object class names (e.g., \u201cdog\u201d, \u201ccat\u201d, \u201chuman\u201d) and asking the model to perform semantic segmentation. We have provided quantitative results for this task in Table 2. Please let us know if we have misunderstood your point.\n\n## References\n\n[1] Fang, Yuxin, et al. \"EVA: Exploring the limits of masked visual representation learning at scale.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6056/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699857315091,
                "cdate": 1699857315091,
                "tmdate": 1699857315091,
                "mdate": 1699857315091,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5Z10qnXVnq",
            "forum": "GKau1ekOtH",
            "replyto": "GKau1ekOtH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6056/Reviewer_K2ur"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6056/Reviewer_K2ur"
            ],
            "content": {
                "summary": {
                    "value": "This paper merges CLIP and SAM, the two foundation models, into a single one that assimilates both knowledge and expertise learned separately. Specifically, the technical contributions include a reasonable finetuning design and integration of the two distillation losses. The resulting model supports language-driven prompts and enjoys a high-quality segmentation result."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper presents a simple yet effective way to merge two foundation models into a single one, and it inherits both advantages and demonstrates a significant performance boost;\n2. The paper is well-organized, clearly written, and easy to follow;\n3. The resulting model is promising and helpful for future research."
                },
                "weaknesses": {
                    "value": "1. The resulting model inherits the zero-shot capability of CLIP, as demonstrated in Table 1-5. However, it seems that there is no evidence showing the resulting model does not suffer from catastrophic forgetting. Even though the segmentation performance is better than CLIP-head prediction, it still doesn't compare with the segmentation result of SAM and it is unclear how much performance is degraded compared to the original SAM. The demo in Figure 3 shows that the SAM-head refined output is still filled with some artifacts and seems to have a large performance gap with the original SAM.\n2. The proposed method is limited to the sizes of released SAM models. Since the vision encoder must be initialized SAM vision encoder, we cannot obtain a resulting model with an arbitrary size."
                },
                "questions": {
                    "value": "1. The authors should explain more clearly the performance gap with the original SAM in terms of segmentation quality.\n2. The authors should also give the output of the original SAM, with the same examples shown in Figure 3.\n3. The authors should discuss more limitations with the resulting model and the proposed method.\n\nIf the above concerns are addressed, I am willing to improve the rating."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6056/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6056/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6056/Reviewer_K2ur"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6056/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762151966,
            "cdate": 1698762151966,
            "tmdate": 1699636651470,
            "mdate": 1699636651470,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m4MHt3nhXY",
                "forum": "GKau1ekOtH",
                "replyto": "5Z10qnXVnq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6056/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6056/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer K2ur"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their positive and constructive feedback. Below are our itemized responses. We hope that our responses address all questions and concerns and clarify our contributions.\n\n1. [**Clarification on Catastrophic Forgetting**] We have conducted studies on the catastrophic forgetting of SAM-CLIP versus SAM, as shown in Table 1. The last two columns of Table 1 for COCO and LVIS demonstrate that SAM-CLIP experiences some mild performance degradation compared to SAM in instance segmentation. This indicates that while SAM-CLIP does exhibit some degree of forgetting, it is not catastrophic. To provide a more intuitive understanding of the segmentation quality of SAM versus SAM-CLIP, we present two image examples and their corresponding segmentation predictions by SAM and SAM-CLIP in Appendix A.1 of the newly updated manuscript. Please check out the current rebuttal revision of the paper [[PDF](https://openreview.net/pdf?id=GKau1ekOtH)].\n\n2. [**Model Size**] SAM releases three ViT versions of varying sizes: ViT-B(ase), ViT-L(arge), and ViT-H(uge), covering most ViT use cases. In the paper, our experiments were conducted only with ViT-B, but this method can be directly applied to ViT-L and ViT-H as well. The proposed approach can also be applied to subsequent works of SAM, such as MobileSAM [1], to obtain other model sizes for SAM-CLIP.\n\n3. [**Performance Gap with the Original SAM**] For a quantitative comparison with the original SAM, we have conducted _instance segmentation_ studies. We acknowledge that SAM-CLIP has a small performance gap compared to SAM in terms of instance segmentation ability, as shown in the last two columns of Table 1: (-0.3% and -1.8% mAP on COCO and LVIS datasets, respectively). However, compared to SAM, our SAM-CLIP has broader zero-shot capabilities (Fig 1) and representation quality (Fig 4). Furthermore, in the newly [revised manuscript](https://openreview.net/pdf?id=GKau1ekOtH), we provide two image examples in Fig. 5 of Appendix A.1 to qualitatively show that the instance segmentation outputs of SAM and SAM-CLIP have similar quality. \n\n4. [**Qualitative Comparison of Segmentation**] Thank you for the suggestion on qualitative segmentation comparison. In the newly uploaded [revision](https://openreview.net/pdf?id=GKau1ekOtH), we provide segmentation outputs of the original SAM given the same point prompts. One can see from Fig. 6 in Appendix A.1 that SAM tends to segment only a sub-part of the object class pinpointed by the point prompts, instead of segmenting the whole semantic object class. This visual comparison implies that the combination of CLIP and SAM abilities in our SAM-CLIP is quite crucial for (zero-shot) semantic segmentation.\n\n5. [**Discussion of More Limitations**] We will further discuss additional limitations of our trained model and the proposed method in the next revision, including the above discussion on model size.\n\n## References\n\n[1] Zhang, Chaoning, et al. \"Faster Segment Anything: Towards Lightweight SAM for Mobile Applications.\" arXiv preprint arXiv:2306.14289 (2023)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6056/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699856749605,
                "cdate": 1699856749605,
                "tmdate": 1699856749605,
                "mdate": 1699856749605,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QkRLdL1HDZ",
            "forum": "GKau1ekOtH",
            "replyto": "GKau1ekOtH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6056/Reviewer_Kk2t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6056/Reviewer_Kk2t"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a distillation paradigm to incorporate SAM and CLIP, combing their instance segmentation and semantic recognition capabilities. SAM-CLIP uses extensive pre-training data from original models and learns a unified encoder along with two task-specific heads. SAM-CLIP showcases good performance across zero-shot classification and segmentation tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation is reasonable to combine SAM and CLIP to infuse their own advantages.\n\n2. SAM-CLIP shows good performance on zero-shot semantic segmentation tasks.\n\n3. The writing is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The contribution is a little overclaimed as *'we introduce a simple recipe to efficiently merge VFMs into a unified model that assimilates their expertise.'*. I think this method is specifically designed for CLIP and SAM, and cannot be simply generalized to other VFMs.\n\n2. The cost of training SAM-CLIP is expensive. The training data includes many sources up to 41M. Considering CLIP and SAM have already cost large-scale pre-training resources, continually tuning them as SAM-CLIP is not cost-effective. Although SAM-CLIP achieves good results for semantic segmentation, it hurts the original performance of both SAM and CLIP. I think simply cascading SAM and CLIP in a training-free way (CLIP generates prompt by vision-language alignment and then SAM segments or SAM segments all objects and then CLIP classifies) can obtain even comparable results to SAM-CLIP, which is more practical in real-world applications."
                },
                "questions": {
                    "value": "SAM itself can also be prompted by texts (semantics), though not open-sourced. What's the advantage of SAM-CLIP compared to SAM with text prompt?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6056/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698910417200,
            "cdate": 1698910417200,
            "tmdate": 1699636651363,
            "mdate": 1699636651363,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8aegRw5pqb",
                "forum": "GKau1ekOtH",
                "replyto": "QkRLdL1HDZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6056/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6056/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Kk2t"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their constructive feedback and comments. Below, please find our itemized responses. We hope that our responses address all questions and concerns and clarify our contributions.\n\n1. [**Merging VFMs Other Than CLIP and SAM**] Our proposed merging technique, based on an embedding distillation loss and an unlabeled image dataset, can be utilized beyond CLIP and SAM with any image encoder. We will clarify in the next revision how one might use the same approach to merge other vision encoders.\n\n    For instance, one can merge DINOv2[1] with SAM (or SAM-CLIP) in the same pipeline, by substituting the CLIP teacher model with a DINOv2 model. In this paper, we focus on SAM and CLIP because they are both zero-shot models with different capabilities, and merging them provides a vision model with broader zero-shot abilities. In contrast, DINOv2 is a pre-trained image backbone (with strong representation learning ability) that does not operate in a zero-shot manner. However, merging with DINOv2 could enhance representation learning and potentially offer some benefits. For example, one can also merge CLIP and DINOv2 using our pipeline (e.g., using CLIP as the base model and then applying multi-task distillation with CLIP and DINOv2 as teacher models), and possibly obtain a merged model with stronger representation suitable for certain downstream tasks.\n\n2. [**Training and Runtime Cost of SAM-CLIP**] \n   We believe there is a misunderstanding regarding the training cost of SAM-CLIP. \n   SAM-CLIP is trained on a 41M image dataset, which is *20-30x smaller* than the training sets of SAM and CLIP. For instance, SAM is trained on SA-1B, and we use only a 5.7% subset from SA-1B for the SAM self-distillation of our model; the state-of-the-art CLIP model is trained on DataComp-1B (1.4B image-text pairs), and the data we use for CLIP distillation (40M unlabeled images) represents less than 3% of DataComp-1B. Hence, the training cost of SAM-CLIP is quite small compared with the training of SAM and CLIP.\n\n   Regarding deployment, it has been shown that composing SAM and CLIP for semantic segmentation is feasible by using SAM to generate all possible segmentation masks and then using CLIP to provide labels [3]. However, this approach requires loading two models simultaneously (*2x memory footprint*) and, for each image, needs a forward pass of the SAM backbone (under 1024 resolution) to generate K object segments, followed by a forward pass of the CLIP model for each segment to filter (overall K+1 passes).\n\n   With SAM-CLIP, only one ViT model needs to be loaded (lower memory footprint), and a single forward pass of the ViT backbone is required for each image. Overall, our method offers significant efficiency advantages over the model composition approach in terms of memory and computational costs during inference.\n\n   In the next revision, we will elaborate further on the computational costs of existing training-free approaches for zero-shot semantic segmentation.\n\n3. [**Comparison with SAM**]\n   We cannot comment on the performance of the SAM model trained with text (using a different pre-training loss than the public SAM) as it is unreleased. The SAM paper [2] refers to it as a \u201cPreliminary Exploration\u201d and does not provide any quantitative results (such as for zero-shot semantic segmentation as we do).\n\n   Regarding capabilities, SAM-CLIP can perform all zero-shot tasks of CLIP (zero-shot classification & image-text retrieval), which SAM (both the public and unreleased versions) cannot. Furthermore, we want to emphasize that our motivation is not solely to improve the SAM or CLIP model but to demonstrate the merging of two foundational models with different zero-shot capabilities into one, with SAM+CLIP serving as a demonstrative case.\n\n### References\n\n[1] Oquab, Maxime, et al. \"Dinov2: Learning robust visual features without supervision.\" arXiv preprint arXiv:2304.07193 (2023).\n\n[2] Kirillov, Alexander, et al. \"Segment anything.\" arXiv preprint arXiv:2304.02643 (2023).\n\n[3] \"Grounded Segment-Anything.\" https://github.com/IDEA-Research/Grounded-Segment-Anything (2023)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6056/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699853634428,
                "cdate": 1699853634428,
                "tmdate": 1699853634428,
                "mdate": 1699853634428,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]