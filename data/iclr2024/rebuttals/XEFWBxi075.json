[
    {
        "title": "GRANDE: Gradient-Based Decision Tree Ensembles"
    },
    {
        "review": {
            "id": "yYoUMo5vIN",
            "forum": "XEFWBxi075",
            "replyto": "XEFWBxi075",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission258/Reviewer_SMN1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission258/Reviewer_SMN1"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents GRANDE, a novel method for learning hard, axis-aligned decision tree ensembles using end-to-end gradient descent. The paper extends GradTree to a weighted tree ensemble, introduces softsign as a differentiable split function, and proposes a novel instance-wise weighting technique. The paper evaluates GRANDE on a predefined benchmark of 19 binary classification datasets and shows that it outperforms existing gradient-boosting and deep learning frameworks on most datasets The paper also demonstrates that GRANDE can learn simple and complex rules within a single model and provide local explanations based on instance-wise weights."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The transition from GradTree to a Tree ensemble is a natural and straightforward progression, and the reported performance is commendable.\n    \n2. The choice of an approximation function for the Heaviside step function is well-justified. The concept of instance-wise weighting is innovative and logically sound."
                },
                "weaknesses": {
                    "value": "My main concern is about the performance evaluation, please find the specific info in the Questions part."
                },
                "questions": {
                    "value": "1. Instead of using exclusively binary class benchmark datasets, could the authors also present comparative results for multi-label classification problems?\n    \n2. It would be valuable if the authors could include comparisons with other tree ensemble methods, such as random forest and extra trees.\n    \n3. Regarding the interpretability advantages of instance-wise weighting, could the authors provide a more comprehensive analysis? For instance, could they share statistics like the average node count of the highest-weighted estimators across all datasets? Furthermore, could the authors offer any theoretical insights into the benefits of instance-wise weighting?\n    \n4. Instance-wise weighting introduces a larger number of weights compared to estimator-wise weighting. Particularly when trees have greater depth (e.g., depth=10, 2^10 vs. 1 for one estimator), is there a significant impact on the training time due to the increased number of weights?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission258/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission258/Reviewer_SMN1",
                        "ICLR.cc/2024/Conference/Submission258/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission258/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698048401530,
            "cdate": 1698048401530,
            "tmdate": 1700763360631,
            "mdate": 1700763360631,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RT0iyItTHk",
                "forum": "XEFWBxi075",
                "replyto": "yYoUMo5vIN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission258/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission258/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (1 / 4)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and for providing us with promising suggestions!\n\n> Instead of using exclusively binary class benchmark datasets, could the authors also present comparative results for multi-label classification problems?\n\n**TL;DR: GRANDE outperforms existing methods on several multi-class datasets**\n\nIn our paper, we focused on binary classification tasks to present a comprehensive evaluation within the limited scope. Yet, we acknowledge the importance of demonstrating that our method is applicable beyond binary classification. Therefore, we conducted additional experiments on several openml multi-class tasks (we used the multi-class datasets from `[1]`) detailed below. We report the results both without HPO and with HPO (30 trials for all methods, constrained by the limited time available during the rebuttal period).\n\nSimilar to binary classification, GRANDE outperforms SOTA methods on several datasets for multi-class tasks without additional adjustments of our method. Accordingly, the results for multi-class classification are in line with the results on binary classification tasks presented in the paper. We also included the results in the appendix of our revised version.\n\nHPO (30 trials):\n|  | GRANDE | XGBoost | CatBoost | NODE |\n| -------- | -------- | -------- | -------- | -------- |\n| GesturePhaseSegmentationProcessed      |    0.455 | **0.643** |      0.638 |  0.247 |\n| artificial-characters                  |    0.726 | **0.905** |      0.867 |  0.614 |\n| audiology                              |    0.848 | 0.85  |      0.861 |  **0.89**  |\n| balance-scale                          |    **0.78**  | 0.681 |      0.776 |  0.616 |\n| cnae-9                                 |    0.94  | 0.919 |      0.939 |  **0.942** |\n| jungle_chess_2pcs_raw_endgame_complete |    **0.842** | 0.832 |      0.827 |  0.807 |\n| mfeat-fourier                          |    **0.859** | 0.831 |      0.847 |  0.859 |\n| mfeat-zernike                          |    **0.823** | 0.778 |      0.783 |  0.811 |\n| one-hundred-plants-texture             |    0.818 | 0.741 |      **0.852** |  0.753 |\n| splice                                 |    **0.964** | 0.957 |      0.947 |  0.96  |\n| vehicle                                |    0.817 | 0.809 |      **0.819** |  0.816 |\n| **Normalized Mean**                             |    **0.751** | 0.338 |      0.661 |  0.484 |\n| **Mean Reciprocal Rank (MRR)**                                         |    **0.674** | 0.432 |      0.508 |  0.47  |\n| **Count**                                       |    **5**     | 2     |      2     |  2     |\n\n\nDefault Parameters (summarized):\n\n|  | GRANDE | XGBoost  | CatBoost | NODE |\n| -------- | -------- | -------- | -------- | -------- |\n| Normalized Mean | **0.653**     | 0.324    | 0.523         | 0.425     |\n| Mean Reciprocal Rank (MRR)     | **0.674**     | 0.379    | 0.508         | 0.523     |\n| Wins |    **5**     | 1     | 2     | 3     |\n\n\n> It would be valuable if the authors could include comparisons with other tree ensemble methods, such as random forest and extra trees.\n\nWe included random forests and extra trees to our evaluation. The results are presented below. Due to lack of space, we were not able to adjust the results table in the main part of our paper, but included them to the Appendix C (Table 13-15). The results are consistent with the claims in our paper: GRANDE achieves superior results both with default parameters and after HPO.\n\nHPO (250 trials):\n|  | GRANDE | XGBoost  | CatBoost | NODE | ExtraTree | RandomForest |\n| -------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| Normalized Mean |    **0.817**     | 0.518     | 0.705     | 0.382     | 0.385     | 0.404     |\n| Mean Reciprocal Rank (MRR)     | **0.668**     | 0.365     | 0.541     | 0.352     | 0.251     | 0.275     |\n| Wins |    **9**     | 2     | 5     | 2     | 0     | 1     |\n\nDefault:\n|  | GRANDE | XGBoost  | CatBoost | NODE | ExtraTree | RandomForest |\n| -------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| Normalized Mean                |    **0.652**     | 0.607     | 0.587    | 0.352     | 0.380     | 0.471     |\n| Mean Reciprocal Rank (MRR)     | **0.595**     | 0.450     | 0.416     | 0.339     | 0.283     | 0.371     |\n| Wins                           |    **9**     | 3     | 2     | 2     | 0     | 3     |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission258/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723103779,
                "cdate": 1700723103779,
                "tmdate": 1700723131481,
                "mdate": 1700723131481,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WGJAQIPYrC",
                "forum": "XEFWBxi075",
                "replyto": "yYoUMo5vIN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission258/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission258/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (2 / 4)"
                    },
                    "comment": {
                        "value": "> Regarding the interpretability advantages of instance-wise weighting, could the authors provide a more comprehensive analysis? For instance, could they share statistics like the average node count of the highest-weighted estimators across all datasets? \n\n**TL;DR: We provided additional statistics (node count, estimator frequency, skewness, kurtosis, etc.) supporting the claims from our paper.**\n\n[Also see response to `JdQZ`]\n\nWithin our paper, we showed in an ablation study that our instance-wise weighting has a positive impact on the performance of GRANDE. In addition, we showcased a real-world scenario where the weighting significantly increases local interpretability by enabling the model to learn representations for simple and complex rules. We agree with the reviewer that a more comprehensive analysis of the weighting would be beneficial and therefore provide additional statistics. We summarize the main results below and provide detailed statistics for each dataset, along with an additional discussion in Appendix D.\n\nThe following statistics were obtained by calculating the post-softmax weights for each sample and averaging the values over all samples. In addition, we calculated the same statistics for the top 5% of samples with the highest weights. This provides additional insights as we argue that unique local interactions might only exist for a subset of the samples.\n\n\n| statistic | average value|\n| -------- | -------- |\n| **average internal node count of highest weighted estimator**     | 2.918 |\n| **average leaf node count of highest weighted estimator**     | 3.918  |\n| **fraction of samples for highest weighted estimator**    | 0.325  |\n| **fraction of samples for highest weighted estimator top 5% samples**     | 0.631  |\n| **number of highest weighted estimators**    | 44.221  |\n| **number of highest weighted estimators top 5% samples**    | 5.347  |\n\nWe can observe that on average, the highest weighted estimator comprises a small number of ~3 internal and ~4 leaf nodes, allowing for easy interpretation. Furthermore, on average, the highest weighted estimator is the same for ~33% of the data instances with a total of ~44 different estimators having the highest weight for at least one instance. If we further inspect the top 5% of instances with the highest weight for a single estimator, we can observe that the same estimator has the highest weight for ~63% of these instances and only ~5 different estimators achieve the highest weight for at least one instance. This indicates the presence of a small number of local experts for most datasets, having high weights for certain instances where the local rules can be applied.\n\n\nIn the subsequent analysis, we evaluate the distribution of the weights in more detail, focusing on skewness as a measure of distribution asymmetry and kurtosis as a measure of tailedness. In general, it stands out that both values increase significantly when considering only the top 5% of samples with the highest weight for a single estimator. This again indicates the presence of local expert estimators for a subset of the data, where unique local interactions were identified. As this is very dataset-specific, we will give a more detailed analysis in the following. The exact values for each dataset are summarized in the Appendix D.\n\n\n| statistic | average value|\n| -------- | -------- |\n| **skewness (absolute)**     | 0.933  |\n| **skewness top 5% (absolute)**     | 1.400  |\n| **(excess) kurtosis**     | 1.291  |\n| **(excess) kurtosis top 5%**     | 3.630  |"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission258/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723147657,
                "cdate": 1700723147657,
                "tmdate": 1700726398909,
                "mdate": 1700726398909,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LY0Tz31ZUq",
            "forum": "XEFWBxi075",
            "replyto": "XEFWBxi075",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission258/Reviewer_Tbch"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission258/Reviewer_Tbch"
            ],
            "content": {
                "summary": {
                    "value": "This article present \"GRANDE\" a gradient-based extension of GradTree to weighed ensembles of trees.\n\"GradTree: Learning Axis-Aligned Decision Trees with Gradient Descent\" is a (yet-unpuplished to the best of my knowledge) work by (Marton et al. 2023) that proposes to learn by gradient descent on a relaxation of decision trees.\nThis relaxation works by reformulating the decision tree as a sum of indicative functions (one for each leaf), and relaxing the decision thresholds with a \"straight-through\" (ST) operator that replaces the hard decision by a soft decision during the backward pass of gradient descent.\nThe article first summarizes GradTree, then introduces the specificities of GRANDE:\n- a novel variant soft-thresholding called \"softsign\" that is used with the straigh-through operator\n- a novel instance-wise weighing scheme for ensemble of trees\n\nThen a thorough experimental study is provided to evaluate the model and other models on several datasets. The appendix provides several experiments and ablation studies."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Dealing with tabular data, as efficiently as gradient-boosted trees do, though neural networks and gradient descent is yet an open challenge. For this very reason, proposing new, or even slightly new models that are able to train tree ensembles in a reasonable time through gradient descent is an interesting contribution.\n\n- The paper is clear, well written, and illustrated with several illustrating Figures. I liked reading it.\n- I could not manage to run the supplementary material code, but the provided experiments seems serious and convincing with several datasets and models tested and some ablation studies\n- This method, although very slow at training time when compared to gradient-boosting, seems promising for the many neural/tree hybridizations its suggests\n- The instance-based weighing is interesting (especially for explainability)"
                },
                "weaknesses": {
                    "value": "- My major concern is about hyper-parameters tuning (section C of appendix): I understand that compute resources should be spared, but it seems unfair to optimize the number of trees for GRANDE but not for XGBoost and CatBoost, especially given the fact that XGBoost and CatBoost are the cheapest algorithms to train.\n- The results of GRANDE are good on several datasets, but become less impressive when the number of features is high\n- The 2^d term in the sums suggests that the depth is a real limitation of the method\n- Fact is that gradient descent is much slower than greedy optimization\n- The related-works part seems a bit short given the huge literature on ensemble trees\n\nMinor Remarks:\npage 4 (section 3.3) : the W matrix is in R^{E\\times 2^d}, not R^E \\times R^{2^d}\nRegarding the code I tried unsuccessfully to run the notebooks: two packages were missing in the installation script: \"chardet\" and \"cchardet\", even with this that fixed there is a remaining issue with the \"TabSurvey\" sub-package which seems to require a specific environment to run (It fails on \"from models import\" statements).\npage 7:  the Phishing Website study should appear as a subsection"
                },
                "questions": {
                    "value": "- Your weighing scheme seems interesting but does it really require gradient descent ? Could you take a fixed forest, add weights to the leaves and optimize these weights afterwards ? I am thinking of a paper like (Cui et al. 2023) https://arxiv.org/pdf/2304.13761.pdf\n- Did you consider hybridizing greedy and gradient approaches ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission258/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698339339375,
            "cdate": 1698339339375,
            "tmdate": 1699635951400,
            "mdate": 1699635951400,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yntirxMZ19",
                "forum": "XEFWBxi075",
                "replyto": "LY0Tz31ZUq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission258/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission258/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (1 / 2)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's constructive feedback and further thank the reviewer for highlighting the strengths of the proposed method!\n\n> My major concern is about hyper-parameters tuning (section C of appendix): I understand that compute resources should be spared, but it seems unfair to optimize the number of trees for GRANDE but not for XGBoost and CatBoost, especially given the fact that XGBoost and CatBoost are the cheapest algorithms to train.\n\n**TL;DR: Tuning the number of estimators for XGBoost and CatBoost has only minor impact on the performance and the results remain unchanged.**\n\nWe understand the concerns. The decision to use early stopping instead of tuning the number of estimators was made in line with related work `[1]` to increase search efficiency (not only based on the runtime). Yet, we conducted an additional experiment where we also tuned the number of estimators. Overall, this did not significantly impact the performance of tree-based methods: The average performance slightly decreased when tuning the estimators for both XGBoost (by 0.0010) and CatBoost (by 0.0003). Consequently, for both methods, performance slightly increased in 10 out of 19 datasets and decreased in the remaining 9. In the following, we summarized the comparison of all methods when tuning the number of estimators. The complete results can be found in the appendix of the revised paper (Table 21 and Table 22). These additional results are consistent with the claims in our paper.\n\n\n\n|  | GRANDE | XGBoost | CatBoost | NODE |\n| -------- | -------- | -------- |-------- | -------- |\n| **Mean Reciprocal Rank (MRR) (new, with tuning estimators)**     | **0.684**     | 0.434     | 0.566     | 0.399     |\n| **Mean Reciprocal Rank (MRR) (old, without tuning estimators)**     | **0.702**     | 0.417     | 0.570     | 0.395     |\n| **Normalized Mean (new, with tuning estimators)**     | **0.773**     | 0.436     | 0.629     | 0.285     |\n| **Normalized Mean (old, without tuning estimators)**      | **0.776**     | 0.483     | 0.671     | 0.327     |\n| **Wins (new, with tuning estimators)** |    **9**     | 2     | 6     | 2     |\n| **Wins (old, without tuning estimators)** |    **9**     | 4     | 4     | 2     |\n\n\n> The results of GRANDE are good on several datasets, but become less impressive when the number of features is high\n\nIn general, our method excels in handling datasets with a small to medium number of features. For very high-dimensional datasets, learning GRANDE becomes more challenging (as the matrices $I \\in \\mathbb{R}^{2^d-1 \\times n}$ and $T \\in \\mathbb{R}^{2^d-1 \\times n}$ scale with the number of features). This challenge is addressed by using a feature subset for each estimator, supported by our instance-wise weighting, which allows assigning higher weights to estimators focusing on more important features. Although GRANDE showed comparatively modest results on the *madelon* dataset (500 features) and the *Bioresponse* dataset (1,776 features), it outperformed other methods on the SpeedDating dataset (256 features).\n\n> The 2^d term in the sums suggests that the depth is a real limitation of the method.\n\nTheoretically, depth is a limitation of the method. However, in practice, GRANDE can be efficiently trained up to a depth of 10. Preliminary experiments indicated that a depth of 6 is sufficient to achieve SOTA performance, as increasing the depth further does not significantly impact performance."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission258/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722956357,
                "cdate": 1700722956357,
                "tmdate": 1700722956357,
                "mdate": 1700722956357,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2RHyFdVHRR",
            "forum": "XEFWBxi075",
            "replyto": "XEFWBxi075",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission258/Reviewer_JdQZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission258/Reviewer_JdQZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel approach for learning hard, axis-aligned decision tree ensembles using end-to-end gradient descent based on the straight-through operator. The core contributions are two folds: 1) using an alternative differentiable split function (softsign); 2) introducing an advanced instance-wise weighting mechanism for tree ensemble. Experiments on tabular benchmark show that this method outperforms existing gradient-boosting and deep learning frameworks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This is one of the few deep learning based works which beat XGB on tabular benchmark. \n2. The contributions (alternative differentiable split function and instance-wise weighting) are supported by ablation experiments.\n3. It provides all the hyperparameters in appendix, which helps reproduction."
                },
                "weaknesses": {
                    "value": "1. This paper lacks further analysis for instance-wise weighting. Because the final results are weighted by Softmax, the prediction of each tree is not separate now. If we cut off one tree, the contributions of the other tree are also changed. This is different from XGB and NODE, but the authors did not point out it. Moreover, it is better to analysis the distribution of instance weights. For example:\n\na) Is it long-tailed?\n\nb) Are some trees very important for most of the samples?\n\n2. Too many hyperparameters, i.e. 11 are tuned for the proposed method. While the number of hyperparameters is no more than 4 for compared methods. We have reason to doubt that the performance gain is obtained by finer hyperparameter tuning. Could you reduce your number of hyperparameters to 4 and retrain your model for comparison?"
                },
                "questions": {
                    "value": "See \"Weaknesses\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission258/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission258/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission258/Reviewer_JdQZ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission258/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770008857,
            "cdate": 1698770008857,
            "tmdate": 1699635951299,
            "mdate": 1699635951299,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gJFioLVJwU",
                "forum": "XEFWBxi075",
                "replyto": "2RHyFdVHRR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission258/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission258/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (1 / 3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and providing us with valuable feedback!\n\n> Because the final results are weighted by Softmax, the prediction of each tree is not separate now. If we cut off one tree, the contributions of the other tree are also changed. This is different from XGB and NODE, but the authors did not point out it.\n\nThank you for pointing this out. Indeed, when employing a softmax function to calculate the weights for GRANDE, the individual tree predictions are not separated. Therefore, the contributions of individual trees change. This occurs not only in scenarios such as removing one tree from the ensemble but also during instance-wise weighting: Assume we have two data instances that are very similar and the prediction of our ensemble differs only in one tree. For this particular tree, depending on the different paths taken by the samples, we select two distinct logits. Consequently, when calculating the softmax to generate the weights, the contribution of each tree changes.\n\nThis aspect is crucial in enabling the learning of \"local experts\" within GRANDE. If the path capturing a unique local interaction is traversed by a sample, the corresponding tree should have a very high weight, resulting in reduced weights for the remaining trees.\n\nWe acknowledge that this aspect was not adequately addressed in the current version of the paper, and have therefore provided clarification in the revised version (Section 3.3).\n\n> Moreover, it is better to analysis the distribution of instance weights. \nFor example:\na) Is it long-tailed?\nb) Are some trees very important for most of the samples?\n\n**TL;DR: (a) In most cases, the distributions are long-tailed (and heavy-tailed). (b) On average, for 38% of the samples, the same tree is the most important one, but this is very dataset- and instance-specific (see discussion below).**\n\n[Also see response to `SMN1`]\n\nWithin our paper, we showed in an ablation study that our instance-wise weighting has a positive impact on the performance of GRANDE. In addition, we showcased a real-world scenario where the weighting significantly increases local interpretability by enabling the model to learn representations for simple and complex rules. We agree with the reviewer that a more comprehensive analysis of the weighting would be beneficial and therefore provide additional statistics. We summarize the main results below and provide detailed statistics for each dataset, along with an additional discussion in Appendix D.\n\nThe following statistics were obtained by calculating the post-softmax weights for each sample and averaging the values over all samples. In addition, we calculated the same statistics for the top 5% of samples with the highest weights. This provides additional insights as we argue that unique local interactions might only exist for a subset of the samples.\n\n\n| statistic | average value|\n| -------- | -------- |\n| **average internal node count of highest weighted estimator**     | 2.918 |\n| **average leaf node count of highest weighted estimator**     | 3.918  |\n| **fraction of samples for highest weighted estimator**    | 0.325  |\n| **fraction of samples for highest weighted estimator top 5% samples**     | 0.631  |\n| **number of highest weighted estimators**    | 44.221  |\n| **number of highest weighted estimators top 5% samples**    | 5.347  |\n\nWe can observe that on average, the highest weighted estimator comprises a small number of ~3 internal and ~4 leaf nodes, allowing for easy interpretation. Furthermore, on average, the highest weighted estimator is the same for ~33% of the data instances with a total of ~44 different estimators having the highest weight for at least one instance. If we further inspect the top 5% of instances with the highest weight for a single estimator, we can observe that the same estimator has the highest weight for ~63% of these instances and only ~5 different estimators achieve the highest weight for at least one instance. This indicates the presence of a small number of local experts for most datasets, having high weights for certain instances where the local rules can be applied."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission258/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722644002,
                "cdate": 1700722644002,
                "tmdate": 1700722728525,
                "mdate": 1700722728525,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dJ1RWZmCfa",
                "forum": "XEFWBxi075",
                "replyto": "2RHyFdVHRR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission258/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission258/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (3 / 3)"
                    },
                    "comment": {
                        "value": "> Too many hyperparameters, i.e. 11 are tuned for the proposed method. While the number of hyperparameters is no more than 4 for compared methods. We have reason to doubt that the performance gain is obtained by finer hyperparameter tuning. Could you reduce your number of hyperparameters to 4 and retrain your model for comparison?\n\n**TL,DR: When reducing the number of hyperparameters or the number of trials, GRANDE still achieves SOTA performance**\n\nWe understand the concerns regarding the number of hyperparameters. We already tried to account for this with the default hyperparameter experiment included in the original version of the paper (Table 3 and Table 8), demonstrating that GRANDE achieved superior results even without hyperparameter optimization. Acknowledging the importance of this topic, we have included two additional experiments focusing on hyperparameter optimization:\n1. **Reduced number of trials to 30:** We reduced the number of trials in the HPO from originally 250 trials to 30 for all methods (similar to `[1]`) to compare the performance with a less extensive search. We can observe that GRANDE has the greatest benefit when increasing the number of trials. Yet, even with only 30 trials, GRANDE achieves the highest mean reciprocal rank (MRR) and normalized mean, as well as the most wins. The detailed results on all datasets along with a discussion are included in the appendix of the revised paper version and summarized in the following table:\n\n    |  | GRANDE | XGBoost  | CatBoost | NODE |\n    | -------- | -------- | -------- | -------- | -------- |\n    | Normalized Mean                           |    **0.694** | 0.430 |      0.676  |  0.336 |\n    | Mean Reciprocal Rank (MRR)                                       |    **0.636**  | 0.434 |      0.579 |  0.434 |\n    | Count                                     |    **8**     | 3     |      5     |  3     |\n\n\n2. **Reduce grid for GRANDE to 4 parameters:** Following the reviewer's suggestion, we reduced the number of hyperparameters for GRANDE to four (one learning_rate, dropout, loss, and cosine_decay) and conducted an additional HPO with 250 trials. Unfortunately, we could not complete the search for all datasets during the rebuttal period (*nomao* and *adult* datasets are pending and will be included in the camera-ready version). Reducing the grid results in a slightly worse performance for GRANDE. Especially, optimizing separate learning rates for different components (split values, split indices, leaf probabilities, and leaf weights) using a larger grid did further enhances GRANDE's performance. Yet, the results are in line with the claims from the paper and GRANDE still has the highest normalized mean, MRR and number of wins.\n\n\n    |  | GRANDE | XGBoost | CatBoost | NODE |\n    | -------- | -------- | -------- | -------- | -------- |\n    | Normalized Mean                           |    **0.736** | 0.428 |      0.710 |  0.364 |\n    | Mean Reciprocal Rank (MRR)                                       |    **0.652** | 0.358 |      0.608 |  0.466 |\n    | Count                                     |    **7**     | 0     |      6     |  4     |\n\n\n`[1]` McElfresh, Duncan C., et al. \u2018When Do Neural Nets Outperform Boosted Trees on Tabular Data?\u2019 Thirty-Seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023, https://openreview.net/forum?id=CjVdXey4zT."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission258/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722704585,
                "cdate": 1700722704585,
                "tmdate": 1700722746828,
                "mdate": 1700722746828,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jEJBBZy0Tg",
            "forum": "XEFWBxi075",
            "replyto": "XEFWBxi075",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission258/Reviewer_Duhp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission258/Reviewer_Duhp"
            ],
            "content": {
                "summary": {
                    "value": "The paper extends the GradTree support of Marton to weighted ensembles of trees, and using a different splitting function based on the soft-sign.\nThe paper compares their results to XGBoost, CatBoost and NODE, and show that they outperform these based on average rank of F1 score."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper gives a detailed and clear description of the approach. The experimental evaluation and the evaluation protocol are well-defined and sound, and the results look promising."
                },
                "weaknesses": {
                    "value": "Given the popularity of gradient-based tree models in recent years, I feel like a more thorough comparison with competing methods would be warranted. In particular relating this work to work on learning weighting for fixed tree structures would be interesting, as first discussed in \"Practical Lessons from Predicting Clicks on Ads at Facebook\" by He et.al.\n\"Deep Neural Decision Trees\" by Yang et al also seems relevant, as well as \"Deep Neural Decision Forests\" by Kontschieder et al, \"SDTR: Soft Decision Tree Regressor for Tabular Data\" by Luo and \". The tree ensemble layer: Differentiability meets conditional computation.\" by Hazimeh et al.\n\n\n\"WindTunnel: Towards Differentiable ML Pipelines Beyond a Single Model\" also seems closely related, though they only fine-tune existing tree models with gradient descent.\n\nI find it also somewhat confusing that it is claimed that gradient boosted models are the defacto state-of-the-art on tabular data, when there has been a lot of recent work on neural networks for tabular data, often outperforming gradient boosting models, see \"Well-tuned Simple Nets Excel on Tabular Datasets\" by Kadra for example. It would be great to include at least one other neural approach in the comparison.\n\n\nFor the evaluation, I would have expected performance profiles or critical difference diagrams based on AUC or AP, instead of the mean and rank (or in addition to it, though the unnormalized mean is not a good way to aggregate performance across datasets).\n\nOverall, I find the contribution beyond Borisov somewhat slim, though that work is not published at a conference. If these are the same authors, I would recommend combining the two into a single submission.\n\u00a0\nMinor:\nThe cylinder-bands dataset is called out, it would be interesting to see if this is the version with target leakage via the job_number column, or without that column.\nThe \"straight-through operator\" is simply the subgradient of the max, and I think the standard reference for that is \"Evaluation of pooling operations in convolutional architectures for object recognition\" by Scherer et.al."
                },
                "questions": {
                    "value": "How do the results in Table 4 compare to using the hard split function?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission258/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698967254558,
            "cdate": 1698967254558,
            "tmdate": 1699635951229,
            "mdate": 1699635951229,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QpMJqpOWtt",
                "forum": "XEFWBxi075",
                "replyto": "jEJBBZy0Tg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission258/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission258/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (Part 1 / 3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and valuable feedback!\n\n> Given the popularity of gradient-based tree models in recent years, I feel like a more thorough comparison with competing methods would be warranted. In particular relating this work to work on learning weighting for fixed tree structures would be interesting, as first discussed in \"Practical Lessons from Predicting Clicks on Ads at Facebook\" by He et.al. \"Deep Neural Decision Trees\" by Yang et al also seems relevant, as well as \"Deep Neural Decision Forests\" by Kontschieder et al, \"SDTR: Soft Decision Tree Regressor for Tabular Data\" by Luo and \". The tree ensemble layer: Differentiability meets conditional computation.\" by Hazimeh et al.\n\nWe are aware that due to lack of space, the related work section was comparatively short and did not cover all related, gradient-based tree methods in detail. We adjusted the related work section (and the section on instance-wise weighting) to cover the mentioned works and provide a more detailed differentiation of our approach as follows:\n* **\"Practical Lessons from Predicting Clicks on Ads at Facebook\"**: They use GBDTs as feature transformers to generate categorical input features for a sparse linear classifier which can be seen as learning a weighting for the GBDT. There are two main differences to our work (now discussed in Section 3.3):\n    1. Instead of learning a post-hoc weighting, we incorporate the weighting into the training procedure.\n    2. We learn instance-wise weights that are calculated individually for each instance based on the selected paths in the ensemble.\n\n    These two aspects allow learning unique local interactions (\"local experts\") during the training of our model as showcased in the case study (Section 4.3).\n\n* **\"Deep Neural Decision Trees\"**: DNDTs in contrast to GRANDE uses soft trees. However, similar to our trees, they use axis-aligned splits which differentiates DNDTs to most hierarchical, gradient-based methods. The GradTree paper `[6]` also discussed the similarities and differences and includes an empirical comparison. During the empirical comparison, they showed that GradTree (the tree structure we adopted) significantly outperformed DNDTs. In the following we will shortly summarize the differences; a more detailed discussion can also be found in `[6]`:\n    1. As already mentioned, the trees are soft. \n    2. The use of the Kronecker product for building the tree with comes with two major limitations:\n        * The tree size depends on the number of features and therefore scales poorly for large datasets (only a maximum number of 12 features per tree is possible according to the authors).\n        * The use of the Kronecker product prevents splitting on the same feature multiple times within one tree which is often important.\n\n\nThe following methods have in common that they learn soft, oblique trees. We will first give a short summary of the method and then discuss their differences to GRANDE:\n\n* **\"Deep Neural Decision Forests\"**: NDFs use a stochastic routing (soft splits) with oblique splits with the goal of guiding representation learning in the lower layers of a CNN.\n* **\"SDTR: Soft Decision Tree Regressor for Tabular Data\"**: SDTR tries to imitate binary decision trees by a neural model and therefore optimizes soft decision trees (SDTs) e.g. using additional regularization. Their model outperforms non-interpretable MLP structures, but only achieved comparable (slightly worse) performance to non-gradient-based methods (i.e., CatBoost and XGBoost outperformed SDTR).\n* **\"The tree ensemble layer: Differentiability meets conditional computation.\"**: Similarly, they use a stochastic routing and oblique splits with the goal of designing a differentiable layer that can be incorporated in a DL framework.\n\nAll the previously mentioned works have a hierarchical tree ensemble structure, achieving differentiability by employing soft and oblique split functions. Therefore, they are very similar to NODE which we benchmarked against in our evaluation as it is according to recent surveys [2,3] the superior hierarchical tree ensemble method. The main difference in the tree structure between existing work and GRANDE is that we employ the ST-operator to maintain hard, axis-aligned splits which provides a beneficial inductive bias (see discussion in Section 3.2 and Section 5)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission258/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722201749,
                "cdate": 1700722201749,
                "tmdate": 1700722362113,
                "mdate": 1700722362113,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9ezQ4WhxEe",
                "forum": "XEFWBxi075",
                "replyto": "jEJBBZy0Tg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission258/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission258/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (Part 2 / 3)"
                    },
                    "comment": {
                        "value": ">  \"WindTunnel: Towards Differentiable ML Pipelines Beyond a Single Model\" also seems closely related, though they only fine-tune existing tree models with gradient descent.\n\nWindTunnel tries to bring the advantage of a joint, global optimization inherent in DL frameworks to a complete ML pipeline. For GBDTs in the pipeline, this is achieved by smoothing the non-differentiable operations. Thereby, the beneficial inductive bias of axis-aligned splits, which is maintained using GRANDE, is lost. However, we believe that it is an interesting direction for future research to integrate GRANDE in a differentiable pipeline (as proposed by WindTunnel) as this would maintain the inductive bias of axis-aligned splits while still allowing an optimization of the pipeline with gradient descent.\n\n>  I find it also somewhat confusing that it is claimed that gradient boosted models are the defacto state-of-the-art on tabular data, when there has been a lot of recent work on neural networks for tabular data, often outperforming gradient boosting models, see \"Well-tuned Simple Nets Excel on Tabular Datasets\" by Kadra for example. It would be great to include at least one other neural approach in the comparison.\n\n**TL;DR: We included SAINT as additional gradient-based benchmark and GRANDE demonstrates superior results.**\n\nWe are aware that there also exist papers highlighting the strengths of DL methods on tabular data, like the mentioned paper by Kadra et al. However, it is important to note that their evaluation primarily focused on balanced datasets`[1]`, reporting the accuracy and did not consider advanced encoding techniques for GBDT methods. Furthermore, their experiments included an extensive HPO with up to 4 days of time limit for each dataset `[1]`.\nYet, several recent papers still claim that, even though the gap is diminishing, GBDT are still considered SOTA `[2,3,4]`. At NeurIPS2023 there is an additional survey paper by McElfresh, Duncan C., et al.`[5]` accepted that confirms superior results of GBDT methods over a total of 176 datasets (even though they highlight that the gap to gradient-based methods is diminishing). Based on your feedback, we included Kadra et al. `[1]` to the related work section of our revised paper.\n\nIn summary, we agree that an additional, gradient-based benchmark would be beneficial and therefore added SAINT to our evaluation (which is according to `[2]` the superior gradient-based method). The new results are summarized in the following and are also included in the revised paper version (in the appendix due to lack of space in the main part). We provide the results with default parameters and after HPO. Due to the high computational demands of SAINT, we limited the HPO to 30 trials for all methods, which is in line the experiments in `[5]`. Furthermore, SAINT resulted in a OOM error (on an RTX A6000 with 47.55GB VRAM) for *Bioresponse* (1,776 features) which is why we excluded this dataset. The results are summarized in the following and detailed results for each datasets can be found in the Appendix C.\n\nSAINT achieves competitive results on many datasets (best method for one datasets), but also exhibits very poor performance on some datasets (e.g. for *madelon* only the majority class is predicted). As a result, the claims from our paper remain unchanged and GRANDE achieved superior results on most datasets, significantly outperforming existing gradient-based methods.\n\n\nHPO (30 trials):\n\n|  | GRANDE | XGBoost | CatBoost | NODE | SAINT |\n| -------- | -------- | -------- | -------- | -------- | -------- |\n| Normalized Mean                           |    **0.749** | 0.582 |      0.740 |  0.499 |   0.200 |\n| Mean Reciprocal Rank (MRR)                                       |    **0.638** | 0.391 |      0.531 |  0.427 |   0.296 |\n| Count                                     |    **8**     | 2     |      4     |  3     |   1     |\n\nDefault Parameters:\n\n|  | GRANDE | XGBoost | CatBoost | NODE | SAINT |\n| -------- | -------- | -------- | -------- | -------- | -------- |\n| Normalized Mean                           |    0.701 | **0.718** |      0.674 |  0.449 |   0.228 |\n| Mean Reciprocal Rank (MRR)                                       |    **0.631** | 0.495 |      0.442 |  0.382 |   0.333 |\n| Count                                     |    **9**     | 3     |      2     |  2     |   2 |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission258/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722329293,
                "cdate": 1700722329293,
                "tmdate": 1700722442443,
                "mdate": 1700722442443,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]