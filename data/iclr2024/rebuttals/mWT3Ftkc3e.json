[
    {
        "title": "Sampling is as easy as keeping the consistency: convergence guarantee for Consistency Models"
    },
    {
        "review": {
            "id": "MIV8SocCpZ",
            "forum": "mWT3Ftkc3e",
            "replyto": "mWT3Ftkc3e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1569/Reviewer_CiGA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1569/Reviewer_CiGA"
            ],
            "content": {
                "summary": {
                    "value": "This work provides convergence guarantees for the consistency models by Song et al. 2023, which is a one-step generative model achieving state-of-the-art results. The main assumptions are the Lipschitzness of the score function, the score estimation error, and the consistency error. Presented results include consistency mapping error, Wasserstein-2 distance, and TV distance between the mapping and the true probability flow. Multistep consistency sampling is also analyzed to show an improved convergence guarantee compared to the one-step alternative."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The writing is clear and generally good, minus a few typos.\n* To my knowledge, this is the first convergence guarantee result for the consistency model, and it is a valuable effort.\n* Assumptions on the data distribution are weak."
                },
                "weaknesses": {
                    "value": "* While the assumptions on the data distribution are weak, this work assumes the score estimation error and consistency error are low (Assumption 3, 4), which are major assumptions that are conditioned on the success of the optimization procedure. While it is believable that most usual training procedures can result in low score estimation error since the loss is basically an MSE, it is much harder to reason about the training procedure for the consistency model (8). It is good that the authors acknowledge this point in multiple places.\n* Assumption 6 seems unmotivated: the authors attributed it to \"technical reason\", without further explanation.\n* The presentation of the latter results in Section 3.5 seems messy. It looks like the clarity can be improved by using better notations, as many terms are the same."
                },
                "questions": {
                    "value": "* How is $\\theta^-$ defined? Is it a moving average of past $\\theta$'s?\n* Below Assumption 2, it says this paper does not assume convexity or dissipativity on $-\\log p$, unlike previous works. What is the reason that the analysis presented here does not require such assumptions?\n* What is the significance of obtaining a TV bound, compared to a Wasserstein-2 bound? The result seems a lot more messy compared to the ones concerning W2 errors.\n* Minor comments:\n  * Some of the $d$'s in (1)(2) are italicized when they shouldn't be\n  * In (5), is $dt$ a multiplication of $d$ and $t$?\n  * Below (7), \"such a mapping exists ..., and is smoothly relied on\" What's smoothly relied? And what exactly are the mild conditions mentioned here?\n  * Below Assumption 5, \"technique reason\" -> \"technical reason\"\n  * What does the notation $p P_{OU}^s$ mean, for a Markov kernel $P_{OU}^s$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1569/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698376588197,
            "cdate": 1698376588197,
            "tmdate": 1699636085367,
            "mdate": 1699636085367,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GLGkEW7M1H",
                "forum": "mWT3Ftkc3e",
                "replyto": "MIV8SocCpZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1569/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank the reviewer for the constructive comments.\n>While the assumptions on the data distribution are weak, this work assumes the score estimation error and consistency error are low (Assumption 3, 4), which are major assumptions that are conditioned on the success of the optimization procedure. While it is believable that most usual training procedures can result in low score estimation error since the loss is basically an MSE, it is much harder to reason about the training procedure for the consistency model (8). It is good that the authors acknowledge this point in multiple places.\n\nThanks for the reviewer to point out our deficiency. In the training procedure for the consistency model (8), the parameter $\\theta^-$ is a EMA (exponential moving average) to the past $\\theta$'s, thus when the trainning converges, we have $\\theta^- = \\text{stopgrad}(\\theta)$, and the Consistency Distillation object will then become a true loss function. If in addition it is small enough (together with good score estimator), which corresponding to assumption 4, our theoretical results shows that the consistency model $f_\\theta$ is a good approximator to the true consistency function $f^{ex}$. Besides, Song et al. (2023) also mentioned that, although simply setting $\\theta^- = \\theta$ will make the Consistency Distillation object be the true loss, the EMA update and \"stopgrad\" operator can greatly stablize the training procedure and improve the performance.\n\n> Assumption 6 seems unmotivated: the authors attributed it to \"technical reason\", without further explanation.\n\nSorry for our unclear expression. Indeed the special time scheduling was firstly suggested by Sitian Chen, *The probability flow ODE is provably fast*, second paragraph in section 3.2 Algorithm. The reason to choose this time scheduling is to control one term of error in Theorem 2: in equation (26), we get the upper bound with a term $\\sum_{k=1}^N \\frac{h_k^2}{ t_k^{1/2}}$. If we only take a naive time scheduling $h_k \\equiv h$, this term now becomes $O(\\frac{h^2}{\\sqrt{\\delta}})$, as $\\frac{1}{t_k^{1/2}}$ will become larger when $k \\to 0$. One would better choose a relatively smaller discretization step when $t_k$ is small to prevent the discretization error from being unnessary large. Thus a geometrically increasing step size will help a lot.\n\n> The presentation of the latter results in Section 3.5 seems messy. It looks like the clarity can be improved by using better notations, as many terms are the same.\n\nThanks for your suggestion. We will use better notations to simplify our expression, thus make it more readable.\n\n> How is $\\theta^-$ defined? Is it a moving average of past $\\theta$'s?\n\nYes, it is defined as $\\theta^- = \\text{stopgrad}(\\mu\\theta^- + (1-\\mu) \\theta)$. We sorry for putting the definition a little bit later than where it should be, we will modify our expression.\n\n> Below Assumption 2, ... What is the reason that the analysis presented here does not require such assumptions?\n\nIn the begining of the theoretical researchs of Diffusion Models, not so many properties of the forward diffusion process, as well as its probability density $p_t(x)$ have been found in Stochastic, PDE anslyses. Thus they need to assume convexity or dissipativity on $\\log p_0(x)$ to get prior estimations similar to our Lemma 7, Lemma 11, etc. On the other way, informally the Diffusion Models can be viewed as a Simulated Annealing process, which has better ability to bypass the difficulty from non-convexity or non-dissipativity comparing to traditional Langevin Dynamics.\n\n> What is the significance of obtaining a TV bound, compared to a Wasserstein-2 bound?\nLet the output distribution be $\\mu(dx)$. A guarantee of TV error bound can reflect the continuity of its density, thus can further support other operations over Likelihood. However if only have a $W_2$ bound, the output distribution density may be discontinuous, and have some points with infinity density (like Dirac measure).\n\n> In (5), is $dt$ a multiplication of $d$ and $t$?\n\nYes, it's from the change-of-variable formula for the mapping $x \\mapsto e^{-t} x$, where $x$ is a $d-$ dimensional vector.\n\n> Below (7)... What's smoothly relied? And what exactly are the mild conditions mentioned here?\n\nThis can be found in many textbooks on Ordinary Differential Equations. A simple case is that when $v$ is differentiable and bounded, then the solution of this ODE uniquely exists, and the mapping through ODE path is differentiable.\n\n> What does the notation $p P^s_{OU}$ mean, for a Markov kernel $P^s_{OU}$?\n\nThis is a density defined as follows:\n\nLet $\\xi \\sim p$, and let the $P^s_{OU}$-corresponded Markov process be started at $\\xi$. This Markov process generate another random variable $\\xi'$, then we denote the density of $\\xi'$ as $p P^s_{OU}$.\n\n> Other Minor comments:\n\nThanks for the correction and we have fixed the typos."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700017804104,
                "cdate": 1700017804104,
                "tmdate": 1700017804104,
                "mdate": 1700017804104,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eAjXrSs7Wp",
                "forum": "mWT3Ftkc3e",
                "replyto": "MIV8SocCpZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1569/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We kindly ask the reviewer if they have any outstanding questions or clarifications regarding our paper. We are happy to engage in a dialogue and conduct any additional requested work in the remaining discussion period. Thank you!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700097764897,
                "cdate": 1700097764897,
                "tmdate": 1700097764897,
                "mdate": 1700097764897,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pyuFGXGPGJ",
                "forum": "mWT3Ftkc3e",
                "replyto": "c399f5tEn2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1569/Reviewer_CiGA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1569/Reviewer_CiGA"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "Thank you for the detailed reply. It has addressed many of my questions. I would like to keep my current score as I agree with other reviewers that the novelty of the analysis can be limited."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700345465453,
                "cdate": 1700345465453,
                "tmdate": 1700345465453,
                "mdate": 1700345465453,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UqEMZCYZJy",
            "forum": "mWT3Ftkc3e",
            "replyto": "mWT3Ftkc3e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1569/Reviewer_fM3B"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1569/Reviewer_fM3B"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides the convergence guarantee for the consisitency models in terms of Wasserstein distance. The authors also show that Multi-step consistency sampling procedure can further reduce the error comparing to one step sampling. Finally, with some Langevin-based modifications, total variation errors are also provided."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. As far as I am concerned, this is the first time a convergence guarantee for consistency models is established.\n2. The improvement of multi-step consistency sampling over one step sampling has been clearly demonstrated theoretically."
                },
                "weaknesses": {
                    "value": "1. Not much technical contribution, most of the techniques has already been proposed in the literatures and the proof mostly follows Chen et al.(2023a). Also, Lemma 7 is not new, similar results have been established in [1]\n2. There are a lot of typos in the manuscript, e.g., on page 4: equation 4, the expression for $v^{\\mathrm{em}}(x,t)$; on page 15: in the second equation, $W_2(\\mathcal{N}(0,(1-e^{-2T})I_d), p_T)$ should be $W_2(\\mathcal{N}(0,I_d), p_T)$. Please reexamine your manuscript carefully.\n3. There are also some technique issues. For example, Lemma 11 in this paper is for the OU noise schedule, while Lemma 1 in Chen et al.(2023a) is for the variance explosion schedule.\n\n[1] Chen, H., Lee, H., and Lu, J. Improved analysis of scorebased generative modeling: User-friendly bounds under minimal smoothness assumptions. arXiv preprint arXiv:2211.01916, 2022a."
                },
                "questions": {
                    "value": "1. In corollary 6, $n_k$ is taken to be a constant $\\hat{n}$ for all $k$. While in Song et al. (2023), $n_k$ is suggested to be decreasing. The theoretical results do not seem to support a decreasing $n_k$, any explanation?\n2. The results on multistep sampling that only requires a constant lower bound of $T$ is amazing. However, it seems that $L_f$ implicitly depends on $T$, especially when the data distribution is complicated (so it takes more time to transform a simple Gaussian noise to the data distribution). Not sure how pratical this benefit can be."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1569/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698585837492,
            "cdate": 1698585837492,
            "tmdate": 1699636085288,
            "mdate": 1699636085288,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "upixoBTBiV",
                "forum": "mWT3Ftkc3e",
                "replyto": "UqEMZCYZJy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1569/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank the reviewer for the constructive comments.\n> Not much technical contribution ...\n\nThanks for your kindly comment. We admit that we use many techniques developed by former researchers, especially in proving Theorem 2, but we still have developed many new proofs, like Theorem 3, Corollary 5, Corollary 6 and Corollary 9. Besides, it is also important to use existing method to solve emerging new problems. We will keep in working harder to develop more new techniques.\n\n> There are a lot of typos in the manuscript ...\n> There are also some technique issues ...\n\nThanks for the correction. We have fixed these points, especially the Lemma 11 should corresponding to the Lemma 3 in Chen et al.(2023a).\n\n> In corollary 6, $n_k$ is taken to be a constant for all $k$. While in Song et al. (2023), $n_k$ is suggested to be decreasing. The theoretical results do not seem to support a decreasing $n_k$, any explanation?\n\nActually $n_k$ can be a decreasing sequence and the multistep error can still decrease. Let's look at the recursion inequality in the proof of Corollary 6,\n\n$\\mathcal{E}\\_k \\le L_f e^{-(t_{n_k}-\\delta)} \\mathcal{E}\\_{k-1} + t_{n_k}D,$\n\nthe best choice of $t_{n_k}$ should be chosen to minimize the function\n\n$ H(t) = L_f e^{-t - \\delta} \\mathcal{E}\\_{k-1}   + t D$.\n\nHowever, this is an transcendental equation, and we do not have an explicit evaluation of $\\mathcal{E}\\_{k-1}$, thus it would be impossible to calculate the best $n_k$. Besides, we can prove that, in upper-bound meaning, the best $n_k$ will finally converge to our defined $\\hat n$. Let us suppose the best-choice $n_k$ finally converge to $n^\\ast$, with corresponding $W_2$ error\n$\\mathcal{E}^\\ast$, then we have\n\n$\\mathcal{E}^\\ast\\le L_f e^{-(t_{n^\\ast}-\\delta)} \\mathcal{E}^\\ast + t_{n^\\ast}D,$\n\n$\\mathcal{E}^\\ast \\le \\frac{t_{n^\\ast}D}{1-L_f e^{-(t_{n^\\ast}-\\delta)}}$.\n\nThis suggests that we can directly find the extreme point $n^\\ast$ of the best-choice sequence $n_k$ by \n$\\min_{n} \\frac{t_{n}D}{1-L_f e^{-(t_{n}-\\delta)}}$, under some prior estimations on $\\varepsilon_{cm}, \\varepsilon_{sc}, L_f, L_s, d$ and $h$.\n\nNote that a simple fixing $n_k \\equiv n^\\ast$ also make the $\\mathcal{E}\\_k$ converging to $\\mathcal{E}^\\ast$ exponentially, we thus choose to simply use a fixing $n_k$.\n\n> The results on multistep sampling that only requires a constant lower bound of $T$ is amazing. However, it seems that $L_f$\n implicitly depends on $T$\n, especially when the data distribution is complicated (so it takes more time to transform a simple Gaussian noise to the data distribution). Not sure how pratical this benefit can be.\n\nIn actual it can be proven that, for the exact consistency function $f^{ex}$, given the data distribution is bounded-supported, it's Lipschitz constant function $L_f(t)$ has an upper bound irrelavent to $T$. You may look at our rebuttal to Reviewer 8oEy, where we gave a simple proof. \n\nBut as the reviewer 8oEy pointed, the Lipschitz constant of $f_\\theta$ could still grow with $T$ due to the score error. We thus propose an alternative definition of Lipschitz constant, and an additional penalty to keep the $f_\\theta$ having a bounded Lipschitz constant. This can be found in our second rebuttal to Reviewer 8oEy."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699957121954,
                "cdate": 1699957121954,
                "tmdate": 1700010706096,
                "mdate": 1700010706096,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HzIHDGmiJb",
                "forum": "mWT3Ftkc3e",
                "replyto": "UqEMZCYZJy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1569/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We kindly ask the reviewer if they have any outstanding questions or clarifications regarding our paper. We are happy to engage in a dialogue and conduct any additional requested work in the remaining discussion period. Thank you!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700097757737,
                "cdate": 1700097757737,
                "tmdate": 1700097757737,
                "mdate": 1700097757737,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VbZSfIXrUH",
                "forum": "mWT3Ftkc3e",
                "replyto": "X055tN6QNn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1569/Reviewer_fM3B"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1569/Reviewer_fM3B"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response that has addressed many of my questions. I would like to keep my score as the assumption on Lipshitz constant $L_f$ awaits further investigation."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700357218740,
                "cdate": 1700357218740,
                "tmdate": 1700357218740,
                "mdate": 1700357218740,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QRbxcAdgki",
            "forum": "mWT3Ftkc3e",
            "replyto": "mWT3Ftkc3e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1569/Reviewer_8oEy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1569/Reviewer_8oEy"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides the first convergence guarantee for Consistency Models (CMs), a newly emerging type of one-step generative model with the ability to produce samples comparable to those generated by state-of-the-art Diffusion Models."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper provides the first convergence guarantee for Consistency Models, which is a notable contribution to the field of generative modeling;\n2. The results do not rely on strong assumptions about the data distribution, making them broadly applicable to a variety of scenarios;\n3. Very clear writing. A comprehensive conclusion on future directions."
                },
                "weaknesses": {
                    "value": "I'm not entirely certain about the reasonability of Assumption 5. See Questions in details. \n\nBesides, there are no major weaknesses. A typo need to be corrected in the revision: at the beginning of section 3.3, it should be 'analyze' instead of 'analysis'."
                },
                "questions": {
                    "value": "The only point I am concerned about is Assumption 5, which assumes the consistency model $f_\\theta$ is $L_f$-Lipschitz. But as far as I \n know, even if our model $f_\\theta$ can approximate the backward mapping $f^v$ very well, we should still expect that $L_f$ is of $O(e^T)$ (which comes from the Gronwall's Inequality applied on the exact reverse ODE). And so the first term in Corollary 4 could look like $max (d^{1/2}, m)$, which cannot be arbitrarily small. Can you overcome such Gronwall-type of error? If not, I would doubt that the results of this paper prove the efficiency of CMs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1569/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1569/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1569/Reviewer_8oEy"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1569/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698629870449,
            "cdate": 1698629870449,
            "tmdate": 1700347398762,
            "mdate": 1700347398762,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wOzoQgKAJc",
                "forum": "mWT3Ftkc3e",
                "replyto": "QRbxcAdgki",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1569/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Rebuttal"
                    },
                    "comment": {
                        "value": "> The only point I am concerned about is Assumption 5, which assumes the consistency model \n is $L_f$-Lipschitz. But as far as I know, even if our model can approximate the backward mapping very well, we should still expect that \n is of $e^T$ (which comes from the Gronwall's Inequality applied on the exact reverse ODE). And so the first term in Corollary 4 could look like $max(d^{1/2},m)$, which cannot be arbitrarily small. Can you overcome such Gronwall-type of error? If not, I would doubt that the results of this paper prove the efficiency of CMs.\n\nThank the reviewer for pointing this out and we apologize for unsufficient explaination. Your concern is reasonable, and we should prove that the backward ODE would not accumulate the Lipschitz constants exponentially fast. In intuition, this should be true, at least in the ideal case, as when $t \\to \\infty$, $p_t(x)$ gradually converge to the normal distribution $N(0,I)$, and thus $\\nabla \\log p_t(x) \\to -x$, the particles under the  backward ODE is nealy stopped in large $t$. In fact, we can prove the following statement for the exact backward ODE $dx_t = v(x,t) dt$,\n where $v(x,t) = -x - \\nabla \\log p_t(x)$, $p_t(x) = e^{dt}p(e^{t}x) \\ast \\mathcal{N}(0,(1-e^{-2t})  I_d) \\sim \\int_{\\mathbb{R}^d} e^{dt} p(e^{t}u)e^{-\\frac{\\Vert x-u\\Vert^2}{2(1-e^{-2t})}}du$ with a bounded data distribution density $p_0(x) = p_{data}(x), \\text{supp } p(x) \\subseteq B(0,R)$ for some $R > 0$.\n\n***\n\n**Property** . The velocity field $v(x,t)$ in the exact backward ODE satisfies:\n\n$ \\Vert \\nabla v(x,t) \\Vert_{op} \\le e^{-2t} M,$\nfor some $M > 0$.\n\n*Proof*.\nBy the proof of Lemma 7 in our work, we have proved that \n\n$\\nabla^2 \\log (\\mu \\ast \\psi_{\\sigma^2}(x)) = \\frac{1}{\\sigma^4} \\int_{\\mathbb{R}^d} (u -\\mathbb{E}\\_{\\mu_{x,\\sigma^2}}[u] )\\otimes (u -\\mathbb{E}\\_{\\mu_{x,\\sigma^2}}[u] )  \\mu_{x, \\sigma^2}(du) - \\frac{1}{\\sigma^2} I_d$\n\nwhere\n\n $\\psi_{\\sigma^2}(u - x) \\sim e^{-\\frac{\\Vert x - u\\Vert_2^2}{2\\sigma^2}},$\n\n$  \\mu_{x, \\sigma^2}(du) = \\frac{e^{-\\frac{\\Vert x - u\\Vert_2^2}{2\\sigma^2} }\\mu(du)}{\\int_{\\mathbb{R}^d} e^{-\\frac{\\Vert x - \\tilde u\\Vert_2^2}{2\\sigma^2}} \\mu(d\\tilde u)}.$\n\nTaking $\\mu(u) = e^{dt} p_0(e^t u)$ which is bounded on a set of radius $e^{-t}R$ and $\\sigma^2 = 1-e^{-2t}$, noticing that\n$\\nabla v(x,t) = -I_d - \\nabla^2 \\log (\\mu \\ast \\psi_{\\sigma^2})$, we immediately get the result. \n\n***\n\nNow we can prove that the analytical consistency mapping $f^v(x,t)$ has a uniform Lipschitz constant for all $t \\ge \\delta$. We notice that, for two different end points $x_T^1, x_T^2$, we define \n\n$d x^1_t = v(x_t^1,t) dt$\n\n$d x^2_t = v(x_t^2,t) dt$\n\nand\n$f^v(x_T^1,T) - f^v(x_T^2,T) = x^1_\\delta - x^2_\\delta$\n\nwith early stopping time $\\delta >0$. Notice that, \n\n$ \\frac{d}{dt} \\Vert x^1_t - x^2_t \\Vert^2_2 = \\langle x^1_t - x^2_t, v(x_t^1 ,t) - v(x_t^2 , t)\\rangle \\le \\max  \\Vert \\nabla v(x,t) \\Vert_{op} \\Vert x^1_t - x^2_t \\Vert_2^2 = e^{-2t} M \\Vert x^1_t - x^2_t \\Vert^2_2,$\n\nthus by Gronwall's Inequality, \n\n$ \\Vert x^1_\\delta - x^2_\\delta \\Vert^2_2 \\le   \\Vert x^1_T - x^2_T \\Vert^2_2  \\exp( \\int_{\\delta}^{T}e^{-2t}M) \\le  \\Vert x^1_T - x^2_T \\Vert^2_2  \\exp( \\frac{M}{2}(1- e^{-2T})),$\n\nwhich shows a bounded Lipschitz constant when $T \\to \\infty$.\n\nWhen dealing with the model $f^{em}$ under the approximated score function $s_\\phi$, it would be much more difficult, as we should ensure $-x - s_\\phi(x,t)$ has a similar operator bound like $-x - \\nabla \\log p_t(x)$. This can be proved by further strengthen the assumption 3 to a $L_\\infty$ norm, and multiply the right-hand-side with an exponential-decreasing term $e^{-2t_n}$,\n\n $\\Vert s_{\\phi}(x,t_n) - \\nabla \\log p_{t_n}(x) \\Vert_2^2 \\le \\frac{e^{-2t_n}}{1-e^{-2t_n}}\\varepsilon^2_{sc}$ for all $x$ and $t_n$.\n\nIn words, we believe that our assumption 5 is reasonable, however further analysis can be conducted to replace this assumption to more reasonable assunptions. This will be our future work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699870132357,
                "cdate": 1699870132357,
                "tmdate": 1699953205866,
                "mdate": 1699953205866,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2ly2HLxdbu",
                "forum": "mWT3Ftkc3e",
                "replyto": "wOzoQgKAJc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1569/Reviewer_8oEy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1569/Reviewer_8oEy"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. I agree with your argument here. But does it mean that we need an $L^\\infty$ score estimation assumption instead an $L^2$ one? If so, I still cannot see why assumption 5 is reasonable. You are kind of skipping the Gronwall type of error induced by score estimation and discretization."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699896450407,
                "cdate": 1699896450407,
                "tmdate": 1699896450407,
                "mdate": 1699896450407,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KWX2fG6Dmm",
                "forum": "mWT3Ftkc3e",
                "replyto": "QRbxcAdgki",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1569/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your kindly response. \n\nYou are correct, as our existing assumptions can not ensure the backward $f_\\theta$ have a uniform Lipschitz constant. Actually in our first response we said we need an $L^\\infty$ score, that should still not enough: we need further ensure $||\\nabla s_\\phi(x,t) - \\nabla^2 p_t(x)||$ to be small, such that the Gronwall type of error introduced by score estimation won't increase exponentially. This can be partially reduced by the so-called higher order score matching ([1], Theorem 4.1), however still not enough as the higher order score matching only guarantee a small $L^2$ error either. This is just why the theory and performance for probability ODEs are weaker than probability SDEs. \n\nNevertheless, as the consistency model $f_\\theta$ offers us a direct way to estimate the endpoints of the ODE flows, comparing to traditional higher order score matching for ODE flows that carefully restrict the score function, we can directly constrain $f_\\theta$ to satisfy some Lipschitz conditions. This is reasonable, as the true consistency mapping $f^{ex}$ should have this property. Here I'll explain how to introduce this constraint inside the trainning.\n\nFirst let us modify our Assumption 5, as we do not need global and strong Lipschitz conditions in $L^\\infty$ meaning; actually the only usage of assumption 5 is in Equation (20), which is local and under $L^2$ meanings.\n\n**Assumption 5'**\nThe consistency model $f_{\\theta}(x,t_n)$ satisfies\n\n$ \\mathbb{E}\\_{x_{t_{n+1}} \\sim p_{t_{n+1}} }\\left[\\left\\Vert   f_\\theta( \\hat x_{t_n}^\\phi,t_n) - f_\\theta( x_{t_n},t_n)    \\right\\Vert_2^2 \\right] \\le L_f^2     \\mathbb{E}\\_{x_{t_{n+1}} \\sim p_{t_n+1} }\\left[\\Vert \\hat x_{t_n}^\\phi - x_{t_n}  \\Vert_2^2\\right] $\nfor all $n \\in [\\\\![1,N-1]\\\\!]$.\n\nThere are many methods to restrict our model to satisfy this assumption. For example, we can add a penalty term \n\n$ \\gamma \\max\\left(0, \\mathbb{E}\\_{x_{t_{n+1}} \\sim p_{t_n+1} }\\left[\\left\\Vert   f_\\theta( \\hat x_{t_n}^\\phi,t_n) - f_\\theta( x_{t_n},t_n)    \\right\\Vert_2^2 - L_f^2    \\Vert \\hat x_{t_n}^\\phi - x_{t_n}  \\Vert_2^2\\right]\\right),$ \n\nwhere $\\gamma$ is a scaling parameter. We should choose $L_f \\ge \\exp(\\frac{M}{4})$, where $M$ is the operator norm of exact backward ODE field $v(x,t) = -x - \\nabla \\log p_t(x)$ introduced in the discussion before. We can approximate $x_{t_n}$ using initial point $x_{t_{n+1}}$ and finer stepsize ODE solvers as a reference. We can choose less sample points than the main Consistency Loss term to save computational costs.  Although this is not exactly what it should be, we believe this can preventing the consistency models from getting an exponentially-increasing Lipschitz constants. \n\n***\n\n\n[1] Maximum Likelihood Training for Score-Based Diffusion ODEs by High-Order Denoising Score Matching"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699953102670,
                "cdate": 1699953102670,
                "tmdate": 1700030488201,
                "mdate": 1700030488201,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "43T9Z1EKUt",
                "forum": "mWT3Ftkc3e",
                "replyto": "KWX2fG6Dmm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1569/Reviewer_8oEy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1569/Reviewer_8oEy"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I agree with your argument here. Although I am still not sure if restricting the model to satiscy the Lipschitz assumption is reasonable either in theory/practice, I will change my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700347371734,
                "cdate": 1700347371734,
                "tmdate": 1700347371734,
                "mdate": 1700347371734,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9ex0upvsis",
            "forum": "mWT3Ftkc3e",
            "replyto": "mWT3Ftkc3e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1569/Reviewer_N63V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1569/Reviewer_N63V"
            ],
            "content": {
                "summary": {
                    "value": "The authors of this work provide the first convergence guarantees for Consistency Models, a new class of diffusion models that achieves one-step generation. Under classical assumptions (smooth data density, bounded second moment of the data distribution, and an L2 bound on the score estimation) plus an additional assumption on the error of the consistency sampler, the authors provide a bound on the Wasserstein that is polynomial in all problem parameters. The authors further analyze multistep consistency sampling which removes the linear dependence on the total diffusion time $T$. Finally, the authors provide TV bounds by: i) early stopping and ii) modifying the reverse process with Langevin correctors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The authors provide the first analysis for Consistency Diffusion Models, which is an emerging class of diffusion models with nice theoretical and practical properties. It is nice to see that the theory follows closely the practical advancements and complements our understanding of why certain learning algorithms work.\n- The authors do a thorough analysis of Consistency Diffusion Models, providing bounds for Wasserstein distance (for smooth and bounded densities) and TV distance (under certain modifications).\n- The obtained bounds match the ones known for Score-Based Generative Models from the \"Sampling is as easy as learning the score\" paper.\n- The theoretical analysis is novel, especially for the result of Theorem 2."
                },
                "weaknesses": {
                    "value": "Even though I overall consider this a strong submission, there are some weaknesses that need to be addressed or clarified.\n\n\n- The presentation of the paper could be improved. There are several typos in the main text -- I would encourage the authors to do another pass over the manuscript and fix them. Some examples:\n     * \"be further weaken\" -> \"be further weakened\"\n     * \"consistency model is nature\" -> \"is natural\"\n     * same sentence, fix formatting of the parenthesis.\n     * Figure 1: punctuation missing.\n     * \"a asymptotic analysis\" -> \"an asymptotic analysis\"\n     * \"For technique reason\" -> for technical reasons. It would also help to explain what these reasons are.\n- To improve the clarify of the paper, I think it would be better to explain what $\\theta_{-}$ is the first time it is introduced.\n- For Assumption 4, I think that the expectation should be over $x_{t_{n+1}}$. Given a $x_{t_{n+1}}$ everything is deterministic. Does that affect the proof of Theorem 2?\n- Corollary 4 is a little confusing. It seems that we require a lower bound on $T$, however, in the text, it is presented as having a large value of $T$ is a bad thing -- which is why the authors claim that multistep consistency sampling helps. I think the way this result is presented is a little misleading since it requires the learning errors to be O(1/T). \n- I am a little concerned about how much of the complexity of the problem is hidden in the upper-bound assumption on the learning of the consistency model. The model is trained to solve the whole probability flow ODE. If the score is inaccurately learned, the prediction of the solution can be poor because of error propagation. It seems that the error in the learning of the score and the consistency errors are actually related.\n- I also don't understand why this special time scheduling is needed. I don't think this is the scheduling that has been used in practice. I also don't recall seeing this assumption in prior work and it would be great to understand where this is coming from."
                },
                "questions": {
                    "value": "See weaknesses above. Repeating here the points for completeness:\n- For Assumption 4, I think that the expectation should be over $x_{t_{n+1}}$. Given a $x_{t_{n+1}}$ everything is deterministic. Does that affect the proof of Theorem 2?\n- Corollary 4 is a little confusing. It seems that we require a lower bound on $T$, however, in the text, it is presented as having a large value of $T$ is a bad thing -- which is why the authors claim that multistep consistency sampling helps. I think the way this result is presented is a little misleading since it requires the learning errors to be O(1/T). \n- I am a little concerned about how much of the complexity of the problem is hidden in the upper-bound assumption on the learning of the consistency model. The model is trained to solve the whole probability flow ODE. If the score is inaccurately learned, the prediction of the solution can be poor because of error propagation. It seems that the error in the learning of the score and the consistency errors are actually related.\n- I also don't understand why this special time scheduling is needed. I don't think this is the scheduling that has been used in practice. I also don't recall seeing this assumption in prior work and it would be great to understand where this is coming from."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1569/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1569/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1569/Reviewer_N63V"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1569/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699615004955,
            "cdate": 1699615004955,
            "tmdate": 1700824336319,
            "mdate": 1700824336319,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4N11VJ1zKw",
                "forum": "mWT3Ftkc3e",
                "replyto": "9ex0upvsis",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1569/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Rebuttal"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the constructive comments. \n> The presentation of the paper could be improved.\n> To improve the clarify of the paper, I think it would be better to explain what $\\theta^{-}$ is the first time it is introduced.\n\nThanks for pointing this out, we have modified all the mentioned typos or misleading in the updated paper\n\n> For Assumption 4, I think that the expectation should be over $x_{t_{n+1}}$. Given a $x_{t_{n+1}}$  everything is deterministic. Does that affect the proof of Theorem 2?\n\nThanks for pointing this out, we have modified all the mentioned typos in the updated paper. This would not affect the proof of Theorem 2, as in each individual term that needs to calculate expectations, $x_t$ are keeped in the same ODE path for all $t\\in[\\delta, T]$.\n\n> Corollary 4 is a little confusing. It seems that we require a lower bound on $T$, however, in the text, it is presented as having a large value of $T$ is a bad thing -- which is why the authors claim that multistep consistency sampling helps. I think the way this result is presented is a little misleading since it requires the learning errors to be $O(1/T)$.\n\nThat's correct: as $T$ increase, the discretization error will increase; however, if $T$ is not large enough, the error introduced by replacing $p_T$ to a normal distribution would become large. This is also obtained in other works such as Theorem 2, *Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions*.\n\n> I am a little concerned about how much of the complexity of the problem is hidden in the upper-bound assumption on the learning of the consistency model. The model is trained to solve the whole probability flow ODE. If the score is inaccurately learned, the prediction of the solution can be poor because of error propagation. It seems that the error in the learning of the score and the consistency errors are actually related.\n\nThe definition of consistency error can be disentangle to the score matching error, as our Assumption 4 do not need $\\hat x_{t_n}^\\phi$ to be exact enough: in fact, given any score model $s_\\phi(x,t)$, no matter how far from the exact score $\\nabla \\log p_t(x)$,  we can always define the consistency mapping $f^{em}$. And if one can not learn the score well enough, one still can not generate good enough samples by diffusion models, let alone consistency models (in consistency distillation way).\n\n> I also don't understand why this special time scheduling is needed. I don't think this is the scheduling that has been used in practice. I also don't recall seeing this assumption in prior work and it would be great to understand where this is coming from.\n\nSorry for the insufficient explaination. The special time scheduling was firstly suggested by Sitian Chen, *The probability flow ODE is provably fast*, second paragraph in section 3.2 Algorithm. The reason to choose this time scheduling is to control one term of error in Theorem 2: in equation (26), we get the upper bound with a term $\\sum_{k=1}^N \\frac{h_k^2}{ t_k^{1/2}}$. If we only take a naive time scheduling $h_k \\equiv h$, this term now becomes $O(\\frac{h^2}{\\sqrt{\\delta}})$, as $\\frac{1}{t_k^{1/2}}$ will become larger when $k \\to 0$. One would better choose a relatively smaller discretization step when $t_k$ is small to prevent the discretization error from being unnessary large. Thus a geometrically increasing step size will help a lot."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699847641709,
                "cdate": 1699847641709,
                "tmdate": 1699873171644,
                "mdate": 1699873171644,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kw6KH3SSzj",
                "forum": "mWT3Ftkc3e",
                "replyto": "9ex0upvsis",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1569/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We kindly ask the reviewer if they have any outstanding questions or clarifications regarding our paper. We are happy to engage in a dialogue and conduct any additional requested work in the remaining discussion period. Thank you!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700097721918,
                "cdate": 1700097721918,
                "tmdate": 1700123810830,
                "mdate": 1700123810830,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]