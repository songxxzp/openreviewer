[
    {
        "title": "SpikeBERT: A Language Spikformer Learned from BERT with Knowledge Distillation"
    },
    {
        "review": {
            "id": "oAK32ySLBZ",
            "forum": "6c4gv0E9sF",
            "replyto": "6c4gv0E9sF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4742/Reviewer_sLn7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4742/Reviewer_sLn7"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a spiking neural network variant of BERT called SpikeBERT, thereby employing knowledge distillation using BERT as the teacher model. The main advantage of SpikeBERT compared to vanilla BERT seems to be that it is consuming less \u201cenergy\u201d (measured in mJ)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- **Table 2**: It\u2019s interesting and promising to see that SpikeBERT has much lower energy consumption than FT BERT.\n\n\n- **Section 4.5**: I appreciate the comprehensive ablation study that was performed on the hyperparameters. It\u2019s good scientific practice to scrutinize the impact/effect of different hyperparameters."
                },
                "weaknesses": {
                    "value": "- **Contributions**: I think contribution 2 is misleading. While the authors show that their model performs better than existing SNN methods, their method performs on par with a simple TextCNN (from **ten** years ago) which I think is computationally much less expensive than their whole pre-training and fine-tuning pipeline because TextCNN is a much smaller architecture whose outputs are non-contextual/static word representations that can easily be pre-computed.\n\n- **Section 3**: The entire \u201cpre-training + knowledge distillation + fine-tuning\u201d pipeline appears to require a vanilla BERT model that has previously been pretrained on a large language corpus as is standard. If you rely on BERT, why would I not \u201cjust\u201d fine-tune or probe BERT instead of applying your pipeline? What is the advantage here? \n\n\n- **Notation/Math**: The math and notation in Section 3 are a bit sloppy and not very precise.\n\n\n- **Table 1**: The results in Table 1 are misleading. The authors bold-faced their model\u2019s performances. However, \u201cFT BERT\u201d (which is clearly not SOTA anymore on these tasks) achieves much stronger performance than their model across all reported datasets. Moreover, TextCNN --- which was one of the **first** CNN models for text sequences and whose representations are non-contextual/static word representations --- shows better performance on two datasets (Subj and ChnSenti) and only marginally worse performance on the other four datasets. I\u2019d be curious to see the standard deviations here. Because, if they overlap, then the performances between TextCNN and SpikeBERT are not statistically significantly different. Please report the standard deviations in brackets next to the averages and unbold your numbers or at least explain in the caption what bold-face means here. It\u2019s not good practice to mislead the reader by simply bold-facing your numbers without further explanation.\n\n\n- **Conclusion**: The conclusion is pretty short for a scientific conference paper. There is no discussion of results or impact. Moreover, I think the claim \u201c*[...] can even achieve comparable results to BERTs with much less energy consumption across multiple datasets for both English and Chinese, leading to future energy-efficient implementations of BERTs or large language models.*\u201d is misleading. I think SpikeBERT achieves comparable results to TextCNN but not to FT BERT. Also, BERT is not SOTA anymore since 2021. How would SpikeBERT compare against more recent variants of Transformer-based foundation models such as RoBERTa, Albert, or T5? (see the [Glue](https://gluebenchmark.com/), [SuperGlue](https://super.gluebenchmark.com/) and [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) leaderboards for an up-to-date list of models in NLP). I am not convinced that the approach reported in this paper is \u201c*leading to future energy-efficient implementations of BERTs or large language models*\u201d. There are numerous other approaches that have demonstrated this via distillation techniques (e.g., [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)).\n\n\n- No limitations are discussed as part of the conclusion. Unfortunately, there is no discussion section.\n\n\n- An entire body of work that has employed distillation techniques over the past 4 years in NLP is not discussed here."
                },
                "questions": {
                    "value": "- **Section 3**: The entire \u201cpre-training + knowledge distillation + fine-tuning\u201d pipeline appears to require a vanilla BERT model that has previously been pretrained on a large language corpus as is standard. If you rely on BERT, why would I not \u201cjust\u201d fine-tune or probe BERT instead of applying your pipeline? What is the advantage here? Linear probing is much less expensive than fine-tuning (it only requires a linear classifier) and often equally performant (depending on the task).\n\n\n- **Section 4**: Could you elaborate why I would use SpikeBERT over TextCNN although the methods perform equally well on all the reported datasets (see my comment on Table 1 above)? TextCNN is a computationally much less expensive method and has the \"advantage\" of static word representations. So, I could just compute the representations for each word used in the datasets a priori and then run inference as many times as I want without the need to run the sentences through the model. That being said, I don\u2019t think that anyone in the community would still use a TextCNN from 2013 that produces non-contextual word representations for NLP tasks.\n\n\n- Why didn\u2019t you compare SpikeBERT against [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)? DistilBERT is a distilled version of BERT that is much faster and cheaper and has comparable performance to BERT (probably better than SpikeBERT on the benchmarks that you looked at). AFAIK, DistilBERT exists since 2020. So, there must be an even better and more recent version of DistilBERT such as DistilRoBERTa or DistilALBERT. But please take a look yourself.\n\n\n- **Table 2**: Why is the number of FLOPs consistently larger for SpikeBERT than for FT BERT although SpikeBERT\u2019s energy consumption is much lower? How do you explain that? I\u2019d like to see FLOPs and energy consumption of TextCNN reported in this table and not just SpikeBERT vs. FT BERT. Could you please report those? In addition to the FLOPs and energy consumption of TextCNN it would like to see these numbers for [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) which has been shown to be much more computationally efficient/energy efficient than BERT while preserving most of its performance (around 95%) via knowledge distillation but their pipeline seems to be easier than your pipeline and does not necessitate \"embedding alignment\". Again, there probably exist even better distilled versions of BERT or RoBERTa or GPT-2/GPT-3 by now.\n\n\n- **Table 3**: Did you employ the same data augmentation strategies to all methods that you compared SpikeBERT against? If data augmentation plays a crucial role in the performance of the model (which it does according to your ablations), then it seems not fair to compare SpikeBERT + DA against other methods without DA."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4742/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698225647083,
            "cdate": 1698225647083,
            "tmdate": 1699636456293,
            "mdate": 1699636456293,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DXU4DrHuq3",
                "forum": "6c4gv0E9sF",
                "replyto": "oAK32ySLBZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4742/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments!\n\n**Q1:** **If you rely on BERT, why would I not \u201cjust\u201d fine-tune or probe BERT instead of applying your pipeline? What is the advantage here? Linear probing is much less expensive than fine-tuning (it only requires a linear classifier) and often equally performant (depending on the task).**\n\nA1: It is our fault that we do not provide enough background on spiking neural networks. \n\n1) Different from classical artificial neural networks, spiking neural networks are **brain-inspired** networks, which do not transmit information in form of continuous values, but rather the time when a membrane potential reaches a specific threshold. Once the membrane potential reaches the threshold, the neuron fires and generates a pulse signal that travels to the downstream neurons which increase or decrease their potentials in proportion to the connection strengths in response to this signal. SNNs incorporate the concept of time into their computing model in addition to neuronal and synaptic states. They are considered to be more **biologically plausible** neuronal models than classical ANNs. Besides, SNNs are suitable for implementation on low-power brain-inspired neuromorphic hardware, and offer a promising computing paradigm to deal with large volumes of data using spike trains for information representation in a more **energy-efficient** manner.\n\n2) We treat SNNs as a promising avenue to implement deep neural networks in a more biologically plausible and energy-efficient (when inference) way. Once SNNs are well software-trained, they can be deployed on neuromorphic hardware (such as 45nm neuromorphic hardware[6]) for energy-efficient computing (binary-value computing). This computing pattern brings a significant energy reduction when inference (Please see Table 2).\n\n3) Our claim is to use **discrete spike trains** instead of continuous decimal values to compute and transmit information in deep neural networks for language tasks. We hope our work can take a step forward in language processing with a more brain-inspired and energy-efficient method. Our proposed SpikeBERT is the first Transformer-liked SNN for language tasks, and now we are focusing on how to implement spiking version of GPT-liked language model, which may lead to future energy-efficient implementations of large language models **running on neuromorphic** **hardware**.\n\nTherefore, \u201cjust\u201d fine-tuning or probing BERTs, which run on GPUs, can not achieve our targets in this scenario. We hope our explanations can address your concerns. Thank you!\n\n**Q2: Contribution 2 is misleading. I think SpikeBERT achieves comparable results to TextCNN but not to FT BERT.**\n\nA2: We apologize that our statement has mislead you. We follow previous work[1][2] to take spiking neural networks (SNNs) as our baselines, rather than traditional artificial neural networks (ANNs). We think it is not that reasonable to take ANNs as baselines, and the reasons are following: 1) Due to the non-differentiability of spikes, training SNNs is a great challenge; 2) ANNs take a large number of floating-point operations, while SNNs only do spike binary-value (only 0 and 1) operations. Therefore, we take SNN-TextCNN and Directly-trained SpikeBERT as our baselines.\n\nThis point is widely recognized in the field of SNNs. For example, the performance gap between Spikformer[1] and its ANN counterpart model Vision-Transformer[3] reached 6%, which is much more than the gap between fine-tuned BERT and SpikeBERT. But Spikformer outperformed other SNN models. They also said that their model achieved state-of-the-art performance in SNNs and also comparable to Vision-Transformer. Other SNN papers [2][4][5] also take SNNs as the baselines, rather than ANNs.\n\nMoreover, there are very few works that have demonstrated SNNs\u2019 effectiveness in natural language processing (NLP) tasks. Our proposed SpikeBERT is the first deep Transformer-liked SNN for language tasks, whose scale is the same as the scale of BERT.\n\n**Q3:** **Table 1: The results in Table 1 are misleading.** **Please report the standard deviations in brackets next to the averages and unbold your numbers or at least explain in the caption what bold-face means here.**\n\nA3: It is a good suggestion. As mentioned in Q2, we do not take ANNs as our baselines because it is not fair. However, we will follow your suggestion to report the standard deviations in Table 1 and bold the number of Fine-tuned BERT. Thank you for pointing it out, and we will make it clear in the revised version."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699875973931,
                "cdate": 1699875973931,
                "tmdate": 1699875973931,
                "mdate": 1699875973931,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gmv5Hs2LBj",
                "forum": "6c4gv0E9sF",
                "replyto": "oAK32ySLBZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4742/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q4:** **BERT is not SOTA anymore since 2021. How would SpikeBERT compare against more recent variants of Transformer-based foundation models such as RoBERTa, Albert, or T5? (see the Glue, SuperGlue and SQuAD leaderboards for an up-to-date list of models in NLP).**\n\nA4: Our approach is applicable to any model similar to BERT or RoBERTa. Existing literature[7] indicates that BERT and RoBERTa exhibit minimal substantive differences in downstream tasks, and both models share a similar network architecture. The divergence between these models is primarily observed in the aspect of masking strategy, input tokenization and training strategy[8], yet these differences do not impact the conclusions drawn in this paper. What\u2019s more, as BERT serves as a representative example of large pre-trained models, we chose BERT as the teacher model.\n\nBesides, we report the performance of SpikeBERT on GLUE benchmark in **Appendix C**. Although SpikeBERT significantly outperforms the SNN baseline (SNN-TextCNN) on all tasks, we find that the performance of SpikeBERT on the Natural Language Inference (NLI) task (QQP, QNLI, RTE) is not satisfactory compared to fine-tuned BERT. The possible reason is that we mainly focus on the semantic representation of a single sentence in the pre-training distillation stage. Meanwhile, we have to admit that SpikeBERT is not sensitive to the change of certain words or synonyms, for it fails to converge on CoLA and STS-B datasets. We think that\u2019s because spike trains are much worse than floating-point data in representing fine-grained words. In the future, we intend to explore the incorporation of novel pre-training loss functions to enhance the model's ability to model sentence entailment effectively.\n\n**Q5:** **No limitations are discussed as part of the conclusion. There is no discussion section.**\n\nA5: Due to page limitations, we have included \u201cDiscussion of Limitations\u201d in the **Appendix F** of our original manuscript. In this section, we primarily discuss the following limitations: Firstly, there are many neuromorphic event-based image datasets, such as CIFAR-10-DVS and DVS-128-Gesture, which perfectly align with the characteristics of SNN networks. However, such datasets are lacking in the natural language processing tasks. Secondly, the data used for SNN training introduces an additional temporal dimension (T dimension) compared to traditional data. Limited to the GPU memories, we had to reduce the sentence length of input sentences, which significantly constrains the performance of our models.\n\n**Q6:** **Could you elaborate why I would use SpikeBERT over TextCNN although the methods perform equally well on all the reported datasets (see my comment on Table 1 above)?**\n\nA6: The same as A1.\n\n**Q7:** **An entire body of work that has employed distillation techniques over the past 4 years in NLP is not discussed here.**\n\nA7: Thank you for pointing it out. In the Related Work section, we discuss the history of knowledge distillation in the era of deep learning, including three kinds of knowledge distillation methods, the combination of SNN and knowledge distillation, etc. However, our proposed distillation method is mainly designed to transfer knowledge from ANNs to SNNs. We can add some studies on knowledge distillation in the NLP field over the last 4 years in the revised version.\n\nAlso, while it may seem like a natural idea to use knowledge distillation after direct training fails, applying it to spiking neural networks is still poses significant challenges. Firstly, how to align the **spiking signals** of the student model with the **floating-point signals** of the teacher model? We addressed this issue by introducing an external \u201cMLP+LayerNorm\u201d layer to convert the signals from spikes to floating points. Secondly, training spiking neural networks typically requires specific training techniques to stabilize and accelerate the convergence process, so the traditional knowledge distillation methods may not adapt well to these techniques, resulting in training difficulties or suboptimal performance. We addressed this issue by employing many training tricks, some of which were not explicitly mentioned in the paper. These tricks included dynamically adjusting the alignment signal weight ratios based on loss ratios, selectively ignoring representations from certain layers, and using longer warm-up periods. In practice, achieving a convergent spiking neural network language model is challenging because traditional knowledge distillation methods and SNNs training methods are ineffective in these scenarios."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699876009874,
                "cdate": 1699876009874,
                "tmdate": 1699876009874,
                "mdate": 1699876009874,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UaLb8UTeCT",
                "forum": "6c4gv0E9sF",
                "replyto": "oAK32ySLBZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4742/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q8:** **Why didn\u2019t you compare SpikeBERT against DistilBERT?**\n\nA8: Due to page limitations, we have included \u201cenergy reduction compared to other BERT variants\u201d in the **Appendix E** of our original manuscript. We compare two classic BERT variants, DistilBERT[9] and TinyBERT[10], with our SpikeBERT. We want to state that spiking neural networks and model compressing are two different technological pathways to achieve energy efficiency. Future advancements in neuromorphic hardware are expected to decrease energy consumption further.\n\n**Q9: Table 2: Why is the number of FLOPs consistently larger for SpikeBERT than for FT BERT although SpikeBERT\u2019s energy consumption is much lower? How do you explain that?**\n\nA9: We apologize that \u201cFLOPs/SOPs(G)\u201d term in Table 2 is misleading.  For SNNs, we report their SOPs only, while for ANNs, we report their FLOPs only. We can not tell which model is more energy-efficient by directly comparing absolute value of these two metrics (i.e. FLOPs and SOPs). The FLOPs can be approximately converted into SOPs (See Equation 18 in **Appendix D**):\n\nThe number of synaptic operations at the layer $l$ of an SNN is estimated as $SOPs(\u03be) = T \u00d7 \u03b3 \u00d7 FLOPs(l)$, where $T$ is the number of times step required in the simulation, $\u03b3$ is the firing rate of input spike train of the layer $l$.\n\nAppendix D titled \u201cTheoretical Energy Consumption Calculation\u201d explains the relation of FLOPs and SOPs, and also give detailed formulas of energy estimation based on 45nm neuromorphic hardware[6].\n\n**Q10: Table 3: Did you employ the same data augmentation strategies to all methods that you compared SpikeBERT against?**\n\nA10: In Table 3, all reported values, except those shown in \u201cw/o Stage 2\u201d row and \u201cStage 2 w/o DA\u201d row, employ the same data augmentation strategies for the downstream tasks. In addition, according to our ablation study, data augmentation is not the most important factor (only bring 0.76% performance increase). We think our proposed two-stage knowledge distillation with four types of losses is crucial.\n\n **Q11: The math and notation in Section 3 are a bit sloppy and not very precise.**\n\nA11: We have carefully checked our formulas in Section 3, and update some notations in our revised manuscripts.\n\n[1] Zhou Z, Zhu Y, He C, et al. Spikformer: When Spiking Neural Network Meets Transformer[C]//The Eleventh International Conference on Learning Representations. 2022.\n\n[2] Lv C, Xu J, Zheng X. Spiking Convolutional Neural Networks for Text Classification[C]. // The Eleventh International Conference on Learning Representations. 2022.\n\n[3] Dosovitskiy A, Beyer L, Kolesnikov A, et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale[C]//International Conference on Learning Representations. 2020.\n\n[4] Yao M, Hu J K, Zhou Z, et al. Spike-driven Transformer[C]//Thirty-seventh Conference on Neural Information Processing Systems. 2023.\n\n[5] Zhou C, Yu L, Zhou Z, et al. Spikingformer: Spike-driven Residual Learning for Transformer-based Spiking Neural Network[J]. arXiv preprint arXiv:2304.11954, 2023.\n\n[6] Horowitz M. 1.1 computing's energy problem (and what we can do about it)[C]. // 2014 IEEE international solid-state circuits conference digest of technical papers (ISSCC). IEEE, 2014: 10-14.\n\n[7] Qiu X, Sun T, Xu Y, et al. Pre-trained models for natural language processing: A survey[J]. Science China Technological Sciences, 2020, 63(10): 1872-1897.\n\n[8] Liu Y, Ott M, Goyal N, et al. Roberta: A robustly optimized bert pretraining approach[J]. arXiv preprint arXiv:1907.11692, 2019.\n\n[9]Tang R, Lu Y, Liu L, et al. Distilling task-specific knowledge from bert into simple neural networks[J]. arXiv preprint arXiv:1903.12136, 2019.\n\n[10]Jiao X, Yin Y, Shang L, et al. Tinybert: Distilling bert for natural language understanding[J]. arXiv preprint arXiv:1909.10351, 2019.\n\n \n\nFinally, we thank you for your promising insights on our paper. We understand that reviewers are not required to read the Appendix section. However, due to the constraint on page limitation, we have written some parts (such as \"Discussion of Limitations\", \"Comparison of DistilBERT/TinyBERT\", and \"Performance on GLUE\") in the Appendix of our original manuscript. Meanwhile, the Appendix.pdf file can be found in our supplementary material. We sincerely hope the Appendix and the Q&A above can address your concerns. If you have any further questions, please let us know~ Thank you very much :)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699876032444,
                "cdate": 1699876032444,
                "tmdate": 1700104358107,
                "mdate": 1700104358107,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zG6D1RDDXT",
                "forum": "6c4gv0E9sF",
                "replyto": "oAK32ySLBZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4742/Reviewer_sLn7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4742/Reviewer_sLn7"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer's response to authors' rebuttal"
                    },
                    "comment": {
                        "value": "I have read the rebuttal and I thank the authors for their thorough response! However, I will not change my rating. I can see this manuscript being an interesting contribution to a workshop on SNNs or a \u201csmaller\u201d NLP conference like NACL or EACL, but I don't feel comfortable accepting it to ICLR. The contribution is too minor and I am not sure why anyone would use their method over variants of DistillBERT which is a method from a few years ago. I am also not sure about the benefits for the broader ICLR community."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700142071824,
                "cdate": 1700142071824,
                "tmdate": 1700725235244,
                "mdate": 1700725235244,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EcRn0ITqYr",
                "forum": "6c4gv0E9sF",
                "replyto": "oAK32ySLBZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4742/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer sLn7,\n\nWe acknowledge the insightful comments provided by you and appreciate the opportunity to address your concerns.\n\n**1. What is spiking neural network? Why is it important?**\n\nSpiking neural network (SNN) is a brain-inspired neural network proposed by [1] in 1997, which has been seen as the third generation of neural network models and an important **application of neuroscience**. SNNs use discrete spike trains (0 and 1 only) instead of floating-point values to compute and transmit information, which are quite suitable for implementation on neuromorphic hardware. Therefore, compared to traditional artificial neural networks that run on GPUs, the SNNs offer an energy-efficient computing paradigm to deal with large volumes of data using spike trains for information representation when inference. To some degree, we can regard SNN as **a simulation of neuromorphic hardware** used to handle a downstream deep learning task. Nowadays, the neuromorphic hardware mainly refers to brain-like chips, such as 14nm Loihi2 (Intel), 28nm TrueNorth (IBM), etc. As for the training of SNNs, there are no mature on-chip training solutions currently so the training has to be done on GPUs. However, once SNNs are well trained on GPUs, they can be deployed on neuromorphic hardware for energy-efficient computing (0-1 computing only).\n\n**2. The motivation and contribution of this study.**\n\nMotivation:\n\nThere are few works that have demonstrated the effectiveness of spiking neural networks in natural language processing tasks. Current state-of-the-art model is SNN-TextCNN[2], which is based on a simple backbone TextCNN.\n\nHowever, LLMs like ChatGPT perform very well in many language tasks nowadays, but one of their potential problems is the huge energy consumption (even when inference with GPUs). We want to implement spiking versions of LLMs running on low-energy neuromorphic hardware so that models are able to do **GPUs-free inference** and the **energy consumption** when inference can be significantly reduced. Our proposed SpikeBERT can be seen as the first step. We hope SpikeBERT can lead to future energy-efficient implementations of **large language models on brain-inspired neuromorphic hardware**!\n\nContributions:\n\n(1) We propose the **first** Transfomer-like SNN for **language** tasks with the same scale as the scale of BERT. Experiments show that SpikeBERT outperforms all SNN baselines in all benchmarks and achieve comparable performance of BERT with much less energy consumption. Note that the inference of SpikeBERT is **GPUs-free** and **energy-efficient**.\n\n(2) Directly-trained SpikeBERT suffers from the problem of gradient vanishing or exploding due to \u201cself-accumulating dynamics\u201d. Therefore, we choose knowledge distillation to train SpikeBERT. We would like to clarify that our approach involves the challenging task of distilling knowledge from BERT into spiking neural networks (SNNs), where the fundamental distinction lies in the use of discrete spikes for computation and information transmission in the student model (SNN), as opposed to the continuous values in the teacher model (BERT). To address this disparity, we introduce a novel two-stage \"pre-training + task-specific\" knowledge distillation (KD) method. This method incorporates proper normalization across both timesteps and training instances within a batch, enabling a meaningful comparison and alignment of feature representations between the teacher and student models.\n\n**3.** **The performance and baselines concerns.**\n\nSNNs still lag behind ANNs in terms of accuracy yet. Through intensive research on SNNs in recent years, the performance gap between deep neural networks (DNNs) and SNNs is constantly narrowing. SNNs cannot currently outperform DNNs on the datasets that were created to train and evaluate conventional DNNs (they use continuous values). Such data should be converted into spike trains before it can be feed into SNNs, and this conversion might cause loss of information and result in a reduction in performance. Therefore, the comparison is indirect and unfair. In our study, we have conscientiously chosen existing SNNs as baselines for evaluation to provide **a fair and relevant benchmark**. As shown in Table 1, the performance of SpikeBERT surpass all baselines (SNN-TextCNN and Directly-trained Spikformer) in all datasets. This comparison underscores the reasonably good performance of our model.\n\n[1]Maass W. Networks of spiking neurons: the third generation of neural network models[J]. Neural networks, 1997, 10(9): 1659-1671.\n\n[2]Lv C, Xu J, Zheng X. Spiking Convolutional Neural Networks for Text Classification[C]. // The Eleventh International Conference on Learning Representations. 2022.\n\nIf you have any further concerns, please let us know and we will do our best to address your concerns! Thank you very much :-D"
                    },
                    "title": {
                        "value": "Further Discussion on the Response of Reviewer sLn7"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700161501943,
                "cdate": 1700161501943,
                "tmdate": 1700544613829,
                "mdate": 1700544613829,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oFS8M6kCvx",
                "forum": "6c4gv0E9sF",
                "replyto": "oAK32ySLBZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer sLn7"
                    },
                    "comment": {
                        "value": "Dear Reviewer sLn7,\n\nWe sincerely appreciate your thorough review and the valuable comments you provided for our paper.\nWe have carefully considered each point and have addressed them in detail in our rebuttal.\nFurthermore, we have implemented several improvements in our revised manuscript based on all reviewers' suggestions. \nSpecially, according to your suggestions, (1) we have reported standard deviations and bolded the numbers of fine-tuned BERT in Table 1; (2) we have added our research motivations in Appendix H, including what is spiking neural network, why is it important, and our motivations; (3) we have updated the mathematical notations in Section 3.\n\nAs the Author-Review Discussion period is drawing to a close with only two days remaining, we would like to ensure that all your concerns have been adequately addressed.  If there are any questions or unresolved issues, we are eager to provide further clarification or make necessary revisions.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544383802,
                "cdate": 1700544383802,
                "tmdate": 1700555164935,
                "mdate": 1700555164935,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4vWkGkB2jp",
                "forum": "6c4gv0E9sF",
                "replyto": "oAK32ySLBZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sLn7's concerns on our paper's benefits for the broader ICLR community"
                    },
                    "comment": {
                        "value": "Dear Reviewer sLn7,\n\nThank you very much for taking the time to carefully review my submission and providing valuable feedback.\nWe understand your uncertainty regarding the broader benefits of our paper on the intersection of Spiking Neural Networks (SNN) and Natural Language Processing (NLP) for the ICLR community.\nWe would like to clarify the value and contributions of our research to the broader ICLR community through the following points:\n\n**1. Innovative Interdisciplinary Fusion**\n\nOur study focuses on brain-inspired computing (especially on SNNs), a highly relevant and cutting-edge research area.\nActually,  the track of our paper is **\"applications to neuroscience & cognitive science\"**.\nBy delving into the intersection of **neuroscience** and **computer science**, we aim to introduce a biologically-plausible and more effective paradigm for information processing to the ICLR community, which may pave the way for more sustainable and efficient AI solutions.\nWe would like to emphasize that, our ongoing research, including the development of SpikeBERT, is part of a broader initiative aimed at demonstrating the potential of spiking neural networks and facilitating their adoption in mainstream AI applications.\n\n**2. Potential Advantages for Other Reseach Topics**\n\nThe combination of SNNs and NLP may offer potential advantages in handling temporal information, simulating neural activity, and addressing practical issues of interest to the ICLR community.\nFor example, in our responese Q6 to Reviewer is3e, we mention that how to utilize SNNs to process **time series tasks** may be a good reseach topic.\n\nWe hope these points clarify the benefits of our research to the ICLR community.\nIf our explanations meet your expectations, we kindly request you to reflect these clarifications in your scoring.\nYour feedback is highly valued, and we sincerely appreciate the time and attention you have dedicated to reviewing our work.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732833830,
                "cdate": 1700732833830,
                "tmdate": 1700733720810,
                "mdate": 1700733720810,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XY5CGG5bka",
                "forum": "6c4gv0E9sF",
                "replyto": "4vWkGkB2jp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4742/Reviewer_sLn7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4742/Reviewer_sLn7"
                ],
                "content": {
                    "comment": {
                        "value": "I am sorry but I think you just have to accept my decision and stop sending me response after response."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735645664,
                "cdate": 1700735645664,
                "tmdate": 1700735645664,
                "mdate": 1700735645664,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZmquCbQ2JX",
            "forum": "6c4gv0E9sF",
            "replyto": "6c4gv0E9sF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4742/Reviewer_17Yy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4742/Reviewer_17Yy"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes SpikeBERT, a spiking BERT model designed for language tasks based, and describes a two-stage distillation method employed in its training.\n\nThe authors conduct experiments on several text classification tasks on English and Chinese datasets. The results show that SpikeBERT outperforms state-of-the-art spiking neural networks and achieves comparable results to BERT on text classification tasks for English and Chinese, while consuming significantly less energy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors provide the necessary background on spiking neural networks (SNNs) and Spikformer architecture.\n\n2. This work is the first Transformer-based SNNs for language tasks, and achieve state-of-the-art performance on text classification tasks.\n\n3. The authors present an ablation analysis for all their contributions, and compare SpikeBERT with other BERT variants like TinyBERT and DistilBERT on Appendix."
                },
                "weaknesses": {
                    "value": "1. Although SNNs can reduce the energy consumption when inference, the proposed two-stage distillation method may lead to more energy costs when training. Can you explain this matter?\n\n2. In Figure 3(b), it seems there are no emergent abilities in the SpikeBERT, which is different from non-spiking large language models."
                },
                "questions": {
                    "value": "Q: I wonder why the authors choose BERT as their teacher model. \n\nIf the authors can respond reasonably to all my questions and comments, I will improve the score of this manuscript."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4742/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4742/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4742/Reviewer_17Yy"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4742/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698395132330,
            "cdate": 1698395132330,
            "tmdate": 1699889417496,
            "mdate": 1699889417496,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HqBPAuSeMh",
                "forum": "6c4gv0E9sF",
                "replyto": "ZmquCbQ2JX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4742/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments!\n\n**Q1: The proposed two-stage distillation method may lead to more energy costs when training. Can you explain this?**\n\nA1: For spiking neural networks, the energy consumption is mainly reduced at the inference time. Once spiking neural networks (SNNs) are well software-trained, they can be deployed on neuromorphic hardware (such as 45nm neuromorphic hardware[1]) for energy-efficient computing (binary-value computing). However, mature on-chip training solutions are not yet available, and it remains a great challenge due to the lack of efficient training algorithms, even in a software training environment. Thank you for pointing it out.\n\n**Q2:** **It seems** **there are no emergent abilities in the SpikeBERT.**\n\nA2: Firstly, the emergent ability of large language models was proposed by [2]. Large language models usually refer to large **generative** language models, which are mostly decoder-only and mainly used for text generation, such as ChatGPT. SpikeBERT is an encoder-only language **representation** model for language understanding tasks, which is similar to BERT. However, SpikeBERT can be easily extended to the decoder-only models because they are all Transformer-based.\n\nSecondly, as discussed in Section 4.5, the reason why the accuracy of SpikeBERT is generally insensitive to the model depths is that the gradients error may accumulate with the increase of model depths due to the surrogate gradients. What\u2019s more, the depths we used on the experiments were only 8, 12 and 18. Should you wish to explore the potential performance fluctuations across a broader range of depths, we are readily prepared to conduct additional experiments at your behest.\n\n**Q3:** **Why the authors choose BERT as their teacher model?**\n\nA3: Our approach is applicable to any model similar to BERT or RoBERTa. Existing literature[3] indicates that BERT and RoBERTa exhibit minimal substantive differences in downstream tasks, and both models share a similar network architecture. The divergence between these models is primarily observed in the aspect of masking strategy, input tokenization and training strategy [4], yet these differences do not impact the conclusions drawn in this paper. What\u2019s more, as BERT serves as a representative example of large pre-trained models, we chose BERT as the teacher model. \n\n \n\n[1] Horowitz M. 1.1 computing's energy problem (and what we can do about it)[C]. // 2014 IEEE international solid-state circuits conference digest of technical papers (ISSCC). IEEE, 2014: 10-14.\n\n[2] Wei J, Tay Y, Bommasani R, et al. Emergent abilities of large language models[J]. arXiv preprint arXiv:2206.07682, 2022.\n\n[3] Qiu X, Sun T, Xu Y, et al. Pre-trained models for natural language processing: A survey[J]. Science China Technological Sciences, 2020, 63(10): 1872-1897.\n\n[4] Liu Y, Ott M, Goyal N, et al. Roberta: A robustly optimized bert pretraining approach[J]. arXiv preprint arXiv:1907.11692, 2019."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699875773058,
                "cdate": 1699875773058,
                "tmdate": 1699875773058,
                "mdate": 1699875773058,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QpzFa46NL9",
                "forum": "6c4gv0E9sF",
                "replyto": "HqBPAuSeMh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4742/Reviewer_17Yy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4742/Reviewer_17Yy"
                ],
                "content": {
                    "comment": {
                        "value": "Thx!  I believe the author has addressed all my concerns very well. I think it's a high-quality work and deserves to be accepted."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699889437090,
                "cdate": 1699889437090,
                "tmdate": 1699889437090,
                "mdate": 1699889437090,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JOZ3yI5MQ2",
            "forum": "6c4gv0E9sF",
            "replyto": "6c4gv0E9sF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4742/Reviewer_is3e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4742/Reviewer_is3e"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose SpikeBERT, a spiking neural network (SNN) architecture for language tasks. SpikeBERT extends and improves Spikformer architecture to process text instead of images. It replaces certain modules in Spikformer to make it suitable for language tasks.\nThe approach uses a two-stage knowledge distillation method to train SpikeBERT: First stage is pre-training distillation using a large unlabeled corpus to align embeddings and features. Second stage is task-specific distillation using a fine-tuned BERT on a downstream task as teacher. The model is evaluated on 6 English and Chinese text classification datasets: it outperforms prior SNN methods and achieves comparable accuracy to BERT. The estimated theoretical energy consumption is much lower for SpikeBERT as compared to traditional approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Advantages:\nUses a highly scalable Transformer-based architecture as the backbone and outperforms prior SNN methods by 3.49% on average across 6 datasets.\nTwo-stage distillation allows pre-training on large unlabeled data.\nFeature alignment loss aligns hidden representations.\nData augmentation further facilitates distillation.\nEvaluated on diverse English and Chinese datasets: works well for both English and Chinese text classification.\nSignificantly reduces theoretical energy consumption (by 27.82% compared to fine-tuned BERT).\n\nThe claims are reasonably supported by the results. The proposed SpikeBERT outperforms prior SNN methods significantly and achieves comparable accuracy to BERT on multiple datasets. Ablation studies provide insights into model architecture and training."
                },
                "weaknesses": {
                    "value": "Potential weaknesses include:\nThe method relies on the teacher ANN, so can not learn directly from the data.\nThe method does not address zero-shot generalization to novel language tasks, which is the main appeal of the LLMs.\nFails to capture fine-grained word semantics well.\nRequires GPUs with large memory due to additional time dimension.\nEnergy reduction based on theoretical estimates, actual hardware measurements would be more compelling."
                },
                "questions": {
                    "value": "The approach was evaluated on datasets created for ANNs, not neuromorphic data. It would be interesting to consider using e.g. a neuromorphic cochlea for speech signal.\nAdding scaling experiments would be helpful - trying bigger versions of SpikeBERT with more layers, heads and timesteps to explore the scaling law.\nIt would be helpful to provide visualizations of the learned spike patterns to offer insights into model operation and interpretability.\nHow would the chioce of alternate surrogate gradient functions would impact training convergence and accuracy?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4742/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698720431546,
            "cdate": 1698720431546,
            "tmdate": 1699636456103,
            "mdate": 1699636456103,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CBU9TnSkZh",
                "forum": "6c4gv0E9sF",
                "replyto": "JOZ3yI5MQ2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4742/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments!\n\n**Q1:** **The method relies on the teacher ANN, so can not learn directly from the data.**\n\nA1: As a matter of fact, we have tried to directly train our SpikeBERT on downstream tasks, and have reported its performance in Table1 (\u201cDirectly-trained Spikformer\u201d row). However, as discussed in Section 1, **deep** spiking neural networks (SNNs) directly trained with backpropagation through time using surrogate gradients will suffer from the problem of gradient vanishing or exploding due to \u201cself-accumulating dynamics\u201d. \n\nPrevious SNN work[1] on natural language processing used a simple network TextCNN as their backbone. However, our SpikeBERT is the first Transformer-liked SNN for language tasks, whose scale is the same as the scale of BERT. Therefore, we choose to use knowledge distillation for training our SpikeBERT so that the deviation of surrogate gradients in spiking neural networks will not be rapidly accumulated.\n\n**Q2:** **The method does not address zero-shot generalization to novel language tasks, which is the main appeal of the LLMs.**\n\nA2: Zero-shot capability refers to the generation capability of language **generative** model, which are mostly decoder-only and mainly used for text generation, such as GPT3 and ChatGPT. Our SpikeBERT is an encoder-only spiking language **representation** model for language understanding tasks, which is similar to BERT. However, SpikeBERT can **be easily extended** to the decoder-only models because they are all Transformer-based. Now we are focusing on how to implement spiking version of GPT-liked language model, which may lead to future energy-efficient implementations of large language models.\n\n**Q3:** **SpikeBERT fails to capture fine-grained word semantics well.**\n\nA3: The reviewer may concern the ability of SpikeBERT to capture fine-grained word semantics due to the spiking property. Inspired by your comments, we design a set of experiments to show how well SpikeBERT can capture word semantics. For Transformer-liked models, the self-attention module is the key for capturing fine-grained word semantics. To prove our SpikeBERT has successfully captured the word semantics, we will conduct a visualization experiment on **attention map** in spiking-self-attention(SSA) module (See Q8).\n\n**Q4: SpikeBERT requires GPUs with large memory due to additional time dimension.**\n\nA4: Due to the additional time dimension T, SpikeBERT indeed requires large memory on GPUs when **training**. However, once spiking neural networks (SNNs) are well software-trained, they can be deployed on neuromorphic hardware (such as 45nm neuromorphic hardware[2]) for energy-efficient **inference** (binary-value computing), which is GPU-free.\n\n**Q5: Energy reduction based on theoretical estimates, actual hardware measurements would be more compelling.**\n\nA5: Our theoretical energy consumption calculation in Appendix D is based on a classic 45 nm brain-inspired neuromorphic hardware[2], which has been mass-produced. Therefore, theoretical estimates of many previous works like SNN-TextCNN[1] and Spikformer[3] are all based on this chip. We follow them to prove our SpikeBERT is energy-efficient.\n\n**Q6:** **It would be interesting to consider using e.g. a neuromorphic cochlea for speech signal.**\n\nA6: This is a very promising insight! And after careful consideration, we think that speech data is born with time step dimension T, which may be very suitable for SNNs! However, after conducting a meticulous literature review these days, we find current works[4][5][6] on \u201cSNNs + time series tasks\u201d are not that promising (a little trivial). We think how to process time series data by SNNs remains to be researched. Thank you!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699875861410,
                "cdate": 1699875861410,
                "tmdate": 1699875861410,
                "mdate": 1699875861410,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qf6RbzJx5Y",
                "forum": "6c4gv0E9sF",
                "replyto": "JOZ3yI5MQ2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4742/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q7:** **Adding scaling experiments would be helpful.**\n\nA7: It is a good suggestion. In Fig 3(a)(b), we conducted preliminary experiments to increase the time step T and the number of model layers. But the conclusion shows that with the increase of these two hyper-parameters, the performance gradually stops increasing. We are willing to add more hyper-parameter experiments, but model training may take too long time (we can\u2019t finish all these experiments within 2 weeks), so we may add these results in our camera-ready version upon acceptance. We sincerely hope you can understand us on this matter. Thank you!\n\n**Q8:** **It would be helpful to provide visualizations of the learned spike patterns to offer insights into model operation and interpretability.**\n\nA8: It is a good suggestion. We will visualize the **attention map** in spiking-self-attention(SSA) module. We will report the experiment results and the corresponding visualizations in our revised manuscript once the experiment is completed. Thanks for pointing it out :)\n\n**Q9: How would the chioce of alternate surrogate gradient functions would impact training convergence and accuracy?** \n\nA9: We have followed previous SNN works[1][3][7] to choose the widely-used Arctangent-like surrogate gradient function. It may take so much time to train SpikeBERTs with different surrogate gradient functions that we can not complete all experiments before the rebuttal deadline. We may report these results in our camera-ready version upon acceptance.\n\n\n[1] Lv C, Xu J, Zheng X. Spiking Convolutional Neural Networks for Text Classification[C]. // The Eleventh International Conference on Learning Representations. 2022.\n\n[2] Horowitz M. 1.1 computing's energy problem (and what we can do about it)[C]. // 2014 IEEE international solid-state circuits conference digest of technical papers (ISSCC). IEEE, 2014: 10-14.\n\n[3] Zhou Z, Zhu Y, He C, et al. Spikformer: When Spiking Neural Network Meets Transformer[C]//The Eleventh International Conference on Learning Representations. 2022.\n\n[4] Fang H, Shrestha A, Qiu Q. Multivariate time series classification using spiking neural networks[C]//2020 International Joint Conference on Neural Networks (IJCNN). IEEE, 2020: 1-7.\n\n[5] Sharma V, Srinivasan D. A spiking neural network based on temporal encoding for electricity price time series forecasting in deregulated markets[C]//The 2010 international joint conference on neural networks (IJCNN). IEEE, 2010: 1-8.\n\n[6] Gaurav R, Stewart T C, Yi Y. Reservoir based spiking models for univariate Time Series Classification[J]. Frontiers in Computational Neuroscience, 2023, 17: 1148284.\n\n[7] Fang W, Yu Z, Chen Y, et al. Incorporating learnable membrane time constant to enhance learning of spiking neural networks[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2021: 2661-2671."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699875909120,
                "cdate": 1699875909120,
                "tmdate": 1700116455876,
                "mdate": 1700116455876,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]