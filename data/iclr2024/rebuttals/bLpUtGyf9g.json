[
    {
        "title": "Boundary Denoising for Video Activity Localization"
    },
    {
        "review": {
            "id": "LE5TFQUpP6",
            "forum": "bLpUtGyf9g",
            "replyto": "bLpUtGyf9g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6486/Reviewer_61UD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6486/Reviewer_61UD"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method for boundary denoising to tackle video activity localization. A model architecture DenoiseLoc is proposed, together with a boundary denoising training method. The authors argue that a single step denoising is better than the diffusion process with multiple steps. Experiments show some improvement over the previous state-of-the-art."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The experimental results show some improvements over the previous state-of-the-art."
                },
                "weaknesses": {
                    "value": "Most importantly, the method part is not clear. It is rather hard to follow through most of the written parts. Figure 2\u2019s caption also does not provide a clear overview of the proposed method and the novel aspects.\n\n\nIn Figure 2, it is rather confusing what the pipeline actually is. For instance, the ground truth span/noise injection part should definitely not be part of the inference pipeline. So, it is not clear what is done during the inference process.\n\n\nSuddenly, in 3.2.2, the dynamic convolution is used without much explanation. Why is it important to the proposed design? What is the dynamic convolution exactly doing, and why no other design can be used? It is not well motivated.\n\n\nThe boundary denoising training part in 3.2.3 is not clear at all. How the method works, what loss is used, where the loss is applied, and why it is designed this way is not clear. Why do we need to divide into two different groups? How does the model use both of them during training?\n\nImportantly, since boundary denoising has been widely explored, what are the further insights that make the proposed method more effective than previous works? This has not been clearly expressed.\n\n\n\n\nExperimentally, there are also some parts not well established.\n\nMost importantly, it is very strange to me why adding diffusion will lead to performance drops. Furthermore, the more steps used, the worse the performance seems to get. This is totally different from what is usually observed in many diffusion-based works (for generation and for prediction tasks). Usually, the benefit of using a single step is only for efficiency purposes. Furthermore, the given reason is also not convincing. It would be good if the authors provide a lot more details about how diffusion is used, and more qualitative/quantitative evidence to substantiate this claim, since it is quite a strong and counterintuitive claim.\n\n\nIt seems that more ablations are required for various designs, for example the various designs in denoising training. But, currently the method is too unclear for me to suggest concrete settings.\n\n\n\n\n\n\n\nNote that there are some mistakes with the spelling/formatting. This does not affect the score. Some examples are:\n\nPg 2 bottom: citation formats are mixed up, and all the authors of the papers are listed in the citation.\nPg 1 and Pg 2: \u201cQV-Hightlights\u201d\nPg 9: \u201cprediction the denoised proposal\u201d, \u201cmore discussion\u201d\n\nThroughout, please standardize as DenoiseLoc (instead of denoiseloc at several parts)."
                },
                "questions": {
                    "value": "Apart from the above concerns, some other specific questions are below.\n\n\n1)\tCould the authors provide the time gains from using a single denoising step against multiple?\n2)\tCould the authors provide the model size and time gains as compared to previous state-of-the-art methods?\n3)\tIn table 4, when the dB measure for noise is used, what exactly does it mean in this context?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6486/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698250675033,
            "cdate": 1698250675033,
            "tmdate": 1699636726712,
            "mdate": 1699636726712,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UkoJdaprMY",
                "forum": "bLpUtGyf9g",
                "replyto": "LE5TFQUpP6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6486/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6486/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**General Answer** \n\nWe sincerely thank the reviewer for his time dedicated to assessing our work. While we understand that our manuscript may have been challenging to comprehend, we appreciate the reviewer's feedback and have attempted to address the confusion in the revision. We would like to point out that other reviewers have all expressed positive feedback regarding the presentation of the idea. Nonetheless, we are very receptive to constructive and concrete suggestions that can improve clarity and readability and improve our submission. \nFollow a general answer tackling the presented weaknesses. In the next comment box, we have a more discussion on Denoising vs Diffusion, and also answer the specific questions asked by the reviewer. Note that all changes to the manuscript are highlighted in blue in the revised version for the convenience of the reviewers.\n\n**1- Figure 2.** We revised the caption of Figure 2. The reviewer can verify the change in the updated manuscript. We report here the updated text for ease of reference. \n\n\u201cOur proposed model consists of three main components: Encoder, Decoder, and Denoising Training. The input video and text are transformed into video and text embeddings by means of pretrained networks, then concatenated and fed to the Encoder module (left). The output features from the Encoder, termed as memory, are subsequently routed to the Decoder. During training, the Decoder receives in input a set of learnable embeddings, ground truth spans with additional artificial noise, and a set of learnable spans. The objective of the decoder is to enrich instance-level features computed based on the two sets of spans and refine (via denoising) the provided ground truth spans. Note that the GT spans are not used during inference. \u201d\n\nWe would like to clarify that the pipeline refers only to the training phase. The behavior of the model at inference time is presented in the manuscript in Section 3.2, where we explain that the Denoise is disabled during inference. The decoder is the module that refines the learnable temporal spans used as predictions. Further details are present in Section 3.2.3, where we detail the boundary denoising training. \n\n\n**2- DynamicConv.** We would like to clarify that DinamicConv is not a contribution of our work. It was in fact, introduced in Sparse-R-CNN [1] and was borrowed as is. We adopt this computation block in pursuit of a good Decoder design and ablate several configurations in Section 4.5. Therefore, DynamicConv is not a necessary block for the correct functioning of our method, yet it is clearly shown to help achieve the best performance in the reported ablation in Table 5, as the module operation is more sensitive to the temporal feature extracted from the temporal span. Nonetheless, we would like to reinforce that our proposed method brings improvements even when such a block is not utilized. See Tab 5, row 1. \n\nWe are happy to provide further clarification if the reviewer deems it necessary.\n\n[1] Sun, Peize, et al. \"Sparse r-cnn: End-to-end object detection with learnable proposals.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n\n\n**3- Section 3.2.3.** We reorganized this section based on the reviewer's feedback. Please directly check it from our updated version.\n\n\n**4- Missing literature.** We would like to kindly invite the reviewer to provide us with a list of relevant literature papers exploring the temporal boundary denoising problem in video localization tasks that were available before the submission of this manuscript. We believe he might find this literature lacking, reason why we decided to focus our effort in this direction. To the best of our knowledge, the relevant literature is included, however, we acknowledge that one might always miss some recent and promising works. We are happy to include missing literature in our related work and compare performance where possible. \n\n**5- Evaluation protocols.** We would like the reviewer to point out the non-standard evaluation protocols involved in this manuscript. We base our work and evaluation code on the Moment-DETR repository. Please also note that the test-split annotation is not open to the public, so we submit our predictions to their server for a fair comparison. We are, however, open to further entertaining a discussion regarding this topic with the reviewer to clarify any doubts.\n\n\n**6- Additional ablations.** We kindly ask the reviewer to provide actionable suggestions on the ablation designs. This would greatly help us in narrowing down the relevant experiments, helping us in designing and analyzing the results against potential expectations the reviewer might have."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528893371,
                "cdate": 1700528893371,
                "tmdate": 1700528893371,
                "mdate": 1700528893371,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bgCHdp7bsJ",
            "forum": "bLpUtGyf9g",
            "replyto": "bLpUtGyf9g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6486/Reviewer_6dfr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6486/Reviewer_6dfr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed an encoder-decoder model, namely DenoiseLoc, for video activity localization. DenoiseLoc introduces a boundary-denoising paradigm to address the challenge of uncertain action boundaries. DenoiseLoc leverages across modalities in the encoder and progressively refines learnable proposals and noisy ground truth spans in decoder layers. Extensive experiments on standard benchmarks demonstrate the effectiveness of the proposed DenoiseLoc."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- A novel boundary-denoising paradigm is proposed to address the challenge of uncertain action boundaries in video activity localization task.\n- Extensive experiments on standard benchmarks demonstrate the effectiveness of the proposed DenoiseLoc.\n- It is interesting to find that satisfactory performance can be achieved with very few proposals and very few denoising steps."
                },
                "weaknesses": {
                    "value": "-  Lack of visual analysis. It would be helpful to understand the properties of the proposed method if some cases can be visually analyzed.\n- \"DenoiseLoc\" and \"denoiseloc\" are used interchangeably, which confuses readers. It is recommended that all be changed to \"DenoiseLoc\"."
                },
                "questions": {
                    "value": "Please refer to Weaknesses for more details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6486/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698421603641,
            "cdate": 1698421603641,
            "tmdate": 1699636726572,
            "mdate": 1699636726572,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SlrSMwl1y9",
                "forum": "bLpUtGyf9g",
                "replyto": "bgCHdp7bsJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6486/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6486/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**General Answer** \n\nWe sincerely thank the reviewer for his time dedicated to assessing our work, acknowledging its relevance, and providing suggestions on how to strengthen it. We are excited the reviewer has found our work technically robust and well-presented. Moreover, the reviewer's belief that our work makes a valuable contribution to the community is especially gratifying.\n\nIn response to the feedback, we have revised our manuscript to ensure consistency in the naming of our model throughout the document. Note that all our changes are highlighted in blue in the revised manuscript for the convenience of the reviewers.\n\nFurthermore, we are very receptive to the idea of including additional visualizations and would welcome any specific suggestions the reviewer may have in this regard. Currently, we provide an Illustration of the boundary denoising process in Figure 1, where we showcase that we can effectively achieve more accurate temporal boundaries through the cascade of denoiser blocks. This figure showcases the efficacy and cascade effect of the denoising tower. Additionally, in the Appendix (Figure 5), we offer visual representations of the solution process for a specific video-query pair, visually illustrating the task.\n\nWe are happy to provide additional visualizations following the reviewer's suggestions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528220253,
                "cdate": 1700528220253,
                "tmdate": 1700528220253,
                "mdate": 1700528220253,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JCgigvMzWM",
            "forum": "bLpUtGyf9g",
            "replyto": "bLpUtGyf9g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6486/Reviewer_uvci"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6486/Reviewer_uvci"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles an important and common challenge of boundary ambiguity in the video action localization task. The authors adopt the encoder-decoder framework as DETR for embedding video/caption features and predicting the boundary locations. The proposed denoiseloc aims at regress more precise boundaries by noise injection. Extensive experiments to demonstrate the effectiveness of denoiseloc."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The inspiration of boundary-denoising training approach has good novelty.\n+ This paper is well-organized and the proposed method achieves good performance."
                },
                "weaknesses": {
                    "value": "- This paper uses a complex symbol system, which makes it difficult to read. \\epsilon presents the number of fixed consecutive frames and then it presents a vector of a span. n, which represents a quantitative index, is sometimes a subscript and sometimes a superscript.\n- The process of denoising is unclear. Which loss function is used for boundary denoising? The core technology is a proposal augmentation strategy to obtain more candidate proposals for training?\n- Missing related works of boundary ambiguity and temporal grounding.\n\nWang Z, Gao Z, Wang L, et al. Boundary-aware cascade networks for temporal action segmentation[C]. ECCV2020.\n\nXia K, Wang L, Zhou S, et al. Learning to refactor action and co-occurrence features for temporal action localization[C]. CVPR2022.\n- Typo. L5 of Sec. 3.2."
                },
                "questions": {
                    "value": "- What is the definition of an action span or temporal span?\n- What do 0.25 and 0.75 mean in the Sec. 3.2.3? Negative proposal set is from the inside or outside of the ground truth?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6486/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6486/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6486/Reviewer_uvci"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6486/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698475156138,
            "cdate": 1698475156138,
            "tmdate": 1700738230217,
            "mdate": 1700738230217,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LqbuPKS1MY",
                "forum": "bLpUtGyf9g",
                "replyto": "JCgigvMzWM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6486/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6486/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**General Comment**\n\nWe thank the reviewer for his time dedicated to assessing our work. We are pleased the reviewer acknowledged the soundness of our proposed solution and our presentation. Moreover, the reviewer's belief that our work makes a valuable contribution to the community is especially gratifying.\n\nIn response to the concerns raised, we have incorporated the relevant citations in Sections 1 and 2, as suggested by the reviewer. Furthermore, we have addressed the mentioned typo and made a few additional corrections. We have also refined the notation in Section 3 of the manuscript to enhance clarity. We assure the reviewer that the adjustments are minor and do not impact any of the equations. We kindly invite the reviewer to review these modifications, which encompass all the suggestions and hope that they will find these changes to be satisfactory. Note that all our changes are highlighted in blue in the revised manuscript for the convenience of the reviewers.\n\nAdditionally, we would like to clarify the boundary-related losses adopted in this work, and presented in Section 3.3, are L1 loss and gIoU loss. These losses directly affect and guide the denoising process.\n\nFinally, our approach can be perceived as an advanced form of proposal augmentation. However, it is important to emphasize that the core concept involves controlling the noise level and adjusting the ground truth accordingly while training the model to predict the original temporal span under these specific conditions.\n\n**Question 1: Temporal span definition.**\n\nWe are pleased to inform the reviewer we added a short sentence in the problem formulation to formalize the definition of temporal span. We have also redacted the document to only use the naming \u201ctemporal span,\u201d removing any instance of \u201caction span.\u201d We are sure this improved clarity. We thank the reviewer for the suggestion. \n\n**Question 2: Clarification from Sec. 3.2.3.**\n\nWhen creating negative temporal spans, for simplicity, we formulate the set as adding noise to a fixed proposal with temporal span (0.5, 0.5) in center-width coordinates. These coordinates translate to (0.25, 0.75) in (t_start, t_end) convention. We find this simple strategy to be effective and robust to different levels of noise."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528053202,
                "cdate": 1700528053202,
                "tmdate": 1700528053202,
                "mdate": 1700528053202,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MbEtY1maM2",
                "forum": "bLpUtGyf9g",
                "replyto": "LqbuPKS1MY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6486/Reviewer_uvci"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6486/Reviewer_uvci"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal Comment"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their meticulous explanation and clarification. The author's modification helps the reader to understand the method of this paper better. Finally, l raise my score to 6."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738180189,
                "cdate": 1700738180189,
                "tmdate": 1700738180189,
                "mdate": 1700738180189,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "T1qRy9Ldsf",
            "forum": "bLpUtGyf9g",
            "replyto": "bLpUtGyf9g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6486/Reviewer_UMQX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6486/Reviewer_UMQX"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the problem of video activity localization, specifically given language descriptions. The main challenge of this task is boundary ambiguity caused by the annotator subjectivity and the smoothness of temporal events. To this end, the authors design a novel framework, named denoiseloc, aiming to progressively refine the moment predictions. To facilitate the model training, boundary-denoising training scheme is adopted, which encourages the decoder to reconstruct ground truths from the noisy moments. In the experiments on two benchmarks, MAD and QVHighlights, the effectiveness of the proposed method is validated."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper is well-written and easy to follow with good readability.\n+ The figures well represent the proposed method, helping the understanding.\n+ The proposed approach surpasses the strong competitors on both benchmarks.\n+ The comparison between Denoiseloc and Diffuseloc is interesting, and brings valuable insights."
                },
                "weaknesses": {
                    "value": "- Some important details of the method are missing. In its current form, the information about the model is insufficient in the manuscript.\n\n(1) DETR-like approaches conventionally adopt the moment representation of (center, width). In contrast, the authors stated that they express a moment as start and end. In this case, the start position can be predicted as a larger value than the end position. How do the authors handle this?\n\n(2) The details of temporal ROI alignment and DynamicConv layer are missing. I would like to suggest the authors to include graphical illustrations of these operations at least in the appendix for help the understanding.\n\n(3) In the boundary-denoising process, the model generates two types of noise-injected proposals, i.e., positive and negative sets. To my understanding, the proposals in the positive set have their corresponding ground truths by design, so the model learns to recover the ground-truth moments from them. However, there is a lack of explanations about the role of the proposals in the negative set. Are they also used to recover ground truths? Or do they serve as negative samples for classification? If the former is the case, how is the matching performed? In addition, what happens if they overlap with ground truths? Will it disturb the training?\n\n- The comparisons with existing DETR-approaches seem not fair. To my knowledge, the DETR-based approaches (e.g., Moment-DETR and UMT) leverage four encoder layers and two decoder layers with a total of 10 queries on QVHighlights. On the other hand, the proposed architecture utilizes (at most) four times more encoder/decoder layers than those of the competitors, and three times more moment queries than those of the competitors. This makes it unclear whether the performance gains come from increased parameters or the proposed algorithm, and it is highly encouraged to perform comparisons under the same setting. In addition, comparisons on the computational cost and the memory consumption will be beneficial. Meanwhile, one of the state-of-the-art method, QD-DETR [1], is missing in the comparison table. If included, the proposed method shows inferior performances even with more layers and more queries.\n\n[1] Moon et al., \u201cQuery-Dependent Video Representation for Moment Retrieval and Highlight Detection\u201d, CVPR, 2023."
                },
                "questions": {
                    "value": "Please refer to the Weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6486/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6486/Reviewer_UMQX",
                        "ICLR.cc/2024/Conference/Submission6486/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6486/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839780699,
            "cdate": 1698839780699,
            "tmdate": 1700733881569,
            "mdate": 1700733881569,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iIEHQd5GoV",
                "forum": "bLpUtGyf9g",
                "replyto": "T1qRy9Ldsf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6486/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6486/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**General Comment**\n\nWe sincerely appreciate the reviewer's thorough assessment of our manuscript and are happy to hear that they found our proposed method both sound and our presentation of high quality. It is positive to know that our experimental setup effectively demonstrates the superiority of our method over current state-of-the-art approaches and that no further essential ablation studies are required. Moreover, the reviewer's belief that our work makes a valuable contribution to the community is especially gratifying.\n\nBelow, we have addressed each of the concerns raised. We hope these responses provide comprehensive clarifications. Should there be any further queries or need for additional clarification, we warmly invite the reviewer to reach out. Note, that all our changes are highlighted in blue in the revised manuscript for the convenience of the reviewers.\n\n**1 - Temporal span representation.**\n\nThe reviewer is concerned about a possible invalid temporal span due to noise injection. First, we would like to clarify that our manuscript uses the start-end convention (t_start, t_end) for temporal span representation, aligning with standard practices in the field. This choice was made for consistency with the literature on this task. \n\nIn our implementation, these temporal spans are initially expressed as (t_start, t_end) and then converted into a center-width (center, width) format before adding the noise.  To prevent the generation of empty/invalid temporal spans, the width is clamped to a minimum of 1e-4 after adding noise. Furthermore, our implementation of Region of Interest (ROI) alignment can ensure that only valid features are computed to extract proposal features, even in cases where the span width exceeds the video duration.\n\n\nFor a more detailed understanding, we invite the reviewer to consult the code provided in the supplementary material if deemed necessary. Particular attention may be directed to the functions span_cxw_to_xx() and span_cxw_to_xx() utilized during the forward pass operations, as illustrated in files code/model.py:line196, code/model.py:line214.\n\n\nIt is also important to note that before the computation of the loss, the temporal coordinates are converted back to the original (t_start, t_end) format. While the loss computation could, in theory, be performed in the center-width format, we consider this a matter of implementation detail and chose the current approach for consistency and clarity.\n\n\n\n**2- ROI alignment and DynamicConv.**\n\nWe would like to clarify that these two modules were not introduced in this work but were adopted from previous work. We are happy to reference further the original manuscripts that introduce them. \nDynamicConv is directly borrowed from Sparse-R-CNN [1], while ROI alignment was initially introduced in its 2D form in Mask-R-CNN [2] (Figure 3) and successively generalized for 1D signals in [3]. We have updated our Appendix in Section G to include graphical visualizations of the two modules. \n\n\n**3- Boundary-denoising process. **\n\nWe thank the reviewer for the opportunity to elaborate on the role of negative samples in our boundary-denoising process and have taken this chance to enrich the manuscript with additional details in Section 3.2.3.\n\nThe distinction between positive and negative samples is only relevant when generating noisy temporal spans. For positive samples, noise is added to the spans annotated in the dataset. Conversely, for negative samples, noise is introduced to predetermined temporal spans. Following this, both sets are combined and processed through the transformer decoder, as detailed in Section 3.2.3.\n\nThis unified set then undergoes our denoising process before being presented to the Hungarian Matcher for loss computation. Notably, during this stage, negative samples have little probability of being paired with growth truths. Such pairings, when they occur, are effectively penalized by our chosen loss functions.\n\nThe reviewer cleverly noted the possibility of negative samples being denoised in a manner that causes them to overlap with positive ones. While this is a possibility, we would like to emphasize that, in practice, we have not observed this phenomenon to disrupt the training process.\n\n**4- Comparison Fairness.**\n\nWe have structured our response to the reviewer's concern regarding comparison fairness into distinct parts."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527568632,
                "cdate": 1700527568632,
                "tmdate": 1700527568632,
                "mdate": 1700527568632,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xM8c5HxNFo",
                "forum": "bLpUtGyf9g",
                "replyto": "T1qRy9Ldsf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6486/Reviewer_UMQX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6486/Reviewer_UMQX"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal Comment"
                    },
                    "comment": {
                        "value": "I thank the authors for providing the careful response.\n\nThe added details and graphical illustrations are indeed helpful, although I am not sure whether such a large modification in the manuscript is allowed during the rebuttal.\n\nAfter all, most of my concerns are addressed, but I agree with some points of Reviewer 61UD on the other hand.\n\nHence, I will raise my score to 6, yet still remaining on the borderline."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733866396,
                "cdate": 1700733866396,
                "tmdate": 1700734131264,
                "mdate": 1700734131264,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]