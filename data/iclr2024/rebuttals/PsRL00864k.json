[
    {
        "title": "Correct and speak: accent reduction with minimum supervision"
    },
    {
        "review": {
            "id": "SRB9Mte54G",
            "forum": "PsRL00864k",
            "replyto": "PsRL00864k",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8861/Reviewer_Q9U6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8861/Reviewer_Q9U6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a seq-to-seq accent conversion model between Indian and native English. One key idea of the paper is that there should be a semantic token corrector before generating the target audio as there will be some pronunciations in the non-native source audio. The system performs semantic token correction on the HuBERT units and utilizes TFCodec as input to an auto-regressive decoder to generate the target audio. \nExperiments suggest that with a smaller model and faster decoding, the proposed model can achieve higher quality outputs as compared to VALL-E-AC approach. The quality is measured in terms of speech naturalness, speaker similarity, and accentedness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Originality: \n1. The idea of Hubert unit correction to match the non-native accent units to the native ones is a key idea and proves to be important to the model performance\n2. Another practical idea about large-scale pre-training and then fine-tuning on a small amount of data could potentially help accelerate developing accent conversion systems for other non-native English accents. \n\n+ Quality:\nThe paper brings some existing techniques together to solve the accent conversion, it contains a few key ideas and fair evaluation except some concerns mentioned in the Weaknesses section.\n\n+ Clarity: clear enough.\n\n+ Significance: The proposed system can be practical for accent conversion for other non-native English accents."
                },
                "weaknesses": {
                    "value": "My main concern is the reliability of the results and the evaluation protocol. \n\n- Evaluation results show good performance. However, evaluation setups are somewhat weak.\n1. For example, subjective listening tests are at a relatively small scale. \n2. Evaluating Indian to native English conversion might have better with fluent English listeners who are Indian rather than Chinese to better capture some fine details. \n3. It could have been better if the effectiveness of the approach had been shown on a few more non-native English accents.\n \n- Paper layout can be improved. For example, there is a large empty space on Page 8. \n\n- Minor typos need correction."
                },
                "questions": {
                    "value": "1. Did the authors consider other ways of measuring the effect of the influencing factors in Table 1? For example, the edit distance between the Hubert sequences of two files.\n\n2. The subjective analysis in Table 2 has been conducted on a relatively small set of utterances and a small number of evaluators. Could you elaborate more on the reliability of the results in Table 2?\n\n3. References Jiang 2023a and Jiang 2023b are the same. \n \n4. VALL-E has better speaker similarity in Table 3 but not in Table 7. In Table 7 caption, please mention about the zero-shot condition (as opposed to the SPK comparison in Table 3)\n\n5. Minor typing issues: \n\n- Section 4.3.2, first sentence: hubert -> Hubert\n- Section 5.1. Baseline models:  Hubert tokens for encoding Employing Hubert tokens for encoding -> please remove repetition\n- Appendix is not well-formatted and has some typos (e.g. Accemt -> Accent)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "There is a link to a webpage in the paper, but its content seems to be anonymous as far as I can see."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8861/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8861/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8861/Reviewer_Q9U6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8861/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698512725514,
            "cdate": 1698512725514,
            "tmdate": 1700661741015,
            "mdate": 1700661741015,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "62QigpVgyY",
                "forum": "PsRL00864k",
                "replyto": "SRB9Mte54G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8861/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8861/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Review #4"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the useful comments.  \n# Weaknesses:\n**W1**\uff1aFor example, subjective listening tests are at a relatively small scale.  \n**A1**\uff1a In the raters' scale, we have increased the number of listeners to 20 to evaluate our model in Sections 3.2 and 5.2. In the test cases, we have also increased the test pairs to 100 from the original 10 in Sec 3.2.  In Sec 5.2, based on previous methods, we believe that 60 cases from 6 Indian speakers are sufficient for our zero-shot test. Subjective tests are costly. For example, in the accent AB evaluation in only the VCTK dataset, a listener needs to listen to 200 cases. That is $C^2_5 \\times 20$.  \n**W2**\uff1aEvaluating Indian to native English conversion might have better with fluent English listeners who are Indian rather than Chinese to better capture some fine details.  \n**A2**\uff1a Yes, you are right. But that is indeed costly for us, the students. In our test, the raters are college students majoring in foreign languages, who have also been informed about the characteristics of the Indian accent. For example, \"Indian-accented speakers may place stress on different syllables, or their intonation may be more flat or elevated, which can make their speech sound more monotone or sing-song to native English listeners.\" We believe they have the ability to properly conduct accent tests. We also showcase our demo page.  \n**W3**\uff1aIt could have been better if the effectiveness of the approach had been shown on a few more non-native English accents.  \n**A3**\uff1a Thank you for the advice and i am trying this. We think this framework can be easily extended to other accents with low-resource data.  \n**W4**\uff1aPaper layout can be improved.Appendix is not well-formatted and has some typos (e.g. Accemt -> Accent)  \n**A4**\uff1a Sorry about the mistake. We have already made these corrections in our updated paper.  \n# Questions\n**Q1**\uff1aDid the authors consider other ways of measuring the effect of the influencing factors in Table 1? For example, the edit distance between the Hubert sequences of two files.  \n**A1**\uff1aThe edit distance could also be a good choice. But we think the Longest Common Subsequence Ratio (LCSR) is more intuitive. In Table 1, first, we want to explore the percentage of pure semantic tokens without speaker identity leak, and the percentage of tokens affected by the accent. When the influencing factor is speaker identity, a 75% LCSR indicates that 25% of tokens have speaker identity leakage. A lower probability of speaker leak tokens explains why the generative model works in a zero-shot evaluation. And when the accent influence is added, the LCSR degrades from 0.75 to 0.54, indicating that approximately 21% of the tokens are affected by the accent.  \n**Q2**\uff1aThe subjective analysis in Table 2 has been conducted on a relatively small set of utterances and a small number of evaluators. Could you elaborate more on the reliability of the results in Table 2?  \n**A2**\uff1a We've expanded to 100 test pairs and 20 raters.  \n**Q3**\uff1aReferences Jiang 2023a and Jiang 2023b are the same.  \n**A3**\uff1a Sorry about this. We have corrected this in our updated paper.  \n **Q4**\uff1aVALL-E has better speaker similarity in Table 3 but not in Table 7. In Table 7 caption, please mention about the zero-shot condition (as opposed to the SPK comparison in Table 3)  \n**A4**\uff1a Our framework has better speaker similarity. In Table 3, the primary reason for lower Speaker Perplexity (SPK) but comparable values to VALLE-AC in the proposed model is that the speech generated by VALLE-AC (without conversion module) retains more accent information. This suggests that more similar speech acoustic information results in more similar speech representations. This is not fair. Therefore, we present Table 7 to compare our proposed generative-only model with VALLE-AC. To avoid misunderstanding, we directly compare our proposed generative-only module with the baselines in Table 3. The results are shown in Table 3.  \n **Q5**\uff1aMinor typing issues  \n**A5**\uff1a Sorry for such typing issues.  We have corrected these."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8861/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329294245,
                "cdate": 1700329294245,
                "tmdate": 1700329294245,
                "mdate": 1700329294245,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9MDrue2RmR",
                "forum": "PsRL00864k",
                "replyto": "62QigpVgyY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8861/Reviewer_Q9U6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8861/Reviewer_Q9U6"
                ],
                "content": {
                    "title": {
                        "value": "Read the Authors' Response"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for answering my questions. Also, thanks for increasing the sample size for the subjective tests. However, I would like to keep my previous score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8861/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608612639,
                "cdate": 1700608612639,
                "tmdate": 1700608612639,
                "mdate": 1700608612639,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UHZPfIcR6h",
                "forum": "PsRL00864k",
                "replyto": "cVxS0Zw2W8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8861/Reviewer_Q9U6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8861/Reviewer_Q9U6"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response but no score update"
                    },
                    "comment": {
                        "value": "Thanks for additional clarification. However, in a practical application of the proposed system, it would probably be desirable to have high speaker similarity before and after the conversion. Since the results still suggest a slight loss of speaker similarity with the proposed approach, I would like to keep my score as is."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8861/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687695255,
                "cdate": 1700687695255,
                "tmdate": 1700687695255,
                "mdate": 1700687695255,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bAE133Igo5",
            "forum": "PsRL00864k",
            "replyto": "PsRL00864k",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8861/Reviewer_o8oG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8861/Reviewer_o8oG"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript proposes a one-to-one accent conversion approach, consisting of a seq2seq accent correction model and a waveform generation model. The accent correction model converts hidden units extracted from HuBERTs from non-native speakers to the ones from native speakers. The waveform generation model is implemented based on Codec, and it synthesizes waveform conditioned on converted hidden units and acoustic prompt (which encodes the target speaker identity)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Paper fits ICLR scope well.\n* The ideas of using hidden units from HuBERTs as semantic encoding and using Codec based waveform generation models are new. However, the overall framework is mostly the same as prior studies. In summary, the novelty is moderate.\n* The proposed method is solid.\n* Presentations and references are with good quality."
                },
                "weaknesses": {
                    "value": "* Evaluation and analysis in Section 3 and 5 have limited samples and raters. Issues in implementation and evaluation protocols. (see details in question section)"
                },
                "questions": {
                    "value": "Section 3: Please consider comparing hidden units from HuBERT to PPG or other features used in literature. Otherwise the use of hidden units sounds adhoc.\n\nSection 3.1: Please provide more details on the HuBERT setup. How is the model trained? How many clusters are used for hidden units identifications? These have a significant impact on the analysis results. In addition, 10 pairs don\u2019t seem to be enough to have the conclusions on the impact of accent and speaker identity from hidden units. Please verify this on a larger dataset.\n\nSection 3.2: Similarly, the results are less convincing with limited human ratings. Please consider increasing the number of pairs and the number of raters.\n\n\nSample page: Please list an utterance that is used for acoustic prompting as well here.\n\nSection 5.2: In VCTK experiments, please use more source speakers instead of just one. \n\nSection 5.2: Accent: \u201cDuring the AB test on accent score, the ground truth (GT) speech samples were selected from the native speaker \u201dbdl\u201d in the L2-ARCTIC dataset.\u201d Is it still the case when the non-native speaker is a female? If so, please consider using a female voice as the reference.\n\nSection 5.4: Just curious but not required - Does the proposed correction work under one-shot or zero-shot setup? 15-mins of parallel data is already a lot in production scenarios.\n\nSection 5.4.1: What about accent and naturalness of the zero-shot setup?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8861/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8861/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8861/Reviewer_o8oG"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8861/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698706910226,
            "cdate": 1698706910226,
            "tmdate": 1700604947143,
            "mdate": 1700604947143,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JQoBDIxqju",
                "forum": "PsRL00864k",
                "replyto": "bAE133Igo5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8861/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8861/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Review #3"
                    },
                    "comment": {
                        "value": "Dear reviewer, thanks a lot for your review.  \n**Q1**\uff1aSection 3: Please consider comparing hidden units from HuBERT to PPG or other features used in literature. Otherwise the use of hidden units sounds adhoc.  \n**A1**\uff1aIn our framework, we require a speech tokenizer that can extract highly semantic, highly compressed tokens. It has been demonstrated that HuBert model satisfies these requirements [1][2][3]. On the other hand, PPG is a frame-level feature that involves more speaker information leakage, making it more suitable for mapping tasks. The baseline model in AC-PPG illustrates that mapping from L2 PPGs(non-native feature) to L1 PPGs(native feature) may not be as effective.  \n**Q2**\uff1aSection 3.1: Please provide more details on the HuBERT setup. How is the model trained? How many clusters are used for hidden units identifications? These have a significant impact on the analysis results. In addition, 10 pairs don\u2019t seem to be enough to have the conclusions on the impact of accent and speaker identity from hidden units. Please verify this on a larger dataset.  \n**A2**\uff1aThank you for pointing this. We provide additional details in Section 3.1: \"we utilize the HuBERT-Base (Hsu et al., 2021) model and a k-means algorithm with 500 clusters to extract HuBERT tokens\". The pretrained model can be accessed at https://huggingface.co/facebook/hubert-base-ls960. Additionally, in Section 3.1, we've expanded to 1000 test pairs from 10 pairs. We arrived at similar conclusions, and our experimental results and analyses have been publicly commented and updated in the paper.  \n**Q3**\uff1aSection 3.2: Similarly, the results are less convincing with limited human ratings. Please consider increasing the number of pairs and the number of raters.  \n**A3**\uff1aIn Sec 3.2, we've expanded to 100 test pairs and 20 raters.  \n **Q4**\uff1aSample page: Please list an utterance that is used for acoustic prompting as well here.  \n**A4**\uff1aThe utterance used for acoustic prompting is also derived from the source non-native speech. And we also add these in sample page. In our framework, we randomly select 3-second segments from the source speech to extract acoustic prompts, aiming to maintain the same speaker identity post-accent conversion. If the original speech is less than 3 seconds, we take the whole sequence of the original speech.  \n **Q5**\uff1aSection 5.2: In VCTK experiments, please use more source speakers instead of just one.  \n**A5**\uff1aWe follow our baseline model AC-PPG\u2018s evaluation setting, which utilizes 20 test cases from a speaker in the VCTK dataset which is not parallel dataset. To explore why our model performs effectively through the LCSR metric, we require the parallel datasets like Arctic and L2-Arctic datasets.  \n **Q6**\uff1aSection 5.2: Accent: \u201cDuring the AB test on accent score, the ground truth (GT) speech samples were selected from the native speaker \u201dbdl\u201d in the L2-ARCTIC dataset.\u201d Is it still the case when the non-native speaker is a female? If so, please consider using a female voice as the reference.   \n**A6**\uff1aAs A4, the utterance used for acoustic prompting come from the source non-native speech to maintain the same speaker identity. The native speaker, 'bdl', serves as the ground truth speaker in our accent metric.  \n **Q7**\uff1aSection 5.4: Just curious but not required - Does the proposed correction work under one-shot or zero-shot setup? 15-mins of parallel data is already a lot in production scenarios.   \n**A7**\uff1aOur framework operates under zero-shot setup. The six test speakers are unseen by our generative module, and the conversion module has only been exposed to an Indian speaker. We do not require audios with the same content but from the same speakers with different accents. The speaker identities can be different. Furthermore, we are able to acquire 4 hours of such parallel data from the Arctic and L2-Arctic datasets.  \n **Q8**\uff1aWhat about accent and naturalness of the zero-shot setup?  \n**A8**\uff1aAs A7, our evaluation operates under zero-shot setup.  \n[1] Hsu, Wei-Ning, et al. \"Hubert: Self-supervised speech representation learning by masked prediction of hidden units.\" IEEE/ACM Transactions on Audio, Speech, and Language Processing 29 (2021): 3451-3460.  \n[2] Wang, Zhichao, et al. \"LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models.\" arXiv preprint arXiv:2306.10521 (2023).  \n[3]Huang, Rongjie, et al. \"Make-A-Voice: Unified Voice Synthesis With Discrete Representation.\" arXiv preprint arXiv:2305.19269 (2023)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8861/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329081674,
                "cdate": 1700329081674,
                "tmdate": 1700329572233,
                "mdate": 1700329572233,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mPLzFWGh4m",
                "forum": "PsRL00864k",
                "replyto": "JQoBDIxqju",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8861/Reviewer_o8oG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8861/Reviewer_o8oG"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the clarifications!"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for detailed explanations to my questions and the improvements made to the paper, and I increase my soundness scores accordingly. Given the moderate novelty and the issues pointed out by other reviewers, I would keep the overall rating as it is."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8861/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604932002,
                "cdate": 1700604932002,
                "tmdate": 1700604932002,
                "mdate": 1700604932002,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "d6MMlYkwnl",
            "forum": "PsRL00864k",
            "replyto": "PsRL00864k",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8861/Reviewer_SdoC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8861/Reviewer_SdoC"
            ],
            "content": {
                "summary": {
                    "value": "* This paper proposes a new framework for accent conversion (AC) with minimum supervision. The framework consists of a correction module and a speaking module. The correction module corrects the source accented semantic tokens to the target native ones, while the speaking module generates speech with native prosody and the same speaker identity. The correction module is pre-trained with a pretext task in a self-supervised manner using large amounts of native speech and finetuned with only 15 minutes of parallel data. Experimental results show that the proposed framework achieved the state-of-the-art performance in terms of accentedness, speech quality and speaker maintenance.\n\n* The manuscript includes a non-anonymous github link for the samples (https://jiazj-jiazj.github.io/Speak-and-correct/)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The strengths of the paper are as follows:\n\n- originality: The paper proposes a framework based on generative models for accent conversion with minimum supervision. Accent Conversion is an existed task, and the author try to address two challenges including: 1) training with less parallel data, 2) removal of accent effects on prosody and pronunciation patterns. Although pre-training with large unlabeled data and finetuning with a few parallel data is not a creative idea (e.g., Spear-TTS), the paper does effectively solve the two problems, and the authors have comprehensively analyzed and verified the relationship between accent, semantic tokens and acoustic tokens. Besides, introducing TF-Codec instead of Encodec/SoundStream shows the improvement in reducing the complexity and latency, which is helpful for speech generation.\n\n- quality: The quality of the paper is high, with clear problem definition, adequate literature review, comprehensive analysis and well-organized presentation.\n\n- clarity: The paper is generally well-written and easy to follow. \n\n- significance: The significance of the paper is that it provides a novel method for accent conversion that can be used with a small amount of parallel data. The similar idea of decomposing accent conversion into semantic token generation and acoustic token generation has been attractive recently, which can be inspiring for other works in the field of speech generation.\n\nOverall, the paper is a relatively valuable contribution to the field of accent conversion and speech generation."
                },
                "weaknesses": {
                    "value": "The weakness of the paper are as follows:\n\n* From the perspective of method, introducing TF-Codec as a contribution seems independent and unrelated to the theme of paper (accent conversion or accent reduction). If the speaking module is replaced with a multi-stage generative model such as Encodec, will it affect the accentedness and speaker similarity of accent conversion? \n\n* Although generally well-written, there exists some unclear and confusing statements in the paper. Some technical details and discussions are missing and need to be included. Please checkout the questions below."
                },
                "questions": {
                    "value": "* Does it remove all the dupilicated semantic tokens in HuBERT in both correction module and speaking module as stated in Section 3.1?\n\n* The formula definition in \u201cPretraining\u201d in Section 4.2. What is the purpose of defining $C^{t-1}$? It seems to be not used. Does $X^{t-1}$ already include $x_t$ or not? This part may be roughly understood, but it is not clear enough. \n\n* In Section 4.2, what is the token mask ratio and strategy in pretraining stage? These details are important to reproduce the paper. Please give more explanations. \n\n* According to Section 4.3, the input of speaking module is the concatenation of the prompt accented semantic tokens, the target native semantic tokens, and the prompt accented acoustic tokens during inference, right? If so, the combination of accented semantic tokens and native semantic tokens has not been seen when training the speaking module. Does this mismatch affect the performance of accent conversion?\n\n* In section 5, what is the difference between \u201cproposed\u201d and \u201cproposed-VC\u201d? Are they the same one? Is \u201cthe traditional AC model (Liu et al., 2020)\u201d equal to \u201cAC-PPG\u201d? It should be better to present the name and settings of each model more clearly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8861/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698913267996,
            "cdate": 1698913267996,
            "tmdate": 1699637114513,
            "mdate": 1699637114513,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ggwhQR3b9G",
                "forum": "PsRL00864k",
                "replyto": "d6MMlYkwnl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8861/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8861/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Review #2"
                    },
                    "comment": {
                        "value": "We thank the reviewer very much for recognizing our work and suggestions. This is the final version of our comment. \n# Weaknesses:\n**W1**\uff1aFrom the perspective of method, introducing TF-Codec as a contribution seems independent and unrelated to the theme of paper (accent conversion or accent reduction). If the speaking module is replaced with a multi-stage generative model such as Encodec, will it affect the accentedness and speaker similarity of accent conversion?     \n**A1**\uff1a TF-Codec is the core design of our generative model which enables a single-stage causal speech generation. Compared with multi-stage speech generation based on EnCodec, it achieves lower complexity and has higher potential for low-latency applications. Besides complexity, TF-Codec generates speech with higher speaker similarity (0.502 vs 0.429 in terms of SPK\uff09and naturalness (4.08 vs 3.84 in terms of MOS) in ***Table 3***.  \n# Questions  \n**Q1**\uff1aDoes it remove all the duplicated semantic tokens in HuBERT in both correction module and speaking module as stated in Section 3.1?  \n**A1**\uff1aIn the training and inference phase, we don't remove duplicated HuBERT semantic tokens. Somehow, it helps to retain the original speech duration.  In ***Section 3.1***,  we remove duplicated semantic tokens to eliminate the effect of the duration of each word in the accent analysis on HuBERT tokens.  \n**Q2**\uff1aThe formula definition in \u201cPretraining\u201d in Section 4.2. What is the purpose of defining $C^{t-1}$? It seems to be not used. Does \n$X^{t-1}$ already include $x_{t}$or not? This part may be roughly understood, but it is not clear enough.  \n**A2**\uff1aSorry for the mistake. We correct the formula in our paper.  Please refer to ***Formula (1)*** in ***Section 4.2***.   \n**Q3**\uff1aIn Section 4.2, what is the token mask ratio and strategy in pretraining stage? These details are important to reproduce the paper. Please give more explanations.  \n**A3**\uff1aWe have added an explanation in our paper in ***Section 4.2 Pretraining*** as ***\"We have experimented with corruptions like token masking, token deletion, and token infilling and we find the masking scheme works the best. Specifically, following the masking scheme in BERT, 15\\% tokens have been randomly selected first. Then 80\\% of them are replaced with [MASK] tokens, 10\\% of them are filled with random tokens and the remaining 10\\% are left unchanged.\".***  \n **Q4**\uff1aAccording to Section 4.3, the input of speaking module is the concatenation of the prompt accented semantic tokens, the target native semantic tokens, and the prompt accented acoustic tokens during inference, right? If so, the combination of accented semantic tokens and native semantic tokens has not been seen when training the speaking module. Does this mismatch affect the performance of accent conversion?  \n**A4**\uff1aThat's a good question. The setting is similar to ***Section 3.2***. In ***Section 3.2***, the generative model is trained to generate speech conditioned on native semantic tokens. During inference, the semantic token part is the concatenation of the native content semantic tokens and the prompt semantic tokens. The experiment compares the native prompt semantic tokens with the accent semantic tokens and the result shows similar which indicates such mismatch doesn`t affect the performance. This can also be verified in the performance of our current model on AC. Please refer to our demo: https://convert-and-speak.github.io/demo/.  \n**Q5**\uff1aIn section 5, what is the difference between \u201cproposed\u201d and \u201cproposed-VC\u201d? Are they the same one? Is \u201cthe traditional AC model (Liu et al., 2020)\u201d equal to \u201cAC-PPG\u201d? It should be better to present the name and settings of each model more clearly.  \n**A5**\uff1a***Proposed-VC*** in Section 5 in previous paper is ***Proposed*** without conversion module. Sorry for the unclear expression. We correct these in ***Table 3*** in the updated paper, in which ***Proposed-VC*** refers to ***Generative model(TF-Codec)*** and ***Liu. et al*** refers to existing best machine-learning based AC method available in the public, previously named  ***AC-PPG***."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8861/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328781234,
                "cdate": 1700328781234,
                "tmdate": 1700628000376,
                "mdate": 1700628000376,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pJNEYC4QGy",
                "forum": "PsRL00864k",
                "replyto": "ggwhQR3b9G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8861/Reviewer_SdoC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8861/Reviewer_SdoC"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for Authors' responses"
                    },
                    "comment": {
                        "value": "Thank the authors for detailed explanations and revisions to the paper in response to my questions.  I have also read the other reviewers' comments and authors' responses.  I would keep my original rating."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8861/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740117483,
                "cdate": 1700740117483,
                "tmdate": 1700740117483,
                "mdate": 1700740117483,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iwoNNYQAt7",
            "forum": "PsRL00864k",
            "replyto": "PsRL00864k",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8861/Reviewer_KUHy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8861/Reviewer_KUHy"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an accent conversion model, composed of two major components: 1) an correction module for converting the accent in a discrete latent domain; 2) a generation module for generation speech features. The generated speech features are discrete tokens that can be converted into audio waveforms with a neural vocoder.\n\nThe experiment was conducted by training the model on LibriTTS + ARCTIC + L2 ARCTIC, and evaluating on 5 speakers from L2 ARCTIC and another speaker from VCTK."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The presented method is sound."
                },
                "weaknesses": {
                    "value": "- The presentation and writing needs improvement.\n- There are concerns on technical correctness, see \"Questions\".\n- There are concerns on discrimination, see \"Ethics Review\". My initial rating is primarily based on such concerns, and was updated after the authors addressed them."
                },
                "questions": {
                    "value": "1. Abstract: mentioned three terms: \"accent conversion\", \"accent removal\", and \"accent reduction\". It would be helpful to distinguish or consolidate.\n2. Abstract: \"TF-Codec\" used without explanation nor reference.\n3. Sec 2: \"there has not been a parallel corpus that contains pairs of audios having the same contents yet coming from the same speakers in different accents\" -- better to restrict such claims as \"public corpus\", as you don't know proprietary ones.\n4. Sec 3.1: `LCSR = LCS / utterance_length` -- clarification is needed because the two utterance can have different lengths.\n5. Sec 3.1: Table 1 is misleading because \"accent without phoneme corruption\" and \"accent with phoneme corruption\" implies different speaker. So the trends in the table is completely expected. It cannot draw conclusion as the paper stated, that \" the speaker identity causes little impact on the Hubert tokens and the content is not disturbed\", \"accent feature ... brings larger influence on the Hubert tokens\".\n6. Sec 3.2: which model is used for synthesis? this section looks like experimental results, rather than analysis.\n7. Sec 5.1: any explanation on why the same speaker \"ASI\" is used for both training and evaluating? \n8. Sec 5.1: \"In the AB test for accenteness, participants first listened to native and non-native reference audio. Subsequently, they heard paired speech samples (A and B) with identical content and were asked to choose the one that sounded more native-like.\" -- will rater be able to infer from the voice identity instead of accent?\n9. Sec 5.4: This seems completely unreadable to me. I don't understand what the sentences mean, and have no idea about what \"3.5, 3, 2.7\" numbers refer to as they are not presented in the corresponding Table.\n10. Sec 5.4.1: Does the 2% LCSR in Table 6 consistent with 54% LCSR in Table 1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I'm concerned on the way that this paper describes about foreign accents. It describe foreign accents as \"corrupted\" or \"distorted\" pronunciation, and what the proposed model does is \"correcting\". This is discriminative and should be avoided. There are neutral words can be used, such as \"translate\", or \"convert\", instead of \"correct\"."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8861/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8861/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8861/Reviewer_KUHy"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8861/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699227385764,
            "cdate": 1699227385764,
            "tmdate": 1700695877948,
            "mdate": 1700695877948,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sITIolVeby",
                "forum": "PsRL00864k",
                "replyto": "iwoNNYQAt7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8861/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8861/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Review #1 \uff081/2\uff09"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the useful advice.  \n**About Ethics Concerns**  \n**A**\uff1aWe are very grateful to the reviewer for pointing out the ethics issues in our work. We have revised the wrong description such as \"distorted\" pronunciation updated to \"different\" pronunciation in foreign accents. We use the term 'convert' instead of 'correct' in our paper, including title, abstract and main parts.   \n**Q1**\uff1aAbstract: mentioned three terms: \"accent conversion\", \"accent removal\", and \"accent reduction\". It would be helpful to distinguish or consolidate.  \n**A1**\uff1a  Thank you for advising this. We refer to these as 'accent conversion' in our updated paper.  \n**Q2**\uff1aAbstract: \"TF-Codec\" used without explanation nor reference.  \n**A2**\uff1a TF-Codec is a speech neural codec which follows paper[1]. We update this in abstract \"*TF-Codec is a pretrained speech neural codec with group quantization which can be used as a single-stage autoregressive generation.*\"  \n**Q3**\uff1aSec 2: \"there has not been a parallel corpus that contains pairs of audios having the same contents yet coming from the same speakers in different accents\" -- better to restrict such claims as \"public corpus\", as you don't know proprietary ones.  \n**A3**\uff1aWe update this \"*For accent conversion task, there has not been a public parallel corpus that contains pairs of audios having the same contents yet coming from the same speakers in different accents.*\"  \n**Q4**\uff1aSec 3.1: LCSR = LCS / utterance_length -- clarification is needed because the two utterance can have different lengths.  \n**A4**\uff1aWe update this \"*The length of the parallel data in each pair is similar and the utterance length ranges from 3 seconds to 5 seconds. The smaller utterance length is used to calculate the LCSR of each pair.*\"  \n**Q5**\uff1aSec 3.1: Table 1 is misleading because \"accent without phoneme corruption\" and \"accent with phoneme corruption\" implies different speaker. So the trends in the table is completely expected. It cannot draw conclusion as the paper stated, that \" the speaker identity causes little impact on the Hubert tokens and the content is not disturbed\", \"accent feature ... brings larger influence on the Hubert tokens\".  \n**A5**\uff1a We update the description in my paper \u201cAs the results shown, with the accent source, HuBert tokens have changed a lot, degrade from 0.75 to 0.57 in terms of LCSR. For those cases with specific phoneme changes, more tokens have changed\nfrom their native references(LCSR: 0.54).\u201d  \n[1] Jiang, Xue, et al. \"Latent-Domain Predictive Neural Speech Coding.\" IEEE/ACM Transactions on Audio, Speech, and Language Processing (2023)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8861/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328656354,
                "cdate": 1700328656354,
                "tmdate": 1700329684426,
                "mdate": 1700329684426,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iNDVgJvA8Y",
                "forum": "PsRL00864k",
                "replyto": "iwoNNYQAt7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8861/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8861/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Review #1 \uff082/2\uff09"
                    },
                    "comment": {
                        "value": "**Q6**\uff1aSec 3.2: which model is used for synthesis? this section looks like experimental results, rather than analysis.   \n**A6**\uff1aThe generative model in our proposed framework is used for this analysis experiment. In this section, we evaluate if the accent feature will be extended through the in-context learning. Based on this analysis, we find the accent style can hardly be transferred through the in-context learning in the generative model, which guides us to use the original accent source as the prompt to maintain the speaker identity.  \n**Q7**\uff1aSec 5.1: any explanation on why the same speaker \"ASI\" is used for both training and evaluating?  \n**A7**: We utilize the speaker \"ASI\" for training and use 6 Indian speakers from VCTK, ARCTIC, and L2-ARCTIC for evaluation. We have now corrected this in Sec 5.1\u201c*In the process of fine-tuning the correction model, we utilize data from speaker \"bdl\" as the native English speaker source ...... and data from speaker \"ASI\" ....... In the evaluation, we employ data from speaker p248 originating from the VCTK dataset, as well as data from five speakers, specifically ASI, KSP, RRBI, and SVBI, derived from the L2-ARCTIC dataset to assess the performance of models.*\u201d  \n**Q8**\uff1aSec 5.1: \"In the AB test for accenteness, participants first listened to native and non-native reference audio. Subsequently, they heard paired speech samples (A and B) with identical content and were asked to choose the one that sounded more native-like.\" -- will rater be able to infer from the voice identity instead of accent?  \n**A8**:  The native and non-native reference audio initially listened to have different speaker identities compared to the test pairs. Or if you're asking whether the speaker's identity is related to the accent in the test cases? Before the test, the raters have been taught to discern differences in Indian accents. And we direct the speakers to focus solely on accent differences.  These 5 speakers are all indian- accent speakers in l1-l2 arctic dataset.  \n**Q9**\uff1aSec 5.4: This seems completely unreadable to me. I don't understand what the sentences mean, and have no idea about what \"3.5, 3, 2.7\" numbers refer to as they are not presented in the corresponding Table.  \n**A9**:  Sorry for this mistake. We have already deleted this.  \n**Q10**\uff1aSec 5.4.1: Does the 2% LCSR in Table 6 consistent with 54% LCSR in Table 1?  \n**A10**: Apologies for the misunderstanding due to the lack of explanation. In Table 6,  \"without correction model,\" means that in  the finetuning stage, we directly use the L2-semantic token and L1-acoustic tokens with different speaker identities but with the same phoneme content to fine-tune the generative model instead of using a converted model to convert non-native semantic tokens to native semantic tokens. 2% LCSR means it does not work."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8861/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328692401,
                "cdate": 1700328692401,
                "tmdate": 1700328720078,
                "mdate": 1700328720078,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FgMGf8ZVh7",
                "forum": "PsRL00864k",
                "replyto": "iNDVgJvA8Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8861/Reviewer_KUHy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8861/Reviewer_KUHy"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses from the authors. A lot of my questions are answered clearly.\n\nI have a few follow up questions:\n\nA5: My concern remains, because the table still implies that the first row is the impact of speaker identity, and the last two rows are the impact of the accent. It should be made clear in the table that the last two rows are comparison across different speakers as well.\n\nA6: From the structure of the writting, Sec 3.2 is supposed to be a pre-analysis to lay out the ground and the motivation of the approach, but it's actually analysis on the proposed method. It fits better in the \"Experiments\" section.\n\nA7: So speaker \"ASI\" is used in both training and evaluation indeed. What's the impact in the evaluation results and the explanations?\n\nA8: According to the paper, \"the ground truth (GT) speech samples were selected from the native speaker \u201dbdl\u201d\". In the meantime, to model outputs to be evaluated sounds clearly synthesized (because of artifacts or other factors results into lower naturalness). So it would be quite clear to the raters which audio samples is supposed to be the \"native\" version (true recording of speaker bdl) and which audio sample is supposed to be \"more accented\" (synthesized, may or may not be in a different voice). Such clue can crew the side-by-side evaluation result drastically, despite that you \"direct the speakers to focus solely on accent differences\".\n\nA10: What's the reason of that (\"it does not work\")? A doing-nothing model should get 54% LCSR here based on Table 1, right?"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8861/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613546210,
                "cdate": 1700613546210,
                "tmdate": 1700613546210,
                "mdate": 1700613546210,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6xQoZfMT0A",
                "forum": "PsRL00864k",
                "replyto": "IloaOGJmUg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8861/Reviewer_KUHy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8861/Reviewer_KUHy"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses and additional revisions, especially on address the ethics concerns. I have updated my overall rating score from 1 to 6, and contribution from 2 to 3.\n\nAlthough the evaluation protocol and the experiment set up in the paper has a room for improvement (Q7 and Q8), I believe the current form is informative and helpful to the community. I'd appreciate if the authors can continue revise the text to clarify the limitations of the evaluation and the results."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8861/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696199720,
                "cdate": 1700696199720,
                "tmdate": 1700696199720,
                "mdate": 1700696199720,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]