[
    {
        "title": "Quantifying Zero-shot Coordination Capability with Behavior Preferring Partners"
    },
    {
        "review": {
            "id": "URkLM9r9kd",
            "forum": "wTRpjTO3F7",
            "replyto": "wTRpjTO3F7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1499/Reviewer_PVtL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1499/Reviewer_PVtL"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to create a reliable and comprehensive evaluation method for zero-shot coordination. The authors design an evaluation workflow includes three stages. Firstly the method generates behavior preferring agents with corresponding BRs. Secondly representative policies are selected based on a BR-diversity. Finally the selected policies and their BRs are used to evaluate the ego policy. The overall framework has potential to provide a more effective metric (BR-Prox) and also indicates some shortcuts of overcooked scenarios."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The methodology of constructing a \"diversity-complete\" set of BRs is reasonable and meaningful.\n2. The authors propose the BR-Prox metric to measure the performance of an ego policy under the ZSC manner and use this metric to benchmark previous methods on ZSC.\n3. The authors propose an algorithm to construct an evaluation population by first generating adequate policies and then selecting diverse ones."
                },
                "weaknesses": {
                    "value": "1. Though the authors claim that a \"diversity-complete\" set may be intractable for complex environments, the proposed methodology (generation and selection) fails to well adhere to the exact definition in the desiderata (Section 3.2) with the lack of revealing the gap with a real \"diversity-complete\" set.\n2. The proposed metric and evaluation workflow lack necessary discussions with previous approaches (e.g., methods in Table 1).\n3. The presentation of this paper is generally obscure. The authors involve a sort of techniques during the evaluation workflow but do not well explain the necessity and details (e.g., how to represent the behavior feature of a policy and reasons to involve event-based rewards).\n4. The evaluated baselines seem not distinguishable in the evaluation setting while the authors do not provide further insights of what kinds of approaches are generally useful on ZSC.\n5. The evaluation workflow is restricted to a two-agent form while previous ZSC methods can generalize to multi-agent settings."
                },
                "questions": {
                    "value": "1. How can we extend the evaluation manner to multi-agent settings?\n2. In Figure 2, how are the high-level behaviors visualized? What is the meaning of different data points in Figure 2?\n3. In Figure3, why does the population diversity first rise and then drop with the population size increasing? As near 0 values mean linear correlation, why is the diversity low in a small population size?\n4. Does the P-Div metric mean $\\text{PD}$ on $\\pi_i$ instead of $BR(\\pi_i)$?\n5. In Section 5.2, how do Figure 4 and 5 show \"increasing population size contributes to the improvement of performance under the condition that the diversity of the population is also grown\". \n6. How do the event-based rewards contribute to behavior preferring policies? Specifically, how do the method design and adjust $w$? How is this approach related to proposed \"skill-level diversity\"?\n7. At the end of Section 3.2, \"by selecting earlier checkpoints of the evaluation partners, it is simple to acquire evaluation partners with diverse skill levels.\" What is the actual method of selecting earlier checkpoints of the evaluation partners?\n8. How can we represent the behavior feature $\\theta$ of a policy?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1499/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1499/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1499/Reviewer_PVtL"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1499/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697711546595,
            "cdate": 1697711546595,
            "tmdate": 1699636078632,
            "mdate": 1699636078632,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "539OueCNGY",
                "forum": "wTRpjTO3F7",
                "replyto": "URkLM9r9kd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PVtL (1/5)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's insightful comments and suggestions, and we are glad to learn that you regard our method as reasonable and meaningful.\n\n### 1. Relations between our method and \"diversity-complete\"\n> (Weakness 1) Though the authors claim that a \"diversity-complete\" set may be intractable for complex environments, the proposed methodology (generation and selection) fails to well adhere to the exact definition in the desiderata (Section 3.2) with the lack of revealing the gap with a real \"diversity-complete\" set.\n\n**Meeting the Desiderata**. We acknowledge that our evaluation workflow is approximating the 'diversity-complete' by meeting the desiderata as much as possible.\u00a0\n- Reasonable: We restrict the reward space to indicate reasonable behaviors by adding the original task reward and filtering sabotaging agents after generating candidates.\n- Skill-style diverse: We traverse the reward space for diverse fully trained behavior-preferring agents and select the representative set by BR-Div.\n- Skill-level diverse: We collect the earlier checkpoints of the selected fully trained evaluation partners to increase the skill-level diversity.\n\nWe highlight how our evaluation workflow meets the desiderata in Sec. 4.2 of the update paper.\n\n**Gap with \"diversity-complete\"**. First, we need to claim that we cannot get a real \"diversity-complete\" set of evaluation partners for most tasks. We reveal the gap of our generated partners with a real \"diversity-complete\" set as follows:\n- Events are designed based on human knowledge and may not cover all possible reasonable behaviors.\n- We can only obtain approximate BRs.\n- Style levels of different checkpoints may not cover all skill levels.\n\nThough our evaluation partners still have a gap with a real \"diversity-complete\", we are the first to propose the concept of ideal evaluation partners, i.e., \"diversity-complete\" and our evaluation workflow makes a significant improvement over previous evaluation methods in providing a fair and comprehensive evaluation.\n\n\n### 2. Discussion between our method and previous methods\n> (Weakness 2) The proposed metric and evaluation workflow lack necessary discussions with previous approaches (e.g., methods in Table 1).\n\nWe first clarify that we list the previous evaluation methods in the first column and the works that utilize them in the second column in Table 1. For the previous evaluation methods, we have already discussed their defects in Sec. 3.2 and Appendix C.\n\nDefects of previous evaluation partners:\n- Human Players: The human players in ZSC problem are 'perfect' candidates for evaluation because human are strictly qualified as unseen partners. While evaluating with human players are expensive, time-consuming and unrepeatable. We need a more efficient evaluation method as a supplement for human evaluation. \n- Human Proxy Agents: Human proxy agents in overcooked environment do not account for human behaviors [5], which shows that using human proxy agents does not represent the diversity of human.\n- Trained Self-play Agents: Train self-play agents are similar to the agents used to train the ego agent and are not diverse, as shown in Figure 2.\n- Rule-based Specialist: Manually building expert rules is difficult to implement in complex environments and may not meet diversity requirements. We point out that designing a set of events is much easier than designing a policy.\n- Random Agents: The diversity of the random initializations cannot be ensured and random initializations lack of high level performance.\n\nDefects of previous metric:\n- Cross-play with Trained Adapted Agents: Some works compare their ZSC methods by cross-playing the agents trained by other ZSC methods and their ego agents. This results include self-play performance and does not completely reflect the capabilities of ZSC. On the other hand, excluding the self-play results leads to potential unfairness.\n\nThe major difference is that **previous evaluation partners and metrics are unable to provide a comprehensive and fair evaluation due to the evaluation partners are not diverse or the metric is unfair. Instead, our evaluation workflow provide a comprehensive and fair measurement of ZSC capability by assembling reasonable evaluation partners with diverse skill-styles and skill-levels and by proposing BR-Prox to assess the generalization capability and improvement potential.**\n\nWe have highlighted the differences in Sec. 2 of the update paper. We also remark that we are the first to systematically investigate the construction of evaluation partners and the measurement of ZSC capability. Our aim is to ensure a more fair and comprehensive assessment of ZSC algorithms, addressing the current gaps in evaluation methodologies."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245123469,
                "cdate": 1700245123469,
                "tmdate": 1700245222569,
                "mdate": 1700245222569,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ksbhajCiXs",
                "forum": "wTRpjTO3F7",
                "replyto": "URkLM9r9kd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PVtL (3/5)"
                    },
                    "comment": {
                        "value": "### 5. Explanations\n\nThank you for your suggestions on the improvement of our representation. We have added the explanations to the revised paper. We are glad to further discuss if you have other suggestions or questions about our paper.\n\n### Explanations of the figures\n\nDue to the space limitation, details of our experiments can be found in Appendix B.\n\n#### a. Explanation of Fig. 2\n> (Question 2) In Figure 2, how are the high-level behaviors visualized? What is the meaning of different data points in Figure 2?\n\nTo get Figure 2, we collect the occurrence of the designed events by counting the triggered events alongside the episodes and representing each trajectory as a vector containing the normalized times of event occurrences. Then we reduce the vectors into 2-dimension through principal components analysis (PCA [3]). Finally we plot the 2-dimension vectors in Figure 2. **Briefly, each data point means the normlized event occurrences of a trajectory.**\n\n#### b. Explanation of Fig. 3\n> (Question 3) In Figure 3, why does the population diversity first rise and then drop with the population size increasing? As near 0 values mean linear correlation, why is the diversity low in a small population size?\n\nRecall that we define the population diversity as $\\operatorname{PD}(\\\\{\\pi _{i}\\\\} _{i=1}^{M}) = \\operatorname{det}(\\bf{K})$, where $\\bf{K} _{ij} = \\theta _{i}\\cdot\\theta _{j}$ and each element in $\\bf{K}$ is normalized into \\[0,1\\]. Thus, population diversity is a determinant. The determinant of a matrix represents the \"volume\" of the matrix but is also related to the dimension of the matrix.\n\nTherefore, the population diversity metric is related to both the diversity and the size of the population. Strictly speaking, we should only compare the points on the two curves with the same population size. The population diversity of a small population is low because it is the determinant of a small and normalized matrix.\u00a0\n\nFigure 3 illustrates that sampling based on BR-Div reaches higher population diversity than that based on P-Div at the same population size before the population diversity diminishes, which means BR-Div is more effective in constructing a diverse population from a set of policies.\n\n\n\n#### c. Explanation of Fig. 4 and Fig. 5\n> (Question 5) In Section 5.2, how do Figure 4 and 5 show \"increasing population size contributes to the improvement of performance under the condition that the diversity of the population is also grown\".\n\nThank you for your suggestion to improve our representation. We have updated the explanation in the revised paper.\u00a0\n\nSpecifically, in Figures 4 and 5, some algorithms get performance improvement with increasing population sizes in some layouts, which is exactly why we conclude that \"increasing population size contributes to the improvement of performance under the condition that the diversity of the population is also grown\". In the following, we explain what the condition is from two aspects.\n- Some ZSC methods lack an explicit mechanism to promote population diversity, including FCP and COLE. Thus, the performance of FCP and COLE does not benefit from increasing the population size from 24 to 36.\n- On the other hand, ZSC methods with explicit mechanisms to promote population diversity may sometimes fail to expand population diversity in some layouts. We provide a case study in the Counter Circ. and Forced Coord. layouts to show that the reason is existing ZSC methods may fail to effectively produce enough diverse, high-performing agents in some layouts."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245386394,
                "cdate": 1700245386394,
                "tmdate": 1700247050615,
                "mdate": 1700247050615,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2ySmBf6Afr",
                "forum": "wTRpjTO3F7",
                "replyto": "URkLM9r9kd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PVtL (4/5)"
                    },
                    "comment": {
                        "value": "### Explanation of notions and concepts \n#### a. P-Div and BR-Div\n> (Question 4) Does the P-Div metric mean PD on $\\pi _i$ instead of $BR(\\pi _i)$?\n\nYes, and P-Div is first mentioned in Sec. 4.1, \"according to the partners population diversity (P-Div)\". We further add the definition that $\\operatorname{P-Div}(\\\\{\\pi _{i}\\\\} _{i=1}^{M}) = \\operatorname{PD}(\\\\{\\pi _{i}\\\\} _{i=1}^{M})$ to make it clear.\n\n#### b. Behavior Feature Representation and Event-based Reward Design\n> (Weakness 3) The presentation of this paper is generally obscure. The authors involve a sort of techniques during the evaluation workflow but do not well explain the necessity and details (e.g., how to represent the behavior feature of a policy and reasons to involve event-based rewards). \n\n> (Question 6) How do the event-based rewards contribute to behavior preferring policies? Specifically, how do the method design and adjust $w$? \n\n> (Question 8) How can we represent the behavior feature $\\theta$ of a policy?\n\n**Event-based Reward**. To clarify, we explain our motivation for introducing event-based rewards in Sec. 4.2. We expect partners to have diverse and reasonable behaviors, while the behavior space is intractably large. Inspired by [2] in which human preferences can be regarded as event-centric and modeled as event-based reward functions, we apply the event-based reward shaping method to encourage behavior discovery and generate behavior-preferring partners. The event-based reward space can be formulated as follows:\n$$\n\\mathcal{R}=\\\\{r _{\\bf{w}}| r _{\\bf{w}}(s _{t}, \\bf{a} _{t}) = r + \\phi(s _{t}, \\bf{a} _{t})^{T}\\bf{w}, \\bf{w} \\in \\mathbb{R}^{m}, \\|\\bf{w}\\| _{\\infty} \\leq B _{\\text{max}}, \\sum _{i} \\mathbb{1}(\\bf w _{i} \\neq 0) \\leq C _{\\text{max}}\\\\}\n$$\nThe hyper-paramters $B _{\\text{max}}$ and $C _{\\text{max}}$ are used to restrict the reward space to indicate reasonable behaviors. $\\phi(s _t,\\bf{a} _{t})$ is the indicator of pre-defined events.\n\n**Policy Behavior Feature**. The policy behavior features that are used to figure out BR-Div depend on the environment and the task at hand, which are related to the skills being tested. For simplicity, we count the occurrence of the pre-defined events, i.e., $\\mathbb{E}[\\sum _{t=1}^{T}\\phi(s _t,\\bf{a} _t) ]$, as the policy behavior feature.\n\n\n**$\\bf{w}$ Design**. Given the above formulation of reward space $\\mathcal{R}$, we have restricted the space of $\\bf{w}$ to indicate only reasonable combinations of events. $\\mathcal{R}$ includes preferences that do not cause the game to be disrupted or unable to cooperate with. **Then we traverse the restricted reward space, i.e., the space of $\\bf{w}$**. An event-based reward function $r _{w}$ indicates a behavior preference since $r _{w}$ encourages the preferred events (behaviors). The agent equipped with this event-based reward function is trained with an agent equipped with the reward according to the given task. The two-agent team approximates an NE in a two-player Markov game, which models the scenario in which a task-reward-maximizing agent collaborates with another agent or a human with its own preferences to complete the given task. Therefore, we get an agent preferring the behaviors presented by $r _{w}$ after approximating the NE.\n\n\n\n*An example of Overcooked*\n\nIn Overcooked, we use $B _{\\text{max}} = 20$, $C _{\\text{max}}=3$ and generate up to 194 candidates and select up to 30 evaluation partners. The generated candidates are excluded if they cannot complete a delivery when cooperating with their BRs. The pre-defined events are listed as follows:\n| Events | Weights |\n| --- | --- |\n| Put an onion or a dish or a soup onto the counter  | 0  |\n| Pickup an onion or a dish or a soup from the counter | 0 |\n| Pickup an onion from the onion dispenser | -20,0,10  |\n| Pickup a dish from the dish dispenser  | -20,0,10 |\n| Pickup a soup | -20,0,5,10 |\n| Place an ingredient into the pot | -20,0,3,10 |\n| Deliver a soup | -20,0 |\n| Stay | -0.1,0,0.1 |\n| Movement | 0 |\n| Order Reward | 0.1,1 |\n\nWe also provide the pseudocode for using these events to show that one can make a moderate effort to implement the event-based rewards.\n```python=\nstate = current state of the game\naction = action of the agent whose events are collected\nreward = 0\nfor e_i in range(NUM_EVENTS):\n    if True == EVENT_JUDGER(e_i, state, action):\n        reward += EVENT_WEIGHTS[e_i]\n```\nThe details are also present in Appendix B.1."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245539070,
                "cdate": 1700245539070,
                "tmdate": 1700246210687,
                "mdate": 1700246210687,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nFdoryUjju",
                "forum": "wTRpjTO3F7",
                "replyto": "URkLM9r9kd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PVtL (5/5)"
                    },
                    "comment": {
                        "value": "**References**\n\n[1] DJ Strouse, Kevin McKee, Matt Botvinick, Edward Hughes, and Richard Everett. Collaborating with humans without human data. NeurIPS 2021.\n\n[2] Chao Yu, Jiaxuan Gao, Weilin Liu, Botian Xu, Hao Tang, Jiaqi Yang, Yu Wang, and Yi Wu. \"Learning zero-shot cooperation with humans, assuming humans are biased\". ICLR. 2023.\n\n[3] George H Dunteman. \"Principal components analysis\", volume 69. Sage, 1989.\n\n[4] Zhao, Rui, et al. \"Maximum entropy population-based training for zero-shot human-ai coordination\". AAAI. 2023.\n\n[5] Li, Yang, et al. \"Cooperative Open-ended Learning Framework for Zero-shot Coordination.\" ICML. 2023.\n\n[6] Carroll, Micah, et al. \"On the utility of learning about humans for human-ai coordination.\" NeurIPS. 2019.\n\n[7] Bard, Nolan, et al. \"The hanabi challenge: A new frontier for ai research.\" Artificial Intelligence. 2020.\n\n[8] Lupu, Andrei, et al. \"Trajectory diversity for zero-shot coordination.\" ICML, 2021.\n\n[9] Hu, Hengyuan, et al. \"\u201cother-play\u201d for zero-shot coordination.\" ICML. 2020.\n\n[10] Lucas, Keane, and Ross E. Allen. \"Any-Play: An Intrinsic Augmentation for Zero-Shot Coordination.\" AAMAS. 2022.\n\n[11] Kurach, Karol, et al. \"Google research football: A novel reinforcement learning environment.\" AAAI. 2020."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245560391,
                "cdate": 1700245560391,
                "tmdate": 1700246317053,
                "mdate": 1700246317053,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Aes6FrpsmP",
                "forum": "wTRpjTO3F7",
                "replyto": "nFdoryUjju",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1499/Reviewer_PVtL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1499/Reviewer_PVtL"
                ],
                "content": {
                    "title": {
                        "value": "Some followup comments"
                    },
                    "comment": {
                        "value": "I thank the authors for make a thorough rebuttal that partly address my questions. Here are some followup comments about the authors' response that better express my concerns.\n\n### Relations between the method and \"diversity-complete\"\n\nI acknowledge that implementing a real \"diversity-complete\" set can be hard. However, I cannot foresee a strong relation from the desiderata, e.g., the necessity of dividing diversity into skill-style diversity and skill-level diversity. Though the authors use BR-Div to select policies, it is doubtful that the construction process actually generate diverse population. \n\nBesides, can you show the process of that \"we collect the earlier checkpoints of the selected fully trained evaluation partners to increase the skill-level diversity\" in Algorithm 1?\n\n### Discussion between our method and previous methods & Overcooked Evaluation Overhaul\n\nI think Table 1 is generally useful for clarifying related work. My concern is about how to compare different evaluation methods (e.g., policy construction methods) practically. Although you mention pros & cons of other methods (e.g., human players and self-play agents), I think a more significant work is to show what indeed Behavior Preferring Evaluation can bring (e.g., requirements of emerging behaviors that cannot be found by other evaluation methods). Evaluations and comparisons about this point can be helpful to illustrate the superiority of your method.\n\n### Extend to Multi-agent Setting\nI do not think splitting multiple agents into two groups is a trivial idea of MARL, but I believe that considering multi-agent case can be a future extension. \n\n### About Behavior Feature Representation and Event-based Reward Design\nThe diversity of population generation seems highly depend on the design of events and weights. Probably the method may overfit the Overcooked environment. I used to think that the weights and behavior features are given from learning. The current approach, however, may restrict its use cases, as there can be various evaluation designs for an environment, with different implementations of events and weights.\n\nBesides, how do you choose the weights and what is the meaning of multiple weights for an event?"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625575583,
                "cdate": 1700625575583,
                "tmdate": 1700625575583,
                "mdate": 1700625575583,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Jy8FmmPfhl",
                "forum": "wTRpjTO3F7",
                "replyto": "URkLM9r9kd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request for Consideration of Score Increase"
                    },
                    "comment": {
                        "value": "Thank you for your valuable time and insightful feedback. We have made significant modifications to our paper based on your suggestions and thoroughly explained your concerns. Our work makes unique contributions to the zero-shot coordination problem, including the fact that we are the first to investigate the evaluation of ZSC capability and formally define the concepts of ideal evaluation partners. We propose an effective evaluation workflow and first provide a benchmark on the most used Overcooked environment. We believe these contributions are of significant relevance to research in this field.\n\nAs you may be aware, unlike previous years, the discussion period this year can only last until November 22, and we are rapidly approaching this deadline.\n\nConsidering the above improvements and contributions to our paper, we kindly request that you consider increasing the score of our manuscript. We believe these improvements reflect our commitment to quality and strongly enhance the overall quality of the paper.\n\nIf you have any further questions or require more information to raise your score, please feel free to let us know. \n\nThank you again for your time and consideration.\n\nSincerely, authors"
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725470262,
                "cdate": 1700725470262,
                "tmdate": 1700732586932,
                "mdate": 1700732586932,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BJ9TZck6vO",
            "forum": "wTRpjTO3F7",
            "replyto": "wTRpjTO3F7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1499/Reviewer_tgST"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1499/Reviewer_tgST"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new evaluation metric for the zero-shot coordination problem. The method involves first training diverse policies and the corresponding best responses, employing the similar method as HSP, and then select evaluation partners based on the best response diversity metric. \nThe ego agent is then evaluated with the selected partners and a metric called best response proximity is calculated based on the performance of the ego agent versus the best response policy. The experiment results demonstrate that some widely used layouts in the literature may lack enough complexity to evaluate the effectiveness of different methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. In general, the authors propose an important question that the evaluation protocol should be improved in the literature of ZSC problem. \n2. The idea that uses a set of sufficiently diverse policies as evaluation partners is straightforward and promising. \n3. The experiment result demonstrates the effectiveness of the proposed metric to distinguish different methods in conflicts layouts."
                },
                "weaknesses": {
                    "value": "A significant limitation is the absence of a crucial baseline, specifically HSP. Since the paper's approach to training evaluation candidates and best response policies closely mirrors that of HSP, it should not pose a substantial challenge to also train an ego agent for HSP. I would consider increasing my score if this limitation is addressed."
                },
                "questions": {
                    "value": "1. In figure 3, how would the population diversity of selection with population diversity lower than that of selection with BR-Div? This seems to be counterintuitive as it is expected to find the subset with largest population diversity if P-Div is used as the selection metric.\n2. Please explain the difference among different layouts in more details.  Are there other kinds of events except conflicts should be considerred to influence the perfomance of different methods in the introduced layouts?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1499/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1499/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1499/Reviewer_tgST"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1499/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698569370555,
            "cdate": 1698569370555,
            "tmdate": 1700573563373,
            "mdate": 1700573563373,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jeBAcymSNh",
                "forum": "wTRpjTO3F7",
                "replyto": "BJ9TZck6vO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tgST (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's insightful comments and suggestions, and we are glad to learn that you regard the evaluation of ZSC capability as an important problem.\n\n### 1. Comparing HSP\n> (Weakness) A significant limitation is the absence of a crucial baseline, specifically HSP. Since the paper's approach to training evaluation candidates and best response policies closely mirrors that of HSP, it should not pose a substantial challenge to also train an ego agent for HSP. I would consider increasing my score if this limitation is addressed.\n\nThank you for suggesting that HSP be included in the Overcooked evaluation. The HSP algorithm has been implemented and included in the comparison. We have updated the HSP results in figures 4, 5, 10, and 11. We present the performance rank under our evaluation workflow as follows for your convenience. Table 4 of Appendix B.3 contains the rankings as well.\n\n| Method \\ Rank | 1 | 2 | 3 | 4 | 5 | 6 |\n| --- | --- | --- | --- | --- | --- | --- |\n| SP     | 0.0 | 5.56 | 11.11 | 5.56 | 5.56 | 72.22 |\n| FCP  |  5.56 | 16.67 | 44.44 | 5.56 | 22.22 | 5.56  |\n| MEP  |  38.89 | 22.22 | 22.22 | 5.56 | 11.11 | 0.0  |\n| TrajDi | 11.11 | 16.67 | 16.67 | 33.33 | 5.56 | 16.67 | \n| COLE   | 0.0 | 11.11 | 0.0 | 27.78 | 55.56 | 5.56   |\n| HSP | 44.44 | 27.78 | 5.56 | 22.22 | 0.0 | 0.0|\n\nThe table shows the percentage of ranks that ZSC methods achieve; for example, HSP ranks first at 44.44 percent of all layouts and population sizes. The results show that HSP outperforms the other algorithms evaluated.\n\n### 2. BR-Div is counterintuitive?\n> (Question 1) In figure 3, how would the population diversity of selection with population diversity lower than that of selection with BR-Div? This seems to be counterintuitive as it is expected to find the subset with largest population diversity if P-Div is used as the selection metric.\n\nBR-Div is a novel diversity metric for population diversity in the ZSC domain, as we demonstrate. We argue that the diversity of a set of cooperative agents' best responses should be used to measure population diversity.\n\nThe insight is that the ego agent strives to emulate the BRs to any partner in the training population. As a result, **an ego agent with higher ZSC capability emulates more BRs**. To evaluate the ZSC capability, we should measure the ego agent's ability to emulate a large number of BRs, i.e., expose the ego agent to partners with a large number of BRs.\n\nWe verify the effectiveness of BR-Div in Figure 2, Figure 3, Figure 8 and Figure 9, both qualitatively and quantitatively. **A simple and intuitive explanation of why BR-Div is more effective than P-Div is that two different partners may respond to similar BRs or even the same BR.** This phenomenon has also been observed in recent works [1,2]."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244935350,
                "cdate": 1700244935350,
                "tmdate": 1700246909577,
                "mdate": 1700246909577,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iMA19F58Sw",
                "forum": "wTRpjTO3F7",
                "replyto": "BJ9TZck6vO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1499/Reviewer_tgST"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1499/Reviewer_tgST"
                ],
                "content": {
                    "comment": {
                        "value": "As the authors present additional experiment results to resolve my reservations, especially incorporating HSP as a baseline, I decided to raise my score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573547687,
                "cdate": 1700573547687,
                "tmdate": 1700573547687,
                "mdate": 1700573547687,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VCJA1kVAqv",
            "forum": "wTRpjTO3F7",
            "replyto": "wTRpjTO3F7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1499/Reviewer_dEYW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1499/Reviewer_dEYW"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the evaluation metric for zero-shot coordination. The authors propose to construct diverse evaluation partners with their approximate BRs, and then compute the proposed BR-Prox across these evaluation partners as the metric. BR-Prox measures the performance similarity between the ego agent and the approximate BRs of the evaluation partners."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The studied question is important and interesting for zero-shot coordination\n- The paper is easy to follow"
                },
                "weaknesses": {
                    "value": "- The main concern is the practicality of the proposed evaluation method. It evolves complicated steps to construct such evaluation agents and prepare their approximate BRs. The idea of computing the BR diversity is straightforward, but it is not easy to really use such a metric in practice. And the huge implementation effort would definitely reduce its impact on the community.\n- The design of reward space is crucial for the proposed evaluation partners. However, it is clearly discussed, at least I didn\u2019t find it yet in the main text. What\u2019s more, it is hard to say the resulting partners would show expected reasonable behaviors.\n- In addition to the evaluation agents, the proposed method requires users to compute the approximate BRs. In simple tasks like Overcooked, it could be fine. However, it could be hard to obtain in complex robot tasks."
                },
                "questions": {
                    "value": "- How to compute the approximate BR, $\\widehat{BR}(\\pi_{\\omega})$ ?\n- How to design the reward space?\n- If all of these evaluation agents can constructed, why not use them to train the ego agents?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1499/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1499/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1499/Reviewer_dEYW"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1499/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834219121,
            "cdate": 1698834219121,
            "tmdate": 1700661687456,
            "mdate": 1700661687456,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U7BNaCH6mP",
                "forum": "wTRpjTO3F7",
                "replyto": "VCJA1kVAqv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dEYW (1/3)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's insightful comments and are glad to learn that you regard our work as important and interesting. We understand the concerns about practicality when applying our evaluation workflow to complex tasks. We want to clarify that the additional implementation effort is not as heavy as intuitively understood, and we have opened our code and models to reduce the implementation effort.\n\n**We first explain the practicality of computing approximate BRs and describe how we design the reward space, then further address the concern about practicality.**\n\n### 1. Practicality of Approximate BR\n> (Question 1) How to compute the approximate BR?\n\n> (Weakness 3) In addition to the evaluation agents, the proposed method requires users to compute the approximate BRs. In simple tasks like Overcooked, it could be fine. However, it could be hard to obtain in complex robot tasks.\n\n**Computing approximate BR.** We have detailed discussed why training approximate BRs is practical for complex tasks in the common response. Our evaluation workflow includes two kinds of approximate best responses. We further explain how we compute the approximate BR here.\u00a0\n\nThe first ones are obtained when approximating the Nash Equilibrium (NE) of the two-player Markov game constructed with event-based rewards. As described in the paragraph Event-based Behavior Preferring Partners of Sec. 4.2, the two agents receiving an event-based reward and the original task reward, respectively, construct a two-player Markov game. Then, each agent works on their own to maximize their own rewards using the Proximal Policy Optimization (PPO) algorithm. This converges to the NE of this two-player Markov game because the event-based reward still encourages the agents to work together [1]. Then we obtain an approximate NE, which consists of two policies that approximate the best responses to each other. Thus, we obtain a fully trained behavior-preferring partner and its approximate BR. **It won't be more difficult than training the ZSC methods in this case because approximating the NE is like training a self-play agent-pair**.\n\nThe second ones are the approximate best responses to earlier checkpoints of the fully trained behavior preferring agents. We train an approximate BR by performing the PPO algorithm to maximize the original task reward with the partner as one of those earlier checkpoints. **The complexity of this case will be lower than training a self-play agent pair since one of the agents is fixed.**\n\nTherefore, we would like to alleviate the concern raised about the feasibility of obtaining Approx. BRs in more complex tasks, such as robotics. We argue that training partner agents and Approx. BRs could be more practical than intuitively thought and more convenient and practical compared to frequently recruiting humans for evaluations.\u00a0\n\nIf an algorithm is capable of training two robots to collaboratively solve complex tasks, i.e., at least capable of training a self-play agent pair to solve complex tasks, **the extension of this capability to approximate an NE or train strong responses to fixed partners should be viewed as practically achievable**. The creation of these kinds of algorithms naturally shows that they can handle the computational and training problems that come with making approximate BRs.\n\nMoreover, it's important to understand that we only approximate the BRs instead of finding the exact BRs in our framework. The approximate BRs serve primarily as strong responses for each evaluation partner. The implementation and computation requirements for approximating BRs are much easier than finding exact BRs. With feasible implementation and computation costs, approximate BRs benefit the ZSC evaluation by supporting the BR-Div and providing a fair and robust measurement for ZSC performance."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244245228,
                "cdate": 1700244245228,
                "tmdate": 1700246717391,
                "mdate": 1700246717391,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0wIdnkgz4h",
                "forum": "wTRpjTO3F7",
                "replyto": "VCJA1kVAqv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dEYW (2/3)"
                    },
                    "comment": {
                        "value": "### 2. Reward Space Design\n> (Question 2) How to design the reward space?\n\n> (Weakness 2) The design of reward space is crucial for the proposed evaluation partners. However, it is clearly discussed, at least I didn\u2019t find it yet in the main text. What\u2019s more, it is hard to say the resulting partners would show expected reasonable behaviors.\n\nWe have presented our reward-space design in Sec. 4.2, and we further give the details of reward-space in Overcooked in Sec. 5. Specifically, the event-based reward space can be formulated as follows:\n$$\n\\mathcal{R}=\\\\{r _{\\bf{w}}| r _{\\bf{w}}(s _{t}, \\bf{a} _{t}) = r + \\phi(s _{t}, \\bf{a} _{t})^{T}\\bf{w}, \\bf{w} \\in \\mathbb{R}^{m}, \\|\\bf{w}\\| _{\\infty} \\leq B _{\\text{max}}, \\sum _{i} \\mathbb{1}(\\bf w _{i} \\neq 0) \\leq C _{\\text{max}}\\\\}\n$$\nThe hyper-paramters $B _{\\text{max}}$ and $C _{\\text{max}}$ are used to restrict the reward space to indicate reasonable behaviors. $\\phi(s _t,\\bf{a} _{t})$ is the indicator of pre-defined events.\n\nEvent-based rewards have been widely used in lots of complex tasks, including robotics [2], large-scale games [3,4] and product management [5]. **We acknowledge that event-based rewards cannot promise to generate all the expected behaviors, while we can indeed get diverse reasonable behaviors after traversing the designed reward space in Overcooked.** \n\n**We provide visualizations of learned behaviors in Figures 8 and 9 to show that we indeed learned diverse, reasonable behaviors.**\n\nMoreover, we give an example of how to design the reward space.\n\n### An example of Overcooked\nIn Overcooked, we use $B_{\\text{max}} = 20$, $C_{\\text{max}}=3$ and generate up to 194 candidates and select up to 30 evaluation partners. The generated candidates are excluded if they cannot complete a delivery when cooperating with their BRs. The pre-defined events are listed as follows:\n| Events | Weights |\n| --- | --- |\n| Put an onion or a dish or a soup onto the counter  | 0  |\n| Pickup an onion or a dish or a soup from the counter | 0 |\n| Pickup an onion from the onion dispenser | -20,0,10  |\n| Pickup a dish from the dish dispenser  | -20,0,10 |\n| Pickup a soup | -20,0,5,10 |\n| Place an ingredient into the pot | -20,0,3,10 |\n| Deliver a soup | -20,0 |\n| Stay | -0.1,0,0.1 |\n| Movement | 0 |\n| Order Reward | 0.1,1 |\n\nWe also provide the pseudocode for using these events to show that one can make a moderate effort to implement the event-based rewards.\n```python=\nstate = current state of the game\naction = action of the agent whose events are collected\nreward = 0\nfor e_i in range(NUM_EVENTS):\n    if True == EVENT_JUDGER(e_i, state, action):\n        reward += EVENT_WEIGHTS[e_i]\n```\nThe details are also present in Appendix B.1.\n\n### 3. Practicality Concerns\n> (Weakness 1) The main concern is the practicality of the proposed evaluation method. It evolves complicated steps to construct such evaluation agents and prepare their approximate BRs. ... . And the huge implementation effort would definitely reduce its impact on the community.\n\n**We have clarified that the steps, including designing event-based rewards, generating evaluation partners, and training approximate BRs, are practical in the above replies.**\u00a0\n\nWe point out that in the rising ZSC community, Overcooked is the most popular and most used for algorithm evaluation, while the community lacks a fair and comprehensive evaluation method. Our evaluation workflow is a strategic enhancement that promises to deliver a fair and comprehensive evaluation of the ZSC algorithm for the current tasks at hand. **Understanding the potential complexity of this implementation, we open-source our code and train agent policies to make a conscious effort to alleviate the burden on the following researchers.**\n\nIt's crucial to emphasize that our method is far from impractical in complex environments like robotics. **In scenarios where the algorithm under evaluation successfully trains an agent capable of completing the task, incorporating our evaluation process is entirely feasible and beneficial.** The idea of an event-based reward adds the chance for human involvement within the set parameters of evaluation, giving you more precise control over the situations in which the algorithm is evaluated. For instance, if the objective is to enhance an agent's adaptability to various human reaction speeds, we can effectively design event-based rewards to create evaluation partners with differing response times. Moreover, building a reliable evaluation method is a prerequisite for the vigorous development of the ZSC community. As a result, even though we acknowledge that any new evaluation method will inevitably require some implementation work, we think that the potential advantages of using our framework outweigh the practical difficulties. By providing our implementation and offering ready-to-use components, we aim to enhance the accessibility and impact of our evaluation method within the ZSC research community."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244782135,
                "cdate": 1700244782135,
                "tmdate": 1700246796094,
                "mdate": 1700246796094,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UIYRcFTXht",
                "forum": "wTRpjTO3F7",
                "replyto": "VCJA1kVAqv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dEYW (3/3)"
                    },
                    "comment": {
                        "value": "### 4. Novelty of BR Diversity\n> (Weakness 1) The idea of computing the BR diversity is straightforward.\n\nWe emphasize that we propose a novel metric for population diversity in the ZSC domain, BR-Div. We argue that the measurement of population diversity among a group of cooperative agents should be based on the diversity exhibited in their best responses. The idea is that the ego agent strives to replicate the behavior of the best response (BR) while interacting with any partner within the training population.\u00a0\n\n**Thus an ego agent with better ZSC capability means that the ego agent emulates more BRs. To evaluate the ZSC capability, we should measure that capability of emulating a lot of BRs**, i.e., expose the ego agent to partners with a lot of BRs.\u00a0\n\nWe verify the effectiveness of BR-Div via experiments, and the results are shown in Figure 2, Figure 3, Figure 8, and Figure 9, both qualitatively and quantitatively. A simple and intuitive explanation of why BR-Div is more effective than the population diversity of partners is that two different partners may correspond to similar BRs or even the same BR [6].\n\n\n### 5. Why not use the evaluation partners to train the ego agents?\n> (Question 3) If all of these evaluation agents can constructed, why not use them to train the ego agents?\n\nTo clarify, our primary focus is on developing an evaluation methodology with a keen emphasis on the fairness of comparisons. **The key challenge of the ZSC problem is generating ego agents that can cooperate with unknown partners.** Thus, we generate a set of diverse evaluation partners, evaluate the agents with these evaluation partners, and measure the ZSC performance with BR-Prox. The problem of how to train ZSC agents is beyond our scope.\n\nWe discuss this problem in two cases:\n- Under our evaluation workflow, training an ego agent using the evaluation partners and evaluating the trained ego agent with the same set of evaluation partners is similar to using the test dataset as the training data in the supervised learning algorithms, which is unreasonable.\n- However, if you mean training the ego agent with a set of diverse partners constructed using our method and evaluating the trained ego agent with another set of constructed agents, we have added similar experiments to the updated paper. HSP is an algorithm that constructs partners to simulate human behaviors and trains the ego agent with those constructed partners. We train the HSP ego agents with unselected evaluation partner candidates and evaluate the HSP ego agents with the selected evaluation partners.\n\nWe hope our reply addresses your concerns about practicality, and we are glad to discuss further if you have any other suggestions or questions.\n\n**Reference**\n\n[1] Ding, Dongsheng, et al. \"Independent policy gradient for large-scale markov potential games: Sharper rates, function approximation, and game-agnostic convergence.\" ICML. 2022.\n\n[2] Brohan, Anthony, et al. \"Do as i can, not as i say: Grounding language in robotic affordances.\" CoRL. 2023.\n\n[3] Kurach, Karol, et al. \"Google research football: A novel reinforcement learning environment.\" AAAI. 2020.\n\n[4] Deheng Ye, et al. Mastering Complex Control in MOBA Games with Deep Reinforcement Learning. AAAI. 2020.\n\n[5] Shi, Zhenyu, et al. \"Learning expensive coordination: An event-based deep RL approach.\" ICLR. 2020.\n\n[6] Sarkar, Bidipta, Andy Shih, and Dorsa Sadigh. \"Diverse Conventions for Human-AI Collaboration.\" NeurIPS. 2023."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244869351,
                "cdate": 1700244869351,
                "tmdate": 1700246855003,
                "mdate": 1700246855003,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lXjlqS0V9i",
                "forum": "wTRpjTO3F7",
                "replyto": "UIYRcFTXht",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1499/Reviewer_dEYW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1499/Reviewer_dEYW"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your effort"
                    },
                    "comment": {
                        "value": "I appreciate the authors' effort in the discussion, especially regarding the complexity of the proposed method. I think I understand what you mean by saying \"The complexity of this case will be lower than training a self-play agent pair\". However, based on my understanding of the whole pipeline of the proposed method, I don't believe the method is easy to use or transfer to other domains except the overcooked. Anyway, I thank your effort and would raise my score to weak rejection."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661664001,
                "cdate": 1700661664001,
                "tmdate": 1700661664001,
                "mdate": 1700661664001,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VGMm2FWuFW",
            "forum": "wTRpjTO3F7",
            "replyto": "wTRpjTO3F7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1499/Reviewer_6Vtg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1499/Reviewer_6Vtg"
            ],
            "content": {
                "summary": {
                    "value": "The performance of the agent's Zero Shot Coordination (ZSC) capability is difficult to measure and quantify. The difficulties are twofold: (1) how to construct sufficient diverse evaluation partners? (2) how to measure the performance? Most previous methods focus on designing superior ZSC algorithms while not paying much attention to the evaluation metric. \n\nThis paper first proposes to construct 'diversity-complete' evaluation partners by maximizing the best response diversity (the population diversity of the BRs to the evaluation partners). Then the paper proposes a Best Response Proximity (BR-Prox) metric, which quantifies the ZSC capability as the performance similarity to each evaluation partner\u2019s approximate best response, demonstrating generalization capability and improvement potential.\n\nEvaluations conducted on the overcooked environment validate the effectiveness of the proposed evaluation workflow and show some interesting results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper may be the first to systematically study how to measure and quantify the agent's Zero Shot Coordination (ZSC) capability. The proposed evaluation workflow is technically sound. \n* Sufficient experiments are designed to demonstrate the effectiveness of the method. The results find that the most used layouts in the overcooked environment cannot show the ZSC capability difference among the ZSC methods.\n* Overall, the writing of the article is relatively clear."
                },
                "weaknesses": {
                    "value": "* Some parts of the paper are not very clear. For example, the motivation for introducing the event-based rewards is not clear. \n* In practical implementation, the method requires humans to manually define some triggered events so as to derive diverse behaviors, which is difficult to obtain in complex tasks.\n* Comparisons with previous baselines listed in Table 1 may be missing."
                },
                "questions": {
                    "value": "Please see the weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1499/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1499/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1499/Reviewer_6Vtg"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1499/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699859758364,
            "cdate": 1699859758364,
            "tmdate": 1699859758364,
            "mdate": 1699859758364,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nnXbetj4PP",
                "forum": "wTRpjTO3F7",
                "replyto": "VGMm2FWuFW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6Vtg (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's insightful comments and suggestions, and we are glad to learn that you consider our work sound and interesting.\n\n### 1. Event-based reward\n> \uff08Weakness 1\uff09Some parts of the paper are not very clear. For example, the motivation for introducing the event-based rewards is not clear.\n\nThank you for suggesting potential improvements to the presentation. We have elaborated on some technical details in the updated paper.\n\nSpecifically, we explain our motivation for introducing event-based rewards in Sec. 4.2. We expect partners to have diverse and reasonable behaviors, while the behavior space is intractably large. Inspired by [1] in which human preferences can be regarded as event-centric and modeled as event-based reward functions, we apply the event-based reward shaping method to encourage behavior discovery and generate behavior preferring partners.\n\n> (Weakness 2) In practical implementation, the method requires humans to manually define some triggered events so as to derive diverse behaviors, which is difficult to obtain in complex tasks.\n\nWe have detailedly discussed the practicality of our evaluation method in the common response. Specifically, we argue that the extra human effort is not as heavy as intuitively thought when applying our evaluation workflow to a new task. Defining some triggered events in complex tasks is very similar to conventional feature and reward engineering, which is inevitable when exploring new tasks. Moreover, event-based reward shaping has already been applied in both complex robot tasks [2] and large multi-agent games [3,4]. So defining events may not cost a lot of extra effort and is general to the ZSC domain. Besides, there indeed exist works that learn human behaviors without assuming a set of events [5] and we leave making these methods general to the ZSC domain as future work.\n\nWe additionally give an example of the event-based reward design in Overcooked to explain the feasibility of designing event-based rewards. The pre-defined events are listed as follows:\n\n| Events | Weights |\n| --- | --- |\n| Put an onion or a dish or a soup onto the counter  | 0  |\n| Pickup an onion or a dish or a soup from the counter | 0 |\n| Pickup an onion from the onion dispenser | -20,0,10  |\n| Pickup a dish from the dish dispenser  | -20,0,10 |\n| Pickup a soup | -20,0,5,10 |\n| Place an ingredient into the pot | -20,0,3,10 |\n| Deliver a soup | -20,0 |\n| Stay | -0.1,0,0.1 |\n| Movement | 0 |\n| Order Reward | 0.1,1 |\n\nWe also provide the pseudocode for using these events to show that one can make a moderate effort to implement the event-based rewards.\n```python=\nstate = current state of the game\naction = action of the agent whose events are collected\nreward = 0\nfor e_i in range(NUM_EVENTS):\n    if True == EVENT_JUDGER(e_i, state, action):\n        reward += EVENT_WEIGHTS[e_i]\n```\nThe details are also present in Appendix B.1."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243941622,
                "cdate": 1700243941622,
                "tmdate": 1700246546931,
                "mdate": 1700246546931,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iFRV0H6H2L",
                "forum": "wTRpjTO3F7",
                "replyto": "VGMm2FWuFW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6Vtg (2/2)"
                    },
                    "comment": {
                        "value": "### 2. Comparison with previous evaluation methods\n> (Weakness 3) Comparisons with previous baselines listed in Table 1 may be missing.\n\nWe do not fully understand whether you mean comparison between our evaluation workflow and the previous evaluation methods or comparison among the ZSC algorithms.\n\n**Comparisons with Previous Evaluation Methods**\nWe detailedly compare the previous evaluation methods and our evaluation workflow in the common response, which reveals that previous evaluation partners and metrics cannot provide a comprehensive and fair evaluation and that our evaluation workflow has advantages in measuring the ZSC capability.\n\nWe first clarify that we list the previous evaluation methods in the first column and the works that utilize them in the second column in Table 1.\u00a0\n\n**For the previous evaluation methods**, we have already discussed their defects in Sec. 3.2 and Appendix C. \u00a0\n\nDefects of previous evaluation partners:\n- Human Players: The human players in ZSC problem are 'perfect' candidates for evaluation because human are strictly qualified as unseen partners. While evaluating with human players are expensive, time-consuming and unrepeatable. We need a more efficient evaluation method as a supplement for human evaluation. \n- Human Proxy Agents: Human proxy agents in overcooked environment do not account for human behaviors [5], which shows that using human proxy agents does not represent the diversity of human.\n- Trained Self-play Agents: Train self-play agents are similar to the agents used to train the ego agent and are not diverse, as shown in Figure 2.\n- Rule-based Specialist: Manually building expert rules is difficult to implement in complex environments and may not meet diversity requirements. We point out that designing a set of events is much easier than designing a policy.\n- Random Agents: The diversity of the random initializations cannot be ensured and random initializations lack of high level performance.\n\nDefects of previous metric:\n- Cross-play with Trained Adapted Agents: Some works compare their ZSC methods by cross-playing the agents trained by other ZSC methods and their ego agents. This results include self-play performance and does not completely reflect the capabilities of ZSC. On the other hand, excluding the self-play results leads to potential unfairness.\n\nWe have highlighted the differences in Sec. 2 of the update paper. We also remark that we are the first to systematically investigate the construction of evaluation partners and the measurement of ZSC capability.\n\n**Comparison among ZSC algorithms**\nTo improve the reliability of our evaluation results, we have reevaluated five representative ZSC algorithms: SP, FCP, MEP, TrajeDi, and COLE. We also reevaluate the HSP [1] algorithm in Overcooked and add the results to the revised paper.\n\nWe hope that our explanations and clarifications have addressed your concerns and shed light on the insights behind our methodology and its potential impact. We invite further discussion and are open to any additional questions or suggestions you may have.\n\n**Reference**\n\n[1] Chao Yu, Jiaxuan Gao, Weilin Liu, Botian Xu, Hao Tang, Jiaqi Yang, Yu Wang, and Yi Wu. \"Learning zero-shot cooperation with humans, assuming humans are biased\". ICLR. 2023.\n\n[2] Brohan, Anthony, et al. \"Do as i can, not as i say: Grounding language in robotic affordances.\" CoRL. 2023.\n\n[3] Kurach, Karol, et al. \"Google research football: A novel reinforcement learning environment.\" AAAI. 2020.\n\n[4] Deheng Ye, et al. Mastering Complex Control in MOBA Games with Deep Reinforcement Learning. AAAI. 2020.\n\n[5] Shah, Rohin, et al. \"On the feasibility of learning, rather than assuming, human biases for reward inference.\" ICML. 2019."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244014812,
                "cdate": 1700244014812,
                "tmdate": 1700246612810,
                "mdate": 1700246612810,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uurR5F9q2D",
                "forum": "wTRpjTO3F7",
                "replyto": "VGMm2FWuFW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Reminder for Rebuttal Consideration"
                    },
                    "comment": {
                        "value": "Thanks again for your valuable comments and suggestions. We have submitted our rebuttals to your reviews. We explain the problems and try our best to settle your concerns, including paper revisions.\n\nOur work makes unique contributions to the zero-shot coordination problem, including the fact that we are the first to investigate the evaluation of ZSC capability and formally define the concepts of ideal evaluation partners. We propose an effective evaluation workflow and first provide a benchmark on the most used Overcooked environment. We believe these contributions are of significant relevance to research in this field.\n\nAs you may be aware, unlike previous years, the discussion period this year can only last until November 22, and we are rapidly approaching this deadline. \n\nConsidering the above improvements and contributions to our paper, we kindly request that you consider increasing the score of our manuscript if our revised manuscript and rebuttal more closely meet your expectations for the paper.\n\nIf you have any further questions or require more information to raise your score, please feel free to let us know. We look forward to your response and are eager to continue our discussion.\n\nSincerely, Authors"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725122351,
                "cdate": 1700725122351,
                "tmdate": 1700732340852,
                "mdate": 1700732340852,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]