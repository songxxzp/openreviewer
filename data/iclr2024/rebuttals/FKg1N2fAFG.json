[
    {
        "title": "Towards Mitigating Architecture Overfitting in Dataset Distillation"
    },
    {
        "review": {
            "id": "n7ocne4ReO",
            "forum": "FKg1N2fAFG",
            "replyto": "FKg1N2fAFG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1140/Reviewer_w1VW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1140/Reviewer_w1VW"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a series of methods to address the cross-architecture generalization problem of dataset distillation. In detail, combining the DropPath and knowledge distillation, this paper proposed two adapted methods to use a new model design and loss objective to alleviate the overfitting problem. Besides, other tricks like LR and augmentation policy are also used. In experiments, the proposed method performed decently on the cross-architecture tests."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The proposed method achieved good results on the claimed cross-architecture test.\n\n+ The presentation is easy to follow. The code is provided.\n\n+ The experiments on multiple datasets, models, and settings provide a solid validation for the contribution."
                },
                "weaknesses": {
                    "value": "- The results are impressive, however, the method contributions seem relatively marginal. Though the adapted method absorbed from previous works is non-trivial, the discussion lacks enough insight but is empirical.\n\n- The three-phase Keep Rate looks quite complex for tuning. How is the tuning complexity and robustness if we use the proposed method for many different models?\n\n- Though the bag of methods works well, the whole paper gives the readers a feeling of separation.\n\n- typo: in the abs, performance across different network architectures {"
                },
                "questions": {
                    "value": "1. There were only discussions on the residual architecture, why? There are also other multi-branch architectures.\n\n2. \"As a result, we can also expect DropPath to mitigate the architecture overfitting issue in dataset distillation. \" --- any more detailed analysis?\n\n3. \"Architecture overfitting arises from deeper test networks\" --- any citation or discussion?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1140/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1140/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1140/Reviewer_w1VW"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1140/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697966391798,
            "cdate": 1697966391798,
            "tmdate": 1700573809511,
            "mdate": 1700573809511,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zBEGtT0OAw",
                "forum": "FKg1N2fAFG",
                "replyto": "n7ocne4ReO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1140/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1140/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer w1VW,\n\nThanks for your constructive suggestions and insightful comments. In response to your questions, we offer the following point-to-point answers.\n\nFor the problems mentioned in Weaknesses:\n\n1. \\>\\>\\> The results are impressive, however, the method contributions seem relatively marginal. Though the adapted method absorbed from previous works is non-trivial, the discussion lacks enough insight but is empirical.\n    \n    **Reply:** Although our design is motivated by some existing techniques such as DropPath and knowledge distillation, we adapt them in the context of dataset distillation. For DropPath, we make it applicable to single-branch networks and propose improved shortcut connection and three-phase keep rate to boost the performance. For knowledge distillation, we pioneer the use of small networks as teacher models, which is different from current common practices. In the experiments, our method significantly mitigates the architecture overfitting in dataset distillation and improves the generalization of large networks on small datasets. Section 3 comprehensively demonstrates the motivation of our proposed methods. More fundamental analysis of DropPath and knowledge distillation were well analyzed in many works [1 - 3].\n    \n2. \\>\\>\\> The three-phase Keep Rate looks quite complex for tuning. How is the tuning complexity and robustness if we use the proposed method for many different models?\n    \n    **Reply:** In the ablation study, we evaluate the performance under different hyperparameters (HPs). **Moreover, additional ablation studies on HPs of three-phase keep rate were added**. The result is reported below and in Figure 4 (b) and (c). Compared to the significant performance improvement brought by our method, if we choose different HPs in an **appropriate but wide** range, the performance fluctuation caused by different HPs is negligible. For example, when the final keep rate ranges from 0.6 to 1.0, KD weight ranges from 0.3 to 0.9 and KD temperature ranges from 1 to 10, the performance fluctuation is with 1 percentage point. In addition, although lower minimum keep rates and longer periods of decay induce better performance, they also mean a longer training phase. We set them to 0.5 and 500 to balance the efficiency and performance, respectively. Despite that, the performance fluctuation caused by these two HPs is still insignificant. Therefore, we can demonstrate that our training method is robust to different HPs. In addition, the selected HPs also perform well in different settings, including different distilled data, different IPCs and different test networks. \n\n3. \\>\\>\\> Though the bag of methods works well, the whole paper gives the readers a feeling of separation.\n    \n    **Reply:** The main contribution of our work is proposing a bag of methods to mitigate architecture overfitting in dataset distillation. These methods perform like regularization from different perspectives, such as **network architecture** (i.e. DropPath with three-phase keep rate), and **training scheme** (i.e. knowledge distillation, data augmentation and optimizer). Instead of directly using these methods, we adapt them to our scenario to further improve the performance. In the experiments, we demonstrate that our method not only mitigates architecture overfitting in dataset distillation but also improves the generalization of large models on small datasets. We believe our work provides practitioners with a toolbox and some intuition to solve related problems."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1140/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462157919,
                "cdate": 1700462157919,
                "tmdate": 1700462157919,
                "mdate": 1700462157919,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8hGcdqcqf7",
                "forum": "FKg1N2fAFG",
                "replyto": "CRRY0kDkw6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1140/Reviewer_w1VW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1140/Reviewer_w1VW"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the response. After reading the reviews and the responses, my concern about the method's contribution remains. The other reviewers also raise some important issues. Overall, I think this paper has made contributions and helpful discussions, however,  the whole paper's writing and method design, verification of the experiment can be further improved before acceptance."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1140/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641285549,
                "cdate": 1700641285549,
                "tmdate": 1700641285549,
                "mdate": 1700641285549,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1erH3a2Y4E",
            "forum": "FKg1N2fAFG",
            "replyto": "FKg1N2fAFG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1140/Reviewer_wW36"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1140/Reviewer_wW36"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a set of techniques for training networks on distilled datasets given a fixed (usually small) backbone. The networks for training are not constrained to be the same as those used in the distillation process. The authors argue that the \"architecture overfitting\" will happen in the distillation process.  To mitigate this, the authors propose a set of techniques to improve the evaluation process and obtain better performance. The authors conduct experiments on two baseline methods and show improved test accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is generally written in good quality and easy to follow. \n2. The idea is straight-forward and seem to be effective. \n3. The topic is timely and important."
                },
                "weaknesses": {
                    "value": "Questions: \n\n1.  I am not sure whether it is really the \"overfitting\". Why this is an overfitting? Is there any evidence to show that the performance already saturate on one backbone, but decrease on the other backbones? It could be the case that the distillation process is even \"underfit\" on a given architecture - and the performance drop during \"transferring\" (i.e., distill with one backbone and then evaluate on the others) could be just some normal \"generalization error\". I would suggest to use the terminology more carefully to avoid such an confusion, or provide evidence to support the terminology. \n\n2. From what I am understanding, this work actually alters the evaluation process (the process of training a model on the distilled image set). Augmenting the training process is not novel at the high level as many previous works have already done so. A lot of augmentation to architectures (like DropOut and DropPath etc. ) and data (MixUp, CutMix, AutoAugment, etc. ) could be applied here and I think that can serve as potential baselines. \n\n3. Table 2 provides lower results for MTT baselines. From the original paper, MTT reaches 65.3/71.6 accuracy on CIFAR-10, while in Table 2 the authors reported 63.6 and 70.2. I am curious why there is a performance gap since it may create invalid performance advantages for the proposed techniques. \n \n\n4. Section 4.2 is interesting, but I think it is off-topic. The paper is trying to solve the so-called \"architectural overfitting\" but it will not happen when there is no actual \"fitting\" process. Therefore, I think Contribution 3 is do not count and the contribution bullets should be adjusted."
                },
                "questions": {
                    "value": "See the above section. My score will be updated accordingly after the rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1140/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1140/Reviewer_wW36",
                        "ICLR.cc/2024/Conference/Submission1140/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1140/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698561865168,
            "cdate": 1698561865168,
            "tmdate": 1700634226397,
            "mdate": 1700634226397,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jUyUK7vhMv",
                "forum": "FKg1N2fAFG",
                "replyto": "1erH3a2Y4E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1140/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1140/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer wW36,\n\nThanks for your constructive suggestions and insightful comments. In response to your questions, we offer the following point-to-point answers:\n\n1. \\>\\>\\> I am not sure whether it is really the \"overfitting\". Why this is an overfitting? Is there any evidence to show that the performance already saturates on one backbone, but decreases on the other backbones? It could be the case that the distillation process is even \"underfit\" on a given architecture - and the performance drop during \"transferring\" (i.e., distill with one backbone and then evaluate on the others) could be just some normal \"generalization error\". I would suggest to use the terminology more carefully to avoid such a confusion, or provide evidence to support the terminology.\n    \n    **Reply:** We are sorry about the confusion. From the perspective of evaluation, we argue that the architecture overfitting is consistent with the classical definition of overfitting: a model suffers from overfitting if the gap between its performance on the training set and the test set is big. In our cases, the training accuracy on the distilled dataset is always 100\\% for all network architectures, but the test accuracy on the real dataset is lower, so overfitting definitely occurs as we see generalization gaps. Moreover, the generalization gap is bigger when we train larger networks on the distilled dataset than when we use a small network with the same architecture as the training network. We call such difference *architecture overfitting*, meaning the additional overfitting arising from the architecture difference between the training network and the test network in dataset distillation. From the dataset perspective, we agree with the reviewer that the distilled dataset is \"unfit\" to the original large dataset given a network architecture. We believe that the distilled dataset loses some useful information from the original large dataset, leading to noticeable performance degradation when training models on the distilled dataset. Nevertheless, it is important to note that this paper primarily focuses on examining the network architecture perspective. In this context, we utilize the term *architecture overfitting* to describe the phenomenon under investigation.\n\n2. \\>\\>\\> From what I am understanding, this work actually alters the evaluation process (the process of training a model on the distilled image set). Augmenting the training process is not novel at the high level as many previous works have already done so. A lot of augmentation to architectures (like DropOut and DropPath etc.) and data (MixUp, CutMix, AutoAugment, etc. ) could be applied here and I think that can serve as potential baselines.\n    \n    **Reply: We have added more experiments in the revised manuscript in Table 5 of Appendix B.1**, where we adopt DropOut, DropPath, MixUp and CutMix as baselines and evaluate their performance with the same setting in the ablation study (ResNet18, FRePO, CIFAR10, IPC=10). Note that AutoAugment only supports uint8 data, the pixel value can only be in \\{0, 1, ..., 255\\}. However, distilled data are float32 and their pixel values are neither bounded by [0, 1] nor [0, 255]. As a result, we do not include AutoAugment in the comparison. Similar to AutoAugment, we already adopted 2-fold data augmentation in our method, which also randomly samples 2 augmentations from a pool of augmentations. In addition, from the results shown in Table 2 (w/o DP \\& KD v.s. Full), where *w/o DP \\& KD* already uses better data augmentation and better optimizer, we can expect that pure data augmentation will not outperform our method. The results of the comparison with different baselines are reported in the table below. We can observe that simple DropPath and DropOut with a constant keep rate can even deteriorate the performance. In addition, MixUp and CutMix only lead to marginal performance improvement, which further demonstrates the effectiveness of our method."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1140/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461474481,
                "cdate": 1700461474481,
                "tmdate": 1700461474481,
                "mdate": 1700461474481,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RcODBc1FK2",
                "forum": "FKg1N2fAFG",
                "replyto": "1erH3a2Y4E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1140/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1140/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Continue"
                    },
                    "comment": {
                        "value": "3. \\>\\>\\> Table 2 provides lower results for MTT baselines. From the original paper, MTT reaches 65.3/71.6 accuracy on CIFAR-10, while in Table 2 the authors reported 63.6 and 70.2. I am curious why there is a performance gap since it may create invalid performance advantages for the proposed techniques.\n    \n    **Reply:** We carefully checked the hyperparameters of MTT and found the only difference is that the learning rate they adopted in the evaluation is a learnable parameter, which is learned during the distillation process. To ensure a fair comparison with other methods, we disabled this mechanism and used the initial learning rate they defined before distillation, i.e., 0.01. In addition, as pointed out in the original MTT paper [1], the test accuracies fluctuate a lot with different random seeds. Moreover, the accuracies obtained with our hyper-parameters in Table 2 of the manuscript are in the fluctuation range reported in the MTT paper at some runs, e.g., 64.8 / 71.5. Finally, we need to point out that if we compare MTT with the baselines as listed in [1], the fluctuation of the test accuracies does not challenge the performance superiority of MTT.\n\n4. \\>\\>\\> Section 4.2 is interesting, but I think it is off-topic. The paper is trying to solve the so-called \"architectural overfitting\" but it will not happen when there is no actual \"fitting\" process. Therefore, I think Contribution 3 does not count and the contribution bullets should be adjusted.\n   \n    **Reply:** As pointed out in the answer to Question 1, architecture overfitting accounts for the additional generalization gap when we train models of different architectures on the distilled dataset, compared with training on small datasets. In section 4.2, we demonstrate that our methods contribute more performance improvement in the case of training on distilled dataset than in the case of training on sub-sampled dataset, so it is useful to mitigate *architecture overfitting*. In addition, section 4.2 validates the effectiveness of our methods in sub-sampled dataset, which indicates the versatility of our method.\n\n***Table 1: Comparison between our method and baselines. Note that p in DropPath and Dropout denotes the keep rate and alpha in MixUp and CutMix is the parameters $\\alpha$ and $\\beta$ in beta distribution, where $\\alpha$ and $\\beta$ are the same.***\n| Method |  Test Accuracy |\n| ------ |  ---- |\n| Baseline in the paper | 55.6|\n|DropPath (p=0.5) | 46.2 |\n|DropOut (p=0.5) | 44.3 |\n|MixUp (alpha=0.5) | 57.6 |\n| CutMix (alpha=0.5) | 56.9  |\n| **Ours** | **66.2** |\n\n\n> [1] George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu. Dataset distillation by matching training trajectories. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4750\u20134759, 2022."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1140/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461659451,
                "cdate": 1700461659451,
                "tmdate": 1700461687127,
                "mdate": 1700461687127,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0EHZV3lemj",
                "forum": "FKg1N2fAFG",
                "replyto": "1erH3a2Y4E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1140/Reviewer_wW36"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1140/Reviewer_wW36"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "For the first question I am partly agree with the authors. The overfitting **does** happen **in the retraining (evaluation) phase**. But I do not agree with how the authors divide the scope. \nWhen I first read this paper I had the first impression that the authors are trying to address the overfitting **in the distillation phase**.  This leads me to confusion why architecture overfitting even happens. So either make it more clear or change slightly the writing. This is also the main reason hinders me from providing a higher score. \n\nFor the MTT part, could the authors provide the results with learnable learning rates? I think that is the *standard* \"MTT\" people are trying to compare with. Or mark it explicitly and explain why you adopt a fixed learning rate in the paper. \n\nScores have been adjusted. New experiments are helpful. Thank you."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1140/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634214789,
                "cdate": 1700634214789,
                "tmdate": 1700634359552,
                "mdate": 1700634359552,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lAElLUT1GI",
            "forum": "FKg1N2fAFG",
            "replyto": "FKg1N2fAFG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1140/Reviewer_MbH7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1140/Reviewer_MbH7"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a series of methods to improve the performance of models trained by synthetic datasets obtained through dataset distillation. Especially, the paper focuses on models with different architectures from those seen during dataset distillation. The proposed strategies include DropPath with a three-phase keep rate, knowledge distillation, and some other designs on learning rate, optimizer, and data augmentation. Experiments demonstrate that the proposed method improves the performance of training on different architectures using synthetic datasets in dataset distillation by a large margin."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This is the first work that explicitly focuses on the cross-architecture issue in dataset distillation. The topic itself is very meaningful since dataset distillation has been demonstrated to be easily overfitting to a single architecture used in training.\n2. The writing is coherent and easy-to-follow.\n3. The experiments are sufficient to demonstrate the performance of the proposed strategies and the advantages over existing baselines."
                },
                "weaknesses": {
                    "value": "1. I do not think the cross-architecture generalization problem should be addressed this way. The authors do not modify the process of obtaining synthetic datasets during dataset distillation. Instead, they modify the training strategies **given** synthetic datasets. In dataset distillation, we should definitely focus on the former and should not make any assumptions on how users should use provided synthetic datasets for **downstream** training. What I want to see is actually a thorough algorithm for the process of dataset distillation that can improve the cross-architecture performance following the original evaluation protocols. The current form does not really enhance dataset distillation. The improvement is from the strategies of using distilled datasets.\n2. If the paper focuses on how to use distilled datasets, the problem can be cast to some more classical problems, like how to avoid overfitting, synthetic-to-real generalization, few-shot learning, etc. The authors fail to make a broader discussion.\n3. The technical novelty is limited because the authors only provide strategies with minor designs that can empirically improve the performance. Without sufficient analysis, the principles of how the proposed methods work are unclear, which results in limited scientific value.\n4. The proposed methods are somewhat complicated. For example, they assume users would apply a three-phase keep rate during training with DropPath, which introduces lots of hyper-parameters and makes the pipeline complex and less robust."
                },
                "questions": {
                    "value": "Please refer to Weaknesses for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1140/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698671717973,
            "cdate": 1698671717973,
            "tmdate": 1699636040217,
            "mdate": 1699636040217,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gZCZPtEY16",
                "forum": "FKg1N2fAFG",
                "replyto": "lAElLUT1GI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1140/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1140/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer MbH7,\n\nThanks for your insightful and constructive comments. In response to your questions, we offer the following point-to-point answers:\n\n1.  \\>\\>\\> I do not think the cross-architecture generalization problem should be addressed this way. The authors do not modify the process of obtaining synthetic datasets during dataset distillation. Instead, they modify the training strategies **given** synthetic datasets. In dataset distillation, we should definitely focus on the former and should not make any assumptions on how users should use provided synthetic datasets for **downstream** training. What I want to see is actually a thorough algorithm for the process of dataset distillation that can improve the cross-architecture performance following the original evaluation protocols. The current form does not really enhance dataset distillation. The improvement is from the strategies of using distilled datasets.\n    \n     **Reply:** We agree with the reviewer that proposing novel dataset distillation methods may help mitigate architecture overfitting and is definitely a direction for exploration. However, we argue that it is also meaningful to improve the training strategies using the distilled dataset. Our argument is supported by the following points:\n\n    - The overfitting issue studied in this work is not **solely** caused by dataset distillation because a similar overfitting phenomenon is also observed when we train models on tiny real datasets. For example, the results in Section 4.2 indicate overfitting happens when we use a tiny sub-sampled training set to train deep neural networks. Our proposed methods can also improve the performance in this context.\n\n    - Following the first point, from the results in Section 4.1 and 4.2, we observe that training on distilled datasets has better performance than training on sub-sampled datasets. However, the performance gaps between the small and big networks are also larger when training on the distilled datasets. In this context, our proposed methods can induce more performance boosts, so it is more meaningful to apply our methods for distalled datasets, which is also the focus of this paper.\n\n    - In addition, our methods are orthogonal to dataset distillation since our method focuses on **how to use the distilled dataset**, and dataset distillation focuses on **how to obtain the distilled dataset**. These two categories of methods can help improve each other. For example, by adopting our method, we can more comprehensively evaluate the performance on different distilled datasets and thus compare different dataset distillation methods fairer.\n\n2. \\>\\>\\> If the paper focuses on how to use distilled datasets, the problem can be cast to some more classical problems, like how to avoid overfitting, synthetic-to-real generalization, few-shot learning, etc. The authors fail to make a broader discussion.\n    \n    **Reply:** As mentioned above, we argue that discussing our method in the context of dataset distillation is practical. In addition to the distilled dataset, in section 4.2, we also demonstrate the effectiveness of our method when we train deep neural networks on small but real sub-sampled training sets. Although we see bigger performance improvement in applying our method to distilled datasets, its effectiveness on sub-sampled datasets indicates its broader applications to avoid overfitting. However, we argue that the other two cases pointed out by the reviewer are different from our problem settings.\n\n    1. **Synthetic-to-real generalization:** It measures the performance of the models that are trained on the synthetic data but tested on the real data. The goal of this task is to narrow down the gap between the performance on synthetic and real data and thus to make the models more robust to domain shifts [1]. That is to say, the key challenge in generalization lies in addressing the domain shifts rather than insufficient training data or architecture differences as in our work.\n\n    2. **Few-shot learning**: it aims to adapt pre-trained models for new tasks from very few instances. The key to few-shot learning is how to construct a pre-train model on several related tasks with enough training data in the meta-training phase so that it can generalize well to unseen (but related) tasks with just a few examples during the meta-testing phase [2]. However, we focus on training a model from scratch on a small dataset, which is different from few-shot learning settings.  \n\n    In our revised manuscript, we add a brief discussion about these related tasks. Customizing our method for them is out of the scope of this paper. We leave them as future works."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1140/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460065282,
                "cdate": 1700460065282,
                "tmdate": 1700460065282,
                "mdate": 1700460065282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rEATWWU5hP",
                "forum": "FKg1N2fAFG",
                "replyto": "gZCZPtEY16",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1140/Reviewer_MbH7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1140/Reviewer_MbH7"
                ],
                "content": {
                    "comment": {
                        "value": "I sincerely thank the authors for the responses. However, I still think the methods presented in the paper is somewhat empirical and heuristic. I tend to maintain my original score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1140/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626547043,
                "cdate": 1700626547043,
                "tmdate": 1700626547043,
                "mdate": 1700626547043,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WdgdjXAcwm",
            "forum": "FKg1N2fAFG",
            "replyto": "FKg1N2fAFG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1140/Reviewer_Mz7n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1140/Reviewer_Mz7n"
            ],
            "content": {
                "summary": {
                    "value": "The submission considers the problem of architecture overfitting in dataset distillation. In dataset distillation, one derives a small synthetic dataset from a given larger, real dataset (along with a \u201ctraining\u201d network) that captures the learnable properties of the real dataset and can be used to train other \u201ctest\u201d networks more efficiently to hopefully achieve similar performance as achievable with the real dataset. An issue that has been noticed in prior work is that when the training and testing networks differ more in architecture, the distilled synthetic dataset starts being less useful, in terms of the task performance.\n\nThe submission suggests two main modifications to training the test networks. (1) Since the training networks used for distillation tend to be somewhat shallow, while the downstream test networks are intended to be larger, one heuristic is to train the test networks with a form of regularization that simulates shallower networks. In particular, dropping layers randomly can simulate a form of shallowness (with linear transforms to account for dimension matching). (2) Since it is known that performance drops upon moving to a larger network, one can further attempt to improve training of the test networks by using knowledge distillation to match the predictive distributions of the test network to the smaller training network acting as teacher.\n\nThese two tricks seem useful, illustrating significant improvements on existing benchmarks, for a choice of existing dataset distillation methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: While the techniques discussed aren\u2019t particularly original, the exploration in the context of dataset distillation seems unique, to my knowledge.\n\nQuality: The paper seems to be of reasonable quality overall.\n\nClarity: The paper is reasonably clear, as long as the reader is already familiar with how dataset distillation works. (I wasn\u2019t very familiar, and needed to skim past literature to follow the procedure and nomenclature.)\n\nSignificance: My naive understanding about the practical relevance of dataset distillation is that it has the potential to enable training very large models efficiently, by minimizing the dataset size, as well as applications where training for longer is a bottleneck (as in continual learning and neural architecture search). Modifying the training procedure of the test networks with sensible regularizations that enable this transfer can be quite significant in practice."
                },
                "weaknesses": {
                    "value": "I sense no major weaknesses in the submission. Some minor questions remain which are listed in the following section."
                },
                "questions": {
                    "value": "1. I\u2019m not sure I followed the reasoning behind the scaling of the DropPath output maps. It would be nice to have a derivation in the Appendix for how this scaling matches the expectations from training to test.\n\n2. There\u2019s a statement that architecture overfitting occurs due to depth and not width \u2014 has this been recognized in existing work?\n\n3. In the Three-Phase Keep Rate section on page 4, it is said that the variance increases as p increases: isn\u2019t the variance maximal at p = 0.5?\n\n4. There seem to be some hyper-parameters involved, such as the values shaping the shape of the three-phase cycle and the temperature in knowledge-distillation. How are these hyper-parameters tuned? The experiments suggest direct evaluation on test set performances."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1140/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812848370,
            "cdate": 1698812848370,
            "tmdate": 1699636040142,
            "mdate": 1699636040142,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xXKJv1g8kP",
                "forum": "FKg1N2fAFG",
                "replyto": "WdgdjXAcwm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1140/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1140/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer Mz7n, \n\nThank you for your constructive comments and suggestions, especially mentioning our work is unique in the context of dataset distillation and is significant in practice.\nFor your questions, we offer our point-to-point responses as follows:\n\n1.  \\>\\>\\> I'm not sure I followed the reasoning behind the scaling of the DropPath output maps. It would be nice to have a derivation in the Appendix for how this scaling matches the expectations from training to test.\n   \n     **Reply:** Eq. (1) on page 4 shows that $\\mathtt{DropPath}(\\mathbf{x}) = \\frac{m}{p}\\cdot \\mathbf{x},\\quad m=\\mathtt{Bernoulli}(p)$, where $p \\in [0, 1]$ denotes the keep rate, $m = \\mathtt{Bernoulli}(p) \\in \\{0,1\\}$ outputs 1 with probability $p$ and 0 with probability $1-p$. We consider the module output $\\mathbf{y} = \\mathtt{DropPath}(\\mathbf{x})$ in the training phase, then the expectation of $\\mathbf{y}$ given $\\\\mathbf{x}$ is $\\mathbb{E}(\\mathbf{y}) = p \\cdot \\frac{1}{p} \\cdot \\mathbf{x} + (1 - p) \\cdot \\frac{0}{p} \\cdot \\mathbf{x} = \\mathbf{x}$. In the test phase, DropPath is disabled, so the module output is simply $\\mathbf{x}$ and consistent with the expectation in the training phase. If there is no scaling factor $1 / p$ in Eq. (1) and $p < 1$, the expectation of the module outputs in the training and test phases will be different, which leads to performance degradation.\n\n2.  \\>\\>\\> There is a statement that architecture overfitting occurs due to depth and not width. Has this been recognized in existing work?\n   \n    **Reply:** First, to clarify, we do not claim that the width has no impact on architecture overfitting. We use the architectures evaluated in existing works, including [1, 2], and observe that the architecture overfitting occurs and becomes more severe when the models get deeper (from 3-layer CNN to 50-layer ResNet). We do not discuss the impact of width in this context. \n\n3.  \\>\\>\\> In the Three-Phase Keep Rate section on page 4, it is said that the variance increases as p increases: isn't the variance maximal at p = 0.5?\n   \n    **Reply:** We are sorry about the confusion, and we agree with your insightful comment. For a Bernoulli distribution, the variance is indeed the largest when $p=0.5$. We have updated the corresponding statement (please check the text highlighted in blue in our revised manuscript). The key point of three-phase keep rate is to control the **effective depth** of the model by the value of $p$. In the early phase of training, the model is underfitting, stochastic architecture brings optimization challenges for training, so we turn off DropPath by setting the keep rate $p = 1$ in the first few epochs to ensure that the network learns meaningful representations. We then gradually decrease $p$ to decrease the effective depth of the model and thus to narrow down the performance gap between the deep sub-network and the shallow training network until the keep rate reaches the predefined minimum value after several epochs. In the final phase of training, we decrease the architecture stochasticity by increasing the value of $p$ to a higher value to ensure training convergence. \n\n4.  \\>\\>\\> There seem to be some hyper-parameters involved, such as the values shaping the shape of the three-phase cycle and the temperature in knowledge-distillation. How are these hyper-parameters tuned? The experiments suggest direct evaluation on test set performances.\n   \n    **Reply:** In Section 4.3, we conduct ablation study to evaluate the performance under different hyper-parameters (HPs) when using FRePo-generated distilled dataset, IPC = 10 and ResNet18 as the test network. **More ablation studies on HPs of three-phase keep rate have been added in the revised manuscript**. The results are reported below and in Figure 4 (b) and (c). We can conclude that the performance fluctuation by different HPs in an **appropriate but wide** range is negligible, compared with the significant performance improvement brought by our method. For example, when the final keep rate ranges from 0.6 to 1.0, KD weight ranges from 0.3 to 0.9 and KD temperature ranges from 1 to 10, the performance fluctuation is with 1 percentage point. In addition, although lower minimum keep rates and longer periods of decay induce better performance, they also mean a longer training phase. We set them to $0.5$ and $500$ to balance the efficiency and performance, respectively. Despite that, the performance fluctuation caused by these two HPs is still insignificant. In addition, our results in Section 4.1 demonstrate that the selected HPs by ablation studies also perform quite well under other different settings, including different distilled datasets, IPs and test networks. In summary, many experimental observations indicate that our training method is robust to different HPs."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1140/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458841643,
                "cdate": 1700458841643,
                "tmdate": 1700458947179,
                "mdate": 1700458947179,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gyEpHRcJPn",
                "forum": "FKg1N2fAFG",
                "replyto": "xXKJv1g8kP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1140/Reviewer_Mz7n"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1140/Reviewer_Mz7n"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I think what was confusing me about the DropPath is that at training time, the two choices are x or a convblocks(x) while at test time it\u2019s the convblocks(x) part, if I understood right. It\u2019s unclear to me if the scaling by p can actually match the expectations without also taking the expected value of the convblocks output into account (which seems quite non-trivial). As a heuristic, it is fine.\n\nRegarding the hyper-parameters, while the initial settings seem intuitively set, and display some robustness, it is better practice to tune hyper-parameters on a held-out set. Otherwise, one can suspect that the initial settings are always likely to require direct access to test set to find stable regions.\n\nOverall, I think the submission has some potential, and retain my ratings. I also agree with some of the criticisms raised by the other reviews."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1140/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632514819,
                "cdate": 1700632514819,
                "tmdate": 1700632514819,
                "mdate": 1700632514819,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]