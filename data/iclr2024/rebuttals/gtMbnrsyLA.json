[
    {
        "title": "SMAAT: Scalable Manifold-Aware Adversarial Training for Large Language Models"
    },
    {
        "review": {
            "id": "c8Cha4xIxT",
            "forum": "gtMbnrsyLA",
            "replyto": "gtMbnrsyLA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3462/Reviewer_nXMe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3462/Reviewer_nXMe"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces SMAAT, a scalable manifold-aware adversarial training method for large language models. This paper proposes to generate adversarial examples efficiently via the last layer of the model, based on the hypothesis and observation of monotonically decreasing intrinsic dimensionality of the embedding space. The empirical evaluations on AGNEWS, IMDB, and YELP demonstrate improvements in robustness and scalability over previous state-of-the-art methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ originality, this paper proposes to adversarially training the last layer of language model to gain adversarial robustness, with seeking scalability.\n\n+ clarity, this paper is clear to follow and easy to read."
                },
                "weaknesses": {
                    "value": "- lack of evidence for scalability: does the hypothesis of intrinsic dimensionality still hold for larger language models? \n\n- ablation study of adversarial training on different layers: since this paper proposes to only fine-tune the last layer, what will happen if we include more layers for adversarial training? do we gain better robustness since more model parameters are included for adversarial training?\n\n- lack of adaptive attack: I strongly recommend the authors to design the adaptive attack [1] to approximate the lower bound of empirical adversarial robustness under their defense. This paper only evaluates model robustness under a simple 5-step PGD attack, which is not enough.\n\n[1] Florian Tramer, Nicholas Carlini, Wieland Brendel, Aleksander Madry, On Adaptive Attacks to Adversarial Example Defenses"
                },
                "questions": {
                    "value": "no question"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3462/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698924860170,
            "cdate": 1698924860170,
            "tmdate": 1699636298993,
            "mdate": 1699636298993,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9ZeeDN31sF",
                "forum": "gtMbnrsyLA",
                "replyto": "c8Cha4xIxT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3462/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3462/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback and bringing to our attention the adaptive attack work. \n\n> lack of evidence for scalability: does the hypothesis of intrinsic dimensionality still hold for larger language models?\n\n**Response:** We conducted additional experiments on the fine-tuned version of the quantized LLAMA-2 7B model, employing PEFT. Our results indicate that the intrinsic dimensionality of the model displays a non-monotonic pattern. Consistent with our hypothesis, integrating SMAAT into this model results in choosing the first layer to perform adversarial training, which does not provide additional scalability benefits. We nevertheless performed AT at the last layer and, as expected, this did not yield improved robustness.\n\n> ablation study of adversarial training on different layers: since this paper proposes to only fine-tune the last layer, what will happen if we include more layers for adversarial training? do we gain better robustness since more model parameters are included for adversarial training?\n\n**Response:** We evaluated the robustness and generalization impact of adversarial training in the intermediate layers. Our findings are presented in the newly added Figure 4 of the revised paper. In alignment with our main finding, when the adversarially trained layer is positioned closer to the input layer robustness of the model decreases as higher intrinsic dimensionality makes it more difficult to generate off-manifold examples. \n\n> lack of adaptive attack: I strongly recommend the authors to design the adaptive attack [1] to approximate the lower bound of empirical adversarial robustness under their defense. This paper only evaluates model robustness under a simple 5-step PGD attack, which is not enough.\n\n**Response:** To assess the robustness of our method under more targeted attacks, we considered the following three strategies:\n\t1. enhancing the strength of PGD attacks by increasing the steps to 50 and targeting the most vulnerable class, thereby generating attack samples more potent than those utilized during training;\n\t2. implementing targeted PGD attacks focused on the internal representations of an intermediate layer (\u201cfeature-level attack\u201d introduced in [[1]](#1)), rather than targeting the output label, to more effectively exploit our approach which primarily bolsters robustness at the final layer.\nThe robustness results presented in Table 3 of the revised manuscript are the averaged results of these two attacks instead of the 5-step PGD attack applied earlier.  \n\n## References\n[1]\nSara Sabour, Yanshuai Cao, Fartash Faghri, and David J Fleet (2015). \nAdversarial manipulation of deep representations\narXiv preprint arXiv:1511.05122, 2015"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3462/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593488686,
                "cdate": 1700593488686,
                "tmdate": 1700593488686,
                "mdate": 1700593488686,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ptdeYI7Sqz",
            "forum": "gtMbnrsyLA",
            "replyto": "gtMbnrsyLA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3462/Reviewer_6Kya"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3462/Reviewer_6Kya"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a manifold-aware approach to enhance the scalability and efficiency of adversarial training. Specifically, the model generates adversarial examples from higher layers in the deep neural network thereby shortening the gradient-based propagation and accelerating the generation of adversarial examples. Extensive experiments verified the effectiveness of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Observations and theories are very interesting. Generating adversarial examples from the higher layers of the neural network reduces the distance of gradient propagation, and Table 2 also shows the efficiency of the method.\n\n2. The method compares with multiple strong baselines for LLM defense and shows outperformance. The experimental results look good.\n\n3. The paper is clearly presented and easy to follow, with many intuitions discussed in detail and the related work adequately discussed. Although the proposed method has some limitations, it is inspiring for future adversarial training of LLMs."
                },
                "weaknesses": {
                    "value": "1. The three observations mentioned in the abstract appear not to be discussed in detail in the paper. How do these observations motivate the methodology?\n\n2. How effective are SMAAT generated adversarial examples compared to standard adversarial examples in attacking LLMs?\n\n3. Are the baseline and adversarial training models being compared in the paper using the same augmentation or training strengths? The strength and budget of the baselines appear not to be presented in the paper.\n\n4. It is unclear about the generalizability of the method. Will the proposed method be effective against other types of attacks such as typos?"
                },
                "questions": {
                    "value": "See the above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3462/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699016316514,
            "cdate": 1699016316514,
            "tmdate": 1699636298927,
            "mdate": 1699636298927,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FZJqnpxUWp",
                "forum": "gtMbnrsyLA",
                "replyto": "ptdeYI7Sqz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3462/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3462/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your valuable feedback and constructive comments.\n\n> The three observations mentioned in the abstract appear not to be discussed in detail in the paper. How do these observations motivate the methodology?\n\n**Response:** In our revision, we have further clarified how the three fundamental observations motivated our approach. For this, we expanded Section 3.0 to also include our intuitions that drove our problem formulation. \n\n> How effective are SMAAT generated adversarial examples compared to standard adversarial examples in attacking LLMs?\n\n**Response:** AEs generated in the last layer are expected to be more potent compared to AEs generated at the initial layer as attack strength can be more freely adjusted. To test this, we deployed the AEs generated by SMAAT at the last layer by feeding them to the last layer of the BERT model. Our results obtained on three datasets show that the robustness of the standard BERT model reduced to 0 in all cases. This suggests that AEs generated with SMAAT are more effective than those created in the initial layer.  \n\n> Are the baseline and adversarial training models being compared in the paper using the same augmentation or training strengths? The strength and budget of the baselines appear not to be presented in the paper.\n\n**Response:** The strength of AEs in each model varies among methods. PGD-based AT methods that rely on initial layer training are subject to low epsilon values. Whereas our approach, which generates AEs in the last layer, is not subject to such constraint. In contrast, for methods like TMD, SAFER, and RSMI the strength of AE cannot be defined in the same manner. Therefore, in our experiments, we adhered to the hyperparameters recommended in their respective original publications. All models were trained for an equivalent number of epochs. \n\nWe believe overall training complexity of these methods can best be assessed in terms of their training run-times. In this regard, SMAAT brings only a marginal run-time overhead to the clean models, which took only a couple of second for BERT and RoBERTa. \n\n> It is unclear about the generalizability of the method. Will the proposed method be effective against other types of attacks such as typos?\n\n**Response:** To more comprehensively evaluate the effectiveness of our method against a wider array of attacks, we carried out tests using the datasets included in the AdvGLUE benchmark. This benchmark encompasses AEs generated from 17 distinct attack types, typo based TextBugger included. The results of these tests are shown in Table 3 of the revised manuscript. The performance of our method mirrors the trends observed in our preliminary assessments. SMAAT exhibits average robustness improvements of 5.6% and 2.6% on the two new benchmarks for BERT, and 12.1% and 2.5% for RoBERTa, in comparison to standard and FreeLB++ models, respectively, while maintaining comparable generalization."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3462/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593474287,
                "cdate": 1700593474287,
                "tmdate": 1700593474287,
                "mdate": 1700593474287,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SBSf5f71wR",
            "forum": "gtMbnrsyLA",
            "replyto": "gtMbnrsyLA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3462/Reviewer_crCo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3462/Reviewer_crCo"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an efficient adversarial training (AT) method, particularly for language models, called SMAAT. SMAAT speeds up AT by only using the last several layers to generate adversarial samples. In this way, it does not need to back-propagation through all the layers while generating adversarial training data, thus reducing the training time. Empirical results seem to validate the effectiveness of SMAAT in efficiently learning a robust and generalizable language model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The empirical results on three datasets seem to justify the effectiveness of the SMAAT.\n\n2. The authors try to provide some theoretical derivation to motivate the proposed method."
                },
                "weaknesses": {
                    "value": "1. I am confused about the definition of \u2018off-manifold\u2019 and \u2018on-manifold\u2019. It would be better for the authors to provide more clear definitions and high-level explanations to help understand.\n\n2. I am confused about the choice of $l^* = n$. In the objection function in Eq. (10), it seems that any $l^*$ satisfies when $ID(i-1)<ID(i), \\forall I < l^*$. Based on my observation of Figure 3, $l^*$ can be chosen any number smaller than 13 since ID is monotonically decreasing. Therefore, based on Eq. (10), I cannot understand why the choice of $l^* = n$ is optimal.\n\n3. Empirical results are limited. The evaluation should be conducted on various datasets in the GLUE benchmark.\n\n4. The title seems to overclaim the contribution of this paper. All the results in this paper are shown on two language models, i.e., BERT and RoBERTa. However, I did not see the results of \u2018large language models (LLMs)\u2019 such as Llama2-70b. I am wondering whether the proposed method can be scalable to LLMs. If so, please show the empirical justification.  \n\nMinor comments: keep a consistency of the term \u2018ROBERTA\u2019 in Section 3.3 and \u2018RoBerta\u2019 in Introduction."
                },
                "questions": {
                    "value": "Please refer to \u201cWeaknesses\u201d."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3462/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3462/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3462/Reviewer_crCo"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3462/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699026010612,
            "cdate": 1699026010612,
            "tmdate": 1700619196560,
            "mdate": 1700619196560,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GUSuXMTVBw",
                "forum": "gtMbnrsyLA",
                "replyto": "SBSf5f71wR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3462/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3462/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback on our work. \n\n> I am confused about the definition of \u2018off-manifold\u2019 and \u2018on-manifold\u2019. It would be better for the authors to provide more clear definitions and high-level explanations to help understand.\n\n**Response:** The manifold hypothesis stands as one of the most compelling explanations for the susceptibility of deep neural networks to adversarial samples. This hypothesis fundamentally posits that data resides on a low-dimensional manifold within a high-dimensional representation space, and that a network, during training, learns to approximate this manifold. Consequently, an off-manifold sample, deviating from this foundational manifold, leads to undefined behavior in the network. Accordingly, an off-manifold sample is one that diverges from the underlying manifold and the network\u2019s behavior is undefined. For better clarity, we have incorporated this explanation into Section II (last paragraph of page 3) of the revised paper.\n\n> I am confused about the choice of $l^*=n$. In the objection function in Eq. (10), it seems that any $l^*$ satisfies when $ID(i-1) , ID(i), \\forall i < l^*$. Based on my observation of Figure 3, $l^*$ can be chosen any number smaller than 13 since ID is monotonically decreasing. Therefore, based on Eq. (10), I cannot understand why the choice of $l^*=n$ is optimal.\n\n**Response:** In our definition, the optimal layer is the one that minimizes the length of the back-propagation path. Therefore, it is preferable to perform adversarial training at output layers as opposed to input layers, which will enhance scalability. The main finding of our research is the demonstration that when the intrinsic dimensionality of the feature manifold decreases monotonically, adversarial training can be effectively performed at the layer with the lowest intrinsic dimension. As evidenced by the data in Figure 4, the final layer\u2014the one with the highest index\u2014exhibits the lowest intrinsic dimension. Therefore, implementing adversarial training at this layer not only optimizes generalization and robustness but also accomplishes these goals with minimal computational cost.\n\n> Empirical results are limited. The evaluation should be conducted on various datasets in the GLUE benchmark.\n\n**Response:** We further evaluated our method on four datasets in the GLUE benchmark (including SST2, QQP , QNLI, RTE). Rather than performing adversarial attacks separately, as in Table 1, we preferred to use the AdvGLUE benchmark which includes the adversarial counterparts of the same for four datasets. The outcomes of these evaluations are presented in Table 3 of the revised paper. Notably, the patterns identified in other benchmarks are consistently observed here as well. Overall, SMAAT exhibits average robustness improvements of 5.6% and 2.6% on the two new benchmarks for BERT, and 12.1% and 2.5% for RoBERTa, in comparison to standard and FreeLB++ models, respectively, while maintaining comparable generalization.\n\n> The title seems to overclaim the contribution of this paper. All the results in this paper are shown on two language models, i.e., BERT and RoBERTa. However, I did not see the results of \u2018large language models (LLMs)\u2019 such as Llama2-70b. I am wondering whether the proposed method can be scalable to LLMs. If so, please show the empirical justification.\n\n**Response:** We extended our experiments to include a fine-tuned version of the quantized LLAMA-2 7B model, utilizing PEFT. In this process, we discovered that the intrinsic dimensionality of the representations generated by this model is non-decreasing, mirroring the trend observed in vision models. Additional information about these experiments is presented in Appendix Section H."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3462/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593464374,
                "cdate": 1700593464374,
                "tmdate": 1700593464374,
                "mdate": 1700593464374,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PW6tulLwDy",
                "forum": "gtMbnrsyLA",
                "replyto": "GUSuXMTVBw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3462/Reviewer_crCo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3462/Reviewer_crCo"
                ],
                "content": {
                    "comment": {
                        "value": "Hi Authors,\n\nI appreciate the additional experimental results on GLUE, AdvGLUE, and Llama2-7B results. My concerns have been partially resolved. I am willing to increase my score. However, I still have concerns as follows:\n\n1. It is still unknown whether the method is applicable to extremely large-scale LLM (i.e., llama2-70B). Since SMAAT still needs expensive back-propagations, it still requires heavy computational overhead.\n\n2. SMAAT chooses only the last layer for generating adversarial perturbations to achieve computational efficiency. I suppose choosing all the layers to calculate adversarial perturbation can provide a more accurate approximation of the perturbations, thus leading to improved performance. However, this is not the same case as the empirical results. As FreeLB which uses all the layers has a worse performance than SMAAT, I am wondering the explanation for this phenomenon. Besides, is there any ablation study on the layer number chosen for generating adversarial perturbations?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3462/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619174539,
                "cdate": 1700619174539,
                "tmdate": 1700619174539,
                "mdate": 1700619174539,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mJnwo6hO7b",
            "forum": "gtMbnrsyLA",
            "replyto": "gtMbnrsyLA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3462/Reviewer_Tdyi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3462/Reviewer_Tdyi"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents SMAAT, an efficient approach for adversarial training of language models, which relies on fine-tuning a pretrained model using adversarial examples generated from the last layer of the model. To motivate the approach, the paper discusses the link between inputs being out of manifold for consecutive layers, and the difference in the intrinsic dimensionality of these layers. Then, since language models exhibit a monotonically decreasing *intrinsic dimensionality (ID)* of their representations throughout layers, the optimal layer to generate adversarial examples from is the last layer. Using only the last layer in the forward-backward passes of PGD to generate adversarial examples is much more efficient than using the full model. Finally, the paper shows that on top of the efficiency of the proposed approach, the defended models are more robust than recent defenses against common adversarial attacks for text on three text classification benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- **Robustness and Natural Accuracy:** The paper demonstrates substantial improvements in model robustness when compared to other defense methods, without compromising natural accuracy. It achieves consistent results across three benchmark datasets using two different models.  \n- **Efficient Adversarial Training:** The paper offers a very efficient approach to adversarial training. By concentrating on the last layer of the model for generating adversarial examples, it significantly reduces the computational overhead associated with this process. This efficiency is a significant contribution, making adversarial training more accessible for practical applications.\n- **Clear Presentation and Context Setting:** The paper is well-organized and presents related work in the field clearly. The description of the approach is also easy to understand."
                },
                "weaknesses": {
                    "value": "In general, the formulation and description of the motivation behind the approach is not very clear and lacks rigor. This makes the foundations of the approach unsound. There are several quantities vaguely defined, such as the basis $U_l$ obtained from the SVD, the formulation of the theorem 3.1 and its proof, or the Intrinsic Dimension of a layer that is discussed and used before being defined. This leads to critical misunderstandings that require clarification.\n\n### Typos\nHere are some typos I noticed:\n- conjoncture -> conjecture (in figure 2 and in last paragraph of page 3)\n- the $\\exists$ should be $\\forall$ in equation 4 and equation 9\n- orhonormal -> orthonormal (in 3.1, between equation 4 and equation 5)\n- Emperical -> Empirical (in title of 3.3)"
                },
                "questions": {
                    "value": "- As mentioned in the paper, the basis $U_l$ obtained through SVD is an orthonormal basis. Thus, $U_l U_l^\\top = I$, which makes the projection error as defined in the paper always equal to zero. \n- Can the authors clarify how to derive equation 7 from equation 6 ? It is critical as it links the search for the optimal layer $l^*$ to theorem 3.1.\n- In theorem 3.1, the proof in Appendix is given for the opposite side: If $||(I - U_{(i-1)} U_{(i-1)}^\\top)\\delta_{(i-1)}|| < ||(I-U_i U_i^\\top)\\delta_i||$ then $rank(i-1) < rank(i)$. There is also the equality case that is included in the theorem but not in the proof. Maybe the inequalities should be reversed in the formulation of the theorem, which would make a proof by contraposition ? This would also make sense with the remaining of the paper, since we observe decreasing ID and not increasing ID.\n- Similarly, for the objectives of SMAAT (eq. 9 and 10), the inequalities on the ID do not make sense since we observe decreasing ID throughout the layers. I think the inequality should be reversed as well, i.e. $ID(i) < ID(i-1)$.\n-  What is k and $U_l^k$ in the computation of ID (equation 11) ? I assume it is the k first rows and columns of $U_l$, which would make ID close to the rank of the representation. This should be clarified, as well as the link between ID and the rank, since the theorem is stated using the *rank operator*.\n-  What is $\\lambda_{max}$ ? Is it the maximum eigenvalue ? If so, I'm unsure about the assumption used in the proof of theorem 3.1 about the ratio of $\\lambda_{max}$. There is an additional layer in the network from which the jacobian is computed in the numerator, thus the Lipschitz constant of the numerator is greater than the one of the denominator, making the ratio greater than 1. Does the remaining of the proof holds with this ? \n-  The proposed approach is interestingly the opposite of YOPO [1], can the authors develop on the link and differences regarding this method ?\n\n[1] Zhang et al., You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle, NeurIPS 2019."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3462/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3462/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3462/Reviewer_Tdyi"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3462/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699031957837,
            "cdate": 1699031957837,
            "tmdate": 1700668434834,
            "mdate": 1700668434834,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Okhe19j0NW",
                "forum": "gtMbnrsyLA",
                "replyto": "mJnwo6hO7b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3462/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3462/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for carefully reading our paper and for the suggestions. We corrected the mathematical representation and revised the proof of Theorem 3.1 accordingly. \n\n> As mentioned in the paper, the basis $U_l$ obtained through SVD is an orthonormal basis. Thus, $U_lU_L^T = I$, which makes the projection error as defined in the paper always equal to zero.\n\n**Response:** We compute the projection error using the eigenvectors corresponding to the top-k eigenvalues, where k represents the ID of the respective layer. Therefore, samples aligned with the data manifold exhibit lower projection errors, while others deviate with higher errors in the k-dimensional data manifold. Our notation is modified to include the index $k$, representing the top-k eigenvectors, which translates into the effective ID of the layer.\n\n>Can the authors clarify how to derive equation 7 from equation 6 ? It is critical as it links the search for the optimal layer $l^*$\n to theorem 3.1.\n\n**Response:**  To enhance the clarity of the derivation, we introduced an intermediate (Eq. 7). We also note that Eq. 8 (old Eq. 7) sets a sufficient condition to satisfy Eqs. 6 and 7. \n\n> In theorem 3.1, the proof in Appendix is given for the opposite side: If $\\|(I-U_{(i-1)}U_{(i-1)}^T)\\delta_{(i-1)}\\| < \\|(I-U_iU_i^T)\\delta_(i)\\|$ then rank(i-1) < rank(i). There is also the equality case that is included in the theorem but not in the proof. Maybe the inequalities should be reversed in the formulation of the theorem, which would make a proof by contraposition ? This would also make sense with the remaining of the paper, since we observe decreasing ID and not increasing ID.\n\n**Response:** We appreciate your insightful observation. We have carefully revised Theorem 3.1 and its proof. The notation is now coherent and the direction of the inequalities are correct. \n\n>Similarly, for the objectives of SMAAT (eq. 9 and 10), the inequalities on the ID do not make sense since we observe decreasing ID throughout the layers. I think the inequality should be reversed as well, i.e. $ID(i) < ID(i-1)$\n\n**Response:** In accordance with our initial description in the text, the direction of inequalities are corrected. \n\n> What is k and $U_l^k$ in the computation of ID (equation 11) ? I assume it is the k first rows and columns of $U_l$, which would make ID close to the rank of the representation. This should be clarified, as well as the link between ID and the rank, since the theorem is stated using the rank operator.\n\n**Response:** This is correct: the variable $k$ represents the number of top eigenvalues needed to obtain a projection similar to the original samples and also used as the ID of features. We revised the paper for better clarity.\n\n> What is $\\lambda_{max}$? Is it the maximum eigenvalue? If so, I'm unsure about the assumption used in the proof of theorem 3.1 about the ratio of $\\lambda_{max}$. There is an additional layer in the network from which the jacobian is computed in the numerator, thus the Lipschitz constant of the numerator is greater than the one of the denominator, making the ratio greater than 1. Does the remaining of the proof holds with this?\n\n**Response:** We have updated the mathematical formulation of the theorem to be consistent with the revised assumptions. In the new version of the theorem, we employed a proof by contraposition to establish its validity. We believe these modifications greatly contribute to the clarity and accuracy of our work.\n\n> The proposed approach is interestingly the opposite of YOPO [1], can the authors develop on the link and differences regarding this method ?\n\n**Response:** We believe this seeming contradiction is due to differences between vision and text domains. In Section G (Figure 6) of the revised paper, we present the measured intrinsic dimensionality of the ViT model which is noticeably different from that of text models. In fact, our approach suggests that the optimal layer $l^*$ for the ViT model would be the initial layer due to the increasing ID.  We have incorporated this clarification into the paper to provide a more nuanced understanding of the relationship between layer-wise behavior and robustness. \n\n> Typos\n\n**Response:**  We corrected typos. However, in Eqs. 4 and 10 (old Eq. 9), we define an AE to be off-manifold in any layer, and not in all of the layers. Therefore,  we kept $\\exists$ notation instead of $\\forall$."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3462/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593454031,
                "cdate": 1700593454031,
                "tmdate": 1700593454031,
                "mdate": 1700593454031,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Pu60kp85YT",
                "forum": "gtMbnrsyLA",
                "replyto": "Okhe19j0NW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3462/Reviewer_Tdyi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3462/Reviewer_Tdyi"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed answer, the revision of the paper and all additional work. \n- The revision of the proof and the derived results seems correct to me now. The results look more solid now and all my concerns have been addressed.\n- I appreciate the discussion around ViT and LLAMA-2 in the Appendix, it nicely connects this work to YOPO for vision tasks, and gives a better understanding of the inner behaviors. However, I think including the obtained results as a table, even though they are negative, would be better than only stating that the results are not improved in the associated text. Otherwise, this information might be missed by the reader.\n- I appreciate the additional results on GLUE, AdvGLUE, using a stronger attack, and the comparative study of AT at different layers.\n\nI like the idea of the paper and find the developments interesting. I'm increasing my score to an 8 (accept)"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3462/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668389126,
                "cdate": 1700668389126,
                "tmdate": 1700668389126,
                "mdate": 1700668389126,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fnWqnol7GI",
            "forum": "gtMbnrsyLA",
            "replyto": "gtMbnrsyLA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3462/Reviewer_JQp6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3462/Reviewer_JQp6"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces SMAAT, an efficient Adversarial Training method that uses only adversarial examples generated in the last layer of a model in encoder-based large language models. The proposed method has fast training and inference speed since we do not have to do a full forward-backward passes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method is intuitive and backed by good motivation, and both theoretical and experimental findings.\n- The proposed method outperforms most of the previous methods on various attacks and datasets."
                },
                "weaknesses": {
                    "value": "- Argument about the manifolds between the layers is not very clear to the reader.\n- Results on Table 1 are difficult to interpret. I would suggest just to boldify the best result for every attack in each dataset without having any underlined results.\n- I believe that the method is probably not very novel or of high contribution."
                },
                "questions": {
                    "value": "- Have you conducted any experiments where you use adversarial examples generated from other layers instead of the only the last one. Sometimes theorems and experiments can provide very different outcomes.\n- I read the limitations at the end of the paper. Although this is not in the scope of this work I would suggest to still try this method with image data."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3462/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3462/Reviewer_JQp6",
                        "ICLR.cc/2024/Conference/Submission3462/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3462/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699310108304,
            "cdate": 1699310108304,
            "tmdate": 1700610281935,
            "mdate": 1700610281935,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qWrE80wks3",
                "forum": "gtMbnrsyLA",
                "replyto": "fnWqnol7GI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3462/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3462/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time and suggestions. \n\n> Argument about the manifolds between the layers is not very clear to the reader.\n\n**Response:**  We revised our statement in Section I (next to last paragraph on page 2) to better clarify the manifold relation between layers.\n\n> Results on Table 1 are difficult to interpret. I would suggest just to boldify the best result for every attack in each dataset without having any underlined results.\n\n**Response:**  The best results in Table 1 are in bold. \n\n> I believe that the method is probably not very novel or of high contribution.\n\n**Response:**  We would like to emphasize that the novelty of our method lies in (i)  the discovery that the intrinsic dimensionality of internal representations in fine-tuned decoder models monotonically decreases and that, in this case, (ii) applying AT in the higher layers covers all the AEs from the previous layers and offers better robustness than applying AT in the initial layers. This justifies (iii) applying PGD-based adversarial training to only to the last layer. Our method not only improves robustness and generalization capabilities but also achieves these improvements at a significantly lower computational cost compared to state-of-the-art methods. In our revision, we have extended our experimental analysis to include more tasks/datasets, thereby reinforcing the efficacy of our method.\n\n> Have you conducted any experiments where you use adversarial examples generated from other layers instead of the only the last one. Sometimes theorems and experiments can provide very different outcomes.\n\n**Response:**  As part of our revision, we conducted additional experiments focusing on the application of adversarial training to intermediate layers. The outcomes of these experiments are presented in Figure 4 of the revised manuscript. In agreement with our hypothesis, the results demonstrate a reduction in robustness when the adversarially trained layer is closer to the input layer, and the best robustness is achieved in the last layer. \n\n> I read the limitations at the end of the paper. Although this is not in the scope of this work I would suggest to still try this method with image data.\n\n**Response:**  We extended the application of our method to (i) vision and (ii) LLM domains. To achieve this, we evaluated the intrinsic dimensionality of the ViT  model and the quantized LLAMA-2 7B model. Comprehensive details about these models and the outcomes of these experiments are presented in Appendices G and H. Our findings reveal that both models have non-monotonic variations in intrinsic dimensionality. We further applied SMAAT to both models to test our hypothesis. We did not observe enhanced robustness, as expected by our formulation."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3462/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593437778,
                "cdate": 1700593437778,
                "tmdate": 1700593437778,
                "mdate": 1700593437778,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OVYaZEG7zj",
                "forum": "gtMbnrsyLA",
                "replyto": "qWrE80wks3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3462/Reviewer_JQp6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3462/Reviewer_JQp6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the fast and detailed reply. I do like the idea and the results look competitive even though there is an incremental improvement in the adversarial robustness, and in some cases the proposed method does not outperform the SOTA methods. I have also skimmed through the new edits in the paper and they look good. I would prefer for the ViT and LLM experiments to be moved in the main text instead of the end of the Supplement, and be more extensive. For instance you could try to run similar experiments to the ones that you have for BERT and RoBERTa in the main text. I will raise my score to 6, but I will keep an eye for the rest of the reviewers' opinions."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3462/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610272223,
                "cdate": 1700610272223,
                "tmdate": 1700610272223,
                "mdate": 1700610272223,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]