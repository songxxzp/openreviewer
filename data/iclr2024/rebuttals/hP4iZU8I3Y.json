[
    {
        "title": "Understanding Inter-Session Intentions via Complex Logical Reasoning"
    },
    {
        "review": {
            "id": "i5gX6NjCpM",
            "forum": "hP4iZU8I3Y",
            "replyto": "hP4iZU8I3Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2938/Reviewer_kouL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2938/Reviewer_kouL"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the task of Logical Session Query Answering (LSQA) and presents a solution called the Logical Session Graph Transformer (LSGT) model. The objective of LSQA is to learn logical queries for observed user interaction sessions. This task could help understand the logical intention of user interactions. The LSGT model achieves this by uniformly representing sessions, items, relations, and logical operators as tokens and leveraging a transformer-based sequential model for encoding.\n\nThe paper provides a theoretical analysis that primarily focuses on demonstrating the expressiveness of the proposed LSGT model. Additionally, comprehensive experiments are conducted to validate the superiority of the proposed model compared to existing baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper proposes the task of Logical Session Query Answering (LSQA), providing an novel paradigm for enhancing applications like session-based recommendation and query recommendation by understanding the logical structures of users' latent intents.\n- The paper provides a theoretical analysis on the expressiveness of the proposed Logical Session Graph Transformer (LSGT) model.\n- The paper innovatively build a unified representation model for items, sessions and logical operators using hypergraphs and sequential models."
                },
                "weaknesses": {
                    "value": "- Though the proposed task is novel, the proposed technical solution LSGT relies on existing hypergraph structures and transformer architeactures. Such designs have limited differences compared to existing sequential models and graph models. This lower the technical contribution of this paper.\n- The evaluation part could be enhanced with more diverse experiments to conduct a more comprehensive empirical study, such as ablation study, hyperparameter study, case study on the generated queries, and an investigations on the benefits of LSGT brought to downstream tasks like session-based recommendation.\n\nMinor mistake: In the summary for contributions: \"We propose to propose ...\""
                },
                "questions": {
                    "value": "My concerns would be alleviated if the authors could provide further clarification on the technical novelty aspect and the comprehensiveness of the experiments. Please refer to the weaknesses part for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2938/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2938/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2938/Reviewer_kouL"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2938/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698552312464,
            "cdate": 1698552312464,
            "tmdate": 1699636237498,
            "mdate": 1699636237498,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TkPDNCwVUu",
                "forum": "hP4iZU8I3Y",
                "replyto": "i5gX6NjCpM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2938/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2938/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Re W1\n\nAs part of our technical contribution, LSGT proposes a new strategy of using tokens to represent sessions and logical structures. This approach demonstrates its novelty and advantage by achieving an average improvement of 2.32 on complex structures including negations, such as 2ina, 2ins, 3in, inp, and pin, over three datasets compared to a model that also uses a Transformer and backbone (SQE).\n\nFurthermore, we conducted additional experiments on four previously unseen query types (3iA, 3ip, 3inA, and 3inp) and showed that our method achieves an average improvement of 2.44 in MRR over three datasets compared to a previous method that also uses a Transformer encoder (SQE+Transformer). The details of this experiment are provided in Re W2.\n\n\n\n\nRe W2\n\nFollowing your suggestions, we included two ablation studies on the hypergraph and logical structures. In the first study, we removed the tokens representing the logical structures. In the second study, we removed the order information in the hypergraph by removing the positional encoding features of item tokens in each session.\n\nHere are the results:\n\n| Dataset    | Encoder             | Average |   1p  |   2p  |  2ia  |  2is  |   3i  |   pi  |   ip  |   2u  |   up  | 2ina  | 2ins  | 3in   | inp   | pin   |\n|------------|---------------------|---------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|-------|-------|-------|-------|-------|\n| Amazon     | LSGT                |  31.99  | 17.73 |  9.10 | 56.73 | 84.26 | 69.39 | 19.39 | 19.47 | 15.40 |  7.86 | 20.98 | 22.00 | 60.70 | 35.95 |  8.84 |\n|            | w/o Logic Structure |  15.98  |  5.41 |  2.31 | 30.31 | 50.21 | 45.21 |  3.75 |  5.49 |  4.88 |  2.56 | 16.32 | 15.77 | 38.19 |  2.13 |  1.11 |\n|            | w/o Session Order   |   8.45  |  6.29 |  2.59 | 17.22 | 13.85 | 19.34 | 14.07 |  3.23 |  3.49 |  1.73 |  5.50 |  4.75 | 17.54 |  4.92 |  3.73 |\n| Diginetica | LSGT                |  40.59  | 32.00 | 15.27 | 83.34 | 90.61 | 86.05 | 15.62 | 33.80 | 26.34 | 14.45 | 24.15 | 28.69 | 83.04 | 19.21 | 15.62 |\n|            | w/o Logic Structure |  27.17  | 18.61 |  3.84 | 68.40 | 62.80 | 64.87 | 10.13 | 20.22 | 16.08 |  8.49 | 17.38 | 14.17 | 60.37 |  9.21 |  5.74 |\n|            | w/o Session Order   |  17.07  |  5.08 |  9.71 | 45.49 | 34.42 | 43.23 |  9.69 | 21.71 |  3.66 |  7.92 |  4.39 |  2.56 | 35.80 |  9.98 |  5.36 |\n| Dressipi   | LSGT                |  70.22  | 31.12 | 96.16 | 64.26 | 76.85 | 78.66 | 98.02 | 96.98 | 28.83 | 96.04 | 25.58 | 30.66 | 65.93 | 97.74 | 96.30 |\n|            | w/o Logic Structure |  25.13  | 14.87 |  2.45 | 42.03 | 59.63 | 67.62 |  9.27 | 17.71 | 18.05 |  7.64 | 19.62 | 24.67 | 59.01 |  1.95 |  7.29 |\n|            | w/o Session Order   |  39.78  |  9.21 | 42.80 | 21.57 | 19.57 | 23.28 | 88.31 | 61.47 |  6.31 | 68.27 |  7.41 |  6.87 | 15.96 | 96.53 | 89.35 |\n\nRemoving the logical structure information drastically dropped the model's performance, especially for queries involving negations and multi-hop reasoning, such as ip, pi, inp, and pin. Without the logical structure, the model could only use co-occurrence information like \"bag of sessions\" and \"bag of items\" to rank candidate answers. While this information may be useful for simple structured queries, it is not very useful for complex structured queries.\n\nSimilarly, removing the order information within each session also drastically dropped the overall performance. This demonstrates two things: First, the item orders in each session are critical in this task. Second, the LSGT model is effectively able to utilize the order information for this task.\n\nIn addition, we included a new evaluation on the compositional generalization of LSGT. The newly included query types are 3iA, 3ip, 3inA, and 3inp. We selected them because they are all complex types involving three anchors, cover both EPFO queries and queries involving negations, and have both 1-hop and 2-hop relational projections in the reasoning process. These query types were not trained during the training process but were evaluated in a zero-shot manner.\n\nHere are their performances against the baselines:"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2938/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674353209,
                "cdate": 1700674353209,
                "tmdate": 1700674353209,
                "mdate": 1700674353209,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HIVQIFtAHz",
            "forum": "hP4iZU8I3Y",
            "replyto": "hP4iZU8I3Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2938/Reviewer_Kyoc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2938/Reviewer_Kyoc"
            ],
            "content": {
                "summary": {
                    "value": "The paper formulates an item recommendation task based on the previous session history as a complex logical graph query (named as Logical Session Query Answering). In such a query, items and attributes are nodes, several items can be connected in sessions (hyperedges denoting the order of obtaining the items), relations form projection operators in a query, and other logical operators (intersection, union, negation) combine nodes and projections into a single complex query. Instead of operating on the complete hypergraph of items, sessions, and attributes, the authors decide to operate on the single query level and predict answer entities directly after linearizing the query via the Logical Session Graph Transformer (essentially, a TokenGT from [1]). The authors prove that their transformer is permutation invariant (with respect to intersection and union operators), and run experiments on 3 datasets showing marginal improvements over the baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**S1.** The item recommendation task is framed as a complex logical query. While the task per se is not new (LogiRec [2] originally introduced it with projection and intersection operators), this paper extends it to unions and negations and to hypergraphs.\n\n**S2.** Evaluation includes several baselines (that show, on the other hand, that the proposed approach only marginally outperforms existing models, but more on that in W2)"
                },
                "weaknesses": {
                    "value": "Starting from the claimed contributions:\n\n**W1. Task.** The formulated task of Logical Session Query Answering is essentially query answering over hypergraphs. Sessions are n-ary edges, and other relations form 2-ary edges, so the hypergraph has edges of different arity. The temporal aspect of items in session hyperedges (that items follow each other in one session) seems to be of little use as the best-performing models are not using this information anyway. I would recommend the authors to focus the contribution on extending complex query answering to hypergraphs as there is not that much work in that subfield (StarQE is for hyper-relational graphs, and NQE supports both hyper-relational and hypergraphs).\n\n**W2. Encoder + Experimental results.** The proposed logical session graph transformer (LSGT) is just one of the many query linearization strategies, eg, BiQE [3], kgTransformer [4], or SQE [5] that convert the query graph into a sequence with some positional information to be sent jointly into a Transformer. Architecture-wise, LSGT is TokenGT [1] but with a slightly different input format that sends tokens of logical operators. Experimentally, LSGT is very close to SQE [5] (the gap is often <1 MRR point) so it is hard to claim any novelty or effectiveness in this linearization strategy or in a slightly different transformer encoder. \n\n**W3. Theory.** The theoretical study in Section 4.5 is derived from TokenGT and seems to be hardly applicable to the case of logical query answering. TokenGT\u2019s theory of WL expressiveness assumes the graphs are non-relational whereas all logical query graphs studied in this work are relational, i.e., they have labeled edge types. There is a different line of work studying expressiveness of GNNs over relational graphs [6,7] and I would recommend starting from them in order to derive any expressiveness claims. Permutation invariance proofs are rather trivial because the Transformer architecture itself is permutation equivariant.\n\nOverall, I think the paper has more potential if:\n* The authors frame the task as the hypergraph query answering with the full support of first-order logical operators (intersection, union, negation) and demonstrate that several existing Transformer-based models show similar results on 3 benchmarks despite different linearization strategies; \n* Tone down the claims on the _logical session_ QA (it\u2019s a hypergraph), new graph transformer and its expressiveness (TokenGT is not new, theory for non-relational graphs does not apply to relational ones), and state-of-the-art (all Transformer-based models show a very similar performance). \n\nI understand that it would require substantial re-writing of several sections, so I am willing to increase the score if the authors decide to do it during the discussion period. \n\nMinor comments:\n* Too many sentences (especially in Section 3) start with noisy and artificial \u201chowever\u201d and \u201cmeanwhile\u201d. You don\u2019t have to contrast every sentence to each other every time.\n* $p$ and $q$ denote different things in 4.2 (item and session) and 4.3 (just two nodes) and it is confusing.   \n* 4.4 Learning LSGT -> Training LSGT\n\n**References**\n\n[1] Kim et al. Pure transformers are powerful graph learners. NeurIPS 2022.  \n[2] Tang et al. LogicRec: Recommendation with Users' Logical Requirements. SIGIR\u201923.  \n[3] Kotnis et al. Answering complex queries in knowledge graphs with bidirectional sequence encoders. AAAI 2021.  \n[4] Liu et al. Mask and reason: Pre-training knowledge graph transformers for complex logical queries. KDD\u201922.  \n[5] Bai et al. Sequential query encoding for complex query answering on knowledge graphs. TMLR 2023.  \n[6] Barcelo et al. Weisfeiler and Leman Go Relational. LOG 2022.   \n[7] Huang et al. A theory of link prediction via relational Weisfeiler-Leman. NeurIPS 2023."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2938/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698552408151,
            "cdate": 1698552408151,
            "tmdate": 1699636237425,
            "mdate": 1699636237425,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8lmrr7KK6A",
                "forum": "hP4iZU8I3Y",
                "replyto": "HIVQIFtAHz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2938/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2938/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for providing such constructive feedback. We have put in significant effort to revise the paper based on your comments and suggestions, and all the rewritten parts are highlighted in blue.\n\nRe W1: \n\nRegarding the task formulation, we have taken your advice and extended the problem definition of CQA to include sessions as ordered hypergraphs of varying arity, namely LS-CQA. We also want to emphasize that the order of items in the session is crucial for recommending relevant items, and top-performing models such as SQE, LSGT, and session encoders all take this into account.\n\nFurthermore, we want to highlight the novelty of our problem formulation, as we are the first to bridge the CQA problem with session-based recommendations. CQA methods and knowledge graph reasoning are essential for session understanding, and our approach is unique in that it addresses user anonymity, which is a common feature of session-based recommendation tasks. By incorporating logical graph reasoning methods, we believe our approach is not only novel but also useful for improving session-based recommendation systems.\n\n\nRe W2 Encoder + Experiments:\n\nWe have taken your advice and demonstrated that transformer-based methods have comparable performance despite using different linearization strategies. However, we want to highlight that the linearization strategy employed in LSGT outperforms others in queries with negations.\n\nFurthermore, we have conducted additional experiments to investigate the compositional generalization of different linearization strategies. Our results show that the linearization strategy used in LSGT exhibits better compositional generalization than other existing strategies. Below are the detailed performance results: \n\n\n| Dataset | Query Encoder | 3iA | 3ip | 3inA | 3inp | Average OOD |\n|---|---|:---:|:---:|:---:|:---:|:---:|\n| Amazon | FuzzQE + Attn-Mixer | 66.72 | 29.67 | 54.33 | 48.76 | 49.87 |\n|  | Q2P + Attn-Mixer | 33.51 | 11.42 | 51.47 | 41.46 | 34.47 |\n|  | NQE | 61.72 | 1.98 | 46.47 | 34,04 | 36.72 |\n|  | SQE + Transformers | 66.03 | 28.41 | 55.61 | 51.28 | 50.33 |\n|  | LSGT (Ours) | **68.44** | **34.22** | **58.50** | **51.49** | **53.16 (+2.83)** |\n| Diginetica | FuzzQE + Attn-Mixer | 88.30 | 32.88 | 82.75 | 34.50 | 59.61 |\n|  | Q2P + Attn-Mixer | 40.28 | **43.93** | 54.31 | **48.20** | 46.68 |\n|  | NQE | 86.25 | 20.79 | 64.74 | 20.93 | 48.18 |\n|  | SQE + Transformers | 88.05 | 31.33 | 81.77 | 35.83 | 59.25 |\n|  | LSGT (Ours) | **91.71** | 35.24 | **83.30** | 41.05 | **62.83 (+3.22)** |\n| Dressipi | FuzzQE + Attn-Mixer | 65.43 | 95.64 | 53.36 | 97.75 | 78.05 |\n|  | Q2P + Attn-Mixer | 60.64 | 96.78 | 52.22 | 97.28 | 76.73 |\n|  | NQE | 31.96 | 96.18 | 9.89 | 97.80 | 58.96 |\n|  | SQE + Transformers | 72.61 | 97.12 | 55.20 | 98.14 | 80.77 |\n|  | LSGT (Ours) | **74.34** | **97.30** | **58.30** | **98.23** | **82.04 (+1.28)** |\n\n\n\nRe W3 Theory:\n\nWe have read and analyzed the reference papers [1,2], and we developed a new expressive claim based on the relational expressiveness of LSGT. We proved that the LSGT is at least as powerful as 1-RWL, namely the expressiveness of R-GCN and CompGCN. We include the proof in the appendix and mark it in blue. \n\n\n[1] Barcelo et al. Weisfeiler and Leman Go Relational. LOG 2022.\n[2] Huang et al. A theory of link prediction via relational Weisfeiler-Leman. NeurIPS 2023.\n\nWe really appreciate your constructive feedback. If you have any other concerns please do not hesitate to tell us."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2938/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674315542,
                "cdate": 1700674315542,
                "tmdate": 1700674315542,
                "mdate": 1700674315542,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uR9MoVhzuw",
            "forum": "hP4iZU8I3Y",
            "replyto": "hP4iZU8I3Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2938/Reviewer_kt6F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2938/Reviewer_kt6F"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors focus on product and attribute recommendation by modeling complex user intention. They employ the logical session query answering (LSQA) to formulate the task. The proposed logical session graph transformer (LSGT) model runs on a hyper session graph, which uses a standard transformer structure to encode different entities. Experiments on three real-world datasets demonstrate the effectiveness of LSGT for complex session query answering."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The motivation that incorporates logical session query answering into product recommendation to model user intent is novel.\n2. The experimental results demonstrate the effectiveness of the proposed LSGT.\n3. The authors theoretically justify the expressiveness and operator-wise permutation invariance of LSGT."
                },
                "weaknesses": {
                    "value": "1. There are some obvious typos. Authors should scrutinize the writing of the paper.\n(1) In the 5th line of section 4.3, the formula after \u201cThe edge feature is denoted as\u201d lacks a proper superscript.\n(2) In Table 5, the first word \u201cPredicti\u201d in explanation of query type 2p should be \u201cPredict\u201d.\n(3) In Table 5, the word \u201cprodict\u201d in explanation of query type ip should be \u201cproduct\u201d.\n(4) In the 2nd line below Figure 5, the word \u201cdescibed\u201d should be \u201cdescribed\u201d.\n2. In Figure 5, the query structure of ip is the same as up and the query structure of 2iS is the same as 2uS. It would be better to distinguish them like [1].\n3. The paper lacks detailed description for figures especially Figure 3, which is hard to understand for readers.\n4. It would be better to evaluate the model\u2019s generalization ability of unseen query structures like [1,2,3].\n\n[1] Jiaxin Bai, Zihao Wang, Hongming Zhang, and Yangqiu Song. 2022. Query2Particles: Knowledge Graph Reasoning with Particle Embeddings. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 2703\u20132714, Seattle, United States. Association for Computational Linguistics.\n[2] Chen, X., Hu, Z., & Sun, Y. (2022). Fuzzy Logic Based Logical Query Answering on Knowledge Graphs. Proceedings of the AAAI Conference on Artificial Intelligence, 36(4), 3939-3948.\n[3] Jiaxin Bai, Tianshi Zheng, and Yangqiu Song. Sequential query encoding for complex query answering on knowledge graphs. Transactions on Machine Learning Research, 2023. ISSN 2835-8856"
                },
                "questions": {
                    "value": "1. Why do authors not evaluate the model\u2019s generalization ability of unseen query structures like existing works?\n2. Is there an explanation for the author's choice of 14 query structures? Can some other query structures like 2i, and pni be incorporated?\n3. Is it possible to make an ablation study for hypergraph and logical reasoning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2938/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759049009,
            "cdate": 1698759049009,
            "tmdate": 1699636237352,
            "mdate": 1699636237352,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UIbyeAqMxq",
                "forum": "hP4iZU8I3Y",
                "replyto": "uR9MoVhzuw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2938/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2938/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Review Discussions"
                    },
                    "comment": {
                        "value": "Thank you for your review, we would like to address your concerns individually. \n\nRe W1\n\nThank you so much, we have fixed the typos you mentioned and marked them in blue in the revised paper. We have also conducted a thorough check on the grammatical issues and typos and also fixed them at the same time.  \n\nRe W2\n\nWe have made some updates to our work. Specifically, we added the notation of \"u\" and \"n\" as in previous work to distinguish between \u201c2iS\u201d and \u201c2uS\u201d. Additionally, we included query structures for the four out-of-distribution query types in Figure 5, which addresses your concerns on compositional generalizability. You can find more details in Re W4.\n\nRe W3\n\nWe've made some updates to our paper. Specifically, we added a detailed description for Figure 3 to better explain our problem setting and relevant query encoding methods. We marked the newly added explanations in blue for your convenience. \n\nRe W4\n\nWe have followed your suggestions and added new experiment results on compositional generalization. The newly included query types are 3iA, 3ip, 3inA, and 3inp. We selected these query types because they are complex, involve three anchors, cover both EPFO queries and queries involving negations, and have both 1-hop and 2-hop relational projections in the reasoning process. These query types were not trained during the training process but were evaluated in a zero-shot manner.\n\n\n| Dataset | Query Encoder | 3iA | 3ip | 3inA | 3inp | Average OOD |\n|---|---|:---:|:---:|:---:|:---:|:---:|\n| Amazon | FuzzQE + Attn-Mixer | 66.72 | 29.67 | 54.33 | 48.76 | 49.87 |\n|  | Q2P + Attn-Mixer | 33.51 | 11.42 | 51.47 | 41.46 | 34.47 |\n|  | NQE | 61.72 | 1.98 | 46.47 | 34,04 | 36.72 |\n|  | SQE + Transformers | 66.03 | 28.41 | 55.61 | 51.28 | 50.33 |\n|  | LSGT (Ours) | **68.44** | **34.22** | **58.50** | **51.49** | **53.16 (+2.83)** |\n| Diginetica | FuzzQE + Attn-Mixer | 88.30 | 32.88 | 82.75 | 34.50 | 59.61 |\n|  | Q2P + Attn-Mixer | 40.28 | **43.93** | 54.31 | **48.20** | 46.68 |\n|  | NQE | 86.25 | 20.79 | 64.74 | 20.93 | 48.18 |\n|  | SQE + Transformers | 88.05 | 31.33 | 81.77 | 35.83 | 59.25 |\n|  | LSGT (Ours) | **91.71** | 35.24 | **83.30** | 41.05 | **62.83 (+3.22)** |\n| Dressipi | FuzzQE + Attn-Mixer | 65.43 | 95.64 | 53.36 | 97.75 | 78.05 |\n|  | Q2P + Attn-Mixer | 60.64 | 96.78 | 52.22 | 97.28 | 76.73 |\n|  | NQE | 31.96 | 96.18 | 9.89 | 97.80 | 58.96 |\n|  | SQE + Transformers | 72.61 | 97.12 | 55.20 | 98.14 | 80.77 |\n|  | LSGT (Ours) | **74.34** | **97.30** | **58.30** | **98.23** | **82.04 (+1.28)** |\n\nWe compared their performance against the baselines and found that our proposed method showed stronger compositional generalization on these unseen query types. It achieved MRR improvement ranging from 1.28 to 3.22 on three datasets.\n\nRe Q1\nWe have added the evaluation of compositional generalization, as in previous work, and included the results in the original paper. Initially, we did not include compositional generalization because we believed it was not very important in the real setting of session reasoning. We thought that if we wanted to improve performance on some unseen query types, a better approach would be to sample more data with that query type and conduct supervised training on it. \n\nHowever, we now realize the importance of compositional generalization in the general problem of complex query answering. Therefore, we further included the experiment in Re W4 to demonstrate the effectiveness of our method on compositional generalization.\n\n\nRe Q2\n\nWe selected and derived fourteen query types from the original fourteen types of queries in [1,2]. The 2iS and 2iA are both derived from 2i queries. They involve 1 session 1 item and 2 sessions as their anchors, respectively.\n\nRegarding pni, we found it difficult to interpret its practical meaning in a real problem setting. It can be interpreted as \"Predict the attribute value of a given product that is not desired by a given session.\" However, we believe that predicting the attribute value for a single item is not meaningful in session understanding. Therefore, we did not include this type in our training and evaluation so that we could focus on other important query types."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2938/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674218942,
                "cdate": 1700674218942,
                "tmdate": 1700674218942,
                "mdate": 1700674218942,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]