[
    {
        "title": "Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning"
    },
    {
        "review": {
            "id": "mqldrZPwTM",
            "forum": "AY6aM13gGF",
            "replyto": "AY6aM13gGF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5414/Reviewer_ei7j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5414/Reviewer_ei7j"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel framework, termed LaMo, aimed at enhancing the utilization of pre-trained Large Language Models (LLMs) in the context of offline Reinforcement Learning (RL). The novelty of the proposed framework is threefold: Firstly, it employs Low-rank Adaptation (LoRA) during the fine-tuning process, targeting a specific subset of model weights and finetuning with offline experience data. Secondly, it innovatively replaces the conventional linear projections for Query (Q), Key (K), and Value (V) within each attention block with Multi-Layer Perceptrons (MLPs). Thirdly, LaMo incorporates a next-word prediction loss in addition to the primary sequence modeling loss to mitigate overfitting concerns."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality: The framework introduced in this paper demonstrates novelty through a unique amalgamation of established fine-tuning techniques. It draws inspiration from the wiki-RL paradigm but stands out by effectively addressing limitations that hindered previous work.\nQuality: This paper exhibits a commendable level of quality. The authors meticulously design their experiments to empirically validate the individual components of the framework.\nClarity: The manuscript exhibits a high degree of clarity in its presentation. The conceptual underpinnings and experimental setups are readily comprehensible.\nSignificance: The findings presented in this paper hold great promise. Notably, the results span a diverse array of scenarios, including sparse and dense reward tasks, varying data scales, and a comprehensive ablation study."
                },
                "weaknesses": {
                    "value": "In section 5.5 Ablations, while empirical results indicate the superiority of the former, the absence of a deeper analysis of the choice to use MLPs warrants consideration. It is advisable to provide further insight into the theoretical basis and motivations for this decision."
                },
                "questions": {
                    "value": "Several questions and suggestions:\n1. In section 4.2 you mentioned that you used LORA to inject low-rank matrices into attention weights Q, K and V only and freeze all other weights inside the Transformer, given that there are other large MLPs inside it, what is the rationale of only applying LoRA to Q, K and V?\n2. In section 5.5, the benchmark tasks you used for comparison change sometimes, I\u2019m curious how you select ablation benchmarks for showing different components in your framework works better?\n3. It would be nice to see how different scales of GPT-2 could affect the performance on your benchmarks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5414/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698712922824,
            "cdate": 1698712922824,
            "tmdate": 1699636549308,
            "mdate": 1699636549308,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vLBwxddm2y",
                "forum": "AY6aM13gGF",
                "replyto": "mqldrZPwTM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5414/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5414/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments and suggestions. We address each of your comments in the following.\n\n***Weaknesses***:\n\n***Q1**: In section 5.5 Ablations, while empirical results indicate the superiority of the former, the absence of a deeper analysis of the choice to use MLPs warrants consideration. It is advisable to provide further insight into the theoretical basis and motivations for this decision.*\n\n**A1**: In Section 5.5, we propose that \u201cWe find that simple linear projections could not fully exploit the cross-domain knowledge from language pre-training, and thus our design to replace linear projections with MLPs is critical.\u201d As a supplement, we hypothesize that this is because the stronger representation power prevents inductive bias in pre-trained Transformers from being lost during the fine-tuning process. On the other hand, LoRA restricts the model\u2019s learning ability, thus deeper embeddings are required. Actually, this improvement is motivated by our initial explorations: in experiments, we found that w. MLP is much more effective than w.o. MLP on MuJoCo tasks when the Transformer layer is frozen. An overall ablation study on this component further strengthens our statement, shown in **Appendix G, Fig 12**.\n\n***Questions***:\n\n***Q2**: In section 4.2 you mentioned that you used LORA to inject low-rank matrices into attention weights Q, K and V only and freeze all other weights inside the Transformer, given that there are other large MLPs inside it, what is the rationale of only applying LoRA to Q, K and V?*\n\n**A2**: We inject low-rank matrices into Q, K, V for mainly two reasons. The first reason is that Q,K, and V matrices will interact with the embeddings directly in the process of calculating Attention, and since we have changed the embeddings, adaptations of these weights are intuitively required. The second reason is that as is mentioned in the original LoRA paper **[1] page 5, line 8**, for simplicity and parameter-efficiency, we can only adapt attention weights and get good results. We follow their method.\n\n***Q3**: In section 5.5, the benchmark tasks you used for comparison change sometimes, I\u2019m curious how you select ablation benchmarks for showing different components in your framework works better?*\n\n**A3**: We have enhanced the ablations during the rebuttal phase and we hope that these full and consistent ablation experiments in **Appendix G** could show the consistent advantages of our proposed techniques in leveraging the pre-trained language model for offline RL tasks. \n\n***Q4**: It would be nice to see how different scales of GPT-2 could affect the performance on your benchmarks.*\n\n**A4**: Yes, we agree with you that this question is worth exploring. Due to the limited computing resources and the fact that GPT2-small has demonstrated the effectiveness of this approach, we only compare the effects of GPT2-medium on some tasks. The experimental results show that in most cases, there is no significant difference between GPT2-medium and GPT2-small, while in some tasks (Kitchen Complete (30%) and Kitchen Partial (100%)), GPT2-medium slightly exceeds GPT2-small, shown in **Appendix F, Fig 10**. We intend to further explore this aspect in our future research projects.\n\n[1] Hu, et al. \u201cLoRA: Low-Rank Adaptation of Large Language Models\u201d arXiv preprint arXiv:2106.09685"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5414/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700374305983,
                "cdate": 1700374305983,
                "tmdate": 1700374305983,
                "mdate": 1700374305983,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QmAtBlDb3g",
                "forum": "AY6aM13gGF",
                "replyto": "mqldrZPwTM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5414/Reviewer_ei7j"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5414/Reviewer_ei7j"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I think that generally addresses my concerns and I would keep my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5414/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661647318,
                "cdate": 1700661647318,
                "tmdate": 1700661660160,
                "mdate": 1700661660160,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "a1TZllJVvW",
            "forum": "AY6aM13gGF",
            "replyto": "AY6aM13gGF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5414/Reviewer_1Dwk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5414/Reviewer_1Dwk"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method to utilize the power of pretrained LLMs for offline RL with 3 important design choices: 1) use non-linear MLPs for token embedding and prediction layers; 2) Finetune the pretrained model with LoRA; and 3) regularize the fine-tuned model with a language loss. The experiments show that the proposed method boosts the performance of DT in both sparse and dense reward settings, especially in the low-data regime. The paper conducted several ablation studies to show the importance of each of the proposed design choices."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Presentation: the paper was easy to follow because of its simplicity and clear writing. \n- The idea is intuitive and simple, and can potentially be adapted to other LLMs as well as other offline RL models and tasks.\n- The empirical results are good and the ablation studies are comprehensive, proving the importance of each proposed component."
                },
                "weaknesses": {
                    "value": "- Since the paper only studied one LLM (GPT-2) and one RL model (DT), I wonder if the same methodology generalizes to other LLMs and RL models. Since LLMs are improving quickly, it's important that the conclusion in the paper also holds for newer and stronger LLMs.\n- The baselines used for comparison seem a bit outdated. There have been much stronger baselines in offline RL in recent years, especially diffusion-based methods such as Diffusion-QL [1] and DD [2]. The authors should include these as stronger baselines.\n- The experiments only include the medium datasets in D4RL and are missing the medium-replay and medium-expert datasets. I expect LaMO to perform well on medium-replay as these datasets are of low-quality and should highlight the advantage of language pretraining.\n\nMinor comments:\n- The first paragraph of Section 4.1 is a bit confusing, it made me think that the authors pretrained GPT-2 themselves. It should be stated clearly that the authors used the pretrained GPT-2 from HF and only pretrained its special variants.\n\n[1] Wang, Zhendong, Jonathan J. Hunt, and Mingyuan Zhou. \"Diffusion policies as an expressive policy class for offline reinforcement learning.\" arXiv preprint arXiv:2208.06193 (2022).\n\n[2] Ajay, Anurag, et al. \"Is conditional generative modeling all you need for decision-making?.\" arXiv preprint arXiv:2211.15657 (2022)."
                },
                "questions": {
                    "value": "- Can we replace the language loss with a regularization loss so that the fine-tuned model is not too far away from the original pretrained model? For example, we can minimize the discrepancy between the embeddings of the fine-tuned and pretrained models."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5414/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780282500,
            "cdate": 1698780282500,
            "tmdate": 1699636549208,
            "mdate": 1699636549208,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gD7jQdtw7m",
                "forum": "AY6aM13gGF",
                "replyto": "a1TZllJVvW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5414/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5414/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments and suggestions. We address each of your comments in the following.\n\n***Weaknesses***:\n\n***Q1**: Since the paper only studied one LLM (GPT-2) and one RL model (DT), I wonder if the same methodology generalizes to other LLMs and RL models. Since LLMs are improving quickly, it's important that the conclusion in the paper also holds for newer and stronger LLMs.*\n\n**A1**: Yes, we agree with that. As explained in **Appendix I.1**, we have explored stronger language models, such as the application of LLaMA in the DT framework. However, as far as we know, there has been no successful application of LLaMA to DT, and due to the limitation of computing resources, we have not been able to achieve systematic and significant results. Most of the methods mentioned in **Related Works** \u201cTransformers for decision making\u201d are worthy of applying our methods, which are beyond the goal of this work.\n\n***Q2**: The baselines used for comparison seem a bit outdated. There have been much stronger baselines in offline RL in recent years, especially diffusion-based methods such as Diffusion-QL and DD. The authors should include these as stronger baselines.*\n\n**A2**: Thank you very much for your constructive opinion. In recent work focused on Decision Transformer **[1][2]**, we find them not using diffusion policy as the baseline, which may be due to the large gap compared with DT. However, as recent strong baselines, we think they are worth comparing. We have tried it out on our benchmarks, shown in **Appendix F, Table 12**, prominently showing the power of our method under poor-data regime.\n\n***Q3**: The experiments only include the medium datasets in D4RL and are missing the medium-replay and medium-expert datasets. I expect LaMO to perform well on medium-replay as these datasets are of low-quality and should highlight the advantage of language pretraining.*\n\n**A3**: We have conducted experiments on Medium-Replay and Medium-Expert datasets, shown in **Appendix F, Table 8,9**, the results solidate the advantages of our method. As mentioned in **Appendix B**, we explored both Complete and Partial datasets in the Kitchen environment, while we focused on Medium datasets in the MuJoCo and Atari environments. We emphasize again: Medium data sets are mainly used in our work because they meet two properties that we want: 1. low-quality 2. fair and robust for sampling in poor data regime, as we stated in **Appendix B**, \"The utilization of this dataset is aimed at minimizing variations in quality among different trajectories\".\n\n***Q4**: The first paragraph of Section 4.1 is a bit confusing, it made me think that the authors pretrained GPT-2 themselves. It should be stated clearly that the authors used the pretrained GPT-2 from HF and only pretrained its special variants.*\n\n**A4**: Thank you for pointing out that our statements are a bit confusing, and we have rephrased it in our updated paper.\n\n***Questions***:\n\n***Q5**: Can we replace the language loss with a regularization loss so that the fine-tuned model is not too far away from the original pre-trained model? For example, we can minimize the discrepancy between the embeddings of the fine-tuned and pre-trained models.*\n\n**A5**: In this work we actually use language loss as a regularization term, which is particularly effective in certain tasks, as we stated in Section 5.5. Minimizing embedding discrepancy is interesting and reasonable, but we think it cannot be used in our model in a straightforward way: We are using two distinct sets of embedding for language and motion control respectively. As a matter of the fact, the method of reducing the discrepancy of two embeddings has been tried by Wiki-RL, where they utilize K-means clustering and similarity loss, which turns out not to be very helpful, as stated in their paper **[3] Section 5.6 Table 6**.\n\n[1] Wu, et al. \u201cElastic Decision Transformer\u201d arXiv preprint arXiv:2307.02484\n\n[2] Yamagata, et al. \u201cQ-learning Decision Transformer: Leveraging Dynamic Programming for Conditional Sequence Modeling in Offline RL\u201d arXiv preprint arXiv:2209.03993\n\n[3] Reid, et al. \u201cCan Wikipedia Help Offline Reinforcement Learning?\u201d arXiv preprint arXiv:2201.12122"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5414/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700374608204,
                "cdate": 1700374608204,
                "tmdate": 1700374608204,
                "mdate": 1700374608204,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SIcdXJOYxW",
                "forum": "AY6aM13gGF",
                "replyto": "gD7jQdtw7m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5414/Reviewer_1Dwk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5414/Reviewer_1Dwk"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the response and the additional experiments, which have addressed some of my concerns. However, my main concern regarding the applicability of the framework to other LLMs and RL models still remains. Therefore, I'll keep my original score, which leans toward acceptance.\n\nQuestion:\n- In Table 8 and 9 why did you test LaMO on two different ratios?"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5414/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678940374,
                "cdate": 1700678940374,
                "tmdate": 1700678940374,
                "mdate": 1700678940374,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0pw73JoDYO",
            "forum": "AY6aM13gGF",
            "replyto": "AY6aM13gGF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5414/Reviewer_mYMk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5414/Reviewer_mYMk"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the use of decision transformer (DT) and pretrained language models for offline RL control. The authors propose a new framework Language Models for Motion Control (LaMo) that improves upon a naive use of a pretrained LM for DT training. The framework 1) uses a pretrained LM model, 2) uses LoRA fine-tuning, 3) use higher capacity input embedding and output networks and 4) use auxiliary language prediction loss during finetune. Empirical results are provided to show the proposed method outperform other transformer-based methods and also offline RL methods in a number of benchmark tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**originality**\n- Main novelty of the paper is an improved framework to finetune pretrained LM on RL task. Compared to the Reid et al paper, the main additions are the finetuning technique and the increased capacity of the projection layer for input embeddings and output layer. Though the changes are relatively simple, they are shown to provide much stronger performance, can be a novel contribution. \n\n**quality**\n- technical details provided for reproducing the results. \n- good discussion of related works\n\n**clarity**\n- paper is written clearly and easy to follow \n\n**significance**\n- some insights are provided on potential reason that DT methods work better on sparse reward setting. \n- ablations showing the contribution of each component. \n- the proposed changes are not too complex, I appreciate the simplicity.  \n- empirical resutls show significant improvement over DT and DT+Wiki over all 3 benchmarks."
                },
                "weaknesses": {
                    "value": "Comparisons: \n- Figure 1, in Reid et al paper, they show that a pretrained DT tend to give improved performance, why in your figure DT gets better performance in Kitchen and Atari? \n- Did you finetune the methods you are comparing to on the benchmarks you are studying? \n- The other thing is only some of the tasks in each benchmark are studied, so it is also a bit concerning whether there will still be a big perforrmance gap between proposed method and baseline when other tasks are also tested. \n\nNovelty and significance:\n- Technical novelty of the method is a bit lacking. Essentially compared to DT+Wiki, a different finetuning method is used, and the projection and output layers are made bigger. Neither of these are new techniques."
                },
                "questions": {
                    "value": "It is unclear why language pretraining can help RL tasks which have a large domain gap?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5414/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816626103,
            "cdate": 1698816626103,
            "tmdate": 1699636549105,
            "mdate": 1699636549105,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vDaX0jEOsg",
                "forum": "AY6aM13gGF",
                "replyto": "0pw73JoDYO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5414/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5414/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mYMk --- Part 1/2"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments and suggestions. We address each of your comments in the following.\n\n***Weaknesses***:\n\n***Q1**: Figure 1, in Reid et al paper, they show that a pre-trained DT tends to give improved performance, why in your figure DT gets better performance in Kitchen and Atari?*\n\n**A1**: We strongly agree with the point that \u201cpre-trained DT tends to give improved performance\u201d, while we would like to add that techniques proposed in our work are actually required to \u201cunleash\u201d the power of pre-trained DT. In MuJoCo tasks, Wiki-RL gains higher scores in each task compared with DT, which is both examined by **[1]** and our work. However, their work has not tested Wiki-RL in the Kitchen environment, which is harder than MuJoCo. In the Atari environment, the dataset they adopt (1% DQN-replay dataset) is different from ours (100%, 10% D4RL Medium dataset). After running extensive experiments, we find that Wiki-RL could not prominently leverage the power of pre-trained DT in these two difficult settings although they obtain advantages in MuJoCo tasks. We propose this is due to the fact that fully fine-tuning the pre-trained model could compromise the general knowledge of pre-trained models, and we find that full fine-tuning could not work at all in certain tasks. This phenomenon is presented in the ablation part **Appendix G, Fig 13**.\n\n***Q2**: Did you finetune the methods you are comparing to on the benchmarks you are studying?*\n\n**A2**: Yes, we have looked into the hyperparameters configuration of baselines on our benchmarks, which investigate the poor data regime of D4RL. Specifically, we run the baseline on MuJoCo and Kitchen environments based on CORL (https://github.com/tinkoff-ai/CORL) and run the baseline on Atari using d3rlpy (https://github.com/takuseno/d3rlpy). For both DT-based and value-based methods, we tuned parameters on representative tasks from each domain of our benchmark. It is crucial to note that, within the range of settings of hyperparameters explored, the current parameters adopted by our baselines have been identified as not worse than others, and the results are shown in **Appendix F, Fig 11**.\n\n***Q3**: The other thing is only some of the tasks in each benchmark are studied, so it is also a bit concerning whether there will still be a big performance gap between proposed method and baseline when other tasks are also tested.*\n\n**A3**:  To ensure strong alignment with the existing literature on DT and offline RL research, we selected tasks identical to those used in **[1][2][3][4]**, including MuJoCo domains such as Hopper, Walker2d, Halfcheetah, Reacher, and Atari domains like Breakout, Qbert, and Pong. Additionally, we included a challenging domain, Kitchen, where our approach demonstrated promising results. It is crucial to emphasize that our choice of tasks is not due to any limitations in applicability to other tasks; rather, we specifically focused on tasks that are most commonly encountered in this research direction, ensuring maximum comparability. To solidify this point, we conduct additional tests on tasks Freeway, Asterix in Atari and Ant in MuJoCo, as shown in **Appendix F, Table 10,11**, which further prove the claims made in our **Experiments** part.\n \n***Q4**: Technical novelty of the method is a bit lacking. Essentially compared to DT+Wiki, a different finetuning method is used, and the projection and output layers are made bigger. Neither of these are new techniques.*\n\n**A4**: \n\nWe acknowledge that the implementations we utilized, such as LoRA, MLP embedding, and language loss, are neither novel nor overly complex. However, we would like to underscore that the novelty of our work does not solely lie in any individual module we employed. Instead, our innovation stems from the exploration of the scientific question\u2014whether language pre-training can help offline RL, which is inherently novel and insightful. We successfully demonstrated the potential of this direction by effectively combining simple existing methods, highlighting this as the distinctive aspect of our work.\n\nAs articulated in **Related Works** \u201cLLM for decision making\u201d part, many recent studies focus on using language models as high-level planners or are limited to tasks primarily involving textual input. In contrast, our approach investigates the possibility of efficiently leveraging the inductive bias of language models for low-level motion control without expanding model parameters. We provide an affirmative answer to this question. Furthermore, devising and correctly combining these implementations is non-trivial, as evidenced by our ablation experiments. Incorrectly using the pre-trained model resulted in suboptimal performance. Our framework, which combines all these implementations and adeptly adapts the pre-trained model, unleashes the potential of pre-trained language models for offline RL."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5414/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700374995994,
                "cdate": 1700374995994,
                "tmdate": 1700374995994,
                "mdate": 1700374995994,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lFwN3eCFOk",
            "forum": "AY6aM13gGF",
            "replyto": "AY6aM13gGF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5414/Reviewer_GHKm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5414/Reviewer_GHKm"
            ],
            "content": {
                "summary": {
                    "value": "The authors observe shortcomings in previous applications of pre-trained (transformer) language models (LM) in reinforcement learning (RL) and in particular in control: they are only used as initializations or as interfaces and do not outperform non-pre-trained transformer models such as decision transformers (DT).\n\nThey propose LoMa which adds 3 components to better leverage (or \"unleash\") pre-trained LMs in RL: 1) replacing the linear embedding with trainable MLPs, 2) performing low-rank adaptation (LoRA) for the RL training, and 3) maintaining the original language model loss while training with RL.\n\nThe authors claim that LoMa outperforms competitive offline RL baselines and DT on sparse-reward tasks and in low-data regimes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper attempts to overcome important shortcomings of leveraging foundation models (here LMs) in RL in particular offline RL.\n\nOriginality:\n- Adding the sparse-reward experiments compared to the dense-reward environments in the Wiki-RL reference paper [1] to show the benefits of using pre-trained models is original and well-motivated.\n- Using LoRA to fine-tune the LM to a policy is interesting, ~~although not well-motivated.~~\n- The ablation with a model pre-trained on a random corpus to show the importance of sequential modeling is original.\n\n\nClarity:\n- The motivation and the contributions of the paper are very clear. \n- The structure of the paper is easy to follow.\n- Overall the paper is well written.\n\nSignificance:\n- LaMo performs competitively in the low-data regime as opposed to Wiki-RL showing that it potentially leverages/\u201dunleashes\u201d pre-trained LMs in a better way.\n- Overfitting, and other capacity loss phenomena, arising when more gradient steps are performed on the same data are major problems in offline RL. Observing that the auxiliary language loss (and maybe LoRA) introduced in LaMo helps in overcoming this issue is relevant beyond the scope of this paper.\n\n[1] Reid, Machel, Yutaro Yamada, and Shixiang Shane Gu. \"Can Wikipedia help offline reinforcement learning?.\" arXiv preprint arXiv:2201.12122 (2022)."
                },
                "weaknesses": {
                    "value": "*(update: all the weaknesses below have been addressed by the authors.)*\n\nThe points about the experimental results raised below and the questions raised in the question sections make it hard for me to confirm the validity of the claim made in the paper that LoMa outperforms baselines and that its components are crucial.\nClarifications on the experimental setup and results if valid would help raise my soundness and overall score.\n\n**Major:**\n\n1) \nI have a major concern over the validity of the experimental protocol and results presented in the paper from the observation that the performance reported for the baselines differs significantly from their performance as reported in their original papers. In particular\n- BC and CQL in the D4RL paper [2] obtain scores significantly divergent from the scores reported in this paper. Can the authors explain this divergence, and if due to different training budgets/settings explain the motivation behind the different settings?\nE.g. CQL and BC achieve 43.8 and 33.8, respectively, on Franka-complete in D4RL [2], but are reported to achieve 0 scores in this paper. (Overfitting of CQL is mentioned, but does not clarify the discrepancy)\n- Also BC and CQL are reported to perform better on Kitchen with 1% of the datasets (or other fractions) than on Kitchen with full datasets. Same for Wiki-RL and DT on some specific splits.\n- Similarly, Wiki-RL is reported to perform worse than the same model (ChibiT) in its original paper on Atari. Whereas DT is reported to perform better than in the wiki-RL paper. E.g. in Breakout 350 DT  and 130 Wiki-RL in this paper vs 267 DT and 280 Wiki-RL in the Wiki-RL paper.\n\n2) \n The ablation studies are presented each time on a different task&dataset-ratio combination and the full ablation results are not in the appendix (only the pre-training ablation is given). This can lead to a biased evaluation of LaMo if only good results are presented.\nThe combination of components of LaMo is a major contribution of the paper, claimed to \"unleash\" the potential of pre-trained LMs, and ablation experiments are thus crucial to assess the validity of this claim and the significance of the paper.\n\n\n**Significant:**\n- I don't find LoRA to be well-motivated, or at least that its benefits are. The authors mention early overfitting but this is not clear from Figure 5 (LoMa also experiences small drops in performance).\n-  Several elements of the paper can be mistakenly interpreted as original contributions of the paper, such as the language modeling loss (also used in Wiki-RL), the ablation on ImageGPT (also used in Wiki-RL), the selection of the baseline (similar to Wiki-RL).\nWhile not being novel was not the issue, it was often not clear if those contributions were original.\n- It\u2019s hard to assess the significance of an improvement when curves are shown with shaded areas in [\u03bc \u2212 0.5\u03c3, \u03bc + 0.5\u03c3].\n\n**Minor:**\n- Notation is not ambiguous for readers familiar with the topics but would benefit from better presentation. Some variables are undefined in some equations e.g. the expectation distributions are underspecified in equation 1, $a'$ in equation 3, $T$ in equation 4.\n- Citations don\u2019t include the journal/proceedings details which makes it hard to identify which version of a paper was used in some cases.\n\n[2] Fu, Justin, et al. \"D4rl: Datasets for deep data-driven reinforcement learning.\" arXiv preprint arXiv:2004.07219 (2020)."
                },
                "questions": {
                    "value": "*(update: all the questions below have been addressed by the authors.)*\n\n**Major:**\n\n- Can the authors point to the specifications of the Reacher2d environment they used? The Reacher I\u2019m familiar with[2] would not be considered a sparse-reward environment. This would also help to confirm the expert score.\nIt would also help to have the score of the policy that generated the medium dataset.\n- Can the authors indicate which Atari offline datasets they have used? D4RL does not seem to provide Atari datasets in its original version.\n- How much hyperparameter tuning has been spent on the value of the language loss hyperparameter and the fraction of parameters to train with LoRA?\n- Are the hyperparameters in Appendix E, used with all transformers in the paper? (LoMa, Wiki-RL, DT)?\n- Do the transformers and non-transformers used in the paper have a comparable number of parameters to ensure a fair comparison of performance?\n\n**Minor:**\n- Figure 6(a): why do the curves not start from the same point at training step 0? How can the authors explain that the cross-entropy loss decreases significantly for an already pre-trained model (red curve)? and also eventually decreases for the ablation (blue curve)\n\n[3] https://gymnasium.farama.org/environments/mujoco/reacher/"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5414/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5414/Reviewer_GHKm",
                        "ICLR.cc/2024/Conference/Submission5414/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5414/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698858462852,
            "cdate": 1698858462852,
            "tmdate": 1700642153338,
            "mdate": 1700642153338,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1DzNQG6RIB",
                "forum": "AY6aM13gGF",
                "replyto": "lFwN3eCFOk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5414/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5414/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GHKm --- Part 1/4"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments and suggestions. We address each of your comments in the following.\n\n***Weaknesses***:\n\n***Major***:\n\n***Q1**: I have a major concern over the validity of the experimental protocol and results presented in the paper from the observation that the performance reported for the baselines differs significantly from their performance as reported in their original papers. In particular\uff0c*\n\n***Q1.1**: BC and CQL in the D4RL paper obtain scores significantly divergent from the scores reported in this paper. Can the authors explain this divergence, and if due to different training budgets/settings explain the motivation behind the different settings? E.g. CQL and BC achieve 43.8 and 33.8, respectively, on Franka-complete in D4RL, but are reported to achieve 0 scores in this paper. (Overfitting of CQL is mentioned, but does not clarify the discrepancy)*\n\n**A1.1**: \n\nYes, the difference arises from the reported metric. Some offline RL papers use **the performance of the best several checkpoints**, while for more scientific study, we use \u201c**the average performance over the last 20K training steps out of a total of 100K training steps with evaluations conducted every 2500 training steps**\u201d as stated in **Section 5.1**. This is because in the context of offline RL, the RL agent could never interact with the environment during training\uff0c and thus mitigating overfitting is a crucial aspect of a method's effectiveness. This choice aligns with our perspective on the importance of robustness in the evaluation of offline RL methods.\n \nTherefore, when an algorithm exhibits overfitting on a certain task (shown in **Appendix F, Fig 9**) after an excessive number of gradient steps, differences in metrics can result in discrepancies between our results and the values reported in the D4RL paper. We are also willing to report the results on the metric of top performance, as shown in **Appendix F, Table 13**, which shows the consistent advantages of our method and solidates the validity of our claims.\n\n***Q1.2**: Also BC and CQL are reported to perform better on Kitchen with 1% of the datasets (or other fractions) than on Kitchen with full datasets. Same for Wiki-RL and DT on some specific splits.*\n\n**A1.2**: \n\nYes, this is reasonable because of the property of the Kitchen dataset: the Partial dataset contains data that cannot complete the task and data that can, as described in **Appendix B**. Therefore, when the dataset is being downsampled, the number of varieties of data would decrease and thus could be easier for the model to learn. This explains why algorithms could perform better with less data in certain cases.\n\nBesides, as mentioned in **A1.1**, there also exists the issue of overfitting. In cases of overfitting, comparing the weak performance lacks statistical meaning.\n\n***Q1.3**: Similarly, Wiki-RL is reported to perform worse than the same model (ChibiT) in its original paper on Atari. Whereas DT is reported to perform better than in the wiki-RL paper. E.g. in Breakout 350 DT and 130 Wiki-RL in this paper vs 267 DT and 280 Wiki-RL in the Wiki-RL paper.*\n\n**A1.3**: The scores reported in Wiki-RL are different from those in our paper because we use different datasets. Wiki-RL runs its experiments on the 1% DQN-replay Atari dataset, while we run our experiments on the D4RL-Atari medium dataset, of different data ratios.\n\n***Q2**: The ablation studies are presented each time on a different task & dataset-ratio combination and the full ablation results are not in the appendix (only the pre-training ablation is given). This can lead to a biased evaluation of LaMo if only good results are presented. The combination of components of LaMo is a major contribution of the paper, claimed to \"unleash\" the potential of pre-trained LMs, and ablation experiments are thus crucial to assess the validity of this claim and the significance of the paper.*\n\n**A2**: We enhance the ablations during the rebuttal phase and we hope that these full and consistent ablation experiments could show the consistent advantages of our proposed techniques in leveraging the pre-trained language model for offline RL tasks, as shown in **Appendix G**."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5414/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700375597031,
                "cdate": 1700375597031,
                "tmdate": 1700375597031,
                "mdate": 1700375597031,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4GknQi1DOZ",
                "forum": "AY6aM13gGF",
                "replyto": "lFwN3eCFOk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5414/Reviewer_GHKm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5414/Reviewer_GHKm"
                ],
                "content": {
                    "title": {
                        "value": "Most concerns resolved. One remaining."
                    },
                    "comment": {
                        "value": "I thank the authors for their complete answers to all my concerns. Most of them have been resolved, improving the presentation of the paper and increasing my confidence that the experiments support the claims made in the paper (although I cannot validate the numbers themselves without running the code. In the future, I would strongly encourage the authors to share their code or maybe share tracking results from an experiment tracking service like Weights & Biases).\n\nI have one concern remaining regarding the size of the models compared which I'd like the authors to address.\n\nIn their reply, the authors say that\n(**A12:**) *\"The numbers of parameters of DT, Wiki-RL, and our method are 7.3M, 125M, and 128M, respectively. And for non-transformers (value-based) methods, the numbers are relatively small, such as 0.41M for CQL.\"*  \nRevealing a large gap in the number of parameters of the models compared, even within the same family of transformers DT vs LaMo.\nHowever, the authors also point out that the comparison should not be made on the basis of the total number of parameters but on the basis of the number of trainable parameters, which in that case would be 3.5M for LaMo and comparable to DT.\n\nI appreciate this distinction, but believe that it could benefit from an explicit presentation in the paper, so that future readers do not extrapolate or misunderstand from the paper that using LaMo would result in better performance than training a DT model of the same size.\nIt would also be beneficial to state the number of (trainable) parameters of all models and baselines in the appendix.\n\nRegarding value-based models I believe that the following claim would need a reference:  \n*\"For value-based methods, when the model parameters are relatively large, training becomes extremely challenging, leading to results worse than smaller models. Therefore, for value-based baselines, we adhere to widely accepted model sizes, which are much smaller than Transformer-based methods.\"*\n\nWith both of these resolved (making it clear that it's trainable sizes that are compared, listing them, and justifying the size of the value-based ones), I would be happy to significantly increase my score.\n\n*Minor concern*. \n*A10: \"To determine the hyperparameter lora-dim that governs the parameter scale for LoRA, we explored  values of 16, 32, and 64, ultimately choosing 32 for Atari, and 16 for MuJoCo, Kitchen.\"*   \nCan this be reflected in table 7?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5414/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571177077,
                "cdate": 1700571177077,
                "tmdate": 1700571177077,
                "mdate": 1700571177077,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4TKgTVfsJ0",
                "forum": "AY6aM13gGF",
                "replyto": "hL25wRLjoA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5414/Reviewer_GHKm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5414/Reviewer_GHKm"
                ],
                "content": {
                    "title": {
                        "value": "All concerns resolved. Raising my score from 5 to 8."
                    },
                    "comment": {
                        "value": "I thank the authors for addressing and resolving all the concerns I had. I have raised my score from 5 to 8 and updated my review."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5414/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641964284,
                "cdate": 1700641964284,
                "tmdate": 1700641964284,
                "mdate": 1700641964284,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]