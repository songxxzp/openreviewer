[
    {
        "title": "MemStranding: Adversarial attacks on temporal graph neural networks"
    },
    {
        "review": {
            "id": "Roj4SKmU1B",
            "forum": "Pmrc0nEvxf",
            "replyto": "Pmrc0nEvxf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8086/Reviewer_4nwA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8086/Reviewer_4nwA"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the problem of adversarial attack on temporal graph neural networks.  Two challenges (e.g., Noise Decaying and Knowledge Missing) in attacking  temporal GNNs are first proposed. To address these challenges, the authors proposes an effective adversarial attack framework called MemStranding. Experimental results show that MemStranding can significantly decrease the TGNN models\u2019 performances in various tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper focuses on adversarial attacks on TGNNs, studying the robustness of TGNNs to benefit their applications.\n2. The authors provide a clear explanation of why typical attacks fail in dynamic graph scenarios.\n3. In response to the failure of typical attacks, two attack goals are proposed: noise persisting and noise propagating, effectively addressing the issue."
                },
                "weaknesses": {
                    "value": "1. The studied problem is interesting but the application scenarios of attacking TGNNs should be introduced.\n2. The paper provides a good explanation of why typical attacks fail, but does not provide experimental results. I am curious about the effectiveness of typical attacks on dynamic graphs.\n3. Section 4.3, Stage 1, lacks further analysis on victim node sampling. The choice of high-degree nodes as root nodes is explained, but there is no analysis of what would happen if low-degree nodes were chosen as root nodes. Moreover, are there criteria for selecting support nodes?"
                },
                "questions": {
                    "value": "1. If the persisting loss is removed, how would the similarity between the root node's memory and the initial noisy memory change in Figure 8? I'm curious about this.\n2. Still regarding Figure 8, how does the similarity of memory between the root node and its 1-hop and 2-hop neighbors change during normal training of TGNNs?\n3. The noise persisting loss and noise propagating loss proposed in this paper effectively prolong the efficacy of noise, even after multiple new actions. I'm curious about the effect if we combine the attack loss proposed in this paper with meta attack.\n4. Question about Section 4.3, Stage 2. The paper mentions, 'Lastly, we can add the solved noisy message as a fake node or fake edge accordingly and remove it after the attack.'  Is this similar to directly modifying the victim node's memory? Will this noisy message's impact spread with the arrival of the next action? \n\n[1] Adversarial Attacks on Graph Neural Networks via Meta Learning, ICLR 2019."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8086/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698729411433,
            "cdate": 1698729411433,
            "tmdate": 1699637001308,
            "mdate": 1699637001308,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lgiEiomqeo",
                "forum": "Pmrc0nEvxf",
                "replyto": "Roj4SKmU1B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 4nWA (1/3)"
                    },
                    "comment": {
                        "value": "## **Comments**\nWe would like to thank the reviewer for the positive feedback and valuable suggestions. We appreciate the reviewer's acknowledgment that our proposed work is interesting and clearly presented. We carefully addressed all the reviewers' questions and revised the paper accordingly. We hope our response can help clarify the reviewer's questions.\n\n## **Q1. Application scenarios of attacking TGNNs**\n\nThanks for the comment and valuable suggestion. The application scenarios of attacking TGNNs could exist in many dynamic graph prediction tasks. For instance, for social media such as Reddit, there could be node classification tasks that predict if a user/subreddit will be banned or not using TGNNs. The adversarial attacks toward these models can cause an innocent user/subreddit to always be banned mistakenly or prevent a malicious user/subreddit from being banned even if it always gives junk information, which causes severe benefits for attackers. We also add a brief example of this application scenario in **Section 1** of our revised paper. \n\n## **Q2. Effectiveness of typical static attacks on dynamic graphs:**\n\nThanks for the question. We would like to mention our baseline (i.e., FakeNode[1]) is a typical static GNN attack. As highlighted in **Section 2, Table 1, and Figure 6** in the original paper, while this static GNN attack can affect its target nodes and lead to significant performance degradation, the noise cannot persist in the target nodes and propagate to more victims. \n\n\nWe also include more static attacks, including **TDGIA**[2] and **Meta-Attack-Heuristic**[3], for a more comprehensive evaluation. The results are shown in **Table R.10**. Typical static attacks can effectively mislead TGNN models at the attack time; however, the effectiveness drops drastically after a few updates. These results indicate that typical static attacks can hardly work for dynamic graphs due to future updates on the victim nodes. We also added the results to **Table 1 in Section 5.2 and Tables 3&5 in Appendix B.3** in our revised paper.\n\n>**Table R.10.:** Accumulated accuracy over different timestamps of the vanilla/attacked TGNNs in WIKI and REDDIT.\n| Dataset |       | Wiki |       |       |        |   | Reddit |       |       |        |\n|---------|-------|------|-------|-------|--------|---|--------|-------|-------|--------|\n| **Model**   |       | **TGN** | **Jodie** | **Dyrep** | **Roland** |   | **TGN** | **Jodie** | **Dyrep** | **Roland** |\n| Vanilla Acc. |       | 0.93 | 0.87  | 0.85  | 0.94   |   | 0.96   | 0.98  | 0.96  | 0.95   |\n| $t_0$   | FN    | 0.81 | 0.74  | 0.74  | 0.82   |   | 0.84   | 0.83  | 0.84  | 0.83   |\n|         | Meta  | 0.9  | 0.83  | 0.81  | 0.85   |   | 0.93   | 0.95  | 0.9   | 0.92   |\n|         | TDGIA | 0.77 | 0.72  | 0.71  | 0.8    |   | 0.74   | 0.8   | 0.81  | 0.74   |\n|         | ours  | 0.89 | 0.88  | 0.85  | 0.87   |   | 0.75   | 0.84  | 0.94  | 0.82   |\n| $t_{25}$  | FN    | 0.92 | 0.87  | 0.85  | 0.94   |   | 0.97   | 0.97  | 0.96  | 0.93   |\n|         | Meta  | 0.93 | 0.87  | 0.84  | 0.93   |   | 0.96   | 0.98  | 0.94  | 0.96   |\n|         | TDGIA | 0.93 | 0.81  | 0.84  | 0.92   |   | 0.94   | 0.95  | 0.95  | 0.9    |\n|         | ours  | 0.8  | 0.8   | 0.78  | 0.85   |   | 0.81   | 0.84  | 0.91  | 0.8    |\n| $t_{50}$   | FN    | 0.94 | 0.87  | 0.86  | 0.94   |   | 0.97   | 0.97  | 0.96  | 0.95   |\n|         | Meta  | 0.93 | 0.87  | 0.85  | 0.93   |   | 0.97   | 0.98  | 0.94  | 0.95   |\n|         | TDGIA | 0.93 | 0.87  | 0.85  | 0.93   |   | 0.96   | 0.97  | 0.95  | 0.92   |\n|         | ours  | 0.75 | 0.81  | 0.76  | 0.84   |   | 0.8    | 0.84  | 0.91  | 0.8    |\n\n\n[1] Wang, Xiaoyun, et al. \"Fake node attacks on graph convolutional networks.\" Journal of Computational and Cognitive Engineering 1.4 (2022): 165-173.\n\n[2]  Zou, Xu, et al. \"Tdgia: Effective injection attacks on graph neural networks.\" Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 2021.\n\n[3] Li, Kuan, et al. \"Revisiting graph adversarial attack and defense from a data distribution perspective.\" The Eleventh International Conference on Learning Representations. 2022."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476412833,
                "cdate": 1700476412833,
                "tmdate": 1700476412833,
                "mdate": 1700476412833,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5faMAXTSTY",
                "forum": "Pmrc0nEvxf",
                "replyto": "Roj4SKmU1B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 4nWA (2/3)"
                    },
                    "comment": {
                        "value": "## **Q3. Analysis of target/support node sampling strategies**\n\nThanks for the suggestion. To further analyze the impact of victim node sampling, we compare the numbers of affected nodes and the resulting accuracy of selecting target nodes from the highest and lowest degree. The results are shown in **Table R.11**. As one can observe, the effectiveness of our attack is weakened in these cases. The main reason is that these low-degree nodes can only propagate noises to fewer nodes across the graph. Specifically, the affected nodes are significantly lower than the highest-degree first strategy. The main reason is that these low-degree nodes can only propagate noises to fewer nodes across the graph. \n\n> **Table R.11.** Number of affected nodes and accumulated accuracies of our attack under selecting 1% highest/lowest-degree victim nodes.\n|                |                    | t0   | t10  | t20  | t30  | t40  | t50  |\n|----------------|--------------------|------|------|------|------|------|------|\n| **Highest Degree** | # of Affected Node | 74   | 176  | 211  | 253  | 285  | 313  |\n|                | Overall Acc.       | 0.91 | 0.88 | 0.86 | 0.86 | 0.85 | 0.85 |\n|                |                    |      |      |      |      |      |      |\n| **Lowest Degree**  | # of Affected Node | 79   | 134  | 159  | 174  | 192  | 211  |\n|                | Overall Acc.       | 0.83 | 0.88 | 0.87 | 0.87 | 0.87 | 0.86 |\n\n\nHowever, it is noteworthy that even when targeting low-degree nodes, our attack still induces a significant drop in accuracy. This effect can be attributed to the fact that the injected noise, originating from low-degree nodes, propagates to their connected high-degree neighbors, maintaining sufficient potency to impact the network.\n\n\nFor the support node sampling, we also sample their highest-degree neighbors for the same purpose. We clarify this in **Section 4.3** in our revised paper.\n\n\n\n## **Q4~Q5. Similarities w/o persisting loss and in vanilla TGNNs.**\n\nThanks for the question. To conduct a more comprehensive study, we compare the cosine similarity between the memories of the root nodes at $t_0$ with those in themselves and their one-hop and two-hop neighbors at each timestamp after the attack in three versions: (1) MemStranding, (2 )MemStranding w/o persisting loss, and (3) those similarities in the original TGNN without attacks. As shown in **Table R.12**, the similarity between nodes' initial attacked memory and their future counterparts drops drastically in the normal TGNNs and fails to propagate to their neighbors and persist them. If the persisting loss is not enabled, the similarities also suffer drops and achieve much lower similarities in the future. We also include more results in **Figure 7 in Section 5.2, and Figure 14 in Appendix B.5** in our revised paper.\n\n>**Table R.12.** Similarities between nodes in (1) MemStranding, (2 )MemStranding w/o persisting loss, and (3) those similarities in the original TGNN without attacks.\n|                   |                  | t1    | t3   | t5   | t7   | t9   | t11  | t13  | t15  |\n|-------------------|------------------|-------|------|------|------|------|------|------|------|\n| **MemStranding** | Root Node        | 0.98  | 0.97 | 0.96 | 0.96 | 0.96 | 0.95 | 0.95 | 0.94 |\n|                   | One-Hop Neighber | 0.52  | 0.76 | 0.84 | 0.88 | 0.90 | 0.91 | 0.92 | 0.93 |\n|                   | Two-Hop Neighber | 0.26  | 0.54 | 0.68 | 0.79 | 0.83 | 0.86 | 0.89 | 0.90 |\n|                   |                  |       |      |      |      |      |      |      |      |\n| **MemStranding W/O Persist Loss**  | Root Node        | 0.86  | 0.77 | 0.74 | 0.70 | 0.69 | 0.68 | 0.69 | 0.67 |\n|                   | One-Hop Neighber | 0.02  | 0.26 | 0.39 | 0.52 | 0.57 | 0.60 | 0.61 | 0.63 |\n|                   | Two-Hop Neighber | -0.02 | 0.00 | 0.08 | 0.29 | 0.43 | 0.48 | 0.52 | 0.57 |\n|                   |                  |       |      |      |      |      |      |      |      |\n| **Normal TGNN**       | Root Node        | 0.39  | 0.12 | 0.06 | 0.05 | 0.01 | 0.01 | 0.01 | 0.00 |\n|                   | One-Hop Neighber | 0.00  | 0.03 | 0.04 | 0.08 | 0.10 | 0.08 | 0.09 | 0.09 |\n|                   | Two-Hop Neighber | 0.01  | 0.08 | 0.01 | 0.01 | 0.02 | 0.01 | 0.02 | 0.02 |"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476429750,
                "cdate": 1700476429750,
                "tmdate": 1700476429750,
                "mdate": 1700476429750,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Q1gRHectY9",
                "forum": "Pmrc0nEvxf",
                "replyto": "Roj4SKmU1B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 4nWA (3/3)"
                    },
                    "comment": {
                        "value": "## **Q6. The potential of combining our approach with the Meta Attack**\n\nThanks for the insightful question. It could be exciting to combine our approach with the meta-attack [4]. However, combining meta-attack with our approach is non-trivial for two reasons: (1) it is tailored for the GCN model; however, the approach is hard to migrate to TGNN, which adopts different message aggregation schemes. (2) it adds/deletes edges with no edge features; however, in our cases, the edge contains many features (e.g., 300 dimensions).\n\nTherefore, we leverage insights from an enhanced meta-attack version mentioned in [5] to evaluate the potential of combining our approach with the meta-attack. Specifically, we **(1) adopt its meta-attack-based heuristic to select the most effective fake edges (i.e., victim nodes and fake messages) and then (2) utilize our loss to solve the fake messages. The results are shown in Table R.13.**\n\n> **Table R.13.** Results of Meta-h, MemStranding (Ours), and combined attack (Meta-h + Ours)\n| **Dataset** |             | **WIKI** |       |       |    | **REDDIT** |       |       |\n|---------|-------------|------|-------|-------|----|--------|-------|-------|\n| **Model**   |             | **TGN**  | **Jodie** | **Dyrep** | \u3000 | **TGN**  | **Jodie** | **Dyrep** |\n| Vanilla |             | 0.93 | 0.87  | 0.85  | \u3000 | 0.96   | 0.98  | 0.96  |\n| $t_0$      | Meta-h        | 0.92 | 0.85  | 0.83  | \u3000 | 0.95   | 0.96  | 0.94  |\n|         | ours        | 0.90 | 0.85  | 0.86  | \u3000 | 0.93   | 0.94  | 0.94  |\n|         | Meta-h + Ours | 0.86 | 0.81  | 0.80  |    | 0.90   | 0.90  | 0.91  |\n|         |             |      |       |       |    |        |       |       |\n| $t_{25}$     | Meta-h        | 0.93 | 0.87  | 0.84  | \u3000 | 0.96   | 0.98  | 0.94  |\n|         | ours        | 0.80 | 0.80  | 0.78  | \u3000 | 0.81   | 0.84  | 0.91  |\n|         | Meta-h + Ours | 0.79 | 0.81  | 0.78  |    | 0.78   | 0.82  | 0.90  |\n|         |             |      |       |       |    |        |       |       |\n| $t_{50}$     | Meta-h        | 0.93 | 0.87  | 0.85  | \u3000 | 0.97   | 0.98  | 0.94  |\n|         | ours        | 0.75 | 0.81  | 0.76  | \u3000 | 0.80   | 0.84  | 0.91  |\n|         | Meta-h + Ours | 0.76 | 0.80  | 0.79  |    | 0.80   | 0.82  | 0.90  |\n\n\nAs we can observe, the combined approach shows better results. The reasons are twofold: First, the added message is not removed after the attack; hence, it keeps affecting victim nodes. Second, the Meta-h attack uses a more advanced scheme to find victim nodes instead of our highest-degree policy.\n\n[4] Z\u00fcgner, D., and S. G\u00fcnnemann. \"Adversarial attacks on graph neural networks via meta learning.< i> ICLR</i>.\" (2019).\n\n[5] Li, Kuan, et al. \"Revisiting graph adversarial attack and defense from a data distribution perspective.\" The Eleventh International Conference on Learning Representations. 2022.\n\n\n## **Q7. The impact of noisy messages after future actions.**\n\nThanks for the question, and sorry for the unclear explanation. For the first question, solving and inserting noisy messages is similar to modifying the victim node\u2019s memory. Since it is specifically solved to forge the victim\u2019s node memory into the expected format, we opted for this injection method due to practical constraints: direct modification of a node\u2019s memory might not be feasible, particularly if the memory is stored server-side and requires specific authorization to alter. **Conversely, generating fake messages through fake comments or users is more feasible and realistic.**\n\n**The noisy message\u2019s impact will not spread with the arrival of the next action since we will remove it after the attack, as we highlighted in Figure 5** in the original paper. In such cases, the fake message itself will no longer be visible and participate in the future updates of this node. For example, we may delete the fake comments or have the fake user unfollow a post after a few days. However, the impact of the noisy message will be injected into the victim node\u2019s memory and then spread to all future updates involving this node\u2019s memory."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476446049,
                "cdate": 1700476446049,
                "tmdate": 1700476446049,
                "mdate": 1700476446049,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FGUxfSr7yZ",
                "forum": "Pmrc0nEvxf",
                "replyto": "Roj4SKmU1B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8086/Reviewer_4nwA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8086/Reviewer_4nwA"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Dear Authors\uff0c\n\nThanks for the detailed responses. Most of my concerns have been addressed. I keep my rating unchanged."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561899766,
                "cdate": 1700561899766,
                "tmdate": 1700561933434,
                "mdate": 1700561933434,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NMBtskJK6r",
            "forum": "Pmrc0nEvxf",
            "replyto": "Pmrc0nEvxf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8086/Reviewer_LiVK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8086/Reviewer_LiVK"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the MemStranding framework, which is designed to launch adversarial attacks on Temporal Graph Neural Networks (TGNNs) by utilizing node memories to generate persistent and propagating adversarial perturbations in dynamic graphs. The authors empirically validate the efficacy of MemStranding in diminishing the performance of TGNN models across a spectrum of tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper stands out for its innovative approach to attacking TGNNs. It identifies the limitations of existing adversarial methods and introduces a novel framework that uses node memories to create persistent and spreading adversarial disturbances in dynamic graphs, an unexplored concept in prior research.\n\n2. The paper is lucidly written and easily comprehensible. It offers clear explanations of the paper's concepts and techniques, ensuring accessibility to a broad readership. \n\n3. This paper makes a significant contribution by introducing a previously unexplored approach to TGNN attacks. The proposed framework is practical and effective, as demonstrated through compelling experimental results in various scenarios. The paper's findings carry vital implications for TGNN model security and advocate for further research in this domain."
                },
                "weaknesses": {
                    "value": "1. The paper lacks a comprehensive discussion of the limitations of the proposed method, including performance variations with different TGNN architectures and graph data characteristics.\n\n2. It is not clear that fakenode is the state-of-the-art attack method. However, the paper only compare the proposed method with fakenode only."
                },
                "questions": {
                    "value": "1. Can you provide more insights into the limitations of the proposed method, such as its sensitivity to the size and density of the graph, the number of target nodes, and the choice of the attack budget?\n\n2. Can you provide more insights into the potential defenses against the proposed method, such as the use of adversarial training, graph regularization, or outlier detection?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8086/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765476760,
            "cdate": 1698765476760,
            "tmdate": 1699637001185,
            "mdate": 1699637001185,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LhR3vyi5C6",
                "forum": "Pmrc0nEvxf",
                "replyto": "NMBtskJK6r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer LiVK (1/4)"
                    },
                    "comment": {
                        "value": "## **Comments**\nWe would like to thank the reviewer for the positive feedback and valuable suggestions. We appreciate the reviewer's acknowledgment that our proposed work is novel and significant. We carefully address all the reviewer\u2019s questions and revise the paper accordingly, including more discussion on our attack\u2019s limitations and its potential defenses. Additionally, we implemented and compared more state-of-the-art GNN attacks to our approach. We hope our response can help clarify the reviewer's questions.\n\n## **Q1. Comparisons with more state-of-the-art adversarial attacks on diverse datasets and TGNN architectures**\n\nThank you for the kind suggestion. We add more comparisons as following.\n\n- #### **Additional Baseline Attacks and TGNN model:** \n\nTo provide more comprehensive comparisons, we compare our work with two more recent state-of-the-art attacks for static GNNs: **TDGIA**[1] and **Meta-Attack-Heuristic (Meta-h)** mentioned in [2]. Additionally, as suggested, we also include the evaluation of our work in conjunction with **ROLAND**[3]. We show the results under the 5% node attack budget in **Table R.5.** The results indicate that our attack can outperform various baselines on more recent TGNN models. \n\n>**Table R.5.:** Accumulated accuracy over different timestamps of the vanilla/attacked TGNNs in WIKI and REDDIT.\n| **Dataset** |       | **WIKI** |       |       |        |   | **REDDIT** |       |       |        |\n|---------|-------|------|-------|-------|--------|---|--------|-------|-------|--------|\n| **Model**   |       | **TGN** | **Jodie** | **Dyrep** | **Roland** |   | **TGN** | **Jodie** | **Dyrep** | **Roland** |\n| Vanilla Acc. |       | 0.93 | 0.87  | 0.85  | 0.94   |   | 0.96   | 0.98  | 0.96  | 0.95   |\n| $t_0$   | FN    | 0.81 | 0.74  | 0.74  | 0.82   |   | 0.84   | 0.83  | 0.84  | 0.83   |\n|         | Meta-h  | 0.9  | 0.83  | 0.81  | 0.85   |   | 0.93   | 0.95  | 0.9   | 0.92   |\n|         | TDGIA | 0.77 | 0.72  | 0.71  | 0.8    |   | 0.74   | 0.8   | 0.81  | 0.74   |\n|         | ours  | 0.89 | 0.88  | 0.85  | 0.87   |   | 0.75   | 0.84  | 0.94  | 0.82   |\n| $t_{25}$  | FN    | 0.92 | 0.87  | 0.85  | 0.94   |   | 0.97   | 0.97  | 0.96  | 0.93   |\n|         | Meta-h  | 0.93 | 0.87  | 0.84  | 0.93   |   | 0.96   | 0.98  | 0.94  | 0.96   |\n|         | TDGIA | 0.93 | 0.81  | 0.84  | 0.92   |   | 0.94   | 0.95  | 0.95  | 0.9    |\n|         | ours  | 0.8  | 0.8   | 0.78  | 0.85   |   | 0.81   | 0.84  | 0.91  | 0.8    |\n| $t_{50}$   | FN    | 0.94 | 0.87  | 0.86  | 0.94   |   | 0.97   | 0.97  | 0.96  | 0.95   |\n|         | Meta-h  | 0.93 | 0.87  | 0.85  | 0.93   |   | 0.97   | 0.98  | 0.94  | 0.95   |\n|         | TDGIA | 0.93 | 0.87  | 0.85  | 0.93   |   | 0.96   | 0.97  | 0.95  | 0.92   |\n|         | ours  | 0.75 | 0.81  | 0.76  | 0.84   |   | 0.8    | 0.84  | 0.91  | 0.8    |\n\nWe also include more results in **Table 1 in Section 5.2 and Table 5&7 in Appendix B.3** in our revised paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476333449,
                "cdate": 1700476333449,
                "tmdate": 1700476333449,
                "mdate": 1700476333449,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mNSqFhLYV9",
                "forum": "Pmrc0nEvxf",
                "replyto": "NMBtskJK6r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer LiVK (2/4)"
                    },
                    "comment": {
                        "value": "- #### **Additional Dataset:**\n\nTo measure if our approach can generalize the larger real-world applications, we evaluate our works on two larger datasets from real-world social media: **Reddit-body** and **Reddit-title** [4].\nCompared to WIKI and REDDIT, which contain approximately 10,000 nodes, these datasets have more nodes (i.e., 35,000 and 54,000 nodes, respectively). The details of the datasets are introduced in **Table 2 in Appendix B.1** in our revised paper.\n\n\nWe show the results under the 5% node attack budget in **Table R.6**. The results indicate that our attack can outperform baselines on larger graphs.\n\n\n>**Table R.6** Accumulated accuracy over different timestamps of the vanilla/attacked TGNNs in Reddit-body and Reddit-title.\n| **Dataset** |       | **Reddit-body** |       |       |        |   | **Reddit-title** |       |       |        |\n|---------|-------|-------------|-------|-------|--------|---|--------------|-------|-------|--------|\n| **Model**   |       | **TGN** | **Jodie** | **Dyrep** | **Roland** |   | **TGN** | **Jodie** | **Dyrep** | **Roland** |\n| Vanilla Acc. |       | 0.9         | 0.87  | 0.9   | 0.88   |   | 0.93         | 0.92  | 0.91  | 0.91   |\n| $t_{0}$     | FN    | 0.76        | 0.82  | 0.77  | 0.79   |   | 0.79         | 0.84  | 0.77  | 0.81   |\n|         | Meta-h  | 0.86        | 0.83  | 0.88  | 0.85   |   | 0.88         | 0.88  | 0.87  | 0.89   |\n|         | TDGIA | 0.72        | 0.81  | 0.74  | 0.76   |   | 0.76         | 0.81  | 0.75  | 0.76   |\n|         | ours  | 0.84        | 0.85  | 0.81  | 0.78   |   | 0.85         | 0.83  | 0.83  | 0.82   |\n| $t_{25}$     | FN    | 0.9         | 0.86  | 0.89  | 0.88   |   | 0.91         | 0.92  | 0.9   | 0.9    |\n|         | Meta-h  | 0.89        | 0.86  | 0.9   | 0.87   |   | 0.93         | 0.92  | 0.9   | 0.91   |\n|         | TDGIA | 0.89        | 0.85  | 0.89  | 0.88   |   | 0.91         | 0.92  | 0.89  | 0.89   |\n|         | ours  | 0.81        | 0.84  | 0.76  | 0.8    |   | 0.79         | 0.83  | 0.77  | 0.82   |\n| $t_{50}$    | FN    | 0.9         | 0.86  | 0.9   | 0.88   |   | 0.93         | 0.92  | 0.9   | 0.9    |\n|         | Meta-h  | 0.9         | 0.86  | 0.9   | 0.88   |   | 0.93         | 0.92  | 0.91  | 0.91   |\n|         | TDGIA | 0.89        | 0.86  | 0.9   | 0.87   |   | 0.92         | 0.92  | 0.9   | 0.9    |\n|         | ours  | 0.77        | 0.82  | 0.76  | 0.77   |   | 0.77         | 0.82  | 0.75  | 0.81   |\n\n\nWe also added the results to **Table 1 in Section 5.2 and Table 3&5 in Appendix B.3** in our revised paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476349358,
                "cdate": 1700476349358,
                "tmdate": 1700476349358,
                "mdate": 1700476349358,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vaA6DojyLJ",
                "forum": "Pmrc0nEvxf",
                "replyto": "NMBtskJK6r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer LiVK (3/4)"
                    },
                    "comment": {
                        "value": "- #### **Different Attack budget and target node density:**\n\nMoreover, we evaluate how our approach works under different numbers of target nodes (i.e., attack budget) and densities of target nodes. Specifically, we investigate the edge prediction accuracies of our attack under different budgets and check if there is a difference between selecting 1% of highest-degree and lowest-degree nodes as victim nodes. We show the results in **Table R.7** and **Table R.8**. The results demonstrate that: (1) Our attack can be effective and scale under different attack budgets. (2) It is helpful to select high-degree victim nodes since lowest-degree nodes lead to worse performances.\n\n> **Table R.7.** Accuracies of our attack under different attack budgets\n|            |     | 1%   | 3%   | 5%   | 7%   | 9%   | 11%  | 13%  | 15%  |\n|------------|-----|------|------|------|------|------|------|------|------|\n| TGN+WIKI   | t0  | 0.89 | 0.84 | 0.81 | 0.82 | 0.74 | 0.77 | 0.76 | 0.71 |\n|            | t25 | 0.83 | 0.81 | 0.8  | 0.75 | 0.76 | 0.7  | 0.69 | 0.64 |\n|            | t50 | 0.85 | 0.8  | 0.75 | 0.72 | 0.73 | 0.7  | 0.67 | 0.63 |\n|            |     |      |      |      |      |      |      |      |      |\n| TGN+REDDIT | t0  | 0.93 | 0.88 | 0.75 | 0.78 | 0.72 | 0.78 | 0.68 | 0.72 |\n|            | t25 | 0.84 | 0.82 | 0.81 | 0.76 | 0.74 | 0.72 | 0.73 | 0.69 |\n|            | t50 | 0.83 | 0.8  | 0.8  | 0.74 | 0.75 | 0.71 | 0.69 | 0.68 |\n\n> **Table R.8.** Number of affected nodes and accumulated accuracies of our attack under selecting 1% highest/lowest-degree victim nodes.\n|                |                    | t0   | t10  | t20  | t30  | t40  | t50  |\n|----------------|--------------------|------|------|------|------|------|------|\n| **Highest Degree** | # of Affected Node | 74   | 176  | 211  | 253  | 285  | 313  |\n|                | Overall Acc.       | 0.91 | 0.88 | 0.86 | 0.86 | 0.85 | 0.85 |\n|                |                    |      |      |      |      |      |      |\n| **Lowest Degree**  | # of Affected Node | 79   | 134  | 159  | 174  | 192  | 211  |\n|                | Overall Acc.       | 0.83 | 0.88 | 0.87 | 0.87 | 0.87 | 0.86 |\n\nWe added more results to **Figure 8 in Section 5.2 and Figure 15 in Appendix B.6** in our revised paper.\n\n[1] Zou, Xu, et al. \"Tdgia: Effective injection attacks on graph neural networks.\" Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 2021.\n\n[2] Li, Kuan, et al. \"Revisiting graph adversarial attack and defense from a data distribution perspective.\" The Eleventh International Conference on Learning Representations. 2022.\n\n[3] You, Jiaxuan, Tianyu Du, and Jure Leskovec. \"ROLAND: graph learning framework for dynamic graphs.\" Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2022.\n\n[4] S. Kumar, W. L. Hamilton, J. Leskovec, and D. Jurafsky. Community interaction and conflict on the web. In Proceedings of the 2018 World Wide Web Conference on World Wide Web, pages 933\u2013943. International World Wide Web Conferences Steering Committee, 2018\n\n\n\n## **Q2. Limitations of our attack under diverse input graphs and models**\n\n\nAccording to the performance variance between different models, we can observe that while our approach can effectively mislead TGN, ROLAND, and DyRep, its effectiveness is less significant on JODIE, which uses differences between a node's current and its last update time to decay the memory. \n\n\nFrom these observations, we deduce that our attack may encounter limitations in two specific scenarios:\n- **Limited Influence of Node Memory on Predictions:** Our attack's effectiveness may be mitigated in situations where the node memory has a relatively minor role in influencing the model's predictions.\n- **Usage of Additional Information in TGNN Models:** The effectiveness may also be constrained when the targeted TGNN model incorporates additional information beyond the node memory for its predictive processes.\nWhile our attack strategy outperforms the baselines, these insights highlight potential limitations under certain model-specific conditions."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476370091,
                "cdate": 1700476370091,
                "tmdate": 1700476370091,
                "mdate": 1700476370091,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WLdsl3ndWr",
                "forum": "Pmrc0nEvxf",
                "replyto": "NMBtskJK6r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer LiVK (4/4)"
                    },
                    "comment": {
                        "value": "## **Q3. Potential defenses against the proposed method**\n\nThank you for your valuable suggestion. To evaluate the effectiveness of our approach under various adversarial defense strategies, we have incorporated the following methods: \n- **Adversarial Training:** In line with the approach detailed in [5], we introduce perturbations to the node memories in TGNN models during the training. We then employ a minimax adversarial training scheme to enhance the robustness of the TGNN model against these perturbations. \n- **Regularization under Empirical Lipschitz Bound:** Following the methodology in [6], we apply a Lipschitz bound during the TGNN training process. This regularization aims to bound the effectiveness of small perturbations, such as adversarial examples. \n\t\n\nThe results are shown in **Table R.9**. The results demonstrate that our attack can remain effective with TGNNs guarded by state-of-the-art defense schemes.\n\n>**Table R.9.** Accumulated accuracy over different timestamps of the vanilla/attacked TGNNs under different defenses\n|              | **Model**   |      | **TGN**  | **Jodie** | **Dyrep** | **Roland** |\n|--------------|---------|------|------|-------|-------|--------|\n| **Adversarial Training**  | Vanilla |      | 0.88 | 0.78  | 0.81  | 0.91   |\n|   | $t_{0}$      | ours | 0.81 | 0.75  | 0.73  | 0.84   |\n|              |         | FN   | 0.77 | 0.73  | 0.66  | 0.80   |\n|              | $t_{25}$     | ours | 0.82 | 0.75  | 0.72  | 0.82   |\n|              |         | FN   | 0.87 | 0.77  | 0.80  | 0.90   |\n|              | $t_{50}$    | ours | 0.80 | 0.76  | 0.70  | 0.80   |\n|              |         | FN   | 0.88 | 0.78  | 0.81  | 0.91   |\n|              |         |      |      |       |       |        |\n| **Regulation**   | Vanilla |      | 0.90 | 0.82  | 0.70  | 0.92   |\n|              | $t_{0}$      | ours | 0.87 | 0.75  | 0.58  | 0.86   |\n|              |         | FN   | 0.84 | 0.74  | 0.60  | 0.84   |\n|              | $t_{25}$     | ours | 0.81 | 0.73  | 0.47  | 0.83   |\n|              |         | FN   | 0.90 | 0.82  | 0.70  | 0.92   |\n|              | $t_{50}$     | ours | 0.81 | 0.76  | 0.48  | 0.81   |\n|              |         | FN   | 0.90 | 0.82  | 0.70  | 0.92   |\n\nWe also added more results to **Figure 6 in Section 5.2 and Figure 12&13 in Appendix B.4** in our revised paper. We also include an introduction of these defenses in **Appendix B.3** in our revised paper for comprehensive understanding.\n\n\nNotably, most robust GCN models, such as RobustGCN, SGCN, GraphSAGE, and TAGCN mentioned in ROLAND [3], are primarily tailored for static graph benchmarks. Given their design constraints, these models are unsuited for TGNN setup with dynamic graph benchmarks and do not offer a viable defense for the TGNN models targeted by our attack. \n\n\nHowever, we expect a potential attack-oriented defense scheme for our attack using memory filtering. **Specifically, a potential defensive approach is to pay less attention to the nodes' memory and rely more on their current input adaptively.** \n\n\nThis scheme stems from the observation that our attacks are less effective on JODIE. One key difference in JODIE is that it decays the node memory based on the time differences between the prediction time and the node's last update time. This mechanism introduces more hints (i.e., time differences) in addition to the memory itself, which cannot be effectively distorted by the attacks and yields some crucial information. For example, a Wikipedia user is less likely to be banned if he/she makes a new post after being inactive for a long while. Therefore, using this non-memory information or current information that does not interact with node memory could effectively hinder adversarial noises. To this end, an intelligent defense mechanism can judiciously filter out the memory and adaptively focus more on non-memory information if the memory is suspicious or potentially noisy. We also include these discussions in **Appendix C** in our revised paper.\n\n\t\n[5] Madry, Aleksander, et al. \"Towards deep learning models resistant to adversarial attacks.\" arXiv preprint arXiv:1706.06083 (2017).\n\n[6] Jia, Yaning, et al. \"Enhancing node-level adversarial defenses by Lipschitz regularization of graph neural networks.\" Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2023."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476386178,
                "cdate": 1700476386178,
                "tmdate": 1700476386178,
                "mdate": 1700476386178,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fenJ9msskn",
                "forum": "Pmrc0nEvxf",
                "replyto": "NMBtskJK6r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer LiVK,\n\nThanks for your time and reviewing efforts! We appreciate your constructive comments and positive feedbacks.\n\nWe provide suggested results and discussions in the authors' response, we hope our responses have answered your questions.  As the deadline for open discussion is soon, please don't hesitate to respond if you have any further concerns.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633497543,
                "cdate": 1700633497543,
                "tmdate": 1700633497543,
                "mdate": 1700633497543,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ucis1VpSXn",
            "forum": "Pmrc0nEvxf",
            "replyto": "Pmrc0nEvxf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8086/Reviewer_aaPY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8086/Reviewer_aaPY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework called MemStranding, which is used to attack Temporal Graph Neural Networks (TGNNs) by leveraging node memories to create adversarial noises in dynamic graphs. The authors provide experimental results to demonstrate the effectiveness of MemStranding in decreasing the performance of TGNN models in various tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Attacking graph under GNNs is promising\n2. The paper discusses real-world scenarios where dynamic graphs are prevalent, which highlights the relevance and importance of the proposed framework.\n3. The authors identify the limitations of existing adversarial attacks on TGNNs and explore the challenges of adapting them to TGNNs within these constraints."
                },
                "weaknesses": {
                    "value": "1. The experiments are weak\n2. The presentations are unclear\n3. The comparisons are weak"
                },
                "questions": {
                    "value": "1. The authors do not provide a comprehensive comparison of MemStranding with other existing adversarial attacks on TGNNs. For example, TIGIA [1] and a lot of methods in surveys [2] propose injective attacks. But the author only compares one the fakenode baseline.\n\n2. The experimental results are limited to small datasets, which may not be sufficient to generalize the effectiveness of MemStranding in real-world applications.\n\n3. It only uses limited TGNNs for evaluations. Recently, researchers have proposed more powerful TGNNs, such as roland [3]\n\n4. It only uses raw TGNN for evaluations without using current GNN defenders for evaluations. Considering current platforms may use GNN defenders instead of raw GNNs, it should explore the effectiveness of attackers under GNN defenders.\n\n5. The paper assumes that the attacker has complete knowledge of the graph structure and node attributes before each time t, which may not be realistic in real-world scenarios.\n\n6. unclear part: Section 4.1 is unclear, which should add more explanations on why coverage state is so important.\n\n[1]Zou, Xu, et al. \"Tdgia: Effective injection attacks on graph neural networks.\" Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 2021.\n\n[2] Sun, Lichao, et al. \"Adversarial attack and defense on graph data: A survey.\" IEEE Transactions on Knowledge and Data Engineering (2022).\n\n[3] You, Jiaxuan, Tianyu Du, and Jure Leskovec. \"ROLAND: graph learning framework for dynamic graphs.\" Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper focuses solely on attacking TGNN models and does not explore the potential defenses against such attacks."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8086/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8086/Reviewer_aaPY",
                        "ICLR.cc/2024/Conference/Submission8086/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8086/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834519261,
            "cdate": 1698834519261,
            "tmdate": 1700643982470,
            "mdate": 1700643982470,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wWLiwAXIVz",
                "forum": "Pmrc0nEvxf",
                "replyto": "ucis1VpSXn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer aaPY (1/5)"
                    },
                    "comment": {
                        "value": "## **Comments:** \nWe appreciate the valuable comments and suggestions from the reviewer. We add more results as suggested by the reviewer, including comparisons with more baseline attacks on more TGNN models and datasets, and evaluate the performance under various adversarial defenses. We clarify the questions and revise the paper carefully. We hope our response can help alleviate the reviewer's concern.\n\n## **Q1~Q3. Comparisons with more baseline attacks on more TGNNs in larger datasets.**\n\nThank you for the kind suggestion.\n\n- #### **Additional Baseline Attacks and TGNN model:** \n\nTo provide more comprehensive comparisons, we compare our work with two more recent state-of-the-art attacks for static GNNs: **TDGIA**[1] and **Meta-Attack-Heuristic (Meta-h)** mentioned in [2]. Additionally, as suggested, we also include the evaluation of our work in conjunction with **ROLAND**[3]. We show the results under the 5% node attack budget in **Table R.1.** The results indicate that our attack can outperform various baselines on more recent TGNN models.\n\n>**Table R.1.:** Accumulated accuracy over different timestamps of the vanilla/attacked TGNNs in WIKI and REDDIT.\n| Dataset |       | Wiki |       |       |        |   | Reddit |       |       |        |\n|---------|-------|------|-------|-------|--------|---|--------|-------|-------|--------|\n| **Model**   |       | **TGN** | **Jodie** | **Dyrep** | **Roland** |   | **TGN** | **Jodie** | **Dyrep** | **Roland** |\n| Vanilla Acc. |       | 0.93 | 0.87  | 0.85  | 0.94   |   | 0.96   | 0.98  | 0.96  | 0.95   |\n| $t_0$| FN    | 0.81 | 0.74  | 0.74  | 0.82   |   | 0.84   | 0.83  | 0.84  | 0.83   |\n|         | Meta-h  | 0.9  | 0.83  | 0.81  | 0.85   |   | 0.93   | 0.95  | 0.9   | 0.92   |\n|         | TDGIA | 0.77 | 0.72  | 0.71  | 0.8    |   | 0.74   | 0.8   | 0.81  | 0.74   |\n|         | ours  | 0.89 | 0.88  | 0.85  | 0.87   |   | 0.75   | 0.84  | 0.94  | 0.82   |\n| $t_{25}$ | FN    | 0.92 | 0.87  | 0.85  | 0.94   |   | 0.97   | 0.97  | 0.96  | 0.93   |\n|         | Meta-h  | 0.93 | 0.87  | 0.84  | 0.93   |   | 0.96   | 0.98  | 0.94  | 0.96   |\n|         | TDGIA | 0.93 | 0.81  | 0.84  | 0.92   |   | 0.94   | 0.95  | 0.95  | 0.9    |\n|         | ours  | 0.8  | 0.8   | 0.78  | 0.85   |   | 0.81   | 0.84  | 0.91  | 0.8    |\n| $t_{50}$  | FN    | 0.94 | 0.87  | 0.86  | 0.94   |   | 0.97   | 0.97  | 0.96  | 0.95   |\n|         | Meta-h  | 0.93 | 0.87  | 0.85  | 0.93   |   | 0.97   | 0.98  | 0.94  | 0.95   |\n|         | TDGIA | 0.93 | 0.87  | 0.85  | 0.93   |   | 0.96   | 0.97  | 0.95  | 0.92   |\n|         | ours  | 0.75 | 0.81  | 0.76  | 0.84   |   | 0.8    | 0.84  | 0.91  | 0.8    |\n\n\nWe also include more results in **Table 1 in Section 5.2 and Table 5&7 in Appendix B.3** in our revised paper. \n\nAdditionally, we would like to mention that our approach is the first attack on TGNNs within the domain of Continuous-Time Dynamic Graphs (CTDG). Existing methodologies and benchmarks in the literature primarily cater to static graphs. Consequently, there are no established baseline approaches or precedents on CTDG-based TGNNs to compare our work against directly. Therefore, we leverage the core mechanisms of these attacks and implement their attacks in the dynamic graph scenarios. More detailed information regarding the baseline attack setups has been added to Appendix B.2 for comprehensive understanding."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476104881,
                "cdate": 1700476104881,
                "tmdate": 1700476218049,
                "mdate": 1700476218049,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yPZwhXG1x6",
                "forum": "Pmrc0nEvxf",
                "replyto": "ucis1VpSXn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer aaPY (2/5)"
                    },
                    "comment": {
                        "value": "- ####  **Additional Dataset:**\n\nTo measure if our approach can generalize the larger real-world applications, we evaluate our works on two larger datasets from real-world social media: **Reddit-body** and **Reddit-title** [4].\nCompared to WIKI and REDDIT, which contain approximately 10,000 nodes, these datasets have more nodes (i.e., 35,000 and 54,000 nodes, respectively). The details of the datasets are introduced in **Table 2 in Appendix B.1** in our revised paper. We show the results under the 5% node attack budget in **Table R.2**. The results indicate that our attack can outperform baselines on larger graphs.\n\n\n>**Table R.2** Accumulated accuracy over different timestamps of the vanilla/attacked TGNNs in Reddit-body and Reddit-title.\n| **Dataset** |       | **Reddit-body** |       |       |        |   | **Reddit-title** |       |       |        |\n|---------|-------|-------------|-------|-------|--------|---|--------------|-------|-------|--------|\n| **Model**   |       | **TGN** | **Jodie** | **Dyrep** | **Roland** |   | **TGN** | **Jodie** | **Dyrep** | **Roland** |\n| Vanilla Acc. |       | 0.9         | 0.87  | 0.9   | 0.88   |   | 0.93         | 0.92  | 0.91  | 0.91   |\n| $t_{0}$     | FN    | 0.76        | 0.82  | 0.77  | 0.79   |   | 0.79         | 0.84  | 0.77  | 0.81   |\n|         | Meta-h  | 0.86        | 0.83  | 0.88  | 0.85   |   | 0.88         | 0.88  | 0.87  | 0.89   |\n|         | TDGIA | 0.72        | 0.81  | 0.74  | 0.76   |   | 0.76         | 0.81  | 0.75  | 0.76   |\n|         | ours  | 0.84        | 0.85  | 0.81  | 0.78   |   | 0.85         | 0.83  | 0.83  | 0.82   |\n| $t_{25}$     | FN    | 0.9         | 0.86  | 0.89  | 0.88   |   | 0.91         | 0.92  | 0.9   | 0.9    |\n|         | Meta-h  | 0.89        | 0.86  | 0.9   | 0.87   |   | 0.93         | 0.92  | 0.9   | 0.91   |\n|         | TDGIA | 0.89        | 0.85  | 0.89  | 0.88   |   | 0.91         | 0.92  | 0.89  | 0.89   |\n|         | ours  | 0.81        | 0.84  | 0.76  | 0.8    |   | 0.79         | 0.83  | 0.77  | 0.82   |\n| $t_{50}$    | FN    | 0.9         | 0.86  | 0.9   | 0.88   |   | 0.93         | 0.92  | 0.9   | 0.9    |\n|         | Meta-h  | 0.9         | 0.86  | 0.9   | 0.88   |   | 0.93         | 0.92  | 0.91  | 0.91   |\n|         | TDGIA | 0.89        | 0.86  | 0.9   | 0.87   |   | 0.92         | 0.92  | 0.9   | 0.9    |\n|         | ours  | 0.77        | 0.82  | 0.76  | 0.77   |   | 0.77         | 0.82  | 0.75  | 0.81   |\n\n\nWe also added the results to **Table 1 in Section 5.2 and Table 3&5 in Appendix B.3** in our revised paper.\n\n[1] Zou, Xu, et al. \"Tdgia: Effective injection attacks on graph neural networks.\" Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 2021.\n\n[2] Li, Kuan, et al. \"Revisiting graph adversarial attack and defense from a data distribution perspective.\" The Eleventh International Conference on Learning Representations. 2022.\n\n[3] You, Jiaxuan, Tianyu Du, and Jure Leskovec. \"ROLAND: graph learning framework for dynamic graphs.\" Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2022.\n\n[4] S. Kumar, W. L. Hamilton, J. Leskovec, and D. Jurafsky. Community interaction and conflict on the web. In Proceedings of the 2018 World Wide Web Conference on World Wide Web, pages 933\u2013943. International World Wide Web Conferences Steering Committee, 2018"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476235275,
                "cdate": 1700476235275,
                "tmdate": 1700476636773,
                "mdate": 1700476636773,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "knw3Q5dyAD",
                "forum": "Pmrc0nEvxf",
                "replyto": "ucis1VpSXn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer aaPY (3/5)"
                    },
                    "comment": {
                        "value": "## **Q4. Evaluation results on adversarial defenses.**\nThank you for your valuable suggestion. To evaluate the effectiveness of our approach under various adversarial defense strategies, we have incorporated the following methods: \n- **Adversarial Training:** In line with the approach detailed in [5], we introduce perturbations to the node memories in TGNN models during the training. We then employ a minimax adversarial training scheme to enhance the robustness of the TGNN model against these perturbations. \n- **Regularization under Empirical Lipschitz Bound:** Following the methodology in [6], we apply a Lipschitz bound during the TGNN training process. This regularization aims to bound the effectiveness of small perturbations, such as adversarial examples. \n\t\nThe results are shown in **Table R.3**. The results demonstrate that our attack can remain effective with TGNNs guarded by state-of-the-art defense schemes.\n\n>**Table R.3.** Accumulated accuracy over different timestamps of the vanilla/attacked TGNNs under different defenses\n|              | **Model**   |      | **TGN**  | **Jodie** | **Dyrep** | **Roland** |\n|--------------|---------|------|------|-------|-------|--------|\n| **Adversarial Training**  | Vanilla |      | 0.88 | 0.78  | 0.81  | 0.91   |\n|   | $t_{0}$      | ours | 0.81 | 0.75  | 0.73  | 0.84   |\n|              |         | FN   | 0.77 | 0.73  | 0.66  | 0.80   |\n|              | $t_{25}$     | ours | 0.82 | 0.75  | 0.72  | 0.82   |\n|              |         | FN   | 0.87 | 0.77  | 0.80  | 0.90   |\n|              | $t_{50}$    | ours | 0.80 | 0.76  | 0.70  | 0.80   |\n|              |         | FN   | 0.88 | 0.78  | 0.81  | 0.91   |\n|              |         |      |      |       |       |        |\n| **Regulation**   | Vanilla |      | 0.90 | 0.82  | 0.70  | 0.92   |\n|              | $t_{0}$      | ours | 0.87 | 0.75  | 0.58  | 0.86   |\n|              |         | FN   | 0.84 | 0.74  | 0.60  | 0.84   |\n|              | $t_{25}$     | ours | 0.81 | 0.73  | 0.47  | 0.83   |\n|              |         | FN   | 0.90 | 0.82  | 0.70  | 0.92   |\n|              | $t_{50}$     | ours | 0.81 | 0.76  | 0.48  | 0.81   |\n|              |         | FN   | 0.90 | 0.82  | 0.70  | 0.92   |\n\nWe also added more results to **Figure 6 in Section 5.2 and Figure 12&13 in Appendix B.4** in our revised paper. We also include an introduction of these defenses in **Appendix B.3** in our revised paper for comprehensive understanding.\n\nNotably, most robust GCN models, such as RobustGCN, SGCN, GraphSAGE, and TAGCN mentioned in ROLAND [3], are primarily tailored for static graph benchmarks. Given their design constraints, these models are unsuited for TGNN setup with dynamic graph benchmarks and do not offer a viable defense for the TGNN models targeted by our attack. \n\t\n[5] Madry, Aleksander, et al. \"Towards deep learning models resistant to adversarial attacks.\" arXiv preprint arXiv:1706.06083 (2017).\n\n[6] Jia, Yaning, et al. \"Enhancing node-level adversarial defenses by Lipschitz regularization of graph neural networks.\" Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476258013,
                "cdate": 1700476258013,
                "tmdate": 1700476258013,
                "mdate": 1700476258013,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q7Re0BjFNj",
                "forum": "Pmrc0nEvxf",
                "replyto": "ucis1VpSXn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer aaPY (4/5)"
                    },
                    "comment": {
                        "value": "## **Q5. Why assume the attacker has complete knowledge of the graph structure and node attributes before each time t?**\nThank you for your insightful question. Our assumption is grounded in practicality for two key reasons:\n- **Observable Historical Graph Structure:** In real-world dynamic graphs, the historical graph structure is typically accessible. For example, data on platforms like Reddit and Wikipedia allows public access, and one can systematically gather all historical information related to targeted posts (i.e., a node in the Reddit graph).\n- **Publicly Accessible Message Embedding Techniques:** Message information is embedded using publicly available methods, such as LIWC [8] or BERT [9]. Additionally, in TGNN prediction frameworks, the node features attributes are initiated from an all-zero state. Thus, by collating messages related to a specific node (like comments linked to a post), we can effortlessly reconstruct the attributes of the target node.\n\n\nOur assumption is aligned with the assumption for attacks in static GNN (our baseline methodologies, i.e., FakeNode[7], TDGIA[1], and Meta-Attack-Heuristic (Meta-h) mentioned in [2]). These attacks uniformly assume that the attacker has complete knowledge of the input graphs, including graph structure and node attributes. Significantly, our method is designed to require only historical information related to the target nodes and, therefore, demands less information compared to the assumptions in previous studies. \n\n\nWe concur that these attacks are less effective when the graph structure and node information are entirely obscured from public view. In scenarios where the data is completely private, we need to adopt system-side attack methodologies [10] to access private data before deploying these attacks. However, such considerations fall outside the scope of both our study and those of our baseline references. \n\n[7] Wang, Xiaoyun, et al. \"Fake node attacks on graph convolutional networks.\" Journal of Computational and Cognitive Engineering 1.4 (2022): 165-173.\n\n[8] Pennebaker, James W., Martha E. Francis, and Roger J. Booth. \"Linguistic inquiry and word count: LIWC 2001.\" Mahway: Lawrence Erlbaum Associates 71.2001 (2001): 2001.\n\n[9] Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" arXiv preprint arXiv:1810.04805 (2018).\n\n[10] Chandra, Sudipta, Soumya Ray, and R. T. Goswami. \"Big data security: survey on frameworks and algorithms.\" 2017 IEEE 7th International Advance Computing Conference (IACC). IEEE, 2017."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476283014,
                "cdate": 1700476283014,
                "tmdate": 1700476283014,
                "mdate": 1700476283014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U9LC2ioSef",
                "forum": "Pmrc0nEvxf",
                "replyto": "ucis1VpSXn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer aaPY (5/5)"
                    },
                    "comment": {
                        "value": "## **Q6. Why is coverage state so important?**\nThank you for pointing out the need for clarity in **Section 4.1** regarding the importance of a node's converge state. The converge state of a node plays a pivotal role in our analysis, primarily due to its impact on the stability of the node's memory. **Figure 4** of our paper illustrates that a node surrounded by similar neighbors tends to fall into a stable converge state after a few updates. The interactions or 'messages' from these neighboring nodes effectively push the node's memory towards this state, subsequently maintaining its stability.\n\n\nIt's important to highlight that although having similar neighbors is necessary for stabilizing the node's memory, it is insufficient. The memory of a node can only achieve stabilization upon reaching its converge state. Therefore, it is significant to forge the node\u2019s noisy memory into the converged state to prevent this converging process from breaking our noise. In other words, if we do not produce a noisy and converged memory state for a target node, the injected noise could be lost during its converging process.\n\n\nTo demonstrate the necessity of the converged state, we show the effectiveness of our attack without ensuring this state. Specifically, we show the similarities between root nodes' initial noisy memories (at the time of the attack) and themselves'/their subsequent neighbors' memories in MemSranding with and w/o converge state in **Table R.4**. If the converge states are not guaranteed, the similarities also suffer drops and achieve much lower similarities in the future. This is because the memories will change before they reach their converged states, making the final converged state different from the original noise ones, indicating that the converged state is essential for persisting noisy memories.\n\n\n>**Table R.4.** The similarities between root nodes' initial noisy memories (at the time of the attack) and themselves'/their subsequent neighbors' memories in MemSranding with and w/o converge state.\n|                     |                  | $t_{1}$   | $t_{3}$   | $t_{5}$   | $t_{7}$   | $t_{9}$   | $t_{10}$  | $t_{11}$  | $t_{13}$  |\n|---------------------|------------------|------|------|------|------|------|------|------|------|\n| **With Converge State** | Root Node        | 0.98 | 0.97 | 0.96 | 0.96 | 0.96 | 0.95 | 0.95 | 0.94 |\n|                     | One-Hop Neighber | 0.52 | 0.76 | 0.84 | 0.88 | 0.90 | 0.91 | 0.92 | 0.93 |\n|                     | Two-Hop Neighber | 0.26 | 0.54 | 0.68 | 0.79 | 0.83 | 0.86 | 0.89 | 0.90 |\n| **W/O Converge State**  | Root Node        | 0.96 | 0.90 | 0.89 | 0.84 | 0.84 | 0.81 | 0.79 | 0.78 |\n|                     | One-Hop Neighber | 0.25 | 0.43 | 0.58 | 0.58 | 0.67 | 0.69 | 0.61 | 0.66 |\n|                     | Two-Hop Neighber | 0.02 | 0.27 | 0.46 | 0.57 | 0.59 | 0.60 | 0.58 | 0.63 |\n\n\nWe also include more results in **Figure 7 in Section 5.2, and Figure 14 in Appendix B.5** in our revised paper. Additionally, we revise our description in **Section 4.1** to emphasize the importance of reaching the converged state."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476301102,
                "cdate": 1700476301102,
                "tmdate": 1700476301102,
                "mdate": 1700476301102,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lWDznbahHE",
                "forum": "Pmrc0nEvxf",
                "replyto": "ucis1VpSXn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer aaPY,\n\nWe would like to sincerely thank you again for your thoughtful suggestions to improve our work.\n\nWe provide additional experimental results and explanations in the authors' response and revision. As the deadline for open discussion is soon, we would sincerely hope to use this opportunity to see if our responses are sufficient and if any concern remains. It will be our great pleasure if you would consider updating your review or score.\n\nThanks again for your time.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633058948,
                "cdate": 1700633058948,
                "tmdate": 1700633058948,
                "mdate": 1700633058948,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P8zFvKuou8",
                "forum": "Pmrc0nEvxf",
                "replyto": "lWDznbahHE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8086/Reviewer_aaPY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8086/Reviewer_aaPY"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Reply"
                    },
                    "comment": {
                        "value": "Thanks for your new results and explanations. Currently, I think this new stuff has addressed some of my concerns, but the defense GNN model used for evaluation is still limited. \n\nAlso, considering the real datasets have millions of nodes, this paper only uses graphs with ~50 thousand nodes, which is still limited. \n\nBased on this rebuttal, I will increase the score accordingly."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643952928,
                "cdate": 1700643952928,
                "tmdate": 1700643952928,
                "mdate": 1700643952928,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IYTqNpXGRd",
                "forum": "Pmrc0nEvxf",
                "replyto": "ucis1VpSXn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8086/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Responses (1/2)"
                    },
                    "comment": {
                        "value": "## Followup: Comparisons with more defending methods and larger datasets.\n\nWe would like to thank you again for your valuable time and suggestions for our responses. \n\n- #### **For limited adversarial defenses.**\n\nWe agree that it is significant to investigate how our attack performs under diverse defending schemes. However, we would like to mention that many defenses for static GNNs/GCNs are not suitable/applicable to our attack scenarios. The main reason is that many adversarial defenses are specialized for static GNN/GCNs. As we mentioned in our earlier responses, many robust GNN models are especially well-tuned and specialized for static GNN setups. For example, TDGIA[1] adopts several robust GNNs that are either fine-tuned for the KDD-CUP or specially designed on top of static GCNs, such as layer-norm-GCN[2], GraphSage[3], RobustGCN[4], SGCN[5], GIN[6], etc. On the one hand, these defending methods rely on special static GCN model architectures (e.g., SGCN, RobustGCN) or static GNN models (e.g., GraphSage, GIN); on the other hand, these special GCN models are not transferable to TGNN tasks.\n\n\nWe also consider some preprocessing defending schemes such as low-rank approximating[7] or data-filtering[8]. However, they are not natively suitable for TGNNs as well. The low-rank approximation [7] assumes that the graphs\u2019 adjacency matrix is directly involved in the computing (like the GCN formula Y=AXW). It adopts Principal Component Analysis (PCA) on the adjacency matrix to remove redundant and non-robust information. However, in TGNN, the dynamic graphs are not represented in the adjacency matrix format but in a sequence of attributed events. Additionally, the adjacency matrix is not directly involved in the TGNN computing. The data-filtering[8][9] targets poison attacks (i.e., perturb training data to break model parameters). However, similar to TDGIA, our attack is an evasion attack, which misleads models by perturbing the input after the training. \n\n\nTo this end, we chose adversarial training and regularization as our defense methods. Compared to the abovementioned methods, these defenses are more commonly adopted and transferable to TGNNs.\n\n\nTo give a more in-depth evaluation, we design a defending method by leveraging the data-filtering concept in GNNGuard[9] for the evasion attack. Specifically, following the insights that only the similar node may provide significant information for prediction, GNNGuard[9] adopts a cosine-similarity-based approach to discount the messages passing between dissimilar nodes. So, we also use the cosine similarities to rank and filter the messages. Specifically, similar to the GNNGuard, we compute the similarities between two nodes. For each node, we normalize the similarities between it and its neighbors, then prune the lower 50% (same as GNNGuard). We show the experiment results in **Table F.1.** \n\n>**Table F.1.** Attack Performance under the GNNGurad\n| **Dataset**  |                    |          | **Wiki** |        | **Reddit** |        |\n|----|--------------------|----------|------|--------|--------|--------|\n| **Model**   |                    |          | **TGN**  | **Roland** | **TGN**    | **Roland** |\n|    Vanilla Acc. |       |          | 0.93 | 0.94   | 0.96   | 0.95   |\n|   After defense Acc. |  |          | 0.92 | 0.91   | 0.94   | 0.9    |\n| | | | | | |\n| 1% | $t_{0}$                 | Ours     | 0.87 | 0.88   | 0.91   | 0.88   |\n|    |                    | FakeNode | 0.87 | 0.81   | 0.9    | 0.84   |\n|    | $t_{25}$                | Ours     | 0.84 | 0.87   | 0.82   | 0.81   |\n|    |                    | FakeNode | 0.9  | 0.91   | 0.93   | 0.9    |\n|    | $t_{50}$                | Ours     | 0.83 | 0.85   | 0.81   | 0.81   |\n|    |                    | FakeNode | 0.92 | 0.91   | 0.94   | 0.9    |\n| | | | | | |\n| 5% | $t_{0}$                 | Ours     | 0.85 | 0.83   | 0.8    | 0.82   |\n|    |                    | FakeNode | 0.82 | 0.86   | 0.82   | 0.81   |\n|    | $t_{25}$               | Ours     | 0.79 | 0.81   | 0.81   | 0.8    |\n|    |                    | FakeNode | 0.91 | 0.91   | 0.92   | 0.9    |\n|    | $t_{50}$                | Ours     | 0.76 | 0.82   | 0.8    | 0.83   |\n|    |                    | FakeNode | 0.92 | 0.91   | 0.93   | 0.9    |"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717260401,
                "cdate": 1700717260401,
                "tmdate": 1700717323024,
                "mdate": 1700717323024,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]