[
    {
        "title": "Learning to Act from Actionless Videos through Dense Correspondences"
    },
    {
        "review": {
            "id": "b1Gl2d3GzV",
            "forum": "Mhb5fpA1T0",
            "replyto": "Mhb5fpA1T0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1395/Reviewer_Hwk5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1395/Reviewer_Hwk5"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a method called Actions from Video Dense Correspondences (AVDC) that learns to perform robotic tasks without ever accessing labels of robot actions. The method first trains a text-conditioned diffusion video prediction model on videos of robot data. It then computes optical flows for adjacent frames of the prediction, and by segmenting out the object of interest and finding a rigid body transformation that best fits the corresponding points, computes an object (manipulation) or robot (navigation) transformation that enables the robot to follow the plan. Experiments are conducted on several environments: simulated meta-world and iTHOR tasks, visual pusher for evaluating learning from a human embodiment, and bridge data/a real Panda arm."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Learning from action-free videos is an important and challenging problem, which could enable data-driven robotics to access scale through sources like Youtube.\n- The proposed method makes sensible design decisions, including the video model and correspondence computation strategy. \n- The evaluation is very thorough in terms of the number of environments considered, and AVDC appears to yield consistent performance gains.\n- The authors have committed to making their video model, which requires significantly fewer resources than prior works, open source. I think this is also a valuable contribution (although less related to the main message of the work).\n- The presentation is quite good, the writing is clear, and main ideas are communicated directly."
                },
                "weaknesses": {
                    "value": "- The most apparent weakness of this work is that not all tasks that a robot might want to solve can be solved by a trajectory of target object poses. For example, it is unclear how to plan a task that would require the robot to use another object as a tool. It is also unclear how deformable objects could be handled. There also may be tasks such as pressing a button on a microwave, which do not involve the microwave moving, but require a particular amount of force to be applied, for which this may not be applicable.\n- The work \u201cZero-Shot Robot Manipulation from Passive Human Videos\u201d by Bharadhwaj et al. presents very similar (although not identical) ideas and is not discussed in the prior work or cited. Could the authors discuss and ideally perform a comparison to some of the ideas in that work?\n- Related to the previous point, the ideas presented in this paper are not entirely novel. However, I believe that this particular combination/instantiation of them, as well as the evaluation and exploration of them that is provided, is a valuable contribution to the community."
                },
                "questions": {
                    "value": "- The video generation performance is seemingly quite good even when very few training trajectories are provided (just 165 videos for Meta-World). Can you comment about how overfitting can be avoided or provide some intuition?\n- The performance of the UniPi baseline is surprisingly poor. Could you please provide an explanation for the common failure modes or visualizations? Same goes for the BC baselines. Is this due to the low amount of data provided, thus causing action prediction models to overfit? If so, would it be possible to report results with greater number of demonstrations (like 50, or even 15, rather than 5)?\n- Are the camera poses that are used for evaluating the policies the same as the ones in the training data? I assume that they are but it would be good to have confirmation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1395/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736029395,
            "cdate": 1698736029395,
            "tmdate": 1699636067198,
            "mdate": 1699636067198,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BI1z58cP8i",
                "forum": "Mhb5fpA1T0",
                "replyto": "b1Gl2d3GzV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1395/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1395/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Hwk5 (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thorough and constructive comments. Please find the response to your questions below.\n\n**Q1** Generalization to more complex tasks.\n> The most apparent weakness of this work is that not all tasks that a robot might want to solve can be solved by a trajectory of target object poses. For example, it is unclear how to plan a task that would require the robot to use another object as a tool. It is also unclear how deformable objects could be handled. There also may be tasks such as pressing a button on a microwave, which do not involve the microwave moving, but require a particular amount of force to be applied, for which this may not be applicable.\n\n**A:** We thank the reviewer for bringing up these points. We will revise the paper to discuss them. \n\nWe would like to clarify that **tasks requiring the use of tools** can be indeed achieved with our proposed method. In particular, our method can achieve this by predicting the movements of a tool and rendering actions that induce such movements. For example, in Meta-World, the hammer task requires using a hammer as a tool.\n\nThe current implementation of our method cannot complete tasks that require interacting with **deformable objects**. One possible extension would be to consider tracking key points (e.g., corners of a piece of cloth) and use the key points to recover robot motion. However, this requires additional knowledge, such as key points or other representations of deformable objects. \n\nAnother limitation of learning-from-passive-video frameworks is the missing of **force information**. Therefore, for forceful manipulation tasks, additional training or models for forceful actions would be needed (but being able to predict that a button should be pushed down would still be useful).\n\n**Q2** Related work by Bharadhwaj et al.\n> The work \"Zero-Shot Robot Manipulation from Passive Human Videos\" by Bharadhwaj et al. presents very similar (although not identical) ideas and is not discussed in the prior work or cited. Could the authors discuss and ideally perform a comparison to some of the ideas in that work?\n\n**A:** We thank the reviewer for suggesting this paper. It is definitely sharing the same spirit and some ideas with our work. We will discuss this paper in the revision in detail. While this paper and our work tackle the problem of learning from passive videos, we notice three differences between our method and Bharadhwaj et al.\n\nFirst, our video prediction model is conditional on a textual description of the task. By contrast, Bharadhwaj et al. used an unconditional generation or goal-image conditioning.\nSecond, we predict an object-centric trajectory given the initial frame; by contrast, Bharadhwaj et al. learned to predict hand poses. While hand poses contain more details of manipulator-object interactions, they may have cross-embodiment transfer issues. Finally, Bharadhwaj et al. directly predict pose trajectories, while ours predicts videos first. In our early experiments, we have found that predicting video frames gives better results than predicting object motion (via prediction optical flows). We posit that this is because video prediction is a dense prediction problem, and convolutional neural networks have been designed for this task; by contrast, optical flows are more sparse for most of the scenes because there are usually only a few objects moving in the scene.\n\nUnfortunately, comparing with Bharadhwaj et al. is not possible because the authors have not yet released their code/models at this moment."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1395/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332847581,
                "cdate": 1700332847581,
                "tmdate": 1700332847581,
                "mdate": 1700332847581,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2VPk29U7Pj",
                "forum": "Mhb5fpA1T0",
                "replyto": "b1Gl2d3GzV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1395/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1395/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Hwk5 (2/3)"
                    },
                    "comment": {
                        "value": "**Q3** Explanation of why overfitting did not happen. \n> The video generation performance is seemingly quite good even when very few training trajectories are provided (just 165 videos for Meta-World). Can you comment about how overfitting can be avoided or provide some intuition?\n\n**A:** In general, we find that predicting trajectories with diffusion has less tendency to overfit than single forward models, as the diffusion model must learn the task of denoising multi-modal samples similar to trajectories of the training distribution (which gives much more supervision than directly learning a single state/action mapping). This can also be validated by comparing a simple BC baseline and Diffusion Policy. Here, we show an extended experiment below: a Diffusion Policy (Chi, Cheng, et al. \"Diffusion policy: Visuomotor policy learning via action diffusion\") with a similar network architecture outperforms the BC baseline.\n\n| method | Door Open | Door Close | Basketball | Shelf Place | Button Press | Button Press Topdown | Faucet Close | Faucet Open | Handle Press |  Hammer | Assembly | Overall |\n| -------------------------------------- | ---------:| ----------:| ----------:| -----------:| ------------:| --------------------:| ------------ | -----------:| ------------:| -------:| --------:| -------:|\n|           BC |        21.3 |         36.0  |            0.0 |           0.0   |           34.7 |                  12.0  |           18.7 |          17.3 |           37.3 |      0.0   |        1.3 |      16.2 |\n| Diffusion Policy                          |      45.3 |       45.3 |        8.0 |         0.0 |         40.0 |                 18.7 | 22.7|**58.7**|21.3|4.0|1.3|24.1|\nAVDC (Full) |**72.0**| **89.3** | **37.3** | **18.7** | **60.0** | **24.0** | **53.3** | 24.0 | **81.3** | **8.0** | **6.7** | **43.1** | \n\nHowever, the task performance of AVDC still surpasses the Diffusion Policy by a large margin. We posit that video prediction is more data-efficient way to learn behavior than behavior cloning-based methods because it only needs to learn to predict object motions (how things should move in a video). By contrast, BC-based methods, including Diffusion Policies, require \"joint\" learning of object motion and how to control robots."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1395/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332995017,
                "cdate": 1700332995017,
                "tmdate": 1700335846470,
                "mdate": 1700335846470,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2p6BHnLRmx",
                "forum": "Mhb5fpA1T0",
                "replyto": "ZbKnSCnz9T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1395/Reviewer_Hwk5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1395/Reviewer_Hwk5"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thank you for your thorough rebuttal and for addressing my questions. I continue to recommend acceptance."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1395/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689152617,
                "cdate": 1700689152617,
                "tmdate": 1700689152617,
                "mdate": 1700689152617,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SwtG9uXKJ6",
            "forum": "Mhb5fpA1T0",
            "replyto": "Mhb5fpA1T0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1395/Reviewer_itAQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1395/Reviewer_itAQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a method for constructing a video-based robot policy, capable of performing diverse tasks across different environments. This approach doesn't require action annotations but uses images for a task-agnostic representation. Text is employed for specifying robot goals. By synthesizing videos to predict robot actions and employing dense correspondences between frames, the model infers actions without explicit training labels. It can leverage the large-scale RGB videos on the internet for training, and use this knowledge for robotic manipulation. The paper showcases the effectiveness of this approach in tabletop manipulation and navigation tasks and also provides an open-source framework for efficient video modeling."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper proposed a new correspondence based method to obtain robot action in forecasted robot videos. It proves that a latent dynamic model is not needed if the forecasted video has good quality.\n2. The authors proposed a new method to generate future videos using a diffusion model, which achieves efficient training. It provides a promising toolbox for the community.\n3. The method is evaluated on two tasks, table-top manipulaion and in-door navigation, demonstrating its effectiveness in different domains.\n4. The paper is well-written and solid."
                },
                "weaknesses": {
                    "value": "1. The selected robot tasks are relatively toy, and the potential of such kind of video prediction method is not evaluated. However, this is not the weakness of this paper, but a common practice for video prediction based robot control."
                },
                "questions": {
                    "value": "1. In the appendix H.1.2, the authors say \"We calculate such direction by extrapolating the line between the grasp point and the\nfirst subgoal more than 10cm away from the grasp\" Should it be push point?\n2. In Sec. H.1.3, \"For the push mode, we re-initialize the gripper as described above \" is not clear. What does the re-initialization refer to?\n3. When learning the diffusion model for the IThor environment, why not apply the adaptable frame sampling technique  in this case?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1395/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820594210,
            "cdate": 1698820594210,
            "tmdate": 1699636067126,
            "mdate": 1699636067126,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4Yf9MwVdwP",
                "forum": "Mhb5fpA1T0",
                "replyto": "SwtG9uXKJ6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1395/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1395/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer itAQ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thorough and constructive comments. Please find the response to your questions below.\n\n**Q1** Grasp point => push point in H.1.2.\n> In the appendix H.1.2, the authors say \"We calculate such direction by extrapolating the line between the grasp point and the first subgoal more than 10cm away from the grasp\" Should it be push point?\n\n**A:** Yes. We thank the reviewer for catching this and will fix it in the revision.\n\n**Q2** Re-initialization in the push mode in H.1.3.\n> In Sec. H.1.3, \"For the push mode, we re-initialize the gripper as described above \"is not clear. What does the re-initialization refer to?\n\n**A:** When replanning is triggered, we re-initialize the gripper by (1) synthesizing a video plan, (2) sampling the grasp and calculating subgoals, and (3) calculating the direction for placing the gripper by extrapolating the line between the grasp point and the first subgoal. Once the re-initialization is done, we can start the robot execution. We will revise the paper to make this re-initialization procedure clear.\n\n**Q3** Why not applying adaptive frame sampling to iTHOR?\n>When learning the diffusion model for the IThor environment, why not apply the adaptable frame sampling technique in this case?\n\n**A:** We thank the reviewer for raising this question. We did not use adaptive sampling because iTHOR uses discrete actions such as moving and rotating for a constant distance or angle. We will clarify this in the paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1395/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332651519,
                "cdate": 1700332651519,
                "tmdate": 1700332651519,
                "mdate": 1700332651519,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WWTeOBq2O3",
            "forum": "Mhb5fpA1T0",
            "replyto": "Mhb5fpA1T0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1395/Reviewer_cxfp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1395/Reviewer_cxfp"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an approach for learning video-based policies in robot manipulation settings. The key benefit of the approach is training on actionless video data across human and robot embodiments. The method, termed Actions from Video Dense Correspondences (AVDC), consists of three stages: (1) diffusion-based video prediction given a text-based goal and starting image, (2) optical flow prediction creating dense correspondences, and (3) executed on a robot platform using off-the-shelf inverse kinematics and motion planners. AVDC uses the ability to project a 3D point onto the image plane both from depth and optical flow to compute the transformation of rigid objects across the predicted video frames. These transformations allow AVDC to infer actions in the environment. Then off-the-shelf robotics primitives can be used to enact the planned trajectory. The approach is benchmarked on the Meta-World and iTHOR simulation platforms and on a real-world robot platform, outperforming the considered baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* The general problem of making use of actionless human video data is of interest and importance to the research community.\n* The problem is well-motivated and the literature review does a good job of contextualizing the paper in prior work.\n* The paper is strong, well-written and easy to follow.\n* The use of geometry to reconstruct the transformation of the predicted objects (stationary camera) or embodiment (moving camera) which can be derived simultaneously from the optical flow and depth camera during deployment is clever. This allows the training data for the video prediction and optical flow not to require depth, with depth only being necessary during deployment. The transformations of either the objects or the embodiment then can be used in conjunction with off-the-shelf inverse kinematics, motion planners, grasp point predictors, etc. This also allows for learning from human videos and then zero-shot deploying to the robot, which is very impressive.\n* The figures are informative and effectively illustrate the benefits of the proposed approach.\n* The experiments consider both simulation and real robot evaluation, as well as an ablation study, demonstrating AVDC's superior performance as compared to the considered baselines and support for AVDC's design choices. In particular, I appreciated the discussion and later the results for why not to directly predict the optical flow without the intermediate step of video prediction.\n* The discussion did a good job of describing the weaknesses and failure modes of the proposed method."
                },
                "weaknesses": {
                    "value": "* The literature review is missing a number of relevant works.\n  * V-PTR: similar high-level motivation of using video-based, prediction-focused pre-training and then action-based finetuning. This should have likely served as a baseline for the proposed method. \n  * [A] Bhateja, Chethan, et al. \"Robotic Offline RL from Internet Videos via Value-Function Pre-Training.\" arXiv preprint arXiv:2309.13041 (2023).\n  * Diffusion policy: diffusion policy has shown very good results in terms of multi-task, low-data regime performance. \n    * [B] Chi, Cheng, et al. \"Diffusion policy: Visuomotor policy learning via action diffusion.\" arXiv preprint arXiv:2303.04137 (2023). \n    * [C] Ha, Huy, Pete Florence, and Shuran Song. \"Scaling up and distilling down: Language-guided robot skill acquisition.\" arXiv preprint arXiv:2307.14535 (2023).\n* In particular, my biggest concern with the paper is the lack of comparison to a strong BC baseline. As the AVDC method uses diffusion to predict images, it seems natural to baseline against a diffusion policy (e.g., [B, C]). R3M is a fairly old representation at this point (e.g., Voltron [D] would be a better representation) and the simple MLP-based BC policy would strongly underperform diffusion policy. This is confirmed by, for example, the very poor baseline performance in Tables 1 and 2. In Sec. 4.3, it is mentioned that since 'R3M is pretrained on robot manipulation tasks ... it might not be suitable for visual navigation tasks'. Something like [E] could be a better baseline here.\n  * [D] Karamcheti, Siddharth, et al. \"Language-driven representation learning for robotics.\" arXiv preprint arXiv:2302.12766 (2023).\n  * [E] Shah, Dhruv, et al. \"ViNT: A Foundation Model for Visual Navigation.\" arXiv preprint arXiv:2306.14846 (2023).\n* In Sec. 4.5 'Results', the paper states that Fig. 10 presents screenshots of robot trajectories, but I believe that is Fig. 9? Fig. 10 shows human predicted trajectories.\n\nSome typos and points of confusion are listed below:\n1. Page 3 - 'Unipi'.\n2. Sec. 4.1:  'compare AVDC to its [variants] that also predict dense correspondence'.\n3. Sec. 4.2: 'maximum number of planning affects' -> 'maximum number of replanning steps affects'.\n\n**Post-rebuttal: Most of my concerns have been addressed! I am raising my score as such."
                },
                "questions": {
                    "value": "1. In the related work, you mention that RL based methods often have to interact with the environment. However, offline RL-based methods avoid this issue (e.g., [A]). What is the downside of such approaches compared to the proposed method?\n2. Was the choice of the factorized spatial-temporal ResNet block ablated?\n3. I did not quite understand in Sec. 3.3 'Predict object-centric motion', what happens to achieve subsequent subgoals after the first grasp-contact point is reached. Do you pick the next one in the subsequent predicted video frame?\n4. In the replanning strategy, why would a smaller robot movement necessarily be indicative of failure? What if the inaccuracy in compounding error results in large, but inaccurate robot movements?\n5. Is there a reason not to use a receding horizon-style replanning strategy as in [B]?\n6. Do you have a sense as to why AVDC (Full) underperformed in the 'btn-press-top' task in Table 1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1395/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1395/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1395/Reviewer_cxfp"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1395/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698985521342,
            "cdate": 1698985521342,
            "tmdate": 1700500130830,
            "mdate": 1700500130830,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jfr7XgcGwJ",
                "forum": "Mhb5fpA1T0",
                "replyto": "WWTeOBq2O3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1395/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1395/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cxfp (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thorough and constructive comments. Please find the response to your questions below.\n\n**Q1** Related work.\n> The literature review is missing a number of relevant works.\n> - V-PTR: similar high-level motivation of using video-based, prediction-focused pre-training and then action-based finetuning. This should have likely served as a baseline for the proposed method.\n>     - [A] Bhateja, Chethan, et al. \"Robotic Offline RL from Internet Videos via Value-Function Pre-Training.\" arXiv preprint arXiv:2309.13041 (2023).\n> - Diffusion policy: diffusion policy has shown very good results in terms of multi-task, low-data regime performance.\n>     - [B] Chi, Cheng, et al. \"Diffusion policy: Visuomotor policy learning via action diffusion.\"\n>     - [C] Ha, Huy, Pete Florence, and Shuran Song. \"Scaling up and distilling down: Language-guided robot skill acquisition.\" \n    \n> In particular, my biggest concern with the paper is the lack of comparison to a strong BC baseline. As the AVDC method uses diffusion to predict images, it seems natural to baseline against a diffusion policy (e.g., [B, C]). R3M is a fairly old representation at this point (e.g., Voltron [D] would be a better representation) and the simple MLP-based BC policy would strongly underperform diffusion policy. This is confirmed by, for example, the very poor baseline performance in Tables 1 and 2. In Sec. 4.3, it is mentioned that since 'R3M is pretrained on robot manipulation tasks ... it might not be suitable for visual navigation tasks'. Something like [E] could be a better baseline here.\n> - [D] Karamcheti, Siddharth, et al. \"Language-driven representation learning for robotics.\" arXiv preprint arXiv:2302.12766 (2023).\n> - [E] Shah, Dhruv, et al. \"ViNT: A Foundation Model for Visual Navigation.\" arXiv preprint arXiv:2306.14846 (2023).\n    \n**A:** We appreciate the reviewer pointing out these relevant works. We will discuss them in the revised paper.\n\n**Video pre-training for robots (V-PTR)**: V-PTR learns from videos (the Ego4D dataset) without action labels by extracting a value function during Pre-Training Phase 1. Then, V-PTR learns a Q-value function during Pre-Training Phase 2, which requires actions. Finally, V-PTR is fine-tuned on downstream tasks, which requires learning by interacting with environments. In contrast, our proposed method requires no action labels or learning with environment interactions. With the differences in problem formulation, we believe V-PTR and our method are not comparable. Furthermore, the V-PTR paper was posted on arXiv on Sep 22, six days before the ICLR 2024 deadline (Sep 28), which makes it difficult for us to include a discussion or comparison to V-PTR by the time we submitted our work.\n\n**Diffusion Policy**: We thank the reviewer for suggesting this method. We have implemented and conducted experiments with the Diffusion Policy on Meta-World tasks. Following the paper, the image observation is encoded with the adapted ResNet-18 with group norm and spatial softmax pooling. For the diffusion backbone model, we experimented with the 1D convolutional FiLM U-net architecture proposed in the paper. The backbone model is adapted to take in task embeddings, i.e., the CLIP-Text task embeddings are concatenated with the observation embeddings. The hyperparameters $T_o$ (the number of past frames used as a condition), $T_p$ (the number of future actions to predict), and $T_a$ (the number of actions to execute before replanning, 0<$T_a$ <=$T_p$), were set to align with the configurations used in most of the paper's experiments. Specifically, we set $(T_o, T_p, T_a) = (2, 16, 8)$. The results are presented in the following table.\n\n| method | Door Open | Door Close | Basketball | Shelf Place | Button Press | Button Press Topdown | Faucet Close | Faucet Open | Handle Press |  Hammer | Assembly | Overall |\n| -------------------------------------- | ---------:| ----------:| ----------:| -----------:| ------------:| --------------------:| ------------ | -----------:| ------------:| -------:| --------:| -------:|\n|           BC |        21.3 |         36.0  |            0.0 |           0.0   |           34.7 |                  12.0  |           18.7 |          17.3 |           37.3 |      0.0   |        1.3 |      16.2 |\n| Diffusion Policy                          |      45.3 |       45.3 |        8.0 |         0.0 |         40.0 |                 18.7 | 22.7|**58.7**|21.3|4.0|1.3|24.1|\nAVDC (Full) |**72.0**| **89.3** | **37.3** | **18.7** | **60.0** | **24.0** | **53.3** | 24.0 | **81.3** | **8.0** | **6.7** | **43.1** | \n\nThe results show that Diffusion Policy outperforms BC, and our method AVDC outperforms Diffusion Policy in 10/11 tasks and the overall performance by a large margin. Note that both BC and Diffusion Policy have access to action labels that are not accessible to our proposed method; therefore, the comparison is unfair. We will include the Diffusion Policy in our revised paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1395/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332295814,
                "cdate": 1700332295814,
                "tmdate": 1700333249221,
                "mdate": 1700333249221,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "McWXhP8cm3",
                "forum": "Mhb5fpA1T0",
                "replyto": "x3ZBFXIeGt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1395/Reviewer_cxfp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1395/Reviewer_cxfp"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the thorough response to my review and the extra experiments! Some of these clarifications would be important to have in the updated paper draft and the extra experiments would make the paper stronger. Feel free to omit the Scaling up and distilling down - I agree it is not relevant here. \n\nDo you have any idea why BC achieves 0% on the basketball task?\n\nAssuming these updates to the paper, I will raise my score. Great work!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1395/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500053666,
                "cdate": 1700500053666,
                "tmdate": 1700500053666,
                "mdate": 1700500053666,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pk855MWulF",
            "forum": "Mhb5fpA1T0",
            "replyto": "Mhb5fpA1T0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1395/Reviewer_QCWJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1395/Reviewer_QCWJ"
            ],
            "content": {
                "summary": {
                    "value": "The goal of this paper is to learn robot policies from action-free video data. The motivation is that there exists a lot of video data, but very little action data. Video prediction methods are often over-dependent on actions but have the benefit of being task agnostic. AVDC aims to solve this challenge by learning a video generation model (via diffusion) on the robot video data. From the generated sequence of images, the optical flow is estimated, which conditioned on some 3D knowledge as well as masks/segmentations of different objects gives an idea of the actions that are taken. The robot actions are then taken. To avoid accumulating errors, AVDC allows for replanning after a few actions. The approach is tested on manipulation (Meta-World) and navigation (iThor) setups, as well as qualitative results on a cross-embodiment visual pusher dataset and some robot arm data. Experiments and ablations find that (1) AVDC outperforms inverse dynamics+video prediction and BC baselines (2) all individual components are important."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper tackles an important problem of learning from action-free videos \n- The method, to my knowledge is novel\n- The approach significantly outperforms baselines on many different tasks \n- The ablations are well analyzed \n- The paper is easy to follow and well written"
                },
                "weaknesses": {
                    "value": "- I think one of the main limitations is the setting: AVDC needs videos of robots performing the task. I believe this is a contrived setting as it is very likely that if video + 3D information is available, then this was a robot demonstration, and one can just collect action data. To me, it is unclear how this approach will scale beyond robot data. \n\n- I am concerned by the reported results for the BC baseline. Due to action data being available, as well as the robot data being in-domain for the task a simple BC or kNN baseline should work very well. There are many cases where the results are < 5% success. This should be addressed. I would be willing to increase my score if this weakness is addressed. \n\n- AVDC relies on object/robot masks - a simple baseline would be to use those as a proxy for the actions. One could get pseudo action labels from the videos and train a policy. \n\n- AVDC assumes that all objects are going to be directly manipulated by the robot directly but this is not the when one object as a tool. \n\n- Navigation approaches have many action free baselines which should be explored as well\n\n- It would be good to see real world experiments\n\n- It would be good to have more of an analysis on the quality of the video prediction model. I suspect it has a"
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1395/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1395/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1395/Reviewer_QCWJ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1395/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699642738055,
            "cdate": 1699642738055,
            "tmdate": 1700675180031,
            "mdate": 1700675180031,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KXVeGysSg7",
                "forum": "Mhb5fpA1T0",
                "replyto": "pk855MWulF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1395/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1395/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QCWJ (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thorough and constructive comments. Please find the response to your questions below.\n\n**Q1** Setting limitations: robot video and 3D.\n> I think one of the main limitations is the setting: AVDC needs videos of robots performing the task. I believe this is a contrived setting as it is very likely that if video + 3D information is available, then this was a robot demonstration, and one can just collect action data. To me, it is unclear how this approach will scale beyond robot data.\n\n**A:** We would like to clarify the following two points:\n- **Our work does not need any 3D information during training**, and therefore, our proposed method learns from only videos with RGB frames. The 3D information is only required during inference for rendering robot actions. \n- **Our work can learn from videos of humans or robots that are different from the hardware setup performing actions.** Specifically, to show the ability of our proposed method to transfer across different embodiments, we have shown successful results with visual pusher experiments in Section 4.4, where the robot learns from *human video* (whose actions are not available). Additionally, in our real-world Franka Emica Panda arm experiment presented in Section 4.5, the robot learns from a large offline dataset from *other robots*, whose actions are not directly usable, and a small number of *human data* without action labels. AVDC only needs videos of an arbitrary agent performing tasks, such as a human or a different robot. This ability to learn from diverse data sources underscores its potential for real-world applications.\n\n**Q2** BC Baseline.\n> I am concerned by the reported results for the BC baseline. Due to action data being available, as well as the robot data being in-domain for the task a simple BC or kNN baseline should work very well. There are many cases where the results are < 5% success. This should be addressed. I would be willing to increase my score if this weakness is addressed.\n\n**A:** Thank you for the suggestion. To validate our BC implementation, we included an additional experiment of training BC with more data. The results are reported below.\n\n|  method-total_demos |   door-open |   door-close |   basketball |   shelf-place |   button-press |   button-press-topdown |   faucet-close |   faucet-open |   handle-press |   hammer |   assembly |   overall |\n|------------|------------:|-------------:|-------------:|--------------:|---------------:|-----------------------:|---------------:|--------------:|---------------:|---------:|-----------:|----------:|\nAVDC-165 |72.0| 89.3 | 37.3 | 18.7 | 60.0 | 24.0 | 53.3 | 24.0 | 81.3 | 8.0 | 6.7 | 43.1 | \n|           BC-165 |        21.3 |         36.0  |            0.0 |           0.0   |           34.7 |                  12.0  |           18.7 |          17.3 |           37.3 |      0.0   |        1.3 |      16.2 |\n|         BC-330|        21.3 |         65.3 |            0.0 |           0.0   |           45.3 |                   21.3 |           44.0 |          29.3 |           29.3 |      2.7 |        0.0   |      23.5 |\n|         BC-660|        61.3 |         72.0 |            0.0 |           1.3 |           77.3 |                   49.3 |           77.3 |          77.3 |           62.7 |     10.7 |        0.0   |      44.5 |\n|        BC-1650|        96.0 |         81.3 |            0.0 |           0.0   |           96.0 |                   85.3 |           93.3 |          94.7 |           86.7 |     12.0 |        0.0   |      58.7 |\n\nThe experiment results above show that plain BC needs 20 videos per view (in total 660 videos with action labels, compared to AVDC trained with 165 videos without action labels) to perform similarly to our method AVDC (43.1% overall). That said, BC needs around four times more videos and action labels than our method, which highlights the efficiency of our proposed method. More importantly, BC still cannot learn most tasks that require grasping (e.g., pick and place, use tools) even with 50 demonstrations per view (BC-1650). \n\nWe would like to emphasize that we included the performance of BC to calibrate the difficulty of the tasks, and BC has access to action labels that are not accessible to our proposed method; therefore, the comparison is not fair."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1395/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700331799484,
                "cdate": 1700331799484,
                "tmdate": 1700331836816,
                "mdate": 1700331836816,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p2PtsJR66f",
                "forum": "Mhb5fpA1T0",
                "replyto": "pk855MWulF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1395/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1395/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QCWJ (2/2)"
                    },
                    "comment": {
                        "value": "**Q3** Baseline that uses pseudo action labels.\n> AVDC relies on object/robot masks - a simple baseline would be to use those as a proxy for the actions. One could get pseudo action labels from the videos and train a policy.\n\n**A:** We thank the reviewer for the suggestion. However, this is not directly applicable to our setting because computing robot actions requires depth, and we do not assume depth information during training, and none of the Bridge and Visual Pusher datasets have depth data. \n\nStill, to provide an idea of the performance of using object masks as a proxy for actions as suggested by the reviewer. We have conducted experiments with the following setting: We trained a model that takes in a segmented object and directly predicts optical flow within the segmentation (without diffusion). Then, we used the procedure as AVDC to calculate actions. The results are presented as follows.\n\n||   door-open |   door-close |   basketball |   shelf-place |   button-press |   button-press-topdown |   faucet-close |   faucet-open |   handle-press |   hammer |   assembly |   overall |\n|---|------------:|-------------:|-------------:|--------------:|---------------:|-----------------------:|---------------:|--------------:|---------------:|---------:|-----------:|----------:|\n|Object mask proxy|1.3|20.0|0.0|0.0|12.0|2.7|25.3|9.3|17.3|2.7|0.0|8.2|\n|AVDC (Full) |72.0| 89.3 | 37.3 | 18.7 | 60.0 | 24.0 | 53.3 | 24.0 | 81.3 | 8.0 | 6.7 | 43.1 |\n\nThe results show that our proposed AVDC outperforms the method that predicts the object masks as a proxy for the actions. We will revise the paper to include this experiment.\n\n**Q4** Not applicable for tasks that require using tools.\n> AVDC assumes that all objects are going to be directly manipulated by the robot directly but this is not the when one object as a tool.\n\n**A:** We would like to clarify that tasks requiring using tools can be indeed achieved with our proposed method. In particular, our method can achieve this by predicting the movements of a tool and rendering actions that induce such movements. For example, in Meta-World, the hammer task requires using a hammer as a tool.\n\n**Q5** Action-free baseline for navigation.\n> Navigation approaches have many action free baselines which should be explored as well\n\n**A:** We thank the reviewer for the suggestion. However, we are unsure what the \"action-free\" navigation baselines mean. Can the reviewer kindly provide specific papers or instruct on conducting additional comparisons?\n\n**Q6** Real-world experiments.\n> It would be good to see real world experiments\n\n**A:** We have shown real-world experiments with a Franka Emika Panda arm in Section 4.5; a further discussion can be found in Appendix H.3. Please also kindly see our videos in the [supplementary website](https://flow-for-action-from-video.github.io/supplements/#realresults).\n\n**Q7** Video prediction quality.\n> It would be good to have more of an analysis on the quality of the video prediction model. I suspect it has a\n\n**A:** We thank the reviewer for the suggestion. To measure the quality of synthesized videos, we provide qualitative results in Figure 5, Figure 7, Figure 8, Figure 10, and Appendix G.4. Also, we quantify the quality of synthesized videos by reporting the MSE between the last frames of synthesized videos and ground truth videos, presented in Appendix E and on the [supplementary website](https://flow-for-action-from-video.github.io/supplements/#vidgencomparison). We believe this sufficiently justifies the quality of the synthesized videos.\n\nSince the reviewer asked for additional analyses, we further quantitatively compared the synthesized videos to the ground truth videos regarding PSNR, SSIM, MSE, and LPIPS. Specifically, we synthesized videos with our trained Meta-World video model given unseen first frames, and compared every synthesized video frame to the corresponding ground truth video frame. We report average PSNR, SSIM, MSE, and LPIPS (AlexNet) scores over 15 videos for each view, totaling 15\\*3\\*11=495 videos being evaluated. Also, we report the scores comparing the last frames of synthesized videos and ground truth videos. The following table summarizes the result.\n\n||PSNR \u2191|SSIM \u2191|MSE \u2193|LPIPS \u2193|\n|---|---:|---:|---:|---:|\n|last frame|25.46|0.8920|0.0050|0.0525|\n|whole video|25.02|0.8847|0.0057|0.0557|\n\nThe results show that our proposed video diffusion model can reliably synthesize videos of task execution."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1395/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700331825232,
                "cdate": 1700331825232,
                "tmdate": 1700331890805,
                "mdate": 1700331890805,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dtuguJrEEU",
                "forum": "Mhb5fpA1T0",
                "replyto": "pk855MWulF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1395/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1395/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback and discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer QCWJ\n\nThank you for reviewing our submission and your valuable feedback. We hope our clarifications on problem settings and new results (BC with more data, pseudo-action labels, and qualitative and quantitative results for video generation) can address your concerns. We are happy to discuss with you and answer any further questions. As the deadline for discussion is approaching, we very much look forward to your feedback.\n\nThanks,\nAuthors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1395/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672917961,
                "cdate": 1700672917961,
                "tmdate": 1700672917961,
                "mdate": 1700672917961,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XcAoFvtxYX",
                "forum": "Mhb5fpA1T0",
                "replyto": "dtuguJrEEU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1395/Reviewer_QCWJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1395/Reviewer_QCWJ"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Dear authors, \n\nThanks for the thorough rebuttal. I have increased my score to 6. \n\nBest regards,\nReview QCWJ"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1395/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675155407,
                "cdate": 1700675155407,
                "tmdate": 1700675155407,
                "mdate": 1700675155407,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]