[
    {
        "title": "Polynomial-based Self-Attention for Table Representation learning"
    },
    {
        "review": {
            "id": "caoeuvHVug",
            "forum": "pk0iUCNVPa",
            "replyto": "pk0iUCNVPa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4760/Reviewer_uHc3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4760/Reviewer_uHc3"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a polynomial-based self-attention layer that enhances the representation performance of existing methods for tabular data. The experiments are thorough and convincing, showing that the proposed layer outperforms state-of-the-art methods. However, the paper lacks a detailed analysis of the computational complexity of the proposed layer and a thorough comparison with other recent approaches. Additionally, it is recommended to improve the presentation of results by adding arrows to the indicators in the charts and to test the scalability of the layer on larger datasets or more complex models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.The paper is well-written and the experiments are thorough and convincing.\n2.The paper proposes a novel self-attention layer that enhances the representation performance of existing methods for tabular data. The experiments show that the proposed layer outperforms state-of-the-art methods."
                },
                "weaknesses": {
                    "value": "Lack of detailed analysis of computational complexity. Inadequate comparison with other recent approaches. Presentation of results could be improved. Unclear scalability to larger datasets or more complex models."
                },
                "questions": {
                    "value": "To further improve the quality of the manuscript, here are several suggestions:\n\n1. The paper does not provide a detailed analysis of the computational complexity of the proposed matrix polynomial-based self-attention layer.\n2. The paper does not provide a thorough comparison of the proposed approach to other methods for addressing the over-smoothing issue in Transformer-based methods for tabular data. While the experiments show that the proposed layer outperforms state-of-the-art methods, it is unclear how the proposed approach compares to other recent approaches in the literature.\n3. It is recommended to add up or down arrows to the indicators in the chart, such as such as Table 2, Table 3, Table 4, and Table 5.\n4. The experiments show that the layer is effective. While data set used in the experiment is small, it is unclear how the layer would scale to larger datasets or more complex models. It is recommended to increase the results of testing on large data sets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4760/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698160052534,
            "cdate": 1698160052534,
            "tmdate": 1699636458429,
            "mdate": 1699636458429,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dSsCKuq7qT",
                "forum": "pk0iUCNVPa",
                "replyto": "caoeuvHVug",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4760/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful feedback and questions on our work. We have revised our manuscript following your comments.\n\n**Q1. Detailed analysis of the computational complexity**\n\nStarting with the self-attention mechanism\u2019s time complexity, the original attention mechanism operates at $\\mathcal{O}(n^2d)$, where $n$ represents the number of tokens and $d$ is the dimension of each token. Our method introduces additional complexity to compute $A^k$ with $k-1$ matrix multiplications, resulting in a time complexity of $\\mathcal{O}(n^2d + (k-1)n^{2.371552})$, where we assume that we use algorithm in [1]. Practically, if $d > (k-1)n^{2.371552}$, the time complexity of CheAtt becomes $\\mathcal{O}(n^2d)$, a condition observed to be met across almost all 10 datasets.\n\n\nAlso, we have updated our manuscript based on your suggestion in red. \n\n**Q2. Thorough comparison of CheAtt to other methods**\n\nTo the best of our knowledge, this study is the first to investigate over-smoothing in transformers applied to tabular data. Unfortunately, this uniqueness made it impossible to compare our work with other methods addressing over-smoothing in Transformer-based tabular data approaches.\n\n**Q3. Add up or down arrows in the chart**\n\nThank you for the suggestion; we have added arrows to the tables. \n\n**Q4. How the layer would scale to larger datasets or more complex models**\n\nWe emphasize that our proposed layer is applicable to any transformer-based model, regardless of its complexity. Scaling up the layer for larger or more complex datasets can be achieved through conventional methods, such as increasing the number of transformer blocks or augmenting the hidden vector size. This scaling process enables the attention matrix to expand, and the Chebyshev polynomial of the attention matrix can express a more complex data distribution. Additionally, we are conducting further experiments on larger datasets and will promptly report the results upon completion.\n\n**References**\n\n[1] Williams et al. New bounds for matrix multiplication: from alpha to omega. In SODA, 2024."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146700740,
                "cdate": 1700146700740,
                "tmdate": 1700146700740,
                "mdate": 1700146700740,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VkikWdKLG4",
                "forum": "pk0iUCNVPa",
                "replyto": "dSsCKuq7qT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4760/Reviewer_uHc3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4760/Reviewer_uHc3"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "My concerns have been well addressed. I will maintain the rating."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470786560,
                "cdate": 1700470786560,
                "tmdate": 1700470786560,
                "mdate": 1700470786560,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "J8HTlHMZSj",
            "forum": "pk0iUCNVPa",
            "replyto": "pk0iUCNVPa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4760/Reviewer_BrLK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4760/Reviewer_BrLK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to improve the self-attention module with the matrix polynomial fashion, in order to deal with the over-smoothing issue in Transformer. The improved Transformer shows advantages in the task of understanding tabular data. The proposed polynomial-based layer, namely CheAtt, enables Transformer performs well with good efficiency due to the less-token nature of tabular data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written. The motivation is clear, and the proposed solution is reasonable. The experiments validate the effectiveness. The inherit issue of computational efficiency of polynomial-based layer is avoided in the task of tabular data understanding. Anyway, as far as I know, this is compatible to the current mainstream accelerating techniques."
                },
                "weaknesses": {
                    "value": "It is better to present more details about the task of tabular data understanding."
                },
                "questions": {
                    "value": "As I have limited experience in dealing with the tabular data, could the authors provide if any existing method tackling the issue of over-smoothing for this task?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4760/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698241518675,
            "cdate": 1698241518675,
            "tmdate": 1699636458347,
            "mdate": 1699636458347,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "91zTYSdc27",
                "forum": "pk0iUCNVPa",
                "replyto": "J8HTlHMZSj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4760/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful feedback and questions on our work. We have revised our manuscript following your comments.\n\n\n**W1. Details of the task of tabular data understanding**\n\nThank you for the comment. In response, we have included additional details about the task in our manuscript.\n\n\n**Q1. Existing research addressing the over-smoothing issue**\n\nTo the best of our knowledge, our study is the first to address the issue of over-smoothing specifically for tabular data. The mitigation of over-smoothing for GCN has been a long-standing research area [1, 2]. In specific, GREAD [2] is our special case, where Equation (7) in our main paper with $k=2$, $w_0=1$, and $w_1=-1$. Moreover, research to alleviate over-smoothing and feature collapse in ViT (Vision Transformer) has recently become active [3].\n\n\n**References**\n\n[1] Rusch et al. A survey on oversmoothing in graph neural networks. arXiv preprint arXiv:2303.10993, 2023.\n\n[2] Choi et al. GREAD: Graph neural reaction-diffusion networks. In ICML, 2023.\n\n[3] Gong et al. Vision transformers with patch diversification. arXiv preprint arXiv:2104.12753, 2021."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146631443,
                "cdate": 1700146631443,
                "tmdate": 1700146631443,
                "mdate": 1700146631443,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G2BPI8q7ZQ",
                "forum": "pk0iUCNVPa",
                "replyto": "91zTYSdc27",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4760/Reviewer_BrLK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4760/Reviewer_BrLK"
                ],
                "content": {
                    "title": {
                        "value": "Feedback well received."
                    },
                    "comment": {
                        "value": "My concerns are well addressed. I will remain the rating."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700184789704,
                "cdate": 1700184789704,
                "tmdate": 1700184789704,
                "mdate": 1700184789704,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "140EahW61K",
            "forum": "pk0iUCNVPa",
            "replyto": "pk0iUCNVPa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4760/Reviewer_gFjL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4760/Reviewer_gFjL"
            ],
            "content": {
                "summary": {
                    "value": "In order to solve the over smoothing issue caused by the self-attention layer when applying a transformer to tabular data, this paper proposes Chebyshev polynomial-based self-attention. Firstly, inspired by graph signal processing, this paper considers the self-attention mechanism as a graph filter in the form of matrix polynomials and then uses finite degree Chebyshev polynomials to approximate the graph filter based on the PageRank algorithm. Experiments show that the method can effectively improve the performance of the base model without a significant increase in computation, and effectively alleviate the oversmoothing problems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1)New ideas: This paper introduces the study of self-attention in the field of tabular data, effectively solving the oversmoothing problem.\n\n2)Inspired new approaches: Inspired by graph signal processing and the PageRank algorithm, this paper utilizes matrix polynomials for optimizing the self-attention mechanism and uses Chebyshev polynomials to stabilize the training of coefficients.\n\n3)Better experimental results: Experiments show that the base models, when combined with the approach in this paper, exhibit significant improvements in performance in downstream tasks such as classification and regression."
                },
                "weaknesses": {
                    "value": "1\uff09The motivation for the paper is not adequately supported by theory. The paper mentions that better flexibility and customization can be achieved by considering the self-attention mechanism as a graph filter in graph signal processing, but does not cite enough papers or theorems to fully convince this point. In addition, when proving that the self-attention matrix conforms to the three properties of the transition matrix required by the convergence of pagerank algorithm, the authors only make a rough qualitative analysis but do not carry out a more sufficient and detailed analysis and relevant theoretical or experimental proof. Since the self-attention matrix in the transformer is unpredictable, once it does not meet the corresponding requirements, the self-attention based on matrix polynomials will not be able to approximate the graph filter, and thus will not be able to realize the expected results. Therefore, I suggest the authors to provide more details in this regard.\n \n \n2\uff09Some experimental data with excessive errors will interfere with the experimental results. In the experimental part, the error range of individual experimental results is too large relative to other data. For example, 70.9\u00b113.90 in Table 2 and 58.1\u00b124.32 in Table 3. When these margins of error are taken into account, it becomes a question which method yields the best experimental results. This may potentially interfere with the fairness of comparisons between different methods, thereby affecting the correctness of experimental results.\n \n \n3\uff09The applicability of CheAtt should be further discussed. The paper mentioned that the effect of CheAtt is very dependent on the quality of the base model. As can be seen from Table 1 and Table 4, in TabTransformer and MET, the effect of CheAtt is outstanding, but there is almost no improvement in SAINT. They are all table representation methods based on transformer, and the original performance of SAINT is the best among the three. So what exactly does the \"quality of the base model\" mentioned in the paper refer to? According to the author's analysis, self-attention based on Chebyshev polynomials can improve the flexibility of the self-attention mechanism. This improvement should not be strongly related to the base model, so do the experimental results mean that CheAtt is not applicable in many situations? I suggest that the authors conduct further analysis in this area.\n \n \n4\uff09The complexity of CheAtt still needs further discussion. First, the data in Table 5 are all in the range of a few milliseconds, does it refer to the time to generate output after the model training is completed? If so, this does not take into account the large number of matrix multiplication operations required during model training. In addition, it is meaningless to only compare the absolute time difference, and it is more convincing to compare the relative time consumption. It can be seen that in Phishing dataset, the additional time spent can exceed up to 40% of the original, which is a huge and unacceptable increase. Another question is why in the MET+CheAtt method and Phishing dataset, the time after using CheAtt is reduced (from 2.7538 to 2.4625). Is this a clerical error or real experimental data? I recommend the authors to perform a more comprehensive analysis and more realistic experiments in terms of computational complexity."
                },
                "questions": {
                    "value": "Please refer to the weakness above. I combined my questions with the weakness presentation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4760/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807196976,
            "cdate": 1698807196976,
            "tmdate": 1699636458276,
            "mdate": 1699636458276,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R7GjCZZF45",
                "forum": "pk0iUCNVPa",
                "replyto": "140EahW61K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4760/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful feedback on our work. Following your comments, we have revised our manuscript. \n\n**W1. To include more citations, theorems, and a more rigorous analysis to substantiate the paper's motivation and claims**\n\nThe self-attention matrix and the PageRank matrix share one key common characteristic that their adjacency matrices are fully connected with many small values. PageRank uses a matrix of $(1-\\alpha) \\mathbf{A} + \\alpha\\mathbf{\\frac{1}{N}}$, where $\\mathbf{A}$ is an adjacency matrix, $N$ is the number of nodes, and $\\alpha$ is a damping factor, and therefore, all elements have non-zero (small) values. (Please refer to Section 3.2 PageRank and Equation (5) for details.) In the self-attention matrix, this is also the case since Transformers use the softmax function. Because of this characteristic, in addition, PageRank converges and so does our method.\n\nAs you said, the self-attention matrix is unpredictable. As long as the self-attention matrix is fully connected, however, our method is correct (since we rely on the PageRank theory). It is practically unbelievable that the softmax produces zeros although some values are small. Note that in PageRank, small values are also used when $N$ is very large, i.e., a web-scale graph of billions of nodes.\n\nThe first and second conditions shown in the paper are obvious. For the last condition, we refer to [1]. An irreducible chain, as defined in [1], has a period that is common to all states. The irreducible chain will be called $\\textit{\\textbf{aperiodic}}$ if all states have a period of 1. This implies that a Markov chain is aperiodic if there is at least one self-loop. As discussed earlier, the self-attention matrix has non-negative values for all elements, including diagonal elements, making the self-attention matrix aperiodic.\n\n\nWe added a section in Appendix in red regarding this point.\n\n\n**W2. Results with excessive errors**\n\nFor reproducibility, we repeated all experiments 5 times and reported the mean and standard deviation. We used the same seed numbers for all experiments for fair comparison. The two scores you mentioned have one or two outliers in the set. In Table 2, 70.9\u00b113.90 is the mean and standard deviation of 73.58, 78.07, 79.44, 76.87, **46.30**. In Table 3, 58.1\u00b124.32 is the mean and standard deviation of 77.94, **25.98**, 78.17, 77.56, **30.72**. Among those 5 repetitions, one or two scores are unusually low, attributed to the unstable training of AE. \n\n\n**W3. Improvement on SAINT**\n\nWe are now testing more hyperparameters for SAINT+CheAtt since as can be seen in Appendix C.3 Table 8, we have searched only 3 hyperparameters recommended in its original paper even though SAINT has a number of hyperparameters to search. We will provide the results as soon as the ongoing experiments are completed. We will also update our paper one more time afterward.\n\n**Reference**\n\n[1] David A. et al. Markov chains and mixing times. Vol. 107. In American Mathematical Soc., 2017."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146831680,
                "cdate": 1700146831680,
                "tmdate": 1700146831680,
                "mdate": 1700146831680,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NeGjROxxgb",
                "forum": "pk0iUCNVPa",
                "replyto": "140EahW61K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4760/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (2/2)"
                    },
                    "comment": {
                        "value": "**W4. Relative time consumption and more comprehensive analysis are needed**\n\nWe have incorporated your suggestions into our manuscript, and we kindly ask you to refer to the updated version.\nTo begin, we have discussed computational complexity in detail in our revised manuscript, marked in red. Originally, the attention mechanism operated at $\\mathcal{O}(n^2d)$, where $n$ represents the number of tokens, and $d$ is the dimension of each token. Our method introduces additional complexity to compute $A^k$ with $k-1$ matrix multiplications, resulting in a time complexity of $\\mathcal{O}(n^2d + (k-1)n^{2.371552})$, assuming the use of the algorithm in [2]. In practical terms, if $d > (k-1)n^{2.371552}$, the time complexity of CheAtt becomes $\\mathcal{O}(n^2d)$, a condition observed across almost all 10 datasets.\n\n\nSecondly, we originally reported the absolute wall clock time to emphasize efficiency. However, we also acknowledge the importance of relative time. Therefore, in Table 5, we now present both absolute and relative time consumption. Additionally, we have updated the increase in inference time to the time taken to infer 1,000 samples. We have also included a summary of training time in Table 5, representing the average time for 5 epochs of training, along with reporting the relative time. For training time, we train the model for 5 epochs and average the time. The updated table is presented below.\n\n\nLastly, we apologize for any confusion, but the reported reduction in time after using CheAtt on Phishing was a typo. This has been corrected in our updated paper. Thank you for bringing it to our attention. \n\nBelow is the updated Table 5:\n\n|                  | Training time (per epoch)           | Inference time (for 1,000 samples)           |\n|------------------|--------------------------|---------------------------|\n| TabTransformer   | 3.07s                    | 5.42ms                    |\n| TabTrans.+CheAtt | 3.57s ($\\uparrow$20.32%) | 6.26ms ($\\uparrow$17.27%) |\n| SAINT            | 4.34s                    | 5.25ms                    |\n| SAINT+CheAtt     | 5.27s ($\\uparrow$18.91%) | 6.68ms ($\\uparrow$28.95%) |\n| MET              | 2.68s                    | 2.67ms                    |\n| MET+CheAtt       | 3.34s ($\\uparrow$23.56%) | 3.47ms ($\\uparrow$31.09%) |\n\n**Reference**\n\n[2] Williams et al. \"New bounds for matrix multiplication: from alpha to omega.\" In SODA, 2024."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146931862,
                "cdate": 1700146931862,
                "tmdate": 1700146931862,
                "mdate": 1700146931862,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Km2B17MYGg",
                "forum": "pk0iUCNVPa",
                "replyto": "140EahW61K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4760/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Final experimental result"
                    },
                    "comment": {
                        "value": "Dear reviewer gFjL, \n\nThank you for waiting for the final result. We have finished our additional experiments. To be more specific, we have searched for more hyperparameters for SAINT+CheAtt. The summarized result is as follows:\n\n|                   | TabTransf. | SAINT | MET   |\n|-------------------|------------|-------|-------|\n| Base model        | 77.5       | 84.5  | 79.4  |\n| Base model+CheAtt | 84.2       | 85.1  | 83.1  |\n| Improvement       | 8.65%      | 0.64% | 4.66% |\n\nThe improvement of SAINT after applying CheAtt was 0.01% before we searched for more hyperparameters. After additional experiments, we achieved an improvement of 0.64%.\n\nRegarding the \"quality of the base model,\" our initial intention was to emphasize the fact that the performance after applying CheAtt is proportional to the original performance of the base model, irrespective of CheAtt's performance. However, recognizing the potential for confusion, we have opted to remove this statement from the text.\n\nIn addition, we have uploaded our new manuscript highlighting changed results in blue. Please refer to our revised manuscript following your comments."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658651412,
                "cdate": 1700658651412,
                "tmdate": 1700658674030,
                "mdate": 1700658674030,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]