[
    {
        "title": "Ring-A-Bell! How Reliable are Concept Removal Methods For Diffusion Models?"
    },
    {
        "review": {
            "id": "eZiNyIhuHz",
            "forum": "lm7MRcsFiS",
            "replyto": "lm7MRcsFiS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5834/Reviewer_nnoZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5834/Reviewer_nnoZ"
            ],
            "content": {
                "summary": {
                    "value": "This work proposed Ring-A-Bell, a model-agnostic red-teaming tool for T2I diffusion models, which serves as a prompt-based concept testing framework that generates problematic prompts to red-team T2I diffusion models with safety mechanisms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall this paper proposed a practical and intersting offline method in generating problematic prompts for 'safe models'. The experiments are very convincing and concrete."
                },
                "weaknesses": {
                    "value": "Please see the questions."
                },
                "questions": {
                    "value": "There are a bunch of notation issues. I list some of them below:\n1. What is $\\rho$ in (2)? I cannot find it in the main paper.\n2. What is the training parameter of (2)? Is it $\\widetilde c$?\n3. Are there brackets in (3)\n4. Should $\\tilde{\\mathbf{P}}_{cont}$ be a function of $c$ or $\\hat c$?\n\nQuestion about experiments:\n1. How to tell whether the percentage of nudity is greater than 50%? The output propobility? Please be rigorous."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5834/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5834/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5834/Reviewer_nnoZ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5834/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698631619265,
            "cdate": 1698631619265,
            "tmdate": 1699636616123,
            "mdate": 1699636616123,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JyRRImwAIl",
                "forum": "lm7MRcsFiS",
                "replyto": "eZiNyIhuHz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer nnoZ"
                    },
                    "comment": {
                        "value": "We appreciate the review for providing feedback such as \u201c*The experiments are very convincing and concrete...*\u201d. We are content that the reviewer shares common concerns on the safety of T2I diffusion models. We further address the reviewer\u2019s comment in the following.\n\n&nbsp;\n\n### Notation Issues\n----\n\n\nThanks for pointing out the crucial point, we will improve the notation in the revised version, marked in blue. Furthermore, we will include a notation table in Appendix G for the reviewer\u2019s reference. Below we will list the responses regarding notations in a pointwise manner.\n* $\\rho$ denotes the weight of the loss between the original and the modified model.\n* Indeed, the training objective is to search for an adversarial concept $\\tilde{c}$ such that the loss is minimized, i.e., $\\tilde{c} := \\arg\\min_{\\tilde{c}} L_{white}$.\n* There should be brackets in (3) as we are extracting the concept information by contrasting two semantically similar prompts. \n* Since we have no true latent vector of $c$, when producing $\\mathbf{\\tilde{P}}_{cont}$ we instead opt for the empirical representation $\\hat{c}$ for attack. Therefore, it is a function of $\\hat{c}$.\n\n&nbsp;\n\n### Nudity Percentage of I2P Prompts\n----\n\nAccording to the dataset description [R1], the percentage of nudity is an attribute in the I2P dataset, which is the percentage of images depicting explicit nudity as per the NudeNet classifier/detector out of 10 generated images using Stable Diffusion. \n\n&nbsp;\n\n[R1]: Artificial Intelligence & Machine Learning Lab at TU Darmstadt. \u201cAIML-Tuda/I2P \u00b7 Datasets at Hugging Face.\u201d AIML-TUDA/I2p \u00b7 Datasets at Hugging Face, 20 Nov. 2022, huggingface.co/datasets/AIML-TUDA/i2p."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5834/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700456201990,
                "cdate": 1700456201990,
                "tmdate": 1700456201990,
                "mdate": 1700456201990,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G8fGRsvYq3",
                "forum": "lm7MRcsFiS",
                "replyto": "eZiNyIhuHz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to reviewer nnoZ\u2019s feedback"
                    },
                    "comment": {
                        "value": "&nbsp;\n\nWe appreciate the positive feedback and constructive comments from reviewer nnoZ in the initial reviews. As the discussion deadline nears, we haven't received additional feedback on our responses. We intend to use OpenReview's interactive feature to engage in discussions with the reviewer, confident that our response adequately addresses your concerns. Specifically, we've organized our response according to the reviewer\u2019s suggestion as the following:\n\n* Notation Issues \n* Explanation of Nudity Percentage of I2P Prompts\n\nWe hope our responses convince the reviewer about the merits of this work. If the reviewer has any other suggestions or comments, please don't hesitate to let us know!\n\nBest Regards, \\\nAuthors of Ring-A-Bell"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5834/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655293630,
                "cdate": 1700655293630,
                "tmdate": 1700655293630,
                "mdate": 1700655293630,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WmXFRT04ZC",
            "forum": "lm7MRcsFiS",
            "replyto": "lm7MRcsFiS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5834/Reviewer_nmRw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5834/Reviewer_nmRw"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the safety of text-to-image models. The paper proposes a model-agnostic attack to evade safety mechanisms and generate sensitive and inappropriate images. The proposed work is evaluated on online services to explore their safety risks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper investigates red-teaming text-to-image models, which is a critical topic for generative AI safety.\n2. The proposed method is validated on four T2I online services.\n3. The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The model-agnostic design of the proposed framework is not convincing. The entire design is based on an offline CLIP model and is irrelevant to the online services. This design implicitly assumes that the framework that applies to the offline CLIP model can be transferable and effective for online services. What if the online services use a more robust text encoder? In addition, one of the contributions, claimed by the paper, is that Ring-A-Bell is \u201cbased solely on either the CLIP model or general text encoders.\u201d However, in the evaluation, only the CLIP model is evaluated. It would be great to see if the proposed work can be extended to other and more recent text encoders.\n2. The paper only compares the proposed framework with QF-Attack, which is insufficient. Many recent works are encouraged to be investigated [1-4]. In addition, although P4D is designed for offline attacks, it would be great to consider P4D as a baseline to compare the performance gap between online and offline attacks.\n3. The paper aims to evade the safety mechanism of online diffusion models. However, the paper only considers concept removal defenses. For an online service, an easy and effective way is to develop a detector to identify inappropriate images. For example, the service provider could build a detector (e.g., NudeNet detector used in the evaluation) to detect nudity in the images. The proposed framework that mainly focuses on the text domain may not be effective. \n\n[1] Qu, Yiting, Xinyue Shen, Xinlei He, Michael Backes, Savvas Zannettou, and Yang Zhang. \"Unsafe diffusion: On the generation of unsafe images and hateful memes from text-to-image models.\" ACM CCS 2023.\n[2] Mehrabi, Ninareh, Palash Goyal, Christophe Dupuy, Qian Hu, Shalini Ghosh, Richard Zemel, Kai-Wei Chang, Aram Galstyan, and Rahul Gupta. \"Flirt: Feedback loop in-context red teaming.\" arXiv preprint arXiv:2308.04265 (2023).\n[3] Rando, Javier, Daniel Paleka, David Lindner, Lennart Heim, and Florian Tram\u00e8r. \"Red-teaming the stable diffusion safety filter.\" arXiv preprint arXiv:2210.04610 (2022).\n[4] Yang, Yuchen, Bo Hui, Haolin Yuan, Neil Gong, and Yinzhi Cao. \"SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models' Safety Filters.\" arXiv preprint arXiv:2305.12082 (2023)."
                },
                "questions": {
                    "value": "Please clarify the model-agnostic design and explain why it is effective for online services."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5834/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727783050,
            "cdate": 1698727783050,
            "tmdate": 1699636616014,
            "mdate": 1699636616014,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ia0ndrAzv4",
                "forum": "lm7MRcsFiS",
                "replyto": "WmXFRT04ZC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer nmRw -- Part I"
                    },
                    "comment": {
                        "value": "Thanks for your genuine appreciation of the clarity and precious reviews of our work! We are delighted to receive a comment denoting that \u201c*The paper investigates red-teaming text-to-image models, which is a critical topic for generative AI safety.*\u201d Please see the response below as we address your comments.\n\n&nbsp;\n\n\n### Transferability and Model-Agnostic Design\n---\nWe thank the reviewer for raising the question. Indeed we\u2019ve attempted to expand Ring-A-Bell to other possible text encoders such as BERT and T5. However, there are certain critical differences that hinder such encoders from achieving comparable performance against the CLIP-Based Ring-A-Bell. The main reason lies in that these text encoders specialize **in the text domain and construct respective embedding space, while the embedding space for CLIP is much more aligned from the vision-language aspect.** Secondly, the hard prompt provided by Ring-A-Bell starts from an original soft prompt specific to CLIP with further discrete optimization. This model-specific conversion could hinder the generated prompt to transfer under various text encoder settings.\n\nNevertheless, we note that while the transferability is limited, in practice, **there is no reason to restrain the attacker to only use one text encoder such as CLIP.** We do not consider the differences in transferability as a weakness. **As long as the attacker can find one prompt to sabotage the target model, the attack is considered successful.** Therefore, the more text encoders the attacker can use, the more diverse prompts the attacker can create to test the target model.\n\nMoreover,  we note that most online T2I services would reveal not all but certain information about the model architecture or the underlying concept. This allows us to obtain the corresponding text encoder information for Ring-A-Bell to perform concept extraction. On the other hand, Ring-A-Bell could as well actively extract copies of certain concepts with possible text encoder candidates and apply the discrete optimization to acquire the respective hard prompt for evaluation. \n\n**Under this aspect, as long as a single problematic prompt from any text encoder has succeeded, this signifies the security breach of the corresponding of T2I models and its subsequent risk. Therefore we argue that the transferability of Ring-A-Bell does not hinder its potential in seeking possible risk of T2I models.**\n\nAs for the model-agnostic design, we note that since most of the concept removal methods fine-tune the diffusion model itself and simply fix the text encoder, the concept might not be certifiably removed from the diffusion model. In fact, one of the recent studies demonstrated through causal mediation analysis that the **text encoder has a certain extent of participation in generating the images** [R8]. Therefore, we postulate that by **forming a holistic empirical representation $\\hat{c}$** we are able to leverage the remnant part of the concept, using the knowledge of text encoders, to produce inappropriate images. \n\n&nbsp;\n\n### Concept Removal and Beyond\n---\nAs for the comment of \u201conly evaluating the concept removal methods\u201d, we believe that the reviewer has confused some of our experiment settings in the evaluation. We here clarify that since Ring-A-Bell requires only black-box access to the T2I diffusion model, the attack applies to both concept removal methods and online services with safety filters. Particularly, we note that current online services have a content policy that includes safety measures [R1, R2, R3], some possessing both input and output detection. Therefore, evaluating the online T2I services is equivalent to circumventing the safety filtering implemented. \n\nTo further elaborate, our experiment evaluated three settings. The first one is naturally the online T2I service that **employs safety filtering**, evaluated in Section 4.1. Secondly, we evaluated existing concept removal methods **without safety filters** to demonstrate the risk and unreliability of such methods. Lastly, as a simple defense and simulation of possible scenarios, we evaluated the setting where concept removal is used in tandem **with safety filters** to show the efficacy of Ring-A-Bell in Section 4.2.\n\n&nbsp;\n\n[R1] Ultimate guide to DALL\u00b7E 2: how to use it & how to get access. (2022, July 1). DALL\u00b7Ery GALL\u00b7Ery. Retrieved November 19, 2023, from https://dallery.gallery/dall-e-ai-guide-faq/ \\\n[R2] Midjourney community guidelines. (n.d.). Midjourney Documentation. Retrieved November 19, 2023, from https://docs.midjourney.com/docs/community-guidelines \\\n[R3] Frequently Asked Questions. (n.d.). Runway Help Center. Retrieved November 19, 2023, from https://help.runwayml.com/hc/en-us/categories/21663959852435-Frequently-Asked-Questions \\\n[R8] Basu, Samyadeep, et al. \"Localizing and Editing Knowledge in Text-to-Image Generative Models.\" arXiv preprint arXiv:2310.13730 (2023)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5834/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459985754,
                "cdate": 1700459985754,
                "tmdate": 1700460198403,
                "mdate": 1700460198403,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gZhIrIjnx2",
                "forum": "lm7MRcsFiS",
                "replyto": "WmXFRT04ZC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer nmRw -- Part II"
                    },
                    "comment": {
                        "value": "&nbsp;\n\n### Investigation with Related Works -- Part I\n---\nWe appreciate the reviewer for bringing up related work for discussion and investigation. Furthermore, we note that some references [R4, R6] have been discussed in Appendix B. Nevertheless, in the following, we would discuss the related work in order. Meanwhile, we will include the discussion in Appendix M for reviewer's reference.\n\nFirstly, [R4] evaluates the safety of T2I models in an exploratory manner. By manually collecting unsafe prompts from online forums, the authors examine the risk of T2I models generating inappropriate images using these collected prompts. On the other hand, the authors also trained a customized safety filter that superseded most filters deployed in online T2I services. Meanwhile, they take a step forward by aiming to fine-tune T2I models such that the model could generate hateful memes, a specific type of unsafe content in images. **While we appreciate the exploratory analysis of [R4], we note that this differs from our direction as the manually collected unsafe prompts cannot scale to provide an overall examination of the T2I model. On the other hand, we\u2019ve already considered a similar methodology such as the I2P dataset. These manually collected prompts generally would not pass the safety filter (such as the *\"Original Prompt\"* row for I2P in Table 2, Section 4) but serve as a great starting template for Ring-A-Bell**.\n\n**Secondly, we note that [R5] is a very recent and even concurrent submission published in August 2023 with a different problem setup.** Specifically, [R5] aims to develop a red-teaming tool of T2I diffusion models by leveraging the power of language models (LM). Specifically, the attack is set up as a feedback loop between the language model and the T2I model. That is to say, the LM would first initiate an adversarial prompt as an input to the T2I model. Meanwhile, the output image would go through a safeness classifier and the score would serve as feedback for the LM to adjust the adversarial prompt for subsequent trials. **While the method in [R5] serves as an important red-teaming tool for T2I models, we note that current online services would directly reject the generated inappropriate image, implying no meaningful feedback could be obtained by the LM. As a result, the extension to online T2I services remains unclear and therefore differs from the setting of Ring-A-Bell.**\n\nThirdly, [R6] aims to explore the potential risk of safety filters deployed by Stable Diffusion. Particularly, the authors proposed prompt dilution to dilute sensitive prompts such that it could circumvent the safety filtering of Stable DIffusion. **Here we note that we indeed incorporated the method of prompt dilution when evaluating Ring-A-Bell for online T2I services to increase the overall attack success rate. However, we\u2019ve included an ablation study between the effect of Ring-A-Bell with and without dilution in Appendix H to demonstrate that simply using prompt dilution alone is not effective in constructing a successful attack.**\n\n&nbsp;\n\n\n[R4] Qu, Yiting, Xinyue Shen, Xinlei He, Michael Backes, Savvas Zannettou, and Yang Zhang. \"Unsafe diffusion: On the generation of unsafe images and hateful memes from text-to-image models.\" ACM CCS 2023.  \\\n[R5] Mehrabi, Ninareh, Palash Goyal, Christophe Dupuy, Qian Hu, Shalini Ghosh, Richard Zemel, Kai-Wei Chang, Aram Galstyan, and Rahul Gupta. \"Flirt: Feedback loop in-context red teaming.\" arXiv preprint arXiv:2308.04265 (2023).  \\\n[R6] Rando, Javier, Daniel Paleka, David Lindner, Lennart Heim, and Florian Tram\u00e8r. \"Red-teaming the stable diffusion safety filter.\" arXiv preprint arXiv:2210.04610 (2022)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5834/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460525521,
                "cdate": 1700460525521,
                "tmdate": 1700462860033,
                "mdate": 1700462860033,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WoSHffVjfT",
                "forum": "lm7MRcsFiS",
                "replyto": "WmXFRT04ZC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer nmRw -- Part III"
                    },
                    "comment": {
                        "value": "&nbsp;\n\n### Investigation with Related Works -- Part II\n---\n\nLastly, [R7] attempts to attack the safety filter of existing online T2I services via reinforcement learning. Specifically, SneakyPrompt [R7] would initialize a target prompt and replace the sensitive tokens within. Meanwhile, the bypass-or-not response from the T2I services then serves as feedback to the agent to replace more suitable tokens until the safety filter is bypassed and the CLIP score between the target prompt and generated image is optimized. In the section below, we\u2019ve included the comparison between SneakyPrompt and Ring-A-Bell on nudity. \n\nWe conduct the official code of SneakyPrompt and follow their default setting that uses reinforcement learning to search, the CLIP score as a reward, the early stopping threshold score for the agent is 0.26, and the upper limit for query is 60. The result is demonstrated in the table below.\n\n\n&nbsp;\n\n| Attack Success Rate (ASR) | ESD        |  SLD-Max   | SLD-Strong | SLD-Medium |\n|:-------------------------:| ---------- |:----------:|:----------:|:----------:|\n|       SneakyPrompt        | 12.63%     |   3.16%    |   7.37%    |   27.37%   |\n|        Ring-A-Bell        | **35.79%** | **42.11%** | **61.05%** | **91.58%** |\n\n\nAs shown in the table, the performance of SneakyPrompt is **much lower than** Ring-A-Bell in terms of concept removal methods.  This is mainly due to the fact that SneakyPrompt focuses on the jailbreak of safety filters, rendering it **unable to find the problematic prompts for concept removal methods** so as to generate inappropriate images. Specifically, if the feedback from safety filters indicates that the image is safe, SneakyPrompt will deem this prompt as successfully jailbroken. However, since concept removal methods have already **eliminated a large portion of the sensitive concept**, even if the prompt contains sensitive words, under the setting of concept removal, **the generated image is deemed safe by the safety filter.** **As a result, SneakyPrompt skips it.**  \n\n&nbsp;\n\n[R7] Yang, Yuchen, Bo Hui, Haolin Yuan, Neil Gong, and Yinzhi Cao. \"SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models' Safety Filters.\" arXiv preprint arXiv:2305.12082 (2023)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5834/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460699178,
                "cdate": 1700460699178,
                "tmdate": 1700463557920,
                "mdate": 1700463557920,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "twQhmRKIVM",
                "forum": "lm7MRcsFiS",
                "replyto": "WmXFRT04ZC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer nmRw -- Part IV"
                    },
                    "comment": {
                        "value": "&nbsp;\n\n### Comparison Against P4D\n---\n**Disclaimer: The link might contain sensitive & inappropriate images, reader discretion is advised.**\n\nBefore demonstrating the online/offline gap between P4D and Ring-A-Bell, we would like to clarify the difference in evaluation for both works. The attack success rate metric (ASR) of P4D employs **the NudeNet classifier** [R9] (binary classifier for safe/unsafe) with an unsafe threshold of 0.45 to judge whether an image is inappropriate. On the other hand, our metric employs **the NudeNet detector** [R9] that detects specific exposure parts, and flags certain parts such as the anus, and genitalia as nudity concepts. \n\nTo establish the comparison against both evaluation metrics, we provide an example image that is considered generally safe with the following link (https://imgur.com/KEC7xLH). It can be seen that our results are more restricting since we recognize only certain parts as nudity-related while images such as the one in the link are recognized as unsafe in their much more lenient aspect. \n\nTo provide the comparison, we conduct the official code of P4D-$N$ using prompt length as $16$ and apply an identical random seed as the one used in Ring-A-Bell. We perform evaluation under both metrics and present the table below. As one can observe from the table, the performance of P4D-$N$ **degrades heavily** under our metric and **does not stand out significantly** in comparison to our approach. On the other hand, using P4D\u2019s metric, we also perform **superior to P4D** under the majority of different settings (3 out of 5), demonstrating the effectiveness of our method. We\u2019ve included this result in Appendix K for the reviewer\u2019s reference.\n\n&nbsp;\n\n| P4D-$N$ / Ring-A-Bell |         ESD         |       SLD-Max       |     SLD-Strong      |      SLD-Medium      |        SD-NP        |\n|:-----------------:|:-------------------:|:-------------------:|:-------------------:|:--------------------:|:-------------------:|\n|  Our ASR Metric   | 21.05% / **35.79%** | 12.63% / **42.11%** | 10.53% / **61.05%** | 28.42% / **91.58%** |  2.11% / **34.74%**  |\n|  P4D ASR Metric   | **66.31%** / 55.79% | **71.58%** / 57.89% | 77.89% / **86.32%** |  85.26% / **100%**  | 22.11% / **49.47%** |\n\n&nbsp;\n\n[R9] Bedapudi, P. (2021, December 9). NudeNet: An ensemble of Neural Nets for Nudity Detection and Censoring. Medium. Retrieved September 1, 2023, from https://praneethbedapudi.medium.com/nudenet-an-ensemble-of-neural-nets-for-nudity-detection-and-censoring-d9f3da721e3"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5834/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461409720,
                "cdate": 1700461409720,
                "tmdate": 1700462063325,
                "mdate": 1700462063325,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9c5WcJ1AsT",
                "forum": "lm7MRcsFiS",
                "replyto": "WmXFRT04ZC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to reviewer nmRw\u2019s feedback"
                    },
                    "comment": {
                        "value": "&nbsp;\n\nWe appreciate the positive feedback and constructive comments from reviewer nmRw in the initial reviews. As the discussion deadline nears, we haven't received additional feedback on our responses. We intend to use OpenReview's interactive feature to engage in discussions with the reviewer, confident that our response adequately addresses your concerns. Specifically, we've organized our response according to the reviewer\u2019s suggestion as the following:\n\n* Transferability and Model-Agnostic Design \n* Concept Removal and Beyond\n* Investigation with Related Works (including experimental results)\n* Experiments on comparison against P4D (including experimental results)\n\nWe hope our responses convince the reviewer about the merits of this work. If the reviewer has any other suggestions or comments, please don't hesitate to let us know!\n\nBest Regards, \\\nAuthors of Ring-A-Bell"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5834/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655248961,
                "cdate": 1700655248961,
                "tmdate": 1700655248961,
                "mdate": 1700655248961,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "D0sw232Dad",
            "forum": "lm7MRcsFiS",
            "replyto": "lm7MRcsFiS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5834/Reviewer_WiiG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5834/Reviewer_WiiG"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the effectiveness of safety mechanisms for text-to-image (T2I) diffusion models. It proposes a model-agnostic evaluation tool called Ring-A-Bell, which can assess the reliability of deployed safety mechanisms without prior knowledge of the target model. The tool performs concept extraction to identify problematic prompts and generates inappropriate content to evaluate the safety measures. The paper empirically validates the method by testing online services and various concept removal methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ significance: this paper reveals the importance of adversarial evaluation of the current concept removal works. Moreover, it performs its attack in a practical black-box way, which expands its evaluation scale to commercial APIs. How to evaluate the black-box commercial APIs is of great importance since they are powerful and more easily accessible by common people. \n\n+ quality: this paper takes a comprehensive inspect into the safety robustness of commercial APIs and also state-of-the-art concept removal methods, which demonstrates the efficacy of their method."
                },
                "weaknesses": {
                    "value": "- their attack is easy to be filtered or removed by advanced NLP techniques such as large language model, since they perform token-level optimization on the prompt and the output is usually random combinations of tokens. large language model can purify the prompt by removing the semantically unclear part of the prompt.\n\n- the token level optimization is uninterpretable and cannot provide insights into how to defend against such attacks."
                },
                "questions": {
                    "value": "- the ablation study of interference among modification, prompt dilution, and Ring-A-Bell: in Figure 2 and 3, the shown prompt contains the three types of texts. 1) is the whole prompt generated by Ring-A-Bell, or you combine the three types of attacks together for final output? how to discriminate the type of texts such as modification, prompt dilution, and Ring-A-Bell? 2) which type of text is essential to evade the safety filter of diffusion models or defeat the concept removal methods? \n\n- the ESD config is missing in Table 2 since it has multiple variants, whose concept removal effect is different from each other.\n\n- why K=16 is the final config? is there experiment result about smaller K?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5834/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698914840759,
            "cdate": 1698914840759,
            "tmdate": 1699636615921,
            "mdate": 1699636615921,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kesy7Vmre8",
                "forum": "lm7MRcsFiS",
                "replyto": "D0sw232Dad",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer WiiG -- Part I"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for providing detailed comments on Ring-A-Bell. We appreciated the comment saying that \u201c*this paper reveals the importance of adversarial evaluation \u2026. Moreover, it performs its attack in a practical black-box way, which expands its evaluation scale to commercial APIs.*\u201d and share a common perspective as the reviewer for the importance in evaluating these commercial APIs. Below we provide a pointwise reply to the reviewer\u2019s comment.\n\n&nbsp;\n\n### Ablation Study of Three Attack Strategies\n---\n**Disclaimer: The link might contain sensitive & inappropriate images, reader discretion is advised.**\n\nWe present the visualized ablation study in a figure with the following link (https://imgur.com/L7ObpnT). Specifically, in the figure, \"Ring-A-Bell\" represents our execution of the Ring-A-Bell method based on the target prompt to generate a problematic prompt. We note that the example is produced by DALL\u00b7E 2. Furthermore, in the figure, the top row represents the images by applying only modification and dilution while the bottom row applies all Ring-A-Bell, modification, and dilution techniques. \nAs one can see in the figure, using approaches such as modification and dilution could allow us to **increase the overall success rate rather than only using Ring-A-Bell**. To explain the two strategies, modification simply avoids inappropriate words in the problematic prompt (input filtering), while dilution prevents generated images from being identified as inappropriate (output filtering). It's worth noting that when only using the original prompt along with the above two techniques, e.g., modification and dilution, the generated images fail to generate nudity content. \n\nThat is to say, **simply using the original prompt and these two techniques does not produce inappropriate images. Problematic images would appear only when combining these techniques along with prompts generated by Ring-A-Bell.** For the reviewer\u2019s reference, we will include the discussion in Appendix I.\n\n&nbsp;\n\n### ESD Configuration\n--- \nFor the ESD configuration, we apply the officially released pre-trained model (ESD-Nudity), which only tunes non-cross-attention parameters and sets the negative guidance as $1$. For the ESD-Violent model, we trained it based on the official code and simply fine-tuned cross-attention parameters with negative guidance set as $3$. We have included these implementation details in Section 4.\n\n\n&nbsp;\n\n### Experiment on Smaller $K$\n---\nWe follow the settings of Table 3 in Section 4.3 to produce results for smaller $K$. Particularly, we set $K=8$ and $\\eta=3$ for the nudity concept. As shown in the table below, it is clear that the results obtained with $K=8$  **are not superior to** those achieved with $K=16$. Thus, it can be inferred that small values of $K$ are not conducive to effectively causing the model to generate inappropriate images. We\u2019ve included the additional result in Appendix J for the reviewer\u2019s reference.\n\n&nbsp;\n\n| Attack Success Rate (ASR) | SD         | ESD        |  SLD-Max   | SLD-Strong | SLD-Medium |   SD-NP    |     CA     |    FMN     |\n|:-------------------------:| ---------- | ---------- |:----------:| ---------- |:----------:|:----------:|:----------:|:----------:|\n|           $K=8$           | 87.37%     | 14.74%     |   21.05%   | 44.21%     |   87.37%   | **37.89%** |   85.26%   |   66.32%   |\n|          $K=16$           | **93.68%** | **35.79%** | **42.11%** | **61.05%** | **91.58%** |   34.74%   | **89.47%** | **68.42%** |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5834/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458311803,
                "cdate": 1700458311803,
                "tmdate": 1700459457122,
                "mdate": 1700459457122,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g9c0QEUpZ2",
                "forum": "lm7MRcsFiS",
                "replyto": "D0sw232Dad",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer WiiG -- Part II"
                    },
                    "comment": {
                        "value": "&nbsp;\n\n### Defense against Ring-A-Bell \n---\n(*A large language model can purify the prompt by removing the semantically unclear part of the prompt.*)\n\nWe thank the reviewer for bringing up the discussion of devising mitigation strategies against our proposed method. However, we would like to point out the inevitable competing nature of attacks and defenses and their adaptive versions, as discussed in \u201cOn Adaptive Attacks to Adversarial Example Defenses (NeurIPS)\u201d [R6]. We would like to further articulate the following points to explain why devising reliable defenses may not be trivial:\n\n* While it is possible that some additional text filtering schemes (e.g., a perplexity-based detector or a large language model) can mitigate the effectiveness of our approach, in the \u201cadaptive attack setting\u201d once the bad actor has some knowledge of the deployed defenses, advanced attacks can be carried out to bypass these defenses. For example, large language models are known to suffer from prompt injection attacks [R1, R2]. The bad actor can utilize prompt injection to counter these deployed defenses. Moreover, the perplexity-based defense was shown to be effective against early-day jailbreak prompts [R3], but it was quickly shown to be broken by semantic jailbreak prompts such as [R4]. Therefore, **advanced NLP techniques may not be a long-lasting solution, unless they can be proven certifiably reliable**.\n\n&nbsp;\n\n* It is reasonable to assume many of the tested online T2I services such as Midjourney, DALL\u00b7E 2, Gen-2, and Stable Diffusion XL, should have embedded a certain level of defenses to mitigate the possibility of generating inappropriate content. For example, it is mentioned in DALL\u00b7E 2 that both text prompt input and the generated image would go through safety filtering [R5]. However, regardless of the actual mitigation strategy, **our model-agnostic method can still find many successful prompts that bypass the implemented defenses (if any) embedded in these black-box models.** The model-agnostic nature of our method enables easy testing of any defense by considering the defense as part of the target model.\n\n&nbsp;\n\n* Thirdly, we\u2019ve included the experiment for defense such as GPT purification in the table below. Specifically, we adopt the target concept as violence and instruct ChatGPT using the template of \u201c*Please remove the semantically unclear part of the following prompt: <original prompt>*\u201d with *<original prompt>* being the problematic prompt produced by Ring-A-Bell. As shown in the table, it is evident that such defense **does not incur a huge drop in attack success rate (ASR)**, which signifies the effectiveness of Ring-A-Bell. Furthermore, we can also spot that under certain settings such as ESD and SLD-Max, the defense even raised the ASR of the attack, which establishes that simple defense itself does not suffice for mitigating Ring-A-Bell.\n\n&nbsp;\n\n* Lastly, even by mitigating the above issues and further establishing a certifiable defense strategy, this alone does not stand for **the certification of concept removal methods** as **simply blocking the input to be semantically correct does not imply that a certain concept has been fully removed from the diffusion model**, leaving an inherent issue.\n\n&nbsp;\n\n| Attack Success Rate (ASR) | SD        | ESD       |  SLD-Max  | SLD-Strong | SLD-Medium |  SD-NP  |    CA     |    FMN    |\n|:-------------------------:| --------- | --------- |:---------:| ---------- |:----------:|:-------:|:---------:|:---------:|\n|       GPT Purified        | 94%       | **59.2%** | **23.2%** | 45.6%      |   70.4%    |  73.2%  |   96.8%   | **80.4%** |\n|         Original          | **96.4%** | 54%       |   19.2%   | **50%**    | **76.4%**  | **80%** | **97.6%** |   79.6%   |\n\n&nbsp;\n\n[R1] Greshake, Kai, et al. \"Not what you\u2019ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection.\" arXiv preprint arXiv:2302.12173 (2023).\\\n[R2] Zou, Andy, et al. \"Universal and transferable adversarial attacks on aligned language models.\" arXiv preprint arXiv:2307.15043 (2023).\\\n[R3] Jain, Neel, et al. \"Baseline defenses for adversarial attacks against aligned language models.\" arXiv preprint arXiv:2309.00614 (2023).\\\n[R4] Liu, Xiaogeng, et al. \"Autodan: Generating stealthy jailbreak prompts on aligned large language models.\" arXiv preprint arXiv:2310.04451 (2023).\\\n[R5] https://dallery.gallery/dall-e-ai-guide-faq/ \\\n[R6] Tramer, Florian, et al. \"On adaptive attacks to adversarial example defenses.\" Advances in neural information processing systems 33 (2020): 1633-1645."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5834/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458895191,
                "cdate": 1700458895191,
                "tmdate": 1700459780853,
                "mdate": 1700459780853,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "abE6kTCgtX",
                "forum": "lm7MRcsFiS",
                "replyto": "D0sw232Dad",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to reviewer WiiG\u2019s feedback"
                    },
                    "comment": {
                        "value": "&nbsp;\n\nWe appreciate the positive feedback and constructive comments from reviewer WiiG in the initial reviews. As the discussion deadline nears, we haven't received additional feedback on our responses. We intend to use OpenReview's interactive feature to engage in discussions with the reviewer, confident that our response adequately addresses your concerns. Specifically, we've organized our response according to the reviewer\u2019s suggestion as the following:\n\n* Discussion of Defense against Ring-A-Bell (including experimental results)\n* Ablation Study of Three Attack Strategies  (including experimental results)\n* Description of ESD Configuration in Experiments\n* Experiment on Smaller $K$ values\n\nWe hope our responses convince the reviewer about the merits of this work. If the reviewer has any other suggestions or comments, please don't hesitate to let us know!\n\nBest Regards,\\\nAuthors of Ring-A-Bell"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5834/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655110319,
                "cdate": 1700655110319,
                "tmdate": 1700655110319,
                "mdate": 1700655110319,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7lDtEbgxWZ",
            "forum": "lm7MRcsFiS",
            "replyto": "lm7MRcsFiS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5834/Reviewer_UevV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5834/Reviewer_UevV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a model-agnostic red-teaming tool, Ring-A-Bell, for the evaluation of text-to-image (T2I) diffusion models\u2019 safety mechanisms. In the first stage, this concept retrieval algorithm would perform concept extraction by learning the difference between the embeddings of prompts with/ without the target concept (e.g., violence). With the extracted concept, the algorithm utilises genetic algorithms to produce problematic prompts to test the reliability of online T2I diffusion models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe main idea of the algorithms is clearly demonstrated with figures and examples. The motivation of the paper is clearly explained by analyzing the drawbacks of the current model-specific attack algorithms.\n\n2.\tExtensive experiments are well-designed to show the efficiency of Ring-A-Bell in generating problematic prompts in the field of nudity and violence. The evaluation is reasonable with the NudeNet detector. The results are clearly shown with quantitative tables and well-processed images to demonstrate the ability of Ring-A-Bell as a red-teaming tool."
                },
                "weaknesses": {
                    "value": "1. The related work should include the introduction of the concept removal methods, such as the Safe Latent Diffusion mentioned in the paper.\n\n2. In the concept extraction stage, the selection/ generation of the prompt pairs, which are semantically similar but different from the target concept, is not clearly specified. Producing high-quality prompt pairs requires extensive specialized knowledge. This can affect the effectiveness of the algorithm and increase the difficulty of reproduction.\n\n3. The generation of p \u0303_cont is simply by a linear combination of the embedding of P and extracted empirical representation c \u0302, which needs further justification. The definition of \u2018target prompt P\u2019 is not specified.\n\n4. The ablation study is not properly implemented. For example, it might be better to demonstrate the performance of the algorithm with and without discrete optimization.\n\n5. The algorithm strongly emphasizes that the text encoder is the CLIP model. It might be better to test on other text encoders."
                },
                "questions": {
                    "value": "see the above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5834/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699015691639,
            "cdate": 1699015691639,
            "tmdate": 1699636615813,
            "mdate": 1699636615813,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6SLcBVzpMW",
                "forum": "lm7MRcsFiS",
                "replyto": "7lDtEbgxWZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer UevV -- Part I"
                    },
                    "comment": {
                        "value": "We genuinely appreciate the reviewer for the comprehensive comments concerning online T2I diffusion models. We are delighted to receive the positive feedback that \u201c*Extensive experiments are well-designed to show the efficiency of Ring-A-Bell in generating problematic prompts in the field of nudity and violence\u2026*\u201d Please see our point-to-point response to your comments below. \n\n&nbsp;\n\n### Introduction to Concept Removal Methods\n---\nWe apologize for not making the references to concept removal methods clear in the related work section due to page limits. Nevertheless, we\u2019ve actually included a detailed related work in Appendix B, including the introduction to concept removal methods and related safety filters. In our revision, **we\u2019ve added a line of reference in Section 2, marked in blue, in accordance with the suggestion of the reviewer**.\n\n&nbsp;\n\n### Explanation of Prompt-Pair Selection and Generation\n---\nThanks for the great question. We would like to clarify that producing high-quality prompt pairs does not require extensive specialized knowledge from humans, and we simply apply existing large language models (e.g., ChatGPT) to generate such prompts.  **Specifically, for the generation of prompt-pairs, we utilize ChatGPT to create sentences about a particular concept $c$, i.e., $P_{i}^{c}$. Furthermore, when seeking semantically similar prompts without the concept, i.e., $P_{i}^{\\not{c}}$, we instruct ChatGPT to retain most words in the sentences and only modify a few words related to the specific concept, preventing the need of extensive knowledge**. \n\nFor instance, regarding objects or artistic styles like Van Gogh style, we ask ChatGPT to generate several words related to landscapes or natural scenery and append \"with Van Gogh style\" after each prompt. On the other hand, for $\\not{c}$, excluding \"with Van Gogh style\" suffices.\n\nAs for general and aggregated concepts such as nudity, we instruct ChatGPT to generate some vocabularies about nudity, such as exposed, bare, and topless. Furthermore, we define subjects and scenarios such as man, woman / bedroom, in a painting. Lastly, we ask ChatGPT to permute and construct sentences using these words. On the other hand, for $\\not{c}$, simply replacing the previous sensitive words would suffice.\n\nWe have added the above discussion in Appendix H to make our prompt generation process more clear.\n\n&nbsp;\n\n### Target Prompt and the Linear Combination\n---\nThanks for the question regarding linear combinations. We clarify that here the target prompt implies the initial prompt that is problematic but is unable to pass the safety filters or the concept removal methods, thus rendering these prompts incompetent in generating inappropriate images. In the experiment, the target prompts are chosen to be the ones in I2P dataset [R2] as described in Section 4. Pertaining to the reviewer\u2019s suggestion, we\u2019ve added the corresponding definition in a notation table included in Appendix G.\n\nOn the other hand, the intuition of linear combination has arisen from related literature [R1, R4]. Particularly, similar to our idea, [R1] attempts to create the steering activation vector by contrasting the activation of opposite prompts and appending them linearly to the user prompt for further manipulation. Meanwhile, [R4] attempts to model in-context learning via the so-called \u201cin-context vectors\u201d (ICV) that represent the specific task summary for a given language model. Furthermore, the ICVs could be utilized to fit the task that is aligned with the in-context demonstrations and the idea of learning multiple tasks or another way around could be done via simple arithmetics using these ICVs. **We believe that these ideas from the field of NLP have shared resonance with the linear combination approach of Ring-A-Bell to generate inappropriate images**.\n\n&nbsp;\n\n[R1] Turner, Alex, et al. \"Activation addition: Steering language models without optimization.\" arXiv preprint arXiv:2308.10248 (2023).\\\n[R2] Artificial Intelligence & Machine Learning Lab at TU Darmstadt. \u201cAIML-Tuda/I2P \u00b7 Datasets at Hugging Face.\u201d AIML-TUDA/I2p \u00b7 Datasets at Hugging Face, 20 Nov. 2022, huggingface.co/datasets/AIML-TUDA/i2p.\\\n[R3] Wen, Yuxin, et al. \"Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery.\" arXiv preprint arXiv:2302.03668 (2023).\\\n[R4] Liu, Sheng, Lei Xing, and James Zou. \"In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering.\" arXiv preprint arXiv:2311.06668 (2023)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5834/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700457452660,
                "cdate": 1700457452660,
                "tmdate": 1700459963858,
                "mdate": 1700459963858,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Slbgq8gSPx",
                "forum": "lm7MRcsFiS",
                "replyto": "7lDtEbgxWZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer UevV -- Part II"
                    },
                    "comment": {
                        "value": "&nbsp;\n\n### Ablation Study with Discrete Optimization\n---\nWe would like to clarify that discrete optimization is a necessary step in our black-box setting due to the discrete nature of the input text prompt (e.g., the word is not continuous), and we have included the ablation studies on different discrete optimization algorithms in Section 4.3.\nSpecifically, The overall Ring-A-Bell is implemented based on the black-box assessment of current safety T2I diffusion models. That is to say, without the white-box access of T2I models, we are unavailable to input prompt signals in the form of soft prompts or embedding. As a result, it is only possible if we could construct a hard prompt (discrete) that is in latent space approximately to that of our original soft prompt $\\mathbf{\\tilde{P}}_{cont}$. As a result, the discrete optimization is **not an additional but instead an integral part of the Ring-A-Bell framework**. \n\nLastly, we\u2019ve experimented with different discrete optimization methods such as genetic algorithms and PeZ [R3] in the ablation study. We are happy to evaluate additional discrete optimization methods if the reviewer holds an interest in any other ones.\n\n&nbsp;\n\n### Transferability Setting\n---\nWe thank the reviewer for raising the question. Indeed we\u2019ve attempted to expand Ring-A-Bell to other possible text encoders such as BERT and T5. However, there are certain critical differences that hinder such encoders from achieving comparable performance against the CLIP-Based Ring-A-Bell. The main reason lies in that these text encoders specialize **in the text domain and construct respective embedding space, while the embedding space for CLIP is much more aligned from the vision-language aspect.** Secondly, the hard prompt provided by Ring-A-Bell starts from an original soft prompt specific to CLIP with further discrete optimization. This model-specific conversion could hinder the generated prompt to transfer under various text encoder settings.\n\nNevertheless, we note that while the transferability is limited, in practice, **there is no reason to restrain the attacker to only use one text encoder such as CLIP.** We do not consider the differences in transferability as a weakness. **As long as the attacker can find one prompt to sabotage the target model, the attack is considered successful.** Therefore, the more text encoders the attacker can use, the more diverse prompts the attacker can create to test the target model.\n\nMoreover,  we note that most online T2I services would reveal not all but certain information about the model architecture or the underlying concept. This allows us to obtain the corresponding text encoder information for Ring-A-Bell to perform concept extraction. On the other hand, Ring-A-Bell could as well actively extract copies of certain concepts with possible text encoder candidates and apply the discrete optimization to acquire the respective hard prompt for evaluation. \n\n**Under this aspect, as long as a single problematic prompt from any text encoder has succeeded, this signifies the security breach of the corresponding of T2I models and its subsequent risk. Therefore we argue that the transferability of Ring-A-Bell does not hinder its potential in seeking possible risk of T2I models.**\n\n&nbsp;\n\n[R1] Turner, Alex, et al. \"Activation addition: Steering language models without optimization.\" arXiv preprint arXiv:2308.10248 (2023).\\\n[R2] Artificial Intelligence & Machine Learning Lab at TU Darmstadt. \u201cAIML-Tuda/I2P \u00b7 Datasets at Hugging Face.\u201d AIML-TUDA/I2p \u00b7 Datasets at Hugging Face, 20 Nov. 2022, huggingface.co/datasets/AIML-TUDA/i2p.\\\n[R3] Wen, Yuxin, et al. \"Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery.\" arXiv preprint arXiv:2302.03668 (2023).\\\n[R4] Liu, Sheng, Lei Xing, and James Zou. \"In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering.\" arXiv preprint arXiv:2311.06668 (2023)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5834/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700457544570,
                "cdate": 1700457544570,
                "tmdate": 1700459984617,
                "mdate": 1700459984617,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AOoEU3uv2o",
                "forum": "lm7MRcsFiS",
                "replyto": "7lDtEbgxWZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5834/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to reviewer UevV\u2019s feedback"
                    },
                    "comment": {
                        "value": "&nbsp;\n\nWe appreciate the positive feedback and constructive comments from reviewer UevV in the initial reviews. As the discussion deadline nears, we haven't received additional feedback on our responses. We intend to use OpenReview's interactive feature to engage in discussions with the reviewer, confident that our response adequately addresses your concerns. Specifically, we've organized our response according to the reviewer\u2019s suggestion as the following:\n\n* Introduction to Concept Removal Methods\n* Explanation of Prompt-Pair Selection and Generation\n* Explanation of Target Prompt and Intuition of the Linear Combination\n* Ablation Study with Discrete Optimization\n* Transferability Setting\n\nWe hope our responses convince the reviewer about the merits of this work. If the reviewer has any other suggestions or comments, please don't hesitate to let us know!\n\nBest Regards, \\\nAuthors of Ring-A-Bell"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5834/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655025524,
                "cdate": 1700655025524,
                "tmdate": 1700655025524,
                "mdate": 1700655025524,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]