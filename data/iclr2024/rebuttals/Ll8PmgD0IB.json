[
    {
        "title": "Divide and Orthogonalize: Efficient Continual Learning with Local Model Space Projection"
    },
    {
        "review": {
            "id": "7Nkt1LsRE3",
            "forum": "Ll8PmgD0IB",
            "replyto": "Ll8PmgD0IB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4563/Reviewer_apk6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4563/Reviewer_apk6"
            ],
            "content": {
                "summary": {
                    "value": "This paper improves the efficiency of SVD decomposition in gradient-projection-based continual learning method. They introduce local model space projection (LMSP) to improve the running efficiency of SVD decomposition. At the same time, LMSP can facilitate both forward and backward transfer of gradient-projection-based methods in continual learning. The authors also provide some theoretical analysis of LMSP. Experiments on several datasets evaluate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper introduces local model space projection to GPM to improve its running efficiency."
                },
                "weaknesses": {
                    "value": "* This paper writing needs to be further improved.  It would be better to directly state the intuitive idea and its illustration. This would make the main idea clearer and easier to understand. \n\n\n* The authors argue that SVD decomposition is computationally costly. This is true but it seems not an important problem in GPM since SVD decomposition only happens after finishing training each task, not every iteration. Therefore, the computation cost of SVD decomposition is minor compared to the overall training cost. \n\n\n* The authors state that their method could reduce the complexity of SVD basis computation, but there is no empirical evaluation of the overall training efficiency improvement with the proposed method compared to the GPM itself. \n\n\n* From the empirical results, LMSP improves the backward transfer, but the overall accuracy drops in some cases. The paper states that LMSP can improve both the forward and backward transfer, which does not support the claim."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4563/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697744961517,
            "cdate": 1697744961517,
            "tmdate": 1699636434117,
            "mdate": 1699636434117,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SucZdxjFIJ",
                "forum": "Ll8PmgD0IB",
                "replyto": "7Nkt1LsRE3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer apk6 Part 1"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer's constructive comments and valuable insights, which help improve the quality of our work significantly. We have carefully revised our paper according to your comments and suggestions. Please see our revised submission, where we have highlighted all major changes in **Blue** color. Please also see our point-to-point responses as follows:\n\n> **Your Comment 1:** This paper writing needs to be further improved. It would be better to directly state the intuitive idea and its illustration. This would make the main idea clearer and easier to understand.\n\n**Our Response:** Thanks for your suggestions. We fully agree with the reviewer that adding more intuition discussions and the rationale behind our proposed LMSP approach will make the presentation of our key idea clearer and easier to understand. In this revision, we have added such discussions in introduction to further clarify our key idea: To use local low-rank approximation to reduce the complexity in continual learning with forward and backward knowledge transfers, while not sacrificing too much performance. \n\n> **Your Comment 2:** The authors argue that SVD decomposition is computationally costly. This is true but it seems not an important problem in GPM since SVD decomposition only happens after finishing training each task, not every iteration. Therefore, the computation cost of SVD decomposition is minor compared to the overall training cost.\n\n**Our Response:** Thanks for your comments. The reviewer is correct that one round of layer-wise SVD operations is performed in GPM after finishing the learning of a new task. In fact, all orthogonal-projection-based CL approaches (not only GPM, but also TRGP, CUBER, and our LMSP methods) all perform SVD once for each layer after the training of each task. However, we note that such a one-round layer-wise per-task SVD does **not** necessarily mean that the resultant computation is cheap. In fact, such SVD computations remain highly expensive. Specifically, note that we need to perform SVD for each layer. With the ever-increasing widths and depths of large and deep learning models, computing one SVD even just for one layer becomes more and more difficult due to the $\\mathcal{O}(n^3)$ complexity as the width $n$ of each layer gets large. \n\nOn the other hand, we note that the training cost of each task is **not** necessarily higher than performing SVD, as the total number of iterations of most first-order methods typically does *not* scale with the model size/dimension. In our experiments, we find that the processing time of SVD is significantly higher than those of other components of the model. This is also evidenced by our newly added walk-time comparison experiments in the table in the response to your Comment 3 below. In that table below, we summarize the wall-clock training times of our LMSP algorithm is much **shorter** than those baselines with full SVDs.\n\nAll these results and analyses suggest that the computational complexity of SVD is the paint point of orthogonal-projection-based CL approaches. In addition, it is also mentioned in CUBER (Lin et al, 2022) that running a full SVD is time-consuming, which is consistent with our observations."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700192947979,
                "cdate": 1700192947979,
                "tmdate": 1700192947979,
                "mdate": 1700192947979,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fiInEINKhD",
                "forum": "Ll8PmgD0IB",
                "replyto": "7Nkt1LsRE3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer apk6 Part 2"
                    },
                    "comment": {
                        "value": "> **Your Comment 3:** The authors state that their method could reduce the complexity of SVD basis computation, but there is no empirical evaluation of the overall training efficiency improvement with the proposed method compared to the GPM itself.\n\n**Our Response:** Thanks for your comments. In this rebuttal period, we have added an additional set of experiments to evaluate the wall-clock training time  of our LMSP approach and compare with several closely related baselines. In the table below, we summarize the normalized wall-clock training times of our LMSP algorithm and several baselines with respect to the wall-clock training time of GPM (additional wall-clock training time results can also be found in [R3]). Here, we set the rank $r$ to 5 for each local model. We can see that the wall-clock time of our LMSP method with *only one anchor point* can already reduce the total wall-clock training time of CUBER by 86% on average. Moreover, thanks to the fact that our LMSP approach endows distributed implementation that can run different local models in a parallel fashion, the total walk-clock training time with $m$ anchor points is similar to the single-anchor-point case above. \n \n    \n| Training time | Cifar-100 Split | 5-Dataset | MiniImageNet | \n| ---------------- | --- | --- | --- |\n|OWM               | 2.41| -   | -   |\n|EWC               | 1.76| 1.52| 1.22|\n|HAT               | 1.62| 1.47| 0.91|\n|A-GEM             | 3.48| 2.41| 1.79|\n|ER_Res            | 1.49| 1.40| 0.82|\n|GPM               | 1.00| 1.00| 1.00|\n|TRPG              | 1.65| 1.21| 1.34|\n|CUBER             | 1.86| 1.55| 1.61|\n|**LMSP (Ours)**   |**0.24**|**0.42**|**0.18**|\n    \n[R1] S. Gobinda et al., \"Gradient Projection Memory for Continual Learning,\" in Proc. ICLR 2020.\n\n> **Your Comment 4:** From the empirical results, LMSP improves the backward transfer, but the overall accuracy drops in some cases. The paper states that LMSP can improve both the forward and backward transfer, which does not support the claim.\n\n**Our Response:** Thanks for your comments. We would like to clarify that, due to the information loss of using local model approximation in our LMSP method, it could happen that the overall accuracy of LMSP may be outperformed by other baseline methods. However, we want to emphasize that our goal in this paper is to significantly reduce the computational complexity from $\\mathcal{O}(n^3)$ to $\\mathcal{O}(n^2)$ by using local model approximation, even though this could lead to a slight performance loss. In other words, we would like to pursue *low-complexity CL algorithmic design* by potentially and slightly trading-off learning performance. \n\nAlso, in this rebuttal period, we have added additional experiments to evaluate the forward knowledge transfer (FWT) performance. As shown in the following table, we compared the FWT performance of our LMSP approach to those of the GPM, TRGP, and CUBER methods, which are the most related work to our paper. The value for GPM is zero because we treat GPM as the baseline and consider the relative FWT improvement over GPM. We compare them using four public datasets. We can see from the table that the FWT performance of LMSP outperforms those of TRGP and CUBER (two most related and state-of-the-art methods) on the PMNIST, Cifar-100 Split, and 5-Dataset datasets, and is comparable to those of TRGP and CUBER on the MiniImageNet dataset. This shows that LMSP does improve both FWT and BWT in most cases.\n\n| FWT (%) | PMNIST | Cifar-100 Split | 5-Dataset | MiniImageNet | \n| ---------------- | --- | --- | --- | --- |\n|GPM               | 0 | 0 | 0 | 0 |\n|TRPG              | 0.18| 2.01| 1.98| 2.36|\n|CUBER             | 0.80| 2.79| 1.96| **3.13**|\n|**LMSP (Ours)**   | **0.92**| **2.89** | **2.43** | 2.79|"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700193004955,
                "cdate": 1700193004955,
                "tmdate": 1700193004955,
                "mdate": 1700193004955,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eznEOYYUER",
            "forum": "Ll8PmgD0IB",
            "replyto": "Ll8PmgD0IB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4563/Reviewer_CGvu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4563/Reviewer_CGvu"
            ],
            "content": {
                "summary": {
                    "value": "Based on the basic framework of orthogonal-projection-based CL methods, this article proposes a local model space projection (LMSP) based efficient continual learning framework to help reduce the complexity of computation. The authors provide a theoretical analysis of backward knowledge transfer. Experiments based on multiple datasets demonstrate the effectiveness of the method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-structured with clear writing.\n- Leveraging the problem definition from previous research, this study presents a novel local model space projection approach, optimizing continual learning.\n- The authors also provide a theoretical analysis of the convergence."
                },
                "weaknesses": {
                    "value": "- The problem definition, framework, and convergence analysis of this work are derived from existing work. While the efficiency approach is intuitive and easy to understand, its novelty causes me concern.\n- The authors use local low-rank matrices defined by anchor points to approximate each layer parameter matrix. However, the accuracy of this approximation, and in particular how it is affected by m, is not discussed. Moreover, the proposed framework and analysis also ignore this issue. \n- The author introduces LLRA to improve computational efficiency. However, they do not perform experiments to evaluate the computational complexity and specifically do not show the saved wall-clock time compared with the LRA method."
                },
                "questions": {
                    "value": "- The author states that there is no significant difference between the two methods in selecting anchor points. Can you give some intuitive explanation?\n- Is there some relationship between ranking and the number of anchors?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4563/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4563/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4563/Reviewer_CGvu"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4563/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698421290685,
            "cdate": 1698421290685,
            "tmdate": 1700631783364,
            "mdate": 1700631783364,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B9PhwLbnkF",
                "forum": "Ll8PmgD0IB",
                "replyto": "eznEOYYUER",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CGvu Part 1"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer's constructive comments and valuable insights, which help improve the quality of our work significantly. We have carefully revised our paper according to your comments and suggestions. Please see our revised submission, where we have highlighted all major changes in **Blue** color. Please also see our point-to-point responses as follows:\n\n> **Your Comment 1:** The problem definition, framework, and convergence analysis of this work are derived from existing work. While the efficiency approach is intuitive and easy to understand, its novelty causes me concern.\n\n**Our Response:** Thanks for the comments. We would like to point out that, although the continual learning (CL) problem definition and framework in this paper are not completely new, it does *not* necessarily mean that there is no more research to be done. We note that CL is a broad and very active research field in recent years (see [R1] for the latest survey). However, there remains a large number of open fundamental research problems in CL and the performance of existing CL methods are still far from satisfactory. In this paper, we note that even the state-of-the-art orthogonal-projection-based CL approaches (TRGP and CUBER) still suffer from *high computational complexity* due to their use of the expensive SVD operations. This problem is further exacerbated by the ever-increasing large and deep vision and language models in the CL regime (i.e., sequential multi-task training). Therefore, our goal in this paper is to develop **new** orthogonal-projection-based CL methods with a significantly lower computational complexity. Toward this end, we propose a local model space projection (LMSP) approach, which is **new** in the CL literature.\n    \nFor the convergence analysis for our LMSP method, it is true that our proof is based on the framework of first-order optimization algorithm convergence analysis, which starts from bounding one-step descent and finishes at telescoping one-step descent and rearranging to arrive at stationarity gap bound. However, we point out that the similarity of our convergence performance analysis compared to other methods ends there. The complications arising from the use of local model projection approximations renders our convergence proof significantly different from those of existing CL methods. Specifically, our convergence proof and analysis involves the new notion called \"local relative orthogonality\" (see Definition 5 in our revised paper). Theorem 1 focuses on proving the convergence of our local algorithm and Theorem 2 proves that under such conditions and Definition 5, the out LMSP could achieve even better results than the global algorithm counterpart (CUBER and TRGP).\n\n[R1] L. Wang et al., \"A Comprehensive Survey of Continual Learning: Theory, Method and Application,\" https://arxiv.org/abs/2302.00487\n\n> **Your Comment 2:** The authors use local low-rank matrices defined by anchor points to approximate each layer parameter matrix. However, the accuracy of this approximation, and in particular how it is affected by $m$, is not discussed. Moreover, the proposed framework and analysis also ignore this issue.\n\n**Our Response:** Thanks for your comments. In Section 5, we have provided in-depth ablation studies on the impacts of different rank values $r$ in low-rank approximations and different number of anchor points $m$ on the learning accuracy (ACC) and backward knowledge transfer (BWT). More specifically, with the use of local model approximation to reduce computational complexity, information loss of the original learning model is inevitable. Our goal in this paper is to reduce the computational complexity without sacrificing too much performance. \n\nAlso, we did not ignore the impacts of $m$ in our theoretical analysis since these local model approximation errors have already been implicitly captured in $\\bar{\\mathbf{g}}_i(\\mathbf{W})$. As shown in the paper, the local model approximation affects the convergence analysis through the $\\bar{\\mathbf{g}}_i(\\mathbf{W})$, $i=1,2$. Thus, by choosing the top-K correlated local tasks, the more anchor points we have, the smaller approximation error of $\\bar{\\mathbf{g}}_i(\\mathbf{W})$ compared to their true versions $\\ddot{\\mathbf{g}_i}(\\mathbf{W})$, $i=1,2$ we get. Moreover, with the approximation error bound from [R2], we can theoretically characterize the impact of $m$ in our analysis.\n    \n[R2] J. Lee et al., \"Local Low-Rank Matrix Approximation,\" in Proc. ICML 2013."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700192763531,
                "cdate": 1700192763531,
                "tmdate": 1700192763531,
                "mdate": 1700192763531,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dn5nIBD4LF",
                "forum": "Ll8PmgD0IB",
                "replyto": "eznEOYYUER",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CGvu Part 2"
                    },
                    "comment": {
                        "value": "> **Your Comment 3:** The author introduces LLRA to improve computational efficiency. However, they do not perform experiments to evaluate the computational complexity and specifically do not show the saved wall-clock time compared with the LRA method.\n\n**Our Response:** Thanks for your comments. We note that we do have experimental results to evaluate the computational complexity of our LMSP approach. In the table below, we summarize the normalized wall-clock training times of our LMSP algorithm and several baselines with respect to the wall-clock training time of GPM (additional wall-clock training time results can also be found in [R3]). Here, we set the rank $r$ to 5 for each local model. We can see that the wall-clock time of our LMSP method with *only one anchor point* can already reduce the total wall-clock training time of CUBER by 86% on average. Moreover, thanks to the fact that our LMSP approach endows distributed implementation that can run different local models in a parallel fashion, the total walk-clock training time with $m$ anchor points is similar to the single-anchor-point case above. \n    \n| Training Time | Cifar-100 Split | 5-Dataset | MiniImageNet | \n| ---------------- | --- | --- | --- |\n|OWM               | 2.41| -   | -   |\n|EWC               | 1.76| 1.52| 1.22|\n|HAT               | 1.62| 1.47| 0.91|\n|A-GEM             | 3.48| 2.41| 1.79|\n|ER_Res            | 1.49| 1.40| 0.82|\n|GPM               | 1.00| 1.00| 1.00|\n|TRPG              | 1.65| 1.21| 1.34|\n|CUBER             | 1.86| 1.55| 1.61|\n|**LMSP (Ours)**   |**0.24**|**0.42**|**0.18**|\n    \n[R3] S. Gobinda et al., \"Gradient Projection Memory for Continual Learning,\" in Proc. ICLR 2020.\n\n> **Your Comment 4:** The author states that there is no significant difference between the two methods in selecting anchor points. Can you give some intuitive explanation?\n\n**Our Response:** Thanks for the suggestion. As we discussed in the paper, if the new task is strongly correlated with some old tasks, there should be better knowledge transfer from the correlated old tasks to the new task. Thus, the performance largely relies on finding correlated tasks. Supposing the data is not biased, simply choosing enough anchor points should provide enough candidates for the new task to choose.\n\n> **Your Comment 5:** Is there some relationship between ranking and the number of anchors?\n\n**Our Response:** Thanks for your question. In theory, the values of rank and number of anchors can be chosen independently and arbitrarily in our LMSP approach. In the extreme case, if we use full rank and set number of anchor points to be 1, then our LMSP method reduces to the CUBER baseline method. In practice, we often prefer to choose a small rank value since it will significantly reduce the computational complexity. Also, if permitted by computational resources, choosing more anchor points is more preferable, since this would yields better approximation to the original model. Moreover, since each local model approximation could run in a parallel fashion (implied by our LMSP method's distributed implementation), having more anchor points will not significantly increase the wall clock time performance."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700192825650,
                "cdate": 1700192825650,
                "tmdate": 1700192825650,
                "mdate": 1700192825650,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nj26PVXObl",
                "forum": "Ll8PmgD0IB",
                "replyto": "B9PhwLbnkF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4563/Reviewer_CGvu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4563/Reviewer_CGvu"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "1. To clarify, I did not intend to suggest that the field of continual learning has been fully explored. I agree with Reviewer apk6 regarding the presentation of the manuscript. A concise explanation of existing frameworks and a focused discourse on the unique aspects of your method, would enhance the paper's readability and effectively highlight its novel contributions.\n\n2. Regarding the relationship between rank and the number of anchor points, my understanding is as follows: The rank reflects the number of local modes within the representation matrix. The number of anchor points influences the accuracy of this local approximation. For matrices with few information, a lower rank suggests that fewer anchor points are needed to accurately represent the information. Both your experiments and response suggest that these two elements function independently.\n\n3. Concerning the two methods of selecting anchor points: random selection may result in points that are too similar or possess overlapping information, whereas pre-clustering to find centroids is likely to provide a more distinct and diverse representation. I am unsure why both methods are deemed equally viable. Additionally, I'm curious about the role of data bias in relation to these selection methods."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700473799783,
                "cdate": 1700473799783,
                "tmdate": 1700473799783,
                "mdate": 1700473799783,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "skmTyuqXZU",
                "forum": "Ll8PmgD0IB",
                "replyto": "UkYxaLnLsc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4563/Reviewer_CGvu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4563/Reviewer_CGvu"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "Thanks for your response. My comments have been addressed, so I'll bump up the score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631766238,
                "cdate": 1700631766238,
                "tmdate": 1700631766238,
                "mdate": 1700631766238,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "93bR56DBnw",
            "forum": "Ll8PmgD0IB",
            "replyto": "Ll8PmgD0IB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4563/Reviewer_Shvn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4563/Reviewer_Shvn"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Local Model Space Projection, a method in continual learning that aims at avoiding forgetting and encourage knowledge transfer by performing orthogonal updates of parameters over the sequence of tasks. The method considers three regimes: 1) forgetting avoidance, 2) forward transfer, 3) backward transfer, which can be represented as variants of the problem of finding orthogonal directions for parameter updates. The method constructs local model spaces of each task by selecting some anchoring points from the task's representation. Using these representations, a similarity between tasks can be measured across local representations to determine whether the task has local sufficient projection, local positive correlation or local relative orthogonality. Theoretical analyses along with experimental results are provided. Experiments are reported for 4 benchmark datasets, and compared to a range of SOTA continual learning methods from different families (regularization, replay, orthogonalization). Results are provided in terms of accuracy and backward transfer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper proposes an original method that exploits the idea of orthogonal projections to learn new tasks whilst controlling forgetting and encouraging forward and backward transfer. The consideration of particular regimes for each of these problems, and the fact that each of these regimes can be addressed with the same underlying idea of projections that consider local representations of tasks seems novel and useful. \n- The paper is very clear, easy to follow and mostly complete as it considers both theoretical and experimental demonstrations of how and why it works. \n- The paper is somewhat significant in the sense that it seemingly not only tackles forgetting but also knowledge transfer, and it presents some good results in both accuracy and backward transfer."
                },
                "weaknesses": {
                    "value": "- Although the proposed method seems quite competitive in terms of experimental results, there is no report on the performance of forward transfer. This is extremely relevant as forward and backward transfer are usually in trade-off (the more forward, the less backward transfer and vice-versa). How can you guarantee that the good results in backward transfer do not require sacrificing forward transfer, or even just the fact of learning the new task reasonably well?"
                },
                "questions": {
                    "value": "- Can you provide actual performance numbers for forward transfer?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4563/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731960791,
            "cdate": 1698731960791,
            "tmdate": 1699636433837,
            "mdate": 1699636433837,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "41NHjNcnUX",
                "forum": "Ll8PmgD0IB",
                "replyto": "93bR56DBnw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Shvn"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer's constructive comments and valuable insights, which help improve the quality of our work significantly. We have carefully revised our paper according to your comments and suggestions. Please see our revised submission, where we have highlighted all major changes in **Blue** color. Please also see our point-to-point responses as follows:\n\n> **Your Comment 1:** Although the proposed method seems quite competitive in terms of experimental results, there is no report on the performance of forward transfer. This is extremely relevant as forward and backward transfer are usually in trade-off (the more forward, the less backward transfer and vice-versa). How can you guarantee that the good results in backward transfer do not require sacrificing forward transfer, or even just the fact of learning the new task reasonably well?\n\n**Our Response:** Thanks for this suggestion. In this rebuttal period, we have added additional experiments to evaluate the forward knowledge transfer (FWT) performance. As shown in the following table, we compared the FWT performance of our LMSP approach to those of GPM, TRGP, and CUBER methods, which are the most related work to our paper. The value for GPM is zero because we treat GPM as the baseline and consider the relative FWT improvement over GPM. We compare them using four public datasets. We can see from the table that the FWT performance of our LMSP approach beats those of the TRGP and CUBER (two most related and state-of-the-art methods) on the PMNIST, Cifar-100 Split, and 5-Dataset datasets, and is comparable to those of the TRGP and CUBER on the MiniImageNet dataset. Clearly, this shows that the good BWT performance of our LMSP method is **not** achieved at the cost of sacrificing the FWT performance.\n\n| FWT (%) | PMNIST | Cifar-100 Split | 5-Dataset | MiniImageNet | \n| ---------------- | --- | --- | --- | --- |\n|GPM               | 0 | 0 | 0 | 0 |\n|TRPG              | 0.18| 2.01| 1.98| 2.36|\n|CUBER             | 0.80| 2.79| 1.96| **3.13**|\n|**LMSP (Ours)**   | **0.92**| **2.89** | **2.43** | 2.79|\n\n> **Your Comment 2:** Can you provide actual performance numbers for forward transfer?\n\n**Our Response:** Thanks for the suggestion. Please see the table above."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700192612715,
                "cdate": 1700192612715,
                "tmdate": 1700192612715,
                "mdate": 1700192612715,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "esEfKCH2Ai",
            "forum": "Ll8PmgD0IB",
            "replyto": "Ll8PmgD0IB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4563/Reviewer_ZQXi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4563/Reviewer_ZQXi"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies an interesting topic, continual learning, which aims to learn a series of tasks without forgetting. . The existing CL methods require either an extensive amount of resources for computing gradient projections or storing a large number of old tasks\u2019 data.  . In this paper, a local model space projection\n(LMSP) is proposed to not only significantly reduce the complexity of computation, but also enables forward and backwardknowledge transfer. Extensive experiments on several public datasets demonstrate the efficiency of our approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-written.\n2. The research topic is very interesting."
                },
                "weaknesses": {
                    "value": "1. Performing the forward and backward knowledge transfer has been done in the existing works.\n2. The proposed approach relies on the task information, which can not be used in task-free continual learning.\n3. The proposed approach does not always achieve the best performance in some datasets.\n4. Although the proposed approach can reduce computational costs but would increase more parameters."
                },
                "questions": {
                    "value": "Please see the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4563/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4563/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4563/Reviewer_ZQXi"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4563/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773698469,
            "cdate": 1698773698469,
            "tmdate": 1699636433757,
            "mdate": 1699636433757,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OvKTNX3nlv",
                "forum": "Ll8PmgD0IB",
                "replyto": "esEfKCH2Ai",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZQXi Part 1"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer's constructive comments and valuable insights, which help improve the quality of our work significantly. We have carefully revised our paper according to your comments and suggestions. Please see our revised submission, where we have highlighted all major changes in **Blue** color. Please also see our point-to-point responses as follows:\n\n> **Your Comment 1:** Performing the forward and backward knowledge transfer has been done in the existing works.\n\n**Our Response:** Thanks for the comments. Although it is true that both forward knowledge transfer (FWT) and backward knowledge transfer (BWT) have been studied in the literature of continual learning (CL), the FWT and BWT performances of existing work in this area *remain far from satisfactory*. More specifically, the FWT and BWT in the existing CL methods require either an extensive amount of resources for computing gradient projections (in orthogonal-projection-based CL) or storing a large amount of old tasks\u2019 data (in experience-replay-based and regularization-based CL). The limitations of these existing work motivate us to propose a new CL method to improve the FWT and BWT performances.\n\nSpecifically, in this paper, we focus on reducing the $\\mathcal{O}(n^3)$ computational complexity in SVD and also increasing the scalability of orthogonal-projection-based CL methods. This is due to the nice fact that orthogonal-projection-based CL methods do not need to access old tasks' data. Toward this end, we propose a local model space projection (LMSP) approach that could achieve $\\mathcal{O}(n^2)$ instead of $\\mathcal{O}(n^3)$ complexity."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700192380338,
                "cdate": 1700192380338,
                "tmdate": 1700192380338,
                "mdate": 1700192380338,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fypQR9Ugvj",
                "forum": "Ll8PmgD0IB",
                "replyto": "esEfKCH2Ai",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZQXi Part 2"
                    },
                    "comment": {
                        "value": "> **Your Comment 2:** The proposed approach relies on the task information, which can not be used in task-free continual learning.\n\n**Our Response:** Thanks for your comments. We clarify that our focus in this paper is the standard task-based CL setting, i.e., tasks arrive at the learner *sequentially* with clear task boundaries (see, e.g., [R1] for the description of this standard setting of CL). However, we would like to point out that our work focuses on the *orthogonal-projection-based* CL approach, which requires the *least* (in fact, almost zero) amount of task information since orthogonal-projection-based CL methods do *not* need to save any old tasks data. All we need is to compute the new null space of the model parameters upon finishing the learning of the previous task.\n\nOn the other hand, we also note that \"task-free continual learning\" is a new CL paradigm, which refers to CL systems with **no** clear boundaries between tasks and data distributions of tasks gradually and continuously changing (see [R2] for the detailed description of task-free CL). Clearly, task-free CL is a more complex CL paradigm. How to conduct CL without requiring previous tasks' information is a far more challenging open problem in the community, which deserves an independent paper dedicated to this topic. But this is beyond the scope of our current work, and could be an interesting and important future direction. We thank the reviewer for suggesting this direction.\n\n[R1] L. Wang et al., \"A Comprehensive Survey of Continual Learning:\nTheory, Method and Application,\" https://arxiv.org/abs/2302.00487\n\n[R2] R. Aljundi et al., \"Task-Free Continual Learning,\" in Proc. CVPR 2019.\n\n> **Your Comment 3:** The proposed approach does not always achieve the best performance in some datasets.\n\n**Our Response:** Thanks for your comments. We would like to clarify that, due to the information loss of using local model approximation in our LMSP method, it could happen that LMSP may be outperformed by other baseline methods. However, we want to emphasize that our goal in this paper is to significantly reduce the computational complexity from $\\mathcal{O}(n^3)$ to $\\mathcal{O}(n^2)$ by using local model approximation, even though this could lead to a slight performance loss. In other words, we would like to pursue *low-complexity CL algorithmic design* by potentially and slightly trading-off learning performance. \n\nInterestingly, our experiments show that, due to other complex factors in CL systems, our LMSP approach actually *outperforms* the baseline approaches in most scenarios (cf. Table 1). Also, it is worth noting that we theoretically characterized the conditions under which our LMSP approach could achieve better results.\n\n> **Your Comment 4:** Although the proposed approach can reduce computational costs but would increase more parameters.\n\n**Our Response:** Thanks for the comments. It appears that there are some misunderstandings and confusions that are perhaps due to the relatively complex math notations in our algorithm. We would like to clarify that our local model projection approach does *not* increase the number of model parameters (i.e., our LMSP approach remains having the same number of parameters compared to CUBER, which is the most related work). Specifically, we apply local model approximation on each layer's output representation by partitioning the layer's matrix into smaller submatrices (defined by anchor points), which allows faster processing of these smaller submatrices *in parallel*. During this process, the total number of parameters remains the same (cf. the description at the bottom of Page 4 and Eq. (3)). Then, our LMSP method updates the new weights $\\mathbf{W}^l$ from previous model weights using the LMSP-based projected gradients and scaling parameters, hence the number of parameters remains the same as those of CUBER (cf. Eq. (7))."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700192442594,
                "cdate": 1700192442594,
                "tmdate": 1700192442594,
                "mdate": 1700192442594,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]