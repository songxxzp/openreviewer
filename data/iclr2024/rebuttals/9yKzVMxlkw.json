[
    {
        "title": "TiG-BEV: Multi-view BEV 3D Object Detection via Target Inner-Geometry Learning"
    },
    {
        "review": {
            "id": "DV8qrijyAR",
            "forum": "9yKzVMxlkw",
            "replyto": "9yKzVMxlkw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission371/Reviewer_kyMY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission371/Reviewer_kyMY"
            ],
            "content": {
                "summary": {
                    "value": "TiG-BEV proposes an internal geometric learning scheme for the target, which enhances the camera based BEV detector from both depth and BEV features by utilizing LiDAR mode. Firstly, an internal deep supervision module is introduced to learn the low-level relative depth relationships of each target, thereby enabling the camera detector to have a deeper understanding of the target level spatial structure. Secondly, an internal feature BEV distillation module was designed to mimic the high-level semantics of different key points within the foreground target. In order to reduce the domain difference between the two modes, distillation within the channel and between key points was used to model feature similarity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The research methodology proposed in this paper involves two main components. \n1. An inner-depth supervision module is introduced to learn the low-level relative depth relations within each object. This helps the camera-based detectors gain a deeper understanding of object-level spatial structures. \n2. An inner-feature BEV distillation module is designed to imitate the high-level semantics of different keypoints within foreground targets using inter-channel and interkeypoint distillation."
                },
                "weaknesses": {
                    "value": "1. The improvement over other methods is limited, as there have been numerous distillation methods proposed for camera-based detectors such as BEV-LGKD, DistillBEV, BEVSimDet, and X3KD. However, this method lacks clear advantages compared to previous approaches, and no extensive comparison with other methods has been provided.\n2. The comparison is biased. Firstly, BEVDistill is specifically designed for BEVFormer, which only has one distillation module for BEVDepth. Secondly, the experiments conducted on the test set were augmented, which does not provide a fair comparison. Thirdly, I noticed that the baseline model achieved similar scores to BEVDistill in Table 2, but the official code does not provide the checkpoint for that. Therefore, I strongly recommend conducting comparisons using the Res-50 model to demonstrate improvements over previous methods such as BEV-LGKD, DistillBEV, BEVSimDet, and X3KD."
                },
                "questions": {
                    "value": "1. The paper should report results based on long-term temporal methods, such as solofusion, to demonstrate the effectiveness of the proposed method.\n2. The paper should provide a more comprehensive and fair comparison with previous methods, specifically in the ResNet-50 (R50) framework, to showcase the improvement of the proposed method over existing approaches such as BEV-LGKD, DistillBEV, BEVSimDet, and X3KD. This will provide a clearer understanding of the performance gains achieved by the proposed method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission371/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission371/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission371/Reviewer_kyMY"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission371/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646861677,
            "cdate": 1698646861677,
            "tmdate": 1699635964113,
            "mdate": 1699635964113,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HKStCurKNi",
                "forum": "9yKzVMxlkw",
                "replyto": "DV8qrijyAR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission371/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission371/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kyMY"
                    },
                    "comment": {
                        "value": "Thank you for your time and effort. \n\n### **Q1. Compare with other related works in the R50 framework.**\n\nThank you for your suggestion. We have shown the comparison in **General Response to All Reviewers**. We are sure that our comparison is fair. According to the Reviewer Policy, we have reported the results of several methods using the LiDAR->Camera mode on ResNet50 and published in top-tier conferences. As there is a lack of consistency in the baselines reproduced by everyone, we have also included the baseline in the table for comparison. It is evident that our method performs competitively. We hope it can provide a clearer understanding.\n\n### **Q2. The comparison is biased?**\n\nWe apologize for any confusion and misunderstanding that we may have caused you. We need to state that we place great emphasis on comparing fairly and we have enough confidence to ensure that our comparison is fair.\n\n- Firstly, BEVDistill also used two distillation modules (BEV Feature Level and Instance Response Level) proposed in the paper for the BEVDepth experiment, which is referred to in Appendix B.3 in BEVDistill. These two modules are silimar to distillation methods in 2D object detection and are proved to be generalizable. The author adapted instance-level distillation on two different frameworks (bevdepth and bevformer), while bev feature level distillation is generalizable, like our inner-feature distillation. Moreover, as shown in Table 7 in our paper, we also list the baseline results and show the improvement respectively to make a fair comparison.\n\n- Secondly, both BEVDistill and our method used the same and common data augmentation for training, such as image flipping, scaling, etc., and neither of us used other augmentation methods such as TTA for test set. Therefore, there is no unfair comparison here.\n\n- Thirdly, we are a little confused about your comment. We are not quite sure which paper's 'Table 2' you are referring to (BEVDepth/BEVDistill/TiG-BEV?). The Table 2 in our paper shows the NuScenes Test Leaderboard result, all participants do not need to, and are not allowed to provide checkpoints for it. While the results we provide for the baseline are consistent with the results reported by BEVDistill, because our implementation of the baseline method is also completely consistent. We all use the BEVDet_v1.0 codebase to reproduce the result of the BEVDepth Baseline. What's more, the config of the baseline we used was given by the author of BEVDistill, we had ever asked for it. So our baselines for test leaderboard are the same and we also found that our results reproduced by the same config were also the same. Therefore, from Table 2, we can see the superiority of our method clearly and it's also completely fair. \n\n### **Q3. Report results based on long-term temporal methods.**\n\nThanks for your suggestion. We did not specifically design for long-term setting before. In order to show results based on long-term setting, we trained a LiDAR Teacher with BEVDet_v2.0 codebase [1]. Due to resource limitations, our teacher model appears to be not powerful enough (mAP:49.7/ NDS:52.3), but it still can be effective. Without any tuning, we were still able to further improve the BEVDet4D-Depth on the long-term setting (8 + 1 frames) as shown in the table.\n\n\n|Method|w/ TiG-BEV |mAP $\\uparrow$|NDS $\\uparrow$|\n|--- | --- | --- | --- | \n|BEVDepth|   |39.4|51.5|\n|BEVDepth| $\\checkmark$  |41.3|52.5|\n\n #### Note: For the table, we use ResNet50 as the backbone, the resolution is 256 $\\times$ 704.\n\n\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n\nOverall, thank you for bringing your concerns to our attention. We hope that our response has effectively addressed them and cleared up any misunderstandings. \n\n### Reference:\n\n[1] https://github.com/HuangJunJie2017/BEVDet/tree/dev2.0"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission371/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596225323,
                "cdate": 1700596225323,
                "tmdate": 1700596225323,
                "mdate": 1700596225323,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GrXxuUR5zT",
                "forum": "9yKzVMxlkw",
                "replyto": "DV8qrijyAR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission371/Reviewer_kyMY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission371/Reviewer_kyMY"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Feedback"
                    },
                    "comment": {
                        "value": "The reviewer expresses gratitude to the authors for their feedback; nevertheless, there is a lingering issue as some of his concerns remain unaddressed.\n\nQ1. The progress in points is somewhat limited compared to previous approaches such as DistillBEV. He maintains that this paper presents only a narrow scope of new insights, as emphasized by Reviewer oss3. Regarding the feature distillation component, its characteristics did not appear markedly distinct from those of DistillBEV. Instead, it seemed predominantly foregrounded, albeit in a divergent manner. Besides, through ablation experiments, it has been ascertained that the depth distillation module yields only marginal improvements.\n\nQ2. The reviewer acknowledges the inclusion of new references to established works. However, there are still some works not included as follows.\n\n[1]. BEVSimDet: Simulated Multi-modal Distillation in Bird's-Eye View for Multi-view 3D Object Detection.\n\n[2]. Leveraging Vision-Centric Multi-Modal Expertise for 3D Object Detection.\n\n[3]. Distilling Focal Knowledge From Imperfect Expert for 3D Object Detection.\n\n[4]. BEV-LGKD: A Unified LiDAR-Guided Knowledge Distillation Framework for BEV 3D Object Detection."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission371/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650719020,
                "cdate": 1700650719020,
                "tmdate": 1700653170379,
                "mdate": 1700653170379,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1pzC6StVQy",
            "forum": "9yKzVMxlkw",
            "replyto": "9yKzVMxlkw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission371/Reviewer_CVGt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission371/Reviewer_CVGt"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new method for multi-camera 3D object detection, where existing techniques for knowledge distillation from a pre-trained LiDAR-based detector to a camera-based detector are enhanced. The authors propose to extract a reference point in the image space as well as keypoints in bird\u2019s eye view and focus the distillation from LiDAR-based models to camera-based models on these points. Evaluation on the nuScenes dataset shows that the method is able to outperform various previous baseline methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-\tThe introduction and related work provide a clear and concise motivation for the proposed method. Figures 1 and 2 give a nice qualitative insight into the proposed contributions.\n-\tThe method description is clear and easy to follow. The mathematical description is overall quite precise and Figures 2-6 help a lot to better understand the method.\n-\tThe authors show that their method can be combined with various baseline methods.\n-\tThe ablation study verifies the effectiveness of the single method components."
                },
                "weaknesses": {
                    "value": "Issues:\n\n1.\tRecently, there have been some relevant related works [a, b], which seem quite related to the method presented in this paper. I think it would be good to compare to these works in the SOTA comparison as well and discuss differences in a bit more detail.\n[a] Klingner et al. \u201cX3KD: Knowledge Distillation Across Modalities, Tasks and Stages for Multi-Camera 3D Object Detection,\u201d CVPR 2023\n[b] Zhou er al. \u201cUniDistill: A Universal Cross-Modality Knowledge Distillation Framework for 3D Object Detection in Bird's-Eye View,\u201d CVPR 2023\n\n2.\tI am wondering how generalizable the method is. It seems that especially the improvement in terms of learning a better depth distribution is quite constraint to methods making use of feature projection via depth maps. However, e.g., DETR-like methods for multi-camera 3DOD do not have this feature. It would be interesting to provide insights how the method could be extended to such methods.\n\n3.\tDue to the two mentioned issues I feel that the contribution and scope of this works appears a bit limited. If they can be addressed in the rebuttal, I am open to reconsider this point.\n\nMinor comments and typos:\n\n4.\tSection 3.1: I think it would be good to have a better differentiation in terms of notation between the number of depth bins D and the depth map D. I also did not completely understand the upper index A in the depth loss.\n\n5.\tSection 3.1: I think it is a bit confusing to talk about 3d and 2d features if they have exactly the same shape. The same goes for 3D and 2D detection heads in Figure 4, which are probably exactly the same just for different modalities as input. Maybe it would help to rather talk about camera and LiDAR features?\n\n6.\tFigure 4: It would be nice to include the math notation also in the figure to be able to better connect the method description and the visualization of the method in the figure\n\n7.\tThe text in Figure 5 and 6 is quite small. It would be nice to increase the font size a bit.\n\n8.\tTable 1 and 2: It would be good to include the latest works on multi-camera 3DOD in the SOTA comparison, e.g., [a,b] or to explain why they are excluded. As far as I saw, they also report on nuScenes, so shouldn\u2019t they be comparable?\n\n9.\tIt would be nice to verify the method on more than one dataset, e.g., the Waymo dataset.\n\n10.\tTable 2: It would be good to also report resolution and backbone in this comparison.\n\n11.\tThere are a few (minor) typos remaining throughout the whole paper. It would be good to resolve them with another round of proofreading."
                },
                "questions": {
                    "value": "-\tIt is interesting to see that using only a reference point for depth supervision is superior to using a dense depth map as supervision. I have not completely understood the motivation though. Could the authors provide a bit more insight, maybe also from their experience from qualitative results? Also, I was asking myself what happens if there is no ground truth available at the reference point since supervision usually comes from sparse LiDAR data?\n-\tSimilarly, could the authors maybe explain why it is beneficial to uniformly sample key points from the bounding box in BEV space instead of supervising throughout the whole area of the bounding box?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission371/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791262031,
            "cdate": 1698791262031,
            "tmdate": 1699635964034,
            "mdate": 1699635964034,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rxcjM2WTki",
                "forum": "9yKzVMxlkw",
                "replyto": "1pzC6StVQy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission371/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission371/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CVGt - Part1"
                    },
                    "comment": {
                        "value": "Thanks for your constructive and highly detailed comments that help us a lot.\n\n### **Q1. Compare with other related works and discuss differences.**\n\nThanks you very much for this suggestion, we make the comparison in **General Response to All Reviewers**, please refer to it for a more clear understanding. In the table, we list three relevant related works published in top-tier conferences and discuss our differences among these works. \n\n### **Q2: How generalizable the method is?**\n\nIt's an interesting topic. Your inquiry has provided valuable insights, motivating us to reevaluate the contributions of our method and explore potential synergies with other approaches. \n\nThe framework of the DETR-like series leverages implicit depth information and position encoding. In this paradigm, learnable queries, preset on the BEV grid, are employed to seek relevant image features based on their spatial location in the BEV space. This approach contrasts with the LSS-like methodology, where explicit depth information supervises the process, projecting image features onto the BEV. It's noteworthy that our supervision includes the relative depth within foreground objects, constituting a form of explicit guidance for depth prediction. \n\nHowever, the DETR-like approach encounters challenges when querying image features due to occlusion and other phenomena in the image. This often leads to inconsistencies between the detected image features and their counterparts in the BEV space, resulting in occasional false detections. Addressing these issues with implicit depth information remains a challenge.\n\nIn recent developments, we've observed innovative approaches, such as FB-BEV [1]. specifically, FB-BEV ultilizes BEVFormer with depth-aware module, like BEVDepth, integrates LSS-like module for explicit depth supervision to enhance DETR-like module. This method aims to harness the complementary strengths of both methods, yielding a more accurate BEV representation.\n\nSimilarly, we recognize the potential for enhancing the DETR-like approaches by incorporating our method and our experimental validation of this concept has yielded promising results. Essentially, our method acts as a plug-and-play module that seamlessly integrates with and improves the DETR-like approach by utilizing auxiliary inner-depth information, akin to the approach outlined in FB-BEV.\n\n\n|Method|$\\mathcal{L}^R_{\\rm{depth}}$ | $\\mathcal{L}_{\\rm{bev}}$ |mAP $\\uparrow$|NDS $\\uparrow$|\n|--- | --- | --- | --- | --- | \n|FB-BEV|  |  |32.8|41.5|\n|FB-BEV| $\\checkmark$|  |34.1|42.8|\n|FB-BEV| $\\checkmark$| $\\checkmark$|35.0|43.6|\n\n #### Note: For the table, we use ResNet50 as the backbone, the resolution is 256 $\\times$ 704. We don't use temporal information due to limited resources.\n\n ### **Q3: Minor comments and typos.**\n\n\nWe sincerely apologize for any writing issues in the previous version. We have diligently addressed all typos and continued fine-tuning the paper's writing to enhance its overall quality. Our team is actively engaged in preparing for the final submission, and we are committed to delivering a polished and refined manuscript. Thank you for your patience and guidance.\n\n\n> (1) The upper index A in the depth loss?\n\nThe upper index A in the depth loss means \"Absolute\", we use absolute depth supervision like BEVDepth. The loss was named with the first letter A of **A**bsolute to correspond and distinguish it from the **R**elative inner-depth supervision loss we proposed.\n\n> (2~5) A bit confusing to talk about 3d and 2d features; Include the math notation also in the Fig 4; Increase the font size of Fig 5,6 a bit; Report resolution and backbone in Tab 2.\n\nSorry for your confusion and thank you for your suggestion. We have made modifications according to it and updated in the latest manuscript.\n\n> (6) Include the latest works on multi-camera 3DOD in Tab 1,2.\n\nWe have updated Table 2 in the latest manuscript. For the sake of fairness in comparison, we have included the detailed table in Appendix B.2 (Table 8), as we did in the **General Response to All Reviewers**.\n\n> (7) Verify the method on more than one dataset.\n\nAs for other datasets, we have already shown the performance of our method on KITTI in Appendix B.2 (Table 10). \n\nIf you have any other suggestions, we welcome and appreciate your continued feedback."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission371/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596052194,
                "cdate": 1700596052194,
                "tmdate": 1700596052194,
                "mdate": 1700596052194,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JgDReZSboe",
            "forum": "9yKzVMxlkw",
            "replyto": "9yKzVMxlkw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission371/Reviewer_oss3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission371/Reviewer_oss3"
            ],
            "content": {
                "summary": {
                    "value": "this paper investigates the problem of BEV 3D object detection from multi-view RGB images. using the LiDAR as teacher supervision, this manuscript introduces a relative depth supervision and relationship matching for knowledge distillation. the effectiveness of the proposed method is verified on nuScenes dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ relatively easy to read\n+ good results\n+ ablation and variant study"
                },
                "weaknesses": {
                    "value": "the major issue that the reviewer has with this current manuscript is that it does not introduce significant new knowledge to the readers.\n- the proposed 'inner-depth supervision' is a object-normalized version of the absolute depth from BEVDepth. yes, normalizing the depth within the objects can make it easier to learn, where the absolute depth (normalized to 1) might range from 0.1 to 0.15, and the relative depth can be 0 and 1 correspondingly. it is great that the ablation shows improvement, but this is to be expected and well-recognised, and feels more like a trick. \n- the channel-wise and pixel-wise relationship supervision in Section 3.3 has been investigated by previous works [r1,r2,r3] on multiple tasks, so their capability in the BEV detection task (Table 3 and Table 5) is also somewhat expected. also, please refer to related works on 'relationship supervision', which could greatly benefit readers new to this field.\n\noverall, the reviewer does not feel entirely confident in recommending this manuscript at its current state.\n\n\n\n[r1]. Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. \"Image style transfer using convolutional neural networks.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2414-2423. 2016.\n\n[r2]. Tung, Frederick, and Greg Mori. \"Similarity-preserving knowledge distillation.\" In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1365-1374. 2019.\n\n[r3]. Hou, Yunzhong, and Liang Zheng. \"Visualizing adapted knowledge in domain transfer.\" In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 13824-13833. 2021."
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission371/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission371/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission371/Reviewer_oss3"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission371/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801698619,
            "cdate": 1698801698619,
            "tmdate": 1699635963932,
            "mdate": 1699635963932,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EFLMjPX2Df",
                "forum": "9yKzVMxlkw",
                "replyto": "JgDReZSboe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission371/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission371/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oss3"
                    },
                    "comment": {
                        "value": "Thanks for your recognition and valuable feedback.\n\n### **Q1. The proposed 'inner-depth supervision' is a object-normalized version of the absolute depth from BEVDepth?**\n\nWe are very sorry for the confusion. We propose the inner-depth supervision module, which is mainly motivated by focusing on the internal relationships of foreground objects. It's not a trick. The introduction of relative depth is different from normalization which amplifies the differences between similar depth clusters in a certain area. It does not involve any normalization operations. Insteadly, it establishes a connection between the depths of the internal points of foreground objects, rather than predicting the depth of each point in isolation. \n\nAbsolute depth supervision ignores the implicit relationships between points, while our inner-depth supervision aggregates the depths of points belonging to the same object, allowing the model to learn the relationships between internal points of objects and project object features more accurately into the BEV space. \n\nA more intuitive visualization after inner-depth supervision can be seen in Appendix B.2 (Fig 8), and even the depth prediction of the edge has become clearer.\n\n### **Q2. The channel-wise and pixel-wise relationship supervision has been investigated by previous works?**\n\nThanks for the additional references, we have added an extra paragraph of these related works in Appendix A according to your kind advice. \n\nWe agree that some related work has explored the relationship supervision between features in the fields of knowledge distillation and style transfer. These involve knowledge distillation between teachers and students of different scales or knowledge transfer between different domains, but few works have been done on different modalities of teachers and students, where one feature is learned from 2D data and the other from 3D data. Models under these two different modalities not only differ in model parameters and feature domains, but also in data format and source, which poses some difficulties that are not present in normal relationship learning. \n\nBesides, in normal knowledge distillation, direct feature alignment can be a good auxiliary method, but in our setting, differences between modalities can be harmful. Therefore, our work has conducted in-depth exploration on this issue. From the experimental results, it is evident that our method is feasible and can significantly outperform direct feature alignment, which not only meets expectations but also makes sense. \n\nIn terms of specific design, we have made detailed designs through extensive trials. We did not learn the relationship of the entire feature map, because a large amount of background noise can cause interference and heavy computational burden. Through exploration, we applied relationship supervision to foreground objects which is more efficient and can bring better improvement. Moreover, we used uniform sampling points to learn the relationship within foreground objects to save computational costs. In addition, we also found that learning both spatial correlation and channel-level correlation can complement each other. Finally, we found that learning feature correlation implicitly depends on previous depth prediction. The inner-depth supervision we used better constrained the feature projection at the foreground level, which further assisted in the learning of inner-feature. The combination of these two modules can achieve better results. Thus, our paper proposes a method that integrates and complements each other."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission371/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595959467,
                "cdate": 1700595959467,
                "tmdate": 1700595959467,
                "mdate": 1700595959467,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ExFFTbhLCt",
                "forum": "9yKzVMxlkw",
                "replyto": "EFLMjPX2Df",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission371/Reviewer_oss3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission371/Reviewer_oss3"
                ],
                "content": {
                    "title": {
                        "value": "rebuttal feedback"
                    },
                    "comment": {
                        "value": "the reviewer thanks the authors for their feedback. however, his concerns are not fully addressed.\n\nQ1. regarding the 'inner-depth supervision', the updated equations 3 and 4 verify the point raised by the reviewer that the 'inner-depth' is an object-normalized version of the absolute depth, albeit there is only subtraction of 'mean' (see the subtraction of the reference depth in Eq. 3) and no division by 'std'. the description \"amplifies the differences between similar depth clusters in a certain area\" also feels like per-object 'normalization', where the subtraction of a large reference value helps the network to focus on the nuances. overall, this per-object 'normalization' with subtraction still feels like a trick to the reviewer. \n\nthe reviewer has a different suspicion of the working mechanism underneath. enforcing the *softmax averaged* depth in Eq. 1 helps the categorical depth $D_i$ to be *softer* and provides more granularity (continuous) than splitting the depth into D bins (discrete). this in turn helps the BEV feature representation in LSS or BEVDet, which is also *softmax averaged* using the now potentially 'softer' or better categorical depth $D_i$. would it be possible to run ablations without the 'reference depth subtraction' in Eq. 3, so as to verify what is helping the system? the 'inner-depth' with reference depth subtraction, or enforcing depth in the *softmax averaged* fashion instead of D-bin classification?\n\n(p.s. some of the notations in the updated Sec 3.2 are redundant and overall it is very difficult to read. e.g., $S_j$ and $D_i$ seem to be from the same depth estimation but only with different subscripts. )\n\nQ2. the reviewer appreciates the newly added references to existing works. with that said, he still feels that this paper introduces very limited new knowledge in terms of the distillation method itself (Eq. 5-8 are all direct applications of [r2])."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission371/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616654488,
                "cdate": 1700616654488,
                "tmdate": 1700616654488,
                "mdate": 1700616654488,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Uo0B3PKtPG",
            "forum": "9yKzVMxlkw",
            "replyto": "9yKzVMxlkw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission371/Reviewer_CeGw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission371/Reviewer_CeGw"
            ],
            "content": {
                "summary": {
                    "value": "Recently, improving camera based 3D object detection by leveraging the LiDAR pure 3D information is get attention in the academia and industry. This paper belongs to this category. Overall, the paper proposed a new multi-view 3D detection via TARGET INNER-GEOMETRY LEARNING. Specifically, the authors proposed to use inner-depth supervision and inner-feature BEV distillation to boost the performance of mutli-view BEV detection. Benchmarked on the nuScenes dataset, it showed the better performance. Meanwhile, the authors conducted the ablation studies to make the work solid."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Good initiatives and motivation to resolve the issues in camera based 3D detection\n+ Good presentation with clear illustration to show the proposed method and great description of the proposed method (although it has some typos)\n+ Comprehensive results and ablation studies on the nuScenes datasets"
                },
                "weaknesses": {
                    "value": "I think the novelty and motivation of this paper is pretty clear and I really like the showcase in the Figure 1. However, I have several concerns in the experimental verification parts. \n\nThe first concern is the experimental dataset is pretty limited to nuScenes. I would like to see some experimental analysis on the Waymo One, KiTTI or 3D KiTTI dataset which is also the standard for the 3D detection. \n\nTo follow the first concern, some results in the paper is cherry picked, such as in Figure 7, the author wanted to show the improvement visually, however, in my opinion, it is pretty cherry picked. If the author could illustrate the performance of small and far object detection, it will be better. \n\nAnother big concern is the proposed method is only compared to the BEV4D/BEVDepth and its variances. The author did not list the latest method, such as Cross-Modality Knowledge Distillation Network for Monocular 3D Object Detection etc. Although the proposed method is different, these methods are in the same catrgory as the proposed method, that is L-> C. I would like to see the comparisons in this category."
                },
                "questions": {
                    "value": "Please check the weakness part and address the questions there. Overall I hope the author could present clearly and solid in the experimental results to make the paper as a strong submission."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission371/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698865234550,
            "cdate": 1698865234550,
            "tmdate": 1699635963851,
            "mdate": 1699635963851,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "66YURTL5yU",
                "forum": "9yKzVMxlkw",
                "replyto": "Uo0B3PKtPG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission371/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission371/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CeGw"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your recognition of our motivation, and your valuable feedback serves as a tremendous source of encouragement to us. We apologize for any concerns that may have arisen, and we are committed to addressing them.\n\n### **Q1. Experimental analysis on other 3D dataset.**\n\nWe are very sorry for the misunderstanding. We have conducted relevant experiments on the KITTI dataset before which can be found in Appendix B.2 (Table 10). With our proposed method, we can achieve a higher 3D AP compared to CMKD [1]. \n\n### **Q2. Illustrate the performance of small and far object detection.**\n\nThank you for your suggestions. We can illustrate the performance of small and far object detection from two perspectives:\n\n- **For qualitative analysis**, there are more visualization results shown on Figure 9 in Appendix B.2. These supplementary visualizations also serve to demonstrate that our proposed method is capable of mitigating a range of issues in detection, including the reduction of false positives and ghosting objects, as well as the refinement of certain 3D locations and orientations of bounding boxes. These benefits also apply to small and far objects.\n\n- **For quantitative analysis**, we conduct some additional studies and take BEVDet4D-R101 for an example.\n  - We consider small objects such as *pedestrians, motorcycles, bicycles, traffic cones, and barriers*, and evaluate the performance of small object detection by comparing the mAP across these categories:\n  \n    |Baseline|$\\mathcal{L}^A_{\\rm{depth}}$| $\\mathcal{L}^R_{\\rm{depth}}$ | $\\mathcal{L}_{\\rm{bev}}$ |mAP$_{pedestrian} \\uparrow$|mAP$_{motorcycle} \\uparrow$|mAP$_{bicycle} \\uparrow$|mAP$_{traffic\\ cone} \\uparrow$|mAP$_{barrier} \\uparrow$|\n    |--- | --- | --- | --- | --- | --- | --- | ---|---|\n    |BEVDet4D-R101|  | | |43.3|34.5|32.4|54.8|55.2\n    |BEVDet4D-R101| $\\checkmark$| | |44.6|35.6|32.8|56.3|55.9\n    |BEVDet4D-R101| $\\checkmark$| $\\checkmark$||45.4|37.3|33.7|57.8|58.8|\n    |**BEVDet4D-R101**| **$\\checkmark$**| **$\\checkmark$**| **$\\checkmark$**|**45.7**|**38.8**|**37.5**|**58.9**|**57.2**\n    \n    As demonstrated in the table, our proposed method enables more accurate detection of small objects.\n    \n  - We use the distance between the object and the ego in the ego coordinate system to filter out far objects, and evaluate the performance of far object detection by comparing the mAP across different distance ranges:\n    \n    |Distance (m)|Baseline|$\\mathcal{L}^A_{\\rm{depth}}$| $\\mathcal{L}^R_{\\rm{depth}}$ | $\\mathcal{L}_{\\rm{bev}}$ |mAP $\\uparrow$|NDS $\\uparrow$|\n    |--- | --- | --- | --- | --- | --- | --- |\n    |[0,30)|BEVDet4D-R101|  | | |44.2|53.7|\n    |[0,30)|BEVDet4D-R101| $\\checkmark$| | |46.6|55.6|\n    |[0,30)|BEVDet4D-R101| $\\checkmark$| $\\checkmark$||47.4|55.8|\n    |**[0,30)**|**BEVDet4D-R101**| **$\\checkmark$**| **$\\checkmark$**| **$\\checkmark$**|**48.6**|**56.8**|\n    |[30,60)|BEVDet4D-R101|  | | |10.2|28.7|\n    |[30,60)|BEVDet4D-R101| $\\checkmark$| | |10.3|28.5|\n    |[30,60)|BEVDet4D-R101| $\\checkmark$| $\\checkmark$||11.7|30.1|\n    |**[30,60)**|**BEVDet4D-R101**| **$\\checkmark$**| **$\\checkmark$**| **$\\checkmark$**|**12.6**|**30.2**|\n    \n    As shown in the table, our method can improve detection performance even for distant objects.\n\n### **Q3. Compare with other LiDAR-to-Camera Methods.**\n\nPlease refer to **General Response to All Reviewers**, we list some latest works in the table. We also compare with CMKD as said in **Q1**. We're sorry for any concern this may have caused you.\n\n### Reference:\n\n[1] Hong Y, Dai H, Ding Y. Cross-modality knowledge distillation network for monocular 3d object detection[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022: 87-104."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission371/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595900448,
                "cdate": 1700595900448,
                "tmdate": 1700595900448,
                "mdate": 1700595900448,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nejLTR8C5i",
            "forum": "9yKzVMxlkw",
            "replyto": "9yKzVMxlkw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission371/Reviewer_v25e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission371/Reviewer_v25e"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel approach to 3D object detection, emphasizing the improvement of camera-based detectors. The proposed method introduces \"Inner-depth Supervision\" and \"Inner-feature BEV Distillation\" techniques to enhance the learning of spatial structures and depth perception. The main results show a comprehensive performance evaluation on the nuScenes test set, where the authors compare their method with established benchmarks across several metrics, demonstrating its effectiveness. The ablation study likely delves into the specific contributions of different method components, though the exact details weren't extracted here."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The manuscript presents innovative techniques such as \"Inner-depth Supervision\" and \"Inner-feature BEV Distillation\", indicating a high degree of originality by potentially filling a gap in the camera-based 3D object detection literature. The quality of research seems robust, as evidenced by comprehensive evaluations and methodical ablation studies. Clarity, while harder to fully assess without the complete text, appears to be aided by the use of illustrative figures and a structured presentation of methods and results. The significance of the work is underlined by its potential to enhance the practicality and cost-effectiveness of 3D object detection systems, which could have far-reaching implications for autonomous driving and robotics. This paper could represent a valuable contribution to the field, provided the results hold under peer review and the methods are as scalable and adaptable as implied."
                },
                "weaknesses": {
                    "value": "To improve the paper, the authors could expand the methodology section, provide additional experimental results, and include more thorough comparisons with state-of-the-art techniques. Moreover, an in-depth discussion of the limitations and broader implications of the work would add value and show the authors' comprehensive understanding of their method's place in the field."
                },
                "questions": {
                    "value": "Can you provide additional details on the mathematical formulation and implementation details of the \"Inner-depth Supervision\" and \"Inner-feature BEV Distillation\" techniques? \nThe paper mentions results on the nuScenes test set. Have you evaluated your method on other datasets or in varied real-world conditions to test its generalizability?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "none."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission371/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699073330183,
            "cdate": 1699073330183,
            "tmdate": 1699635963781,
            "mdate": 1699635963781,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rfoPaSY5TA",
                "forum": "9yKzVMxlkw",
                "replyto": "nejLTR8C5i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission371/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission371/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer v25e"
                    },
                    "comment": {
                        "value": "Thank you for your positive and thorough review of our paper. We greatly appreciate you taking the time to provide such thoughtful feedback.\n\n### **Q1: Comparisons with state-of-the-art techniques.**\n\nThank you for your advice. We have made a comparison with other state-of-the-art techniques in **General Response to All Reviewers**. Besides, we also found that our method is not only highly competitive in 3D detection tasks, but camera-based models pre-trained with our method can also serve as better backbones for other 3D camera-only tasks, such as 3D occupancy prediction task.\n\n### **Q2: An in-depth discussion.**\n\nThank you for your valuable insight and we have added it in Appendix B.4.\n\n- One limitation that will inevitably be involved in depth-related explicit supervision is the ground truth from LiDAR. Due to the sparsity of LiDAR, both absolute depth supervision and relative depth supervision will be affected to some extent, though the performance of object detection will be better with inner-depth supervision under the same conditions. For sparse point clouds, a good way to alleviate it is through depth completion.\n\n- Another limitation is the occlusion problem of the target. Due to visual occlusion, the ground truth of the occluded object that we can obtain is limited, which will affect the learning of depth prediction and inner-depth learning. A good way to alleviate it is to consider multi-frame images for inner-depth supervision of the same object's interior, but a potential risk here is that if the intrinsic and extrinsic parameters are inaccurate, it will directly affect the coordinate system transformation between multiple frames, thereby affecting the ground truth of depth values.\n\n### **Q3: Evaluated method on other datasets.**\n\nThanks for your suggestion. We have evaluated our method on KITTI before and results can be seen in Appendix B.2 (Table 10).\n\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n\nWe also have further added details, refined our approach and conducted new experiments to address concerns raised by other reviewers. The revisions have improved the quality and clarity of our work. We hope that you will find our changes satisfactory and look forward to hearing your assessment of the revised manuscript."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission371/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595806115,
                "cdate": 1700595806115,
                "tmdate": 1700595806115,
                "mdate": 1700595806115,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]