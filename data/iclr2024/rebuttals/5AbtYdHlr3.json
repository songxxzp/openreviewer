[
    {
        "title": "Stochastic Safe Action Model Learning"
    },
    {
        "review": {
            "id": "Ha775ix4zo",
            "forum": "5AbtYdHlr3",
            "replyto": "5AbtYdHlr3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4152/Reviewer_Ksk7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4152/Reviewer_Ksk7"
            ],
            "content": {
                "summary": {
                    "value": "This study investigates a very interesting topic and introduces an algorithm for learning stochastic planning models, specifically targeting domains with dynamics that are challenging to model manually. The proposed approach could efficiently learn from example trajectories, ensuring accurate and safe action modeling."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The research topic is interesting.\n2. The theoretical analysis sounds good."
                },
                "weaknesses": {
                    "value": "1. The manuscript requires substantial improvements in writing quality, with an emphasis on a more coherent logical structure.\n2. The paper contains numerous grammatical errors, even within the abstract. For instance, on page 1, there's a repeated \"the\", \"model\" in the abstract should be \"models\", \"at some point\" should be \"at some points\", and \"some other condition is satisfied\" should be \"some other conditions are satisfied\".\n3. Ensure that abbreviations are expanded upon their first use, for example, \"PPDDL\".\n4. Once an abbreviation has been defined, it's redundant to reintroduce it; consider the case with \"Stochastic Safe Action Model (SAM)\".\n5. The experimental section is lacking, making it challenging to evaluate the method's effectiveness."
                },
                "questions": {
                    "value": "1. Could you clarify the meaning of \"IPC probabilistic tracks\"?\n2. Is there a correlation between the level of stochasticity and model performance?\n3. What is the relationship between effect probabilities and sample complexity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698536126281,
            "cdate": 1698536126281,
            "tmdate": 1699636380919,
            "mdate": 1699636380919,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vFdphE3eNP",
                "forum": "5AbtYdHlr3",
                "replyto": "Ha775ix4zo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4152/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the review. The following are the answers to the questions.\n\nIPC probabilistic tracks is a set of benchmark datasets for planning, including parking, satellite, and transport planning. In these real world planning problems, the number of effects for each action is less than 5. The number of effects is the tensor rank in our formulation. Constantly small rank is a key requirement in our algorithm.\n\nThe second question is not very clear to us. What do you mean by \u201cthe level of stochasticity\u201d?\n\nThe sample complexity does not depend on true effect probabilities values, but the confidence threshold and success rate. If the number of samples is large enough then the computed effect probabilities will be closer to the true probabilities."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544101975,
                "cdate": 1700544101975,
                "tmdate": 1700544101975,
                "mdate": 1700544101975,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pJPo2Nlwhn",
            "forum": "5AbtYdHlr3",
            "replyto": "5AbtYdHlr3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4152/Reviewer_cva7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4152/Reviewer_cva7"
            ],
            "content": {
                "summary": {
                    "value": "The focus of the paper is on model learning in stochastic PPDDL. Here, the overarching goal is to learn a model of the domain from trajectories. The model here specifically refers to a set of preconditions and effects of taking a particular action. The trajectories are executed with a set of policies in a domain with discrete states. Each state is characterized by a set of boolean fluents. The goal of the paper is to learn a stochastic model where the probability of each effect is extracted from the data. Previous work in this setting provides safety and approximate completeness guarantees by assuming that each effect\u2019s action on each fluent is an independent random variable. This assumption eases the analysis. In contrast, this paper attacks a more challenging case using tools from tensor algebra. By performing a low-rank decomposition of the transition probability tensor using the method of moments, the authors are able to extract a model that is shown to satisfy safety and approximate-completeness criteria."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The contribution is novel, clear and significant. The idea of using tensor decompositions for PDDL has not been explored.\n\n2) The method is theoretically sound."
                },
                "weaknesses": {
                    "value": "1) The presentation and clarity needs significant improvement. As a standalone contribution, the paper should be more rigorous in terms of presentation and lacks a diligent writing style. A more scrupulous approach to explaining all the math will help presenting the paper (with the appendix).\n\n2) It would be nice if half a page of the paper is delegated to demonstration of the method on one dataset.\n\n3) More preliminaries and related work on the method of moments algorithm applicable to tensors is encouraged. The related work section only attributes around five papers."
                },
                "questions": {
                    "value": "1) I have some questions surrounding Lemma 1. I believe $|S|$ denotes the number of distinct elements in $S$. Is there any reason why the elements of $V$ would not be distinct? Are they necessary to be all distinct? Is all that is sufficient is that $rank(V)=r$ where $r$ satisfies Lemma 1? In that case, $rank(V)+2 rank(V^{\\otimes k}) \\geq 3r$? This part is unclear to the reader.\n\n2) More illustrations similar to section 3.2 equations (2) and (3) will help improve clarity.\n\n3) Section 4.1 is not explained properly and there are some cyclical arguments. Given that these are mainly a variation of Jennrich\u2019s algorithm, a preliminaries section can help ease the exposition.\n\n4) There is no explanation of what is a \u201cgeneric\u201d tensor? Is the qualification in Kruskal\u2019s theorem?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698590285897,
            "cdate": 1698590285897,
            "tmdate": 1699636380844,
            "mdate": 1699636380844,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "voCMQdaQKA",
                "forum": "5AbtYdHlr3",
                "replyto": "pJPo2Nlwhn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4152/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the review. Here are some explanation of the details of our tensor decomposition based method.\n\nThe tensor decomposition will not return a non-distinct set of vectors V, because if there are two identical vectors, their weights will simply merge together in the decomposition, as we are seeking the minimum number of decomposed vectors.\n\n$Rank(V)=r$ would indeed be a sufficient condition but it\u2019s generally not true. We do not make such assumptions but instead raise the moment to a degree that is large enough to ensure such rank lower bound. \n\nCould you please point out some of the cyclical arguments in Sec.4.1. so that we can further clarify? \n\nGeneric tensor is the set of all tensors with the exclusion of a measure zero set. It is a term that has been used by the literature for the convenience of mathematical proof. We also found this term ambiguous since binary valued tensors can possibly fall into this set of measure zero. Therefore, we seek to provide a sufficient guarantee for the uniqueness of tensor decomposition in Sec.2.2, and Sec. 2.3."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543966885,
                "cdate": 1700543966885,
                "tmdate": 1700543991274,
                "mdate": 1700543991274,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h2GBjAe0ms",
                "forum": "5AbtYdHlr3",
                "replyto": "voCMQdaQKA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4152/Reviewer_cva7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4152/Reviewer_cva7"
                ],
                "content": {
                    "comment": {
                        "value": "Overall, after reading all the reviews and author comments, i believe there is progression over the discussion phase. However, I would like to retain my scores as the initial submission was unnecessarily cryptic and there are significant changes needed to improve the score further,"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703481051,
                "cdate": 1700703481051,
                "tmdate": 1700703481051,
                "mdate": 1700703481051,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BQjsU1Ierl",
            "forum": "5AbtYdHlr3",
            "replyto": "5AbtYdHlr3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4152/Reviewer_YR2D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4152/Reviewer_YR2D"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an approach based on tensor decomposition for learning stochastic action models for symbolic planning. The problem is really relevant and important, given the amount of work going on in different fields model learning like learning abstractions or learning symbolic models. \n\nThe paper theoretically shows that the learned model is safe (or conservative) in terms of the action only applicable in a state if and only if it is permissible in the true model (but given that they learn a conservative model from a set of only positive trajectories this is not surprising)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The problem is relevant, important and unsolved. \n\n- The approach is theoretically sound and strong."
                },
                "weaknesses": {
                    "value": "As I mentioned, the problem is really interesting. However, the paper is equally inaccessible to a reader. The low novelty score given is because even though the paper may have novel contributions, these are not understandable for the reader. \n\n- There are many unsubstantiated claims in the paper. Theorems and Lemmas in the paper have almost no explanations.  While I support having theoretical results in the paper, they should be complete. The readers should not be left reading some previous work to understand even the basic premise of the theoretical results of the paper (in this case [Juba and Stern, 2022]) as the paper  does not have proofs for theorems and lemmas (Theorem under 2.2, Lemma 1, Theorem 1, and Theorem 2) or defer proofs to previous work. \n\n- The notations are non-intuitive. For, e.g., the preliminaries section is meant to be make the rest of the paper understandable. However, they have unproven lemmas and theorems as well as equations with undefined symbols (superscript cross d ). In Theorem under Sec 2.2, what are a_k,b_k and c_k? \n\n- The paper attempts to solve a very intuitive problem with a very non-intuitive approach. The most intuitive thing  would have been to include a running example that makes it really easy for the readers to follow. \n\n- The next big problem with the paper is a lack of empirical evaluation. Without an empirical evaluation, there is no practical explanation to if the approach is feasible for learning real world domain models. There are plenty of PPDDL domains available to learn. \n\n- The paper presents a similar functional approach as [Juba and Stern 2022] with near similar theoretical guarantees. It is not clear from the paper what is the motivation behind a different approach without any significant improvements."
                },
                "questions": {
                    "value": "Please refer to the weaknesses highlighted in the previous section. \n\nThe most important question is: \n\n- Would it be possible to provide a running example in the **main paper** to help the reader understand the paper as it is currently extremely difficult to understand. \n- Why was not empirical evaluation provided and would it be possible to provide empirical evaluation on standard PPDDL domains?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4152/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4152/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4152/Reviewer_YR2D"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779378742,
            "cdate": 1698779378742,
            "tmdate": 1699636380758,
            "mdate": 1699636380758,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OJDONcdzqf",
                "forum": "5AbtYdHlr3",
                "replyto": "BQjsU1Ierl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "It's not clear if this problem is intuitive"
                    },
                    "comment": {
                        "value": "We appreciate the interest in the symbolic learning aspect of our paper. \n\nHowever, we would like to point out that our solution to this problem is not very intuitive. Firstly, even if the learning data consist of only positive trajectories, the success rate can still go wrong if the learned effect distribution of action model is off. Besides, the sampled policy that we used to produce the trajectories in the training set does not always end up reaching the goal state: the agent can simply reach the maximum number of steps and stop (please see the end of Sec. 2.1).\n\nThe theorem under Sec.2.2 and Lemma 1 are classical tensor decomposition results. Theorem 1,2 both require long proofs which are not the focus of this paper. As long as we obtain the learning guarantee of the action\u2019s effect distribution, we can plug it into their proof without much change. The main focus in this paper is how to learn each action\u2019s effects without the strong assumption of independence of the environment factors.\n\nThe superscript cross d means doing outer product with itself d times. $a_k, b_k, c_k$ are column vectors of the matrix.\n\nAgain, this paper presents a completely different learning approach than Juba & Stern 2022. We are solving a problem with a more general setting, where the probability of the change of each environmental factor given an action is not necessarily independent of those of other factors (please also see our answer to reviewer bFXN)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543816515,
                "cdate": 1700543816515,
                "tmdate": 1700543816515,
                "mdate": 1700543816515,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "koHotKKw2M",
                "forum": "5AbtYdHlr3",
                "replyto": "OJDONcdzqf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4152/Reviewer_YR2D"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4152/Reviewer_YR2D"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I thank the author for their response. They have tried to address some of the issues raised. However, most issues (even which they tried to resolve) are still unresolved. \n\n- The paper needs to be very clear about differences between Juba & Stern 2022 and the submitted work which it isn't. Even the response to the review is unclear. \n\n- The paper needs to be self sufficient, again, which it isn't. The presented theoretical paper heavily relies on Juba & Stern 2022 for most of the proofs which puts a lot of effort on the side of the readers. I do not think this is allows this paper to be published. \n\n- The authors claim that this paper solves the problem of learning a stochastic model in a  more general setting that Juba & Stern 2022, but imposes even stricter assumptions on the number of effect facts each action can have. They also fail to clearly outline this in the paper. \n\n- The authors in their response fails to acknowledge or comment on the need of a running example. \n\nDue to all this issues, I will like to keep my score as it is."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655934905,
                "cdate": 1700655934905,
                "tmdate": 1700655934905,
                "mdate": 1700655934905,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PAY2hkrnW8",
                "forum": "5AbtYdHlr3",
                "replyto": "BQjsU1Ierl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further clarification"
                    },
                    "comment": {
                        "value": "Thank you for the response. We would strongly recommend reading our answer to reviewer bFXN, as it clarifies the difference between our paper and Juba & Stern 2022, and more importantly, why their theoretical proof of safety and completeness is not our focus and we do not heavily rely on that. Regardless, we will simply repeat it as follows.\n\nPlease note that the learning algorithm we propose has very little to do with the learning method proposed by Juba & Stern (2022). Their learning algorithm is simply computing the empirical estimates of $Pr( \\ell\u2019|a, \\ell)$ for all the environment factor $\\ell\u2019$ and $\\ell$ to reconstruct the transition probabilities $Pr(s\u2019 | a, s)$, which is a product of $Pr( \\ell\u2019|a, \\ell)$ probabilities, since they assume that the environment factors \\ell are independent. This is not the case in our problem because we lifted the restriction of the factors being independent. If we simply compute the empirical estimates of all the transition probabilities $Pr(s\u2019 | a, s)$, it will be infeasible since the number of states is exponentially large. Our tensor decomposition based learning method allows us to infer the set of effects of each action by only estimating a polynomial number of moments with bounded degree. \n\nThe key that enables the use of Juba & Stern (2022)\u2019s proof of safety and approximate completeness is the learning guarantee of each action\u2019s set of effects in Alg.1 in this more general stochastic action model, which is the focus of this paper. Juba & Stern (2022) did not need such guarantee for their proof of safety or completeness, since their strong assumption of the independence of state factors allows them to simply learn the action\u2019s effect on each factor independently.\n\nBecause we do not assume such independence. Our setting is more general. The assumption of small number of effects for each action is justified in all the realistic benchmark datasets for planning in IPC probabilistic tracks, including parking, satellite, and transport planning, where the number of effects for each action is less than 5. \n\nThank you for the response! As for running example, please see illustrations in section 3.2 equations (2) and (3), and a toy example that we included in Appendix C of the revision."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700811775,
                "cdate": 1700700811775,
                "tmdate": 1700700891301,
                "mdate": 1700700891301,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "N921nNgH6q",
            "forum": "5AbtYdHlr3",
            "replyto": "5AbtYdHlr3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4152/Reviewer_bFXN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4152/Reviewer_bFXN"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce the problem of learning an action model in a stochastic environment of a PPDDL-type planning problem. Unlike the more standard MDP formulations of RL, here the state formulation consists of a set of 'fluents' which take boolean values, and the action model describes which 'effects' can follow after taking certain actions in given 'preconditions'. Compared to previous research in learning action models, in their formulation, the stochasticity of the effects that follow certain actions can be more general. The authors then show that, under these assumptions of the stochasticity, following closely the methodology of Juba & Stern (2022), they can learn an action model using tensor decomposition. They analyze the method and show that it can be used to achieve a particular notion of 'safety' and 'approximate completeness'."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The authors come up with an algorithm to learn the action model and they can then guarantee \"safeness\" and the \"approximate completeness\" of the approach."
                },
                "weaknesses": {
                    "value": "* The paper is unnecessarily dense at times, please consider the use of examples and captions to illustrate the main ideas, especially to new audiences.\n\n* No experiments were performed to show the benefits of the introduced algorithm.\n\n* It is not clear at times what the contribution is compared to Juba & Stern 2022 paper. It seems that all the proof techniques rely on that previous paper. In particular note the last sentence of the paper: \"The only difference\nbetween the proofs of these theorems and Juba & Stern (2022) is that we change the dependence on\nthe number of fluents |F | to the dependence on the number of effects |F |O(log r).\"\n\n* It is not clear if the stochastic model considered reflects real-world problems accurately. In particular it would be nice for the authors to give an example of a real-world problem that is captured by the particular stochastic model."
                },
                "questions": {
                    "value": "* I'm not sure that ICLR is a good conference to submit this type of paper, it seems rather to belong to the more standard AI/planning-focused conferences.\n\n* Is the Algorithm1 the authors' contribution, or is it also based on the Juba & Stern (2022) paper?\n\n* It's not clear if the proposed algorithm would actually run on a computer. Have the authors tried to do so? Are there any complications?\n\n* Minor comment: Two 'the's in the first sentence."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698918399352,
            "cdate": 1698918399352,
            "tmdate": 1699636380668,
            "mdate": 1699636380668,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KaIcLoiHbG",
                "forum": "5AbtYdHlr3",
                "replyto": "N921nNgH6q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The focus of this paper is not repeating the proof of Juba & Stern (2022)"
                    },
                    "comment": {
                        "value": "Thank you for pointing out the confusing part of this paper. \n\nWhen you say \u201cmethodology of Juba & Stern (2022)\u201d, do you mean the learning method or the method to prove safety and approximate completeness? \n\nIf you meant the learning method, please note that our assumptions of a more general stochasticity does not help us learn, and the learning algorithm we propose has very little to do with the learning method proposed by Juba & Stern (2022). Their learning algorithm is simply computing the empirical estimates of $Pr( \\ell\u2019|a, \\ell)$ for all the environment factor $\\ell\u2019$ and $\\ell$ to reconstruct the transition probabilities $Pr(s\u2019 | a, s)$, which is a product of $Pr( \\ell\u2019|a, \\ell)$ probabilities, since they assume that the environment factors \\ell are independent. This is not the case in our problem because we lifted the restriction of the factors being independent. If we simply compute the empirical estimates of all the transition probabilities $Pr(s\u2019 | a, s)$, it will be infeasible since the number of states is exponentially large. Our tensor decomposition based learning method allows us to infer the set of effects of each action by only estimating a polynomial number of moments with bounded degree. \n\nIf you meant the latter, then please note that the key that enables the use of Juba & Stern (2022)\u2019s proof of safety and approximate completeness is the learning guarantee of each action\u2019s set of effects in Alg.1 in this more general stochastic action model, which is the focus of this paper. Juba & Stern (2022) did not need such guarantee for their proof of safety or completeness, since their strong assumption of the independence of state factors allows them to simply learn the action\u2019s effect on each factor independently.\n\nPlease note that PPDDL has an established action model that applies to the real world. Our model is strictly more general, which at least captures these real world situations and beyond.\n\nWe agree that experiments will make this work stronger. However, the focus of this work is to derive a provable learning guarantee for the complex environment we consider, where it was not clear even a polynomial-time learning algorithm exists for this setting."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543195291,
                "cdate": 1700543195291,
                "tmdate": 1700543195291,
                "mdate": 1700543195291,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0QLMaGvMFK",
                "forum": "5AbtYdHlr3",
                "replyto": "KaIcLoiHbG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4152/Reviewer_bFXN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4152/Reviewer_bFXN"
                ],
                "content": {
                    "title": {
                        "value": "my rating remains the same"
                    },
                    "comment": {
                        "value": "I thank the authors for their reply, in particular the sentence beginning with: \"Their learning algorithm is simply computing the empirical estimates...\" was quite clear and made me understand the difference in methodology between Juba & Stern (2022) and the current paper.\n\nI would have wished the paper to be equally clear but alas, it was quite difficult for me to make sense of the method and the proof. As the other reviewers also mentioned, a simple toy example together with some simulated experiments would greatly help. As of now, it is not suitable for publication in ICLR, which is a very interdisciplinary conference (as most ML conferences are) and requires very clear writing (such that people from many different backgrounds can understand the content). I keep my score as a result unchanged."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661894353,
                "cdate": 1700661894353,
                "tmdate": 1700661894353,
                "mdate": 1700661894353,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]