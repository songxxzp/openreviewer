[
    {
        "title": "$\\sigma$-zero: Gradient-based Optimization of \\\\$\\ell_0$-norm Adversarial Examples"
    },
    {
        "review": {
            "id": "S8jjLikZA0",
            "forum": "dXRWP4n15q",
            "replyto": "dXRWP4n15q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4978/Reviewer_sy67"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4978/Reviewer_sy67"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose a $l_0$-norm attack, called $\\sigma$-zero, which leverages an differentiable approximation of the $l_0$  norm to facilitate gradient-based optimization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper's primary contribution lies in its application of the $l_0$ norm approximation function, as introduced by Osborne et al. (2000b), to $l_0$ attacks. The research presents thorough experiments on various robust models (e.g., C1-C8) presented in Robustbench and multiple datasets. \n\nWhen compared with existing sparse attacks, the results convincingly demonstrate that sigma-zero outperforms in terms of attack rates and offers reduced computational costs.\n\nWhile the authors have included the code in the supplementary materials, I haven't personally tested it. Nonetheless, I anticipate that the broader community will benefit once the authors make their code publicly available in the future."
                },
                "weaknesses": {
                    "value": "A primary shortcoming of the paper is its resemblance to an experimental or technical report rather than a comprehensive academic study.\n\nThe discussion about the scientific principles about why $\\sigma$-zero performs better is quite sparse in the current presentation.\n\nAdditionally, the term \"VRAM\" is not defined. It would enhance clarity if its full name were provided initially for the readers not familiar with."
                },
                "questions": {
                    "value": "see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4978/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799339267,
            "cdate": 1698799339267,
            "tmdate": 1699636485299,
            "mdate": 1699636485299,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xSLC351Odo",
                "forum": "dXRWP4n15q",
                "replyto": "S8jjLikZA0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4978/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4978/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sy67"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the value of our contribution and its benefit to the community.\n\n**Writing style**. We acknowledge the constructive criticism of the reviewer, and will improve our writing style by better highlighting the contributions and novelty of our work, as well as the reasons behind why the attack works well. Our purpose was genuinely to make our work reproducible, as also encouraged by the conference policy. This is why we reported too many results and technical details in the paper. However, we are committed to enhancing the paper's accessibility and will relocate the more technical details and results to the appendix. \n\n**sigma-zero results.** We will improve the paper emphasizing the main contributions and condensing their descriptions in a well-identifiable paragraph in the paper. In particular, we posit that several factors contribute to the effectiveness of $\\sigma$-zero, including the type of approximation used for the L0 norm, the novel loss formulation presented in Eq. 7, and the incorporation of gradient normalization in our optimization process. To elaborate further, we will clarify how the L0-norm approximation can enhance the effectiveness of our approach, offering a numerically stable method for minimizing L0 in adversarial perturbations. Additionally, we will detail the characteristics of the loss formulation introduced in Equation 7, explaining that it is carefully designed such that its solution corresponds to a minimum L0-norm perturbation. Finally, we will delve into the role of gradient normalization towards ensuring convergence during our optimization process. \n&nbsp;\n\n**VRAM.** We will also revise the paper to make clear that VRAM refers to Video Random Access Memory, and its role. Specifically, VRAM is a type of memory designed explicitly for use in graphics processing units (GPUs). In machine learning, GPUs have become increasingly important due to their parallel processing capabilities, making them well-suited for tasks involving large-scale mathematical computations, such as those found in machine learning algorithms.\n\nWe will add these considerations to the paper and would be happy to know if the reviewer has any other requests. We would appreciate a score increase from the reviewer if he/she finds our clarifications satisfactory. We remain available for further clarification."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700060884940,
                "cdate": 1700060884940,
                "tmdate": 1700060884940,
                "mdate": 1700060884940,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BuPGRRBjCX",
                "forum": "dXRWP4n15q",
                "replyto": "xSLC351Odo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4978/Reviewer_sy67"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4978/Reviewer_sy67"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the response.\n\nI gone through the opinion from other reviewers and realize the issue raised in the experiments on Sparse RS. I appreciate the effort made by the authors in addressing this issue. I intend to keep my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711341251,
                "cdate": 1700711341251,
                "tmdate": 1700711341251,
                "mdate": 1700711341251,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jBzWkNKWaP",
            "forum": "dXRWP4n15q",
            "replyto": "dXRWP4n15q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4978/Reviewer_oGkp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4978/Reviewer_oGkp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes $\\sigma$-zero attack, a sparse attack gained by minimizing the norm-0 of the perturbation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea is simple and intuitive. \n\n- The experimental results look good."
                },
                "weaknesses": {
                    "value": "-  The adversarial images created by the proposed approach do not look visually appealing according to Figure 3. It is easy to spot out highly-intensive pixels. \n\n-  The purpose of adversarial attacks is to generate visually appealing images that is imperceptible to human vision. Therefore, less norm-0 perturbations do not mean better visually appealing adversarial images. I feel that although this approach can help to restrict the number of pixels perturbed, it tends to perturb other pixels more, leading to adversarial images as in Figure 3.\n\n- The norm-0 solely is not adequate to measure the quality of generated adversarial images. The norm-0 of the proposed approach is smaller because it directly minimize an approximation of the norm-0. It would be better if the paper reports some other metrics such as  SSIM, PSNR, and LPIPS.\n\n- Moreover, the robust classifiers in RobustBench are trained mostly with $\\ell_\\infty$ and $ell_2$, hence they cannot defend well the proposed approach. What does it happen if we train a robust classifier with 20-steps $\\sigma$-zero and then evaluate on the same attack with 100 steps?"
                },
                "questions": {
                    "value": "Please refer to the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4978/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698892429176,
            "cdate": 1698892429176,
            "tmdate": 1699636485220,
            "mdate": 1699636485220,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sySqVCGPD9",
                "forum": "dXRWP4n15q",
                "replyto": "jBzWkNKWaP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4978/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4978/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oGkp"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the simplicity and effectiveness of our proposed attack, and reply to their concerns below. \n&nbsp;\n\n**Visually-imperceptible Adversarial Examples.** We aim to clarify with the reviewer that visual imperceptibility of adversarial examples is considered now a known misconception [S0,S1]. In fact, as highlighted by the examples reported in [S0,S1], including image spam and visibly-modified traffic signs, the perturbations are clearly visible to the human eye (even though the main content is still preserved), as their sole purpose is to evade automatic detection. In addition, quoting Justin Gilmer et al. [S0]: \u201cat the time of writing, we were unable to find a compelling example that required indistinguishability.\u201d Recall indeed that indistinguishability is a much stricter requirement than just preserving the main content (e.g., the ability of humans to still recognize the correct object in the image). This is also consistent with the definition of adversarial example given by Ian Goodfellow, which does not even mention the requirement of visual indistinguishability: \u201cAdversarial examples are inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake\u201d. Accordingly, we can say that the goal of an adversarial example is simply to fool the machine-learning model, and not necessarily be imperceptible to the human eye. This is especially true for sparse (L0 and L1) attacks, as they are constrained to only manipulate a few input values, but it is also an issue that affects all previous work related to sparse attacks, not just ours [S2, S3, S4, S5]. In fact, by inspecting the attacks proposed in previous work, it is not difficult to see that L0 and L1 adversarial perturbations are always clearly visually distinguishable (see, e.g., Figure 3 in our paper). Their goal, indeed, is not to be indistinguishable to the human eye, but rather show whether and to which extent models can be fooled by just changing a few input values -- i.e., to evaluate their adversarial robustness against sparse perturbations. We hope that the reviewer agrees with this perspective and, if necessary, we will take this opportunity to clarify this aspect in the paper too.\n\n[S0] J. Gilmer, R. P. Adams, I. J. Goodfellow, D. Andersen, and G. E. Dahl. Motivating the rules of the game for adversarial example research. CoRR, abs/1807.06732, 2018.\n\n[S1] Biggio, Battista and Fabio Roli. \u201cWild Patterns: Ten Years After the Rise of Adversarial Machine Learning.\u201d Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (2017).\n\n[S2] Croce, Francesco and Matthias Hein. \u201cMind the box: l1-APGD for sparse adversarial attacks on image classifiers.\u201d International Conference on Machine Learning.\n\n[S3] Modas, Apostolos et al. \u201cSparseFool: A Few Pixels Make a Big Difference.\u201d 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\n\n[S4] Croce, Francesco et al. \u201cSparse-RS: a versatile framework for query-efficient sparse black-box adversarial attacks.\u201d AAAI Conference on Artificial Intelligence (2020).\n\n[S5] Su, Jiawei et al. \u201cOne Pixel Attack for Fooling Deep Neural Networks.\u201d IEEE Transactions on Evolutionary Computation 23 (2017).\n\n&nbsp;\n\n**Robust Models.** As suggested by reviewer g7Nu, we now have integrated two more models trained to be robust against sparse attacks [S6, S7]. The results have been reported above in  our response to all reviewers (https://openreview.net/forum?id=dXRWP4n15q&noteId=JKLyN3Qnl9). \nEven against such robust models, $\\sigma$-zero continues to outperform existing attacks, exhibiting higher attack success rates, lower distances, and faster execution. We will add these experiments and considerations to the paper and would be happy to know if the reviewer has any other requests. \n\n[S6] Croce, Francesco, and Matthias Hein. \"Mind the box: -APGD for sparse adversarial attacks on image classifiers.\" International Conference on Machine Learning. PMLR, 2021.\n\n[S7] Jiang, Yulun, et al. \"Towards Stable and Efficient Adversarial Training against  Bounded Adversarial Attacks.\" International Conference on Machine Learning. PMLR, 2023.\n&nbsp;\n\nWe would appreciate a score increase from the reviewer if he/she finds our clarifications satisfactory. We remain available for further clarification."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700060708471,
                "cdate": 1700060708471,
                "tmdate": 1700060708471,
                "mdate": 1700060708471,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "74c6CFqgAx",
                "forum": "dXRWP4n15q",
                "replyto": "jBzWkNKWaP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4978/Reviewer_oGkp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4978/Reviewer_oGkp"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks the authors for your feedback. I do not agree with your point of the unnecessity of maintaining visual imperceptibility of adversarial examples. If it happens, why you need to minimize the $\\ell_0$ of the perturbation. If we relieve such constraints, we can easily find the adversarial examples with attack successful rate 100%. It would be more convincing to me if the authors can give a practical scenario of using the easily human-recognized adversarial examples. Moreover, it would be better if the paper reports some other metrics such as SSIM, PSNR, and LPIPS. I am leaning to keep my current score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727011488,
                "cdate": 1700727011488,
                "tmdate": 1700727079396,
                "mdate": 1700727079396,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rt9oNpikKm",
                "forum": "dXRWP4n15q",
                "replyto": "jBzWkNKWaP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4978/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4978/Authors"
                ],
                "content": {
                    "title": {
                        "value": "SSIM and PSNR evaluation"
                    },
                    "comment": {
                        "value": "We have been able to run some more additional experiments, reported in the supplementary material (see comparison_SSIM_PSNR.pdf),  where we computed SSIM and PSNR measures for different examples. Note that only EAD exhibits marginally superior values w.r.t. the other attacks, given that it works using a slightly different perturbation model.\n\nThe proposed $\\sigma$-zero  attack instead achieves very high SSIM and PSNR, outperforming or remaining comparable to the other $\\ell_0$-norm attacks.\n\nLet us finally thank again the reviewer for giving us the opportunity to improve our work. We hope that this last evaluation will convince the reviewer that our attack is valuable and that it can improve our current understanding of adversarial robustness against sparse, $\\ell_0$-norm attacks."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740552677,
                "cdate": 1700740552677,
                "tmdate": 1700740718028,
                "mdate": 1700740718028,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aQlSshJfbQ",
            "forum": "dXRWP4n15q",
            "replyto": "dXRWP4n15q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4978/Reviewer_nLC7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4978/Reviewer_nLC7"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes $\\sigma$-zero, a white-box adversarial attacks for the $\\ell_0$-threat model. In particular, $\\sigma$-zero obtains sparse perturbations minimizing a differentiable surrogate of the $\\ell_0$-norm. In the experiments on several datasets and target classifiers, $\\sigma$-zero is shown to outperform existing attacks in terms of success rate and and size of the perturbations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed approach of using a differentiable approximation of the $\\ell_0$-norm is reasonable, and yields a simple method.\n\n- The effectiveness of $\\sigma$-zero is supported by the experimental results."
                },
                "weaknesses": {
                    "value": "- The configuration in which some of the competitors are used seems suboptimal:\n    - BB can be initialized from an image of another class or another dataset, as done in [A, B], to avoid the issue of not finding a starting point (as mentioned in Sec. 3.2).  \n    - If I understand it correctly, Sparse-RS is re-run with different sparsity levels until reaching the desired success rate (averaged over the test points), but then the results are reported for all points with the same sparsity level. In this case, the results would be suboptimal, as the binary search should ideally be done for each point individually (for comparison to $\\sigma$-zero and other attacks which optimize the perturbation size independently for each test image). As a cheaper solution, one could run Sparse-RS on several $k$ values, and the select for each point the smallest $k$ which finds an adversarial perturbations. \n    - In general, while the paper uses the default parameters for all baseline attacks, it's not clear whether these are optimal: ideally, one could tune (some of) them on a small subset of test cases (models or datasets).\n    - Sparse-RS is a black-box method, i.e. doesn't need a backward pass at each iteration, which means that using the same number of iteration as for the white-box attacks results in significantly lower computational cost (e.g. 2x fewer network passes). How would the results compare when equating the number of network passes?\n\n- The overall technical contribution is limited: while the proposed algorithm has some task-specific solutions, e.g. inducing sparsity in $\\delta$ by clipping the smallest components, the $\\ell_0$-norm approximation has already been used in the context of adversarial robustness (Cin\u00e0 et al., 2022).\n\n[A] https://arxiv.org/abs/2102.12827  \n[B] https://arxiv.org/abs/2103.01208"
                },
                "questions": {
                    "value": "Since the experimental results are the main part of the paper, I think it is important that the configuration in which the baselines are used is clarified to provide a comprehensive comparison and assess the effectiveness of the proposed method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4978/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698939036229,
            "cdate": 1698939036229,
            "tmdate": 1699636485135,
            "mdate": 1699636485135,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P2oKjkMstW",
                "forum": "dXRWP4n15q",
                "replyto": "aQlSshJfbQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4978/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4978/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nLC7"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the significant results sigma-zero offers over all the experimental settings we provide. \n\n**Additional comparison with BB.** We have run additional experiments considering BB with adversarial initialization (BBadv), whose results are reported in  our response to all reviewers (https://openreview.net/forum?id=dXRWP4n15q&noteId=JKLyN3Qnl9). These experiments further confirm the effectiveness of sigma-zero against competing attacks, in terms of ASR, perturbation size, and execution time. We also remark that, over five additional experimental configurations, only once BBadv finds slightly better results (i.e., 3 features less in median perturbation size). However, this improvement comes at the cost of a 12-fold increase in execution time, raising the computational complexity from 0.42 seconds per sample to 5.28 seconds. Furthermore, BBadv exploits an unfair initialization strategy compared to the other attacks, given that all the other ones start from the same source sample, and not from an adversarial region. We will add these experiments and considerations to the paper.\n\n**Comparison with Sparse-RS.** We agree with the reviewer that SparseRS was tested in an unfair setting. We have thus re-run it using a sample-wise binary search on the perturbation budget, to efficiently find minimum-norm adversarial examples, as suggested in [S0], and as also detailed in the response to reviewer g7Nu (see **Unfair comparison with Sparse-RS**, https://openreview.net/forum?id=dXRWP4n15q&noteId=9oqEGzhkh4 ). Even under this configuration, $\\sigma$-zero shows better results and computational efficiency. We\u2019d finally like to remark also that SparseRS should not be seen as a direct competitor of gradient-based (white-box) attacks, including $\\sigma$-zero, given its gradient-free (black-box) nature. We report it in our experiments as an additional, interesting baseline, and will clarify this aspect in the paper too. Note finally that the number of network passes **(i.e. forwards and backwards)** is already set to be the same between white-box and black-box attacks. This means that SparseRS is already performing twice the number of forward passes w.r.t. gradient-based attacks. We'll clarify this aspect in the paper.\n\n[S0] Rony, J\u00e9r\u00f4me et al. \u201cAugmented Lagrangian Adversarial Attacks.\u201d 2021 IEEE/CVF International Conference on Computer Vision (ICCV).\n\n**Hyperparameter optimization.** We agree in principle with the reviewer that optimizing the attack hyperparameters may be beneficial for some attacks. However, this is normally too costly and not considered in the majority of papers dealing with attack and defense evaluations (see, e.g.,  [S0, S1, S2, S3, S4, S5]). This is also why AutoPGD (and AutoAttack) algorithms are so popular, namely, they are parameter-free and work well with their default hyperparameter values. For this reason, we also consider here the same setting of using the default hyperparameter values reported in the original papers/implementations of each attack, and show that without any specific hyperparameter tuning, $\\sigma$-zero turns out to be more effective. In this sense, we do believe that our comparison remains fair (namely, all attacks use default hyperparameters, and no computational overhead is spent in hyperparameter tuning).\n\n[S1] Croce, Francesco and Matthias Hein. \u201cMind the box: l1-APGD for sparse adversarial attacks on image classifiers.\u201d ICML.\n\n[S2] Hajri, Hatem et al. \u201cStochastic sparse adversarial attacks. 2021 IEEE 33rd ICTAI.\n\n[S3] Rony, J\u00e9r\u00f4me et al. \u201cDecoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses.\u201d 2019 IEEE CVPR.\n\n[S4] Modas, Apostolos et al. \u201cSparseFool: A Few Pixels Make a Big Difference.\u201d 2019 IEEE/CVF CVPR.\n\n[S5] Croce, Francesco and Matthias Hein. \u201cSparse and Imperceivable Adversarial Attacks.\u201d 2019 IEEE/CVF ICCV.\n\n\n**Paper\u2019s contributions.** Finally, we apologize to the reviewer for not having highlighted our contributions well within the paper. Specifically, the loss term in Eq. 7 represents a novel contribution as it enables us to simultaneously search for an adversarial example while also minimizing the L0 norm of the perturbation (i.e., a non-trivial task given the non-convexity of this norm). Despite its simplicity, the approximation function used in Cin\u00e0 et al. enables the implementation of a simple yet very fast and surprisingly effective attack, as witnessed by our extensive experimental analysis and acknowledged by the other reviewers. We thus firmly believe that the idea of leveraging such an approximation to develop a very effective L0-norm adversarial attack is not trivial.\n\nWe would appreciate a score increase from the reviewer if he/she finds our clarifications satisfactory. We remain available for further clarification."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700049190123,
                "cdate": 1700049190123,
                "tmdate": 1700074761923,
                "mdate": 1700074761923,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "N7XPIC51qv",
                "forum": "dXRWP4n15q",
                "replyto": "P2oKjkMstW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4978/Reviewer_nLC7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4978/Reviewer_nLC7"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the response and additional experiments.\n\nWhen using initialization BB performs quite close (sometimes better, interestingly on the most robust model) to $\\sigma$-zero, although I agree its cost is higher. I don't think the initialization gives a particular advantage to BB, since I assume it's completely non-sparse (this could be verified just starting the other attacks from the same point).\n\nAlso, since in the new results SparseFool gets median $||\\delta||_0$ up to 3071, it means that the considered threat model counts color channels independently, instead of counting just number of perturbed pixels. I think it's worth clarifying this aspect in the paper (unless it's already there and I missed it) since there are some attacks use the pixel space (or e.g. Sparse-RS handles both, and I assume it's used in the feature space mode -- is that correct?).\n\nOverall, I'm leaning towards keeping the original score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580304599,
                "cdate": 1700580304599,
                "tmdate": 1700580304599,
                "mdate": 1700580304599,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mMwl7KK0oC",
                "forum": "dXRWP4n15q",
                "replyto": "lZccehkCrM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4978/Reviewer_nLC7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4978/Reviewer_nLC7"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the update. The revised version differs quite significantly from the original, i.e. all major parts of the paper (proposed method, baselines, main experiments) have been modified. This has some side effects: for example, the impact of the new thresholding scheme is not well discussed, and I guess the ablation study in the appendix is now outdated. Moreover, Table 1 has a fair amount of missing results, and the comparison on ImageNet misses some of the strongest baselines.\n\nAs additional question, when using binary search for maximum confidence attacks, is the iteration budget split across the different perturbation sizes used or kept the same (i.e. 1000 or 2000 iterations for each $k$)?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736121209,
                "cdate": 1700736121209,
                "tmdate": 1700736121209,
                "mdate": 1700736121209,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MpFuXzuTSt",
                "forum": "dXRWP4n15q",
                "replyto": "aQlSshJfbQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4978/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4978/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nLC7"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for their response. \n\n**Thresholding scheme**. The main modification of the thresholding scheme aims to provide a sample-wise threshold tuning which drastically improved the performance with respect to having a fixed threshold equal for all samples.\n\n**Ablation study.** The reported ablation study, as detailed in the appendix (refer to Figure 5), is not outdated. It has been updated to incorporate the updated version of our attack including the new thresholding scheme. We demonstrate that the initial value selection for the thresholding parameter, denoted as $\\tau$, has negligible influence on the outcome, given that the parameter dynamically adapts throughout the optimization process.\n\n**Binary Search.** Regarding binary search, yes, the iteration budget is split across different perturbation sizes."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737204412,
                "cdate": 1700737204412,
                "tmdate": 1700737271123,
                "mdate": 1700737271123,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E4hn1uugLJ",
                "forum": "dXRWP4n15q",
                "replyto": "MpFuXzuTSt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4978/Reviewer_nLC7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4978/Reviewer_nLC7"
                ],
                "content": {
                    "comment": {
                        "value": "> Binary Search. Regarding binary search, yes, the iteration budget is split across different perturbation sizes.\n\nHow does this work in practice then? I think this should be described somewhere."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737620322,
                "cdate": 1700737620322,
                "tmdate": 1700737620322,
                "mdate": 1700737620322,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qAvRgYIxjk",
                "forum": "dXRWP4n15q",
                "replyto": "aQlSshJfbQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4978/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4978/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We'll detail that aspect better in the paper, but we essentially followed the same mechanisms explained in [S0].\nWe imposed 400 and 100 number of steps respectively for Sparse-RS and PGD-$\\ell_0$.\n\nAs depicted in Table 1 (column q), with these configurations, the two attacks execute approximately 2000 queries or more each. Consequently, they exploit, on average, the same number of queries compared to the other attacks, ensuring a fair comparison.\n\n[S0] Rony, J\u00e9r\u00f4me et al. \u201cAugmented Lagrangian Adversarial Attacks.\u201d 2021 IEEE/CVF International Conference on Computer Vision (ICCV)."
                    },
                    "title": {
                        "value": "Response to Reviewer nLC7"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738031845,
                "cdate": 1700738031845,
                "tmdate": 1700738493420,
                "mdate": 1700738493420,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "apaOrkXsU3",
                "forum": "dXRWP4n15q",
                "replyto": "qAvRgYIxjk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4978/Reviewer_nLC7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4978/Reviewer_nLC7"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification. I think this effectively limits the performance of the baselines: in fact only a small number of iterations are used for each $k$, and only a limited number of $k$ values can be tested (because of the overall budget of iterations), which might make the initial value of $k$ quite influential on the final results."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738465998,
                "cdate": 1700738465998,
                "tmdate": 1700738465998,
                "mdate": 1700738465998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FmdqszQPek",
                "forum": "dXRWP4n15q",
                "replyto": "aQlSshJfbQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4978/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4978/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nLC7"
                    },
                    "comment": {
                        "value": "Recall that PGD-L0 has 100 iterations by default, and that increasing the iterations of PGD-L0 and Sparse-RS for each k would then be unfair to minimum-norm attacks. However, if this is important, we can also include results with more iterations for these attacks too, or even better, compare ASR at fixed k, using the same number of iterations for sigma-zero, Sparse-RS, and PGD-L0. Despite this scenario being slightly unfair to sigma-zero, we are confident that sigma-zero will outperform the other attacks also in that configuration. Finally, we express again our sincere gratitude for your insights and feedback. We hope that we have provided convincing evidence that our attack works very well, especially post rebuttal, and that it can thus provide an additional, sound algorithm to help improve adversarial robustness evaluations in the L0 case."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738904590,
                "cdate": 1700738904590,
                "tmdate": 1700739089291,
                "mdate": 1700739089291,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SQxD0iZDYL",
            "forum": "dXRWP4n15q",
            "replyto": "dXRWP4n15q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4978/Reviewer_g7Nu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4978/Reviewer_g7Nu"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an algorithm utilizing gradient information to generate sparse adversarial perturbations. Compared with perturbations bounded by $l_2$ or $l_\\infty$ norms, sparse perturbation is more challenging because of its non-convexity nature. The authors use a continuous function to approximate the $l_0$ norm of the perturbation and design a new loss objective function that facilitates optimizing adversarial perturbations. The experiments show the effectiveness and efficiency of the proposal algorithm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Robustness against sparse perturbation is an interesting problem to explore. The algorithm is well-motivated and clearly demonstrated. The experiments are conducted on various datasets and the results indicate the advantages of the proposal algorithm over the baselines considered."
                },
                "weaknesses": {
                    "value": "1. Although the experiments are conducted in various datasets, I think more sparse attack algorithms should be included as the baselines for comparison. For example, PGD$_0$ [A] should be included as the baseline, since it is also a white-box attack for $l_0$ bounded perturbations. Sparsefool, based on constructing sparse perturbations on top of popular deep fool method, should also be studied.\n\n[A] Francesco Croce and Matthias Hein. Sparse and imperceivable adversarial attacks. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 4724\u20134732. 2019.\n\n[B] Modas, Apostolos, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. \"Sparsefool: a few pixels make a big difference.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.\n\n2. The comparison might not be fair. Some algorithms are proposed in a different formulation as in this paper. Some baselines (such as Sparse-RS) are proposed to generate adversarial examples such that the $l_0$ norms of the perturbations are smaller than $\\epsilon$. These algorithms are not designed to minimize the $l_0$ of the perturbation, they search for a perturbation whose $l_0$ norm is smaller than a threshold. Comparing the $l_0$ norm - ASR trade-off is probably unfair for these methods.\n\n3. In addition to $l_2$ and $l_\\infty$ robust models, I think including $l_1$ robust model for evaluating the attacks would make the experiment more comprehensive, especially considering the $l_1$ norm is the closest convex $l_p$ norm to $l_0$ norm. Possible baselines include those trained by AA-$l_1$ [C] and Fast-EG-$l_1$. [D]\n\n[C]: Croce, Francesco, and Matthias Hein. \"Mind the box: $ l_1 $-APGD for sparse adversarial attacks on image classifiers.\" International Conference on Machine Learning. PMLR, 2021.\n\n[D]: Jiang, Yulun, et al. \"Towards Stable and Efficient Adversarial Training against $ l_1 $ Bounded Adversarial Attacks.\" International Conference on Machine Learning. PMLR, 2023."
                },
                "questions": {
                    "value": "Major concerns are demonstrated in the weakness part. In addition to these concerns, I have the following questions:\n\n1. Function $\\mathcal{L}$, as defined by Equation (7), is not continuous and thus not differentiable everywhere. Will this cause some problems when calculating the gradient of $\\mathcal{L}$ in line 4 of Algorithm 1? I think using a continuous and differentiable function as the loss objective would be better.\n\n2. Regarding Sparse-RS with a super-script 100 or 85. If understood correctly, these super-scripts mean the value of k, max allowed $l_0$ norm of the perturbations, why Sparse-RS100 is worse than Sparse-RS85? And why the average $|\\delta|_0$ is bigger than the corresponding k in some cases?\n\nDue to the major concerns and questions as pointed out, I cannot recommend acceptance. I welcome the discussions with the authors and will reconsider my rating."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4978/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4978/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4978/Reviewer_g7Nu"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4978/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699171941452,
            "cdate": 1699171941452,
            "tmdate": 1700709739178,
            "mdate": 1700709739178,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9oqEGzhkh4",
                "forum": "dXRWP4n15q",
                "replyto": "SQxD0iZDYL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4978/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4978/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer g7Nu"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging credit for our contribution and its clarity, and reply to their concerns below.\n\n**Additional attacks and robust models.** We have run additional experiments considering the additional attacks PGD-L0 [A] and Sparsefool [B], and the L1-norm robust models [C] and [D], as suggested by the reviewer. We report the results in our response to all reviewers (https://openreview.net/forum?id=dXRWP4n15q&noteId=JKLyN3Qnl9), showing that $\\sigma$-zero outperforms such attacks on the suggested robust models as well as on the other CIFAR10 models from our paper. We will add these results to the revised paper, as well as the comparisons on the remaining models we used in the paper (which will take some more time to execute). With these additional results, our work will include more than 320 diverse dataset-model-attack configurations, which we believe should provide sufficient convincing empirical evidence that our attack achieves state-of-the-art performance.\n\n**Unfair comparison with Sparse-RS**. We agree with the reviewer that comparing minimum-norm attacks against maximum-confidence attacks (i.e., attacks that optimize the loss within a maximum perturbation budget, like PGD-L0 and Sparse-RS) may be tricky. In particular, minimum-norm attacks are typically more complicated as they require not only finding an adversarial example, but also the minimum perturbation budget required to do that (i.e., they optimize both the loss and the perturbation size, not only the loss). \nTo compare these two types of attack in a fair manner, we have re-run our experiments by performing a binary search on the perturbation budget of **each sample** when using maximum-confidence attacks (rather than doing that per batch), as also suggested in [S1]. In addition, we do not only report the median distance (i.e., the distance that corresponds to an ASR of 50%), but also the ASR at a fixed budget with eps=10 and eps=50, as typically done when evaluating maximum-confidence attacks. Given that we re-evaluated Sparse-RS within this setting, we decided to remove the two redundant SparseRS versions achieving different ASRs (85% and 100%, respectively).\n\n**Loss gradient.** Regarding Eq 7, we agree with the reviewer that using a sigmoid-like function instead of a step function may be more appropriate. We have however seen in practice that this never induces numerical instabilities on the gradient even on the 18 models we consider in our experimental analysis. We will clarify this point within the paper, after Eq. 7.\n\n**Superscripts in Sparse-RS.** First, let us point out that comparing against Sparse-RS is only meant to provide an additional baseline, but it should not be considered a direct competitor of gradient-based (white-box) attacks, given that Sparse-RS is a gradient-free (black-box) attack (working under stricter assumptions). As explained before, the superscripts in Sparse-RS were not referred to the perturbation budget, but to the ASR. This means that the perturbation budget of Sparse-RS^85 and Sparse-RS^100 was tuned to respectively achieve an ASR of 85% and 100%. However, given that we now tune the perturbation budget in a sample-wise manner to find minimum-norm adversarial examples also for SparseRS, we decided to remove these two (confusing) versions of SparseRS from our evaluation.\n\nWe would appreciate a score increase from the reviewer if he/she finds our clarifications satisfactory. We remain available for further clarification."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700048776730,
                "cdate": 1700048776730,
                "tmdate": 1700048776730,
                "mdate": 1700048776730,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E3ews4RPUD",
                "forum": "dXRWP4n15q",
                "replyto": "9oqEGzhkh4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4978/Reviewer_g7Nu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4978/Reviewer_g7Nu"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I thank the author for comprehensive feedback. I agree with the author that minimum-norm attacks are typically more challenging than the attack with a fixed norm requirement. However, as far as what I know, there is no algorithm currently to **guarantee** the minimum $l_0$ norm is achieved to generate adversarial examples. By contrast, there are indeed several methods that can *successfully* generate adversarial examples bounded by a specific level of $l_0$ norm.\n\nI encourage the author to clarify the difference between these two categories of methods to avoid confusion.\n\nFor the rest questions, it is well addressed. The score is adjusted accordingly."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709722040,
                "cdate": 1700709722040,
                "tmdate": 1700709722040,
                "mdate": 1700709722040,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]