[
    {
        "title": "Smooth Min-Max Monotonic Networks"
    },
    {
        "review": {
            "id": "hSXAczNiXs",
            "forum": "Y5Xkw9fpty",
            "replyto": "Y5Xkw9fpty",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3515/Reviewer_RbCP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3515/Reviewer_RbCP"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses an intriguing aspect of machine learning: monotonic modeling, focusing specifically on the min-max architecture. The authors thoroughly summarize various techniques and identify a key issue for min-max architecture known as the \"silent neuron\" problem. In response, they propose a smooth variant and develop what they term the SMM architecture. This new architecture demonstrates strong experimental results."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This SMM architecture is not only innovative but also well-motivated solution by transitioning from the conventional hard min-max to a LogSumExp-based approach. Furthermore, the paper establishes theoretical guarantees about model's approximation property when the parameter $\\beta$ is sufficiently large.\n\nThe experimental results are another major strength of this work. The authors demonstrate the effectiveness of the smooth min-max (SMM) architecture, thereby confirming both the practicality and the potential of their approach."
                },
                "weaknesses": {
                    "value": "One significant concern lies in the treatment of $\\beta$ as a learnable parameter. The authors' exploration of this parameter is fascinating, particularly in light of Corollary 1's suggestion that a lower bound on fitting error is inherently linked to the value of $\\beta$. This implies that a $\\beta$ not sufficiently large would fail to approximate certain functions. Conversely, an excessively large $\\beta$ might impact the training dynamics adversely, as some nearly silent neurons may remain untrained. \n\nWhile the authors utilize trainable $\\beta$ in experiments, the paper could benefit from a deeper exploration of $\\beta$'s behavior during training, such as its trajectory and its relationship with loss changes. Reporting the final values of $\\beta$ after training would also have provided valuable insights."
                },
                "questions": {
                    "value": "The observation that test errors can vary significantly with different initial $\\beta$ values raises an important question. \nDoes it suggest that the optimization process may not fully converge or that $\\beta$ plays a more complex role in the model's training dynamics than currently understood?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3515/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698622281626,
            "cdate": 1698622281626,
            "tmdate": 1699636304813,
            "mdate": 1699636304813,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9tmGdPBpQY",
                "forum": "Y5Xkw9fpty",
                "replyto": "hSXAczNiXs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3515/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3515/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial reply to review"
                    },
                    "comment": {
                        "value": "Thank you for your feedback. \n\nCorollary 1 is an existence proof, showing that for a certain beta value an SMM with a particular approximation accuracy exists. This network \u2013 and beta parameter \u2013 may not be the \u201cbest\u201d one for the given task, there may be SMNs that have fewer parameters, that have smaller weights, are Lipschitz with a smaller constant, are faster to learn, etc. with the same or better approximation accuracy. Thus, Corollary 1 is not strong enough to establish a link between fitting error and the value of beta for practical experiments as considered in the paper.\n\nThank you for the question regarding beta. In general, too extreme beta values may cause numerical trouble, which leads to vanishing gradients. If beta is too large, then the SMN gets stuck in undesired local minima as the original MM. To show this, we reran the experiments underlying Table C.4 with ln(beta)=2. This reduced the mean number of active SMM neurons from 35 to 29.3 (means for $f_\\text{sq}$, $f_\\text{sqrt}$, and $f_\\text{sig}$ were 31.81, 30.71, 25.42, respectively). \n\nWe are happy to add an extended analysis to the paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3515/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074393525,
                "cdate": 1700074393525,
                "tmdate": 1700074393525,
                "mdate": 1700074393525,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K3rXkwP3lG",
                "forum": "Y5Xkw9fpty",
                "replyto": "9tmGdPBpQY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3515/Reviewer_RbCP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3515/Reviewer_RbCP"
                ],
                "content": {
                    "title": {
                        "value": "Response to author's response"
                    },
                    "comment": {
                        "value": "Indeed, Corollary 1 is an existence proof. However, by looking into the proof, I only see why an approximation could exist when $\\beta$ is sufficiently large. I am not sure what is the \u201cbest\u201d $\\beta$. It seems network with smaller $\\beta$ is always representable by another network with larger $\\beta$. In theory, the merit of small $\\beta$ only lies in avoiding vanishing gradient. Yet the link between that with local minima is not clear. I think the role of $\\beta$ is still not totally clear.\n\nDespite this uncertainty, I am inclined to keep my rating of weakly accept for the innovation of LogSumExp-based approach."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3515/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709287016,
                "cdate": 1700709287016,
                "tmdate": 1700709287016,
                "mdate": 1700709287016,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IilRzKQhcf",
            "forum": "Y5Xkw9fpty",
            "replyto": "Y5Xkw9fpty",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3515/Reviewer_odc5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3515/Reviewer_odc5"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the training and empirical performance of neural networks and non-neural approaches that ensures monotonicity with respect to input parameters. The authors propose a new network module architecture based on min-max (MM) architecture [Sill (1997)] which aims to tackle the problem of silent neurons and non-smoothness properties by applying a LogSumExp function to the max/min function. The authors support their claims by providing empirical evidence on toy examples and on practical data sets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) This paper is well-written and is easy to follow. The authors presented their ideas and results clearly.\n2) The proposed SMM architecture is simple and seems to be an intuitive way to ensure monotonicity through smoothening.\n3) The authors did extensive comparisons of their proposed SMM against other models which aim to ensure monotonicity, and aided readers to understand the potential advantages of SMM over comparable models."
                },
                "weaknesses": {
                    "value": "1) I am not entirely sure about the novelty of this idea of smoothening non-smooth neurons to address the problem of vanishing gradients or silent neurons in the context of monotonic networks. The main idea of this work of using LogSumExp to act as a smooth approximation while preserving monotonicity does not seem too non-trivial due to its popularity in statistic modelling. However, I am not familiar with the line of work with monotone networks thus I will defer this discussion to other reviewers.\n2) While the empirical comparisons are sufficient, they do not provide evidence (especially after accounting the error bars) to suggest that SSM has significant advantage over existing approaches. It is then unclear why practitioners should prefer SSMs over LMNs or XGBoost."
                },
                "questions": {
                    "value": "1) How should the scaling factor $\\beta$ chosen in practice? My understanding is that tuning it to ensure that the output network is monotone is not trivial and requires retraining the entire network."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3515/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3515/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3515/Reviewer_odc5"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3515/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698742795691,
            "cdate": 1698742795691,
            "tmdate": 1699636304723,
            "mdate": 1699636304723,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vAwjyfMpKg",
                "forum": "Y5Xkw9fpty",
                "replyto": "IilRzKQhcf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3515/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3515/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial reply to review"
                    },
                    "comment": {
                        "value": "Thank you for your feedback.\n\n**Question 1:** \"My understanding is that tuning it to ensure that the output network is monotone is not trivial and requires retraining the entire network.\": *The network is monotone for all choices of beta.*  \n\nTuning beta is not an issue in practice. Note that beta is adapted during training. All experiments (except those in reported in Table C.7) were conducted with the same \"default\" initialization ln(beta)=-1. This shows the robustness of our method. We suggest to simply use the default initialization (much too large or too small initial beta values may impact the performance, see also reply to reviewer RbCP).\n\n**Weakness 1:** To the best of our knowledge, our idea is new in the context of monotonic neural networks. This is indeed remarkable, given the many papers building on Sill\u2019s work coming up with complicated alternatives.\n\n**Weakness 2:** The results in Table 1 show statistically significant better performance of SMM compared to all other methods. The results in Table 1 may be regarded as the most relevant ones, because the experimental setup is very controlled and focussed on the monotonicity aspect. \n\nWe are very sorry that we failed to make it clear in our text why one should prefer SSMs over LMNs or XGBoost in practice.\n\nXGBoost vs SMM: XGBoost models are piecewise constant. If you want a smooth model (e.g., in a natural science application), then you should prefer SMM or LMN over XGBoost. If you have a larger (deep) learning system and you need a monotonic module and would like to train end-to-end, then XGBoost is not a good option and you should pick a neural network such as LMN or SMM. If you do not have these constraints and tabular data, then XGBoost is a choice that is difficult to beat by any neural network approach - except perhaps SMMs as shown in Table 1.\n\nSMM vs LMN: \u201cLMNs share many of the desirable properties of SMMs. Imposing an upper bound on the Lipschitz constant, which is required for LMNs and can be optionally added to SMMs, can act as a regularizer and supports theoretical analysis of the neural network. However, a wrongly chosen bound can limit the approximation capabilities. Our experiments show that there are no reasons to prefer LMNs over SMMs because of generalization performance and efficiency. Because of the high accuracies obtained without architecture tuning, the more general asymptotic approximation results, and the simplicity of the min-max approach, we prefer SMMs over LMNs.\u201d \n\nPerhaps this was too modest, so we would like to add: Over all experimental results in our study, SMM performs (most often significantly) better than LMN. While SMMs are very robust w.r.t. to hyperparameter choices (as shown in the study), the Lipschitz bound to be chosen for LMNs might be a problem in practice. We see no reason to prefer LMN over SMM."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3515/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700067605343,
                "cdate": 1700067605343,
                "tmdate": 1700067605343,
                "mdate": 1700067605343,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uhH4FyZAb3",
            "forum": "Y5Xkw9fpty",
            "replyto": "Y5Xkw9fpty",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3515/Reviewer_2p1V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3515/Reviewer_2p1V"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose modification to min-max networks by replacing max and min by appropriate log sum exp functions.\n\nThis is done to improve the learning signal.\n\nSome theoretical/empirical analysis is provided."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is very clear, I could understand most of it in first reading.\n\nThe authors consider an important problem: sometimes \"worse\" models can be empirically better as it is easier to optimise."
                },
                "weaknesses": {
                    "value": "Are there different types of relaxation of min/max that can be used?\n\nI think the results of type Thm 1 are not very meaningful as the network size can increase very quickly when epsilon decreases.\n\nThe empirical results are not very strong. Is e.g. ChestXRay statistically significant? The differences in Table 3 look mostly statistically insignificant."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3515/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3515/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3515/Reviewer_2p1V"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3515/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699141386585,
            "cdate": 1699141386585,
            "tmdate": 1699636304646,
            "mdate": 1699636304646,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GQSMpIuL5o",
                "forum": "Y5Xkw9fpty",
                "replyto": "uhH4FyZAb3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3515/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3515/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial reply to review"
                    },
                    "comment": {
                        "value": "Thank you for your feedback.\n\n**I.** There are several smooth (approximations of the) maximum and minimum functions (e.g., computing the p-norm for large p). However, we need minimum and maximum relaxations which \n* are non-decreasing,\n* are smooth,\n* need to work for positive and negative arguments,\n* have a bounded approximation error which can be controlled (important for Corollary 1), and\n* can be computed efficiently without causing too much numerical trouble.\n\nThe LogSumExp is the best choice we know fulfilling all these properties. If there was a better choice, this would make our idea work even better.\n\n**II.**  Theorem 1 and Corollary 1 have the standard form of a universal approximation theorem. If one wants universal approximation capabilities, there are functions for which the construction used in the proofs will require many neurons to achieve a certain accuracy. This is inevitable. Still, we regard these results as meaningful because they show a fundamental theoretical property of the model, an asymptotic property such as unbiasedness and consistency of learning methods and convergence to an optimum of a gradient-based optimizer. From a statistical learning theory perspective, universal approximation theorems are important, for example, to understand which learning problems are realizable given the neural network hypothesis class. In practice we would argue that the properties \u2013 even if asymptotic \u2013 can help to choose one algorithm over another. \n\nStudying the theoretical properties of machine learning algorithms is an important area of machine learning research. One can of course argue about the practical relevance of each of the properties listed above, however, one should appreciate that there is a large community who is interested in studying these properties ( https://en.wikipedia.org/wiki/Universal_approximation_theorem ).\n\n**III.** The results in Table 3 are not statistically significant. The motivation for Table 3 was to reproduce *exactly* the experimental setting from Niklas Nolte et al. published at ICLR last year so that the results can be compared directly. The experimental setup is not ideal: Only *three* trials, early-stopping on the test set, too complex tasks where monotonicity only plays a minor role, varying hyperparameters for the LMNs. The results show that you can get at least as good (we would argue better) results with SMMs than LMNs if you use exactly the evaluation protocol as in the LMN paper from ICLR last year. \n\nFor a statistical comparison, the more important results are those in Table 1, which come with proper statistical significance testing. Here SMMs show significantly better results."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3515/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068396994,
                "cdate": 1700068396994,
                "tmdate": 1700068396994,
                "mdate": 1700068396994,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]