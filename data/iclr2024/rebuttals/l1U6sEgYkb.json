[
    {
        "title": "DV-3DLane: End-to-end Multi-modal 3D Lane Detection with Dual-view Representation"
    },
    {
        "review": {
            "id": "Q7sdh8UAtQ",
            "forum": "l1U6sEgYkb",
            "replyto": "l1U6sEgYkb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3017/Reviewer_9oXi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3017/Reviewer_9oXi"
            ],
            "content": {
                "summary": {
                    "value": "This paper covers lane detection task using attention mechanism from multi-modal queries of vision and LiDAR."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* Really novel idea that does this task of lane detection using query clustering from multiple perspective. \n* SOTA results, and code should be available online soon \n* Takes care of runtime optimization as well."
                },
                "weaknesses": {
                    "value": "NA"
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3017/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698419109888,
            "cdate": 1698419109888,
            "tmdate": 1699636246364,
            "mdate": 1699636246364,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Xyjx4bkJt7",
                "forum": "l1U6sEgYkb",
                "replyto": "Q7sdh8UAtQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9oXi"
                    },
                    "comment": {
                        "value": "Thank you for your encouraging comments and for recognizing the strengths of our work. \nWe appreciate your acknowledgment of the \"Really novel idea that does this task of lane detection using query clustering from multiple perspective\", \"SOTA results\" and your observation regarding \"runtime optimization\". We assure you that we will release our code."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476687726,
                "cdate": 1700476687726,
                "tmdate": 1700476687726,
                "mdate": 1700476687726,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "O3IDcI7Iuu",
            "forum": "l1U6sEgYkb",
            "replyto": "l1U6sEgYkb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3017/Reviewer_Z8i9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3017/Reviewer_Z8i9"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a multi-modal 3D lane detection method. A bidirectional feature fusion approach is proposed to incorporate multi-modal features into each view space. A unified query generation module is adopted that provides lane-aware prior knowledge from both views. A 3D dual-view deformable attention mechanism that combines discriminative features from both PV and BEV into queries for precise 3D lane detection. Comprehensive experiments on the public benchmark, OpenLane, prove the efficacy and efficiency of DV-3DLane."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "*  The performance of this article exceeded that of existing methods. A significant increase was also achieved in the multimodal feature fusion at the backbone level. \n*  The paper is well written and easy to follow."
                },
                "weaknesses": {
                    "value": "*  Most methods in Table 1 are based on camera. As can be seen from the results in Table 3, the performance of camera-only is not competitive. Multimodal inputs should be introduced to some methods for a comparative performance evaluation. Otherwise, presenting multimodal fusion as a key contribution is somewhat insufficient.\n\n* In Table 4, the performance of the queries generated based on PV and BEV is not as high as random queries, indicating that the adaptive generation of queries doesn't work. Although the queries after clustering have achieved performance improvement, it remains to be seen whether this improvement is brought about by the extra network. \n\n* The feature fusion in the backbone and decoder is quite tricky, making it difficult to be viewed as a major contribution point."
                },
                "questions": {
                    "value": "*  The main performance improvement primarily comes from multi-modalities, but multi-modalities inherently achieve higher points than a single modality. Therefore, it would be best to prove that the multi-modal performance of existing methods is inferior to this paper. The camera result is not much competitiveness, especially when compared to some recent methods, such as Group Lane.\n*  Using Deformable attention to aggregate multi-modal features lacks innovation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3017/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698748469722,
            "cdate": 1698748469722,
            "tmdate": 1699636246293,
            "mdate": 1699636246293,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YYsaflWnFB",
                "forum": "l1U6sEgYkb",
                "replyto": "O3IDcI7Iuu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Z8i9 [1/3]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for throwing time into reviewing our paper. To address the reviewer's concerns and questions, we would like to first highlight our contributions:\n\n**1)** To bridge the gap of multi-modal exploration in 3D lane detection, we propose the DV-3DLane, an end-to-end multi-modal 3D lane detection framework. To the best of our knowledge, this is **the first end-to-end multi-modal 3D lane detection** framework.\nBesides exploring multiple modalities, our method also harnesses the power of both Perspective View (PV) and Bird's Eye View (BEV) representations. All four other reviewers acknowledge this contribution.\n\n**2)** To achieve the goal of end-to-end multi-model 3D lane detection: First, we propose to fuse multi-modal features bidirectionally among PV and BEV spaces and further generate a lane-aware query set within each space. Then, to unify these two query sets, from two spaces, we propose a **lane-centric clustering** strategy. Afterward, to effectively aggregate PV features and BEV features into the unified queries, we propose the 3D DV Deformable Attention to consistently sample features from two different spaces via 3D deformed points, which are generated by predicting both 3D reference points and their 3D offsets.\n\n**3)** We conduct comprehensive experiments on the OpenLane benchmark and our method significantly improves the performance, compared with the previous multi-modal approach M$^2$-3DLaneNet and the SOTA LATR. Notably, we significantly **cut the errors** in both X and Z directions within near/far ranges by **about 50%**. Moreover, we propose to use a more strict evaluation metric, 0.5m (previous 1.5m), to study the problem in a nuanced manner, towards enhancing real-world driving safety.\n\nBelow, please find our response to each of the comments and questions:\n\n**Presentation: 2 fair**\nThis contradicts your 2nd strength point:\n > The paper is well written and easy to follow.\n\nAnd there is no specific weak point or question about the presentation. We, therefore, kindly appeal the reviewer\u2019s re-evaluation of the \u201cpresentation\u201d rating and final rating.\n\n**W1-1: *Regariding performance results in Table 3.***\n\nFirst, we highlight that Table.3 does not compare the camera-only method and the multi-modal one, as stated by the reviewer. \nInstead, Table 3 focuses on comparing various designs in the context of multi-modal fusion, with **each design incorporating both LiDAR and camera data**.\nThe results in Table.3 highlights the effectiveness of our bidirectional fusion within dual-view spaces compared to non-fusion/unidirectional-fusion ones (during the backbone stage).\n\nAs highlighted by the reviewer's observation from Table 1, the majority of methods in the literature lean towards a single-modality solution, with M$^2$-3DLaneNet being the exception, albeit now surpassed by subsequent works like LATR.\nRecognizing the significance of point cloud data, in tasks like 3D detection, we aim to explore the underexplored territory of combining LiDAR and image data for 3D lane detection. Our work addresses this gap in research (as mentioned by reviewer 7Rt7), emphasizing the relevance and importance of leveraging both modalities in this field. \n\n**W1-2: *Introduce LiDAR to some methods for a comparative performance evaluation.***\n\nFirst, we highlight that modifying other methods into multi-modal ones is **non-trivial**. Besides, we compared M$^2$-3DLaneNet, which compared several multi-modal fusion approaches, *e.g.,* PointPillar, BEVFusion results with inferior performance than M$^2$-3DLaneNet. The reviewer has acknowledged the multi-modal lack of research.\n\nTo specifically address your concern, we integrated the SOTA method **LATR with LiDAR** inputs, projecting all points into the image and using them as additional features in the network. The results clearly demonstrate the positive impact of LiDAR, although it falls below the performance achieved by our proposed method. This comparison underscores the unique contribution of our work. \n| Model       | Backbone | F1   | Acc. | X error       | Z error       |\n|-------------|----------|------|------|---------------|---------------|\n| LATR        | Res50    | 54.0 | 91.7 | 0.171 / 0.201 | 0.072 / 0.099 |\n| LATR+LiDAR  | Res50    | 57.4 | 92.1 | 0.167 / 0.185 | 0.071 / 0.088 |\n| DV-3DLane   | Res34    | 63.5 | 92.4 | 0.090 / 0.102 | 0.031 / 0.053 |\n| DV-3DLane   | Res50    | 65.2 | 93.4 | 0.082 / 0.101 | 0.028 / 0.048 |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478736418,
                "cdate": 1700478736418,
                "tmdate": 1700501244446,
                "mdate": 1700501244446,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J5BbLnL8q1",
                "forum": "l1U6sEgYkb",
                "replyto": "O3IDcI7Iuu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Z8i9 [2/3]"
                    },
                    "comment": {
                        "value": "**W2: *Regarding questions to Table 4.***  \nIn fact, we have explained this phenomenon in Section 4.5, titled 'Effect of Unified Query,' in the main text. The results indicate that solely using queries from one view does not effectively aggregate features from another view. However, initializing queries with *nn.embedding* allows queries to learn how to match features from different views. Furthermore, by equipping our proposed clustering strategy, the model achieves the best results, which strongly supports the efficacy of our query clustering in integrating multi-modal queries from different view spaces. **We believe this strategy could facilitate other multi-view or/and multi-modal studies as well.**\n\n**W3: *Regarding the contribution of feature fusion in the backbone and decoder.***\n\nWe appreciate the reviewer's comment regarding the feature fusion aspect of our work. Our primary goal is **to advance the state of 3D lane detection by introducing an effective end-to-end multi-modal framework**. To achieve this, we have developed the dual-view model to harness feature representations from different views.\n\nWhile we acknowledge that our work incorporates feature fusion in both the backbone and decoder, it's essential to emphasize that our contribution extends beyond this specific aspect. Although we have demonstrated the effectiveness of our fusion methods compared to alternative approaches, our research is **not solely centered on** the fusion techniques themselves. Instead, it addresses the broader challenge of multi-modal exploration for 3D lane detection and presents innovative solutions to tackle these challenges.\n\nOne of our key contributions is the introduction of **lane-centric clustering**, a novel technique designed to cohesively synergize queries from different views. This aspect is integral to our approach and constitutes a significant innovation in our field.\n\nThus, while the feature fusion in the backbone and decoder is a component of our model, the breadth and depth of our contributions, particularly in multi-modal exploration and lane-centric clustering, are what we believe to be the major points of novelty and impact in our work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480420421,
                "cdate": 1700480420421,
                "tmdate": 1700482353816,
                "mdate": 1700482353816,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PZ8R5Bl2ru",
                "forum": "l1U6sEgYkb",
                "replyto": "O3IDcI7Iuu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Z8i9 [3/3]"
                    },
                    "comment": {
                        "value": "**Q1: *Regarding comparison with GroupLane.***\n\nWe compare our method against the state-of-the-art model LATR. It is unfair to ask us to compare with GroupLane, which is under review at ICLR2024, the same as us. More critically, based on the results from GroupLane, when utilizing the same ResNet50 backbone, GroupLane exhibits inferior performance than LATR. Therefore, we would like to claim that our comparison is fair and competitive.\n\n**Q2: *Regarding using deformable attention.***\n\nWe acknowledge the reviewer's concern regarding the innovation aspect of using deformable attention for aggregating multi-modal features. \nIt is important to clarify that: \n\n**1)** Our contribution does not reside in using deformable attention per se, a well-established method widely employed in multi-modal/multi-view feature fusion.\n\n**2)** Instead, we propose the 3D DV Deformable Attention, an extension and adaptation of the original deformable attention.\n\nIn Section 3.3, we meticulously outline the architecture and functionality of our 3D DV Deformable Attention, accompanied by Algorithm 2 and Figure 5 for clarity.\nBesides, we conduct a thorough comparison of our method with other methods that are also built upon the original deformable attention, showcasing the effectiveness of our approach. This comprehensive evaluation is summarized in Table 5, providing a nuanced understanding of our method's performance relative to existing approaches."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482000658,
                "cdate": 1700482000658,
                "tmdate": 1700628936474,
                "mdate": 1700628936474,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4FZ1LxLhkD",
            "forum": "l1U6sEgYkb",
            "replyto": "l1U6sEgYkb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3017/Reviewer_ECVn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3017/Reviewer_ECVn"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel 3D lane detection algorithm that exploits a dual-view representation. The proposed algorithm consists of bi-directional feature fusion to aggregate view-specific features, unified query generation that focuses on coherent lane features, and 3D dual-view deformable attention to associate information across the viewpoints. The authors provided outperformed experimental results on OpenLane dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Achieved the best performance on a 3D lane detection benchmark\n- Leveraged multi-modal features to generate a unified query for 3D lane detection"
                },
                "weaknesses": {
                    "value": "- It appears that the 3D DV deformable attention mechanism lifts PV features to 3D using known camera parameters. Also, when the authors concatenate DV features, they depend on these camera parameters. Have the authors attempted to test the tolerance of calibration parameters to noise?\n- Since the evaluation was only performed on OpenLane, it is difficult to check the generalizability of the proposed method."
                },
                "questions": {
                    "value": "."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3017/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3017/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3017/Reviewer_ECVn"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3017/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773349206,
            "cdate": 1698773349206,
            "tmdate": 1699636246208,
            "mdate": 1699636246208,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Fvs7OKqyOs",
                "forum": "l1U6sEgYkb",
                "replyto": "4FZ1LxLhkD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ECVn"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback and address their concerns as below:\n\n**W1: *Tolerance of calibration parameters to noise.***\n\nThank you for your insightful comment. In response, we conducted experiments to assess the impact of noise on the calibration parameters. \n\n#### **Noise Settings:**\nFollowing the methodology of Yu et al. (2023) [1], we introduced two noise settings: '**Noise**' and '**Stronger Noise (SNoise)**', which include rotation and translation randomly. In the 'Noise' setting, the rotation angle ranges from 1 to 5 degrees, and the translation ranges from 0.5cm to 1.0cm. In the 'Stronger Noise' setting, these values are doubled for a more pronounced impact.\n\n#### **Results without training noise:**\nWe first test our model, which has **not** been exposed to any added noise during its training phase, under various noisy conditions. \nThe results, presented in the first row of the table below, show a notable performance decline as noise intensity increases.\nSpecifically, under a 0.7 probability setting, the performance deteriorates from 63.5 to 32.4/31.4 in the 'Noise'/'Stronger Noise' settings. For subsequent analyses, we designate this experiment, utilizing our original model, as the baseline for reference.\n\n#### **Enhancing robustness via training:**\nTo enhance robustness, we incorporated calibration noise during the training phase. This strategy substantially reduces the performance degradation caused by noisy calibration, as shown in the second and third rows of the table below. \n\n#### **Comparative analysis:**\nWhen compared to the baseline (first row), we can observe that training with calibration noise significantly strengthens our model's robustness. It effectively maintains comparable results in noisy calibration conditions.\nThe model trained with 'Stronger Noise' exhibits greater robustness than that trained with less intense noise, underscoring the benefits of this training strategy. \n\nNote: each result in the cell consists of   *F1-score / Accuracy*.\n| **Train @noise (N/SN)\\|**  |  **Eval**  | **+ Noise**      |  |         |   **+ SNoise**           |      |   |\n|------|----------------|--------------|-----------------|--------------|--------------|--------------|--------------|\n|     | **Prob=0.0** | **Prob=0.3** | **Prob=0.5** | **Prob=0.7** | **Prob=0.3** | **Prob=0.5** | **Prob=0.7** |\n| -     @ Prob=0.0      | 63.5 / 92.4 | 52.2 / 89.9 | 40.9 / 85.6 | 32.4 / 82.5 |  52.0 / 89.0 | 40.3 / 83.4 | 31.4 / 79.2 |\n| **N** @ Prob=0.3   | 63.0 / 93.1 | 62.5 / 92.9 | 62.0 / 92.9  | 61.5 / 92.9  | 62.2 / 92.9 | 61.5 / 92.9 | 60.8 / 92.7 |\n| **SN** @ Prob=0.3  | 63.4 / 92.5 | 62.8 / 92.4 | 62.3 / 92.3    | 61.8 / 92.2 | 62.7 / 92.4 | 62.1 / 92.2 | 61.7 / 92.2 |\n\nWe have included these robustness experiments in our updated Appendix A.3 for a comprehensive understanding, with a reference in purple font at the beginning of the main text on page 8. We appreciate your insightful feedback, which has been instrumental in strengthening our work.\n\n**W2: Generalizability**\n\nWe appreciate your concern regarding the generalizability of our proposed method. OpenLane, being the first real-world and large-scale 3D lane dataset built upon Waymo, stands as a unique and valuable resource. The coupling of LiDAR data with OpenLane further enhances its significance.\n\nCurrently, our evaluation is limited to this distinctive dataset due to its pioneering nature and the accessibility of coupled LiDAR data. While we acknowledge the importance of assessing generalizability across diverse datasets, the current focus on OpenLane provides a comprehensive exploration of our multi-modal method within the constraints of available resources. We plan to extend our evaluation to additional datasets as they become available and feasible for our study.\n\nWe appreciate your understanding and consideration.\n\n\n---\nReference\n\n[1] Yu, Kaicheng, et al. \"Benchmarking the robustness of lidar-camera fusion for 3d object detection.\" CVPR 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476247413,
                "cdate": 1700476247413,
                "tmdate": 1700490615366,
                "mdate": 1700490615366,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kAyRGyR6gd",
                "forum": "l1U6sEgYkb",
                "replyto": "Fvs7OKqyOs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3017/Reviewer_ECVn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3017/Reviewer_ECVn"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' feedback and the extra efforts put into robustness experiments."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714236464,
                "cdate": 1700714236464,
                "tmdate": 1700714236464,
                "mdate": 1700714236464,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "w1B2Y4JW0g",
            "forum": "l1U6sEgYkb",
            "replyto": "l1U6sEgYkb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3017/Reviewer_7Rt7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3017/Reviewer_7Rt7"
            ],
            "content": {
                "summary": {
                    "value": "This paper target 3D lane detection problem by proposing a dual view representation method utilizing both camera and Lidar as input. Their proposed method has achieve remarkable improvement over prior work with high efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Compared to prior work that mostly use only camera and feature learning in either Perspective View or Bird Eye View, this work fill the gap by fully investigate feature interaction of both sensor(Lidar+Camera) in both view(PV+BEV). And shows the superiority of their method by showing large improvement over prior work. \n2. The work is very well written, clearly introduce all related work as well as their problems, and how this work target those problem by proposing new solutions. \n3. This work include detail ablation study, make it clear to see the improvement over each step."
                },
                "weaknesses": {
                    "value": "1. There\u2019re some inconsistent in the evaluation result: 1). Although this method have explicitly filling the gap of missing Lidar modality in prior work, it might not seem as an apple to apple comparison in the table. In table 1 1.5m, compare with prior SOTA LATR, the proposed method didn\u2019t achieve overwhelming improvement despite using one extra modality(Lidar). Does that mean the improvement in this range had been saturated to some extent? Also DV-3Dlane use a weaker backbone(ResNet-18/34) compare to ResNet50 LATR, make it even harder to see the full potential of proposed method compare to prior art. 2). Why Use OpenLane1000 in table 1(main experiment) but OpenLane300 for the ablation? Make it incomparable with result in Table1. \n2. For onboard application, it might not always viable, to access both Lidar and Camera information for joint feature, does this method also provide implications for camera only/Lidar only methods? Maybe it\u2019s more clear in ablation study if we could compare Table1 and Table2 directly. A follow up question, if camera-only ablation is possible, could it also show result on Apollo dataset? This is the dataset most commonly used in prior work. \n3. For Figure1, please also consider adding more methods to be more completed, for instance light weight model like GenLane.\n4. What is the \u2018lane-aware prior knowledge\u2019 in the abstract? I feel like \u2018prior knowledge\u2019 it is not discussed in the main text. Please explain or consider rephrase it."
                },
                "questions": {
                    "value": "Please refer to weakness. In general I feel like this paper is well written, happy to raise my score if question addressed.\nSome related papers from prior venue is not properly cited, please consider citing the following works\n\n@inproceedings{liu2022learning,\n  title={Learning to predict 3d lane shape and camera pose from a single image via geometry constraints},\n  author={Liu, Ruijin and Chen, Dapeng and Liu, Tie and Xiong, Zhiliang and Yuan, Zejian},\n  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},\n  volume={36},\n  number={2},\n  pages={1765--1772},\n  year={2022}\n}\n\n@inproceedings{yao2023sparse,\n  title={Sparse Point Guided 3D Lane Detection},\n  author={Yao, Chengtang and Yu, Lidong and Wu, Yuwei and Jia, Yunde},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages={8363--8372},\n  year={2023}\n}\n\n@inproceedings{li2022reconstruct,\n  title={Reconstruct from top view: A 3d lane detection approach based on geometry structure prior},\n  author={Li, Chenguang and Shi, Jia and Wang, Ya and Cheng, Guangliang},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={4370--4379},\n  year={2022}\n}\n\n@inproceedings{ai2023ws,\n  title={WS-3D-Lane: Weakly Supervised 3D Lane Detection With 2D Lane Labels},\n  author={Ai, Jianyong and Ding, Wenbo and Zhao, Jiuhua and Zhong, Jiachen},\n  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},\n  pages={5595--5601},\n  year={2023},\n  organization={IEEE}\n}"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3017/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3017/Reviewer_7Rt7",
                        "ICLR.cc/2024/Conference/Submission3017/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3017/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699253165838,
            "cdate": 1699253165838,
            "tmdate": 1700703291918,
            "mdate": 1700703291918,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vcO1ciuzfw",
                "forum": "l1U6sEgYkb",
                "replyto": "w1B2Y4JW0g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7Rt7 [1/3]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing that our method fills the gap of missing LiDAR modality in prior work. \n\n**W1-1:*Regarding performance improvement and saturation under 1.5m setting***\n\nWe appreciate the reviewer's observation and would like to clarify the evaluation results. First, we'd like to highlight the substantial error reduction by our method in both X and Z near/far ranges, showcasing impressive **46.12\\%/48.26\\%** and **57.33\\%/49.04\\%** improvements, respectively (see Table.1 of the main paper).\n\nWhile the F1 score may not fully capture the extent of this improvement, especially given the leniency of the 1.5m evaluation threshold, we believe there's room for further optimization, such as exploring more effective lane line representations.\n\nMeanwhile, to push beyond the results under the current lenient threshold, we propose to adopt a more strict threshold, 0.5m distance tolerance, for evaluation. This would offer a more nuanced examination of the model's accuracy, with the aim of encouraging further optimization and enhancing performance in alignment with considerations for real-world driving safety.\n\nFurther, to address your concern regarding comparison, we integrated the SOTA method **LATR** with **LiDAR** inputs, projecting all points into the image and using them as additional features in the network. As shown in the following table, the results clearly demonstrate the positive impact of LiDAR, although it falls below the performance achieved by our proposed method. This comparison underscores the unique contribution of our work.\n\nIt's worth noting that in the prior study, M$^2$-3DLaneNet, several multi-modal fusion approaches, such as PointPillar and BEVFusion, were compared, which exhibit subpar performance in the 3D lane task compared to M$^2$-3DLaneNet, falling behind our proposed method.\n\n| Model        | Backbone | F1   | Acc. | X error        | Z error        |\n|--------------|----------|------|------|----------------|----------------|\n| LATR         | Res50    | 54.0 | 91.7 | 0.171 / 0.201  | 0.072 / 0.099  |\n| LATR+LiDAR   | Res50    | 57.4 | 92.1 | 0.167 / 0.185  | 0.071 / 0.088  |\n| DV-3DLane    | Res34    | 63.5 | 92.4 | 0.090 / 0.102  | 0.031 / 0.053  |\n| DV-3DLane    | Res50    | 65.2 | 93.4 | 0.082 / 0.101  | 0.028 / 0.048  |\n\n**W1-2: *Regarding the backbone choice***\n\nThank you for your thoughtful feedback. We appreciate your observation regarding the backbone choice. Our intention was to design an efficient model, leading us to adopt a smaller backbone. However, in response to your concern, we conducted experiments with ResNet-50, demonstrating that a larger backbone can further enhance model performance. The results are presented in the table referenced in **W1-1**. We hope this clarifies our design choice and addresses your comment effectively.\n\n**W1-3: *Regarding using OpenLane300 for the ablation***\n\nWe follow previous works [1] [2] [3] to conduct ablation studies on the subset. \n\n---\nReference\n\n[1] Chen, Li, et al. \"Persformer: 3d lane detection via perspective transformer and the openlane benchmark.\" ECCV 2022.\n\n[2] Huang, Shaofei, et al. \"Anchor3dlane: Learning to regress 3d anchors for monocular 3d lane detection.\" CVPR 2023.\n\n[3] Luo, Yueru, et al. \"LATR: 3D Lane Detection from Monocular Images with Transformer.\" ICCV 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469030412,
                "cdate": 1700469030412,
                "tmdate": 1700469030412,
                "mdate": 1700469030412,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FLll0xcd0F",
                "forum": "l1U6sEgYkb",
                "replyto": "w1B2Y4JW0g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7Rt7 [2/3]"
                    },
                    "comment": {
                        "value": "**W2-1:*Regarding performance on OpenLane-1000 and implications of Camera-only/LiDAR-only methods.***\n\nWe appreciate the reviewer's insightful query about the performance implications of our method with Camera-only and LiDAR-only modalities on the OpenLane-1000 dataset. To provide a comprehensive answer, we conducted two sets of experiments: one with single modality and single view, and another with single modality but dual views.\n\n| Line | Inputs     | View        | Backbone     | F1   | Acc. | X error        | Z error        |\n|------|------------|-------------|------------|------|------|----------------|----------------|\n| \\#1  | Image      | PV          |Res34                | 52.9 | 90.3 | 0.173 / 0.212  | 0.069 / 0.098  |\n| \\#2  | LiDAR      | BEV         |PillarNet34            | 54.1 | 84.4 | 0.282 / 0.191  | 0.096 / 0.124  |\n| \\#3  | Image      | Dual Views  |Res34  | 54.3 | 91.5 | 0.165 / 0.200  | 0.067 / 0.094  |\n| \\#4  | LiDAR      | Dual Views  |PillarNet34 | 55.3 | 87.9 | 0.156 / 0.143  | 0.031 / 0.050  |\n| \\#5  | Image + LiDAR  | Dual Views  |Res34 + PillarNet34 | 63.5 | 92.4 | 0.090 / 0.102  | 0.031 / 0.053  |\n\nIn the first experiment, we evaluated single-modality and **single-view** models, as detailed in Table.2 of the main paper, now applied to the OpenLane-1000 dataset. The outcomes, shown in rows #1 and #2 of the table above, present the performance using individual modalities.\n\nIn the second experiment, we delved into single-modality but **dual-view** approaches:\n- For the image-based model, we adopted a strategy akin to BEVFormer, utilizing deformable attention to transform image features into BEV features. Then, we apply our dual-view decoder upon this, and the outcomes are illustrated in **row #3** of the table above.\n- For the LiDAR-based approach, we projected LiDAR point cloud features onto the 2D image plane to generate perspective-view features. The results of this approach are presented in **row #4** of the table above.\n\nOur results, as demonstrated in the table above, underscore that the dual-view representation significantly enhances the performance of baseline models in single-modal scenarios (comparing #1 with #3 and #2 with #4). This improvement confirms the effectiveness of our dual-view approach in learning 3D lane detection. Most notably, the combination of image and LiDAR modalities, coupled with our dual-view representation, achieves the best results, as shown in **row #5**. This synergy of modalities underlines the superiority of our proposed method.\n\n**W2-2: *Regarding camera-only performance on the Apollo dataset.***\n\nTo address your concern, we conduct our image-only model on Apollo following the Table 2 setting of our main paper.\n\n|     Scene       | Backbone     |   F1   |   AP   | X error    | Z error    |\n|-----------------|---------------|-------|-------|------------|------------|\n| Balanced Scene  | Res34    |  96.4 | 97.6  | 0.046 / 0.299 | 0.016 / 0.213 |\n| Rare Subset     | Res34         |  95.6 | 97.2 | 0.071 / 0.664 | 0.025 / 0.568 |\n| Visual Variations | Res34       | 91.3 |  93.4 | 0.095 / 0.417 | 0.040 / 0.320 |\n\n**W3: *Including more methods in Figure.1***\n\nWe appreciate the reviewer's valuable suggestion regarding enhancing Figure 1 by including additional methods.\n\nOur current results for GenLaneNet and 3DLaneNet are referenced from prior works, like PersFormer [1], Anchor3DLane [2]. Unfortunately, the pretrained models for these methods are not yet publicly available, which hinders our ability to directly evaluate their performance under the 0.5m setting. We will attempt to reproduce these results and assess them in the 0.5m setting. We kindly request your understanding in this matter, given the time constraints of this discussion period.\n\n\n---\nReference\n\n[1] Chen, Li, et al. \"Persformer: 3d lane detection via perspective transformer and the openlane benchmark.\" ECCV 2022.\n\n[2] Huang, Shaofei, et al. \"Anchor3dlane: Learning to regress 3d anchors for monocular 3d lane detection.\" CVPR 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472181400,
                "cdate": 1700472181400,
                "tmdate": 1700490854778,
                "mdate": 1700490854778,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CR08jedVpI",
                "forum": "l1U6sEgYkb",
                "replyto": "w1B2Y4JW0g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3017/Reviewer_7Rt7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3017/Reviewer_7Rt7"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks author for the detail reply and additional experiment provided. \n\nW3: GenLane have their open source code on github and the result should be pretty easy to reproduce. \n\nW4: I still find the term 'prior knowledge' confusing, work [1] and [2] shows example of adopting 'prior' in lane formation. What you describe is more like 'lane aware query' or 'lane aware anchor', than 'prior knowledge'. If you still want to use the word 'prior', you should provide explanation comparing prior method also adopting 'prior'.\n[1]Reconstruct from top view: A 3d lane detection approach based on geometry structure prior\n[2]Learning to predict 3d lane shape and camera pose from a single image via geometry constraints\n\nAuthor should consider putting results on other dataset, like Apollo into the main paper with detailed comparison to prior method, as this is one of the earliest and most popular 3d lane detection dataset. Instead, you can put detail ablation to appendix.\nBeside, author should modify the paper as promised above to reduce misunderstanding in mismatch in experiment settings. And integrate all changed above to your paper. \n\nOther than that, I am satisfied with the current work. I have changed my rating to 6."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703266622,
                "cdate": 1700703266622,
                "tmdate": 1700703280716,
                "mdate": 1700703280716,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w3gewoIGJn",
                "forum": "l1U6sEgYkb",
                "replyto": "w1B2Y4JW0g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3017/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the feedback from the reviewer. Below, we address the reviewer\u2019s concerns:\n\n**W3:** We thank the thoroughness of the reviewer. GenLaneNet is indeed accessible on GitHub, based on Apollo. We have attempted to adapt it to the OpenLane dataset but didn\u2019t achieve the results reported in prior work. Therefore, for the sake of precision in our comparison, we haven't included GenLaneNet in the figure under the 0.5m setting.\nNonetheless, we are still attempting to reproduce the results and aim to evaluate them under the 0.5m setting. We appreciate your understanding and would love to share any additional insights gained from these efforts.\n\n**W4-1: *Regarding to 'prior knowledge'***\n\nWe acknowledge the potential misunderstanding, and to ensure clarity, we have modified our manuscript to consistently use the term \"lane-aware knowledge\", **without using 'prior'**,  to avoid any confusion with the previously used \"geometry prior\"[1] or \"geometry constraints\"[2]. \n\nThese revisions have been incorporated into our updated manuscript, and we appreciate your diligence in bringing this concern to our attention. Please refer to the revised version for further clarification.\n\n**Results on Apollo:** \n\nWe appreciate the valuable suggestion from the reviewer. It's important to note that the Apollo dataset is exclusively an Image-only dataset without LiDAR information, while our work specifically focuses on multi-modal 3D lane detection. \n\nFurther, given the constraints of space in the main paper and the need for a comprehensive table to present results on **Apollo**, we have opted to include the detailed experimental results in **Appendix A.6**.  This ensures maintaining an integrated presentation in the main body of the paper. We believe this approach allows for a more in-depth examination without compromising the main narrative. we appreciate your understanding.\n\n**Singel-modality & Dual-view Results on OpenLane-1000:**\n\nWe appreciate the valuable suggestion from the reviewer. \n\n- We have integrated the **single-modality results** (Image-Only / LiDAR-Only) based on OpenLane-1000, **LATR + LiDAR** results, and the results of **our ResNet50-based model** (now termed *DV-3DLane-Large*) into the main results in the updated main text (See Table.1). Our previous ResNet34-based model is now termed as *\"DV-3DLane-Base\"* and ResNet18-based model is now termed as *\"DV-3DLane-Tiny\"* in the Table.1 of the manuscripts.\n\nCorrespondingly, the performance of DV-3DLane-(Tiny / Base / Large) are all included in the Figure.1 & Appendix various scenario study table.\n\nFurther,  we have moved the bidirectional feature fusion ablation study into our Appendix A.5.\n\nAll the above changes have been included in our revised version, *highlighted in red*. Please kindly refer to our updated manuscripts.\n\n---\n[1] Reconstruct from top view: A 3d lane detection approach based on geometry structure prior.\n\n[2] Learning to predict 3d lane shape and camera pose from a single image via geometry constraints."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741644889,
                "cdate": 1700741644889,
                "tmdate": 1700741806507,
                "mdate": 1700741806507,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ju3R2Fez4g",
            "forum": "l1U6sEgYkb",
            "replyto": "l1U6sEgYkb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3017/Reviewer_nYY6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3017/Reviewer_nYY6"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a LiDAR Camera 3D-Lane detection model. This model consists of 4 main blocks: 1) two backbones (image in camera view (CV) and LiDAR (BEV) in bird-eye-view) linked by a bidirectional feature fusion, 2) a dual view query generation with clustering between BEV and CV queries, 3) a decoder with query clustering that produce point queries and 4) a 3D dual-view deformable attention producing a 3D lane prediction. The experimental part shows that the proposed model outperforms SOTA, including the last ICCV2023 papers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "One original contribution is the bidirectional feature fusion module used to train the backbones in both BEV and CV leveraging for each view information from both sensors. The experimental part shows that this mechanism increases the F1 score about 2%. \nAnother important part of the pipeline is a Unified Query Generation process that win less than 1% F1 score. \nThe authors a 3D Dual-view Deformable Attention model that slightly improves the F1 score of the model. \nRegarding the ablation study, the sensor fusion win more than 10% regarding the LiDAR only and about 20% regarding only the camera. \nExperiments have been achieved on the public dataset OpenLane. The proposed model outperforms SOTA."
                },
                "weaknesses": {
                    "value": "there are no really weakness in the paper. \nI should be interesting to experiment your models on other datasets like  OpenLane-Huawei: a dataset with more horizontal lines.\n\nminor typo : \n\ninstruction <-> introduction\nIncrease or remove line (d) of fig 7. (figures are too small)"
                },
                "questions": {
                    "value": "Did you try to experiment your models on other datasets like  OpenLane-Huawei: a dataset with more horizontal lines?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3017/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3017/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3017/Reviewer_nYY6"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3017/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699808107182,
            "cdate": 1699808107182,
            "tmdate": 1699808107182,
            "mdate": 1699808107182,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kQnNbdZTPD",
                "forum": "l1U6sEgYkb",
                "replyto": "Ju3R2Fez4g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nYY6"
                    },
                    "comment": {
                        "value": "**W1: Typo and Presentation**\n\nThank you for bringing the minor typo and visualization to our attention. \n- We have fixed the typo, changing \"Instruction\" to \"Introduction.\"\n- We have addressed the concern regarding the size of row (d) in Figure 7 by making improvements to the plot. Moreover, We have included corresponding plot improvements in the Appendix. Additionally, a zoom-in reminder has been incorporated into the captions, highlighted in *brown font* for the convenience of your review (*Please kindly refer to the updated manuscript*).  \n\nWe believe these adjustments effectively address the raised concerns.  Your thorough review is greatly appreciated.\n\n**Q1:** \nWe thank you for your positive evaluation of our work and appreciate your suggestion to test our models on the OpenLane-Huawei dataset, which features more horizontal lines. We acknowledge the potential benefits of such an experiment; however, there are specific reasons we have not yet applied our method to this dataset during this discussion period.\n\nFirst, the OpenLane-Huawei dataset, primarily designed for topology reasoning among centerlines and traffic elements, is based on multi-view images. Our current methodology is based on the single-view setting, and adapting it to multi-view contexts is non-trivial. We recognize the importance of this adaptation and plan to explore it in future studies.\n\nSecond, the OpenLane-Huawei dataset comprises two subsets: `SetA` and `SetB`. SetB, sourced from Nuscenes, Z values of centerline annotations are set to 0s built upon HD map. In this work, we focus on 3D space lane detection, making SetB unsuitable for our current approach.\n\nRegarding SetA, sourced from Argoverse-v2, it does have valid z-coordinate information. Nonetheless, the OpenLane-Huawei dataset, centering on the vision-centric approach, does not officially provide LiDAR data. While it's possible to obtain this data from the Argoverse repository, the large scale of the dataset posed a challenge for us to download and integrate within our work during the limited discussion period.\n\nIn light of these considerations, we were unable to apply our method to the OpenLane-Huawei dataset during this discussion period. We sincerely appreciate your understanding in this regard. Moving forward, we plan to extend our method to incorporate both its camera and LiDAR data and will ensure to share our findings, as well as release the source code.\n\nThank you again for your insightful suggestions and support for our work."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700464763555,
                "cdate": 1700464763555,
                "tmdate": 1700555491192,
                "mdate": 1700555491192,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hhd2nOh5Zw",
                "forum": "l1U6sEgYkb",
                "replyto": "kQnNbdZTPD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3017/Reviewer_nYY6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3017/Reviewer_nYY6"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the detail reply. I appreciate this feedback and the answers given to my questions and concerns."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725089514,
                "cdate": 1700725089514,
                "tmdate": 1700725089514,
                "mdate": 1700725089514,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]