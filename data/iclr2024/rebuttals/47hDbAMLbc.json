[
    {
        "title": "OPTIMAL ROBUST MEMORIZATION WITH RELU NEURAL NETWORKS"
    },
    {
        "review": {
            "id": "JRYR8GiqmQ",
            "forum": "47hDbAMLbc",
            "replyto": "47hDbAMLbc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5108/Reviewer_LzNK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5108/Reviewer_LzNK"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of robust memorization, namely exactly fitting the training data while keeping the same prediction in a small neighborhood of each training data. In its first result, it shows that it is NP-hard to compute a robust memorization of the simple network of depth 2 and width 2. It then provides a necessary condition on the width, depth and number of parameters for the existence of robust memorization. It further constructed a neural network which is a robust memorization."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It extends the prior results (Li et al 2022) on robust memorization from $\\lambda/4$ to any value strictly less than $\\lambda/2$.\n\nThe number of parameters in the constructed neural network which is a robust memorization does not depend on the separation bound $\\lambda_D$ or the robust budget $\\mu$.\n\nThe paper is well-organized and clearly written."
                },
                "weaknesses": {
                    "value": "1. The term \u201coptimal robust memorization\u201d is inappropriate, and might be misleading and over-claims the significance of the work. Note that robust memorization with radius $< \\lambda_D/2$ is not significantly different from that with radius $< \\lambda_D/4$, except it is a bit larger neighborhood. This is because $\\lambda_D$ is just the minimal distance, not necessarily the distance for every pair of data samples. Hence, even in the case of the so-called \u201coptimal\u201d $\\lambda_D/2$-robust memorization, there are still many regions that are not covered by the robust-neighborhoods, and it can be non-robust in those areas. Therefore, $\\lambda_D/2$-robust memorization does not really make much difference than the  $\\lambda_D/4$-robust memorization. (The word \u201coptimal\u201d only reflects it is the largest radius in the minimal separation based analysis, however, as I mentioned above, it is far from an optimal robust memorization). Therefore, I would consider the contribution of this paper is on enlarging the robust memorization region, which is limited.\n\n2. It seems to me that some results are not consistent. Proposition 4.7 part 2 already infers that depth 2 width 2 network is not a robust memorization. However, Theorem 4.1 claims that it is NP-hard. I hope the authors can clarify on this point. \n\n3. The discussion below Theorem 1.1 is not quite correct. It somehow avoided the case that the absence of memorization implies the absence of robust memorization. Hence, it is not totally \u201ccannot be deduced from each other\u201d."
                },
                "questions": {
                    "value": "I would like to see some intuition on why the number of non-zero parameters does not depend on $\\lambda_D$, $L$ and $\\mu$. Especially, a comparison with the prior work of Li. et. al. 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5108/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697764334567,
            "cdate": 1697764334567,
            "tmdate": 1699636503012,
            "mdate": 1699636503012,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DO12bqJRnl",
                "forum": "47hDbAMLbc",
                "replyto": "JRYR8GiqmQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5108/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5108/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Detailed Answer"
                    },
                    "comment": {
                        "value": "Thanks for your valuable comments and questions. We give a detailed answer to them and hope that you could re-evaluate our paper based on these answers. \n\n1. The term \u201coptimal robust memorization\u201d is inappropriate.\nThis comment consists of two major issues.\n\n1.1 ``Robust memorization with radius $\\lambda/2$ is not significantly different from $\\lambda/4$, except that it is a bit larger neighborhood''\n\nAnswer:\nPlease note that the difference between $\\lambda/2$ and $\\lambda/4$ is not only in the size of the robust neighborhood. Radius $\\lambda/2$ is the largest possible robust neighborhood that can be achieved, if using the same radii for all samples, and it is in this sense we call our robust memorization optimal.\n\n1.2 ``$\\lambda/2$ is just the minimal distance, not necessarily the distance for every pair of data samples.''\n\nAnswer:\nTo the best of our knowledge, all papers that work on adversarial robustness use the same robust radius for all samples. It is a nice idea to use different radii for different samples, but this needs to consider $O(N^2)$ radii for $N$ samples and the number of parameters of the memorization network is expected to be larger.\n\nIn summary, {\\bf our optimal robust memorization is not indeed absolute optimal, but it is a type of optimal}. To make it more clear, we will change 'optimal' to 'optimal with respect to robust radius'.\n\n2. It seems to me that some results are not consistent.  Proposition 4.7 part 2 already infers that depth 2 width 2 network is not a robust memorization. However, Theorem 4.1 claims that it is NP-hard. \n\nAnswer:\nThese two results are not contradictory.\n\nProposition 4.7 part 2 shows that, {\\bf there exists a dataset  D}, all networks with depth 2 and width 2 are not robustness memorizations for D.\n\nTh 4.1 shows that, there is no polynomial-time algorithm to decide whether there exists a robustness memorization with depth 2 and width 2 {\\bf for any given dataset D}. Note that there do exist data sets D such that networks of depth 2 and width 2 are robustness memorizations for D.\n\n3: The discussion below Theorem 1.1 is not quite correct. It somehow avoided the case that the absence of memorization implies the absence of robust memorization. \n\nAnswer: We believe that The Discussion is correct. The purpose of this part is to show ``NP-hardness of computing robust memorization for a non-zero budget and the NP-hardness of computing memorization cannot be deduced from each other:''\nThe two cases listed subsequently in the paper already serve our purpose.\n\nThe reviewer is correct that we did not list the case ``absence of memorization implies the absence of robust memorization''. We will discuss this case in the revised version. But this case does not affect the correctness of our conclusion.\n\n4: Intuition on why the number of non-zero parameters does not depend on $\\lambda,\\mu,L$.\n\nAnswer: The conclusion of Li2022 is based on some approximate results of the Relu network for polynomials and $1/x$. However, it is obvious that Relu network is locally linear, its growth rate is lower than polynomial or $1/x$ when $x\\to \\infty$ or $x\\to 0$, so this approximation must be within a certain range, and the number of parameters required for approximation is related to this range, so their conclusions need to be based on $\\lambda,\\mu$.\n\nOur work does not use the classical approximation theorem. We make full use of every parameter of the network and make the meaning of these parameter clear. For example, some parameters are used to measure the distance between the input and the point in the training set, so that $\\lambda,\\mu$ only affects the value of these parameters, but not the number of parameters;  some parameters are used to predict the label of inputs, so that $L$ only affect the value of these parameters, but not the number of parameters."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5108/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699775278191,
                "cdate": 1699775278191,
                "tmdate": 1699775278191,
                "mdate": 1699775278191,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "123IEgzmKy",
            "forum": "47hDbAMLbc",
            "replyto": "47hDbAMLbc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5108/Reviewer_Sknc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5108/Reviewer_Sknc"
            ],
            "content": {
                "summary": {
                    "value": "The authors study memorization with neural networks and its connection to deep learning. It emphasizes the significance of \"robust memorization,\" which hasn't been thoroughly explored. The passage mentions the NP-hardness of computing certain network structures for robust memorization and introduces the concept of \"optimal robust memorization.\" It highlights the explicit construction of neural networks with specific parameter counts for optimal memorization. There's also a mention of a lower bound on network width and controlling the Lipschitz constant to achieve robust memorization in binary classification datasets. It is a technical paper addressing these aspects of neural network memorization and generalization. However, it does not provide a clear path for interested readers to understand them."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The strengths of the provided passage are its technical depth, problem formulation, explicit solutions, and mention of a lower bound. It delves into the complexities of neural network memorization, introduces a significant problem in deep learning, provides practical solutions, and hints at valuable insights for network design."
                },
                "weaknesses": {
                    "value": "It is highly technical and requires a strong background in readers to grasp the meaning of this paper."
                },
                "questions": {
                    "value": "The authors may think about the organization of this paper. ICLR may not be a suitable conference for this paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5108/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698498571008,
            "cdate": 1698498571008,
            "tmdate": 1699636502926,
            "mdate": 1699636502926,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0fmn4Y72iN",
                "forum": "47hDbAMLbc",
                "replyto": "123IEgzmKy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5108/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5108/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Learning theory is a topic of ICLR and our paper fits this topic very well"
                    },
                    "comment": {
                        "value": "Thanks for your valuable comments. We give a detailed answer to them and hope that you could re-evaluate our paper based on these answers. \n\n1. It is highly technical and requires a strong background in readers to grasp the meaning of this paper. The authors may think about the organization of this paper.\n\nAnswer: \nIndeed, our paper is  technical and we tried our best to organize the paper to improve readability. For instance, we give informal description of the main results in Introduction Section; give proof sketches for most of the main results; and give remarks/explanations/comparisons after the main results.\nWe believe that {\\bf the main text of the paper should be understandable without difficulty}. We admit that the proofs in the Appendix are quite technical and tried our best to make the proofs more easily readable, including separate the long proofs into several parts and use figures for illustrations. \n\n2.  ICLR may not be a suitable conference for this paper.\n\nAnswer: \nNote that ``Learning theory'' is one of the official topics of ICLR and our paper fits this topic very well."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5108/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699772164838,
                "cdate": 1699772164838,
                "tmdate": 1699772164838,
                "mdate": 1699772164838,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yCxW0wYLua",
                "forum": "47hDbAMLbc",
                "replyto": "123IEgzmKy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5108/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5108/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Sorry, I mistakenly posted something here just now. Please ignore it."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5108/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700226741908,
                "cdate": 1700226741908,
                "tmdate": 1700226938842,
                "mdate": 1700226938842,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BgcAUj9Dh8",
                "forum": "47hDbAMLbc",
                "replyto": "yCxW0wYLua",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5108/Reviewer_Sknc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5108/Reviewer_Sknc"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "Thanks for answering my questions. I read your rebuttal and still keep my score. This paper is technical but may not be suitable for a conference paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5108/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608270359,
                "cdate": 1700608270359,
                "tmdate": 1700608270359,
                "mdate": 1700608270359,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1msB3HKXlR",
            "forum": "47hDbAMLbc",
            "replyto": "47hDbAMLbc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5108/Reviewer_MXq8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5108/Reviewer_MXq8"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the complexity and necessary conditions of robust memorization for ReLU networks. Since it is NP-hard to decide whether there exists a small network which is a robust memorization of a given dataset with a robust budget, studying necessary conditions is very important. Two important results are given in the paper. Let n be the input dimension and N be the number of datapoints. First, under a reasonable setting, a network with width smaller than n can not be robust memorization for some dataset and robust budget. Furthermore, there exists a network with width $3n+1$ and depth $2N+1$, and $O(Nn)$ nonzero weights such that a robust memorization is achieved. However, in this case, the values of the parameters can go to infinity when the robust budget is increased. To address this case, the second important result of this paper utilizes a deeper network to guarantee a bounded Lipschitz constant of the network, provided that the underlying classification problems are binary. The depth in this case is increased by a factor of $log(n)$."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Originality: Most existing memorization bounds are derived without optimal robustness. Given that importance of robustness, the proposed necessary conditions for ReLU networks are novel and interesting.\n\n- Quality and clarity: This paper gives a comprehensive presentation on the existence of of ReLU networks that have robust memorization. The background knowledge is well-organized, and the theoretical results are presented in a flow that is easy to follow and understand.\n\n- Significance: The family of ReLU networks is an important architecture and understanding the limitations of memorization is crucial. The new estimate $O(Nn)$ is an improvement over Theorem 2.2 in (Li et at., 2022) in the sense that it achieves stronger robustness with less number of parameters without assuming binary classification."
                },
                "weaknesses": {
                    "value": "- The paper is mainly dedicated to the existence of robust training. No results on optimization or robust generalization are derived. Given that, the scope seems to be quite limited.\n\n- Since overparameterization can often lead to powerful memorization and good generalization performance, the necessary conditions may have stronger implications if they are connected to generalization bounds. It is not clear in the paper that the constructions of ReLU networks for robust memorization would lead to robust generalization. I know the authors acknowledge this in the conclusion, but I think this is a very serious question.\n\n- The main theorems 4.8 and 5.2 only guarantee the existence of optimal robust memorization. These results would be more useful if an optimization or constructive algorithm is given to find the optimal memorization."
                },
                "questions": {
                    "value": "1. The Theorem 2.2 in (Li et al., 2022) is derived for $p>=2$. However, the bound given in Theorem 4.8 is only valid for the infinity norm. The authors may want to point out that in the paper. The bound given by Theorem B.3 seems to be a bit worse than the bound given by Theorem 2.2 in (Li et al., 2022). I think it would be also helpful to compare such a case in the main text. What are the main difficulties for deriving bounds under $p$-norm?\n\n2. Given that the existence of optimal robust memorization is guaranteed under a ReLU network with bounded size, would it be possible to arrive at such a solution using any optimization algorithm? What would be the complexity of such an algorithm?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5108/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5108/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5108/Reviewer_MXq8"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5108/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821296545,
            "cdate": 1698821296545,
            "tmdate": 1699636502827,
            "mdate": 1699636502827,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gSJdTQiDIU",
                "forum": "47hDbAMLbc",
                "replyto": "1msB3HKXlR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5108/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5108/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Detailed Answer"
                    },
                    "comment": {
                        "value": "Thanks for your valuable comments and questions. We give a detailed answer and hope that we have addressed your concerns. The review is focused on four issues and we will answer them separately. \n\n1. The main theorems 4.8 and 5.2 only guarantee the existence of optimal robust memorization. These results would be more useful if an optimization or constructive algorithm is given to find the optimal memorization.\n\nAnswer: \nWe did give constructive algorithms for the optimal robust memorization networks.\nPlease see Appendix B.5 on page 28 (proof for Theorem 4.8) and Appendix C.2 on page 33 (proof for Theorem 5.2). We will make this clear in the revised version of the paper.\n\n2. Given that the existence of optimal robust memorization is guaranteed under a ReLU network with bounded size, would it be possible to arrive at such a solution using any optimization algorithm? What would be the complexity of such an algorithm?\n\nAnswer: As said in 1, we give direct constructive algorithms for the optimal robust memorization networks, and these are polynomial-time algorithms.\n\nTo the best of our knowledge, all works that can give memorization with polynomial number of parameters are direct construction of the networks from the data sets, and no one used optimization methods such as SGD. This is because using SGD cannot in general to give theoretically guaranteed memorization networks.\n\n3. Robust generalization. It is not clear in the paper that the constructions of ReLU networks for robust memorization would lead to robust generalization. I know the authors acknowledge this in the conclusion, but I think this is a very serious question.\n\nAnswer: \n\n3.1. Having  memorization neural networks with theoretically guaranteed robust generalization ability is one of the most important problems in adversarial learning, and worth studying.\nTo the best of our knowledge, this is still an open problem, and all existing works on memorization do not have theoretical results on generalization.\n\n3.2 Indeed, this work is primarily focused on the expressive power of neural networks, and no theoretical result on generalization is given.\nIn our ongoing work, we link generalization to memorization. Under the assumption that the samples in the dataset are i.i.d. selected from the data distribution, we give generalization analysis of memorization networks. Moreover, for data distributions that satisfy certain assumptions, our Theorems 4.8 and 5.2 also guarantee generalization if the sample in dataset is sufficient. \n\n4.  The Theorem 2.2 in (Li et al., 2022) is derived for  $p\\ge2$ (actually it is $p\\in\\\\{2, \\infty\\\\}$). However, the bound given in Theorem 4.8 is only valid for the infinity norm. The authors may want to point out that in the paper. The bound given by Theorem B.3 seems to be a bit worse than the bound given by Theorem 2.2 in (Li et al., 2022). I think it would be also helpful to compare such a case in the main text. What are the main difficulties for deriving bounds under p-norm?\n\nAnswer:  We are grateful that you read our Appendix. We answer this question in three parts.\n\n4.1 It is not difficult to give an upper bound of robust memorization under the $L_p$ norm. Theorem B3 shows that the required number of parameters in relation to $p$ is no more that $O(Nnp^2\\log(n/\\gamma))$. For the case of $p=1$, Theorem B2 gives a good bound similar to Theorem 1.1. But it is desirable and difficult to have a bound which is not essentially dependent on p.\n\n4.2 To come up with that in Li2022. When $p=2$ and $\\gamma=\\lambda_D^p/4$, our bound in Theorem B.3 becomes $O(Nn\\log(n/\\lambda_D^p))$, but the result in Li2022 is $O(Nn\\log(n/\\lambda_D^p)+O(Npoly\\log(N/\\lambda_D^p)))$. Our result is better than that of Li2022.\n\n4.3 We focus mainly on the $L_\\infty$ norm in this work, so the results on other norms are not mentioned in the main text. We will add these comparisons in the revised version."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5108/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699774623625,
                "cdate": 1699774623625,
                "tmdate": 1699774623625,
                "mdate": 1699774623625,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w0m1UD4oYD",
                "forum": "47hDbAMLbc",
                "replyto": "gSJdTQiDIU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5108/Reviewer_MXq8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5108/Reviewer_MXq8"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their detailed answers. Most of my concerns have been addressed.\n\nRegarding your claim \"all existing works on memorization do not have theoretical results on generalization.\"\n\n1. Could you further clarify or provide some references to support your claim?\n2. By avoiding memorizing training data, would it be possible to achieve higher generalization performance?\n3. Is the usual setting used to prove better generalization not in favor of memorization?"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5108/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700187623803,
                "cdate": 1700187623803,
                "tmdate": 1700187623803,
                "mdate": 1700187623803,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U4xGdgI3p4",
                "forum": "47hDbAMLbc",
                "replyto": "OeoZhAOykA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5108/Reviewer_MXq8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5108/Reviewer_MXq8"
                ],
                "content": {
                    "comment": {
                        "value": "I extend my sincere thanks to the authors for their comprehensive responses. I appreciate the thorough addressing of my concerns, and I am now confident in affirming that this paper makes valuable contributions to the field of memorization."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5108/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643181637,
                "cdate": 1700643181637,
                "tmdate": 1700643181637,
                "mdate": 1700643181637,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]