[
    {
        "title": "Do Pre-trained Transformers Really Learn In-context by Gradient Descent?"
    },
    {
        "review": {
            "id": "uZh0zeZIhK",
            "forum": "992eLydH8G",
            "replyto": "992eLydH8G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8711/Reviewer_pVVn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8711/Reviewer_pVVn"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents arguments against the hypothesis that in context learning ICL emulates gradient descent in trained transformers. The arguments can be listed as follows:\n* The objective considered in [0] is regression and \"is very different from how real language models are trained are trained on the [causal language modeling] objective.\"\n* Gradient descent is agnostic to the sample order in the batch. ICL is not agnostic to the sample order in the batch.\n* The construction of the transformer in~\\ref{} is contrived.\n\nThe paper also presents additional empirical evidences with Llama-7b to support their argument. In summary, they argue that fine-tuning the model does not lead to model outputs that are equivalent to the one obtained with in context learning.\n\n[0] https://arxiv.org/pdf/2212.07677.pdf"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper challenges an emerging theory that transformers learn In Context Learning by gradient descent, which may help bridging the gap between the theory and practical observation. The authors make an effort to be formal about the definitions put forward in the paper, helping in clearing up some of the ideas put forward."
                },
                "weaknesses": {
                    "value": "**The paper presents several arguments with little substance and cohesion.**\nFor example, the authors claims that the because [0] made their analysis with linear regression then it cannot be comparable to a model trained with causal language modeling. This claim can both be true or false. For example, one could argue that training with linear regression leads to exactly the same solution than a model trained with causal language modeling under certain condition. The authors should prove that the model trained with linear regression leads to a different solution than the model with causal language modeling and not merely state it.\n\n**The authors make several arguments that do not prove the main thesis**\nMy understanding of the main thesis of this work is that the ICL setup presented in [0] is not equivalent to gradient descent. However, the setup in [0] considers linear self-attention and not general transformers trained with causal masking. To make their point, the authors should explicitly say what is wrong in the work or setup of [0].\n\nIf what the authors is trying to say is that the setup of [0] is contrived, then the thesis is not very surprising or significant as no one would be astonished to learn that linear self-attention and hand crafted parameters are unrealistic setups.\n\nMinor:\n* The font of some of the figures is a bit too small making it hard to read.\n* The font across the figures and the text is not consistent.\n\nFinally, I would like to encourage the authors to revisit the style of their article. While reading their work, I found the writing to be adversarial against an emerging line of work, which could potentially turn out to be, at least partially, true. Instead, having a constructive writing where they, for example, build on top of the existing theory or correct part of the theory would be more enjoying for me to read than a paper that tries to prove a line of work to be wrong.\n\n[0] https://arxiv.org/pdf/2212.07677.pdf"
                },
                "questions": {
                    "value": "I gave most of my comments/suggestions in the weakness section.\n\n* Under what conditions does a model as considered in [0] does not lead to transformers that learn in-context by gradient descent?\n* Prove that a model as considered in [0] does not lead to transformers that learn in-context by gradient descent.\n* What elements of [0] leads to a contradiction that models as considered in [0] does not lead to transformers that learn in-context by gradient descent?\n\n[0] https://arxiv.org/pdf/2212.07677.pdf"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8711/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698696848894,
            "cdate": 1698696848894,
            "tmdate": 1699637092837,
            "mdate": 1699637092837,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EDJG6ehKQT",
                "forum": "992eLydH8G",
                "replyto": "uZh0zeZIhK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their review and request them to see **our general response highlighting the changes that were made to the initial draft**.\n\n> The authors make several arguments that do not prove the main thesis.\n> the authors should explicitly say what is wrong in the work or setup of [0].\n\nPlease refer to our general response that clarifies our main thesis. As we clarify in the general response, our intention is **not** to prove the existing line of work wrong. Rather, we are studying the curious problem of whether the existing results stretch beyond the boundaries of their assumptions. This is clarified in our updated draft and we request the reviewer to take a look at it.\n\n> ... authors claims that the because [0] made their analysis with linear regression then it cannot be comparable to a model trained with causal language modeling ...\n\nWe would like to clarify the difference between ICL and CLM objectives which has been detailed in our **updated section 3.1**. \u201cCLM objective\u201d refers to training of transformers on _task-agnostic_ data. This general-purpose data does not specifically train transformers to perform ICL on a narrow family of tasks. In contrast, \u201cICL objective\u201d refers to training of transformers exclusively and explicitly on the family of tasks, on which ICL is evaluated later. To gain insights into how ICL is achieved by LMs, choosing a simpler family of tasks like linear regression for analysis is perfectly reasonable, as long as the learning setup remains analogous to real world setups so that the conclusions can be reasonably extrapolated. Therefore, our problem is not with linear regression (the choice of family of tasks), rather how the transformers are trained and the misrepresentation of ICL in that setting. \n\n> \u2026  the authors should prove that the model trained with linear regression leads to a different solution\n\nWe have highlighted many fundamental differences between emergent ICL and the studied $\\widehat{\\text{ICL}}$ in previous works. We believe that these differences are reasonable evidence that they\u2019re likely not the same. If one claims that they are indeed the same, we believe they should prove it.\n\n> the style of their article \u2026  adversarial against an emerging line of work \n\nThanks for sharing this concern. We certainly did not mean to appear adversarial to the prior work or dismiss them \u2013 these are excellent and innovative works. At the same time, the community in our academic discourse would benefit from added clarity on the implications of these works, which we worry may be misunderstood.\n\nIn our revision, we have tried to revise all the statements so that they\u2019re objective and not over-claiming. If you feel that there is a place that would benefit from further precise statements, **please let us know**. Thank you! \n\n> font of some of the figures, consistency \n\nWe have tried to make the font larger and more consistent. Please let us know if anything else catches your eye."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535656764,
                "cdate": 1700535656764,
                "tmdate": 1700536599731,
                "mdate": 1700536599731,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CWlUGZo8h8",
            "forum": "992eLydH8G",
            "replyto": "992eLydH8G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8711/Reviewer_ivcd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8711/Reviewer_ivcd"
            ],
            "content": {
                "summary": {
                    "value": "The paper conducts a deeper study into the hypothesis of in-context learning in LLMs as a simulation of gradient descent on an auxiliary model. The authors formalize a few sets of functional properties and compare in-context learning (ICL) and gradient descent (GD) on these properties.  The authors observe inconsistencies between the two algorithms in different settings and hence provide empirical evidence against the equivalence of ICL and GD in the realistic setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written and the logic is easy to understand. The authors mention the functional properties with which they compare in-context learning and gradient descent on a large LLM model. In their experimental study, instead of simply comparing the two algorithms in terms of performance, the authors use metrics like token overlap and cosine similarity to show a clear distinction between the two algorithms. Overall, this paper provides an extensive study pointing out the differences between the two algorithms and provides clear insights into how the community can redesign the existing hypothesis for realistic model settings."
                },
                "weaknesses": {
                    "value": "I have a few questions on the experimental study. Please find them below.\n\n(a) **Hypothesis under study in section 4:**  In this section, the authors assume equivalence between ICL and GD on the same model. However, Akyurek et al.'22, and Oswald et al'23 argue that transformers train a small auxiliary model (different from the parent transformer) inside. Hence, the setting that the authors consider aligns more with the result of [1], which claims in-context learning as implicit optimization of the same parent model. So, if I understand currently, the authors are simply refuting [1]'s hypothesis. Can the authors comment on this discrepancy? If the authors want to refute the hypothesis completely, then maybe they need to search for all possible sets of auxiliary models that can fit inside.\n\n(b) **Arguments against sparsity:** I believe, the argument that trained transformers are not sparse, as given by the constructions of Akyurek et al.'22, and Oswald et al'23, isn't a valid argument for the discrepancy between the two algorithms because the previous works simply aim to give an expressivity result on transformers. It is certainly possible that transformers find a dense and more compressed solution to simulate gradient descent. To completely refute the argument, the authors need to refute the probing experiments that the previous works did to search for traces of gradient descent inside these trained models (which I believe is a herculean task).\n\nFurthermore, instead of simply looking at the movement of the transformer weights across training to argue that the model doesn't stabilize to a single sparse solution, maybe the authors can come up with experiments to suggest that the model changes its internal mechanism across training, instead of simply using different weight matrices to represent the same internal mechanism across time (which again is a herculean task).\n \n\n(c) In section 4, the model has been fine-tuned with a cross-entropy loss for GD, with the candidate set being the entire vocabulary. Are the contextual examples concatenated with the test query, like [2]? This setting is more likely since ICL uses demonstrations concatenated, which provides a prior to the right candidate set. \n\nFurthermore, instead of simply optimizing the cross-entropy loss with the entire vocabulary being the candidate set, maybe the authors can put more weight on the relevant logits during training and inference.\n\n(d) What do an overlap rate and cosine similarity of 1.0 mean and < 1.0 mean for ICL in 1 demonstration and >2 demonstration settings in Figure 4? \n\nOverall, I believe the paper attempts to take a deep dive into a very difficult question using simple experiments, which is impressive in itself. However, as far as I understand, these experiments don't refute the hypothesis of equivalence between GD and ICL completely. Instead, they simply ask the community to make small changes to the hypothesis (e.g. SGD in place of GD, auxiliary internal model isn't the same parent model, etc.). Hence, I have a slightly lower score but am happy to discuss it during the rebuttal period. \n\n\n1: Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers. Dai et al'23.\n\n2: Making Pre-trained Language Models Better Few-shot Learners. Gao et al'21."
                },
                "questions": {
                    "value": "Please see my questions in the previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8711/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8711/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8711/Reviewer_ivcd"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8711/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782710992,
            "cdate": 1698782710992,
            "tmdate": 1700667360940,
            "mdate": 1700667360940,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RL8Zk7Fkhe",
                "forum": "992eLydH8G",
                "replyto": "CWlUGZo8h8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed review and request them to see our general response highlighting the changes that were made to the initial draft. \n\n> Hypothesis under study in section 4\n\n> the authors assume equivalence between ICL and GD on the same model. \n\nWe have revised our draft so that the connection of Hypothesis 2 with prior work is clearer and we have included experiments on sub-model GD. Section 4 contains experimental results that involve (implicit) sub-models that are more comparable with the setup of Akyurek et al., and Oswald et al., i.e. hypothesis 2. Because we can not search over all possible subsets of the parameters, our experimental setup specifically focused on \u201cintuitive subsets\u201d` that were inspired by the construction of the earlier works: Von Oswald et al. hypothesized implicit model lies in $W_V$ of Transformers, while the probing experiments of Aky\u00fcrek et al. suggest that this optimization happens in top layers (suggesting that initial layers focus more on representation learning). \n\nWe conducted an experiment where we only fine-tuned $W_V$ of a single layer, for different choices of higher layers (with ablations) in the LLaMa model in hopes of fine-tuning only the implicit model. Based on our notation (Figure 1), call this model $\\widehat{\\text{GD}}$. We then compare $\\widehat{\\text{GD}}$ to ICL on our three metrics. We found that the _differences between the two remain significant_. We have added this result in the updated draft (Figure 2). \n\n> Arguments against sparsity ... previous works simply aim to give an expressivity result on transformers\n\nAs we clarify in the general response, we are curious about the generalizability of these expressivity results beyond the scope of their assumptions. The previous \u201cexpressivity\u201d works\u2019 definition of ICL ($\\widehat{\\text{ICL}}$: our notation) is different from the emergent ICL in pre-trained transformers. Moreover, these works contain experiments and statements that imply **stronger** conclusions. \n- In Von Oswald et al., there are a variety of experiments comparing ICL vs. GD and emphasizing the transformer weights with the hand-constructed weights. They conclude that: `\"... when training multi-layer self-attention-only Transformers on simple regression tasks, we provide **strong evidence that the construction is actually found**\"`. \n- You can see their stretching of the claims in the title of the published work: \u201cTransformers learn in-context by gradient descent,\u201d though a more accurate statement is \u201cTransformers **have the capacity to** learn in-context by gradient descent\u201d\n- A similar weight comparison (implicit linear weight difference; section 4.1) is done by Aky\u00fcrek et al, that implies these weights do emerge by training transformers. Although we agree that real models might find some dense counterparts of the simple constructions, more efforts should be made to find those families of weights. \n\nThese stronger conclusions, stretching beyond the boundaries of \u201cexpressivity,\u201d have motivated our work. We provide a variety of empirical and theoretical arguments that interpreting ICL as [implicit] GD is not a foregone conclusion, and the community should view these results with a grain of salt. \n\nJust to be clear, our intention is not to dismiss prior work such as Aky\u00fcrek et al., Von Oswald et al., Dai et al., etc. They are important results, but we feel that the community would benefit from additional clarity about their implications."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535476640,
                "cdate": 1700535476640,
                "tmdate": 1700536569763,
                "mdate": 1700536569763,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8F9wSjBxkq",
                "forum": "992eLydH8G",
                "replyto": "CWlUGZo8h8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Continued"
                    },
                    "comment": {
                        "value": "> ... the authors need to refute the probing experiments ...\n\nThanks for raising this question. First, we are **not** claiming to refute the theoretical formalism of Aky\u00fcrek et al. However, taking their conclusions beyond the scope of their expressivity results needs extreme care. \n\nAs you hinted, Aky\u00fcrek et al. use their \u201cprobing\u201d experiments (Section 5 of their work) to show that a transformer's internal representations can be used to predict parameters or quantities that are necessary for implicit (internal) iterative optimization. \n\nThis cannot be taken as proof, for several reasons: \n\n- The existing \u201cprobing\u201d techniques are known to be problematic as they **conflate \u201ccausation\u201d vs. \u201ccorrelation\u201d**. As the literature suggests, `\u201d...the probing framework may indicate correlations between representations fl(x) and linguistic property z, but **it does not tell us whether this property is involved** in predictions of f\u201d` (Belinkov 2022). \n- Even if we take their result as causal evidence, the probing experiment is done on quantities ($X^TY$), which could be used to perform **many** other algorithms (GD or otherwise). The question should not be whether a transformer representation can predict a solution, but how it is computed (the functional nature of the algorithm) \u2013 which is not covered by the probing setup. \n- Last but not least, the authors use a Transformer that is **already trained with ICL objective**: `\u201cWe take a trained in-context learner, freeze its weights, then train an auxiliary probing model\u201d`. Using the ICL objective has significant inductive biases (about the structure of the sequence it sees) which is different from real LLM training.\n\nBelinkov, Yonatan. \"Probing classifiers: Promises, shortcomings, and advances.\" Computational Linguistics 48.1 (2022): 207-219.\n\n> ... come up with experiments to suggest that the model changes its internal mechanism across training ...\n\nWe do **not** claim that ICL is equivalent to any mechanism (GD or otherwise). With our evolution of weights result, we just want to highlight that the ability of LMs to perform ICL remains stable even with evolving parameters. This just means that any claim of equivalence should be with a mechanism that elicits a family of weights, rather than fixed sparse constructions.\n\n> Are the contextual examples concatenated with the test query\u2026 \n\nWe concatenate each ICL demo, including the test query, with \u2018\\n,\u2019 which is similar to Gao et al'21. (they apply [MASK] in ICL for BERT). \nOur choice to fine-tune the model with a cross-entropy loss over the entire vocabulary was deliberate. This decision aligns with standard fine-tuning practices. While it's valid that placing more weight on relevant logits during training and inference could potentially refine the model's predictions, our primary goal was to evaluate the models in a setting that closely mirrors how they are commonly fine-tuned and deployed in real-world applications (for example, most of the works in the \u201cinstruction tuning\u201d literature). \n\n\n\n> About overlap rate and cosine similarity?\n\nIn the figure, the purple dashed line represents the gap between two instances of ICL that _vary solely in the order of demonstrations_. \n\nWhen the demonstration number equals 1, the overlap rate and cosine similarity metrics equal 1. This is because there is inherently only one possible order for a single demonstration. On the other hand, multiple orders are possible when the number of demonstrations exceeds one, introducing variability in the model\u2019s output. In Figure 5, we include this comparison to show the impact that varying orders of demonstrations have on the model's ICL performance, as quantified by our three metrics."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535553361,
                "cdate": 1700535553361,
                "tmdate": 1700536582421,
                "mdate": 1700536582421,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CLPY7VorsB",
                "forum": "992eLydH8G",
                "replyto": "Gl2ZMaPfDP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8711/Reviewer_ivcd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8711/Reviewer_ivcd"
                ],
                "content": {
                    "comment": {
                        "value": "I am sorry for the delay in my reply. I thank the authors for their detailed rebuttal. I certainly agree with the authors about the results being stretched beyond expressivity in multiple previous works regarding ICL vs GD equivalence. Hence, I liked the idea of disproving equivalence between the two algorithms that this paper aims to achieve. Furthermore, I have always had the same concerns about the differences between $\\hat{\\text{ICL}}$ and ICL that the authors cleanly report in this paper.\n\nI still have concerns about the exact hypothesis that the paper aims to verify in the experiments. It is difficult to find the exact implicit model inside the transformer and tune it with a learning algorithm that a transformer might use to tune on the fly. Previous papers show the expressivity of transformers to be able to internally tune small models using gradient descent implicitly. It's difficult to pinpoint how the model does this in practice. According to me, the current paper aims to remove the simplistic constructions that we can think of. \n\nI am happy to discuss more on this."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627853359,
                "cdate": 1700627853359,
                "tmdate": 1700627853359,
                "mdate": 1700627853359,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WRN0ghVNnR",
                "forum": "992eLydH8G",
                "replyto": "CWlUGZo8h8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8711/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for going through our detailed rebuttal!\n\n> ... concerns about the exact hypothesis that the paper aims to verify in the experiments\n\nAs the reviewer already understands, previous works' claims are about the equivalence of $\\widehat{\\text{ICL}}$ and  $\\text{GD}$/$\\widehat{\\text{GD}}$. What we wanted to test with our experiments was whether this implies any equivalence between $\\text{ICL}$ and $\\text{GD}$/$\\widehat{\\text{GD}}$. Therefore, we performed two sets of experiments (Section 4) for comparing $\\text{ICL}$ with $\\text{GD}$ and $\\widehat{\\text{GD}}$.\n\n1. In practice, $\\text{GD}$ is implemented by fine-tuning the whole model on task-specific data, which is what we did. We found that _$\\text{ICL}$ and $\\text{GD}$ behaved differently on our three evaluation metrics aimed at looking at functional equivalence_. In fact, Dai et. al.  in their experiments look at the same setting (whole model tuning) but only base their equivalence on raw performance which does not paint the whole picture about functional equivalence.\n2. For $\\widehat{\\text{GD}}$, because it is hard to find where exactly would this hypothetical implicit model lie in a big \"transformer\" like LLaMa, we relied on the suggestion of reviewer **uBmS** to look for \"intuitive subsets\" of the model to consider as implicit models. We _based our intuition on previous works which hypothesize the existence of these implicit models_. Particularly, we used von Oswald et al.'s construction, which says that the implicit model lies in $W_V$ of the transformer, and Aky\u00fcrek et al.'s probing experiments to guide us about which layers we should be looking at (deeper layers). Using these intuitions, we performed $\\widehat{\\text{GD}}$ updates in three different studies:\n   - Fine-tuning $W_V$ in one of the deep layers (randomly choosing one of the last 4 LLaMa layers).\n   - Fine-tuning $W_V$ in one of the middle layers (randomly choosing one of the layers in layers 16-20 of the 32 in LLaMa).\n   - Fine-tuning all weights in 8 layers randomly selected out of the 32 in LLaMa. \n\nIn our updated section 4, we have presented results from all these experiments for $\\text{GD}$ and our intuitive understanding of $\\widehat{\\text{GD}}$ when compared with $\\text{ICL}$ and found that they _function differently in all cases_. \n\nWe agree with the reviewer that these comparisons for $\\widehat{\\text{GD}}$ are not exhaustive and do not invalidate the claim of equivalence completely, but as mentioned in our future work section, identifying implicit models used for GD in LLMs in a computationally feasible manner could be an interesting avenue of future research."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631458576,
                "cdate": 1700631458576,
                "tmdate": 1700661697779,
                "mdate": 1700661697779,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8frexKdS5k",
                "forum": "992eLydH8G",
                "replyto": "WRN0ghVNnR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8711/Reviewer_ivcd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8711/Reviewer_ivcd"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response. \n\nI agree with the authors that their paper poses an important question to the community: \"Identifying implicit models used for GD in LLMs in a computationally feasible manner.\" \n\nI like the message of the paper. I still have concerns about the rigor of the experiments and the hypothesis that the paper aims to disprove. I am slightly increasing my score. I will discuss it further with my fellow reviewers during the discussion period."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667315469,
                "cdate": 1700667315469,
                "tmdate": 1700667315469,
                "mdate": 1700667315469,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fy1CSRvSHV",
            "forum": "992eLydH8G",
            "replyto": "992eLydH8G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8711/Reviewer_GeTu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8711/Reviewer_GeTu"
            ],
            "content": {
                "summary": {
                    "value": "Recently, connections have been built between in-context learning (ICL) and gradient descent (GD), in order to better understand in-context learning. This paper challenges such connections from both theoretical and empirical perspectives. Specifically, the authors establish the difference between ICL and GD in terms of order sensitivity and demonstrate that the assumption regarding model parameters for the connection hardly holds in practice. Further, the paper proposes metrics to empirically evaluate to what extent ICL and GD perform differently."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper focuses on the topic of how to understand in-context learning, which is crucial to the field.\n\n2. The proposed perspective of order sensitivity to look into the difference between ICL and GD is interesting and looks novel to me."
                },
                "weaknesses": {
                    "value": "1. I am a bit worried about the significance. The proposed order sensitivity is interesting and yet requires more in-depth analysis (see questions). However, I am not an expert in this field so I will defer to other reviewers regarding this point. \n\n2. The writing can be improved.  E.g., there are typos such as \"We know that both if ...\" and \"This is a relative metric is computed based...\"."
                },
                "questions": {
                    "value": "1. As the author mentioned, the construction of Akyurek et al. \u00a8 (2022) allows for order sensitivity in GD by update on samples one by one. Do we still have the difference in terms of order sensitivity in that setting?\n\n2. What if we use specific types of positional encoding to make ICL agnostic to the order of demonstrations? Would the performance increase or not? \n\n3. Alternatively, we can use the average prediction of many random orders of demonstrations to make ICL agnostic. Is that setting explored?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8711/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796350212,
            "cdate": 1698796350212,
            "tmdate": 1699637092618,
            "mdate": 1699637092618,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SU3175qGyw",
                "forum": "992eLydH8G",
                "replyto": "fy1CSRvSHV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time to review our paper and request them to **look at our general response, which highlights changes** made to the initial draft of the paper to improve its structure. \n\n> The proposed order sensitivity is interesting and yet requires more in-depth analysis ...\n\nBeyond the comparison of the order sensitivity of ICL vs GD, in the appendix, we had results on the order sensitivity of variants of GD (like SGD and Adam) which provide a broader context. As suggested by another reviewer, we have brought these to the main text. \n\nWe also did ablations on the SGD comparison with varying batch sizes (added in the appendix of the updated draft), although it is unclear how the model would decide which order to use in-context examples in the formation of Aky\u00fcrek et al., our comparisons show that _ICL is still substantially more sensitive to order than SGD or Adam_.\n\n> What if we use specific types of positional encoding to make ICL agnostic to the order of demonstrations? \n\nWe want to clarify that our objective with this paper is **not** to make ICL order-stable (although that\u2019s an interesting problem to focus on). ICL does **not** necessitate order-stability as it emerges in real-world models like LLaMa or GPT3 that are shown to be sensitive to input order. \n\nThe LLaMA model (which we use for our experiments) uses relative (specifically, rotary embedding (RoPE, Su et al. 2022) positional embedding, which tends to be more robust to reordering context content. However, as our results in section 4 show, they remain order-sensitive. Whether there will be any future way of encoding information that is robust to order is an interesting goal for future research. \n\nWe review the key thesis of our work in the general response. Briefly, our work is motivated by curiosity about the generalization of the prior results (on an implicit equivalence between ICL and GD) to more realistic scenarios. To present our case, we show various forms of evidence that may indicate that ICL and GD (on full model or sub-model) work differently. For instance, the high variance of ICL performance wrt order permutations _rather highlights it as a functional difference between ICL and GD_. \n\n> ...  we can use the average prediction of many random orders ...\n\nWe note that our results, shown in section 4, have all been averaged across multiple runs where demonstrations and their order are randomly sampled. This is to make sure that any order-sensitivity behavior is not a one-off incident but average behavior over multiple runs.\n\n\n> The writing \u2026 \n\nWe regret that the writing of the paper was not up to the mark in the initial draft. We have fixed the mistakes the reviewer pointed out and hope they find the revised version much improved."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535299421,
                "cdate": 1700535299421,
                "tmdate": 1700536550410,
                "mdate": 1700536550410,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DChUs9HkSn",
                "forum": "992eLydH8G",
                "replyto": "SU3175qGyw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8711/Reviewer_GeTu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8711/Reviewer_GeTu"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. After reading all the review comments, I tend to keep my rating unchanged."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550370235,
                "cdate": 1700550370235,
                "tmdate": 1700550370235,
                "mdate": 1700550370235,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "j61MslowFX",
            "forum": "992eLydH8G",
            "replyto": "992eLydH8G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8711/Reviewer_uBmS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8711/Reviewer_uBmS"
            ],
            "content": {
                "summary": {
                    "value": "This paper scrutinizes the strong claims that LMs implement gradient descent in inference time to achieve their ICL functionality, and assesses whether specific constructions of such LMs are feasible. The claim that LMs implement internal GD to do ICL is discarded by showing that ICL in LMs are order-sensitive, and ICL output distribution is different from a GD trained model\u2019s distribution. The claim that LMs implement internal GD in a specific construction (e.g. as in Von Oswald et al. or Aky\u00fcrek et al.) is discarded by showing that the sparsity of LLama LM\u2019s weights significantly less than sparsity of the proposed constructions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The authors conducted an extensive set of experiments to compare ICL to fine-tuning a model with GD by comparing order-sensitivity, learning curve of two algorithms. They also compare the token overlap of the resulting predictors.\n2) Their results show a clear difference between ICL of LLama model vs fine-tuning LLama model with the same few-shot statsets.\n3) The authors also investigated parameter structure of LLama model and showed that it is far from constructed models in (Aky\u00fcrek et al., and Von Oswald et al.,)"
                },
                "weaknesses": {
                    "value": "I think in general the paper needs to go over argumentations. I read both Aky\u00fcrek et al. and Von Oswald et al. very carefully and here are my issues with the current version of the paper.\n\n1) The GD is an ambiguous algorithm and it is unlucky to be GD in the title of Oswal and hence propagated to this paper. And this has important implications for the experiments presented in this paper. A proper learning algorithm for GD can be specified together with a loss function and a neural network. So, I believe the proper claim to refute should be in the form of \u201cLMs implement  internal GD on X neural network with Y loss function to achieve ICL for all X, Y\u201d  (some X, Y can also be meaningful but doesn\u2019t refute the possibility fully)\n\n2) **Definition-1 and its relation to the strong claim:** The strong claim **cannot be** \u201cLMs implement GD on cross-entropy on themselves\u201d because the Transformer can only implement internal algorithms on a strictly smaller model. For example, Aky\u00fcrek and Von Oswald show Transformers can implement GD on a linear model way smaller than the actual Transformer that does ICL. However, In Figure-1, authors compare ICL on the same model to GD on the same model with cross-entropy which is inherently impossible to be equal. The same issue of evaluating ICL of a model to GD of the same model exists in Token Overlap experiments as well.  And all of these issues arise from Definition-1 which seeks for equivalence of ICL with some fine-tuned version of the same model. \n\nOn the other hand, authors also proposes Definition-2 which is the proper version of the strong claim, however, do not present experiments where only some parts of an LM is finetuned. This unfortunately requires a search over what parameters to finetune which might be computationally expensive. But authors can search over intuitive subsets of all possible parameters.\n\n3) The GD part of the claim is a bit strong to be meaningful. For example, order sensitivity experiments are not related to **SGD** (online GD with some batch size < number of examples) which you also mentioned  in the end of P4 *\u201c... construction of Aky\u00fcrek allows for in GD as the update is performed on samples one-by-one instead \u2026\u201d*. A better experiments is to look at order-sensitivity of SGD. Those experiments left to Appendix, I suggest moving them to body and displaying together with GD.\n\n4) Aky\u00fcrek et al. does not make the strong claim that LMs implement GD to achieve ICL, and does not even imply. The paper argues that Transformers can discover learning algorithms to achieve ICL if it\u2019s trained for ICL. \n\nTheir main result is that the size of the Transformers changes the learned internal algorithm to achieve ICL. Smaller models are more close to SGD whereas large models implement Bayes optimal **Ridge Regression** solution to the linear regression problem. Even if I assume this paper implies something, it cannot be GD from reading these results. Because it suggests Bayesian learning, we expect ICL to be have a prior or a regularization that is learned during training time.\n\nOn the other hand, yes, Von Oswald et al. implies the strong claim in their intro \u201cWe find and describe one possible realization of this concept and hypothesize that the in-context learning capabilities of language models emerge through mechanisms similar to the ones we discuss here.\u201d"
                },
                "questions": {
                    "value": "- Does the sparsity ratio changes from layer to layer of GPT-J?\n- Why do you switch between models GPT-J vs LLama?\n- In SGD experiments in the appendix:\n  - Did you try different mini batch sizes?\n  - Did you shuffle the examples or iterate in the same order?\n  - Did you do one pass SGD or multiple?\n\n**Summary of the Review**\n\nOverall, I find the GD vs ICL experiments interesting and highlighting that the community still needs better explanations for ICL. However, I find that the experiments do not refute some of the claims that the authors want to refute (W2), and (W1, W3, W4) important to be addressed before publication. I am hoping to raise my score if these weaknesses can be fixed or answered."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8711/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8711/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8711/Reviewer_uBmS"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8711/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698860869478,
            "cdate": 1698860869478,
            "tmdate": 1700762440172,
            "mdate": 1700762440172,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZGIbGbnjN2",
                "forum": "992eLydH8G",
                "replyto": "j61MslowFX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comprehensive insights and request them to **look at our general response,** which highlights what changes were made to the initial draft of the paper. \n1. **The GD is an ambiguous algorithm ...**\n> \u2026the proper claim to refute should be \u2026\n\nWe are **not** suggesting that hypothesis 1 is what should be or can be proven. There are two hypotheses in our paper, one which is a more general notion of equivalence between ICL and GD, while the other is what previous works look at. We are just investigating whether the second implies the first in any way. In our general response and the updated draft, we have made the distinction very clear and request the reviewer to kindly read it.\n\n2. **Definition-1 and its relation to the strong claim**, about experiment with sub-models (intuitive subsets of the parameters)\n\n  > \u2026authors compare ICL on the same model to GD on the same model with cross-entropy, which is inherently impossible to be equal\u2026\n\n  > ... do not present experiments where only some parts of an LM is finetuned \u2026\n\nAs the reviewer pointed out, it is infeasible to search over all subsets of sub-models to find if one corresponds to the hypothesized implicit model. At their suggestion, we have added experiments involving `\u201cintuitive subsets\u201d in Section 4. (Thank you for the suggestion!) \n\nOur experimental setup specifically focused on subsets that were inspired by the construction of the earlier works: Von Oswald et al. hypothesized implicit model lies in $W_V$ of Transformers, while the probing experiments of Aky\u00fcrek et al. suggest that this optimization happens in top layers (suggesting that initial layers focus more on representation learning). \n\nWe conducted an experiment where we only fine-tuned $W_V$ of a single layer, for different choices of higher layers (with ablations) in the LLaMa model in hopes of fine-tuning only the implicit model. Based on our notation (Figure 1), call this model $\\widehat{\\text{GD}}$. We then compare $\\widehat{\\text{GD}}$ to ICL on our three metrics. We found that the _differences between the two remain significant_. We have added this result in the updated draft (Section 4). \n\n3. **The GD part of the claim is a bit strong to be meaningful**\n> ... order sensitivity experiments are not related to SGD ...\n\nWe looked at the order-sensitivity for vanilla GD because that is what is studied in Von Oswald et al. We agree with the reviewer\u2019s suggestion about highlighting the sensitivity study with SGD and Adam to show that even if Aky\u00fcrek et al. allow for order sensitivity, ICL exhibits a lot more sensitivity than SGD/Adam. We have updated the order sensitivity figure (**Figure 2**) in the main text.\n\n\n4. **About the claim of Aky\u00fcrek et al.**\n> Aky\u00fcrek et al. does not make the strong claim ...\n\nWhile Aky\u00fcrek et al. are more precise in their claims, there are phrasings that may lead to confusing conclusions of hypothesis 1. Most importantly, their use of ICL (training Transformers with ICL objective) is critically different from the emergent ICL in Transformers (the CLM objective on natural text). These two objectives elicit different families of transformer models ($\\text{ICL}$ vs $\\widehat{\\text{ICL}}$, in our notation in Fig.1), as pointed out in our *updated Background (section 2)*. \n\nThe misleading nature of conclusions is in fact evident in your own phrasing. As you suggested, Aky\u00fcrek et al. argue `\u201cthat Transformers can discover learning algorithms to achieve ICL **if it\u2019s trained for ICL**\u201d`. The key assumption *\u201c... if it is trained for ICL\u201d* is easy to miss. Furthermore, if a model \u201cis trained for ICL\u201d is it really capable of ICL?! There is no work that shows the equivalence of this to the emergent ICL in real-world models like LLaMa. We have tried to clarify this distinction in our revised description in \u201cbackground\u201d. This is one of the first works that adopted the use of ICL objective and its framework has since been used in the several follow-up works that blindly equate $\\widehat{\\text{ICL}}$ with $\\text{ICL}$. \n\nMoreover, even Aky\u00fcrek et al. seem to over-extend their conclusions' scope. For example, their section 4, which `\u201caim[s] to explain ICL at the computational level by identifying the kind of algorithms to regression problems that transformer-based ICL implement,\u201d` is premised by saying that `\u201cit is these iterative algorithms that capture the behavior of **real learners**\u201d`, for which we did not find any concrete evidence. Or the title their published work is: \u201cWhat learning algorithm is in-context learning?\u201d though a more accurate title is \u201cWhat learning algorithm **can be simulated** in-context by Transformers?\u201d\n\nJust to be clear, our intention is **not to dismiss** prior works. These are excellent and innovative works. Our hope is to bring in more clarity between the actual claims and the extend to which they can be generalized beyond their assumptions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535208308,
                "cdate": 1700535208308,
                "tmdate": 1700593249141,
                "mdate": 1700593249141,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SsvLDFBTXe",
                "forum": "992eLydH8G",
                "replyto": "j61MslowFX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Continued"
                    },
                    "comment": {
                        "value": "Next we address minor suggestions/questions:\n\n> Does the sparsity ratio changes from layer to layer of GPT-J?\n\nThe sparsity ratio does not change significantly for $W_K$ and $W_Q$ but drops a little for $W_V$ with deeper layers. If we believe the weights that simulate GD to be sparse (as in Von Oswald et al. and Aky\u00fcrek et al.), this contradicts the probing experiments in Aky\u00fcrek et al. which suggest that the optimization happens in higher layers. We have changed our sparsity figure (**Figure 4**) to reflect this, and also updated the appendix with more details.\n\n> Why do you switch between models GPT-J vs LLama?\n\nWhile LLaMa is open-sourced, to our knowledge its intermediate training checkpoints are not released (which are used in Fig.3; notice that the x-axis is the checkpoint step). Therefore, we used a slightly smaller model (GPT-J, with ~6B parameters) with available checkpoints to compare the evolution of weights and ICL ability with training steps. \n\n> About SGD experiments:\n \nWe did not change the batch size which was fixed to 2. The reason is that, like Von Oswald et al., the batch size was equal to the total samples available in context and for Aky\u00fcrek et al., the batch size is 1 by construction. On the suggestion of the reviewer, we do show an ablation with batch size = 1 for SGD/Adam in Appendix A. \nWe shuffled the samples in all experiments and also did multiple passes (assuming multiple layers perform serial GD in the model)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535230179,
                "cdate": 1700535230179,
                "tmdate": 1700593273581,
                "mdate": 1700593273581,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6ddD5Z6ThO",
                "forum": "992eLydH8G",
                "replyto": "ZGIbGbnjN2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8711/Reviewer_uBmS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8711/Reviewer_uBmS"
                ],
                "content": {
                    "title": {
                        "value": "kindly requesting a reorganization"
                    },
                    "comment": {
                        "value": "Could it be possible to reorganize your answer that addresses my questions in the same format as 1, 2, 3, 4; and for each point can you guide me where to look in the general response or copy the related part?\n\nIf not I'll do my best, but in this format there are quotes without full context and it will be hard to respond everything at once. I had to re-iterate my questions, and the followup discussions can be inefficient with this way."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592375161,
                "cdate": 1700592375161,
                "tmdate": 1700592375161,
                "mdate": 1700592375161,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5nosCMu24J",
                "forum": "992eLydH8G",
                "replyto": "j61MslowFX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8711/Reviewer_uBmS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8711/Reviewer_uBmS"
                ],
                "content": {
                    "title": {
                        "value": "thank you!"
                    },
                    "comment": {
                        "value": "**1) The GD is an ambiguous algorithm**\n\nThe new version is much clear in terms of argumentation, thank you!. But now the main argument of the paper has been substantially changed or had been made it clear to me. So, I am re-reviewing.\n\nThe argument the paper is refuting now is **\"Hypothesis-2 => Hypothesis-1\"** as written in the rebuttal. I am copying them here:\n\n- **H2**: For a given well-defined task, there exist Transformer weights such that ICL is algorithmically equivalent to GD (whole model or sub-model).\n\n- **H1**: For any Transformer weights resulting from self supervised pretraining and for any well-defined task, ICL is algorithmically equivalent to GD (whole model or sub-model).\n\nLet me come up with three more hypotheses for clarity\n\n- **H1O**: For linear Transformer resulting from meta training on the task family in H2, ICL is algorithmically equivalent to GD.\n\n- **H1A**: For large enough Transformer resulting from meta training on the task family in H2, ICL is algorithmically equivalent to Bayes optimal decision rule\n\n- **H1LM**: For an LM resulting from self-supervised pretraining on a large corpous, ICL is algorithmically equivalent to GD (whole model or submodel)\n\n**a)** I think \"**H2 => H1**\" argument **neither made/suggested** by Oswald et al. nor Akyurek et al.\n\nMy understanding is that both papers shows **H2** by theoretical constructions, and Oswald shows **H1O** and Akyurek shows \n**H1A** by experimentation. I think what could be problematic in these papers, and I think this is what the experiments of this paper tries to refute **(H2, H1O/A) => H1LM**. This claim is mildy suggest in Oswald paper, but not in Akyurek (my point in (4)). So, I would **strongly** suggest going back to this (refuting **(H2, H1O/A) => H1LM**) positioning, which is the way I understood at the beginning, or please clarify further. \n\n**b)** To reiterate my worry **in the previous (1)**: **H1LM** is too strong as GD is an unspecified algorithm. What succefully refuteed by the experiments of this paper is ICL cannot be specific form of GD with cross entropy loss on intuitive subsets of LM --- not all forms of GD. \n\nSo, overall what I delineate here still remains a major argumentation issue to me, I am happy to change my scores when this is fixed. \n\n**2) Thank you! This looks great!**\n\n**3) Looks great!**\n\n**4) Please see (1)**\n\nI really liked the new $ICL$ and $\\hat{ICL}$ distinction in the notation."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677489155,
                "cdate": 1700677489155,
                "tmdate": 1700678400604,
                "mdate": 1700678400604,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]