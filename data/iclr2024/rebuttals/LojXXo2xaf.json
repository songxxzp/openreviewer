[
    {
        "title": "GPT Can Solve Mathematical Problems Without a Calculator"
    },
    {
        "review": {
            "id": "Bc8bb41sKr",
            "forum": "LojXXo2xaf",
            "replyto": "LojXXo2xaf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9208/Reviewer_NYtC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9208/Reviewer_NYtC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes MathGLM, a transformer-based language model specifically designed and trained to excel at mathematical reasoning and arithmetic tasks. \n\n1. MathGLM is trained on a large dataset of arithmetic expressions and sequences, ranging from simple to complex multi-step calculations. This allows it to learn the underlying rules and patterns of arithmetic operations.\n\n2. A step-by-step strategy is used during training, where MathGLM is tasked with generating each intermediate step leading to the final result. This mimics human calculation and helps MathGLM deeply comprehend the calculations.\n\n3. Curriculum learning is used, starting with simpler arithmetic tasks and progressively increasing complexity. This improves efficiency and allows handling of large digit numbers.\n\n4. MathGLM demonstrates significantly higher accuracy on arithmetic tasks compared to GPT-4, ChatGPT and other LLMs. It also achieves comparable performance to GPT-4 on a Chinese math word problem dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The core idea of specializing a language model for mathematical reasoning is novel and well-motivated. Math is an important domain where current LLMs struggle.\n\n2. The step-by-step training strategy is creative and helps MathGLM learn the intricacies of arithmetic operations. Generating intermediate steps is akin to human math solving.\n\n3. The arithmetic dataset construction process covers various types of math operations and data formats in a principled manner. This diversity is key for strong training.\n\n4. Extensive experiments demonstrate clear performance gains over GPT-4 and other models, validating MathGLM's capabilities. The scaling experiments also provide useful insights.\n\n5. The work is technically sound, clearly presented and easy to follow. The motivation and proposed techniques are intuitive."
                },
                "weaknesses": {
                    "value": "1. While specializing for arithmetic is beneficial, it could compromise more general capabilities. Testing on broader math/reasoning tasks could help characterize tradeoffs.\n\n2. More analysis and examples demonstrating the step-by-step generation process could be useful to understand MathGLM's learned skills.\n\n3. The reasoning behind curriculum learning's benefits is not fully fleshed out. Is it mainly about efficiency gains?\n\n4. How well do the findings transfer to non-Chinese languages? Cross-lingual experiments could help strengthen claims of language-agnostic reasoning."
                },
                "questions": {
                    "value": "Are there any analysis and examples of the errors made by MathGLM? Understanding the remaining limitations could guide future improvements.\n\nFor real-world usage, how does MathGLM handle novel word problems outside its training distribution? Experiments on out-of-distribution generalization could be insightful."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9208/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698373448183,
            "cdate": 1698373448183,
            "tmdate": 1699637159126,
            "mdate": 1699637159126,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "10WetdPbzu",
                "forum": "LojXXo2xaf",
                "replyto": "Bc8bb41sKr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9208/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to  Reviewer NYtC (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your valuable comments on our paper. We will explain your concerns point by point.\n\n\n```\nWeakness 1: While specializing for arithmetic is beneficial, it could compromise more general capabilities. Testing on broader math/reasoning tasks could help characterize tradeoffs.\n```\n\n**Response**: In our study, we leverage the continue training strategy to address the limitation of fine-tuning that damages the generic ability of the MathGLM. \n\nHere, we report the generic ability of the MathGLM on [Z-bench](https://github.com/zhenbench/z-bench). To evaluate the quality of responses generated by the model, we score responses with the powerful GPT-4 on a scale of 1-10 based on factors such as correctness (high priority), helpfulness, relevance, depth, innovativeness, and level of detail.   \n\n\n|Model | Math | Text|\n|----|----|----|\n|GLM-10B | 0% | 5.91 |\n|MathGLM-10B | 56.6% | 5.35 |\n\n\n\nCompared to GLM-10B, MathGLM-10B shows a significant improvement in handling math tasks, with an accuracy of 56.6% compared to GLM's 0%. However, it is also noticeable that there's a slight reduction in the performance on text tasks for MathGLM-10B (5.35) compared to the original GLM (5.91). While MathGLM-10B excels in mathematics, its ability to handle general text tasks is marginally impacted. However, this reduction is not drastic, indicating that the continue training using a hybrid dataset, which includes both mathematical and general text data, helps in retaining the model's overall linguistic competence.\n\n```\nWeakness 2: More analysis and examples demonstrating the step-by-step generation process could be useful to understand MathGLM's learned skills.\n```\n**Response**: We have included the impact of the step-by-step generation process in the Appendix, particularly in Section D.6. By leveraging the step-by-step generation process, the accuracy on MathGLM-500M rises from 31.96% to 89.57%, while for MathGLM-2B, it increases from 40.76% to 93.03%. This suggests that the step-by-step process is highly effective in enhancing the model's performance. This could be due to the process allowing the model to break down complex problems into smaller, more manageable steps, thereby reducing errors that can accumulate in a single-step solution.\n\n```\nWeakness 3: The reasoning behind curriculum learning's benefits is not fully fleshed out. Is it mainly about efficiency gains?\n```\n**Response**: We leverage curriculum learning to enhance inference performance. This approach involves a gradual introduction of increasingly complex tasks during training. Initially, we focus on simpler training instances, specifically within the 5-digit range. This foundational stage allows the model to efficiently grasp basic calculation rules. \n\nOnce the model achieves stable convergence and satisfactory performance on the test dataset, we then implement curriculum learning as a strategic advancement. We introduce a new set of training data that includes numbers ranging from 5 to 12 digits. This methodical escalation in complexity is designed to incrementally challenge and refine the model's capabilities.\n\nHere, we report the performance on MathGLM-2B without using curriculum learning. From the results, it is evident that the use of curriculum learning has a significant impact on the performance of MathGLM-2B. The model, when trained without curriculum learning, achieves an accuracy of 88.70%. However, with the implementation of curriculum learning, the accuracy increases to 93.03%. \n\n|Model | Accuracy |\n|----|----|\n|MathGLM-2B (w/o curriculum learning) | 88.70% | \n|MathGLM-2B | 93.03% |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506423441,
                "cdate": 1700506423441,
                "tmdate": 1700506670073,
                "mdate": 1700506670073,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IU6s62XCdD",
                "forum": "LojXXo2xaf",
                "replyto": "1vf4FK8l7n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9208/Reviewer_NYtC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9208/Reviewer_NYtC"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer NYtC"
                    },
                    "comment": {
                        "value": "Thank you for the responses.  They provide clarification on the questions raised.  After discussing with other reviewers, I will reassess my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706169977,
                "cdate": 1700706169977,
                "tmdate": 1700706169977,
                "mdate": 1700706169977,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1pVboCiSsh",
            "forum": "LojXXo2xaf",
            "replyto": "LojXXo2xaf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9208/Reviewer_mwYm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9208/Reviewer_mwYm"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce two new datasets that can be used to improve pre-training and fine-tuning of large language models or, more generally, large-scale Transformer models. One dataset contains a large set of arithmetic problems, while the other represents a refined version of Ape210K, which has been augmented with step-by-step solution procedures to solve math word problems involving natural language. The authors exploit these datasets to train a series of Transformer-based language models and show that they indeed achieve more accurate performance in arithmetic and word problem tasks compared to GPT models or other LLMs."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-\tThe article is generally clear and well-written. The research questions are well-motivated.\n-\tInvestigating the arithmetic and mathematical abilities of Transformer-based architectures and LLMs is an important and timely research area.\n-\tReconstructing the Ape210K dataset by adding step-by-step solutions constitutes an interesting extension to the available training corpora (which would become even more useful if the dataset would be made publicly available).\n-\tThe authors also analyze the impact of problem difficulty (e.g., grade levels) and the error distribution."
                },
                "weaknesses": {
                    "value": "-\tThe training/testing setup used in the present work differ from those used in similar work, making it challenging to compare the current results with previous contributions. Overall, it seems that the advantage in the reported benchmarks mostly (only?) stems from the use of an extended training set containing math problems, rather than from architectural innovations. This would still constitute an interesting finding, but it should be demonstrated using out-of-distribution test instances (see next point).\n-\tThe authors claim that MathGLM has a \u201cprofound understanding of the complex calculation process\u201d and \u201ceffectively learns the underlying rules and principles of arithmetic operations\u201d, however I do not think that its generalization abilities have been properly evaluated.\n-\tThere are a few methodological details than require clarification (see questions below).\n-\tThe paper does not include any Reproducibility Statement or any pointer to source code repositories, which makes it difficult to replicate the simulations and the experimental setup."
                },
                "questions": {
                    "value": "-\tThe authors say that MathGLM learns to solve arithmetic tasks \u201cby integrating a step-by-step strategy into its architecture\u201d. However, it is not clear how the model architecture actually implements step-by-step reasoning process (from the description, it seems that such feature is just a property of the solution format, rather than of the architecture design). This point should be clarified.\n-\tIn order to properly test for generalization the authors should demonstrate that the model can solve problems outside the training distribution (e.g., involving much longer numbers, and much more operands, see for example https://arxiv.org/abs/2207.02536). At present, an alternative (and more parsimonious) explanation is simply that the larger-scale of the training data allows to the model to memorize a more consistent amount of arithmetic knowledge.\n-\tThe authors say that \u201cTo assess the generalization ability of MathGLM beyond the 5-digit range, a set of 50,000 training records involving numbers within the 12-digit range are introduced into the training dataset\u201d. This does not guarantee that generalization is properly assessed; it rather shows that by adding more training samples from the testing range the performance increases, which is expected (also see https://arxiv.org/abs/2306.15400).\n-\tIt is not clear whether the curriculum learning strategy is beneficial since there is no comparison with a non-curriculum counterpart.\n-\tIt is not clear how the Ape210K dataset was reconstructed. Were the step-by-step solutions generated in an automatic way? If so, how was their quality verified?\n-\tWhat is the rationale of using different models for the Arithmetic task and the Math Word Problems? Shouldn\u2019t the same MathGLM model be able to solve both types of problems? The authors say that \u201cour goal is to simultaneously advance both mathematical reasoning and arithmetical calculation capabilities of LLMs, addressing both aspects at the same time\u201d, but from my understanding they trained separate models for the Arithmetic and MWP datasets (the \u201cTraining Strategy\u201d section at pg. 5 should be expanded and described in a much clearer way).\n-\tThe authors should more carefully explain how GPT models were tested. Which prompting methods were used to probe these models? How did performance change when using more advanced (e.g., Chain-of-though) prompting strategies?\n-\tThe title is misleading, since it suggests that models from the GPT family (e.g., ChatGPT, GPT-4) achieve the best accuracy, while in fact the authors are tuning a model from the GLM family. A better option could be to just use \u201cLLMs\u201d as a more general term?\n-\t\u201cGLM\u201d has not been properly defined in the introduction (I suggest including both the acronym description and the reference paper).\n-\tThe manuscript content is often redundant; I suggest removing duplicate (or similar) sentences."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9208/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698395910852,
            "cdate": 1698395910852,
            "tmdate": 1699637158998,
            "mdate": 1699637158998,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yNp4oBvZ2j",
                "forum": "LojXXo2xaf",
                "replyto": "1pVboCiSsh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9208/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mwYm (1/3)"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your valuable comments on our paper. We will explain your concerns point by point. \n\n```\nWeakness 2: The authors claim that MathGLM has a \u201cprofound understanding of the complex calculation process\u201d and \u201ceffectively learns the underlying rules and principles of arithmetic operations\u201d, however I do not think that its generalization abilities have been properly evaluated.\n```\n**Response**: Please refer to the Response in Question 2.\n\n```\nWeakness 4: The paper does not include any Reproducibility Statement or any pointer to source code repositories, which makes it difficult to replicate the simulations and the experimental setup.\n```\n**Response**: Here, we provide an anonymous repository (https://anonymous.4open.science/r/MathGLM-anonymous-3085) to release our datasets and code.\n\n\n\n```\nQuestion 1: The authors say that MathGLM learns to solve arithmetic tasks \u201cby integrating a step-by-step strategy into its architecture\u201d. However, it is not clear how the model architecture actually implements step-by-step reasoning process (from the description, it seems that such feature is just a property of the solution format, rather than of the architecture design). This point should be clarified.\n```\n**Response**: We apologize for the confusion. In this study, we do not design a new architecture to solve arithmetic tasks. Instead, we construct a pre-training dataset that includes a variety of arithmetic expressions, encompassing different types of arithmetic operations and various types of numbers. \n\nThe key to facilitating the step-by-step reasoning is in the way these expressions are structured and presented in the dataset. This structure of a pre-training dataset provides the model with examples of grasping calculation rules during training and inference.\n\nFurthermore, to enhance the model's ability to handle arithmetic tasks, we employ a unique tokenization strategy. Each digit in an arithmetic expression is tokenized as a distinct token. This approach allows MathGLM to focus on each component of the problem individually, facilitating more accurate computations and enabling the model to follow the step-by-step reasoning pattern inherent in the dataset.\n\nBy training MathGLM on this carefully constructed dataset with its unique tokenization method, the model implicitly learns a step-by-step reasoning process, despite not having an architecture explicitly designed for this purpose. Compared with the method that directly calculates the answer of the question, the step-by-step methodology enhances the model's accuracy in arithmetic tasks (Cf. Figure 10). \n\n\n\n\n\n```\nQuestion 2: In order to properly test for generalization the authors should demonstrate that the model can solve problems outside the training distribution (e.g., involving much longer numbers, and much more operands, see for example https://arxiv.org/abs/2207.02536). At present, an alternative (and more parsimonious) explanation is simply that the larger-scale of the training data allows to the model to memorize a more consistent amount of arithmetic knowledge.\n```\n**Response**: We thank the reviewer for highlighting the importance of testing for generalization beyond the training distribution. To test the generalizability of MathGLM,  we evaluate the generalizability ability of MathGLM on arithmetic problems that involve numbers beyond the 12-digit range. \n\n\n|Model | 13-D | 14-D| 15-D | \n|----|----|----|----| \n|MathGLM-500M | 16.10% | 10.31% | 18.07% | \n|MathGLM-2B | 28.74% | 36.08% | 37.34% | \n\n\n\n\n\n\n\n```\nQuestion 3: The authors say that \u201cTo assess the generalization ability of MathGLM beyond the 5-digit range, a set of 50,000 training records involving numbers within the 12-digit range are introduced into the training dataset\u201d. This does not guarantee that generalization is properly assessed; it rather shows that by adding more training samples from the testing range the performance increases, which is expected (also see https://arxiv.org/abs/2306.15400).\n```\n**Response**: We appreciate the reviewer\u2019s critical observation regarding our methodology for assessing the generalization ability of MathGLM. In our evaluation setting, our aim is to demonstrate MathGLM's enhanced performance in generalization with a notably smaller dataset. This dataset is significantly smaller in comparison to the volume of data used during the initial training phase of MathGLM. This approach underscores the efficiency and effectiveness of MathGLM in learning and adapting to new data. \n\n\nTo rigorously test the model's inherent generalization capabilities, we evaluate the generalizability ability of MathGLM on arithmetic problems that involve numbers beyond the 12-digit range. The detailed results can be found in Response to Question 3."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509517119,
                "cdate": 1700509517119,
                "tmdate": 1700509702749,
                "mdate": 1700509702749,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fIeYnKKzxV",
                "forum": "LojXXo2xaf",
                "replyto": "1pVboCiSsh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9208/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mwYm (2/3)"
                    },
                    "comment": {
                        "value": "```\nQuestion 4: It is not clear whether the curriculum learning strategy is beneficial since there is no comparison with a non-curriculum counterpart.\n```\n**Response**: We leverage curriculum learning to enhance inference performance and increase model efficiency. This approach involves a gradual introduction of increasingly complex tasks during training. Initially, we focus on simpler training instances, specifically within the 5-digit range. This foundational stage allows the model to efficiently grasp basic calculation rules. \n\nOnce the model achieves stable convergence and satisfactory performance on the test dataset, we then implement curriculum learning as a strategic advancement. We introduce a new set of training data that includes numbers ranging from 5 to 12 digits. This methodical escalation in complexity is designed to incrementally challenge and refine the model's capabilities.\n\nHere, we report the result of the performance on MathGLM-2B without using curriculum learning. From the results, it is evident that the use of curriculum learning has a significant impact on the performance of MathGLM-2B. The model, when trained without curriculum learning, achieves an accuracy of 88.70%. However, with the implementation of curriculum learning, the accuracy increases to 93.03%. \n\n|Model | Accuracy |\n|----|----|\n|MathGLM-2B (w/o curriculum learning) | 88.70% | \n|MathGLM-2B | 93.03% | \n\n\n\n\n```\nQuestion 5: It is not clear how the Ape210K dataset was reconstructed. Were the step-by-step solutions generated in an automatic way? If so, how was their quality verified?\n```\n**Response**: In our reconstruction of the Ape210K dataset, we employ a Python script to enable a detailed, step-by-step calculation for each math problem (Cf. Figure 3). This enhancement in the dataset is specifically designed to bolster MathGLM's proficiency in grasping the intricate calculation rules essential for solving math word problems. By navigating through these step-wise computations, MathGLM is trained to not only reach the correct answer but also to comprehend the underlying mathematical reasoning. Furthermore, we plan to make the enhanced Ape210K dataset publicly available for open research, contributing to the broader academic community.\n\n\n```\nQuestion 6 : What is the rationale of using different models for the Arithmetic task and the Math Word Problems? Shouldn\u2019t the same MathGLM model be able to solve both types of problems? The authors say that \u201cour goal is to simultaneously advance both mathematical reasoning and arithmetical calculation capabilities of LLMs, addressing both aspects at the same time\u201d, but from my understanding they trained separate models for the Arithmetic and MWP datasets (the \u201cTraining Strategy\u201d section at pg. 5 should be expanded and described in a much clearer way).\n```\n**Response**: The reason why we use different models for arithmetic tasks and mathematical word problems (MWP) in our study is that the motivations we study are different.\n\nFor arithmetic tasks, we aim to address the misconception about the capabilities of large language models (LLMs) in performing arithmetic operations. MathGLM challenges the prevailing notion about LLMs' limitations in arithmetic operations, particularly with longer digits and more complex calculations like decimals and fractions. The utilized step-by-step strategy can significantly improve the arithmetic performance (Cf. Figure 10).\n\nConversely, for MWPs, we recognize the limitations of the Ape210K dataset, which originally provided direct answers without detailed computations. To address this, we reconstruct the Ape210K dataset using a step-by-step solution strategy and fine-tune GLM models on this reconstructed dataset, specifically for MWPs.\n\nWe apologize for the confusion and will revise the statements in the \"Training Strategy\" section. \n\n\n\n```\nQuestion 7: The authors should more carefully explain how GPT models were tested. Which prompting methods were used to probe these models? How did performance change when using more advanced (e.g., Chain-of-though) prompting strategies?\n```\n**Response**: In our evaluation, we use the standard direct prompting method. In this approach, the models are presented with straightforward questions without additional contextual or guiding prompts. This method was chosen to ensure a fair and consistent baseline for comparing the performance of different models."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509652920,
                "cdate": 1700509652920,
                "tmdate": 1700509715383,
                "mdate": 1700509715383,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YpHAaUi9wt",
                "forum": "LojXXo2xaf",
                "replyto": "3iXdygUo00",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9208/Reviewer_mwYm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9208/Reviewer_mwYm"
                ],
                "content": {
                    "title": {
                        "value": "The revised paper improves on several weak points, however some critical issues remain."
                    },
                    "comment": {
                        "value": "I thank the authors for their thoughtful responses. I think that several of my concerns would indeed be fixed by implementing the actions mentioned by the authors (e.g., providing an open repository, improving the methodological description and the discussion, etc.).\n\nAt the same time, I think that the paper has some critical weaknesses that would not be addressed according to the planned revision. In particular, I am not convinced that the current results support the idea that the model has a \"profound understanding of the complex calculation process\", neither that it \"effectively learns the underlying rules and principles of arithmetic operations\". The new results related to operations with more digits do not add evidence about this weaknesses and only partially explore the issue of out-of-distribution generalization (e.g., what happens when the problem also involves more operands, or when the structure of the computation [e.g., nestedness] significantly changes w.r.t. the training distribution?). Furthermore, I think that the comparison with GPT models should involve the exploration of a few advanced prompting strategies, since we know that these LLMs achieve much higher accuracy when properly prompted. The authors argue that they did not use advanced prompting \"to ensure a fair and consistent baseline\", but since their MathGLM model has been trained on a different dataset (containing step-by-step problem solutions) I wonder why we shouldn't encourage the LLMs to similarly exploit step-by-step reasoning for solving the task.\n\nOverall, I find this paper of some interest and I think that its quality could improve after the planned revision, but in my opinion it still contains critical flaws that would not justify publication in a top-tier venue like ICLR."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557521435,
                "cdate": 1700557521435,
                "tmdate": 1700557521435,
                "mdate": 1700557521435,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iqlxt1J8HA",
            "forum": "LojXXo2xaf",
            "replyto": "LojXXo2xaf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9208/Reviewer_Bh6z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9208/Reviewer_Bh6z"
            ],
            "content": {
                "summary": {
                    "value": "The paper has a full study of LLM on math problems, with the focus of multi-digit complex operations and math problems in regular text. The dataset are created, and the LLM of different sizes are fine tuned. The MathGLM has been evaluated on many setup and ablation. The new model has proven better performance than the GPT-4 on the two goals."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The math accuracy belongs to one of the core challenge of LLM. The paper has very good CoT dataset and gets enhanced performance compared to the GPT-4 model. The paper appears rather useful among many scholars from relevant area."
                },
                "weaknesses": {
                    "value": "Could we extend the evaluation of the new model and see the performance on non-math tasks? The math focus finetune may have reduced the performance on other tasks, and it is good to know how good / bad that would be."
                },
                "questions": {
                    "value": "Maybe a followup work would seem how to train model for middle-school level math problems, how the size of dataset / model would scale for that"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9208/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830095033,
            "cdate": 1698830095033,
            "tmdate": 1699637158862,
            "mdate": 1699637158862,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wLMtNkd51e",
                "forum": "LojXXo2xaf",
                "replyto": "iqlxt1J8HA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9208/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Bh6z"
                    },
                    "comment": {
                        "value": "Thank you for your insightful and well-articulated points. We will explain your concerns point by point.\n\n\n\n```\nWeakness 1: Could we extend the evaluation of the new model and see the performance on non-math tasks? The math focus finetune may have reduced the performance on other tasks, and it is good to know how good / bad that would be.\n```\n\n**Response**: In fact, fine-tuning the model specifically for mathematical tasks may have implications for its performance in other domains. This specialized fine-tuning could lead to a reduction in the model's effectiveness in handling non-mathematical tasks.\n\nIn order to reduce this effect, we leverage a hybrid dataset dataset that includes both mathematical and text data for the continue training strategy. Specifically, we use a public Chinese text data [Chinese-Vicuna](https://huggingface.co/datasets/Chinese-Vicuna/instruct_chat_50k.jsonl), a rich source of public Chinese text dataset, with our reconstructed Ape210K dataset for continue training. This approach aims to maintain the model's proficiency in general language tasks while enhancing its mathematical abilities. \n\n\nTo evaluate the performance of the model on non-math tasks, we use the [Z-bench](https://github.com/zhenbench/z-bench), consisting of 63 diverse questions. To evaluate the quality of responses generated by the model, we score responses with the powerful GPT-4 on a scale of 1-10 based on factors such as correctness (high priority), helpfulness, relevance, depth, innovativeness, and level of detail. The results are as follows:\n\n\n|Model | Math | Text|\n|----|----|----|\n|GLM-10B | 0% | 5.91 |\n|MathGLM-10B | 56.6% | 5.35 |\n\n\n\nMathGLM-10B demonstrates a significant improvement in the mathematical task, achieving a 56.6% accuracy, a significant leap from GLM's 0%. However, it's important to note the slight decrease in performance on general text tasks for MathGLM-10B (5.35) compared to GLM-10B (5.91). While MathGLM-10B excels in mathematics, its ability to handle general text tasks is marginally impacted. \n\n\nNevertheless, the marginal reduction in text task performance indicates that the continue training strategy of using a hybrid training dataset has been effective in preserving the model's overall linguistic abilities. Therefore, while there is a noticeable specialization in mathematical tasks, MathGLM-10B maintains a competent level of performance in general language processing.\n\n\n\n```\nQuestion 1: Maybe a followup work would seem how to train model for middle-school level math problems, how the size of dataset / model would scale for that\n```\n\n**Response**: Thank you for the suggestion regarding follow-up work. We are indeed in the process of developing MathGLM-v2, a specialized model designed specifically for middle-school level mathematics.\n\nThe pre-training dataset with MathGLM-v2 encompasses a wide spectrum of mathematical problems relevant to middle-school education. This dataset is not just extensive in size but also diverse in content, covering areas from basic arithmetic to more complex algebraic concepts. As for the model itself, MathGLM-v2 is trained on a larger model with 12B training parameters.\n\n\nPreliminary results are promising, showcasing the model's ability to grasp fundamental mathematical concepts and apply them in problem-solving. As we progress, we aim to refine MathGLM-v2 further, enhancing its accuracy and ability to generalize across a wider array of math problems.\n\n\nWe are working towards releasing MathGLM-v2 in the near future."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506328731,
                "cdate": 1700506328731,
                "tmdate": 1700506328731,
                "mdate": 1700506328731,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BX3JZ8QQ4h",
            "forum": "LojXXo2xaf",
            "replyto": "LojXXo2xaf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9208/Reviewer_GLqj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9208/Reviewer_GLqj"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose using LLMs to perform complex mathematical computations. To prove this theory, they trained a model called MathGLM on a dataset with multi-step arithmetic operations and math problems described in text. They verify their results on the APE test set, as well as a K6 dataset they proposed, which consists of elementary-school math word problems. They demonstrate that on a constructed dataset of complex mathematical computations, their model outperforms GPT-4."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Interesting perspective on using LLMs to conduct complex mathematical computations without the use of tools.\n2. The paper states its theory and results clearly."
                },
                "weaknesses": {
                    "value": "1. The claim regarding motivation is not robust. While MathGLM achieves a high accuracy of 93.03% on the constructed dataset for complex computations, these calculations can be done with 100% accuracy using other tools.\n2. The computation is limited to addition, subtraction, multiplication, division, and exponentiation. The method probably wouldn't generalize well to more intricate computations such as log, sin, etc. Moreover, mathematics should aim for complete accuracy, so utilizing LLMs for these calculations isn't a suitable strategy, especially considering the costly pretraining involved for computations that other tools can resolve more efficiently. Instead, LLMs should concentrate on providing more insight and higher-level strategies for solving math problems.\n3. The primary math word problem datasets are APE and K12, both of which are in Chinese. There were no experiments conducted on popular math datasets like MATH and GSM8K. Since GPT-4 is primarily trained in English, and MathGLM is fine-tuned for Chinese math word problems, such a comparison might not be valid. The advantage of MathGLM could be due to the language, rather than its proficiency in resolving math problems."
                },
                "questions": {
                    "value": "See Weaknesses and,\n\nThis paper uses LLMs to conduct complex mathematical computations. This is a novel approach, but the motivation is weak because using LLMs for complex mathematical computations lacks accuracy and generalizability. Additionally, there is a lack of experimentation on MATH and GSM8K, as the primary comparisons are made using Chinese mathematical datasets.\n\nCorrectness: 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\nTechnical Novelty And Significance: 2: The contributions are only marginally significant or novel.\n\nEmpirical Novelty And Significance: 2: The contributions are only marginally significant or novel."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9208/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9208/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9208/Reviewer_GLqj"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9208/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698844263573,
            "cdate": 1698844263573,
            "tmdate": 1699637158754,
            "mdate": 1699637158754,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NF3BDC3yOj",
                "forum": "LojXXo2xaf",
                "replyto": "BX3JZ8QQ4h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9208/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GLqj (1/2)"
                    },
                    "comment": {
                        "value": "Thanks for your appreciation of our work and valuable suggestions! We will explain your concerns point by point.\n\n```\nWeakness 1: The claim regarding motivation is not robust. While MathGLM achieves a high accuracy of 93.03% on the constructed dataset for complex computations, these calculations can be done with 100% accuracy using other tools.\n```\n**Response**: The motivation of our paper is to address the misconception about the capabilities of large language models (LLMs) in performing arithmetic operations. \n\n1. ***Redefining LLMs Capabilities on Arithmetic Tasks*** \nMathGLM challenges the prevailing notion about LLMs' limitations in arithmetic operations, particularly with longer digits and more complex calculations like decimals and fractions. This contributes to the field by expanding the understanding and capabilities of LLMs in mathematical computations. \n\n2. ***Beyond Mere Calculations***\nWe acknowledge that other tools may achieve 100% accuracy. However, we clarify that the uniqueness of MathGLM lies in its ability to handle complex arithmetic operations within the framework of large language models. This allows it to not only perform calculations but also understand and process complex math word problems, which is beyond the capability of many conventional tools. \n\n3. ***Enhanced Performance in Math Word Problems***\nFor math word problems, we incorporate this step-by-step strategy into the original Ape210K dataset, which significantly bolsters MathGLM's answer accuracy (42.29% performance gains). \n\n\n```\nWeakness 2: The computation is limited to addition, subtraction, multiplication, division, and exponentiation. The method probably wouldn't generalize well to more intricate computations such as log, sin, etc. Moreover, mathematics should aim for complete accuracy, so utilizing LLMs for these calculations isn't a suitable strategy, especially considering the costly pretraining involved for computations that other tools can resolve more efficiently. Instead, LLMs should concentrate on providing more insight and higher-level strategies for solving math problems.\n```\n**Response**: In this study, we focus on addressing the misconception about the capabilities of large language models (LLMs) in performing arithmetic operations. MathGLM is currently limited to basic operations like addition, subtraction, multiplication, division, and exponentiation. \n\nIt's important to note that functions like logarithm and sine, which are more complex, are not directly computable by humans without tools. We often rely on calculators for these function calculations, suggesting that LLMs may not be critically needed for such intricate calculations. \n\n\nWe agree with your claim that LLMs should concentrate on providing more insight and higher-level strategies for solving math problems. In our study, we find that calculation errors are a significant error type in solving math word problems (Cf. Figure 9). In order to enhance the performance of MathGLM-10B in solving math word problems, we incorporate an additional 5,500 calculation-specific training data into the reconstructed Ape210K, and evaluate performance on the Ape210K test dataset. From the results, we can observe that by adding calculation-specific data, MathGLM-10B's performance is significantly improved compared with the original MathGLM-10B (reported in our manuscript). Therefore, the study of calculation also plays an important role in solving math word problems. \n\n\n\n|Model | Accuracy |\n|----|----|\n|GPT-3.5 | 39.78% | \n|GPT-4 | 59.57% |\n|MathGLM-10B | 58.68% | \n|MathGLM-10B (with calculation-specific data)  | 63.20% |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506004740,
                "cdate": 1700506004740,
                "tmdate": 1700506133920,
                "mdate": 1700506133920,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WhPcvEfHVg",
                "forum": "LojXXo2xaf",
                "replyto": "BX3JZ8QQ4h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9208/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GLqj (2/2)"
                    },
                    "comment": {
                        "value": "```\nWeakness 3: The primary math word problem datasets are APE and K12, both of which are in Chinese. There were no experiments conducted on popular math datasets like MATH and GSM8K. Since GPT-4 is primarily trained in English, and MathGLM is fine-tuned for Chinese math word problems, such a comparison might not be valid. The advantage of MathGLM could be due to the language, rather than its proficiency in resolving math problems.\n```\n**Response**: In this study, our primary focus is indeed on Chinese language datasets. However, it's important to clarify that the effectiveness of MathGLM is not solely attributable to the language aspect.\n\n\nTo further verify our claim, we develop a Chinese-English mathematical model called MathGLM2-6B. Here, we report a preliminary experimental result on our MathGLM2-6B. We evaluate MathGLM2-6B with the same-scale English mathematical models on three classical inference datasets: Math23K, GSM8K, and MATH. The results indicate that MathGLM2-6B demonstrates promising cross-lingual capabilities. On Math23K, a Chinese language dataset, MathGLM2-6B shows competitive performance with a 61.8% accuracy, slightly behind GPT-4's 63.3%. On GSM8K, MathGLM2-6B achieves an accuracy of 63.00%, slightly lower than the similar-scale model MetaMath-7B. In the MATH dataset, MathGLM2-6B achieves an accuracy of 26.76%. In comparison to other same-scale models, MathGLM2-6B is ahead of WizardMath-7B, Mammonth-7B, and MetaMath-7B.\n\n\nIn conclusion, while language is an important aspect of our model, the primary advantage of MathGLM lies in its ability to understand and solve complex mathematical problems. \n\n\n\n|Model | Math23K | GSM8K| MATH |\n|----|----|----|----|\n|GPT-3.5 | 52.4% |80.8% | 34.1% |\n|GPT-4 | 63.3% | 92% | 42.5% |\n|WizardMath-7B | - | 54.9% |10.70% |\n|Mammonth-7B | - | 50.50% |10.40% |\n|MetaMath-7B | - | 66.50% | 19.80% |\n|MathGLM2-6B | 61.80% | 63.00% | 26.76% |\n\n\n\n\n```\nQuestion 1: This paper uses LLMs to conduct complex mathematical computations. This is a novel approach, but the motivation is weak because using LLMs for complex mathematical computations lacks accuracy and generalizability. Additionally, there is a lack of experimentation on MATH and GSM8K, as the primary comparisons are made using Chinese mathematical datasets.\n```\n**Response**: The motivation of our paper can refer to the response in Weakness 1 and the experimentation on MATH and GSM8K can refer to Weakness 3."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506079657,
                "cdate": 1700506079657,
                "tmdate": 1700506216959,
                "mdate": 1700506216959,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]