[
    {
        "title": "Optimization over Sparse Restricted Convex Sets via Two Steps Projection"
    },
    {
        "review": {
            "id": "Z9Pzyyc1A5",
            "forum": "H8OOlBjhkU",
            "replyto": "H8OOlBjhkU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3393/Reviewer_cMSm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3393/Reviewer_cMSm"
            ],
            "content": {
                "summary": {
                    "value": "Due to a medical emergency, I am unable to assess this article."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "N/A"
                },
                "weaknesses": {
                    "value": "N/A"
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3393/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3393/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3393/Reviewer_cMSm"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3393/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698697606726,
            "cdate": 1698697606726,
            "tmdate": 1699636290359,
            "mdate": 1699636290359,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PsxfNalQ8M",
                "forum": "H8OOlBjhkU",
                "replyto": "Z9Pzyyc1A5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cMSm"
                    },
                    "comment": {
                        "value": "Dear Reviewer cMSm, we sincerely appreciate your support for our paper, even amid a challenging medical emergency."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652498946,
                "cdate": 1700652498946,
                "tmdate": 1700652498946,
                "mdate": 1700652498946,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PWM4q5OrY9",
            "forum": "H8OOlBjhkU",
            "replyto": "H8OOlBjhkU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3393/Reviewer_LAEa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3393/Reviewer_LAEa"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies sparse optimizations problems where the underlying constraints consist of the intersection of an $\\ell_0$ constraint and an extra constraint $\\Gamma$. The authors introduce a IHT algorithm for optimizing over such sets, which uses a novel two step projection (TSP) procedure. By deriving an extension of the three point lemma for this setting, convergence guarantees are derived for this algorithm, assuming the objective satisfies the RSC and RSS conditions. The algorithm is extended to the stochastic and zeroth order settings where similar guarantees are derived."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is written well overall, with a clear state-of-art, problem description, notation, and exposition.\n\n\n2. The proposed TSP operator  is novel in my view, and also very natural.  The extension of the three point lemma derived in this paper is interesting, and is the main technical tool for obtaining the convergence guarantees.\n\n\n3. The convergence results are interesting and I think these would be of interest to those working in the areas of sparse signal recovery."
                },
                "weaknesses": {
                    "value": "1. At present there is no proof outline, which makes it difficult to understand the novelty in the proof technique compared to the literature. I understand there is a space constraint, but given that the main contributions of the paper are theoretical, this is relevant.\n\n2. In the experiments, there is currently no comparison with existing methods. I think this would have been nice to illustrate on synthetic data, and would also provide an empirical validation of the theoretical results."
                },
                "questions": {
                    "value": "I have the following minor remarks\\questions.\n\n1. I think the statement of assumption 3 is currently written in the form of a definition.\n\n2. In Remark 1, the fourth bullet should ideally be the second bullet, just to introduce the notion of convex symmetric sets first.\n\n3. In Algorithm 2, I am a bit confused about the notation $\\mathcal{S}$. Shouldn't the index set $\\mathcal{S}_t$ be a subset of $[n]$? Also in the summation index, it should be $i_t$ instead of $i$?\n\n4. The convergence results pertain to the objective value, but can something be said for the convergence of the iterates to the global minimum? (provided there is a unique global minimum of course)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3393/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698746373052,
            "cdate": 1698746373052,
            "tmdate": 1699636290258,
            "mdate": 1699636290258,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YWZYu30U1h",
                "forum": "H8OOlBjhkU",
                "replyto": "PWM4q5OrY9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LAEa"
                    },
                    "comment": {
                        "value": "Thank you for your insightful review and appreciation of our work.\n\n> **Your comment:** At present there is no proof outline, which makes it difficult to understand the novelty in the proof technique compared to the literature. \n\n**Our response:** Thank you for your suggestion, we have added a proof outline for Theorem 1 in the main paper, and additional explanations on the key parts of our proofs at the beginning of Appendix E.1 and E.3 (from the new revision).\n\n> **Your comment:**  In the experiments, there is currently no comparison with existing methods. I think this would have been nice to illustrate on synthetic data, and would also provide an empirical validation of the theoretical results.\n\n**Our response:** In our experiment from the main paper (Section 5) about the portfolio optimization problem, we compared our algorithm with a naive algorithm: a projected gradient descent with only $\\Gamma$ as constraint, followed by a hard-thresholding at the last iteration. In the new revision, we also added an additional baseline which is IHT followed, after its last iteration, by Euclidean projection onto $\\Gamma$.\nIndeed, with the constraint $\\Gamma$ from this experiment, there is, up to our knowledge, no closed-form for the Euclidean projection (EP) onto our mixed constraint, which is why there are no other baselines to compare our algorithm with. Regarding more synthetic experiments to illustrate our theoretical results, we compared the TSP and EP in our section F.2 which also includes some discussions on our theoretical results and the role of $\\rho$ (we have updated that section to match our new Assumption 3 in the new revision), and following your suggestion, we have added a new section in Appendix F.4 with more synthetic experiments to specifically discuss our novel bounds and their dependence on $\\rho$.\n\n> **Your comment:** I think the statement of assumption 3 is currently written in the form of a definition. \n\n**Our response:** Thank you, we have rewritten all assumptions in an assumption form.  \n\n> **Your comment:** In Remark 1, the fourth bullet should ideally be the second bullet, just to introduce the notion of convex symmetric sets first.\n\n**Our response:** Thank you, we have updated our Assumption on $\\Gamma$ as described above per your suggestion, as well as the following Remark and this has fixed such problem. \n\n> **Your question:** Shouldn't the index set $S$ be a subset of $[n]$? Also in the summation index $i_t$, it should be instead of $i$?\n\n**Our response:**  Thank you, you are right, we have replaced $\\mathcal{S}$ by $[n]$ and fixed the index $i$ into $i\\_t$ in the new revision.\n\n> **Your question:** The convergence results pertain to the objective value, but can something be said for the convergence of the iterates to the global minimum? (provided there is a unique global minimum of course)\n\n**Our response:**  Yes, actually, convergence in terms of $\\\\|\\boldsymbol{w} - \\bar{\\boldsymbol{w}} \\\\|$, with $\\bar{\\boldsymbol{w}} = \\arg\\min\\_{\\boldsymbol{v} \\in \\mathbb{R}^d ~ \\text{s.t.} ~ \\\\| \\boldsymbol{v} \\\\|\\_0 \\leq \\bar{k}} R(\\boldsymbol{v})$, in our extra constraints setting, can be immediately obtained from the result without extra constraint, for an even simpler algorithm which just projects onto $\\Gamma$ at the last step. Indeed, suppose one runs vanilla IHT (or any variant such as the stochastic or zeroth-order one), without the extra projection step, to obtain a bound of the form $\\\\|\\boldsymbol{\\hat{w}\\_T} - \\bar{\\boldsymbol{w}} \\\\|^2 \\leq A\\_T + B$ for some $A\\_T$ and $B$, with $\\boldsymbol{\\hat{w}}\\_T$ the output ($A\\_T$ is usually a decreasing term, and $B$ a system error (in red in Table 1)). Then, one can simply consider $\\boldsymbol{\\hat{z}\\_T} = \\Pi\\_{\\Gamma}(\\boldsymbol{\\hat{w}\\_T})$, i.e. the projection of the output onto $\\Gamma$. Since projection onto a convex set is non-expansive, one will then have $\\\\|\\boldsymbol{\\hat{z}\\_T} - \\bar{\\boldsymbol{w}} \\\\|^2 \\leq \\\\|\\boldsymbol{\\hat{w}\\_T} - \\bar{\\boldsymbol{w}} \\\\|^2 \\leq A\\_T + B$, and therefore the bound will remain unchanged. Note that this cannot be improved in general, since if it could be, one would take $\\Gamma = \\mathbb{R}^d$ and one would obtain better bounds even in the case without extra constraints $\\Gamma$. This is why we did not consider convergence in terms of $\\\\|\\boldsymbol{w} - \\boldsymbol{\\bar{w}} \\\\|$ in our paper: such result is somehow trivial as it does not require any analysis.  As an intuitive explanation, we could say that adding an extra constraint will restrict the space even further, therefore it is easy to see that iterates will only get closer to $\\bar{\\boldsymbol{w}}$: however, what is more interesting and needs to rethink the whole proof of convergence is the impact that such extra constraints have on the risk $R$: this is what we tackle in our paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700317383682,
                "cdate": 1700317383682,
                "tmdate": 1700317383682,
                "mdate": 1700317383682,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p9yl6mucAx",
                "forum": "H8OOlBjhkU",
                "replyto": "YWZYu30U1h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3393/Reviewer_LAEa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3393/Reviewer_LAEa"
                ],
                "content": {
                    "comment": {
                        "value": "Sorry for the delayed response. I have read the response of the authors to my queries and I am satisfied with the response. I will keep my original score for this paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740839611,
                "cdate": 1700740839611,
                "tmdate": 1700740839611,
                "mdate": 1700740839611,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CnAZ091tlY",
            "forum": "H8OOlBjhkU",
            "replyto": "H8OOlBjhkU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3393/Reviewer_MFJV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3393/Reviewer_MFJV"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies rate of convergence of Iterative Hard Thresholding (IHT) on sparse regression with extra constraints on regression coefficients."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper studies rate of convergence of Iterative Hard Thresholding (IHT) on sparse regression with extra constraints on regression coefficients."
                },
                "weaknesses": {
                    "value": "The work is very much similar to literature (Jain et al., 2014; Nguyen et al., 2017; Li et al., 2016; Shen & Li, 2017; de Vazelhes et al., 2022). The referee is afraid that adding an extra constraint \\Gamma of the regression coefficients making any fundamental difference. More importantly, it does not make much sense to enforce an extra constraint \\Gamma. The solution only converges to stationary points. And the authors never verify the assumptions with a practical example."
                },
                "questions": {
                    "value": "The work is very much similar to literature (Jain et al., 2014; Nguyen et al., 2017; Li et al., 2016; Shen & Li, 2017; de Vazelhes et al., 2022). The referee is afraid that adding an extra constraint \\Gamma of the regression coefficients making any fundamental difference. More importantly, it does not make much sense to enforce an extra constraint \\Gamma. The solution only converges to stationary points. And the authors never verify the assumptions with a practical example."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3393/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699103952584,
            "cdate": 1699103952584,
            "tmdate": 1699636290164,
            "mdate": 1699636290164,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m4brBEbryl",
                "forum": "H8OOlBjhkU",
                "replyto": "CnAZ091tlY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MFJV [1/3]"
                    },
                    "comment": {
                        "value": "Thank you for your insightful review. We sincerely hope that the main concerns raised in the review can be clarified by the following responses.  \n\n> **Your comment:**  The work is very much similar to literature (Jain et al., 2014; Nguyen et al., 2017; Li et al., 2016; Shen \\& Li, 2017; de Vazelhes et al., 2022).\n\n**Our response:** As we elaborate upon in the global rebuttal, our work adds some significant contributions to the previous works in IHT: \n\n  1. Even in the case of no extra constraints $\\Gamma$, our developed three-point lemma allows to greatly simplify the previously existing proofs of Jain et al. (2014) (Proof of Theorem 1, Appendix B.1), and Zhou et al. (2018) (Proof of Theorem 2, Appendix B.3): those proofs use intricate observations on the support sets of the iterates, which as such are hardly applicable and extendable. But our corresponding proofs (Proof of Theorem 4 followed by Proof of Theorem 1, and Proof of Theorem 5 followed by Proof of Theorem 2, and finally Proof of Theorem 6 followed by Proof of Theorem 3), are much simpler using more standard tools, which are similar to the ones from convex optimization (three-points lemma, telescopic sums etc), and as such are more extensible to future settings. As an example of such power of extensibility, even in the case of no extra $\\Gamma$, in addition to simplifying and improving existing proofs, we can also provide a new result in the zeroth-order case (Theorem 6, cf. Table 1, described in more details in our Appendix E.3.2): such result is a significant improvement over the result of de Vazelhes et. al. (2022), since it provides a convergence in risk $R(\\boldsymbol{w}\\_t)$, without system error (i.e. without the term in red in Table 1). In the IHT litterature, such type of results are usually harder to obtain than results in terms of $\\\\| \\boldsymbol{w} - \\bar{\\boldsymbol{w}} \\\\|$, which also exhibit an impractical system error.\n\n  2. In terms of technical challenge, since we use a new tool for the proof (the three-points lemma), our work have necessitated to fully rethink the proofs of those variants of IHT, which therefore we believe is a significant contribution to the IHT literature.\n\n  3. And finally, as a main example of the power of our framework, and our main contribution, we provide the novel convergence results that we do in the case of extra constraint $\\Gamma$.\n\n> **Your comment:** The referee is afraid that adding an extra constraint $\\Gamma$ of the regression coefficients making any fundamental difference.\n\n\n**Our response:** First, before addressing your concern regarding extra constraints $\\Gamma$, we would like to mention, as described above, that our work already provides significant contributions to the literature even in the case where there is no extra constraint. With that in mind, when we consider the case with extra constraints and try to build upon our previously introduced techniques, an important new difficulty arises, since the simpler version of our three-point lemma (i.e. Lemma 3 in Appendix) is not valid anymore (it is valid only for a vanilla $\\ell\\_0$ constraint) : this lead us to derive our Lemma 1, which is a three-point lemma which can take into account the extra constraints. This new lemma is key to our analysis and combined with a novel analysis which introduces the trade-off variable $\\rho$, leads to our new results. Additionally, independently of our three-point lemma technique, even trying to directly extend the previous original proofs of convergence for IHT, for convergence in risk without system error, i.e. as in Jain et. al (2014), and Zhou et. al. (2018), in the case of extra constraints, would pose a challenge  (for convergence in terms of $\\\\|\\boldsymbol{w}-\\boldsymbol{\\bar{w}}\\\\|$ it would indeed not change much, as mentioned in the response to Reviewer LAEA Q.4, which is why we focus on convergence in risk). Indeed, the proof of Jain et. al (2014) and Zhou et. al. (2018) crucially rely on a specific lemma (Lemma 1 in Jain et al. (2014), called Lemma 2 in Zhou et. al. (2018)), which does not hold anymore in the case of extra constraint. More precisely, such lemma states that, for a $\\bar{k}$-sparse $\\bar{\\boldsymbol{w}}\\in \\mathbb{R}^d$ and for any $\\boldsymbol{w} \\in \\mathbb{R}^d$:  $\\\\|\\mathcal{H}\\_k(\\boldsymbol{w}) - \\boldsymbol{w} \\\\|^2 \\leq \\frac{d-k}{d-\\bar{k}} \\\\| \\bar{\\boldsymbol{w}} - \\boldsymbol{w}||^2$. However, if we denote our mixed constraints set by $M$ ($= \\mathcal{B}\\_0 \\cap \\Gamma$ ), it does not hold in general that, for a $\\bar{k}$-sparse $\\bar{\\boldsymbol{w}} \\in M: \\\\|\\Pi\\_{M}(\\boldsymbol{w}) - \\boldsymbol{w} \\\\|^2 \\leq \\frac{d-k}{d-\\bar{k}} \\\\| \\bar{\\boldsymbol{w}} - \\boldsymbol{w}\\\\|^2$ (one can see this for instance in the case where $d=3$, $\\bar{k} =1$,  $k = 2$, $\\Gamma$ is the $\\ell\\_{\\infty}$ unit ball, $\\boldsymbol{w} = (10, 10, 0)$ and $\\boldsymbol{\\bar{w}} = (1, 0, 0)$). **[...Continued below...]**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700316614789,
                "cdate": 1700316614789,
                "tmdate": 1700316614789,
                "mdate": 1700316614789,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vNPqxM0k6X",
            "forum": "H8OOlBjhkU",
            "replyto": "H8OOlBjhkU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3393/Reviewer_r73h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3393/Reviewer_r73h"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript investigated a constrained optimization setting where the goal is to minimize an objective function while satisfying some sparsity constraints. A two-step-projected gradient-based approach is proposed. Their performance is analyzed for both stochastic and non-stochastic settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Understanding optimization with sparsity constraint is a long-standing open question. This submission is one of not many works that formulates and provides analysis towards that direction."
                },
                "weaknesses": {
                    "value": "Technically, there are hidden constraints to the main results that could significantly limit the strength of the proposed results. For example, Theorem 1 and the remarks that follow imply that the proposed two-step projection guarantees the convergence to the global minimum if the minimizer is sparse, which is not true in the most generic case (e.g., when k=1). However, the proof was made possible due to the requirement of $k\\geq \\frac{4(1-\\rho)^2 L_s^2}{\\rho^2 \\nu_s^2} \\overline{k}$, which in fact requires the sparsity constrain has to be weak, i.e., lower bounded by the square of the condition number for the non-trivial case of $\\overline{k}\\neq 0$. So, no guarantee is provided for the important case of fixed sparsity $k$, even if we allow k to be an arbitrarily large constant. \n\nSome other minor comments and suggestions:\n\n1. The main results are rather hard to read due to the fact that many needed notations are defined informally inline in different sections, for example, H_k in Theorem 1. Possible ways of improvement include either defining the notations formally in a definition environment or naming the notation so they can be easily searched. \n\n2. Interpretation of  $\\rho$ in the main result: We are generally interested in the achievable result of $R(w_t)$ and the inequality presented in Theorem 1 seems to be a more intermediate result where the role of parameter $\\rho$ could be unclear to the readers. For readability, the reviewer suggests that Remarks 3 and 4 should be summarized as a main theorem, then Theorem 1 is presented as a technical result for proving them."
                },
                "questions": {
                    "value": "As the presented theorem requires a sparsity constraint k that grows unbounded with respect to the condition number. Would it be possible to modify the analysis or the algorithm to extend the results for any large constant k?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3393/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699189185608,
            "cdate": 1699189185608,
            "tmdate": 1699636290101,
            "mdate": 1699636290101,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OnH0tqXjGl",
                "forum": "H8OOlBjhkU",
                "replyto": "vNPqxM0k6X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer r73h"
                    },
                    "comment": {
                        "value": "Thank you for your insightful review. We sincerely hope that the main concerns raised in the review can be clarified by the following responses.  \n\n> **Your comment:** Technically, there are hidden constraints to the main results that could significantly limit the strength of the proposed results. ...So, no guarantee is provided for the important case of fixed sparsity $k$, even if we allow k to be an arbitrarily large constant.\n\n\n**Our response:** Thank you for your suggestions. Regarding the requirement $k \\geq \\frac{1(1 - \\rho)^2 L\\_s^2}{\\rho^2 \\nu\\_s^2}  \\bar{k}$, we would like to highlight that such a relaxation of $k$, depending on the condition number $\\frac{L\\_s}{\\nu\\_s}$ is present in all the literature of hard-thresholding in the RSC-RSS setting, as can be shown by the references in our Table 1 (column \"$k$\") (namely, Jain et al. (2014), Nguyen et al. (2017), Li et al. (2016), Zhou et al. (2018), and de Vazelhes et al. (2022)). This stems from the fact that the problem is NP-hard, and even hard to approximate [1] and as such the original problem $\\min\\_{\\boldsymbol{w}} f(\\boldsymbol{w}) ~ \\text{s.t.} ~ \\\\| \\boldsymbol{w} \\\\|\\_0 \\leq k$ cannot be solved in general to global optimality in its original form with a fixed $k$. In fact, it was actually shown in [1] that the $\\kappa^2 = \\frac{L\\_s^2}{\\nu\\_s^2}$ factor of the sparsity relaxation cannot be improved in the analysis of IHT (Iterative Hard Thresholding). Finally, we highlight that usually hard-thresholding methods are used for very high-dimensional problems, and as such, even relaxing the sparsity by a constant factor provides sparse enough solutions. \n\n\n> **Your comment:** The main results are rather hard to read due to the fact that many needed notations are defined informally inline in different sections... so they can be easily searched.\n\n**Our response:** Thank you for your suggestion, we have gathered our notations at the beginning of Section 2 (Preliminaries), and have added a section in the supplemental referencing them, for sake of clarity.\n\n> **Your comment:** For readability, the reviewer suggests that Remarks 3 and 4 should be summarized as a main theorem, then Theorem 1 is presented as a technical result for proving them.\n\n**Our response:**  Thank you for your suggestion. We agree that the result in terms of $R(\\boldsymbol{w}\\_t)$ is indeed clearer: we have rewritten Theorem 1 so as to display the result in $R(\\boldsymbol{w}\\_t)$ as per Remark 3 and 4 in the new revision.\n\n> **Your question:** Would it be possible to modify the analysis or the algorithm to extend the results for any large constant k?\n\n**Our response:** In general, it would not be possible to modify the analysis to extend the result for any large constant $k$, since it is known that $k$ must be in $O(\\kappa^2 \\bar{k})$, where $\\kappa$ is the condition number, and such dependence is not improvable in the analysis of IHT (as described in [1]). This fundamental bottleneck is essentially due to the NP-hardness of such a non-convex optimization problem.\n\n\n[1] Iterative hard thresholding with adaptive regularization: Sparser solutions without sacrificing runtime, K. Axiotis and M. Sviridenko, ICML, 2022"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700316118518,
                "cdate": 1700316118518,
                "tmdate": 1700316118518,
                "mdate": 1700316118518,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]