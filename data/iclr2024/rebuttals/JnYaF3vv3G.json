[
    {
        "title": "LabelDP-Pro: Learning with Label Differential Privacy via Projections"
    },
    {
        "review": {
            "id": "zKYs4kXYo5",
            "forum": "JnYaF3vv3G",
            "replyto": "JnYaF3vv3G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4094/Reviewer_NHnW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4094/Reviewer_NHnW"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a cute idea -- leveraging the feature vectors (which are non-private under label DP) to construct a prior, which helps to reduce the noise required for label DP. To this end, this paper considers label-DP in the central model rather than the previous local model. Some theoretical results and experimental results are provided."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of leveraging feature vectors to boost the performance under the label DP is cute.\n2. A good balance between theory and experimental results, which is a good fit for ICLR."
                },
                "weaknesses": {
                    "value": "I think the comparisons in the theoretical part need to be clear, i.e., what's the exact gain of the proposed method?"
                },
                "questions": {
                    "value": "I think this paper introduces some nice ideas and I also enjoyed reading this paper. \n\nI have some clarification questions so as to ensure I did not miss something. \n\n1. [Regarding the subtlety of privacy analysis of SELFSPAN]. If I understand it correctly, the problem is that even though the additional projection step does not touch sensitive data (labels) in this case, one cannot directly use post-processing. This is because the projection will leak the index of the sampling result, which then impacts the privacy amplification by subsampling in the original DP-SGD. Due to this, when using SELFSPAN, one cannot enjoy the gain/benefit of subsampling. On the other hand, since it does not touch labels, one can still use post-processing over the **non-subsampling** version of DP-SGD?\n\n2. [Regarding the improvement over DP-SGD] It seems to me that the main gain over DP-SGD is the improvement over dimension d? It might be better to give more discussions on Table 4, as there are different choices of \\sigma. Being more specific or using more particular values will be better I think. \n\n3. [Confusion about the section name] For Section 4, the authors use SCO, which gives readers the impression that the goal is somehow the population excess loss. However, it turns out the authors are actually analyzing ERM. It might be better to replace the section name."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4094/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698629225285,
            "cdate": 1698629225285,
            "tmdate": 1699636374230,
            "mdate": 1699636374230,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bToudhM5qx",
                "forum": "JnYaF3vv3G",
                "replyto": "zKYs4kXYo5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4094/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4094/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your insightful discussion. We were encouraged that you found our method \u201ccute\u201d, and we achieved \u201cA good balance between theory and experimental results, which is a good fit for ICLR\u201d. Here we address your detailed comments, which are helping us revise the paper and chart out future directions.\n\n**Q1. Theoretical contribution**\n> I think the comparisons in the theoretical part need to be clear, i.e., what's the exact gain of the proposed method?\n\n**A**: We thank the reviewer for this question. The goal of the theoretical section is to study the theoretical convex setting to build intuition for the general case of deep neural networks (which are beyond the reach of current theoretical frameworks). The goal is to provide some intuition for how LabelDP-Pro with different denoisers might compare to each other; in fact, the analysis of this setting is precisely what guided us to try the different denoisers that we do, but this section is by no means supposed to a complete explanation for the empirical results. Note that we provide the following caveat in the last paragraph of Section 4, _\u201cNote that the bounds in Table 4 hold only for convex loss functions. Moreover, they are upper bounds on the excess error. For instance, while the bound in Lemma 3 is tight in the worst case, better rates might be achievable under additional assumptions on the loss function. Despite these limitations, we see in Table 4 that the excess error bounds align well with the experimental findings.\u201d_\n\n**Q2. Non-applicability of Subsampling for SelfSpan**\n> If I understand it correctly, the problem is that even though the additional projection step does not touch sensitive data (labels) in this case, one cannot directly use post-processing. This is because the projection will leak the index of the sampling result, which then impacts the privacy amplification by subsampling in the original DP-SGD. Due to this, when using SELFSPAN, one cannot enjoy the gain/benefit of subsampling. On the other hand, since it does not touch labels, one can still use post-processing over the non-subsampling version of DP-SGD?\n\n**A**: Yes, indeed this is correct.\n\nLabel-DP Pro using Self-Span projection can be viewed as a post-processing of non-subsampling version of DP-SGD, whereas Label-DP Pro using Alt-Span denoiser can be viewed as a post-processing of the subsampled version of DP-SGD.\nPerhaps a simple way to see this is as follows. Consider the following two mechanisms:\n1. Mechanism $M_1$ that reveals, for each training step, just the noisy average gradient for the batch $I^G$ (without revealing $I^G$). Amplification by subsampling applies in this case.\n2. Mechanism $M_2$ that reveals, for each training step, the entire _unlabeled_ batch $I^G$ along with noisy average gradients for that batch. Amplification by subsampling does not apply in this case.\n\nDP-SGD and LabelDP-Pro with Alt-Conv denoiser can be viewed as a post-processing of $M_1$, since in the latter, the projection operation only uses non-private information, namely the alt-batch $I^P$.\n\nOn the other hand, LabelDP-Pro with Self-Span / Self-Conv denoisers cannot be seen as a post-processing of $M_1$, but can still be seen as a post-processing of $M_2$."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261446084,
                "cdate": 1700261446084,
                "tmdate": 1700261446084,
                "mdate": 1700261446084,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yp10dPDFl2",
                "forum": "JnYaF3vv3G",
                "replyto": "zKYs4kXYo5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4094/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4094/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "We would like to extend our gratitude to the reviewer once again for their constructive feedback. \n\nDid our responses resolve the reviewer's concerns regarding the clarity of our theoretical results? Given that this appears to be the reviewer\u2019s primary concern, we would like to ensure that we address it before the rebuttal deadline (Nov 22nd). We are also open to continuing discussions and welcome any further comments."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596189253,
                "cdate": 1700596189253,
                "tmdate": 1700596189253,
                "mdate": 1700596189253,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "khjD5m2sV7",
            "forum": "JnYaF3vv3G",
            "replyto": "JnYaF3vv3G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4094/Reviewer_zF53"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4094/Reviewer_zF53"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the algorithm design for label differential privacy (label-DP), where the labels in the dataset are private and the features are public. Previous label-DP algorithms are in the framework of random label flipping. This paper proposes a novel algorithm which utilizes the framework of DP-SGD. By the empirical evaluation, the proposed algorithm has the advantage over previous algorithms in the high-privacy regime."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The clarity is great. The arguments are well-explained and the structure is good.\n2. The proposed algorithm is novel. It is different from previous algorithms which are in the framework of label random flipping. It instead utilizes the DP-SGD framework.\n3. It derives the theoretical utility-privacy trade-off for DP convex optimization and compares this trade-off among different versions of their algorithm.\n4. The proposed methods and baselines are evaluated on both the image benchmark and the Criteo dataset."
                },
                "weaknesses": {
                    "value": "Both the proposed method and baseline numbers in the experiment can be potentially better. As they might be underestimated, the current comparison could be inaccurate, which is the main evaluation to illustrate the advantage of the proposed algorithm.\n- The pure DP-SGD can be much better, which can potentially bring benefits for the proposed algorithm. For example, De et al., 2022 empirically show that DP-SGD can achieve 56.8% accuracy on CIFAR10 when $\\varepsilon=1.0$, while the number reported in the paper is only 43.5%.\n- ALIBI's accuracy on CIFAR-10 is reported as 51.3 when $\\varepsilon=1.0$ in Table 5 in the paper, while in the original paper (Malek et al. (2021)) ALIBI achieves 71% when $\\varepsilon=1.0$.\n- When leveraging self-supervised learning with LabelDP, PATE-FM doesn't utilize the SelfSL on CIFAR-10, which seems a little unfair. It is reasonable to at least utilize a pretrained feature extractor as an initialization instead of random initialization when training teachers in PATE-FM. In this way, PATE-FM leveraging SelfSL is expected to have better accuracy.\n\nMoreover, DP-SGD with gradient projection actually has been investigated for a while [1, 2] and not introduced in the paper. Although I still agree with the novelty of the proposed method given that all previous labelDP algorithms are based on label flipping, it might be worthwhile to see the difference and comparison between the proposed method and the methods in the literature.\n\n[1] Yu, Da, et al. \"Do not Let Privacy Overbill Utility: Gradient Embedding Perturbation for Private Learning.\" International Conference on Learning Representations. 2020.\n\n[2] Kairouz, Peter, et al. \"Fast dimension independent private adagrad on publicly estimated subspaces.\" arXiv preprint arXiv:2008.06570 (2020)."
                },
                "questions": {
                    "value": "Please check the details in \"Weaknesses\" above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4094/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698742383765,
            "cdate": 1698742383765,
            "tmdate": 1699636374156,
            "mdate": 1699636374156,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ooZHJdad4c",
                "forum": "JnYaF3vv3G",
                "replyto": "khjD5m2sV7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4094/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4094/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s insightful feedback. We were encouraged that you found our method novel and our arguments well-explained. Here we address your detailed comments, which are helping us revise the paper and chart out future directions.\n\n**Q1. Concerns about the accuracies**\n> The pure DP-SGD can be much better, which can potentially bring benefits for the proposed algorithm. For example, De et al., 2022 empirically show that DP-SGD can achieve 56.8% accuracy on CIFAR10 when eps=1.0 , while the number reported in the paper is only 43.5%.\n\n> ALIBI's accuracy on CIFAR-10 is reported as 51.3 when eps=1.0 in Table 5 in the paper, while in the original paper (Malek et al. (2021)) ALIBI achieves 71% when eps=1.0.\n\n**A**: Thanks for the comments! It is worth noting that the differences in accuracy between our results and those reported in previous studies can be attributed to the choice of a simpler convolutional network (see Table 10.b in Appendix D) as opposed to the ResNet used in earlier work. This variance in backbone models was driven by efficiency considerations, given our large-scale evaluations across multiple settings and benchmarks.\n\nHowever, as our method builds upon DP-SGD and reduces the noise it injects; if the baseline DP-SGD accuracy improves, we shall be able to achieve better accuracy. For instance, as shown below, when we switch to a more complicated network (with 4 more Conv layers), DP-SGD, ALIBI and our method all achieve higher performance, while our method is still able to retain the improvement for small $\\varepsilon$\u2019s.\n\n| $\\varepsilon$ | ALIBI | DP-SGD | LabelDP-Pro |\n|---|---|---|---|\n| 0.05 | 9.9 | 10.3 | **18.4** |\n| 0.1 | 9.9 | 28.9 | **31.5** |\n| 0.2 | 22.4 | 36.0 | **38.3** |\n| 0.5 | 46.3 | 48.3 | **50.1** |\n| 1.0 | **68.7** | 51.6 | 52.3 |\n\n**Q2. Comparison w/ PATE-FM** \n> When leveraging self-supervised learning with LabelDP, PATE-FM doesn't utilize the SelfSL on CIFAR-10, which seems a little unfair. It is reasonable to at least utilize a pretrained feature extractor as an initialization instead of random initialization when training teachers in PATE-FM. In this way, PATE-FM leveraging SelfSL is expected to have better accuracy.\n\n**A**: Thank you for raising this concern. We want to clarify that while PATE-FM does not incorporate Self-Supervised Learning (SelfSL), its Semi-Supervised Learning (SemiSL) pipeline achieves notably higher accuracy (95.5% non-private accuracy) compared to the SelfSL pipeline we used in our evaluation, which achieves 90.9% non-private accuracy. This observation suggests that their backbone model is, in fact, **stronger** than our model.\n\nIt is important to note that even though PATE-FM utilizes a more robust backbone model, our LabelDP-Pro approach continues to outperform PATE-FM in the high-privacy regime (see Table 6, $\\varepsilon \\leq 0.5$)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261379200,
                "cdate": 1700261379200,
                "tmdate": 1700261379200,
                "mdate": 1700261379200,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vqpGbpeiX5",
                "forum": "JnYaF3vv3G",
                "replyto": "khjD5m2sV7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4094/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4094/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "We would like to extend our gratitude to the reviewer once again for their constructive feedback. \n\nDid our responses resolve the reviewer's concerns regarding 1) the accuracy when using a stronger backbone model, 2) our comparison with PATE-FM, and 3) the discussion and comparison with previous work? Given that they appear to be the reviewer\u2019s primary concern, we would like to ensure that we address them before the rebuttal deadline (Nov 22nd). We are also open to continuing discussions and welcome any further comments."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596137004,
                "cdate": 1700596137004,
                "tmdate": 1700596137004,
                "mdate": 1700596137004,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jkBYn6Zdqb",
                "forum": "JnYaF3vv3G",
                "replyto": "PQ8WgRwPYH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4094/Reviewer_zF53"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4094/Reviewer_zF53"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer zF53"
                    },
                    "comment": {
                        "value": "Thank the authors for the response!\n\n**Concerns on the accuracy.** Thanks for the clarifications on the experiment set-up.\n\n**Comparison w/ PATE-FM.** The comparison between non-private semi-supervised learning and self-supervised learning techniques seems not fair, because the performance of non-private self-supervised learning can be fully developed, while semi-supervised learning in PATE-FM is only developed with noisy labels so the non-private performance can be not fully leveraged.\n\n**Comparison with previous work.** Thanks for the clarification and it addresses my concern. I think it is better to move this discussion to the main paper instead of the appendix.\n\nBecause PATE-FM is the SOTA label-dp algorithm and the advantage of the proposed algorithm over PATE-FM is not clear, I decide to keep my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712913017,
                "cdate": 1700712913017,
                "tmdate": 1700712913017,
                "mdate": 1700712913017,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Bvihbn9p79",
            "forum": "JnYaF3vv3G",
            "replyto": "JnYaF3vv3G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4094/Reviewer_Taxi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4094/Reviewer_Taxi"
            ],
            "content": {
                "summary": {
                    "value": "Paper proposes to use DP-SGD (private features and labels) in a label-only DP setting. It then proposes a way to de-noise DPSGD noisy gradients by projecting them to a convex hull build out of gradients derived only from the features (calculating gradients per existing label class). \n\nPaper then shows how this de-noising process can be made compatible with privacy amplification by subsampling that is fundamental to utility-privacy trade off in DPSGD."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Paper proposes a novel gradient denoising method in label-only private settings. The proposed algorithm outperforms SOTA in high privacy regime on small public datasets like MNIST and CIFAR-10."
                },
                "weaknesses": {
                    "value": "The main issue with applied DP is that production use cases really crave performances close to non-private regimes. Label private settings are very important for ads and marketing use cases and accuracy is of utmost value in these settings. Developers will prefer a medium privacy regime with very close to non-private utility compared to a highly private regime with very low utility. \nFrom the paper results, trends suggest that the proposed algorithm gets inferior to SOTA as epsilon starts to pass 1 which I think is the most important hurdle in using the proposed algorithm in practice."
                },
                "questions": {
                    "value": "I would like to see results of the proposed algorithm for epsilons between 1 and 10 too (for a fairer comparison and decision making for interested users). \n\nI would like to see an empirical epsilon study (like the one referenced by authors, e.g. Malek et al. 2021)\n\nI would like to see studies on CIFAR-100 (along with comparison with SOTA)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4094/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781700418,
            "cdate": 1698781700418,
            "tmdate": 1699636374069,
            "mdate": 1699636374069,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lByGqc4uiU",
                "forum": "JnYaF3vv3G",
                "replyto": "Bvihbn9p79",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4094/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4094/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your insightful comments. We were encouraged that you found our method novel and acknowledged our improvement over SOTA. Here we address your detailed comments, which are helping us revise the paper and chart out future directions.\n\n**Q1. Concerns about targeting high-privacy regime** \n> Developers will prefer a medium privacy regime with very close to non-private utility compared to a highly private regime with very low utility. From the paper results, trends suggest that the proposed algorithm gets inferior to SOTA as epsilon starts to pass 1 which I think is the most important hurdle in using the proposed algorithm in practice.\n\n**A**: We appreciate this comment. We have explained why we consider the high-privacy regime in our [general response](https://openreview.net/forum?id=JnYaF3vv3G&noteId=6bj5h7E7Kl). In short, this consideration results from the intrinsic definition of DP and practical requirements for stronger privacy guarantees, as well as the unsatisfactory performance of existing LabelDP baselines.\n\nFurthermore, we note that in user-level privacy applications (as detailed in Table 7), our approach continues to outperform the baseline for medium-level privacy with $\\varepsilon=5$, which demonstrates the versatility of our method in delivering competitive utility even in scenarios where a moderate level of privacy is required.\n\n**Q2. Results for moderate level of privacy**\n> I would like to see results of the proposed algorithm for epsilons between 1 and 10 too (for a fairer comparison and decision making for interested users).\n\n**A**: Thank you for your suggestion. Please find below the results for $\\varepsilon$ values ranging from 0.05 to 10, with the best accuracy for each $\\varepsilon$ highlighted in **boldface**. However, it is important to note that our method primarily focuses on achieving high privacy (i.e., the low-$\\varepsilon$ regime). As a result, the relatively lower performance in larger $\\varepsilon$ values is expected and has been explained in the theoretical analysis presented in Section 4 of our submission.\n\n| $\\varepsilon$ | RR | RR-Debiased | LP-2ST | ALIBI | DP-SGD | LabelDP-Pro (Ours) |\n|---|---|---|---|---|---|---|\n| 0.05 | 10.0 | 10.0 | 10.0 | 9.9 | 10.0 | **16.0** |\n| 0.1 | 10.0 | 10.0 | 10.0 | 9.9 | 24.6 | **27.0** |\n| 0.2 | 10.0 | 10.0 | 10.0 | 17.9 | 29.8 | **30.8** |\n| 0.5 | 10.0 | 10.0 | 35.8 | 34.4 | 36.9 | **40.2** |\n| 1.0 | 17.0 | 10.0 | **60.9** | 51.3 | 43.5 | 44.8 |\n| 2.0 | 77.6 | 58.8 | 63.8 | **64.2** | 51.4 | 52.0 |\n| 5.0 | 80.4 | 80.2 | 81.7 | **82.3** | 59.5 | 59.8 |\n| 10.0 | 81.5 | 81.8 | 83.5 | **83.5** | 63.6 | 63.7 |\n\n**Q3. Empirical epsilon study**\n> I would like to see an empirical epsilon study (like the one referenced by authors, e.g. Malek et al. 2021)\n\n**A**: Thank you for your suggestion. While we acknowledge the importance of privacy auditing and empirical privacy studies, it appears to be outside the scope of our current work. We intend to consider this for future research endeavors.\n\n**Q4. Results on CIFAR-100**\n> I would like to see studies on CIFAR-100 (along with a comparison with SOTA)\n\n**A**: We appreciate this suggestion and are working on gathering these results. We will try to share them in a later post."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261221275,
                "cdate": 1700261221275,
                "tmdate": 1700261221275,
                "mdate": 1700261221275,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cC8j02bhK3",
            "forum": "JnYaF3vv3G",
            "replyto": "JnYaF3vv3G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4094/Reviewer_3Pz1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4094/Reviewer_3Pz1"
            ],
            "content": {
                "summary": {
                    "value": "This work considers training under label differential privacy by projecting the DP-SGD gradient onto a gradient subspace that only depends on the features. Experiments show that the algorithm works well in the high privacy regime ($\\varepsilon <1$). Theoretical analysis proves convergence in the stochastic convex optimization setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea to project the noisy gradient of DP-SGD onto a smaller subspace is simple and intuitive. \n2. The authors also address practical issues of memory efficiency and stability when calculating projections, which makes the algorithm more practical.\n3. The experiment result is encouraging for the high privacy regime ($\\varepsilon < 1$)\n4. The algorithm has theoretical support which shows that dimension-independent convergence rate can be achieved under stochastic convex optimization."
                },
                "weaknesses": {
                    "value": "1. The user-level private algorithm only leverages group privacy, which is extremely sub-optimal even in the simple problem of mean estimation. A good user-level private algorithm should go beyond the simple application of group privacy.\n2. For $\\varepsilon \\ge 1$ in Tables 5 and 6, the proposed algorithm underperforms the best baseline by a large margin. In practice, it is still reasonable to use $\\varepsilon < 5$, so it would be better if the algorithm could also perform well for moderate level of privacy."
                },
                "questions": {
                    "value": "1. Could you explain why the proposed algorithm underperforms the best baseline by a large margin when training from scratch for $\\varepsilon \\ge 1$? \n2. When using pre-trained weights from SSL, the result for $\\varepsilon > 1$ is much better. Maybe this is due to a more accurate gradient subspace after SSL pretraining?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4094/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805989915,
            "cdate": 1698805989915,
            "tmdate": 1699636373962,
            "mdate": 1699636373962,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yejsw9DdA2",
                "forum": "JnYaF3vv3G",
                "replyto": "cC8j02bhK3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4094/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4094/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your insightful input. We were encouraged that you found our method simple yet practical, and our empirical results encouraging. Here we address your detailed comments, which are helping us revise the paper and chart out future directions.\n\n**Q1. Sub-optimal choice for group privacy**\n> The user-level private algorithm only leverages group privacy, which is extremely sub-optimal even in the simple problem of mean estimation. A good user-level private algorithm should go beyond the simple application of group privacy.\n\n**A**:  We appreciate this comment. We would like to note that we ensure a fair comparison among all methods by **employing the same group privacy rule for all approaches**. Furthermore, we evaluated our approach not only with user-level privacy (Section 6), we also showed that it consistently outperforms the baselines in item-level privacy settings (Section 5).\n\nWe would also like to note that, while there are many recent results on user-level privacy that go beyond group privacy [1,2,3], none of the results are practical as far as we can tell (e.g. in the use of the i.i.d. or other assumptions on the data). This is demonstrated by the lack of empirical evaluations in these works. We would appreciate it if the reviewer could provide further clarification or pointers to prior work on practical user-level privacy that goes beyond simple group privacy.\n\n[1] Daniel Levy, Ziteng Sun, Kareem Amin, Satyen Kale, Alex Kulesza, Mehryar Mohri, Ananda Theertha Suresh. \u201cLearning with User-Level Privacy.\u201d https://arxiv.org/abs/2102.11845 \n\n[2] Esfandiari, Hossein, Vahab Mirrokni, and Shyam Narayanan. \"Tight and robust private mean estimation with few users.\" https://arxiv.org/abs/2110.11876\n\n[3] Badih Ghazi, Pritish Kamath, Ravi Kumar, Raghu Meka, Pasin Manurangsi, Chiyuan Zhang. \u201cOn User-Level Private Convex Optimization.\u201d https://arxiv.org/abs/2305.04912 \n\n**Q2. Performance for moderate level of privacy**\n> For eps>=1 in Tables 5 and 6, the proposed algorithm underperforms the best baseline by a large margin. In practice, it is still reasonable to use eps<5, so it would be better if the algorithm could also perform well for moderate level of privacy.\n\n**A**: We appreciate this comment. We\u2019ve explained why we consider the high-privacy regime in our [general response](https://openreview.net/forum?id=JnYaF3vv3G&noteId=6bj5h7E7Kl). In short, this consideration results from the intrinsic definition of DP and practical requirements for stronger privacy guarantees, as well as the unsatisfactory performance of existing LabelDP baselines.\n\nFurthermore, we note that in user-level privacy applications (as detailed in Table 7), our approach continues to outperform the baseline for medium-level privacy with $\\varepsilon=5$, which demonstrates the versatility of our method in delivering competitive utility even in scenarios where a moderate level of privacy is required.\n\n**Q3. Explanation for inferior performance for eps>=1**\n> Could you explain why the proposed algorithm underperforms the best baseline by a large margin when training from scratch for eps>=1\n\n**A**: Thanks for this question. A possible explanation for this is that the excess variance in the gradients introduced by randomized response decreases exponentially as \u03b5 increases beyond 1. On the other hand, in DP-SGD, the excess variance in the gradients decreases at best only polynomially $1/\\varepsilon^{O(1)}$. This is analyzed formally in the case of convex optimization in Table 4 of our submission. This distinction can provide an explanation of why our method, which is built on DP-SGD, outperforms baseline methods utilizing randomized response in the low-\u03b5 regime but exhibits inferior performance in the high-\u03b5 regime. \n\n**Q4. SSL outperforms trained from scratch**\n> When using pre-trained weights from SSL, the result for eps>1  is much better. Maybe this is due to a more accurate gradient subspace after SSL pretraining?\n\n**A**: Thank you for this question. Yes, the accuracies achieved under SSL (Table 6) are higher than those achieved when trained from scratch (Table 5), which could be attributed to better image representations learnt by SSL and thus resulting in a more accurate gradient subspace."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261059752,
                "cdate": 1700261059752,
                "tmdate": 1700261059752,
                "mdate": 1700261059752,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dr09DImOBg",
                "forum": "JnYaF3vv3G",
                "replyto": "cC8j02bhK3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4094/Reviewer_3Pz1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4094/Reviewer_3Pz1"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comment."
                    },
                    "comment": {
                        "value": "### **Results for moderate and large epsilon**\nThanks for your clarification and explanation. I think the large gap between the proposed method and existing methods in the medium to low privacy regimes is still my biggest concern. If the method is only marginally inferior to the baselines for $\\varepsilon>1$, I would be much more supportive of this work as it proposes a new projective gradient method for Label DP. My opinion remains unchanged for now.\n\n### **Comments on user-level privacy**\nI do not quite agree with the argument that  \"practical applications of user-level DP cannot go beyond group privacy\". Many works in federated learning have integrated user-level privacy. See Figure 2 in [1] and a more recent work of [2]. I don't think any of these works use a simple application of group privacy. These works apply minibatch gradients in DP-SGD/ DP-Fed-Avg for each user, replacing per-example gradients in standard item-level DP-SGD, which should naturally provide user-level privacy guarantees. Using a per-batch gradient would help reduce the variance, which is precisely the important intuition that was used in the three theoretical works you mentioned. \n\nMoreover, the experiments you have demonstrated for user-level DP have no more than 10 samples per user, which is far from practical. Even with such a small $k$, we can also see as $k$ increases, the performance actually drops, which is another empirical evidence that group privacy is highly sub-optimal.  I am curious about what happens when k is slightly larger, say 100, which is very common in practical scenarios. I view this part of the result more as a demonstration of good performance for very small epsilon, rather than an independent and novel contribution to user-level differential privacy.\n\n[1] Practical and Private (Deep) Learning Without Sampling or Shuffling, Kairouz et.al.  ICML 2021\n[2] Learning to Generate Image Embeddings with User-level Differential Privacy, Xu et.al, CVPR 2023"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276507681,
                "cdate": 1700276507681,
                "tmdate": 1700276526899,
                "mdate": 1700276526899,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QgayKHSZ9g",
                "forum": "JnYaF3vv3G",
                "replyto": "cC8j02bhK3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4094/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4094/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**1. On high v.s. low privacy**\n> I think the large gap between the proposed method and existing methods in the medium to low privacy regimes is still my biggest concern. If the method is only marginally inferior to the baselines for $\\varepsilon>1$, I would be much more supportive of this work as it proposes a new projective gradient method for Label DP.\n\n**A**: Our method serves as a better LabelDP option in high-privacy regimes. However, it's worth noting that developers retain the flexibility to choose SOTA methods for low-privacy regimes. Furthermore, we have conducted an empirical study on the point of intersection (as detailed in Appendix D.4), which can serve as a reference for making decisions about which method to deploy.\n\nFurthermore, we wish to point out that it is common for different DP algorithms to be better in different regimes of parameters. For example, for linear queries, there are three pure-DP algorithms which are better for different regimes of parameters; see Theorem 1 in [A].\n\nReferences:\n[A] Aleksandar Nikolov: Private Query Release via the Johnson-Lindenstrauss Transform. SODA 2023: pp. 4982-5002.\n\n\n**2. On user-level privacy**\n> I am curious about what happens when k is slightly larger, say 100, which is very common in practical scenarios.\n\n**A**: Below we present the results for $k=100$. As shown, our method consistently outperforms RR and DP-SGD under $k=100$.\n\n| $\\varepsilon$ | RR | DP-SGD | LabelDP-Pro |\n|---|---|---|---|\n| 0.1 | 0.216 | 0.634 | 0.675 |\n| 0.2 | 0.218 | 0.662 | 0.694 |\n| 0.5 | 0.251 | 0.697 | 0.731 |\n| 1.0 | 0.577 | 0.701 | 0.736 |\n| 2.0 | 0.643 | 0.709 | 0.743 |\n| 5.0 | 0.698 | 0.726 | 0.748 |\n\n> I do not quite agree with the argument that \"practical applications of user-level DP cannot go beyond group privacy\". Many works in federated learning have integrated user-level privacy. See Figure 2 in [1] and a more recent work of [2]. I don't think any of these works use a simple application of group privacy. These works apply minibatch gradients in DP-SGD/ DP-Fed-Avg for each user, replacing per-example gradients in standard item-level DP-SGD, which should naturally provide user-level privacy guarantees. \n\n**A**: We appreciate the reviewer\u2019s prompt response and for providing the references. We would like to clarify that what the reviewer suggested is exactly our implementation for user-level DP-SGD and LabelDP-Pro. We use group privacy only for RR-based methods. We apologize for any lack of clarity in our initial submission, and for the confusion in our previous response.\n\nWe agree with the reviewer and have observed that opting for group privacy results in a decrease in performance for these two gradients-involved methods. This indeed underscores another advantage of our approach, as it is compatible with a more effective user-level privacy implementation.\n\nWe have updated our submission to reflect these details more explicitly (Section 6) and have also included the provided references."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665848351,
                "cdate": 1700665848351,
                "tmdate": 1700665862807,
                "mdate": 1700665862807,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]