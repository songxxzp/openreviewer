[
    {
        "title": "Look, Remember and Reason: Grounded Reasoning in Videos with Language Models"
    },
    {
        "review": {
            "id": "HXmSYIy3u7",
            "forum": "jhPvuc7kxB",
            "replyto": "jhPvuc7kxB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3946/Reviewer_Fa1W"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3946/Reviewer_Fa1W"
            ],
            "content": {
                "summary": {
                    "value": "The authors claim that they propose training an LM end-to-end on low-level surrogate tasks, including object detection, re-identification, and tracking, to endow the model with the required low-level visual capabilities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors claim that they propose training an LM end-to-end on low-level surrogate tasks, including object detection, re-identification, and tracking, to endow the model with the required low-level visual capabilities."
                },
                "weaknesses": {
                    "value": "1. In the experiments, the authors primarily focus on conducting investigations using synthetic datasets, particularly the ACRE dataset. However, it raises concerns about the generalizability of the conclusions/findings obtained from synthetic datasets to real-world datasets.\n\n2. The experimental results primarily focus on classical models. However, the generalizability of the conclusions/findings derived from these classical models to more powerful transformer-based models, such as the models mentioned in *Related Work* part, remains a concern."
                },
                "questions": {
                    "value": "Please refer to Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3946/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3946/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3946/Reviewer_Fa1W"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3946/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698667135205,
            "cdate": 1698667135205,
            "tmdate": 1700652124914,
            "mdate": 1700652124914,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FMhS54XIBS",
                "forum": "jhPvuc7kxB",
                "replyto": "HXmSYIy3u7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3946/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3946/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Fa1W"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their helpful comments and for recognizing that our proposed surrogate tasks such as object detection, re-identification, and tracking, endows our LRR model with the required low-level visual capabilities.\n\n**Synthetic datasets:**  We would like to emphasize that recent work has shown that even on synthetic datasets such as ACRE, state of the art LLMs such as GPT-4 do not perform well (Gendron et. al. 2023). Moreover, we include results on the challenging real-world Something-Else dataset in Section 4.2. We additionally include results on the challenging real-world STAR (Wu et. al. 2021) dataset below and in Appendix B in the paper. \n\nSpecifically, note that the results in Fig. 4 and Fig. 7 (Appendix) on Something-Else clearly show that our LRR model can deal with visually complex real-world scenarios. Our LRR model can deal with: severe occlusions, e.g., the silver ball in Fig. 4 bottom row is occluded by the hand; severe deformations, e.g., the piece of paper in Fig. 7 second row which is torn into two pieces; severe changes in appearance, e.g., the pen in Fig. 7 third row. It outperforms the state of the art STIN + OIE + NL model and the recently proposed Video-ChatGPT multi-modal LLM by a significant margin.\n\nWe additionally include results on the visually complex STAR dataset in Appendix B. The STAR benchmark (https://bobbywu.com/STAR/) consists of questions built upon real-world videos associated with human actions and interactions. Our LRR model with an OPT-350M LM backbone **achieves 66.7% accuracy** and outperforms prior state of the art models such as SeViLA (Yu et. al., 2023), Internvideo (Wang et.al. 2022) and BLIP-2 (Li et. al. 2023). Further details can be found in Appendix B, where we also include ablations of our LRR model (w/o Surrogate tasks) which highlights the advantage of using surrogate tasks during training.\n\n| Method | Int. | Seq. | Pre. | Fea. | Overall|\n| -------- | :-------: | :-------: | :-------: | :-------: | :-------: | \n| Internvideo | 62.7 | 65.6 | 54.9 | 51.9 | 58.7 |\n| BLIP-2 | 65.4 | 69.0 | 59.7 | 54.2 | 62.0 |\n| SeViLA | 63.7 | 70.4 | 63.1 | **62.4** | 64.9 | \n| LRR (ours) | **79.6** | **71.3** | **63.2** | 52.8 | **66.7** |\n\nNote that methods such as SeViLA are trained on a much larger set of image/video - text pairs and, use the 4.1B parameter BLIP as a video encoder and the 3B parameter Flan-T5 XL as an language model (c.f. Section 4.4 in Yu et al. (2023)). In contrast, our LRR model is trained on a much smaller dataset but one that includes low-level surrogate tasks to endow the model with the requisite low-level visual capabilities.\n\nFinally, in terms of visual complexity, CATER (especially the moving camera split) is complex and challenging as shown in Fig. 5. The results in Fig. 5 and Table 3 indicate that our LRR model can not only detect occlusions for tracking the snitch, but also understand camera motion based just on visual observations, e.g., motion of the background.\nTo conclude, the results on Something-Else, STAR, CATER and ACRE clearly show that our LRR model can deal with highly varied complex real-world visual inputs and reasoning tasks.  \n\n**Classical models:** We would like to emphasize that we include powerful transformer-based models in our evaluation. In Table 1, we include the recently proposed state of the art OpenFlamingo (Awadalla et al., 2023) multimodal LLM as a baseline. Furthermore, IV-CL (Sun et al., 2023) and ALOE (Ding et. al., 2021) in Table 1 are also large transformer based models. In Table 2, we include the recently proposed state of the art Video-ChatGPT (Maaz et al., 2023) multimodal LLM as a baseline. In Table 3, we compare to the large transformer based IV-CL (Sun et al., 2023) and ALOE (Ding et. al., 2021) models. In Table 5 (Appendix), we compare to SeViLA (Yu et. al., 2023), Internvideo (Wang et.al. 2022) and BLIP-2 (Li et. al. 2023), all of which are large multi-modal transformer based models. Note that, we are first to propose the use of language models for solving complex reasoning tasks such as causal and spatiotemporal compositional reasoning, while ensuring that we always compare to state of art models across all tasks. We will clarify further in the final version.\n\n**References**\n\n[1] STAR: A Benchmark for Situated Reasoning in Real-World Videos, Wu et. al. NeurIPS 2021.\n\n[2] Self-Chained Image-Language Model for Video Localization and Question Answering, Yu et. al. arXiv 2023.\n\n[3] Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, Li et. al. arXiv 2023.\n\n[4] Internvideo: General video foundation models via generative and discriminative learning, Wang et. al., arXiv 2023.\n\n[5] Large Language Models Are Not Abstract Reasoners, Gendron et. al. arXiv 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3946/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700159837750,
                "cdate": 1700159837750,
                "tmdate": 1700233103002,
                "mdate": 1700233103002,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8Efbr9kDE1",
            "forum": "jhPvuc7kxB",
            "replyto": "jhPvuc7kxB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3946/Reviewer_4932"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3946/Reviewer_4932"
            ],
            "content": {
                "summary": {
                    "value": "Thank you for submitting your manuscript. The proposed three-step process of Look, Remember, Reason for training a Language Model (LM) end-to-end on low-level surrogate tasks, which include object detection, re-identification, and tracking, is indeed novel."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The approach presented is intriguing and demonstrates significant performance improvements."
                },
                "weaknesses": {
                    "value": "1-Throughout the paper, there's a recurring mention of \"low-level surrogate tasks\". Could the authors elucidate the definition of these low-level tasks? Moreover, how do they differ from high-level tasks?\n\n2-The Look, Remember, Reason (LRR) model framework is innovative. However, there seems to be a gap in explicitly correlating this framework with the actual operations carried out in the method. The unique contributions of the \"Remember\" and \"Reason\" steps, in particular, are not clearly highlighted. It would be beneficial for the readers if the authors can provide a clearer mapping of these steps to their corresponding operations.\n\n3-Will the codebase for the presented method be made publicly available?\n\n4-Regarding the results of Video-ChatGPT on the Something-Else dataset: Were these results replicated by the authors? I couldn't find a direct reference to such results in the original Video-ChatGPT paper."
                },
                "questions": {
                    "value": "see Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3946/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773309285,
            "cdate": 1698773309285,
            "tmdate": 1699636355364,
            "mdate": 1699636355364,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sdKNbaqEEo",
                "forum": "jhPvuc7kxB",
                "replyto": "8Efbr9kDE1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3946/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3946/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4932"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful comments and for recognizing that our Look, Remember, Reason (LRR) model framework is innovative. We also thank the reviewer for recognizing that our LRR framework is intriguing and demonstrates significant performance improvements.\n\n**Low-level surrogate tasks:** We consider tasks like object recognition, tracking and re-identification as low-level tasks. These are computer vision tasks which play a crucial role in solving high-level reasoning tasks, like those found in ACRE, CATER, Something-Else and STAR. For example, in the case of Something-Else, the ability to recognize the performed action in Fig. 4 (top) rests upon grounding our LRR model to the motion and interaction of the hand and the stapler. This makes object recognition and tracking constituent low-level capabilities for recognizing the (high-level) compositional action. We have updated the text in Section 3.4 to make this clearer. We would be glad to receive further feedback to help improve the text further for the final version. \n\n**\"Remember\" and \"Reason\" steps:** We apologize for any issues with clarity. As mentioned in Section 3.3, in the \u201cremember\u201d step our LRR model remembers low-level visual information by storing it within the context window of the LLM backbone. In the \u201creason\u201d step, our LRR model aggregates this low-level visual information to arrive at the final answer (see also Fig. 1). This aggregation happens implicitly through the attention mechanism of the backbone LLM model. We have improved the clarity of the text in Section 1, Section 3.3 and 3.4 in the updated pdf. The updated text is in blue. We would be glad to receive additional feedback to help improve the text further for the final version.\n\n**Codebase:** Code will be released, pending legal review.\n\n**Video-ChatGPT:** The Video-ChatGPT model was tested by ourselves. We have updated Table 2 to indicate this. We use the publicly available codebase (https://github.com/mbzuai-oryx/Video-ChatGPT). We obtained the best results by finetuning the Video-ChatGPT model based on the information in the publicly available codebase and the paper (see Maaz et. al. 2023). We have updated the text in Section 4.2 (in blue).\n\n**References**\n\n[1] Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models, Maaz et. al. 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3946/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700159721698,
                "cdate": 1700159721698,
                "tmdate": 1700683397287,
                "mdate": 1700683397287,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xk7qQ5sIbi",
            "forum": "jhPvuc7kxB",
            "replyto": "jhPvuc7kxB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3946/Reviewer_5Fo1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3946/Reviewer_5Fo1"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a Look, Remember, Reason (LRR) framework to enable language models to perform visual reasoning in videos. The proposed LRR framework uses a two-stream video encoder to extract dense spatiotemporal features from video frames capturing structural and motion cues. The language model backbone has cross-attention layers inserted between its self-attention layers to enable top-down attention over the visual features. This allows the model to extract relevant visual information based on the reasoning task. LRR is trained end-to-end using surrogate tasks like object detection, re-identification and tracking. These provide supervision to teach the model the required low-level visual skills.\n\nThe authors also demonstrate that training LRR jointly on multiple datasets leads to a \"generalist\" model that performs competitively compared to task-specific \"specialist\" models. In the experimental results, the authors demonstrate that the LRR models significantly outperform prior state-of-the-art on challenging visual reasoning tasks from the ACRE, Something-Else, and CATER datasets, showing the benefit of the proposed grounded reasoning approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Demonstrates strong performance on multiple challenging visual reasoning datasets by grounding the language model in low-level visual details.\n+ Good demonstration of using surrogate tasks and end to end training"
                },
                "weaknesses": {
                    "value": "- The datasets used are rather simple with low visual complexity, such as CATER."
                },
                "questions": {
                    "value": "1) Could you comment on the nature of surrogate tasks? Are there some tasks that are more suited for reasoning vs others. Do low level recognition tasks (choice in the paper) work better.\n2) Is there evidence that LRR is not ovefitting to these simplistic datasets due to surrogate tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3946/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3946/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3946/Reviewer_5Fo1"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3946/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699109111116,
            "cdate": 1699109111116,
            "tmdate": 1700681256760,
            "mdate": 1700681256760,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sOKGb0CkPR",
                "forum": "jhPvuc7kxB",
                "replyto": "xk7qQ5sIbi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3946/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3946/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5Fo1 [1/2]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful comments and for recognizing that our LRR model demonstrates strong performance on multiple challenging visual reasoning datasets by grounding the language model in low-level visual details.\n\n**Low visual complexity:** We would like to emphasize that recent work has shown that even on synthetic datasets such as ACRE, state of the art LLMs such as GPT-4 do not perform well (Gendron et. al. 2023). Moreover, note that we include results on the challenging real-world Something-Else dataset in Section 4.2. We additionally include results on the challenging STAR (Wu et. al. 2021) dataset below and in Appendix B in the paper.\n\nSpecifically, note that the results in Fig. 4 and Fig. 7 (Appendix) on Something-Else clearly show that our LRR model can deal with visually complex real-world scenarios. Our LRR model can deal with: severe occlusions, e.g., the silver ball in Fig. 4 bottom row is occluded by the hand; severe deformations, e.g., the piece of paper in Fig. 7 second row which is torn into two pieces; severe changes in appearance, e.g., the pen in Fig. 7 third row. It outperforms the state of the art STIN + OIE + NL model and the recently proposed Video-ChatGPT multi-modal LLM by a significant margin.\n\nWe additionally include results on the visually complex STAR dataset in Appendix B. The STAR benchmark (https://bobbywu.com/STAR/) consists of questions built upon real-world videos associated with human actions and interactions. Our LRR model with an OPT-350M LM backbone **achieves 66.7% accuracy** and outperforms prior state of the art models such as SeViLA (Yu et. al., 2023), Internvideo (Wang et.al. 2022) and BLIP-2 (Li et. al. 2023). Further details can be found in Appendix B, where we also include ablations of our LRR model (w/o Surrogate tasks) which highlights the advantage of using surrogate tasks during training.\n\n| Method | Int. | Seq. | Pre. | Fea. | Overall|\n| -------- | :-------: | :-------: | :-------: | :-------: | :-------: | \n| Internvideo | 62.7 | 65.6 | 54.9 | 51.9 | 58.7 |\n| BLIP-2 | 65.4 | 69.0 | 59.7 | 54.2 | 62.0 |\n| SeViLA | 63.7 | 70.4 | 63.1 | **62.4** | 64.9 | \n| LRR (ours) | **79.6** | **71.3** | **63.2** | 52.8 | **66.7** |\n\nNote that methods such as SeViLA are trained on a much larger set of image/video - text pairs and, use the 4.1B parameter BLIP as a video encoder and the 3B parameter Flan-T5 XL as an language model (c.f. Section 4.4 in Yu et al. (2023)). In contrast, our LRR model is trained on a much smaller dataset but one that includes low-level surrogate tasks to endow the model with the requisite low-level visual capabilities.\n\nFinally, we would like to emphasize that CATER, especially the moving camera split, is visually complex and challenging as shown in Fig. 5. The results in Fig. 5 and Table 3 indicate that our LRR model can not only detect occlusions for tracking the snitch, but also understand camera motion based just on visual observations, e.g., motion of the background.\nTo conclude, the results on Something-Else, STAR and CATER clearly show that our LRR model can deal with highly varied complex real-world visual inputs and reasoning tasks.  \n\n**Nature of surrogate tasks:** We consider tasks like object recognition, tracking and re-identification as low-level tasks. These are computer vision tasks which play a crucial role in solving high-level reasoning tasks, like those found in ACRE, CATER, Something-Else and STAR. \n\nSurrogate tasks that are designed to be as simple as possible, while ensuring that our LRR model predictions are grounded in fine-grained low-level details, as discussed in Section 1 and Section 3.4. In the case of spatio-temporal reasoning tasks, e.g., CATER, Something-Else and STAR, model predictions need to be grounded to object motion, actions and interactions. Therefore, we choose the simplest surrogate tasks possible, which are object detection and tracking. Both surrogate tasks are crucial for good performance. To illustrate this, we perform an ablation on the compositional split of the Something-Else dataset where we include only the detection surrogate task (w/o Tracking),\n\n| Method | Top-1 | Top-5 |\n| -------- | :-------: | :-------: | \n| LRR (w/o Tracking) | 56.4  | 81.5 | \n| LRR (Ours) | 62.0 | 86.3| \n\nThe results validate the choice of our surrogate tasks and show that removing any of them would have a negative impact on performance. It is possible that additional surrogate tasks could further improve performance and is an interesting direction of future research. We have improved the clarity of Section 3, in particular Section 3.4 (changes in blue), for an improved explanation of surrogate tasks. We look forward to receiving further feedback to help improve clarity."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3946/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700159412515,
                "cdate": 1700159412515,
                "tmdate": 1700233054607,
                "mdate": 1700233054607,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6qqhm6dYDN",
                "forum": "jhPvuc7kxB",
                "replyto": "xk7qQ5sIbi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3946/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3946/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5Fo1 [2/2]"
                    },
                    "comment": {
                        "value": "**Overfitting:** Thank you for your insightful observation. There is indeed evidence that our surrogate tasks prevent our LRR model from overfitting on datasets such as CATER. Here we report training and test accuracy of our LRR model trained with and without surrogate tasks (see also Table 3) on the moving camera split of CATER.\n\n| Method | Train | Test |\n| -------- | :-------: | :-------: | \n| LRR (w/o Surrogate tasks)| 99.4  | 62.7 | \n| LRR (Ours) | 89.7 | 80.4 | \n\nSimilarly, on the compositional split of the Something-Else dataset (see also Table 2),\n\n| Method | Train | Test |\n| -------- | :-------: | :-------: | \n| LRR (w/o Surrogate tasks) | 92.6  |  50.1 | \n| LRR (Ours) | 87.3 | 62.0 |\n\nThe results show that our surrogate tasks clearly prevent overfitting as the model is grounded to the fine-grained low-level details and is thus able to better \u201cunderstand\u201d the task at hand. We have added these results to Appendix C and we will discuss this in more detail in the final version.\n\n**References**\n\n[1] STAR: A Benchmark for Situated Reasoning in Real-World Videos, Wu et. al. NeurIPS 2021.\n\n[2] Self-Chained Image-Language Model for Video Localization and Question Answering, Yu et. al. arXiv 2023.\n\n[3] Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, Li et. al. arXiv 2023.\n\n[4] Internvideo: General video foundation models via generative and discriminative learning, Wang et. al., arXiv 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3946/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700159478278,
                "cdate": 1700159478278,
                "tmdate": 1700677694917,
                "mdate": 1700677694917,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e9pEHuksZ6",
                "forum": "jhPvuc7kxB",
                "replyto": "K70fkiXTpw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3946/Reviewer_5Fo1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3946/Reviewer_5Fo1"
                ],
                "content": {
                    "title": {
                        "value": "New results"
                    },
                    "comment": {
                        "value": "Thanks for providing the new results, and a author response. I've updated my score in view of your response."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3946/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681330385,
                "cdate": 1700681330385,
                "tmdate": 1700681330385,
                "mdate": 1700681330385,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VUlU5E7go7",
            "forum": "jhPvuc7kxB",
            "replyto": "jhPvuc7kxB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3946/Reviewer_tQdf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3946/Reviewer_tQdf"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a Look, Rember, and Reason (LRR) framework to solve video reasoning task. The structure utilized LM to extract visual information by using surrogate grouding tasks and then integrated the grouding information to arrive at a final answer. The authors propose a two-stream vide encoder to capture sccene structure and object motions to learn low-level skills. Experiments on two sythetic dataset and one real-world datset shows the effectiveness of proposed method on complex spatialtemporal and causal reasoning tasks in videos."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed LRR structure make use of language models to solve surrage groudning task to benefit final video reasoning task. This structural design better utilize the low-level visual skills and information from videos.\n\n2. This paper conduct experiments on two synthetic datasets and one real-world dataset. The proposed methods achieve competative performance three datasets and outperforms other exsisting baseline models, showing the effectiveness of proposed method.\n\n3. Table 2 and 3 also show the performance of LRR without surrogate tasks and two-stream encoder. The ablation study shows the significance of the two proposed components for the overall structure."
                },
                "weaknesses": {
                    "value": "1. Writing, section 3 and Figure 2 is a little unclear and hard to follow.\n2. For different surrogate tasks, where do the ground-truth answers such as localization or box come from?"
                },
                "questions": {
                    "value": "See section weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3946/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699240588732,
            "cdate": 1699240588732,
            "tmdate": 1699636355194,
            "mdate": 1699636355194,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lYty5hhxjf",
                "forum": "jhPvuc7kxB",
                "replyto": "VUlU5E7go7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3946/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3946/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tQdf"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful comments and for recognizing that our LRR model achieves competitive performance on three -- two synthetic datasets and one real-world -- datasets and that it outperforms other existing baseline models.\n\n\n**Writing, section 3 and Figure 2 is a little unclear and hard to follow:** We apologize for any issues with the clarity of the text in Section 3 and Figure 2. We have made extensive changes to Section 1 and 3 to improve clarity of the text, which we highlight in blue. In Section 1 we have improved the explanation of low-level surrogate tasks and, the \u201cremember\u201d and \u201creason\u201d steps of our LRR framework. In Section 3, we have improved the overview paragraph of LRR model, focusing on the role of surrogate tasks in our LRR framework. In Section 3.1, we have provided more details of the inputs to and outputs from our LRR model. In Section 3.2, we have clarified more details of our spatial and temporal attention components. In Section 3.3, we have clarified our top-down attention scheme. Finally, in Section 3.4 we have endeavored to define and explain our low-level surrogate tasks more clearly. We will continue to improve the manuscript and we would be glad to receive further and more specific feedback about parts of the text and figures to help improve it further for the final version. \n\n\n**For different surrogate tasks, where do the ground-truth answers such as localization or box come from?** We employ the ground-truth annotations, such as localization or bounding boxes, provided by the corresponding datasets, to train our LRR model end-to-end. As shown in Fig. 1 and Fig. 4, the Something-Else dataset provides bounding box annotations and tracks of hands and objects involved in the action (c.f. Section 4 in Materzynska et. al. 2020). These annotations are also used by the state of the art STIN + OIE + NL model (see Table 2). Similarly, for the ACRE dataset, the object class annotations and the state of the blicket machine are provided, and for the CATER dataset the tracks of the objects in the scene are provided. These annotations are also used by the state of art ALOE model (Ding et al. 2021; see Tables 1 and 3). Thus, we only use annotations provided with the target datasets, and these annotations are also used by the prior state of the art models, ensuring a fair comparison. In case such annotations are not available, due to the recent advances in perception models reaching human-level performance it is possible to use off-the-shelf vision models (Ren et al., 2017; Tang et al., 2017; Ye et al., 2022) as discussed in Section 3.4. It is worth noting that the annotations need to be used only at training time and not at test time, so the computational efficiency of models generating the annotations is not critical for any real-world use. Finally, in the case of STAR (see Appendix B in the updated paper PDF and response to all reviewers), we observe that training jointly on multiple tasks, some of which do and some of which don\u2019t provide annotations, the model is able to acquire the required low-level visual skills from those tasks that do provide them and transfer them to those that don\u2019t. This allows the jointly trained model to reach very high accuracy on the scarcely annotated STAR dataset."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3946/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700159214819,
                "cdate": 1700159214819,
                "tmdate": 1700294102701,
                "mdate": 1700294102701,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y4EDcGZH7o",
                "forum": "jhPvuc7kxB",
                "replyto": "lYty5hhxjf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3946/Reviewer_tQdf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3946/Reviewer_tQdf"
                ],
                "content": {
                    "comment": {
                        "value": "I'd like to thank the authors for the response and revision of the paper. I think these responses basically address my concerns. I'll make the decision on my final scores after reading other rebuttal materials and discussing with other reviewers"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3946/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682367385,
                "cdate": 1700682367385,
                "tmdate": 1700682367385,
                "mdate": 1700682367385,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]