[
    {
        "title": "A Lie Group Approach to Riemannian Normalization for SPD Neural Networks"
    },
    {
        "review": {
            "id": "etNNQWrrDG",
            "forum": "okYdj8Ysru",
            "replyto": "okYdj8Ysru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4053/Reviewer_R6Kt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4053/Reviewer_R6Kt"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a batch-normalization (BN) method for manifold-valued features in neural networks. While in prior works several BN techniques are proposed for specific types of manifold, the authors present a general manifold-based BN formulation from a viewpoint of Lie-group. Besides, especially for SPD manifolds, practical BN methods are derived from the general formulation based on three types of pull-back metrics [Chen+23].\nThe experimental results using SPDnet and TSMnet demonstrate that the propose methods exhibit competitive performance to SOTAs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ A general formulation of manifold-based BN is presented through reviewing/summarizing several Riemannian-normalization (RN) approaches.\n+ Practical BNs for SPD matrices are derived in an efficient form from the general formulation."
                },
                "weaknesses": {
                    "value": "Novelty of LieBN in Sec.4 is limited as it is rather straightforward from the prior works [Kobler+22a] and [Chakraborty+20].\n\nOn the other hand, the pull-back metrics [Chen+23ab] are effectively applied to the general formulation to instantiate practical BN methods for SPD manifolds in an interesting way.\nThough especially pull-back Euclidean metrics seem to be efficient as shown in Table 3, this paper lacks in-depth analysis about the methods from qualitative and/or computational viewpoint.\nIt is demanded to clarify computational details such as by showing back-props through comparison to the other RN approaches based on complicated manifold-based computation, which would significantly improve reproducibility.\n\nConsidering \\theta-parameterization does not work so well as shown in Sec.6, such a parametric extension might be redundant, rather complicating the discussion in Sec.4. In stead of that extension, it may be better to focus on analyzing the practical BN methods shown in Table 3.\n\nAs to empirical performance results reported in the experiments, superiority of the method is less clear since the performance improvement is not significant due to large stds of performance scores.\nTo clarify the efficacy of the proposed method, it should be compared with the other RN methods in terms of computation cost not only the classification performance.\n\nBased on the experimental results, one cannot identify the best SPD-BN method that outperforms the others consistently. Although the authors insist such an inconsistency shows generality of the approach, it is less understandable and unfavorable from a practical viewpoint. In this case, provide some discussion and/or analysis about connection between types of metrics and tasks (or network architectures) for rendering insights into the SPD metrics.\n\nMinor comments:\nIn Eq.5: $\\frac{1}{N} \\sum$ -> $\\sum$"
                },
                "questions": {
                    "value": "The above-mentioned concerns should be addressed especially in the following points.\n- Analysis about the SPD-BN methods in Table 3 from computational viewpoint in comparison to the other RNs.\n- Empirical comparison regarding computation cost."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4053/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698147529862,
            "cdate": 1698147529862,
            "tmdate": 1699636369176,
            "mdate": 1699636369176,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kwIBuvIlOA",
                "forum": "okYdj8Ysru",
                "replyto": "etNNQWrrDG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4053/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4053/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer R6Kt (R4)"
                    },
                    "comment": {
                        "value": "We thank Reviewer $\\textcolor{blue}{R6Kt}$ ($\\textcolor{blue}{R4}$) for the thoughtful comments. We respond to the concerns as follows.\n***\n\n**1. Difference with the prior work [Kobler+22a] and [Chakraborty+20].**\n\nThe theory and implementation of our LieBN are different from [Kobler+22a] and more general than [Chakraborty+20]. Please refer to CQ#1 in the common response.\n\n**2. Computational details on back-props**\n\nAs summarized in the following table, two types of matrix functions are involved in our LieBN on SPD manifolds: one is based on Eigendecomposition, and the other is the Cholesky decomposition. The Eigen-based matrix functions include matrix logarithm, exponential, and power. Their BP can be calculated by the  Daleckii-Krein formula [2 Thm. V.3.3]. The BP of the Cholesky decomposition has been well established in `torch.linalg.cholesky`. Therefore, we use `torch.linalg.cholesky` for the Cholesky decomposition. We have added a detailed discussion on matrix BP in the App. F in our revised manuscript.\n\nTable A: Matrix functions involved in LieBN on SPD manifolds\n|       Matrix Function      |          Type          |       Involvement       |\n|:--------------------------:|:----------------------:|:-----------------------:|\n|      Matrix Logarithm      |        Eigen-based       | LieBN-AIM and LieBN-LEM |\n|     Matrix Exponential     |        Eigen-based       | LieBN-AIM and LieBN-LEM |\n|     Matrix Square Root     |        Eigen-based       |        LieBN-AIM        |\n| Matrix Inverse Square Root |        Eigen-based       |        LieBN-AIM        |\n|        Matrix Power        |        Eigen-based       |        LieBN-AIM and power deformation        |\n|   Cholesky Decomposition   | Cholesky Decomposition |        LieBN-LCM        |\n\n**3. Large std on EEG**\n\nRecalling Tab. 4-5, it is only on the EEG datasets that the performance shows relatively large std, which should be attributed to the characteristics of the EEG dataset. EEG signals exhibit low signal-to-noise ratio (SNR), domain shifts, and low specificity. These challenges are more obvious in the inter-session and inter-subject scenarios. \n\nDespite the relatively large std, our LieBN outperforms the SPDDSMBN on the inter-subject classification by 3.87% (50.10 v.s. 53.97). Besides, our LieBN can show less std than the SPDDSMBN baseline on EEG datasets. For instance, for the inter-subject task (Tab. 5 (b)), the std of LCM-based LieBN is clearly less than SPDDSMBN ( 51.53\u00b14.96 v.s. 50.10\u00b18.08).\n\n**4.  Discussion and analysis about the choice of metrics.**\n\nThe foremost challenge when building Riemannian learning algorithms is choosing an appropriate Riemannian metric. If the framework is general, there would be many metrics to select from, and the model would be more likely to perform better. This is why we view the generality of our framework as an advantage.\n\nGenerally speaking, AIM is the best candidate metric on SPD manifolds. The most significant reason is the property of affine invariance, which is a natural characteristic of describing covariance matrices. In our experiments, the LieBN-AIM generally achieves the best performance. In some scenarios, there are better choices than AIM. As shown in Tab. 4 (b), the best result on the HDM05 dataset is achieved by LCM-based LieBN, which improves the vanilla SPDNet by 11.71%. Therefore, when choosing Riemannian metrics on SPD manifolds, a safe choice would start with AIM and extend to other metrics."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222355581,
                "cdate": 1700222355581,
                "tmdate": 1700222355581,
                "mdate": 1700222355581,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ykwusvE6S4",
                "forum": "okYdj8Ysru",
                "replyto": "7TFnDCMPtf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4053/Reviewer_R6Kt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4053/Reviewer_R6Kt"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response.\nIt clarifies my concern especially regarding the computation issues."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673253944,
                "cdate": 1700673253944,
                "tmdate": 1700673253944,
                "mdate": 1700673253944,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "l3ksoigxku",
            "forum": "okYdj8Ysru",
            "replyto": "okYdj8Ysru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4053/Reviewer_LaGM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4053/Reviewer_LaGM"
            ],
            "content": {
                "summary": {
                    "value": "The paper concerns batch normalization for Lie group valued data. The authors propose a unified framework for batch normalization that they claim offers theoretical guarantees."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I have a hard time finding strengths that were not already presented in previous papers. I hope the authors can argue to the opposite, but as of now I am not sure of what is the actual contribution of the paper."
                },
                "weaknesses": {
                    "value": "- I am unsure what is the contribution of the paper. As the authors state, the normalization scheme they propose has been used in previous work. There are some claims like \"In contrast, our work provides a more extensive examination, encompassing both population and sample properties of our LieBN in a general manner. Besides, all the discussion about our LieBN can be readily transferred to right-invariant metrics. \" but I was not able to find out what specifically these differences are. The approach seems to be almost exactly the same when I look up in the cited papers where it is applied to Lie groups as well.\n- using the Riemannian or Lie group exp and log maps for batch normalization was a good idea the first time it was presented, but I don't see the value added with the current paper"
                },
                "questions": {
                    "value": "I believe the authors need to argue convincingly what is the contribution of the paper, and why the paper presents a significant contribution relative to the previously methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4053/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698183698284,
            "cdate": 1698183698284,
            "tmdate": 1699636369098,
            "mdate": 1699636369098,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0Xrk7ElyMB",
                "forum": "okYdj8Ysru",
                "replyto": "l3ksoigxku",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4053/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4053/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LaGM (R3)"
                    },
                    "comment": {
                        "value": "We thank reviewer $\\textcolor{green}{LaGM}$ ($\\textcolor{green}{R3}$) for the valuable comment. In the following, we respond to the concerns in detail.\n***\n\n**1. Difference from the previous RBN methods.** \n\nThe RBNs in these three works are **_either designed for a specific manifold or metric_**, or fail to control mean and variance, while our LieBN fulfills normalization on general Lie groups. We have summarized their limitations in Tab. 2 and discussed the difference in Sec. 3.2. Here we clarify the difference between our LieBN and their RBN in details.\n\n**- Difference with the RBN proposed in [1]**\n\n**_The RBN in [1] is confined within the standard Affine-Invariant Metric (AIM)_**. In\u00a0contrast, we implement our LieBN under three invariant metrics: Log-Euclidean Metric (LEM), Log-Cholesky Metric (LCM), and AIM. Moreover, our LieBN further covers the deformed families of LEM, LCM, and AIM. \n\n**_For the specific AIM, our LieBN is also better than the RBN [1] in terms of computational efficiency._** The RBN in [1] is based on parallel transportation, Riemannian exp and log. The specific equation is calculated by the matrix power function (Eq. (10) in [1]). In contrast, our LieBN is based on Lie group left translation, which is fulfilled by Cholesky decomposition (Tab. 3). Note that Cholesky decomposition is more efficient than the matrix power function. As shown in Tab. 5 (b), DSMLieBN-AIM-(1) achieves similar performance to SPDDSMBN with less training time (6.94 v.s. 7.74).\n\n**_Besides, the RBN in [1] is problematic when extending to other manifolds or metrics._** As shown in Eq. (9), the core formulation of the RBN in [1] can be described as\n$$\n\\Gamma _{P \\rightarrow Q}(S)=\\operatorname{Exp} _Q\\left[\\mathrm{PT} _{P \\rightarrow Q}\\left(\\operatorname{Log} _P(S)\\right)\\right].\n$$\n\nSince $\\Gamma_{P \\rightarrow Q}$ becomes affine action under AIM, this formulation can coincidentally control mean under AIM on SPD manifolds. However, the resultant mean and variance are generally agnostic under general manifolds. Therefore, the RBN based on $\\Gamma _{P \\rightarrow Q}$ could not be extended into other metrics or manifolds. On the contrary, our Props. 4.1-4.2 guarantee the rationality of our LieBN on other Lie groups, such as SO(n). For experiments on SO(n), please refer to the 3rd response to $\\textcolor{red}{R1}$.\n\n**- Difference with the RBN proposed by [3]**\n\n**_Similar to [1], the RBN in [3] generally fails to normalize mean or variance either_,** as this RBN is a variant of $\\Gamma_{P \\rightarrow Q}$. In contrast, our LieBN can always control both mean and variance on Lie groups.\n\n**- Difference with the RBN proposed by [4]**\n\n**_The RBN on matrix Lie groups in [4] is also confined within a specific metric and group structures (Sec. 3.2 in [4])_.** On the contrary, our LieBN is designed for general Lie groups. In this sense, the RBN on matrix Lie groups in [4] is a special case of our LieBN. Furthermore, as stated at the end of Sec. 4, our LieBN framework can be extended into right-invariant metrics.\n\nBesides, as stated in Rmk. C.5 in the appendix, the proof presented in [4] is a bit problematic, as the author fails to consider Riemannian volume when dealing with the probability density function. In contrast, our theoretical results in Props. 4.1-4.2 are more solid and general.\n\n**- Summary**\n\nIn summary, the RBNs in [1-3] are either confined within a specific manifold or metric, or fail to control mean and variance. In contrast, we propose a  framework of batch normalization over general Lie groups and showcase its generality over three families of Lie groups on SPD manifolds.\n\n> [1] Kobler R, Hirayama J, Zhao Q, et al. SPD domain-specific batch normalization to crack interpretable unsupervised domain adaptation in EEG.\n> \n> [2] Brooks D, Schwander O, Barbaresco F, et al. Riemannian batch normalization for SPD neural networks. \n> \n> [3] Lou A, Katsman I, Jiang Q, et al. Differentiating through the fr\u00e9chet mean.\n> \n> [4] Chakraborty R. Manifoldnorm: Extending normalizations on Riemannian manifolds."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222243090,
                "cdate": 1700222243090,
                "tmdate": 1700523209276,
                "mdate": 1700523209276,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S4ZfDIiKOF",
                "forum": "okYdj8Ysru",
                "replyto": "0Xrk7ElyMB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4053/Reviewer_LaGM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4053/Reviewer_LaGM"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for the response. The response has not changed my rating."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647171489,
                "cdate": 1700647171489,
                "tmdate": 1700647171489,
                "mdate": 1700647171489,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0rmf6062pD",
            "forum": "okYdj8Ysru",
            "replyto": "okYdj8Ysru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4053/Reviewer_B3vJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4053/Reviewer_B3vJ"
            ],
            "content": {
                "summary": {
                    "value": "Study of Deep Neural Networks (DNNs) on manifolds, associated with normalization techniques, with a unified framework for Riemannian Batch Normalization (RBN) techniques on Lie groups. Theoretical guarantee are provided to caracterize the stability of the process. \nApproach is illustrated for Symmetric Positive Definite (SPD) manifolds, with three families of parameterized Lie groups, in a SPD neural networks.  Experiments have been done for radar recognition, human action recognition, and electroencephalography (EEG) classification."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Interesting algorithm LieBN, which enables batch normalization over Lie groups, to normalize both the sample and population statistics.and apply to SPD manifolds."
                },
                "weaknesses": {
                    "value": "Density of probability on SPD matrix could be only defined as invariant to all the automorphisms of SPD manifold. To assess which density verify this property, you have to consider \"Lie Groups Thermodynamics\" developped by Jean-Marie Souriau. Consider upper-half space of Siegel (pure imaginary axis is the space of SPD matrix) where the Lie group SU(n,n) acts transitivelly. With Souriau method, you are able to compute the Gibbs density of maximum entropy that is covariant to SU(n,n). If you restrict to the imaginary axis, you find the density for SPD matrices. See the following reference and put it in your references:\n[A] Barbaresco, F. (2021). Gaussian Distributions on the Space of Symmetric Positive Definite Matrices from Souriau\u2019s Gibbs State for Siegel Domains by Coadjoint Orbit and Moment Map. In: Nielsen, F., Barbaresco, F. (eds) Geometric Science of Information. GSI 2021. Lecture Notes in Computer Science(), vol 12829. Springer, Cham. https://doi.org/10.1007/978-3-030-80209-7_28"
                },
                "questions": {
                    "value": "Add the following references on batch normalization\n[B] Daniel Brooks. Deep Learning and Information Geometry for Time-Series Classification. Machine Learning [cs.LG]. Sorbonne Universit\u00e9, 2020. English. \u27e8NNT : 2020SORUS276\u27e9. \u27e8tel-03984879\u27e9; https://theses.hal.science/tel-03984879\n[C] D. Brooks, O. Schwander, F. Barbaresco, J. . -Y. Schneider and M. Cord, \"Deep Learning and Information Geometry for Drone Micro-Doppler Radar Classification,\" 2020 IEEE Radar Conference (RadarConf20), Florence, Italy, 2020, pp. 1-6, doi: 10.1109/RadarConf2043947.2020.9266689.\n[D] D. Brooks, O. Schwander, F. Barbaresco, J. -Y. Schneider and M. Cord, \"A Hermitian Positive Definite neural network for micro-Doppler complex covariance processing,\" 2019 International Radar Conference (RADAR), Toulon, France, 2019, pp. 1-6, doi: 10.1109/RADAR41533.2019.171277.\n[E] D. A. Brooks, O. Schwander, F. Barbaresco, J. -Y. Schneider and M. Cord, \"Complex-valued neural networks for fully-temporal micro-Doppler classification,\" 2019 20th International Radar Symposium (IRS), Ulm, Germany, 2019, pp. 1-10, doi: 10.23919/IRS.2019.8768161.\n[F] D. A. Brooks, O. Schwander, F. Barbaresco, J. -Y. Schneider and M. Cord, \"Exploring Complex Time-series Representations for Riemannian Machine Learning of Radar Data,\" ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, UK, 2019, pp. 3672-3676, doi: 10.1109/ICASSP.2019.8683056.\n[G] Brooks, D., Schwander, O., Barbaresco, F., Schneider, JY., Cord, M. (2019). Second-Order Networks in PyTorch. In: Nielsen, F., Barbaresco, F. (eds) Geometric Science of Information. GSI 2019. Lecture Notes in Computer Science(), vol 11712. Springer, Cham. https://doi.org/10.1007/978-3-030-26980-7_78"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I have no conflict of interest with authors."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4053/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4053/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4053/Reviewer_B3vJ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4053/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780140757,
            "cdate": 1698780140757,
            "tmdate": 1699636369026,
            "mdate": 1699636369026,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xRxOYYWX1E",
                "forum": "okYdj8Ysru",
                "replyto": "0rmf6062pD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4053/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4053/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer B3vJ (R2)"
                    },
                    "comment": {
                        "value": "We thank reviewer $\\textcolor{brown}{B3vJ}$ ($\\textcolor{brown}{R2}$) for the encouraging feedback and constructive comments! The following is our detailed response.\n***\n\n**1. Gaussian distribution by Lie groups thermodynamics.**\n\nThanks for the valuable comment. The Gaussian distribution you mentioned on the SPD manifold is very inspiring. We have added this reference in Sec. 4, where we introduce Gaussian distribution on manifolds. In our future research, we will explore this distribution and attempt to establish statistics learning methods based on this Gaussian distribution.\n\n**2. Adding some reference on Riemannian batch normalization.**\n\nWe have added the reference you mentioned in the main paper. These papers are very interesting, especially HPDNet on Hermitant Positive Definite (HPD) manifolds proposed in [1]. As HPD manifolds are the counterparts of SPD manifolds in the complex domain, HPD manifolds also have Lie group structures. Theoretically, our LieBN framework could also be implemented on HPD neural networks. We will explore these possibilities in the future. \n\n> [1] Brooks, D., Schwander, O., Barbaresco, F., Schneider, J. Y., & Cord, M. A Hermitian Positive Definite neural network for micro-Doppler complex covariance processing."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222152019,
                "cdate": 1700222152019,
                "tmdate": 1700222152019,
                "mdate": 1700222152019,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pJPAT2ChMv",
            "forum": "okYdj8Ysru",
            "replyto": "okYdj8Ysru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4053/Reviewer_ZRPd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4053/Reviewer_ZRPd"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a batch normalization layer for neural networks on Lie groups. The authors then focus on SPD neural networks to showcase their approach. The proposed method is validated on radar recognition, action recognition, and electroencephalography (EEG) classification."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* Proofs are given in the supplementary material (I did not thoroughly check them)\n* Experiment results show improvements over some state-of-the-art SPD neural networks"
                },
                "weaknesses": {
                    "value": "* The paper lacks of novelty\n* Experimental results are not convincing\n* No discussion about the limitations of the proposed approach"
                },
                "questions": {
                    "value": "The proposed technique is a simple tweak of those from Kobler et al. (2022b), Lou et al. (2020), Chakraborty (2020). \nNo new concepts or ideas have been developped w.r.t. these works. While the authors state that the proposed technique works for Lie groups and is able to control mean and variance in contrast to these works, extensions of these works to Lie groups, as done in the paper, are trivial.\n\nThe experimental results are not convincing since the proposed method is only compared with some SPD neural networks. For example, on human action recognition, the proposed method is outperformed by the method of Laraba et al. (2017) on HDM05 dataset by a large margin (72.27\\% vs. 83.33\\%). This shows that the proposed technique is probably not effective compared to other learning techniques designed in Euclidean space. \n\n*Question*\n\nHow does the proposed method perform on another Lie groups, e.g. when being used in LieNet (Huang et al., 2017) ?\n\n*References*\n\n1. Sohaib Laraba, Mohammed Brahimi, Jo\u00eblle Tilmanne, Thierry Dutoit: 3D skeleton-based action recognition by representing motion capture sequences as 2D-RGB images. Comput. Animat. Virtual Worlds 28(3-4) (2017)\n\n2. Zhiwu Huang, Chengde Wan, Thomas Probst, Luc Van Gool: Deep Learning on Lie Groups for Skeleton-Based Action Recognition. CVPR 2017: 1243-1252."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4053/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4053/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4053/Reviewer_ZRPd"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4053/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822011366,
            "cdate": 1698822011366,
            "tmdate": 1699636368935,
            "mdate": 1699636368935,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xFx1Mbdg9l",
                "forum": "okYdj8Ysru",
                "replyto": "pJPAT2ChMv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4053/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4053/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZRPd (R1)"
                    },
                    "comment": {
                        "value": "We thank reviewer $\\textcolor{red}{ZRPd}$ ($\\textcolor{red}{R1}$) for the valuable feedbacks. Below is our detailed response.\n***\n\n**1. The proposed technique is not a simple tweak of those from Kobler et al. (2022b), Lou et al. (2020), and Chakraborty (2020).**\n\nAs stated in CQ#1 of the common response, our work is entirely theoretically different from Kobler et al. (2022b) and Lou et al. (2020), and more general than Chakraborty (2020). Please refer to the common response for details.\n\n**2. The experimental results are convincing, given the only modification to SPDNet is the BN layer.**\n\nThis paper mainly focuses on building LieBN on general Lie groups. **_Since we only change the model by adding a single BN layer, this slight modification is not expected to outperform every baseline._** Nevertheless, our experiments on the SPD manifolds indicate that our LieBN can improve the performance of SPD neural networks. **On the Radar, HDM05, FPHA, and EEG datasets, our LieBN improves the SPD baselines by 2.22%, 11.71%, 4.8% and 3.87%, respectively**. This is convincing to claim that SPD neural networks can benefit from our LieBN. \n\n**_Besides, SPDDSMBN is one of the SOTA methods in EEG classification._** Tab. 5 indicates the superiority of our work against SPDDSMBN. Moreover, Tab. 5 shows that LCM- or LEM-based LieBN can achieve comparable performance against SPDDSMBN with less training time. For instance, for inter-subject classification, LCM-based LieBN reaches similar results to SPDDSMBN while costing only half of the training time. Besides, LCM-based LieBN tends to show smaller std than SPDDSMBN (4.96 v.s. 8.08 for the inter-subject task)\n\n**3 LieBN on LieNet (Huang et al., 2017)**\n\nThe Lie group in LieNet [1] is SO(3). Our LieBN can also be implemented in this Lie group. However, we only implement centering and biasing operations for now due to limited time. \n\n**Theoretical ingredient**\n\nSince we only implement the centering and biasing of LieBN, the core operation of LieBN becomes\n$$\nL _{B} \\circ L _{M _{\\odot} ^{-1}}(P _i) = B M ^{-1} P _i\n$$\nwhere $\\{P _i\\}$ is a batch of SO(3), $M$ is the SO(3) batch mean\uff0cand $B$ is an SO(3) biasing parameters. The batch mean can be obtained by [2, Alg. 1], while the running mean can be updated by the geodesic connecting the running mean and batch mean.  The calculation of batch mean [2, Alg. 1] requires Lie group exp & log on SO(3). We present all the necessary ingredients in Tab. 1. For more detail, please refer to [2-3].\n\n\nTable 1: Riemannian operators required in LieBN on SO(3)\n|       Operator      |          Expression          |     \n|:--------------------------:|:----------------------:|\n|Lie group exponentiation |$\\exp(V)$| \n|Lie group logarithm |$\\log(V)$| )\\log(V)$|  \n|Geodesic connecting $S$ and $R$|$\\gamma (t;S,R)= S\\exp(t\\log(S ^\\top R))$|\n|Weighted Fr\u00e9chet mean| [2, Alg. 1]|\n\n**Implementation details and dataset**\n\nAs LieNet was originally implemented by Matlab, we use the open-sourced Pytorch code [4] to reimplement LieNet by torch. We found that the matrix logarithm in the Pytorch code [4] fails to deal with singular cases well. Hence, we use Pytorch3D [5] to calculate the matrix logarithm, i.e., `pytorch3d.transforms.matrix_to_axis_angle`. Besides, we implement matrix exponentiation by Rodrigues' formula [6, Eq. (2)].\n\nFollowing LieNet, we adopt the G3D-Gaming dataset [7], and follow all the learning settings as in the original paper of LieNet. We apply the LieBN layer before each pooling layer. As the RotMap layer in the original LieNet is actually based on left translation as well, we use the LieBN layer to substitute the RotMap layer. Following LieNet, we adopt the architecture of three pooling layers, which can be illustrated as\n$$\nf _{\\mathrm{LieBN}} \\rightarrow f _{\\mathrm{RotPooling}} \\rightarrow f _{\\mathrm{LieBN}} \\rightarrow f _{\\mathrm{RotPooling}} \\rightarrow f _{\\mathrm{LieBN}} \\rightarrow f _{\\mathrm{RotPooling}} \\rightarrow f _{\\log} \\rightarrow f _{\\mathrm{FC}}.\n$$\nwhere $f _{\\log}$ and $f _{\\mathrm{FC}}$ denote the matrix logarithm and FC layer. Similar to LieNet, we train the network by the standard cross-entropy loss.\n\n**Results**\n\nThe results are presented in the following table. Due to different software, our reimplemented LieNet is slightly worse than the performance reported in [1]. However, we still can observe a clear improvement of LieNetLieBN over LieNet.\n\nTable 2: Results on LieNet with or without LieBN\n|       Methods      |          Result          |     \n|:--------------------------:|:----------------------:|\n|LieNet |86.06| \n|LieNetLieBN |88.18|"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221957377,
                "cdate": 1700221957377,
                "tmdate": 1700222428493,
                "mdate": 1700222428493,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HzRJchjN2u",
                "forum": "okYdj8Ysru",
                "replyto": "pJPAT2ChMv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4053/Reviewer_ZRPd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4053/Reviewer_ZRPd"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the response. I still keep my original rating since I fail to see the significance of this work with respect to existing works."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662315163,
                "cdate": 1700662315163,
                "tmdate": 1700662315163,
                "mdate": 1700662315163,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ftv4p0eMPg",
            "forum": "okYdj8Ysru",
            "replyto": "okYdj8Ysru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4053/Reviewer_Urfr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4053/Reviewer_Urfr"
            ],
            "content": {
                "summary": {
                    "value": "The authors describe a new kind of Batch Normalization layer for\nRiemaniann neural networks. The proposed technique is a theoretical\nimprovement over existing batch-norm layers by being a generalized and\nunified view on all previously proposed technique, using the Lie-group\nstructure of the manifold."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- A very pleasant to read recap on all batch norm-like layers for\n  Riemannian networks.\n- Theoretical guaranties on the control provided by the layer.\n- Convincing experimental evaluation (not SOTA obviously, but an\n  improvement over other manifold methods)\n- LieBN reduces to the classical BN for Euclidean manifold."
                },
                "weaknesses": {
                    "value": "- Nearly all tables are barely readables.\n- Novelty of the work should emphasized more. LieBN provides more\n  guaranties and a more sound approach. But in the writing, it is not\n  completely clear of what is a full novelty over previous methods and\n  what is a generalization.\n- A broad zoo of choices (AIM, LEM, LCM and alpha beta variants), but no\n  clue on choosing. It's an usual question with this type of methods,\n  and it always a little bit disapointing to simply benchmark over all\n  the possible choices."
                },
                "questions": {
                    "value": "- What is the interest of the (alpha, beta) generalization ? In\n  particular in context of neural networks ? And what are the value used\n  in the experiments ?\n- In the article about RBN, Brooks et al discuss about the amount of\n  data need to achieve good performance. Any insight about this for your\n  layer ?\n- Is there a link between Frechet variance and the variance of the\n  Gaussian used for normalization ?\n\n- I guess it should be \"neutral element\" instead of identity in Eq 13 ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4053/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4053/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4053/Reviewer_Urfr"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4053/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700429937686,
            "cdate": 1700429937686,
            "tmdate": 1700429937686,
            "mdate": 1700429937686,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BDeJTneEAR",
                "forum": "okYdj8Ysru",
                "replyto": "Ftv4p0eMPg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4053/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4053/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Urfr (R5) (1/2)"
                    },
                    "comment": {
                        "value": "We thank reviewer $\\textcolor{purple}{Urfr}$ ($\\textcolor{purple}{R5}$) for the encouraging feedback and the constructive comments!\ud83d\ude04\ud83d\ude04 In the following, we respond to the concerns point by point.\n***\n\n**1. Table readability.**\n\nThanks for the suggestive comments. Due to the page limits, we presented the tables concisely. After our work gets accepted, we will move some experimental settings into the appendix and allocate more space for the tables (possibly allowing for two lines or a large figure).\n\n**2. The novelty of the work should emphasized more.**\n\nThanks for your constructive suggestion! We emphasize our novelty in the introduction in the revised paper (Page 2), highlighting our contributions that our work can theoretically normalize mean and variance and apply to diverse Lie groups.\n\n**3. How to choose hyper-parameters in LieBN.**\n\nOur SPD LieBN has at most three types of hyper-parameters: Riemannian metric, deformation factor $\\theta$, and $\\mathrm{O}(n)$-invariance parameters $(\\alpha,\\beta)$. The general order of importance should be Riemannian metric $>$ $\\theta$ $>$ $(\\alpha,\\beta)$.\n\nThe most significant parameter is the choice of the Riemannian metric, as all the geometric properties are sourced from the metric. A safe choice would start with AIM, and then decide whether to explore other metrics further. The most important reason is the property of affine invariance of AIM, which is a natural characteristic of covariance matrices. In our experiments, the LieBN-AIM generally achieves the best performance. However, AIM is not always the best metric. As shown in Tab. 4 (b), the best result on the HDM05 dataset is achieved by LCM-based LieBN, which improves the vanilla SPDNet by 11.71\\%. Therefore, when choosing Riemannian metrics on SPD manifolds, an appropriate choice would start with AIM and extend to other metrics. Besides, if efficiency is an important factor, one should first consider LCM as it is the most efficient one.\n\nThe second one is the deformation factor $\\theta$. As we discussed in Sec. 5.1, $\\theta$ interpolates between different types of metrics ($\\theta=1$ and $\\theta \\rightarrow0$). Inspired by this, we select $\\theta$ around its deformation boundaries (1 and 0). In this paper, we roughly select $\\theta$ from $\\{ \\pm 0.5, \\pm 1,\\pm 1.5 \\}$\n\nThe less important parameters are $(\\alpha,\\beta)$. Recalling Tab. 1, $(\\alpha, \\beta)$ only affects the Riemannian metric tensor and geodesic distance. For our specific SPD LieBN, they only affect the calculation of variance, which should have fewer effects than the above two parameters. Therefore, we simply set $(\\alpha,\\beta)=(1,0)$ during experiments.\n\nWe have added the above discussion to the App. G.\n\n**4. $\\mathrm{O}(n)$-invariance hyper-parameters $(\\alpha, \\beta)$**\n\nAs we stated in the last question above, $(\\alpha, \\beta)$ is less important than Riemannian metrics and deformation factor $\\theta$. Nevertheless, we also conduct experiments on the effect of $(\\alpha, \\beta)$ here. As $\\alpha$ is less important as a scaling factor [1], we set $\\alpha=1$ and change the value of $\\beta$. \n\nRecalling Eq. (3), $\\beta$ controls the relative importance of the trace part against the inner product. Therefore, we set the candidate values of $\\beta$ as $\\{1,\\frac{1}{n},\\frac{1}{n^2}, 0, -\\frac{1}{n} + \\epsilon,-\\frac{1}{n^2}\\}$, where $n$ is the input dimension of LieBN, and $\\epsilon$ is a small positive scalar to ensure $\\mathrm{O}(n)$-invariance, *i.e.,* $\\min (\\alpha, \\alpha+n \\beta)>0$. $\\frac{1}{n^2}$ and $\\frac{1}{n}$ means averaging the trace in Eq. (3), while the sign of $\\beta$ denotes suppressing (-), enhancing (+), or neutralizing (0) the trace. \n\nWe focus on AIM-based LieBN on the HDM05 dataset. We set $\\theta=1.5$, as it is the best deformation factor under this scenario. Other network settings remain the same as the main paper. The 10-fold average results are presented in the following table. Note that the dimension $n$ of the input feature of the LieBN layer is 30 in this setting. As expected, $\\beta$ has minor effects on our LieBN. We also add this discussion in App. G.1 in our revised paper.\n\nTable 1: The effect of different $\\beta$ for AIM-based LieBN on the HDM05 dataset.\n| $\\beta$ | -0.0011 | -0.03 | 0.0011 | 0.0333 | 1 | 0 |\n|:-------:|:-------:|:----:|:------:|:------:|:-:|:-:|\n| Mean\u00b1STD | 68.18\u00b10.86 | 68.12\u00b10.74 | 68.20\u00b10.85 | 68.18\u00b10.85 | 68.16\u00b10.80 | 68.16\u00b10.68 |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508534983,
                "cdate": 1700508534983,
                "tmdate": 1700508534983,
                "mdate": 1700508534983,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VopkwFGqoF",
                "forum": "okYdj8Ysru",
                "replyto": "Ftv4p0eMPg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4053/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4053/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Urfr (R5) (2/2)"
                    },
                    "comment": {
                        "value": "**5. Effect of the reduced amount of data.**\n\nFollowing [2], we use 10% of training data to train SPDNet with and without LieBN on the Radar dataset. Following the main paper, we set $\\theta=1, 1, -0.5$ and $(\\alpha,\\beta)=(1,0)$ for $(\\theta,\\alpha,\\beta)$-AIM, $(\\theta,\\alpha,\\beta)$-LEM, and $(\\theta)$-LCM. The 10-fold average results are presented in the table below. We could observe a clear improvement of our LieBN to SPDNet under the limited data availability scenarios.\n\nTable 2: Robustness to lack of data of SPDNet with and without LieBN on the Radar dataset.\n| SPDNet | SPDNetLieBN-AIM | SPDNetLieBN-LCM | SPDNetLieBN-LEM |\n|:------:|:---------------:|:--------------:|:--------------:|\n| 84.17 \u00b1 1.66 | **87.63 \u00b1 1.72** | 86.65 \u00b1 1.48 | 87.60 \u00b1 1.31 |\n\n\n**6. Links between the Fr\u00e9chet variance and the variance of the Gaussian.**\n\nThere are some theoretical links between the Fr\u00e9chet variance and the variance in the Gaussian distribution in Eq. (12).\n\nRecalling the Gaussian distribution (Eq. (12)) on manifold $\\mathcal{M}$\n$$\np\\left(X \\mid M, \\sigma^2\\right)=k(\\sigma) \\exp \\left(-\\frac{\\mathrm{d}(X, M)^2}{2 \\sigma^2}\\right).\n$$\nAs shown in [3, Props. 4.7], $\\mathrm{Var}(X)=\\mathbb{E} _{\\mathbf{X}}\\left[\\mathrm{d}^2(X, M)\\right]=\\sigma^2$. Therefore, $\\sigma^2$ is the population variance. The empirical counterpart of population variance is the Fr\u00e9chet variance [4, P12]. As shown by our Props. 4.2, our LieBN can control the sample variance.\n\nThe Maximum Likelihood Estimation (MLE) of $\\sigma^2$ does not have a general solution, as $k(\\sigma)$ depends on the specific metrics. However, the relation is very clear for the invariant metrics on the Lie groups of SPD manifolds. As shown in [5, Props. 7], under AIM, the Fr\u00e9chet variance $v ^2$ satisfying $\\sigma ^2=\\phi(v ^2)$ with $\\phi$ strictly increasing. For AIM, our LieBN implicitly controls the population variance through the explicit control of sample variance. For the LEM and LCM, our LieBN can directly transfer the latent Gaussian distribution:\n$$\n\\mathcal{N}(M,\\sigma^2) \\xrightarrow{\\text{Centering to }E} \\mathcal{N}(E,\\sigma^2) \\xrightarrow{\\text{\nScaling the variance}} \\mathcal{N}(E, s^2) \\xrightarrow{\\text{Biasing to }B} \\mathcal{N}(B, s^2).\n$$\nThis result is obtained by Cor. C.4 in App. C and has been discussed in detail in App. C (Page 17).\n\n**7. It should be \"neutral element\" instead of identity in Eq. (13).**\n\nThanks for pointing this out! All the \"identity\" or \"identity element\" in the paper means neutral element. We agree that the current usage of \"identity element\" and \"identity matrix\" might lead to potential confusion. In the original submission, we clarified at the beginning of Sec. 4 that an identity element may not necessarily be an identity matrix. Now we have changed the \"identity\" element to the \"neutral\" element throughout the manuscript for better clarity.\n\n> [1] Thanwerdas Y, Pennec X. Is affine-invariance well defined on SPD matrices? A principled continuum of metrics.\n> \n> [2] Brooks D, Schwander O, Barbaresco F, et al. Riemannian batch normalization for SPD neural networks.\n> \n> [3] Chakraborty R, Vemuri B C. Statistics on the Stiefel manifold: theory and applications.\n> \n> [4] Pennec X. Probabilities and statistics on Riemannian manifolds: A geometric approach.\n> \n> [5] Said S, Bombrun L, Berthoumieu Y, et al. Riemannian Gaussian distributions on the space of symmetric positive definite matrices."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508560469,
                "cdate": 1700508560469,
                "tmdate": 1700649428610,
                "mdate": 1700649428610,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Kn4ClVujuq",
                "forum": "okYdj8Ysru",
                "replyto": "Ftv4p0eMPg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4053/Reviewer_Urfr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4053/Reviewer_Urfr"
                ],
                "content": {
                    "comment": {
                        "value": "1) I am deeply disapointed by this answer: it's the job of authors to deal with the page limit. Promising to (re)move useful parts to the appendix if the paper is accepted is not acceptable.\n\n2) Nice, it's much better by now.\n\n3 and 4) Interesting and worth reading, thanks !"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666281431,
                "cdate": 1700666281431,
                "tmdate": 1700666395405,
                "mdate": 1700666395405,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DdvVloTRo9",
                "forum": "okYdj8Ysru",
                "replyto": "VopkwFGqoF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4053/Reviewer_Urfr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4053/Reviewer_Urfr"
                ],
                "content": {
                    "comment": {
                        "value": "5) Very interesting ! Thx.\n\n6) Ok.\n\n7) I thinks it's clearer with all these changes."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666379623,
                "cdate": 1700666379623,
                "tmdate": 1700666379623,
                "mdate": 1700666379623,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0AsoZtAAMa",
                "forum": "okYdj8Ysru",
                "replyto": "HkT99wYrnX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4053/Reviewer_Urfr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4053/Reviewer_Urfr"
                ],
                "content": {
                    "comment": {
                        "value": "Nice !\n\nFor table 2, it is good enough like that (I guess there better ways to present the content, but that is secondary by now).\n\nFor table 3, I really like this kind of summary, and although I would deserve a larger place, it is good enough."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676229549,
                "cdate": 1700676229549,
                "tmdate": 1700676229549,
                "mdate": 1700676229549,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]