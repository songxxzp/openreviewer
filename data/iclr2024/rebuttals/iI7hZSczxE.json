[
    {
        "title": "Disentangling Time Series Representations via Contrastive based $l$-Variational Inference"
    },
    {
        "review": {
            "id": "6yu8w5ZcGV",
            "forum": "iI7hZSczxE",
            "replyto": "iI7hZSczxE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7969/Reviewer_cDdv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7969/Reviewer_cDdv"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses the importance of learning disentangled representations for Time Series data, specifically in the context of home appliance electricity usage. The goal is to enable users to better understand and optimize their energy consumption, thereby reducing their carbon footprint. The authors frame the problem as one of disentangling the role of each appliance (e.g., dishwashers, fridges) in total electricity usage.\n\nUnlike existing methods that assume attributes (appliances in this case) operate independently, this work acknowledges that real-world time series data often show correlations between attributes. For instance, dishwashers and washing machines might be more likely to operate simultaneously during the winter season.\n\nTo address these challenges, the authors propose a method called DisCo (Disentangling via Contrastive), which employs weakly supervised contrastive disentanglement. This approach allows the model to generalize its representations across various correlated scenarios and even to new households. The method incorporates novel VAE layers equipped with self-attention mechanisms to effectively tackle temporal dependencies in the data.\n\nTo evaluate the quality of disentanglement, the authors introduce a new metric called TDS (Time Disentangling Score). The TDS proves to be a reliable measure for gauging the effectiveness of time series representation disentanglement, thereby making it a valuable tool for evaluation in this domain.\n\nOverall, the paper argues that disentangled representations, particularly those achieved using their DisCo method, can enhance the performance in tasks like reconstructing individual appliance electricity consumption."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The method is very sound with mathmaticaly correct derivations. \nThe addressed problem of disentagling latent factors in VAE type of models is very important.\nSpecifically, he paper addresses the unrealistic assumption of independence among generative attributes that is often present in traditional untangling methods. In contrast to these traditional approaches, DisCo focuses on recovering correlated data by encoding a wide range of possible combinations of generative attributes in the learned latent space.\n\nThe authors assert that simply encouraging pairwise factorized support in the latent space is sufficient for achieving effective disentanglement, even when data attributes are correlated. This is an important finding. \n\nIn terms of performance, DisCo is shown to be competitive with downstream task methods, exhibiting significant improvements of over +60% across a variety of benchmarks in three different datasets undergoing correlation shifts (Finding 5.1). This is a strong aspect of the work.\n\nAdditionally, the capability of DisCo to adapt across correlation shifts leads to better out-of-distribution generalization, especially when these shifts are more severe. This fulfills one of the key promises of learning disentangled representations, which is to improve the model's robustness and generalizability."
                },
                "weaknesses": {
                    "value": "I enjoyed the paper and did not find important weaknesses."
                },
                "questions": {
                    "value": "Please discuss how sensitive the method is to hyperparameter selection."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7969/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697576616693,
            "cdate": 1697576616693,
            "tmdate": 1699636980774,
            "mdate": 1699636980774,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZAMCaHOTIc",
                "forum": "iI7hZSczxE",
                "replyto": "6yu8w5ZcGV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7969/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7969/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review"
                    },
                    "comment": {
                        "value": "We appreciate your thorough review and insightful feedback on our paper. The suggestions and concerns raised have been duly noted, and we have made significant updates to address them, we will update the current version during this discussion. Major changes in the paper from the previous version will be highlighted in ***gray color***.\n\nThank you for acknowledging the strengths of our paper in tackling a challenging problem. We note that additional experiments were conducted during the review phase, and our code repos are updated with more ablation studies. This establishes itself as the inaugural framework for disentangling time series in a library.\n\n### Modifications \n\n- **Sensitivity Analysis:** We've integrated supplementary experimental findings and engaged in a thorough discussion covering all aspects of the training process. The forthcoming version will include corresponding adjustments to the codebase, ensuring transparency and reproducibility.\n\n- **Hyperparameter Selection Discussion:** We have included a detailed discussion on our parameter choices, addressing the sensitivity of our method to data and its correlations.\n\nWe believe these enhancements contribute significantly to the overall quality and completeness of our work. Your valuable input has been instrumental in refining our paper, and we are grateful for your time and expertise.\n\nShould you have any further comments or queries, please feel free to let us know. We look forward to your continued guidance.\n\nBest,\n\nAuthors,"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603829091,
                "cdate": 1700603829091,
                "tmdate": 1700684024150,
                "mdate": 1700684024150,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jnzJZdPgw3",
            "forum": "iI7hZSczxE",
            "replyto": "iI7hZSczxE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7969/Reviewer_jyEy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7969/Reviewer_jyEy"
            ],
            "content": {
                "summary": {
                    "value": "This study explores disentangled representations for time series data, with a primary emphasis on achieving representation generalization across diverse, interrelated scenarios. They focused on a specific application of electric load monitoring application where computing different household appliances contribution in a total load is the task. \nIn the context of Variational Autoencoders (VAE), this study draws inspiration from Roth et al., 2023, who addressed correlated attributes in an image processing context by replacing the independence constraint over attributes in the latent space (by a regularization term of the Kullback-Leibler divergence between the posterior of the latent attributes and a standard Guassian distribution), with the Hausdorff Factorized Support (HFS) assumption. The authors have adapted this idea for time series data and introduced the use of cosine similarity instead. Consequently, this approach no longer necessitates independent latent activations for different appliances. \nThe main idea is to address appliance correlations with weakly supervised contrastive disentanglement, promoting similarity for the same appliances and dissimilarity for absent appliances in latent representations. This is achieved through a loss function composed of two terms, one for alignment based on correlation and another to minimize redundancy between latent variables.\nIn addition, the authors proposed l-variational inference layers with self-attention mechanism to address temporal dependencies. Additionally they propose a metric of  Time Disentangling Score (TDS) to evaluate the  disentanglement performance in time series data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper presents several intriguing novelties.\n1) Using pairwise similarity rather than independence assumption in VAE, to consider the correlated representations. \n2) l-variational inference layers with self-attention mechanism\n3) A metric of  Time Disentangling Score (TDS) to evaluate the  disentanglement performance in time series data\n\n The authors have tackled a captivating problem, successfully adapting image processing techniques to the more complex domain of time series data."
                },
                "weaknesses": {
                    "value": "The paper needs some modifications to make it easier to read (some suggestions given in the Questions).\nThe experimental results are very abstract (some suggestions in Questions part)\nThe application worth more explanation, the description lacks either an illustration or it is abstract."
                },
                "questions": {
                    "value": "-Using cosine similarity instead of HFS needs more elaboration.\n\n-Section 3.2 would benefit from a dedicated illustration demonstrating ATTENTIVE l-VARIATIONAL AUTO-ENCODERS, along with the corresponding notations used in the text.\n\n-The authors have effectively presented the formulation for the usecase; however, in the experimental results, which I find somewhat abstract, there's a lack of a specific example illustrating how X and Y values for a time window are displayed, along with different rows of Y, etc.\n\n-In section 4.1, should be included how exactly augmentation is performed and how many, it is very abstract now. \n\n-In Section 2, specifically concerning contrastive learning, the evaluation of appliance dissimilarity in \"x\" and \"x-\" is not explicitly clarified. Is labeling used for this purpose? What if the appliances are not the exact same but should exhibit similar behavior? How are such cases addressed? Additionally, the preparation of negative and positive samples is not detailed. Have you considered ensuring that there are no common or similar appliances in these two sets, and if so, how was this determined? Providing further explanation or an illustration could enhance the clarity of data preparation, which is a crucial aspect of the methodology.\n-How many training examples did you use for linking?\n\u201cWe link the learned latent representation to ground-truth attributes using a limited number of pair labels\u201d\n-After equation 2, In this test, the latent variable is represented as \"z,\" which is defined as a matrix of dimensions (M + K) \u00d7 dz, where \"K\" and \"dz\" should be introduced and define."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7969/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698396421370,
            "cdate": 1698396421370,
            "tmdate": 1699636980661,
            "mdate": 1699636980661,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UB2cCH1Qhp",
                "forum": "iI7hZSczxE",
                "replyto": "jnzJZdPgw3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7969/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7969/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review - More additional experiments have been added with explanations."
                    },
                    "comment": {
                        "value": "We appreciate the thorough review and constructive feedback on our paper. The suggestions and concerns raised have been duly noted, and we have made significant updates to address them, we will update the current version during this discussion. Major changes in the paper from the previous version will be highlighted in ***gray color***.\n\nThank you for acknowledging the strengths of our paper in tackling a challenging problem. We emphasize these contributions more explicitly in the revised version.\n\nWe note that additional experiments were conducted during the review phase, and our code repos are updated with more ablation studies. This establishes itself as the inaugural framework for disentangling time series in a library.\n\n### Weaknesses and Modifications\n\nWe appreciate the feedback regarding the readability and abstract nature of the experimental results and application description. We have addressed these concerns by providing additional details and clarification in the revised version (which will be updated during this discussion). Specific modifications have been made based on the suggestions provided in the Questions section.\n\n#### **[A/Q]**\n\n1.   We have provided a more elaborate more explanation for Hausdaurff Factorization support (HFS) in the revised version.\n\n2. Section 3.2 is moved to 4.2 and now includes a dedicated illustration (Figure. 3) demonstrating Attentive l-Varitiaonla Autoencodeur along with corresponding notations.\n\n3. Experimental results section now includes specific examples illustrating how $\\textbf{x}$ and $\\textbf{y}$ values (see section 5, other results in the appendix) for a time window are displayed for each experiment, addressing concerns about abstractness we add the figure of mixed signal input and unmixed results (prediction).\n\n4. **Negative and Positive Samples.** We define a positive sample $\\textbf{x}$ as a mixed power state where an appliance \\(m\\) is activated (\\(y_m > 0\\)), and `\\(\\textbf{x}^{+}\\)` represents its augmentation. For this setup, labeled data is required. Finding a negative sample is comparatively straightforward, as any other sample $\\textbf{x}$ can be chosen where $m$ is not activated $y_{m}=0$, but at least one other appliance is activated (no need for their labels). Augmentations (see our answer below) of both positive and negative samples generate sets for positives and negatives for appliance $m$. This process is repeated for $M$ appliances as preparation data.\n\n5. Given access to labels for each appliance, we meticulously consider the correlation and similarity among appliances, assessing whether they exhibit similarities, correlations, or are uncorrelated. In response to your feedback, we elaborate on this aspect in the revised version and explicitly specify the number of samples utilized for training.\n\n6. **Time-based Augmentation*. In Section 5.1 experiments, we have provided detailed insights into the augmentation process to enhance clarity. Additional algorithmic details are available in the appendix. The four augmentations were sequentially applied to both negative and positive: 1). **Crop and delay**: crop and delayed in time by 5 steps with a probability of 0.5. 2). **Channel Masks powers**: Randomly mask out each power channel (reactive, active, and apparent) with a probability of 0.4. 3). **Cutout or Masking**: Time cutout of 5 steps with a probability of 0.8. 4). **Add Gaussian Noise**: Introduces random Gaussian noise to window activation  $y_{m}$ and $\\mathbf{x}_{m}$ with a standard deviation of 0.1. We add a comparison of performance between different choices of augmentation.\n\n6. Section.3 now provides explicit clarification on the evaluation of appliance dissimilarity in $\\mathbf{x}$ and $\\mathbf{x}^{+}$ addressing concerns about labeling and similar behavior. Further details on the preparation of negative and positive samples have been included, along with an illustration to enhance clarity.\n\n7. After Equation. 2, we have introduced and defined $K$ and $d_{z}$ fixed before training ($K$ unknowing appliance to find) in the revised manuscript.\n\n8. We favor Cosine Similarity over alternative distance measures based on specific considerations within our context. Our choice is motivated by the computational efficiency of Cosine Similarity compared to Earth Mover's Distance (EMD), as outlined in our study detailed in Appendix C. Notably, we observed that Cosine Similarity exhibits scale-invariance, focusing on direction rather than being affected by the magnitude of the latent. In contrast, EMD incorporates transportation costs between distributions, introducing a sensitivity to scale.\n\nWe value your insightful review and remain committed to addressing all raised concerns diligently. Should you have further inquiries about our paper or its underlying assumptions, we are more than happy to provide detailed responses.\n\nBest,\n\nAuthors,"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538828667,
                "cdate": 1700538828667,
                "tmdate": 1700676221231,
                "mdate": 1700676221231,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bNlRNlRNIB",
                "forum": "iI7hZSczxE",
                "replyto": "UB2cCH1Qhp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7969/Reviewer_jyEy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7969/Reviewer_jyEy"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for further clarifications and your proposed modifications can improve the paper readability. However, when I checked the manuscript it  still does not contain the modification you mentioned."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658354574,
                "cdate": 1700658354574,
                "tmdate": 1700658354574,
                "mdate": 1700658354574,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QeZu1uU8fJ",
                "forum": "iI7hZSczxE",
                "replyto": "jnzJZdPgw3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7969/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7969/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Sorry there was a slight problem of pdf upload. Could you please check the last pdf we uploaded.  Please if you have any further questions about our paper or its underlying assumptions, we will be happy to provide detailed answers.\n\nBest,\n\nAuthors,"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663901093,
                "cdate": 1700663901093,
                "tmdate": 1700671371904,
                "mdate": 1700671371904,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HPGIuqa3tS",
            "forum": "iI7hZSczxE",
            "replyto": "iI7hZSczxE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7969/Reviewer_a2rd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7969/Reviewer_a2rd"
            ],
            "content": {
                "summary": {
                    "value": "The paper seems to be an application of disentangled representation learning for home appliance electricity usage. The authors propose to combine contrastive and variational losses. Unfortunately, the paper falls somewhere between methodological novelty and application, making it difficult to understand where the main contributions of the paper will lie. In general, I found the paper very hard to read."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The main strength of the paper is its approach to tackling the important problem of disentangled representation learning, which may contribute to reducing carbon footprint."
                },
                "weaknesses": {
                    "value": "The paper lacks proper organization and has a tendency to include some unproven (or wrong) claims. For instance, in the introduction, the authors mention that \"Disentanglement via VAE **can be achieved** by a regularization term of the Kullback-Leibler divergence[],\" which is not necessarily true without certain strong assumptions and underlying conditions. The Beta-VAE paper has some qualitative evidence showing how the images are more disentangled compared to VAE. The authors also claim, \"Rather than training separate auto-encoders for individual appliances,\" which requires empirical validation with proper citations.\n\nIn addition to these issues, the notations used are very confusing and are not defined before they are referenced. For example, in the proposed method section, the notation $z_m^+$ is used without description it. It is also unclear how it differs from $\\bf{z}$ or $z$.\n\nThe main goal of this paper remains unclear to me. For example, the authors mentioned, \"The primary goals of this work are twofold: to effectively address the NILM problem and to obtain a disentangled representation of input data.\" However, it is unclear what the NILM problem is, what the nature of the input data is, and how the authors plan to achieve a disentangled representation that distinguishes itself from previous works. One issue might be that the problem statement and preliminaries are somewhat intertwined.\n\nThe color meaning used in the tables of result section is not clear. Even it is not clear how TDS (as a metric) has been compared with VAE and Beta-VAE in Table 1."
                },
                "questions": {
                    "value": "- I strongly suggest the authors make their main contributions clear at the end of the introduction. \n\n- There is no related work section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7969/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819937506,
            "cdate": 1698819937506,
            "tmdate": 1699636980532,
            "mdate": 1699636980532,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]