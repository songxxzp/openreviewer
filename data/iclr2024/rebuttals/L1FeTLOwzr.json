[
    {
        "title": "Dynamic Adapter Merging for Continual Video Question-Answering Learning"
    },
    {
        "review": {
            "id": "CG9s8dVv13",
            "forum": "L1FeTLOwzr",
            "replyto": "L1FeTLOwzr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission375/Reviewer_yoGd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission375/Reviewer_yoGd"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a dynamic adapter merging framework for domain-incremental VideoQA learning. The framework is capable of obtaining multiple domain-specific adapters and dynamically integrating different domain information through model merging techniques. Experiments results on multiple public datasets verify the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe logic of the paper is reasonable.\n2.\tThe experiments are relatively adequate."
                },
                "weaknesses": {
                    "value": "The technical details of this paper are not described clearly enough, my concerns are as follows:\n1.\tWhy do you set up N adapters for each domain instead of one?\n2.\tWhy do you choose to insert domain-specific adapters after the self-attention and feed-forward layers, respectively? What are the considerations?\n3.\tWhat exactly is meant by the pre-trained model f in Eqn. (1)?\n4.\tWhat does the symbol k in the baselines section on page 6 refer to? I cannot find a definition in the previous text.\n5.\tWhat is the exact structure of the adapter?"
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission375/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697695285405,
            "cdate": 1697695285405,
            "tmdate": 1699635964682,
            "mdate": 1699635964682,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VYtLX53g3D",
                "forum": "L1FeTLOwzr",
                "replyto": "CG9s8dVv13",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission375/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "We acknowledge the oversight in omitting specific details, including the adapter structure and design rationales, assuming a shared background with the readers. In response to your valuable feedback, we recognize the need to enhance accessibility for a broader audience. More implementation details will be meticulously incorporated into both the main paper and supplementary materials.\n\n---\n\n> **C4.1 Why do you set up N adapters for each domain? Why insert these adapters after the self-attention and feed-forward layers, respectively?**\n\nWe regret any confusion. Given that adapter design is not the primary focus of our work, we adhere to recent literature  [1,2,3] in incorporating multiple adapters (N) into the pretrained model for multimodal applications. These adapters are typically inserted after the self-attention and feed-forward layers in Transformer-based architectures. The primary objective of employing multiple adapters is to augment the model's capacity with minimal impact on computational cost and parameters. This design choice allows for the dynamic merging of expert models for each test instance with negligible overhead, as detailed in Section C3.3 of the rebuttal.\n\n---\n\n> **C4.2 What is the exact structure of the adapter?**\n\nFollowing previous works [1,2,3], the adapters in our approach consist of a downsampling and an upsampling linear layer, along with a residual connection. The linear layers are set with an 8x downsample scale to intermediate hidden size and the upsampler maps back to the original dimensionality [1]. \n\n[1]: Sung, Yi-Lin et al. \"Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks.\" CVPR 2022.  \n[2]: Houlsby, Neil, et al. \"Parameter-efficient transfer learning for NLP.\" ICML 2019.  \n[3]: Yang, Antoine, et al. \"Zero-shot video question answering via frozen bidirectional language models.\" NeurIPS 2022.\n\n---\n\n> **C4.3 What exactly is meant by the pre-trained model f in Eqn. (1)?**\n\nWe refer to the original pretrained model as \u201cf\u201d, i.e. the model used for zero-shot inference (FrozenBiLM in our case) except that we extract the hidden states from the 4th last layer as the output. We thank the reviewer\u2019s suggestion and will add a detailed description in the supplementary materials.\n\n---\n\n> **C4.4 What does the symbol k in the baselines section on page 6 refer to?**\n\nWe regret any confusion caused by the omission of the definition of the symbol 'k' in our manuscript. In our context, 'k' denotes that only the top-k models participate in the merging process (as described in Section 3.3). The rationale behind selecting only the top-k models is to filter out irrelevant adapters, while still capitalizing on the advantages of adapter merging. Additionally, maintaining a fixed 'k' ensures a constant computational cost, regardless of the number of domains. We appreciate the reviewer's understanding, and we will include this clarification in the revised manuscript.\n\n---\n\nBesides the implementation details, please take a look at our **General Response** above to see more about experimental setup and generalizability. We are happy to have more in-depth discussions with you if you have further concerns!\n\n*If our response addresses your concerns, please consider increasing the scores. Also, feel free to ask follow-up questions during the rebuttal period.*"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434035808,
                "cdate": 1700434035808,
                "tmdate": 1700434035808,
                "mdate": 1700434035808,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aTd5wkSMN0",
            "forum": "L1FeTLOwzr",
            "replyto": "L1FeTLOwzr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission375/Reviewer_DkBm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission375/Reviewer_DkBm"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies VideoQA in a domain continual-learning setting. The task encourages VQA models that can quickly adapt to new domains/datasets while simultaneously prevent catastrophic forgetting on learned domains. To achieve the goal, the paper proposes the dynamic adapter merging (DAM) method. Given a random instance, DAM dynamically (learning-free) merges a series of domain-specific parameter adapters for answer prediction, where the adapters are continually learned across datasets of different domains. The authors conduct extensive experiments on 6 VideoQA datasets and additionally 4 ImageQA datasets to show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe paper conducts the first study on domain-incremental learning in VideoQA. It also presents a nice solution to benchmark the task.\n2.\tThe DAM method is simple, easy to understand and shows strong results as well. Also, the experiments and analyses are in-depth.\n3.\tThe paper is well-presented and easy to read."
                },
                "weaknesses": {
                    "value": "1.\tThe definition of domain regarding VideoQA is not clear. The authors simply treat different datasets as different domains. This is certainly problematic and prevents detailed model analysis. For example, regarding the question type, all datasets define similar questions except for LSMDC with fill-in-blank setting.  Regarding the video type, there are instructional videos (iVQA), social videos (MSVD, MSRVTT, TGIF), movie videos (LSMDC) and activity videos. Regarding video length, all videos are short (3~15s) except for ActivityNet(3 mins). It would be better to experiment with more clarified domains instead of datasets.\n\n2.\tWhile the \u2018dynamic merging\u2019 design mitigates the problem of catastrophic forgetting and improves the overall performance as well, it necessitates all the learned adapters for inference. This resembles more on model ensemble versus continual learning a \u2018single\u2019 model. It is necessary to show the size of the adapters and analyze the efficiency.\n\n3.\tThe authors obtain the upper-bound results by individually finetuning on target datasets. My concern is that this \u2018upper-bound\u2019 may not be the actual upper-bound for incremental-learning because of data augmentation. Moreover, the gap between DAM' results and this upper-bound results is too small to show that there is need for future efforts as a novel task setting. The authors need to find a more convincing upper-bound or just mention the current one as a reference.\n\n4.\tAccording to the task setting, providing more analyses /comparisons on an OOD setting (aside from Fig.3(b)) would make the experiments more sound."
                },
                "questions": {
                    "value": "Minor:\n1. Why is the performance on ActivityNet not as good as on other datasets? \n\n2. In Sec. 3.2, what specific model does \u2018f\u2019 refer to? \n\n3. Analyses of table 5, 6 should be moved from the appendix to the main text."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission375/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission375/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission375/Reviewer_DkBm"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission375/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699101378360,
            "cdate": 1699101378360,
            "tmdate": 1699635964622,
            "mdate": 1699635964622,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OwSc2DM1nH",
                "forum": "L1FeTLOwzr",
                "replyto": "aTd5wkSMN0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission375/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "We appreciate your insightful review and constructive suggestions. In response, we have diligently addressed all raised concerns, and we believe our revisions effectively resolve them.\n\n---\n\n> **C3.1 The definition of domain regarding VideoQA is not clear.**\n\nOur definition of the domain diverges from the conventional definition in the continual learning community. Please refer to the **\u201cDefinition of domains\u201d of G1 in General Response** for more detail.\n\nWe also report the results on 7 diverse VidQA domains (separated by video type) in **\u201cDIL on more diverse domains\u201d of G2 in General Response**.\n\n---\n\n> **C3.2 It would be better to experiment with more clarified domains instead of datasets.**\n\nWe further validate our method DAM on **7 more diverse domains**: social videos (MSVD), instructional videos (iVQA),  movie videos (LSMDC), long activity videos (ActivityNet), indoor human videos (AGQA), traffic videos (TrafficQA) and virtual videos (Env-QA). Results show that DAM has even less forgetting on more clarified domain settings.\n\nPlease refer to **\"DIL on more diverse domains\" of G2 in General Response** for more details. \n\n---\n\n> **C3.3 The proposed DAM necessitates all the adapters for inference. It is necessary to analyze the efficiency.**\n\nThe adapters introduced in each domain contribute merely **2.5%** of the pretrained model's parameters (CLIP-L/14 + DeBerTa-V2-XLarge), totaling **30M** parameters. With 10 domains, this results in only a 25% increase in parameters, a reasonable augmentation given DAM's robust performance.\n\nIn terms of computational cost, merging adapter parameters incurs just **0.09 GFLOPs** (30M *(2k-1), k=2 in our case), notably lower than the **162 GFLOPs** required by CLIP-L/14 for a single image processing. We appreciate the reviewer's suggestion and will integrate this analysis into the revision.\n\n---\n\n> **C3.4 The authors obtain the upper-bound results by individually finetuning target datasets. My concern is that this \u2018upper-bound\u2019 may not be the actual upper-bound for incremental learning because of data augmentation.**\n\nWe conducted both individual finetuning (Ind-FT) and multi-task finetuning (MLT-FT), the latter involving joint training on all datasets. As indicated in the table, MLT-FT yields a notable 1.8% enhancement in MSVD-QA, while maintaining a comparable average accuracy to Ind-FT (**52.5% vs. 52.6%**).\n\nIn the realm of continual learning, multi-task finetuning is conventionally considered an upper bound. We apologize for the confusion and will replace the upper bound with MLT-FT in the revised manuscript. \n\n|Method|iVQA|MSVD|MSR-VTT|LSMDC|ActivityNet|TGIF|Avg.|\n|-|-|-|-|-|-|-|-|\n|Ind-FT|39.8|54.8|46.7|63.0|42.4|68.0|52.5|\n|MLT-FT|39.7|56.6|46.7|62.9|42.2|67.8|52.6|\n\n---\n\n> **C3.5 The gap between DAM's results and these upper bounds is too small to show the need for future efforts in a novel task setting.  Also, why is the performance on ActivityNet not as good as others?**\n\nWe thank the reviewer for pointing out the room for future work. There are still unaddressed issues that lead to a relatively large accuracy drop on some specific datasets, i.e. MSRVTT (**-4.5%**) and ActivityNet (**-6.1%**). Other future work includes experimenting with **many more domains (or datasets) like 100**, and extending our method to extremely large models (i.e. > **10B** parameters).\n\nRegarding the ActivityNet dataset, we analyze the performance drop might be due to the high similarity of its question type with the iVQA dataset, while the videos are quite different. This inconsistency may lead to confusion within our proposed model.\n\n---\n\n> **C3.6 Providing more analyses /comparisons on an OOD setting (aside from Fig.3(b)) would make the experiments more sound.**\n\nWe further experiment on 7 diverse domains as described in G2 of the General Response. We continually train the proposed DAM on the first 3 domains and evaluate the trained model on the other 3 domains. From the table, DAM outperforms S-Prompts consistently in all the domains, attributed to the larger capacities of adapters and the robustness of dynamic merging.\n\n||In Distribution|||Out-of-Distribution|||\n|-|-|-|-|-|-|-|\n|Method|MSVD|iVQA|LSMDC|ActivityNet|AGQA|Env-QA|TrafficQA|\n|Upper-Bound|56.6|39.7|63.0|42.2|63.4|32.3|67.8|\n|DAM|54.4|39.4|63.0|20.3|25.4|5.6|18.9|\n|S-Prompts|47.7|34.0|57.4|18.7|22.9|5.5|15.1|\n\n---\n\n> **C3.7 In Sec. 3.2, what specific model does \u2018f\u2019 refer to?**\n\nWe refer to the original pretrained model as \u201cf\u201d, i.e. the model used for zero-shot inference (FrozenBiLM in our case) except that we extract the hidden states from the 4th last layer as the output. We thank the reviewer\u2019s suggestion and will add a detailed description in the supplementary materials.\n\n---\n\n*If our response addresses your concerns, please consider increasing the scores. Also, feel free to ask follow-up questions during the rebuttal period.*"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700436379303,
                "cdate": 1700436379303,
                "tmdate": 1700436379303,
                "mdate": 1700436379303,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZRBOHTmEmU",
            "forum": "L1FeTLOwzr",
            "replyto": "L1FeTLOwzr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission375/Reviewer_thbM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission375/Reviewer_thbM"
            ],
            "content": {
                "summary": {
                    "value": "The article presents to address continual video question-answering (VidQA) learning with a simple framework, named DAM. Through sequentially training domain-specific adapters and leveraging a video-language router to merge the adapters for inference, DAM outperforms prior methods by 9.1% while forgetting less by 1.9%."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The paper assumes that this is the first attempt to address the issue of continual learning in VideoQA.\n2) Comprehensive Ablation Studies: The article includes sufficient and in-depth set of ablation experiments, which provide a thorough understanding of the method's performance and help identify critical components.\n3) Clear Method Framework: The method's framework is straightforward and well-explained, making it accessible to readers and researchers in the field."
                },
                "weaknesses": {
                    "value": "1) Limited Dataset Diversity: The article's experimental use of six datasets with relatively small differences between them, especially MSVD and MSR-VTT, raises concerns about the method's domain adaptation and continual learning capabilities. The use of internet-sourced videos in the datasets does not fully explore the potential challenges posed by more diverse datasets, such as those collected in virtual environments (e.g., Env-QA[1]), traffic scenarios (e.g., TrafficQA[2]), or indoor human activities (e.g., AGQA[3]). What\u2019s more, the out-of-date issue proposed in Figure 1 hasn\u2019t been evaluated, also.\n2) While the article demonstrates the effectiveness of the adapter and router, their simple design might not generalize well to more challenging datasets. The reviewer has doubts about their applicability in more complex scenarios.\n3) The article does not provide a fair comparison with backbone models under few-shot learning setting. A direct comparison between in-context learning using FrozenBiLM and the proposed approach could offer a more comprehensive evaluation.\n[1] Gao, Difei, et al. \"Env-qa: A video question answering benchmark for comprehensive understanding of dynamic environments.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n[2] Xu, Li, He Huang, and Jun Liu. \"Sutd-trafficqa: A question answering benchmark and an efficient network for video reasoning over traffic events.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n[3] Grunde-McLaughlin, Madeleine, Ranjay Krishna, and Maneesh Agrawala. \"Agqa: A benchmark for compositional spatio-temporal reasoning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021."
                },
                "questions": {
                    "value": "1) The article does not provide sufficient evidence of severe catastrophic forgetting in current large models.\n2) It is worth discussing whether there are unique challenges related to continual learning in the domain of VideoQA."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission375/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699116323095,
            "cdate": 1699116323095,
            "tmdate": 1699635964565,
            "mdate": 1699635964565,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hDEmZ9F3BS",
                "forum": "L1FeTLOwzr",
                "replyto": "ZRBOHTmEmU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission375/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "We appreciate your insightful review and constructive suggestions. In response, we have diligently addressed all raised concerns, and we believe our revisions effectively resolve them.\n\n---\n\n> **C2.1 Limited Dataset Diversity: The article uses 6 datasets with relatively small differences between them, especially MSVD and MSR-VTT.**\n\nWe thank the reviewer for the valuable suggestion. We believe the confusion is caused by the question of **whether domains should be orthogonal**.\n\nWe elaborate on our point in \u201c**Dataset Diversity\u201d of  G1 in the General Response** above, please refer to that section for more detail. We also report the results with more diverse datasets in **G2 in the General Response**.  \n\n---\n\n> **C2.2 The out-of-date issue proposed in Fig. 1 hasn\u2019t been evaluated.**\n\nWe further evaluate the CIL scenario to mimic the evaluation of the out-of-date issue. The old model may not be able to answer questions in new tasks as they never see the classes in the new tasks before, which is similar to the example in Fig. 1 that a VidModel trained in 2021 may struggle with questions about the 2023 movie \u201cBarbie\u201d. Our proposed DAM method archives significant improvements compared to the second-best approach on CIL settings.\n\nWe refer the reviewer to **G2 (Generalization) in the General Response** for more details.\n\n---\n\n> **C2.3 The use of internet-sourced videos in the datasets does not fully explore the potential challenges posed by more diverse datasets, such as those collected in virtual environments (Env-QA), traffic scenarios (TrafficQA), or indoor human activities (AGQA).**\n\nThanks for the insightful suggestion. We further experiment with **7 diverse video datasets** including all the datasets that the reviewer mentioned. Results show that our DAM method achieves even less forgetting on more diverse datasets. \n\nPlease refer to \u201c**DIL on more diverse domains\u201d of G2 in General Response** with more detail. \n\n---\n\n> **C2.4 Their simple design might not generalize well to more challenging datasets. The reviewer has doubts about this.**\n\nIn **G2 Generalization of the General Response**, we showcase DAM\u2019s generalization ability by experimenting on DIL with more diverse domains, CIL, and TIL scenarios. In Sec 4.5 and Tab. 4 in the main paper, we show DAM can be applied to the visual (image)-QA task using a different large model BLIP2.\n\nAll the experimental results support the conclusion that DAM can generalize well to challenging datasets, scenarios, and even more tasks and models. This is because we do not need complex design on top of pretrained large models as they already provide enough generalizability.\n\n---\n\n> **C2.5 The article does not provide a fair comparison with few-shot learning. A direct comparison between in-context learning using FrozenBiLM and the proposed approach could offer a more comprehensive evaluation.**\n\nWe appreciate that the reviewer pointed out another interesting direction (in-context learning, ICL) to address the continual learning problem. We further experiment with the one-shot in-context learning using FrozenBiLM and report the results below. The proposed DAM outperforms one-shot FrozenBiLM by **29.3%** in average accuracy. The inferior performance of one-shot ICL is because LLM with at least 6.7B parameters **begin to have** in-context learning ability on multimodal tasks (Koh et al. 2023).\n\nWe will add this comparison and mention that ICL with LLM could be a potential direction for continual learning in the revised manuscript.\n\n|Method|iVQA|MSVD|MSR-VTT|LSMDC|ActivityNet|TGIF|Avg.|\n|-|-|-|-|-|-|-|-|\n|Zero-shot FrozenBiLM|26.8|33.0|15.0|51.5|25.5|41.9|32.3|\n|One-shot ICL FrozenBiLM|17.9|22.5|9.7|34.5|17.8|23.1|20.9|\n|DAM(Ours)|39.1|53.6|42.2|63.0|36.3|66.8|50.2|\n\nKoh, Jing Yu et al. \"Grounding Language Models to Images for Multimodal Inputs and Outputs.\" (2023).\n\n\n---\n\n> **C2.6 The article does not provide sufficient evidence of severe catastrophic forgetting in current large models.**\n\nAs shown below, sequentially finetuning (Seq-FT) the large model (FrozenBiLM) without any continual learning technique leads to a 12.7% accuracy drop, which we believe is sufficient to show severe catastrophic forgetting. Thanks for the suggestion and we will add this baseline in Table 1 in the revision.\n\n|Method|iVQA|MSVD|MSR-VTT|LSMDC|ActivityNet|TGIF|Avg.|\n|-|-|-|-|-|-|-|-|\n|UpperBound|39.8|54.8|46.7|63.0|42.4|68.0|52.5|\n|Seq-FT|28.4|36.0|23.7|52.1|31.2|67.6|39.8|"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700436563562,
                "cdate": 1700436563562,
                "tmdate": 1700438298495,
                "mdate": 1700438298495,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w0gCKNdsrr",
                "forum": "L1FeTLOwzr",
                "replyto": "ZRBOHTmEmU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission375/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "> **C2.7 It is worth discussing whether there are unique challenges related to continual learning in the domain of VideoQA.**\n\nAs mentioned on Page 2 (below Fig. 1) in the main draft, in contrast to unimodal tasks like image classification, VidQA involves both video and text inputs, necessitating the model to jointly reason on both modalities. Within VidQA domains, question types may exhibit significant similarity (e.g., when, how, where), while the corresponding answers can vary considerably. The juxtaposition of a shared input space (video+question) and a diverse output space adds complexity to continual VidQA. Consequently, router-based continual learning methods may face challenges in accurately predicting domain identities, leading to inferior answer prediction accuracies.\n\nOur proposed method, DAM, effectively mitigates the issue of inaccurate routers by dynamically merging domain adapters (refer to Sec. 4.4). We appreciate the suggestion and will incorporate this discussion in the revision.\n\n---\n\n*If our response addresses your concerns, please consider increasing the scores. Also, feel free to ask follow-up questions during the rebuttal period.*"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700436720524,
                "cdate": 1700436720524,
                "tmdate": 1700436720524,
                "mdate": 1700436720524,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "L5HvVUH8eE",
            "forum": "L1FeTLOwzr",
            "replyto": "L1FeTLOwzr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission375/Reviewer_t6Nd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission375/Reviewer_t6Nd"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the Dynamic Adapter Merging (DAM) for video question-answering under Domain-Incremental Learning scenario, which is a rehearsal-free approach. DAM leverages the fusion of parameters from multiple adapters to mitigate the interference of erroneous predictions, thereby enhancing the performance of the model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well organized and the proposed method is verified through many experimental results.\nThe DAM is straightforward and easy to follow."
                },
                "weaknesses": {
                    "value": "The paper provides a detailed elaboration to the framework of the model. However, the authors do not explicitly mention the loss function used during the training of adapters.\n\nThe contributions of the paper may be insufficient.  Although the Introduction section mentions four contributions, these contributions revolve primarily around one aspect, i.e. related to combining domain-specific adapter learning and model merging techniques.\n\nThe proposed method may lack innovation as the idea of model merging techniques in deep/machine learning is frequently used. The non-parametric router function is simply based on cosine similarity with no improvements. However, the application of such a concept to Continual Learning does introduce somewhat novelty."
                },
                "questions": {
                    "value": "Can such ideas bring about desired performance improvements when extended to class-incremental learning and task-incremental learning scenarios? Can the author incorporate some results to demonstrate the generalizability of the idea in the context of continual learning?\n\nHow were the experimental results in the article obtained? Were multiple runs conducted to obtain an average, or was only a single experiment performed? I would like to know the stability of the proposed method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission375/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699440815529,
            "cdate": 1699440815529,
            "tmdate": 1699635964492,
            "mdate": 1699635964492,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2HsXYJvQwm",
                "forum": "L1FeTLOwzr",
                "replyto": "L5HvVUH8eE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission375/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "We appreciate your insightful review and constructive suggestions. In response, we have diligently addressed all raised concerns, and we believe our revisions effectively resolve them. Should any of your questions or concerns persist, please feel free to communicate with us.\n\n---\n\n> **C1.1 The loss function used during the training of adapters.**\n\nWe didn\u2019t apply additional loss functions to the adapters. Our loss function is the cross-entropy loss between the predicted tokens and ground-truth answer tokens, which is the same as the original VidQA model, i.e. FrozenBiLM in our paper. \n\n---\n\n> **C1.2 The contributions of the paper may be insufficient, i.e. related to combining domain-specific adapter learning and model merging techniques.**\n\nWhile we acknowledge drawing inspiration from both continual learning and model merging, our work **introduces a novel technique**, \"Dynamic Adapter Merging (DAM),\" previously unexplored in both domains. Unlike existing methods that yield a single or a set of fixed models, DAM innovatively generates a **personalized expert model for each testing sample** with minimal overhead, which is not a simple adaption or combination of existing model merging methods and continual learning methods.\n\nFurthermore, our novelty extends to the **in-depth analyses** (as highlighted by [thbM] and [DkBm]) in Sec. 4.3 and Sec. 4.4, detailing how and when model merging can enhance the effectiveness of the router-based technique in the continual learning domain.\n\nFinally, the empirical results of DAM could be **inspiring to the model merging community**. As shown in the table below, we compare DAM with the other merging approaches, including average merging and RegMean (Jin et al., 2022), while all the approaches merge the same set of domain models that are individually finetuned on each dataset.  Unlike the other approaches, DAM determines the merging ratios for domain adapters based on the input instance, and this flexibility makes DAM outperform RegMean by **6.0%** and average merging by **7.5%** in average accuracy. The results show the potential of the proposed selective and dynamic merging strategy to inspire model-merging communities.\n\n|Method|iVQA|MSVD|MSR-VTT|LSMDC|ActivityNet|TGIF|Avg.|\n|-|-|-|-|-|-|-|-|\n|Avg. Merging|**38.0**|45.7|27.7|54.5|27.0|56.6|41.6|\n|RegMean|36.6|49.7|32.5|54.0|27.7|57.8|43.1|\n|DAM (Ours)|36.5|**51.6**|**39.5**|**63.0**|**36.5**|**67.7**|**49.1**|\n\nThanks for your suggestion and we will highlight these contributions in our revised manuscript.\n\n---\n\n> **C1.3 The non-parametric router function is simply based on cosine similarity with no improvements.**\n\nAs demonstrated in Table 2, Section 4.3 of the main paper, our straightforward cosine-similarity-based router **surpasses all existing counterparts** with more complex router design, including MLP-based learnable routers proposed by L2P and CODA-Prompts, as well as the KMeans-based router employed in S-Prompts. Notably, our simple non-parametric router exhibits a **2.7%** higher accuracy in domain-identity prediction compared to the second-best router while having no training cost and faster inference speed. We also tried more complex router functions on the DAM framework. As shown in the table below, their performance is only comparable to our router. Thus, we keep our router simple but effective in the DAM approach. \n\n|Router|iVQA|MSVD|MSR-VTT|LSMDC|ActivityNet|TGIF|Avg.|\n|-|-|-|-|-|-|-|-|\n|GMM|38.5|55.1|43.4|63.0|31.2|65.4|49.4|\n|Learnable MLP|39.1|49.9|42.9|63.0|31.1|67.4|48.9|\n|Ours (cos.sim.)|39.1|53.6|42.2|63.0|36.3|66.8|50.2|\n\n---\n\n> **C1.4 Generalizability: Can such ideas be extended to Class-Incremental Learning (CIL) and Task-Incremental Learning (TIL) scenarios?**\n\nWe thank the reviewer for the valuable suggestion. We report both the CIL and TIL results in **G2 in General Response**. Results show that DAM is capable of outperforming the state-of-the-art continual learning approach under all settings.\n\nAdditionally, In Sec 4.5 and Tab. 4 in the main paper, we show DAM can be applied to the visual (image)-QA task using a different large model BLIP2 on 4 ImageQA datasets.\n\n---\n\n> **C1.5 Stability: How were the experimental results in the article obtained, multiple runs or a single run?**\n\nThe results in our main table (Tab. 1) are obtained with 5 different random seeds. We report an average accuracy of **50.23 \u00b1 0.12%**. Furthermore, in Tab. 9 (Ablations on domain orders) of the Appendix (following the reference section in the main paper PDF), we randomly sampled five different domain orders, observing consistent average accuracies (**50.56 \u00b1 0.26%**).\n\nThese findings affirm the stability of our method to variations in both random seeds and domain orders.\n\n---\n\n*If our response addresses your concerns, please consider increasing the scores. Also, feel free to ask follow-up questions during the rebuttal period.*"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700436787567,
                "cdate": 1700436787567,
                "tmdate": 1700553523182,
                "mdate": 1700553523182,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]