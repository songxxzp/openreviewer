[
    {
        "title": "A Long Way To Go: Investigating Length Correlations in RLHF"
    },
    {
        "review": {
            "id": "btDoiVcI8K",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8099/Reviewer_McHv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8099/Reviewer_McHv"
            ],
            "forum": "sNtDKdcI1f",
            "replyto": "sNtDKdcI1f",
            "content": {
                "summary": {
                    "value": "The authors show that when applying RLHF for optimize helpfulness, much if not most of the gain comes simply from learning to increase the length of the response, and in fact solely optimizing for length can achieve most of the gains of optimizing for helpfulness in many cases."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "--comprehensive evaluations on a few different RLHF settings/datasets for helpfulness optimization\n\n--nice in-depth exploration confirming the extent of a phenomenon (length correlations for helpfulness optimization) that i think hasn't been comprehensively analyzed previously"
                },
                "weaknesses": {
                    "value": "--presentation: it's quite difficult to parse what several of the abbreviations mean in the tables (and honestly i'm still a bit confused on what exactly you're showing in the first couple of figures and tables after looking at them for several minutes). it would be great if you could make the figs/tables more self-contained, e.g., define abbreviations in the captions and summarize takeaways, so i don't have to keep bouncing between the figs/tables and the text."
                },
                "questions": {
                    "value": "--even if we're just optimizing for length, your model still learns something nontrivial, right? an interesting point of comparison might be an extra baseline where instead of sampling 8 outputs from the base LM and picking the longest one, you just sample one output and forcibly make it keep generating past the end - i imagine this would perform a lot worse. so intuitively, i feel that optimizing for length is also somehow optimizing for some less trivial notion of comprehensiveness, maybe?\n\n--just to confirm, weighted reward gain i should think of as measuring how much reward improves within a particular bin of length, i.e. the reward gain that comes from factors other than length?\n\n--would be curious also if you have any qualitative sense of what are the non-length axes along with helpfulness improves in your experiments (i.e., any intuitions for the remaining 10-30% of helpfulness that wasn't attributed to increased length?).\n\n--as you wrote in the conclusion, it would be interesting to just see a quick experiment on e.g., harmlessness or honesty, if only to confirm our intuition that length is primarily a heuristic for optimizing helpfulness and not really applicable to other common tasks we RLHF for. (not necessary if it's troublesome to setup though.)\n\n--i'm not very clear on the concrete actionable suggestions for practitioners based on your analysis? (though they're not strictly necessary - i see that you did try a few ways to regularize length already)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8099/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8099/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8099/Reviewer_McHv"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8099/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697399848197,
            "cdate": 1697399848197,
            "tmdate": 1699637003481,
            "mdate": 1699637003481,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RGKDl6g4Nv",
                "forum": "sNtDKdcI1f",
                "replyto": "btDoiVcI8K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8099/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8099/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer McHv"
                    },
                    "comment": {
                        "value": "Thanks for the feedback and interesting questions! \n\n> presentation: it's quite difficult to parse what several of the abbreviations mean in the tables\n\nThanks so much for this feedback, we uploaded a new version with figures and tables that we simplify and tie more clearly to presentation in the text, as well as clarifying experiments further to be easier to read. \n\n> concrete actionable suggestions for practitioners based on your analysis? (though they're not strictly necessary - i see that you did try a few ways to regularize length already)\n\nThanks for asking! Some concrete suggestions are: (1) As you suggested, if one simply wants to get most of the \u201cimprovement\u201d of RLHF without dramatic length increases, several of our proposed interventions are actually good options. (2) Downstream evaluation isn\u2019t a gold standard for RLHF learning good features. Currently, better human/simulated preferences are often assumed to indicate that RLHF is learning \u201cdesired\u201d features, dismissing further investigation. Especially if it\u2019s high-risk (medical, legal application, etc.) RLHF practitioners should measure success in highly-specific terms, to validate assumptions about features learned. (3) RLHF work should focus mostly on reward modeling (better objective functions, collecting better data, engineering to enable larger scale with less compute, etc.). This direction is relatively underexplored, and will likely be the first step to substantial improvements\n\n> even if we're just optimizing for length, your model still learns something nontrivial, right? \n\nWe thought that this was really interesting too, our hypothesis is that the joint optimization of length with the KL penalty is what leads to the \u201cnon-trivial\u201d learning of features. Since repetitive, pathological outputs, etc. would likely have a higher KL divergence from the initial policy, this term likely forces the model to learn how to generate more descriptive outputs while ALSO maximizing length. This is probably why the lambda=0 baseline is sometimes much worse than the standard lambda=0.04 counterpart for pure length optimization. \n\n>  it would be interesting to just see a quick experiment on e.g., harmlessness or honesty, if only to confirm our intuition that length is primarily a heuristic for optimizing helpfulness and not really applicable to other common tasks we RLHF for. (not necessary if it's troublesome to setup though.)\n\nWe trained a reward model for harmlessness on the Anthropic data. We did not find length correlation in this model. The within-batch length correlation is around -0.3, and doing PPO with just the harmlessness reward model didn\u2019t increase length either once converged. This is perhaps expected since a shorter response (such as abstention from answering) will often be harmless. We also observed that the outputs started to look strange eventually, so optimizing for just harmlessness has its own set of shallow features and should not be optimized on its own. Thanks for the suggestion!\n\n> just to confirm, weighted reward gain i should think of as measuring how much reward improves within a particular bin of length\n\nYes, weighted reward gain gives an aggregate notion of reward improvement purely within bins \n\n> Non-length axes for remaining 20-30%\n\nThanks for asking! We thought a bit about this too. Based on looking at outputs and RM scores from before and after RLHF on some different settings, some alternate axes may be related to things like answer relatedness to the question (completely unrelated outputs will get lower scores, off-topic generation seems to go down from RLHF), and grammaticality. That said, a more comprehensive study of qualitative changes from RLHF (especially once we have more robust reward models in the future), would be interesting for future work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8099/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700339959734,
                "cdate": 1700339959734,
                "tmdate": 1700339959734,
                "mdate": 1700339959734,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zcHFFkj7vN",
                "forum": "sNtDKdcI1f",
                "replyto": "RGKDl6g4Nv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8099/Reviewer_McHv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8099/Reviewer_McHv"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for answering my questions. I'd encourage you to also update your paper with some of these discussions, and add your new harmlessness experiment to the Appendix too (I don't think I saw it in there?)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8099/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694312957,
                "cdate": 1700694312957,
                "tmdate": 1700694312957,
                "mdate": 1700694312957,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "82UjGNAfO7",
            "forum": "sNtDKdcI1f",
            "replyto": "sNtDKdcI1f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8099/Reviewer_rbH1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8099/Reviewer_rbH1"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores how length correlates with performance improvement in RLHF. The author starts by examining the output length increase in RLHF and the correlation between output length and reward model scores, and then they reveal that some intervention in PPO can mitigate the length increase to some extent. Further, they find that the reward model can only learn \"easy\" examples, i.e. the examples that can be correctly predicted according to the length heuristic. The authors examine several intervention methods for reward modeling and find that there could be a trade-off between length bias and reward model accuracy in some cases. Finally, they show that purely optimizing the policy model for increasing output length can lead to an improvement in output quality."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper focuses on a very important problem: what role does length play in RLHF? The paper conducts extensive experiments to demonstrate the correlation between length and reward model scores and explores several ways to mitigate length bias. The results can provide constructive guidance for future research."
                },
                "weaknesses": {
                    "value": "1. The most confusing part for me is the evaluation. What is the rationale for adopting a length-bias metric, GPT-4 evaluation [1], when evaluating the correlation between length and RLHF performance? Two potential factors can be affected by length, making it hard to disentangle the attribution of length correlation. Or do you aim to reveal the bias issue of GPT-4 in this paper? Then could you please explain more clearly what you mean by \"length correlations in RLHF\" and what the length correlates with? The reward modeling? The optimization algorithm? The evaluation? Or all of them? If so, I'd suggest investigating them separately.\n2. I am skeptical about the experiment and claim that \"reward models are not able to make progress on most training examples\" (sec 4.1). The results may, at least partly, be due to the reward model capacity. I suppose that reward modeling is a relatively difficult task, so weak models may only capture some shallow features. With a more capable reward model, the \"hard\" samples may be learned. A supporting evidence is that, as shown in Table 3, the length heuristics accuracy on WebGPT is 55.7%. However, in the result of a recent paper [2], a more powerful reward model that is not trained on WebGPT can yield 65.2% accuracy. Therefore, according to this paper, at least 9.5% of counter-biased samples are learned by the reward model. Perhaps the authors can supplement experiments by adopting more powerful reward models, e.g. scaling the base from LLaMA-7B to 70B following the original LLaMA-2 paper [3] (if computation allowed), and see if the learning patterns remain the same.\n\nReferences:\n[1] AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. Dubois et al.\n\n[2] UltraFeedback: Boosting Language Models with High-quality Feedback. Cui et al.\n\n[3] Llama 2: Open Foundation and Fine-Tuned Chat Models. Touvron et al."
                },
                "questions": {
                    "value": "Please refer to weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8099/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8099/Reviewer_rbH1",
                        "ICLR.cc/2024/Conference/Submission8099/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8099/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698498044494,
            "cdate": 1698498044494,
            "tmdate": 1700474268266,
            "mdate": 1700474268266,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mUuaKa8Gys",
                "forum": "sNtDKdcI1f",
                "replyto": "82UjGNAfO7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8099/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8099/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rbH1"
                    },
                    "comment": {
                        "value": "Thanks for the constructive feedback and questions! \n\n> Could you please explain more clearly what you mean by \"length correlations in RLHF\" and what the length correlates with? The reward modeling? The optimization algorithm? The evaluation? Or all of them? If so, I'd suggest investigating them separately.\n\nWe examine \u201clength correlations\u201d in this paper in the following contexts: \n- Preference data: Is label distribution imbalanced to favor length?\n- Reward modeling (Section 4): Does reward score correlate with length? Does \u201ceasiness\u201d of examples correlation with length? (connects to preference data)\n- PPO (Section 3.5): Does PPO lead to longer outputs if we do things like increase the KL penalty?\nSince these are different notions, we generally dedicate different sections / experiments to each of these components, but we can make sure to clarify these distinctions further. Importantly, we *don\u2019t* seek to analyze length bias within evaluation, since our hypotheses largely don\u2019t depend on GPT-4 evaluation. \n\n> The most confusing part for me is the evaluation. What is the rationale for adopting a length-bias metric, GPT-4 evaluation [1], when evaluating the correlation between length and RLHF performance? Two potential factors can be affected by length, making it hard to disentangle the attribution of length correlation. Or do you aim to reveal the bias issue of GPT-4 in this paper? \n\nYes, both GPT-4 and human evaluators likely have a bias towards longer responses (recall our goal isn\u2019t to reveal these evaluation biases), but we find they can still reveal useful information when length biases are controlled (see claims below). For comparability with prior work, we follow the protocol of [3] for \u201csimulated preference\u201d downstream evaluation, as it was shown to correspond well with human preference.\n\nWe use simulated preferences as follows: \n- Context: RLHF generally makes \u201cnumbers go up\u201d relative to SFT on standard settings (Table 2), which largely follows prior work\n- Claim: Reward interventions can improve quality of outputs independent of length (Table 5)\n- Claim: Length-only PPO is both better than a baseline of sampling longer outputs, and comparable to standard PPO (Table 6). \n\nImportantly, notice that for both claims, the simulated win rates are actually higher for models with much shorter lengths on average: these results are not explained away by saying that GPT-4 prefers longer outputs in simulated win rate experiments.\n\nFinally, note that most of our results (Fig 2,3,4; Table 1,2,3,4) are based on intrinsic measures like reward model scores and output length, which are unaffected by any biases from GPT-4. \n\n> I am skeptical about the experiment and claim that \"reward models are not able to make progress on most training examples\" (sec 4.1). The results may, at least partly, be due to the reward model capacity. [...] Perhaps the authors can supplement experiments by adopting more powerful reward models [...]\n\nThanks for mentioning this! We actually agree with this point. We didn\u2019t intend to claim that reward models *in general* couldn\u2019t learn from most examples on those datasets, but rather that this happens with the 7B scale reward models with current objectives and data, then propagating length correlations throughout RLHF. This is why a big takeaway for future work is to focus on improving reward modeling especially at more scientifically feasible scales. \n\nWe did our experiments with 7B-sized models largely due to hardware limitations (because PPO involves storing several large models). Note that:  \n- The majority of open RLHF work uses 7B, or often even smaller-scale models [1][2][3][4].\n- *All* our trained 7B reward models actually outperform length heuristic (61.5% on WebGPT vs 55.7), and thus are likely learning other features, yet PPO still largely optimizes for length.\n- Regarding UltraFeedback (the paper with 65.2% accuracy on WebGPT, which we note was arxived since the ICLR deadline), scale may be a component, but they also use very different (potentially better) preference data. This supports our overall message of needing to improve preference data and reward modeling\n\nThat said, based on your feedback, we scaled our RM experiments up to LLaMA-2 13B. We report intrinsic reward modeling accuracy results below (compare with Table 4 in the paper):  \n\n| Model | WebGPT | Stack | RLCD |\n|-------|--------|-------|------|\n| LLaMA 7B    | 61.5%  | 70%   | 80%  |\n| LLaMA-2 13B   | 64.5%  | 71.3% | 81.2%|\n\nAs shown in the table, we found that reward modeling accuracy is only marginally better than it was before, suggesting that model scale likely isn\u2019t the main bottleneck on the \u201chard\u201d examples. We agree it would be good to scale up these results further, but we think the main takeaway of our results and those in UltraFeedback is the need for better datasets rather than just scaling. \n\nThanks again for the experiment suggestion!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8099/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700339867709,
                "cdate": 1700339867709,
                "tmdate": 1700339867709,
                "mdate": 1700339867709,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IqggFepj2e",
                "forum": "sNtDKdcI1f",
                "replyto": "82UjGNAfO7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8099/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8099/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Citations"
                    },
                    "comment": {
                        "value": "[1] Simeng Sun, Dhawal Gupta, and Mohit Iyyer. Exploring the impact of low-rank adaptation on the performance, efficiency, and regularization of RLHF. ArXiv, abs/2309.09055, 2023.\n\n[2] Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, and Yuandong Tian. RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment.\n\n[3] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback, 2023.\n\n[4] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Hanna Hajishirzi. Fine-grained human feedback gives better rewards for language model training. ArXiv, abs/2306.01693, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8099/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700339890661,
                "cdate": 1700339890661,
                "tmdate": 1700339917354,
                "mdate": 1700339917354,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GfGQvTnA00",
                "forum": "sNtDKdcI1f",
                "replyto": "mUuaKa8Gys",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8099/Reviewer_rbH1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8099/Reviewer_rbH1"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thanks for the clarification. I would like to raise my scores, but I hope that the authors can improve the presentation of this paper in the revision."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8099/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700474246681,
                "cdate": 1700474246681,
                "tmdate": 1700474246681,
                "mdate": 1700474246681,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EXSJAbdOjV",
            "forum": "sNtDKdcI1f",
            "replyto": "sNtDKdcI1f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8099/Reviewer_tqJD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8099/Reviewer_tqJD"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies whether RLHF scores are correlated with the length of responses. Given that length preference is a well-known issue in the RLHF literature, in the work, the authors explore how much of the optimization and improvement in RLHF is based on length, as opposed to other factors.\n\nUsing a comprehensive set of experiments, the paper shows that length constitutes the majority of the reward, indicating that length may play a much larger role than previously documented.\n\nFinally, the authors discuss the implication of these findings on the RLHF and state the importance of developing better and more robust algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper offers a well-executed investigation of a well-known pattern: the correlation between RLHF scores and length. The findings are persuasive and supportive of the conclusions"
                },
                "weaknesses": {
                    "value": "The paper's comprehensibility is somewhat challenging. The conclusions drawn from the tables and figures lack clarity, and it's not easy to discern the key takeaway from the experiments presented. The paper would be improved with some rewriting and clarification.\n\nThe experiments themselves are well-executed, and they do support the main message. However, it's worth noting that this pattern has been observed in numerous other studies, and strategies to address this bias/reward hacking have been extensively documented elsewhere. Consequently, the contribution of this paper does not look very strong for an ICLR  conference."
                },
                "questions": {
                    "value": "NIT\n* The \"STD\" term was introduced in Figure 2 way before defining it later in the text. \n* For Figure 3, why not have both settings (red and black) for all the lengths? Does that mean that sometimes there are no examples in a particular bin for a particular setting or are there some missing datasets in the figure?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8099/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698690059283,
            "cdate": 1698690059283,
            "tmdate": 1699637003215,
            "mdate": 1699637003215,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sGrjJXmXAf",
                "forum": "sNtDKdcI1f",
                "replyto": "EXSJAbdOjV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8099/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8099/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tqJD"
                    },
                    "comment": {
                        "value": "Thanks for the thoughtful comments! \n\n> a well-known pattern: the correlation between RLHF scores and length\n> this pattern has been observed in numerous other studies, and strategies to address this bias/reward hacking have been extensively documented elsewhere. Consequently, the contribution of this paper does not look very strong for an ICLR conference.\n\nWe provide the most in-depth study of this phenomenon to date, indicating that length may play a much larger role than previously anticipated by the community. Length increases from RLHF have been observed previously (e.g., in \u201cSecrets of PPO\u201d, which we cite), but we believe our analysis, and the new tools we introduce, contribute a lot to our understanding beyond just observing the pattern. \n\nFor instance, we conclude the following, which are not documented in prior work:\n- PPO\u2019s reward optimization strategy relies *substantially* on length at the cost of other features\n- PPO\u2019s overall reward optimization strategy is qualitatively similar across various RL interventions given the same reward model: RMs are the most crucial component\n- Reward robustness issues are the primary source of length biases; current open reward models struggle on preference data, and *heavily* rely on \u201ceasy\u201d subsets of training data with shallower features\n- Doing PPO with *just length* is comparable to full RLHF with trained reward models\n\nIf there are additional papers we should cite or contextualize our work with respect to, we\u2019d love any pointers!\n\n> Feedback on clarity, STD term\n\nThanks for pointing these out! We made a further pass through the paper to clarify things, especially key take-aways from experiments, and improve captions / Figure 3 based on your feedback. \n\n> For Figure 3, why not have both settings (red and black) for all the lengths? Does that mean that sometimes there are no examples in a particular bin for a particular setting\n\nYes. Due to length shifts, there are cases where, for example, PPO doesn\u2019t generate any outputs at shorter lengths, leading to us being unable to have a comparative bin (which is one of the reasons why we use a length-constrained high-KL version to better validate our hypotheses in a case with more overlapping bins). \n\nWe realized that putting both versions in the plot was a bit overloaded, so we made a simpler version with only the high-KL arrows, keeping both normal and high-KL numbers in the WRG table and moving the 2-color plot to the appendix. Thanks for the feedback!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8099/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700339828544,
                "cdate": 1700339828544,
                "tmdate": 1700339828544,
                "mdate": 1700339828544,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0ndkXxe1Dh",
            "forum": "sNtDKdcI1f",
            "replyto": "sNtDKdcI1f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8099/Reviewer_PX4u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8099/Reviewer_PX4u"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the length corrections with the RLHF paradigm for language model (LM) training. Specifically, it has been observed that models finetuned with RLHF tend to generate longer sentences. In this paper, the authors study the causes of the phenomenon through multiple aspects, including the RL algorithm (i.e., PPO) for training the LM, the reward models (RM), and preference data. They demonstrate the PPO generally increases the length compared to the supervised model regardless of the reward engineering applied to mitigate the issue. In addition, altering the data for reward model training does not fully solve the issue either. Lastly, they show that solely optimizing for the length recovers most of the performance improvements. \n\nThe highlight of the paper is it clearly documents the length-increasing issue in the RLHF pipeline. They conduct experiments on several datasets across different domains to demonstrate the issue.\n\nHowever, the paper is rather descriptive than prescriptive. Specifically, the authors describe the correlation between length increasing and standard PPO training without providing the underlying reason for the phenomenon. Although they propose several heuristic-inspired remedies, the problem is not fully resolved. Therefore, it might not directly contribute to improving the existing RLHF method."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper clearly documents the length-increasing issue in the RLHF pipeline. They conduct experiments on several datasets across different domains to demonstrate the issue."
                },
                "weaknesses": {
                    "value": "The paper is rather descriptive than prescriptive. Specifically, the authors describe the correlation between length increasing and standard PPO training without providing the underlying reason for the phenomenon. Although they propose several heuristic-inspired remedies, the problem is not fully resolved. Therefore, it might not directly contribute to improving the existing RLHF method."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8099/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808055297,
            "cdate": 1698808055297,
            "tmdate": 1699637003071,
            "mdate": 1699637003071,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "r5bG5QTjqq",
                "forum": "sNtDKdcI1f",
                "replyto": "0ndkXxe1Dh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8099/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8099/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PX4u"
                    },
                    "comment": {
                        "value": "Thanks for the thoughtful comments! We\u2019ll try to address concerns below: \n\n>The paper is rather descriptive than prescriptive\u2026 Although they propose several heuristic-inspired remedies, the problem is not fully resolved. Therefore, it might not directly contribute to improving the existing RLHF method.\n\nWe agree that the problem is not fully resolved. Length correlation is difficult to fully resolve since longer responses *are* in some cases more informative! So we do not believe it is possible to provide a crisp solution here without building new preference datasets. Instead, our main goal is to characterize the phenomenon and possible solutions. We explore \u201cheuristic-inspired remedies\u201d precisely because they are simple solutions that practitioners will reach for, and shed light on how different components of RLHF influence the observed patterns.\n\n> Specifically, the authors describe the correlation between length increasing and standard PPO training without providing the underlying reason for the phenomenon.\n\nWe believe that we do provide reasons. The initial length stratification and intervention-based experiments focus on studying how PPO optimizes reward, demonstrating that reward model biases are the main cause behind length increases in PPO. We then dedicate Section 4 to analyzing in depth the underlying reasons for those biases; we discover the main culprits to be imbalances in preference data, brittleness in the reward modeling objective, and \u201ceasy\u201d examples from our dataset cartography analysis."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8099/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700339782631,
                "cdate": 1700339782631,
                "tmdate": 1700339782631,
                "mdate": 1700339782631,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]