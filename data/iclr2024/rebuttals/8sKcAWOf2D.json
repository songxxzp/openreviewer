[
    {
        "title": "Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking"
    },
    {
        "review": {
            "id": "V0Bdp7WPo4",
            "forum": "8sKcAWOf2D",
            "replyto": "8sKcAWOf2D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission439/Reviewer_AnWX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission439/Reviewer_AnWX"
            ],
            "content": {
                "summary": {
                    "value": "This paper examines the impact of fine-tuning on the internal computations of language models through a case study. Specifically, the authors investigate the entity tracking mechanism of Llama-7B and its fine-tuned versions. The authors argue that the performance enhancements resulting from fine-tuning can be attributed to the improved ability of attention heads to handle positional information.\nThey utilize Desiderata-based Component Masking (DCM) to confirm that the entity tracking mechanism and the functionality of a subset of attention heads remain the same in Llama-7B and its fine-tuned variants. Additionally, they introduce CrossModel Activation Patching (CMAP) to reveal the improved mechanisms of attention heads."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is generally well-written and easy to follow.\n\n2. The authors provide an explanation for the performance enhancement observed in the fine-tuned model, focusing on the entity tracking circuits."
                },
                "weaknesses": {
                    "value": "1. The methods employed in this paper are limited to a single type of model on an entity tracking dataset. Moreover, the entity tracking mechanism is likely just one of many contributing factors, raising questions about the generalizability of their claims.\n\n2. I personally do not see the significance of the entity tracking problem to be particularly high."
                },
                "questions": {
                    "value": "Why is it justified to employ attention mechanisms to delve into the entity-tracking circuit? I acknowledge there may be some associations between them, but are these connections truly substantial?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission439/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698735990668,
            "cdate": 1698735990668,
            "tmdate": 1699635970434,
            "mdate": 1699635970434,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VXfzFkgBN2",
                "forum": "8sKcAWOf2D",
                "replyto": "V0Bdp7WPo4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission439/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission439/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer AnWX"
                    },
                    "comment": {
                        "value": "Thanks for the review! We\u2019re glad that you found that paper clear and easy to follow. We plan to answer your questions with an updated version of the paper soon, but we wanted to address a few of them quickly here.\n\n*Note: Kindly review the manuscript for information regarding references.*\n***\n\n**The methods employed in this paper are limited to a single type of model on an entity tracking dataset. Moreover, the entity tracking mechanism is likely just one of many contributing factors, raising questions about the generalizability of their claims.**\n- Although we have focused on the specific task of entity tracking, the methods employed and introduced in this work are generic and can be easily applied to other tasks and models, by generating relevant data. Both path patching and DCM have already been applied to other tasks like indirect object identification and variable binding in arithmetic expression (Wang et. al (2022), Davies et. al (2023)). Additionally, CMAP can also be employed among various kinds of models that share the same dimensionality.\n- While we have meticulously analyzed the impact of fine-tuning on the entity tracking mechanism, we acknowledge that understanding whether such mechanism invariance is universal would require further studies involving additional tasks and models. *We have updated our discussion section to clarify this limitation.*\n***\n\n**I personally do not see the significance of the entity tracking problem to be particularly high.**\n- We understand not all researchers might be interested in the same topics, but there is a significant community of researchers who are interested in both entity tracking and name binding: it has been a significant area of study in not just AI, but also in Linguistics and Cognitive Neuroscience (Karttunen et. al (1976), Heim et. al (1983), Nieuwland et. al (2006), Kamp et. al (2010), Marcus et. al (2018)). \n- For instance, Kamp et. al (2010) proposed multiple structures for discourse representation, with the fundamental atomic steps as binding names with their attributes defined in the context, making it crucial for understanding the discourse capabilities of LLMs. \n- Additionally, Marcus et. al (2018) argued that representing abstractions, instantiating variables with instances, and applying operations to those variables are indispensable to the human mind, which is often ignored in AI research.\n- While we are among the first to study the internal mechanism for the entity tracking task, multiple researchers have previously investigated such capabilities in various deep neural networks, particularly using probing classifiers (Li et. al (2021), Li et. al (2022), Kim et. al (2023)). This interest is current and shared by others; for example, there is another anonymous submission to ICLR (https://openreview.net/forum?id=zb3b6oKO77) that examines the name-binding capabilities from another perspective, indicating the current interest within the research community in this topic.\n - *We have also updated our manuscript to highlight these points to provide context on the significance of the entity tracking task.*\n***\n\n**Why is it justified to employ attention mechanisms to delve into the entity-tracking circuit?**\n\nTo discover the information flow circuit for the entity tracking task, path patching retraces the crucial components from the end. As some of the heads in the circuit are primarily attending to heads at other tokens, the information they pass on to later components mainly comes from the heads at other tokens, due to the autoregressive structure. Consequently, we use the attention pattern to locate additional components in the circuit.\n***\n\n**I acknowledge there may be some associations between them, but are these connections truly substantial?**\n\nWe are confident that associations between groups of attention heads in the identified circuit are substantial due to multiple pieces of evidence triangulating it.\n - First, with the path patching algorithm, we observe that these are the paths/connections which when substituted from corrupt run to clean run lead to the most degradation in output probability of correct token.\n - Second, when computing the faithfulness score of the identified circuit, we mean ablate all the heads except those in the circuit and yet observe a good performance on the entity tracking task, suggesting the paths/connections are indeed causally influential to how the output is generated.\n - Third, these paths/connections are not only substantial in a single base model (Llama-7B), but also in its fine-tuned versions, as can be concluded from the high faithfulness scores on these models.\n - Fourth, the cross-model activation patching experiment clearly shows that patching these paths/connections leads to improvement in the performance of the base model.\n - Finally, we would like to mention that all the techniques (Path patching, DCM, and CMAP) used in this work are causal, rather than correlational or observational, in nature."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission439/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699923060820,
                "cdate": 1699923060820,
                "tmdate": 1699923060820,
                "mdate": 1699923060820,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UiJqv8XTle",
                "forum": "8sKcAWOf2D",
                "replyto": "VXfzFkgBN2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission439/Reviewer_AnWX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission439/Reviewer_AnWX"
                ],
                "content": {
                    "title": {
                        "value": "Response read."
                    },
                    "comment": {
                        "value": "I have read the response to my questions and deciced to keep my rating."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission439/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633756935,
                "cdate": 1700633756935,
                "tmdate": 1700633756935,
                "mdate": 1700633756935,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tFlSncg7q8",
            "forum": "8sKcAWOf2D",
            "replyto": "8sKcAWOf2D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission439/Reviewer_fhvV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission439/Reviewer_fhvV"
            ],
            "content": {
                "summary": {
                    "value": "This work aims to answer the question: why fine-tuning language models (LMs) can enhance their performance on a range of tasks? As a case study, the authors experiment with LLaMA-7B and two fine-tuned versions, Vicuna-7B and Goat-7B, on the entity tracking task. They first apply Path Patching to extract the circuit, a subset of LM heads, from LLaMA-7B and find that all three models can reach high faithfulness scores with the circuit identified in Llama-7B, a.k.a., all three models share a similar circuit. Then, they apply Desiderata-based Component Masking (DCM) to identify LM heads responsible for specific functionality and find that each group of LM heads on all three models share the same functionality. Finally, they propose the Cross-Model Activation Patching (CMAP) to attribute the performance gain of fine-tuning to specific components, a.k.a., Value Fetcher, and Position Transmitter heads.\n\nThe work conducts extensive experiments to uncover the internal mechanisms of fine-tuning by applying two existing methods, Path Patching and DCM, and proposing one novel method, CMAP. The authors disclose some interesting experiment findings, e.g., the performance gain of fine-tuning attributes to the improved ability to handle positional information."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- A novel method CMAP for mechanistic interpretability.\n\n- The authors conducted extensive experiments by applying Path Patching, DCM, and their proposed CMAP to analyze the underlying mechanism of fine-tuning, which discloses several exciting findings, e.g., (1) The language model (LM) and its two fine-tuned versions share the same circuit. (2) The components of this circuit in these three models share the same functionality. (3) The performance gain of fine-tuning attributes to the improved ability to handle positional information.\n\n- The presentation is clear, although it requires the reader to have some background of mechanistic interpretability."
                },
                "weaknesses": {
                    "value": "- The experiment results can not lead to the claim that the original model and its fine-tuned versions implement entity tracking with the \u201csame\u201d circuit: (1) the fine-tuned model, Goat-7B, reaches an accuracy of 82% while the circuit identified in Llama-7B reaches an accuracy of 68%; this considerable performance gap indicates that Goat-7B\u2019s circuit may be different from Llama-7B\u2019s circuit, although these two circuits may have considerable overlap. (2) It is necessary to explore the mentioned overlap. For example, could authors apply Path Patching to Vicuna-7B and Goat-7B to extract their circuits and compute the overlap between their circuits and Llama-7B\u2019s circuit?\n\n- As mentioned in (Wang et al., 2022) [1], faithfulness is not sufficient to prescribe which circuits explain the behavior well. Why do not authors show the completeness and minimality scores, similar to (Wang et al., 2022) [1]? \n\n- In Section 4.2: CIRCUIT EVALUATION, the expression F(Cir \\ K) - F(Cir \\ (K U {v})) / F(Cir \\ (K U {v})) in the Minimality paragraph seems wrong. Moreover, the authors mention that they filter out the heads that contribute less than 0.5% of the functionality defined by subset K. This description is inconsistent with the above expression since the denominator of the equation is the functionality defined by the remaining nodes after removing K and v.\n\n[1] Wang, Kevin, et al. \"Interpretability in the wild: a circuit for indirect object identification in gpt-2 small.\" arXiv preprint arXiv:2211.00593 (2022)."
                },
                "questions": {
                    "value": "- What is the motivation to divide heads into four groups (A, B, C, D) instead of three or five? In other words, since authors iteratively identify groups of heads with high direct effects on each other using the path patching score, why did authors end with four groups instead of other numbers?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission439/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772771940,
            "cdate": 1698772771940,
            "tmdate": 1699635970345,
            "mdate": 1699635970345,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ogcr2Iweuo",
                "forum": "8sKcAWOf2D",
                "replyto": "tFlSncg7q8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission439/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission439/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer fhvV [1/3]"
                    },
                    "comment": {
                        "value": "Thanks for the review! We\u2019re glad that you found the paper clear and the experiments extensive. We have incorporated your suggestions in our revised version, but also wanted to respond to your questions here:\n***\n\n**The experiment results can not lead to the claim that the original model and its fine-tuned versions implement entity tracking with the \u201csame\u201d circuit: (1) the fine-tuned model, Goat-7B, reaches an accuracy of 82% while the circuit identified in Llama-7B reaches an accuracy of 68%; this considerable performance gap indicates that Goat-7B\u2019s circuit may be different from Llama-7B\u2019s circuit, although these two circuits may have considerable overlap. (2) It is necessary to explore the mentioned overlap. For example, could authors apply Path Patching to Vicuna-7B and Goat-7B to extract their circuits and compute the overlap between their circuits and Llama-7B\u2019s circuit?**\n\n-   We acknowledge that while the performance of the circuit identified in Llama-7B on Goat-7B is satisfactory, its faithfulness score is slightly lower than that of the circuit on Llama-7B, suggesting that the circuit employed by Goat-7B might have a few additional components. Hence, we applied path patching (as suggested) on the Goat-7B model to recover its circuit.\n    \n-   We found that similar to the circuit identified in Llama-7B, the Goat-7B circuit comprises of four sets of attention heads, each exhibiting a similar information flow mechanism to that of the Llama-7B circuit. Using the minimality score, we pruned out redundant heads, making it a circuit with 76 attention heads, comparable to the previously identified Llama-7B circuit consisting of 75 heads.\n    \n-   We evaluate the performance of the Goat-7B circuit across models and compare its performance to the circuit found in Llama-7B (see the following table):\n    \n\n\t1.  Not only the performance of the Goat and Llama circuits in Goat-7B is similar (0.70 vs. 0.68), but their faithfulness scores are also comparable (0.85 vs. 0.83), indicating that the two circuits have roughly the same performance.\n    \n\t2.  The identified Goat circuit also performs well on other models. For instance, the Goat circuit could achieve the performance of the entire Vicuna-7B model, indicating that it is generalizable across models, similar to the Llama circuit.\n    \n\t3.  Both Goat and Llama circuits exhibit comparable performance across various models, suggesting that identifying a circuit in one model and applying it to another can yield results as effective as identifying the circuit directly within the target model itself.\n\n| Model      | Full-Model Accuracy | Goat Circuit Accuracy | Llama Circuit Accuracy | Random Circuit Accuracy   | Goat Circuit Faithfulness | Llama Circuit Faithfulness |\n|------------|------------|--------------|---------------|-------------------|--------------|---------------|\n| Llama-7B   |  0.66        |  0.57          |  0.60           |  0.005   |  0.86          |  0.91           |\n| Vicuna-7B  |  0.67        |  0.67          |  0.65           |  0.0342  |  1.00          |  0.96           |\n| Goat-7B    |  0.82        |  0.70          |  0.68           |  0.0044 |  0.85          |  0.83           |\n\n\n-   To understand it thoroughly, we computed the overlap between the attention heads of each of the four groups of heads in both circuits and found that there is a significant overlap between the heads of each of the four groups, as shown in the following table. We observe that more than half of the attention heads are shared between both circuits. We also found that most heads with high causal impact on the final output are shared between both circuits, based on Fig. 6 (Appendix D). \n\n| Head Group(s)     | #Heads in Llama Circuit | #Heads in Goat Circuit | Intersection | Precision   | Recall |\n|------------|------------|--------------|---------------|-------------------|--------------|\n| A   |  40        |  34          |  18           |  0.53    |  0.45          | \n| B  |  9        |  14          |  6           |  0.42  |  0.67          |\n| C   |  22        |  21          |  15           |  0.71  |  0.68          |\n| D   |  4        |  7          |  3           |  0.43  |  0.75          |\n| A + B   |  49        |  48          |  28           |  0.58  |  0.57          |\n\n\n- The findings from the previous table and Fig. 6 suggest that Llama-7B and Goat-7B have roughly the same circuit, with overlapping components that have a high causal impact on the final output. While some of the components are unique, their overall impact is minimal.\n    \n-   Please refer to Appendix D for more details."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission439/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671834368,
                "cdate": 1700671834368,
                "tmdate": 1700685372629,
                "mdate": 1700685372629,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dHnNCQ846w",
                "forum": "8sKcAWOf2D",
                "replyto": "tFlSncg7q8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission439/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission439/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer fhvV [2/3]"
                    },
                    "comment": {
                        "value": "**As mentioned in (Wang et al., 2022) [1], faithfulness is not sufficient to prescribe which circuits explain the behavior well. Why do not authors show the completeness and minimality scores, similar to (Wang et al., 2022) [1]?**\n\n**Minimality**\n-   Please note that we do use the minimality criterion from Wang et al. (2022), but instead of reporting the scores we are using it as an optimization criterion; A drawback of path patching is its inability to determine the optimal number of components to incorporate into the circuit. Consequently, there is a high probability that the initially assumed number of components constituting the circuit may contain redundancy. To address this challenge, we employ the minimality criterion to systematically remove redundant attention heads from our initial circuit; unlike Wang et al. (2022), who simply reported the minimality scores without explaining how did they decide on the number of heads to include in the circuit. \n    \n-   Specifically, we remove heads that contribute less than 0.5% of the functionality defined by their subcircuit (after removing respective subset K), thus reducing from 100 to 75 attention heads (as described in sec 4.2 in the paper).\n    \n-   Further, we are investigating the Llama-7B model which is significantly larger than GPT-2 small. As a result, we had to come up with a greedy approach to compute the subset K for each head in the circuit when computing their minimality score, instead of directly using all the heads in a specific group.\n\n**Completeness**\n-  Following your suggestion, we have added an evaluation with the completeness score from Wang et al. (2022) to Appendix B of the paper. We used the following two sampling methods, proposed in the original paper:\n\t1.  *Random*: We uniformly sample a group of circuit components.\n\t2.  *Circuit subgroups*: We define the subset to be one of four groups of the circuit: A, B, C, and D.\n\n- Results are reported in the following table and Fig. 5 (Appendix B). For the random sampling methods, we report the mean and standard deviation over 20 random subsets.\n\n|           | Full model                      | Circuit                       | Completeness                 |   \n|-----------|-----------------------|-----------------------|----------------------|\n| Random         |  0.166 \u00b1 0.028                 |  0.068 \u00b1 0.023                  |  0.098 \u00b1 0.037                 |\n| Group A        |  0.34                            |  0.004                          |  0.336                         |\n| Group B        |  0.152                           |  0.132                          |  0.012                         |\n| Group C        |  0.146                          |  0.144                          |  0.002                         |\n| Group D        |  0.612                         |  0.424                          |  0.188                         |\n\n\n-   When we measure completeness over the circuit, we obtain 0.09, which indicates that, for randomly selected components that have an impact on the task, only about 9% of the performance gap is unexplained by the circuit itself.  For groups B and C, the measurement is even lower, indicating highly complete identification of heads within those groups.  But for groups A and D we obtain 0.34 and 0.19, suggesting a lower level of completeness. Although the performance of the current circuit is quite good, this result suggests that including more heads in groups A and D can improve it further.\n    \n-   Please refer to Appendix B for more details."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission439/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671976782,
                "cdate": 1700671976782,
                "tmdate": 1700689743370,
                "mdate": 1700689743370,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jrDLeKJjA2",
            "forum": "8sKcAWOf2D",
            "replyto": "8sKcAWOf2D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission439/Reviewer_M52Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission439/Reviewer_M52Y"
            ],
            "content": {
                "summary": {
                    "value": "The authors investigate the effects of fine-tuning on circuit-level mechanisms within large language models (LLMs) using entity tracking as a focal point. Initially, they employ a path-patching technique to isolate circuits responsible , categorizing attention heads into four groups based on the characteristics inherent to entity tracking. By defining four distinct groups, they ascertain the consistency and effectiveness of these circuits across various models. To delve deeper into the functionality of each group, they outline three main desiderata: Object, Label, and Position. Through activation patching, they discern the role of each group and confirm the commonality of circuit functionality across all models using Desiderata-Based Component Masking. The final phase involves Cross-Model Activation Patching, a method requiring the overlay of activations from similar components of different models on identical inputs. This assists in elucidating how a math-fine-tuned model augments an existing circuit in a base model, leading to enhanced performance in entity tracking. Experiments on LLaMA-7B and its two fine-tuned variants, Vicuna-7B and Goat-7B, reinforce their conclusions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors effectively elucidate the impact of fine-tuning on the internal computations of large models, particularly with respect to entity tracking. This provides a deeper understanding of how fine-tuning influences model behavior. Utilizing path patching, they constructed four distinct groups of attention heads and enable a granular examination of the model's functionalities. Demonstrated consistency across different models, validating the universality of the identified circuits in performing the entity tracking task.\n2. In subsequent tests of the individual capabilities within the identified paths, three out of the four attention head groups exhibited similar functionalities.This substantiates the hypothesis that the circuits in fine-tuned models implement similar functionalities.\n3. Experimental results on DCM concerning the Positional Transmitter and Value Fetcher in Goat-7B, as opposed to the original LLaMA-7B, align well with the CAMP experiment. This consistent alignment between the experimental findings and the initial hypotheses strengthens the paper's credibility."
                },
                "weaknesses": {
                    "value": "1. The study's scope is limited to a single foundational model, raising concerns about the generalizability of the conclusions. Without further investigation across a broader spectrum of models, it's challenging to ascertain if the observed mechanisms are universally applicable.\n2. When establishing the Desiderata for identifying circuit functionality, the connection between the three tasks and their corresponding abilities remains ambiguously articulated. A clearer exposition of these relationships would have enhanced the clarity and rigor of the study.\n3. The section detailing the use of CMAP to patch activations from Goat-7B to Llama-7B lacks clarity, particularly when validating the impact of QKV on the model's performance. The rationale behind why patching the QK-circuit of the Value Fetcher and the value vector of the Position Transmitter heads results in the most significant enhancement is not well-explained."
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission439/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805495058,
            "cdate": 1698805495058,
            "tmdate": 1699635970270,
            "mdate": 1699635970270,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zXDZVkLUpd",
                "forum": "8sKcAWOf2D",
                "replyto": "jrDLeKJjA2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission439/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission439/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer M52Y"
                    },
                    "comment": {
                        "value": "Thanks for the review! We\u2019re glad that you found the paper insightful. We will incorporate your suggestions in an updated version of the paper, but we also wanted to address your comments here:\n***\n\n**The study's scope is limited to a single foundational model, raising concerns about the generalizability of the conclusions. Without further investigation across a broader spectrum of models, it's challenging to ascertain if the observed mechanisms are universally applicable.**\n\nThe methods and workflow employed in this work are generic and can be directly applied to a variety of settings. While we are among the very first to mechanistically study the effect of fine-tuning on LLMs, we have taken care to note in our Limitations section that understanding whether such circuit invariance is universal would require further investigations on additional tasks and models. Our research is intended as a first step, thus focusing on depth rather than breadth, and we believe that our finding that the circuit is invariant across fine-tuned models is surprising, even though observed in a single set of models, and opens the door to further research.\n***\n\n**When establishing the Desiderata for identifying circuit functionality, the connection between the three tasks and their corresponding abilities remains ambiguously articulated. A clearer exposition of these relationships would have enhanced the clarity and rigor of the study.**\n\nTo identify the functionality of various circuit components, we conceptualized and defined three desiderata:\n \n - *Object desideratum*: It is used to identify circuit components that are encoding the value of correct object information in their output. Consequently, when the output of these components is patched from the counterfactual to the original run, the final output of the original run changes to the correct object of the counterfactual example, as shown in Fig. 2(a).\n - *Label desideratum*: It is used to identify circuit components that are encoding the query box label value information. Hence, when their output is patched from the counterfactual to the original run, the final output of the original run changes to the object associated with the query box label, of the counterfactual, in the original example, as shown in Fig. 2(b).\n - *Position desideratum*: It is used to identify circuit components that encode the positional information of the correct object, i.e. when they are patched from counterfactual to the original run, the final output of the original run changes to the object in the original example which is at the same location as the correct object of the counterfactual example, as shown in Fig. 2(c).\n\nFor each of the three desiderata, we train a binary mask over the model components to identify the circuit components encoding the corresponding vital information to accomplish the task. *We will elucidate these details in the next version of the manuscript.*\n\n***\n**The section detailing the use of CMAP to patch activations from Goat-7B to Llama-7B lacks clarity, particularly when validating the impact of QKV on the model's performance. The rationale behind why patching the QK-circuit of the Value Fetcher and the value vector of the Position Transmitter heads results in the most significant enhancement is not well-explained.**\n\nAs described in Elhage et. al (2021), the QK (query-key) circuit and OV (output-value) circuit are two independent computations of an attention head. The QK circuit computes the attention pattern and the OV circuit computes the output.\n\nWe observe a significant performance gain when the QK-circuit of Value Fetcher heads are patched from Goat-7B to Llama-7B (Fig. 4(b)) which indicates that their capability to attend to the correct object using the positional information forwarded by the Position Transmitter heads has improved during fine-tuning. The computation of the QK-circuit depends on 1) Weight matrices (W_Q & W_K) and 2) their inputs, which in the case of the Value Fetcher heads are primarily constructed from the output of the Position Transmitter heads. Since we observe only a marginal performance increment when patching the output of the Position Transmitter heads across models (Fig. 4(a), purple bar), we attribute the improved capability of Value Fetcher heads to the transformation in the W_Q and W_K matrices of these heads, i.e. fine-tuning has refined these matrices, making them more adept at leveraging positional information to attend to the correct object token. On the other hand, in Position Transmitter heads, the value vector (hence OV-circuit) is responsible (Fig. 4(c)) for the increase of performance, suggesting that their contribution to the performance gain can be attributed to the information encoded in their outputs, which we know from DCM experiments is primarily positional information.\n\n*We will elucidate these details in the next version of the manuscript.*"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission439/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228184742,
                "cdate": 1700228184742,
                "tmdate": 1700228184742,
                "mdate": 1700228184742,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jv3ckhZWpa",
                "forum": "8sKcAWOf2D",
                "replyto": "zXDZVkLUpd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission439/Reviewer_M52Y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission439/Reviewer_M52Y"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' response. I'd like to keep my score and look forward to the better version of the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission439/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665536586,
                "cdate": 1700665536586,
                "tmdate": 1700665536586,
                "mdate": 1700665536586,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]