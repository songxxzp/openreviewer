[
    {
        "title": "Fairness Metric Impossibility: Investigating and Addressing Conflicts"
    },
    {
        "review": {
            "id": "UzJnJqTlVO",
            "forum": "LIBZ7Mp0OJ",
            "replyto": "LIBZ7Mp0OJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1191/Reviewer_RKvV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1191/Reviewer_RKvV"
            ],
            "content": {
                "summary": {
                    "value": "In this manuscript, the authors propose and present a many-objective optimization approach to fair ML, where models are optimized simultaneously for a standard performance metric (F1 score) as well as multiple fairness-motivated metrics. Standard tools from many-objective optimization are used (NSGA-III, an evolutionary algorithm) to solve the optimization problem, and the multi-dimensional fairness-accuracy Pareto frontiers resulting from the specified hyperparameter search space are analyzed.\n\nThroughout the manuscript, particular emphasis is placed on incompatibilities between different fairness metric: are two metrics indeed conflicting, and if yes, how strongly? The authors propose a way to quantify the strength of the association between two metrics in terms of Pareto set hypervolume contrasts.\n\nThe authors apply their method to three different model structures - XGBoost, Random Forests, and shallow MLPs - on five standard (tabular) algorithmic fairness datasets. They show a range of visualizations of the Pareto frontiers as well as the conflicts (or compatibilities) between all optimization objectives. They conclude with a call for multi-objective optimization-based exploration of potentially conflicting fairness objectives as a standard analysis step in fair ML."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is very well written and easy to follow. \n\nThe idea is simple, intuitive, and widely applicable, and the visualizations are very helpful in understanding the extent of practical conflicts between different fairness objectives. \n\nThe case studies, while limited to tabular data and rather low-dimensional models, showcase the broad applicability of the proposed method.\n\nStandard methodology is used throughout, and I would expect the results to be relatively simple to reproduce."
                },
                "weaknesses": {
                    "value": "## Many-objective optimization as the solution to fairness metric conflicts\nI am generally very receptive to the idea of using MOO in fairML, both for interactive exploration and visualization of trade-offs, as well as for training and selecting the final model. I am, however, concerned about the broader framing adopted in the paper.\n\nThe manuscript begins by acknowledging the well-known fairness impossibility theorems. The authors then proceed to make statements such as:\n- \"We open the door to further socio-technical research on effectively combining the complementary benefits of different notions of fairness\"\n- \"Rather than resisting these criticisms, we embrace them, agreeing that FairML approaches that over-simplify the complex and socio-technical nature of the FairML problem actually risk doing more social harm than good.\"\n- \"It is essential to recognize and treat fairness metrics as potentially conflicting objectives.\"\n\nThe authors then proceed to take a more or less arbitrary selection of fairness metrics and apply them indiscriminately to five different datasets.\n\nHowever, I would contest that this is the right approach to dealing with these impossibility statements (of which there are less than is often believed*). There is a reason why, for instance, demographic parity and equalized odds are fundamentally incompatible: *they are asking for completely different things*. They are also applicable (=usually considered ethically desirable) in completely different scenarios. There are no \"complementary benefits\" of enforcing these simultaneously; I cannot think of a real situation where one would want a little bit of both. Enforcing statistical parity in a medical scenario will give you a classifier that predicts breast cancer on 50% of men and 50% of women, even though women have a much higher disease incidence, and provide no benefit at all.\n\nThe way to address incompatibility results is, in my opinion, not to mash all of them together and hope that the resulting model will be somewhat \"fair\" according to all metrics; it is to understand and reflect on the *reasons* why certain metrics are incompatible, and to deliberate with stakeholders and practitioners which fairness conception would appear to be the right one for a given application scenario.\n\nWith all that said, I still believe that MOO can be a very useful tool! I can certainly imagine it being very useful for interactive explorations of various trade-offs between multiple metrics that have been selected as indeed desirable in a certain application. I would, however, suggest that the authors place more emphasis on *understanding* the different fairness metrics, and actively deciding on the appropriate ones for a given application, before embarking on fully-automated many-objective optimization of all of them simultaneously. \n\nIn this regard, I would also suggest choosing a more realistic combination of fairness metrics for the application scenarios, or at least a cursory justification of the ones selected here. (Also note in this regard that DEOP is a subset of DEOD, and selecting both of them as simultaneous but separate optimization objective appears a bit odd.)\n\nMany of the experimental results might also make more sense if interpreted based on knowledge about what these metrics mean. For instance, the harms caused by enforcing statistical parity depend on the magnitude of the prevalence differences between the different groups in the dataset (Zhao and Gordon, 2022). This might explain some of the differences observed between the five datasets, such as in Fig. 2. (The group-stratified prevalences are currently not given in the manuscript, so this is hard to assess right now.) Similarly, it might be expected that DDSP and INVD do not seem to be in conflict, since they probably optimize for something very similar. (I could not figure out what exactly INVD does - what is \"m\" in table 2 in the appendix, and what are the sets being summed over?)\n\n*Equalized odds and calibration by groups and AUROC fairness are all compatible in principle, see e.g. Lazar Reich and Vijaykumar (2020) or Petersen et al. (2023). They are all at odds with statistical parity, which simply does not make any sense in any predictive scenario and is fully incompatible with any predictive performance-based fairness notion (Zhao and Gordon, 2022). They are also in conflict with PPV/NPV equality, which simply does not make sense to ask for in the case of prevalence differences. For a predictive performance fairness setting, I am not aware of any meaningful fundamental impossibility statements between different actually desirable fairness metrics.\n\n## Prior work and significance of contributions\nFairness-related trade-offs and Pareto frontiers have already received significant attention in the literature; cf., e.g., Cooper et al. (2021), Islam et al. (2021), Martinez et al. (2020),  Rodolfa et al. (2021), Wei and Niethammer (2021), Yu et al. (2020).\n\nWhat does the present study contribute to this already quite large body of literature? I would venture to say: a useful, practical tool, as well as appropriate metrics, for exploring such trade-offs in a given dataset. That is most certainly a useful contribution (even though probably limited to rather small and simple cases with low-dimensional models?), but then I would recommend discussing this prior work more extensively, and clarifying the contributions of the present manuscript in this regard.\n\nFinally, I am not entirely certain about the topical fit of this piece for ICLR, seeing that the manuscript is not at all focused on representation learning. (Fairness constraints will, of course, affect the learned representations. However, these learned representations are also not assessed in any way in the study, and, again, only low-dimensional, tabular case studies are considered.)\n\n## References\n- Cooper et al. (2021), Emergent Unfairness in Algorithmic Fairness-Accuracy Trade-Off Research, https://dl.acm.org/doi/abs/10.1145/3461702.3462519\n- Islam et al. (2021), Can we obtain fairness for free?, https://dl.acm.org/doi/abs/10.1145/3461702.3462614\n- Lazar Reich and Vijaykumar (2020), A Possibility in Algorithmic Fairness: Can Calibration and Equal Error Rates Be Reconciled?, https://drops.dagstuhl.de/opus/volltexte/2021/13872\n- Martinez et al. (2020), Minimax Pareto Fairness: A Multi Objective Perspective, http://proceedings.mlr.press/v119/martinez20a.html\n- Petersen et al. (2023), On (assessing) the fairness of risk score models, https://arxiv.org/pdf/2302.08851.pdf\n- Rodolfa et al. (2021), Empirical observation of negligible fairness\u2013accuracy trade-offs in machine learning for public policy, https://www.nature.com/articles/s42256-021-00396-x\n- Wei and Niethammer (2021), The fairness-accuracy Pareto front https://onlinelibrary.wiley.com/doi/full/10.1002/sam.11560\n- Yu et al. (2020), Keeping Designers in the Loop: Communicating Inherent Algorithmic Trade-offs Across Multiple Objectives, https://doi.org/10.1145/3357236.3395528\n- Zhao and Gordon (2022), Inherent Tradeoffs in Learning Fair Representations, https://jmlr.org/papers/volume23/21-1427/21-1427.pdf"
                },
                "questions": {
                    "value": "--"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1191/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1191/Reviewer_RKvV",
                        "ICLR.cc/2024/Conference/Submission1191/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1191/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767441639,
            "cdate": 1698767441639,
            "tmdate": 1700662996153,
            "mdate": 1700662996153,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vlGSHb6DiJ",
                "forum": "LIBZ7Mp0OJ",
                "replyto": "UzJnJqTlVO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> I am generally very receptive to the idea of using MOO in fairML, both for interactive exploration and visualization of trade-offs, as well as for training and selecting the final model. I am, however, concerned about the broader framing adopted in the paper.\n\nWe thank you for this incredibly valuable feedback regarding the groundedness of our work with respect to the fairness incompatibility theorem. We would like to point out that ManyFairHPO is a general framework that should be specifically applied to different fair machine learning scenarios. In other words, we do not propose that practitioners optimize for an indiscriminate set of fairness metrics, and choose a model that is relatively \u201cfair\u201d in terms of all of them. On the contrary, we believe that the involvement of domain experts in the discussion of which fairness metrics to optimize for is crucial, and should also be accompanied with discussion of what the risks are if one notion of fairness is satisfied by violating another relevant notion. With that being said, we acknowledge that we should be clearer about how ManyFairHPO should be used in practice, and, we have updated our methodology to include a more careful deliberation on how ManyFairHPO should be applied in practice (see Figure 1 and Section 4). We have also provided an exemplary use-case to show the efficacy of our approach in providing solutions in the face of many conflicting fairness objectives and conflict related risks (Section 6.3). We believe that our modified and extended approach clarifies that each FairML problem is categorized by a unique set of objectives and risks, and the MaO problem formulation we propose is extremely useful in such scenarios. To be clear, we do not seek to automate the FairML process, but provide a socio-technical approach that allows practitioners to make informed model selection in FairML problems that are categorized by social objectives that span across multiple conflicting notions of fairness.."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1191/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699952831055,
                "cdate": 1699952831055,
                "tmdate": 1700500178881,
                "mdate": 1700500178881,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "imVe0IKEyn",
                "forum": "LIBZ7Mp0OJ",
                "replyto": "UzJnJqTlVO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Fairness-related trade-offs and Pareto frontiers have already received significant attention in the literature; cf., e.g., Cooper et al. (2021), Islam et al. (2021), Martinez et al. (2020), Rodolfa et al. (2021), Wei and Niethammer (2021), Yu et al. (2020).\nWhat does the present study contribute to this already quite large body of literature? I would venture to say: a useful, practical tool, as well as appropriate metrics, for exploring such trade-offs in a given dataset. That is most certainly a useful contribution (even though probably limited to rather small and simple cases with low-dimensional models?), but then I would recommend discussing this prior work more extensively, and clarifying the contributions of the present manuscript in this regard.\n\nYes, as you said! We appreciate your suggestion to clarify the contribution of our work. We will include your suggestions into our related work section. As we have emphasized in our general reply, we are the first work that treats fairness metrics as conflicting objectives, and thoroughly evaluates this methodology."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1191/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699952875782,
                "cdate": 1699952875782,
                "tmdate": 1699952875782,
                "mdate": 1699952875782,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8aNmlGse2m",
                "forum": "LIBZ7Mp0OJ",
                "replyto": "UzJnJqTlVO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1191/Reviewer_RKvV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1191/Reviewer_RKvV"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their quite substantial rebuttal and revision of the paper!\n\nI really like the new section 4.0.1 + Fig.1 / the revised framing in terms of domain knowledge-driven deliberation for picking fairness metrics; I feel like this works a lot better now and provides a much more thorough and grounded discussion.\n\n**As a result, I am increasing my score.** The paper is well written, and the results, experiments and analyses are quite comprehensive. I do think that this adds something useful to the literature, even though a few questions currently remain open (see below).\n\nA few more things that could still be improved:\n1) There is still no discussion of _why_ specific fairness criteria might be incompatible. In particular, I still believe that a (short) discussion of different base rate differences across the different datasets as a potential explanation of some of the observed fairness conflicts w.r.t. DDSP would add an essential piece of the puzzle. \n2) In regards to table 2, there are two more or less obvious questions: i) Why are the conflicts so much stronger in some models compared to others, and ii) why are the conflict matrices so asymmetric, e.g. in the COMPAS/RF, Adult/RF, Adult/XGB cases? It would also be very helpful to add information about the predictive performance of the models in the table. E.g. NNs are known to under-perform on tabular data; could that somehow be related to the fact that we see fewer conflicts for the NN?\n3) Could Fig. 4 maybe be augmented with a second panel that shows trade-offs w.r.t. performance? Currently, it looks a bit odd that the the black model is selected and not one of those in the bottom left corner that dominate this model in terms of fairness w.r.t. both shown criteria. I suspect this is due to model performance, but this is currently not visible in the graph.\n4) To gain some more trust in the method, it would also be useful to add one or two more standard baseline methods w.r.t. a single fairness criterion (e.g. the standard postprocessing method of Hardt et al., Equality of opportunity in supervised learning, and maybe also the equalized odds version of EGR), in order to demonstrate that the MOO framework indeed recovers optimal Pareto bounds in most cases."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1191/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662934829,
                "cdate": 1700662934829,
                "tmdate": 1700662934829,
                "mdate": 1700662934829,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Nbojc2LTQB",
            "forum": "LIBZ7Mp0OJ",
            "replyto": "LIBZ7Mp0OJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1191/Reviewer_k3oE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1191/Reviewer_k3oE"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the trade-offs among different fairness objectives in fair machine learning. Motivated by the well-known impossibility results among fairness metrics in fair ML, the authors adopted a many-objective optimization (MaO) perspective to formulate the problem of ML with multiple fairness objectives. Their main result included a ManyFairHPO framework that enables model designers to specify multiple fairness objectives, and utilizes hyperparamter optimization to select a ML model attaining desirable trade-offs among the specified objectives. On several datasets, the authors first provided new empirical evidence for the necessary trade-offs among fairness metrics in ML, then applied the ManyFairHPO framework to simultaneously consider multiple fairness metrics in the ML model. They argued that their framework is effective at reaching a superior fairness balance compared to the conventional approach of ML with a single fairness metric."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper provided a systematic approach for understanding and mitigating the potential conflicts among multiple fairness objectives. The \u2018Contrast\u2019 measure gives a convenient way to visualize the differences between the fairness-accuracy trade-offs under different fairness requirement. The experiment considered a broad set of datasets and ML models. In addition, the proposed framework is compatible with existing ML tool (scikit-learn) and fair ML library (IBM aif360)."
                },
                "weaknesses": {
                    "value": "The problem of incorporating multiple fairness objectives is well recognized in the fair ML community, and the adopted method based on multi-objective optimization is also conventional. When one wishes to consider multiple fairness goals, it is a natural attempt to apply multi-objective optimization methods to include all of them. Although there is certainly value in working on the technical details and running experiments to validate performance, I see limited novelty and depth in the proposed ManyFairHPO framework special and useful.   \n\nAnother concern is that using multiple fairness objectives makes the task of model interpretation and selection more difficult. The proposed framework can give more information about whether two fairness objectives are conflicting or not, but it is not as helpful for handling the more fundamental task of what combination of the candidate fairness objectives should be used. Since there are always trade-offs among different fairness goals, instead of trying to optimize all of them, one may wish to examine the more important goals and be more selective with which fairness definitions to include. The paper may benefit from highlighting the need for caution when applying the new framework."
                },
                "questions": {
                    "value": "1.\tThe experiments only considered the fair ML methods available from IBM aif360 library. To consider other fairness definitions or fair ML methods, how to achieve that with the proposed framework? \n\n2.\tWhat is the use of hyperparameter optimization in the ManyFairHPO framework? Are conventional approaches to hyperparameter tuning sufficient, or are special techniques requires to handle the new multi-objective setup?\n\n3.\tAre there practical evidence that decision makers will prefer to simultaneously optimize multiple types of fairness, rather than separately study each fairness definition and select the better one?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1191/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788004861,
            "cdate": 1698788004861,
            "tmdate": 1699636045421,
            "mdate": 1699636045421,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ptf2xmhnYu",
                "forum": "LIBZ7Mp0OJ",
                "replyto": "Nbojc2LTQB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> The problem of incorporating multiple fairness objectives is well recognized in the fair ML community, and the adopted method based on multi-objective optimization is also conventional. When one wishes to consider multiple fairness goals, it is a natural attempt to apply multi-objective optimization methods to include all of them. Although there is certainly value in working on the technical details and running experiments to validate performance, I see limited novelty and depth in the proposed ManyFairHPO framework special and useful.\n\nDespite the natural application of multi-objective optimization, we point out that optimizing for multiple notions of fairness as conflicting objectives has not been thoroughly evaluated in the literature. The closest case of this is [1], who argue that their framework is extensible to multiple fairness metrics, but do not include any experiments. In addition to our validation of the performance of this methodology, we also are working on contextualizing it in a theoretical framework that can allow practitioners to use it in real case scenarios. This represents an additional contribution to our paper.\n\nIn addition to the distinction pointed out above, we have focused on the re-framing of ManyFairHPO. In order to do so, we have reformulated our methodology (Figure 1 and Section 4) to provide a start-to-finish framework from fairness metric selection and conflict-related risk identification to Many-Objective model selection, going even deeper into this socio-technical problem than the previously mentioned work [1].\n\n[1] https://www.amazon.science/publications/multi-objective-multi-fidelity-hyperparameter-optimization-with-application-to-fairness"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1191/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699952412719,
                "cdate": 1699952412719,
                "tmdate": 1700500054454,
                "mdate": 1700500054454,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s9WnoQiQQC",
                "forum": "LIBZ7Mp0OJ",
                "replyto": "Nbojc2LTQB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Another concern is that using multiple fairness objectives makes the task of model interpretation and selection more difficult. The proposed framework can give more information about whether two fairness objectives are conflicting or not, but it is not as helpful for handling the more fundamental task of what combination of the candidate fairness objectives should be used. Since there are always trade-offs among different fairness goals, instead of trying to optimize all of them, one may wish to examine the more important goals and be more selective with which fairness definitions to include. The paper may benefit from highlighting the need for caution when applying the new framework.\n\nThank you for your input regarding the inherent challenge of selecting fairness metrics. We would argue a middle ground, that practitioners should still be very selective on which fairness metrics to use for their given problem, but should also be open to the fact that problem-specific social objectives might span across multiple notions of fairness. \nWe have emphasized the role of domain expert driven deliberation in ManyFairHPO (Figure 1) to clarify our framework does not propose to simply select all metrics and select a model that is somewhat fair in terms of all notions. We have also been more selective in our experiments, providing a case study (Section 6.3) that exemplifies how specific FairML problems have conflicting fairness objectives and conflict-related risks."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1191/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699952489288,
                "cdate": 1699952489288,
                "tmdate": 1700500078180,
                "mdate": 1700500078180,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kgc6MiluMu",
                "forum": "LIBZ7Mp0OJ",
                "replyto": "Nbojc2LTQB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> The experiments only considered the fair ML methods available from IBM aif360 library. To consider other fairness definitions or fair ML methods, how to achieve that with the proposed framework?\n\nIncorporating other fairness metrics into the ManyFairHPO framework is quite straightforward from a technical perspective. We decided to use IBM aif360 due to its popularity in the fairness community. However, end-users are encouraged to incorporate any fairness metrics that fit their specific FairML problem."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1191/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699952558131,
                "cdate": 1699952558131,
                "tmdate": 1699952558131,
                "mdate": 1699952558131,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qdASdxjm5d",
                "forum": "LIBZ7Mp0OJ",
                "replyto": "Nbojc2LTQB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> What is the use of hyperparameter optimization in the ManyFairHPO framework? Are conventional approaches to hyperparameter tuning sufficient, or are special techniques requires to handle the new multi-objective setup?\n\nThank you for your question regarding the use of HPO in our framework. In order to go from the MO setting to the MaO setting (three or more objectives) we applied NSGA-III, an extension of the common NSGA-II algorithm that incorporates the concept of reference directions in order to encourage equal exploration of all objectives. However, we note that NSGA-III is a classic HPO method from 2014; our problem formulation thus does not require novel multi-objective optimization algorithms."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1191/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699952718681,
                "cdate": 1699952718681,
                "tmdate": 1699952718681,
                "mdate": 1699952718681,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7KTs76MaeP",
                "forum": "LIBZ7Mp0OJ",
                "replyto": "Nbojc2LTQB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Are there practical evidence that decision makers will prefer to simultaneously optimize multiple types of fairness, rather than separately study each fairness definition and select the better one?\n\nWe thank you for your inquiry regarding practical evidence that motivates the demand for our approach. The short answer is that practical evidence is limited due to the lack of approaches that optimize for multiple notions of fairness as conflicting objectives. However, our motivation for this methodology came from considering specific scenarios where multiple conflicting fairness notions might be of value (e.g. statistical parity vs. individual fairness in University admissions) and where conflicts between these notions might produce unwanted risks (e.g. self-fulfilling prophecy from positive discrimination). We believe that if decision makers want to mitigate these risks, they will strongly consider the prospect of optimizing for multiple notions of fairness."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1191/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699952754337,
                "cdate": 1699952754337,
                "tmdate": 1699952754337,
                "mdate": 1699952754337,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gNkrZOcouw",
                "forum": "LIBZ7Mp0OJ",
                "replyto": "Nbojc2LTQB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1191/Reviewer_k3oE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1191/Reviewer_k3oE"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the detailed responses"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for the detailed response to my questions. I recognize and appreciate the authors' effort towards proposing a framework to allow multiple fairness perspectives to be considered simultaneously, but I am unconvinced of its value as yet another framework to including fairness requirement into ML. What might strengthen the paper's position is to apply the framework on a realistic decision problem and gather relevant stakeholders' feedback on whether and how this ManyFairHPO framework is superior to more convenient alternatives."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1191/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700447089727,
                "cdate": 1700447089727,
                "tmdate": 1700447089727,
                "mdate": 1700447089727,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1BMM1iww6Y",
            "forum": "LIBZ7Mp0OJ",
            "replyto": "LIBZ7Mp0OJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1191/Reviewer_XwAL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1191/Reviewer_XwAL"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors discuss challenges in Fairness-aware Machine Learning (FairML), highlighting that optimizing for a single fairness objective can lead to neglect of other important fairness criteria. To address this, the authors introduce ManyFairHPO, a new many-objective hyper-parameter optimization framework, aimed at balancing multiple fairness objectives and exploring various fairness-accuracy trade-offs. The results on five real-world datasets are present."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper addresses the complexities of machine learning fairness by considering multiple, potentially conflicting notions of fairness.\n\n2.\t The authors have conducted extensive evaluations of their proposed framework across multiple real-world datasets. This comprehensive testing not only demonstrates the framework's versatility and adaptability to different contexts and applications but also adds credibility and robustness to the presented results.\n\n3.\tThe incorporation of visual representations effectively communicates the experimental results, making it easier for readers to comprehend the performance and benefits of the proposed method."
                },
                "weaknesses": {
                    "value": "1.\tWhile the authors propose optimizing model fairness through the Pareto frontier and simultaneous measurement of multiple fairness indicators, they do not provide a theoretical demonstration of how trading off one fairness metric for another could lead to an overall improvement in model fairness. A deeper theoretical exploration in this area could strengthen the paper, offering clearer guidelines on how to navigate fairness trade-offs effectively.\n\n2.\tThe paper lacks theoretical analysis on how to select among different Pareto-optimal outcomes, especially when one fairness metric  is already at its optimal is one of the Pareto-optimal outcomes, i.e., there is no difference from a single optimization outcome. A theoretical framework or set of criteria for making these choices would be beneficial, providing practitioners with a robust method for decision-making in situations with multiple optimal fairness solutions.\n\n3.\tThe authors only use one performance to evaluate the model performance. In the context of FairML, where the applications are intricate and multifaceted, relying on a single performance metric may not sufficiently capture the model\u2019s overall performance and impact. A diverse set of performance metrics would provide a more holistic view, ensuring a balanced and thorough evaluation.\n\n4.\tIn the experimental section, the authors have not conducted comparisons with existing fairness algorithms. Integrating benchmark comparisons against state-of-the-art fairness algorithms would significantly enhance the paper. It would offer tangible evidence of the proposed method's performance and effectively position the ManyFairHPO framework within the existing FairML research landscape."
                },
                "questions": {
                    "value": "In the paper, the authors propose optimizing model fairness through the Pareto frontier by measuring multiple fairness indicators simultaneously. I appreciate it if the authors could provide more theoretical insights or guidelines on how trading off one fairness metric for another could lead to an overall improvement in model fairness? Specifically, how should practitioners approach situations where improving one aspect of fairness might lead to a decrease in another, and how can they ensure that these trade-offs result in a net positive impact on model fairness?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1191/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813698409,
            "cdate": 1698813698409,
            "tmdate": 1699636045346,
            "mdate": 1699636045346,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EI3lMlGJnE",
                "forum": "LIBZ7Mp0OJ",
                "replyto": "1BMM1iww6Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 1. While the authors propose optimizing model fairness through the Pareto frontier and simultaneous measurement of multiple fairness indicators, they do not provide a theoretical demonstration of how trading off one fairness metric for another could lead to an overall improvement in model fairness. A deeper theoretical exploration in this area could strengthen the paper, offering clearer guidelines on how to navigate fairness trade-offs effectively.\n\nThank you for your insight regarding a theoretical framework on effectively navigating fairness trade-offs. We are actively working on an intended use case which should clear up how MaO model selection works in the context of multiple fairness metrics, conflicts, and conflict-related risks. Regarding your comment regarding overall fairness, we would like to provide a clarification that trading off one fairness metric for another doesn\u2019t lead to an overall improvement in fairness, but rather aims to mitigate the risks of fairness metric incompatibility."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1191/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699952125336,
                "cdate": 1699952125336,
                "tmdate": 1699952598693,
                "mdate": 1699952598693,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W28jtHSXbr",
                "forum": "LIBZ7Mp0OJ",
                "replyto": "1BMM1iww6Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">  2. The paper lacks theoretical analysis on how to select among different Pareto-optimal outcomes, especially when one fairness metric is already at its optimal is one of the Pareto-optimal outcomes, i.e., there is no difference from a single optimization outcome. A theoretical framework or set of criteria for making these choices would be beneficial, providing practitioners with a robust method for decision-making in situations with multiple optimal fairness solutions.\n\nWe thank you for this extremely valuable insight, and align that our MaO scenario makes model selection especially difficult to imagine. We have elected to incorporate a clarification into our central methodology, and have added a new Many-Objective Model Selection step to ManyFairHPO (Section 4.3), which details the use of objective weight scalarization to transform differing preferences towards multiple fairness metrics into a concrete model selection decision. We also provide a concrete example of our model selection framework on the Lawschool Admissions problem, exemplifying how ManyFairHPO can be used on complex real-world FairML problems with multiple fairness objectives and risks (Section 6.3)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1191/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699952227268,
                "cdate": 1699952227268,
                "tmdate": 1700499846310,
                "mdate": 1700499846310,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jZ8lWlxsyj",
                "forum": "LIBZ7Mp0OJ",
                "replyto": "1BMM1iww6Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 4. In the experimental section, the authors have not conducted comparisons with existing fairness algorithms. Integrating benchmark comparisons against state-of-the-art fairness algorithms would significantly enhance the paper. It would offer tangible evidence of the proposed method's performance and effectively position the ManyFairHPO framework within the existing FairML research landscape.\n\nThank you for your constructive feedback to include a comparison to specially designed bias-mitigation techniques. We have included a comparison of our MO Pareto Fronts to bias mitigation techniques in Appendix Figure 8, where we postprocess high-accuracy hyperparameter configurations with the SOTA in-processing method Exponentiated Gradient Reduction (EGR). We find that ManyFairHPO is competitive with EGR, finding dominating solutions in many scenarios. Although our study is not concerned with the specific performance of bias-mitigation methods (and rather the overarching problem formulation best suited to FairML problems), we believe that this result validates the performance of ManyFairHPO and motivates the importance of HPO in fairness-aware ML applications."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1191/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699952295908,
                "cdate": 1699952295908,
                "tmdate": 1700499917919,
                "mdate": 1700499917919,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qke8X0Qw1z",
                "forum": "LIBZ7Mp0OJ",
                "replyto": "1BMM1iww6Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> The authors only use one performance to evaluate the model performance. In the context of FairML, where the applications are intricate and multifaceted, relying on a single performance metric may not sufficiently capture the model\u2019s overall performance and impact. A diverse set of performance metrics would provide a more holistic view, ensuring a balanced and thorough evaluation.\n\nWe thank you again for your suggestion. We find it crucial (as in any ML problem) to select appropriate performance metrics that effectively capture the utility of the algorithm in its specific application domain. While we emphasize in newly updated Section 4.1 that performance metric selection is a critical decision (and many different performance metrics can be applied), we maintain that performance-metric trade-offs are not the central focus of our paper, and open the door for future study in this direction. There is also a clear connection between fairness metrics and different aspects of performance (as many fairness metrics can be derived from false positives rate (FPR), false negatives rate (FNR), etc.), and we suggest instead that the fairness-specific impact of models be determined by carefully selecting fairness metrics and thoroughly considering their trade-offs. Having said that, our improved approach now can be viewed as a general one in which many objectives (performance and fairness metrics) can be used with careful consideration by the domain experts."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1191/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500008167,
                "cdate": 1700500008167,
                "tmdate": 1700500008167,
                "mdate": 1700500008167,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U8WgcHINDi",
            "forum": "LIBZ7Mp0OJ",
            "replyto": "LIBZ7Mp0OJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1191/Reviewer_bcZi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1191/Reviewer_bcZi"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed ManyFairHPO framework, a many-objective (MaO) hyper-parameter optimization (HPO) approach to study and balance the tradeoff between different fairness metrics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and organized.\n2. The paper is well-motivated.\n3. Experiments are comprehensive."
                },
                "weaknesses": {
                    "value": "1. The biggest problem of the paper is the lack of novelty and technical contributions. The proposed ManyFairHPO does not show any technical improvement *adapted to fairness* compared to the prior work on multi-objective optimization or hyperparameter optimization. \n2. The authors mentioned constrained optimization (CO) approaches in the related work. However, such a baseline is missing in the empirical evaluation. It would be better to compare CO with MaO empirically and provide more insights into the pros and cons of both approaches."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1191/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698993370767,
            "cdate": 1698993370767,
            "tmdate": 1699636045267,
            "mdate": 1699636045267,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LKe8AAVInv",
                "forum": "LIBZ7Mp0OJ",
                "replyto": "U8WgcHINDi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 1. The biggest problem of the paper is the lack of novelty and technical contributions. The proposed ManyFairHPO does not show any technical improvement adapted to fairness compared to the prior work on multi-objective optimization or hyperparameter optimization.\n\nThank you for your constructive criticism regarding the contribution of this work. We have addressed this matter in our 'general response' statement; kindly refer to it for details. We made significant changes to our methodology section, which we detail as follows: We have focused our efforts on grounding our approach to real-world applications, and providing a start-to-finish framework to guide practitioners in the use of our approach. ManyFairHPO now contains guidelines on how to select and prioritize fairness metrics and identify conflict-related risks (Section 4.1), and ultimately make model selection decisions (Section 4.3) that successfully incorporate a complex set of fairness objectives and risks. We also provided a case study (Section 6.3) for real-world applications in order to guide practitioners in the effective use of our approach."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1191/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699951968878,
                "cdate": 1699951968878,
                "tmdate": 1700499786644,
                "mdate": 1700499786644,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u1hB7L5s9n",
                "forum": "LIBZ7Mp0OJ",
                "replyto": "U8WgcHINDi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1191/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 2. The authors mentioned constrained optimization (CO) approaches in the related work. However, such a baseline is missing in the empirical evaluation. It would be better to compare CO with MaO empirically and provide more insights into the pros and cons of both approaches.\n\nThank you for your comment regarding the lack of CO baselines in our work. Our line of thinking for not including such baselines was that CO and MO fairness approaches are designed for separate scenarios, and the best choice depends on whether an appropriate set of fairness metrics, meaningful constraints, and achievable values are known ad-hoc. Regardless, the pros and cons of MaO vs. CO, especially in the presence of multiple fairness objectives/constraints is an interesting research question.\nWe would appreciate your input and thoughts on how we can follow up with experiments in this regard to compare with CO methods (we would also appreciate references to available CO methods, as the two CO papers we cited in our related work section have no public code available)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1191/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699952017477,
                "cdate": 1699952017477,
                "tmdate": 1699952615206,
                "mdate": 1699952615206,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]