[
    {
        "title": "A Unified Framework for Reinforcement Learning under Policy and Dynamic Shifts"
    },
    {
        "review": {
            "id": "RCcdTysKk5",
            "forum": "fTiU8HhdBD",
            "replyto": "fTiU8HhdBD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3398/Reviewer_6QE6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3398/Reviewer_6QE6"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a unified framework to address distributional shifts induced by policy changes and/or dynamic variations. This is done by firstly leveraging the concept of Transition Occupancy Distribution (TOD), which augments the state-action occupancy distribution with the next states (which \u201caccounts\u201d for the dynamics of the MDP). Then, the work develops a surrogate policy learning objective based on the TOD, and further reformulates such objective in a minimax optimization problem, ensuring that all terms are tractable. Finally, the work proposes a practical learning algorithm, OMPO, which implements this learning objective as an actor-critic paradigm. The work presents experiments in several continuous control environments, showing superior performance in all settings that accounts for policy or dynamic distributional shifts."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The work approaches the challenge of handling distributional shifts in RL, which is very relevant and spawn different active areas of research. Therefore, it is well motivated.\n\n- The proposed framework is very sound and elegant. It unifies and generalizes several challenges from Reinforcement Learning, often approached by different RL subareas (off-policy/offline RL, meta-RL, multi-task RL, sim-to-real transfer), into a single and generic learning objective.\n    - It also empirically shows that OMPO can provide superior performance to several algorithms that are specialized in a single type of distributional shift, which provides strong evidence of the effectiveness from the theoretical framework and practical algorithm.\n\n- The experimental setup is very complete. It brings all combinations of shifts (stationary environment, domain adaptation and non-stationary environments), which progressively cover all scenarios of shifts. For each case, it compares against solid and recent baselines. It also provides visualizations of the transition occupancy distributions (which explicit the problems addressed in the paper), as well as ablations for the main hyperparameters in OMPO.\n\n- The Appendices are also rich and improve the clarity of the paper. For instance, they detail the experimental setup, hyperparameters, provide pseudocode, contrast with prior DICE methods, etc. Hence, the work looks very reproducible and mature."
                },
                "weaknesses": {
                    "value": "While I do not have major concerns, I believe the paper could be improved in some directions, as detailed below:\n\n- I believe the proof of Proposition 3 could be clearer and more didactic with a better explanation on the assumptions and on why some steps are taken. Perhaps starting with an initial \u201crationale\u201d behind the proof (describing the strategy to be followed) would be helpful. \n    - In fact, this could be also extended to the Section 4.2, which would help clarify some of the steps taken to arrive in the tractable learning objective (see my question below).\n\n- First, I believe the work provided sufficient empirical evidence to support the proposed framework. Nevertheless, one question remains: does OMPO scale for harder problems? For instance, does it work in meta-RL benchmarks such as Meta-World ML10, ML45? If not, why?\n\n- Following the previous point, the paper does not well describe the limitations of the proposed technique. It only states the challenges of tuning the buffer size but does not bring more practical information. For instance, I am curious to know about the stability of the method and how sensible it is for other learning parameters (such as those on Table 2). Additionally, it would be interesting to describe the computational resources needed to run the method for the presented benchmarks.\n\n\n**Minor Concerns:**\n\nIn Section 3, while describing the MDP, I believe the paper refers to the initial state distribution by two different symbols ($\\mu_0$ and $\\rho_0$). Based on the rest of the paper, I believe that the $\\rho_0$ should be replaced with $\\mu_0$. \n\n\n**Summary of the Review:**\n\nThe work provides a strong theoretical contribution by providing a framework that unifies addressing policy and dynamics shifts. The empirical part also provides a good support to the presented algorithm. The raised concerns are minor, and I recommend acceptance. Nonetheless, I am also stating medium confidence, as I do not have profound familiarity with DICE-related literature."
                },
                "questions": {
                    "value": "- In the beginning of Section 4.2, the work motivates the dual reformulation due to the presence of the distribution induced by the current policy in the historical dynamics (which is unknown). Then, the paper includes the Bellman flow constraint, arriving at equations 7-8, which still depends on this unknown distribution. Could you please better justify this step? From my understanding, this seems needed to arrive at the tractable objective in Equation 12, but it is unclear if there is another justification.\n\n- Figure 3: Would it not be possible to combine Domain Randomization with DARC? In some environments, DARC is a stronger baseline than SAC, and their combination could match or outperform OMPO-DR.\n\n\n\n======================================== **POST-REBUTTAL** =========================================\n\n\nAfter carefully checking other reviews and authors\u2019 responses for all reviews, I understand that the paper improved during the rebuttal in many directions and, personally, addressed almost all of my concerns. The paper improved in clarity (more didactic and transparent in the proofs, besides new discussions) and richness in the experimental methodology (new ablations, longer runs).\n\nI still believe that it is not clear whether OMPO would scale for more complex distributional shifts and harder environments. Nevertheless,  considering the proposed scope of this paper, I do not believe this is a major concern, as the current experiments are satisfactory to validate the proposed method. I am more confident that this paper is ready for acceptance. Therefore, I am raising my confidence (3 -> 4)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3398/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3398/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3398/Reviewer_6QE6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3398/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698591753199,
            "cdate": 1698591753199,
            "tmdate": 1700653173699,
            "mdate": 1700653173699,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hBpeItbwRB",
                "forum": "fTiU8HhdBD",
                "replyto": "RCcdTysKk5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6QE6"
                    },
                    "comment": {
                        "value": "Thank you for your recognition and valuable suggestions. We sincerely appreciate your positive comments on the value and the results of our work. We carefully answer each of your concerns as below.\n\n> **W1&Q1: A better explanation of Proposition 3; More detail of Section 4.2; Justify the include of Bellman flow constraint.**\n\nThanks for your suggestions. We apologize for the over brief discussion of theoretical derivation in Section 4.2, due to the length limits of the paper.\n\n* To facilitate better understanding the proof of Proposition 3, we have included a sketch of the theoretical flow for the theory of Section 4.2 in Appendix A.\n* We have provided the definition and the application of Fenchel Conjugate--the initial \u201crationale\u201d of Proposition 3--in Appendix A.3 in our revision.\n* Regarding the introduction of Bellman flow constraint:\n  * Yes, we incorporate this constraint to ensure the tractability of optimization, a technique inspired by previous DICE works [1, 2, 3]. The Bellman flow constraint is essentially the definition of occupancy measure $d$, which enforces its behavior to satisfy its definition and thus will not change the original optimization problem. \n  * The key benefit of introducing this constraint is that, if we write the corresponding constraint problem into its Lagrangian dual form (Eq.(21) in the Appendix), and apply Fenchel conjugate transformation (Eqs. (19) and (22)), we can use the change of variable trick similar to DualDICE [4] and AlgaeDICE [5] (Eq.(23)) to get rid of the non-obtainable $\\rho_{\\widehat{T}}^{\\pi}$ and convert the problem to a tractable form that only involves sampling from $\\rho_{\\widehat{T}}^{\\widehat{\\pi}}$ (Eq.(24)).\n\n\n\n> **W2: First, I believe the work provided sufficient empirical evidence to support the proposed framework. Nevertheless, one question remains: does OMPO scale for harder problems? For instance, does it work in meta-RL benchmarks such as Meta-World ML10, ML45? If not, why?**\n\nThanks for your suggestions. Meta-World ML10, and ML45 are meta-learning tasks, which involve multiple tasks with different rewards, such as button press and drawer open. Our proposed OMPO is a single-task RL algorithm rather than a multi-task algorithm, thus is not directly applicable to these tasks. Nevertheless, we think it is possible to use OMPO as a backbone algorithm to extend it to the multi-task/meta-learning setting. We will explore it in our future works.\n\n\n\n> **W3: the stability of the method and how sensible it is for other learning parameters (such as those on Table 2). Additionally, it would be interesting to describe the computational resources needed to run the method for the presented benchmarks.**\n\n* In our ablation, we have investigated the parameters, the local buffer size $\\mathcal{D}_L$ and the weighted factor $\\alpha$. \n* To further address the reviewer's concern, we have conducted ablations on the order $q$ of conjugate function in Figure 21, Appendix F.7 in our revision. The results show that $q\\in[1.2,2]$ can yield satisfactory performance, with $q=1.5$ showing superior results in our experiments.\n* Regarding the computation performance, although looks complex, OMPO actually does not have a high computational cost, its training time is at a similar level as SAC. We have provided computing infrastructure and training time in Appendix G in the revision.\n\n\n\n> **W4: Based on the rest of the paper, I believe that the $\\rho_0$ should be replaced with $\\mu_0$.**\n\nThanks for your kindly reminder. We have corrected the typo, by replacing the $\\rho_0$ as the $\\mu_0$.\n\n\n\n> **Q2: Figure 3: Would it not be possible to combine Domain Randomization with DARC? In some environments, DARC is a stronger baseline than SAC, and their combination could match or outperform OMPO-DR.**\n\nThank you for the valuable suggestion. We have conducted the experiment of DARC-DR within domain adaption tasks, which is included in Figure 3 in our revision. The results show that, the combination with domain randomization can indeed improve the performance of DARC, but OMPO-DR can still outperform DARC-DR by a large margin, indicating its superior performance. Here are the brief results of convergence performance in four tasks.\n\n| Domain Adaption | Hopper | Walker2d | Ant    | Humanoid |\n| --------------- | ------ | -------- | ------ | -------- |\n| OMPO-DR         | $2985$ | $5356$   | $4032$ | $6738$   |\n| DARC-DR         | $1743$ | $1435$   | $3187$ | $4166$   |\n| OMPO            | $2289$ | $2648$   | $3792$ | $6154$   |\n| DARC            | $894$  | $1587$   | $1261$ | $958$    |\n\n---\n\nWe would like to thank you again for the constructive comments from the reviewer. We hope our responses and refined paper can fully address your remaining concerns. If the reviewer has any further questions or comments, we are happy to address them."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233522297,
                "cdate": 1700233522297,
                "tmdate": 1700235482763,
                "mdate": 1700235482763,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AGKYHegSAk",
                "forum": "fTiU8HhdBD",
                "replyto": "RCcdTysKk5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Reference**\n\n[1] Versatile offline imitation from observations and examples via regularized state-occupancy matching. ICML 2022.\n\n[2] Mind the gap: Offline policy optimization for imperfect rewards. ICLR 2023.\n\n[3] OptiDICE: Offline Policy Optimization via Stationary Distribution Correction Estimation. ICML 2021.\n\n[4] Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. NeurIPS 2019.\n\n[5] Algaedice: Policy gradient from arbitrary experience. arXiv 2019."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233568957,
                "cdate": 1700233568957,
                "tmdate": 1700233568957,
                "mdate": 1700233568957,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a8pEOWNmGU",
                "forum": "fTiU8HhdBD",
                "replyto": "RCcdTysKk5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Please let us know if we have resolved your concerns"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe were wondering if our responses have resolved your concerns. We hope that these clarifications and details can serve to enhance your confidence in the ratings of our work. If you have any further questions or require additional insights, we are glad to provide any necessary information to support your assessment.\n\nBest wishes!\n\nThe Authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565274100,
                "cdate": 1700565274100,
                "tmdate": 1700565274100,
                "mdate": 1700565274100,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wqvxwtNhHv",
                "forum": "fTiU8HhdBD",
                "replyto": "a8pEOWNmGU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Reviewer_6QE6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Reviewer_6QE6"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for addressing my concerns"
                    },
                    "comment": {
                        "value": "Thank you for your responses and actions. I believe most of my concerns were addressed and I am going to raise my confidence level. Please refer to the updated version of the review."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652910242,
                "cdate": 1700652910242,
                "tmdate": 1700652910242,
                "mdate": 1700652910242,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "44dmORkyHc",
            "forum": "fTiU8HhdBD",
            "replyto": "fTiU8HhdBD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3398/Reviewer_9mJF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3398/Reviewer_9mJF"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a unified framework to tackle diverse settings of policy and dynamic shifts by performing transition occupancy matching, leading to a surrogate policy learning objective that can be cast into a tractable min-max optimization problem through employing dual formulation. Moreover, this paper conducts extensive experiments to demonstrate the efficacy of the proposed method under different policy and dynamic shifts."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed unified framework performs consistently well in diverse settings with different policy and dynamic shifts. Notably, the authors claim they use the same set of hyperparameters for all experiments in the paper, making the results rather impressive.\n2. The paper tackles the issue of policy and dynamic shifts, an important problem in deploying RL policy in real-world applications. \n3. The paper is clearly written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The experimental evaluations under the Stationary environments setting can be improved. For example, 1M environment steps are not usually enough when evaluating on the `Humanoid` task. I would suggest the authors provide the results of their OMPO when training for more than 2.5M environment steps and at least compare with SAC on `Walker2d`, `Ant`, and `Humanoid`.\n\n2. The authors should provide more intuition to explain why their OMPO outperforms the baseline methods under the Stationary environments setting. Is it a consequence of incorporating $R(s, a, s')$ into the training loss? Do the authors also employ the double-Q technique for OMPO or only use a single Q?\n\n3. The pseudo-codes provided in Algorithm 1 should provide more training details. For example, calculating Eqs (23) and Eqs (25) requires sampling $s_0$ from the distribution $\\mu_0$. How does the proposed method perform this operation exactly? I suggest the authors provide more training details, at least in the appendix."
                },
                "questions": {
                    "value": "1. The proposed OMPO enjoys a low variance across different random seeds in terms of performance given stationary environments, as shown in Figure 2. Can the author provide some insights into this phenomenon? \n\n2. Eqs (23) and Eqs (25) minimize the $Q(s, a)$ specifically for $s\\sim\\mu_0$. Since $Q$ is parameterized by a neural network, the  $Q(s, a), s\\sim\\mu_0$ can be minimized spuriously low. How do the authors combat this potential training stability?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3398/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3398/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3398/Reviewer_9mJF"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3398/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698721141124,
            "cdate": 1698721141124,
            "tmdate": 1699636290785,
            "mdate": 1699636290785,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DcidsugL1f",
                "forum": "fTiU8HhdBD",
                "replyto": "44dmORkyHc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9mJF"
                    },
                    "comment": {
                        "value": "Thanks for your comments and suggestions. We have provided the experimental results with longer training steps, and added details in our revision. We hope our response can resolve your concerns.\n\n> **W1: The experimental evaluations under the Stationary environments setting can be improved for 2.5M.**\n\nThanks for your suggestion. We have provided the experimental results with 2.5M environment steps in Figure 18, Appendix F.5 in our revision. We compare OMPO with SAC and TD3 through Hopper, Walker2d, Ant and Humanoid tasks. The results demonstrate that, OMPO exhibits significantly better sample efficiency and competitive convergence performance.\n\n\n\n> **W2: The authors should provide more intuition to explain why their OMPO outperforms the baseline methods under the Stationary environments setting. Is it a consequence of incorporating $R(s, a, s')$ into the training loss? Do the authors also employ the double-Q technique for OMPO or only use a single Q?** \n\n> **Q1: The proposed OMPO enjoys a low variance across different random seeds in terms of performance given stationary environments, as shown in Figure 2. Can the author provide some insights into this phenomenon?**\n\n* OMPO outperforms the baselines under the Stationary environments because it can more effectively handle policy shifts. Note that in stationary environments where $\\widehat{T}=T$, objective (5) reduces to:\n\n\n$\\widehat{\\mathcal{J}}(\\pi)=\\mathbb{E}\\_{(s,a,s')\\sim\\rho^\\pi_T}\\left[\\log r(s,a)-\\alpha\\log(\\rho^\\pi_T/\\rho^\\widehat{\\pi}_T)\\right]-\\alpha D_f\\left(\\rho^\\pi_T\\Vert\\rho^\\widehat{\\pi}_T\\right)$\n\n\n$=\\mathbb{E}\\_{(s,a)\\sim\\rho^\\pi}\\left[\\log r(s,a)-\\alpha\\log(\\rho^\\pi/\\rho^\\widehat{\\pi})\\right]-\\alpha D_f\\left(\\rho^\\pi\\Vert\\rho^\\widehat{\\pi}\\right)$\n\n\n$=\\mathbb{E}\\_{(s,a)\\sim\\rho^\\pi}[\\log r(s,a)]-\\alpha\\left[D_{KL}(\\rho^\\pi\\Vert\\rho^\\widehat{\\pi})+D_f(\\rho^\\pi\\Vert\\rho^\\widehat{\\pi})\\right]$\n\nwhich essentially regularizes the discrepancy between on-policy occupancy $\\rho^{\\pi}$ and the occupancy induced by off-policy samples $\\rho^\\widehat{\\pi}$ from the replay buffer, which helps to alleviate potential instability caused by off-policy learning [1,2]. We have provided a more detailed discussion about the performance improvement in Appendix B in our revised paper.\n\n* Yes, we also used the double-Q technique for OMPO, same as TD3 and SAC.\n\n\n\n> **W3: The pseudo-codes provided in Algorithm 1 should provide more training details. For example, calculating Eqs (23) and Eqs (25) requires sampling $s_0$ from the distribution $\\mu_0$. How does the proposed method perform this operation exactly? I suggest the authors provide more training details, at least in the appendix.**\n\nThank you very much for this valuable suggestion! To deal with $s_0\\sim\\mu_0$ in Eqs (23) and Eqs (25), we adopt an initial-state buffer to store initial state $s_0$ similar to the implementation of previous DICE works [3,4]. When calculating Eqs (23) and Eqs (25), we can sample $s_0$ from the initial-state buffer. We have provided this detail in Appendix C in our revision.\n\n> **Q2: Eqs (23) and Eqs (25) minimize the $Q(s, a)$ specifically for $s\\sim\\mu_0$. Since $Q$ is parameterized by a neural network, the $Q(s, a), s\\sim\\mu_0$ can be minimized spuriously low. How do the authors combat this potential training stability?**\n\nIn our empirical experiments, we did not observe any numerical issue with minimizing $Q$ with $s\\in \\mu_0$. It should be noted that in many existing DICE-based offline RL algorithms [3, 4, 5, 6, 7], the same $\\min \\mathbb{E}_{s\\sim \\mu_0} Q(s,a)$ term is also used, and no notable computation issue is reported. We think in the online setting, the possible instability issue is even less severe than in the offline case, as the online interactive environment will randomly reset after each episode ends, potentially providing an even broader distribution for $\\mu_0$.\n\n---\n\nThank you again for your review and comments. We hope the above responses can address your concerns and we are happy to have further discussion."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233750067,
                "cdate": 1700233750067,
                "tmdate": 1700234186004,
                "mdate": 1700234186004,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fgCjJ0rtQL",
                "forum": "fTiU8HhdBD",
                "replyto": "44dmORkyHc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Reference**\n\n[1] Off-policy policy gradient with state distribution correction. Arxiv. 2019\n\n[2] State Regularized Policy Optimization on Data with Dynamics Shift. NeurIPS. 2023.\n\n[3] Algaedice: Policy gradient from arbitrary experience. arXiv preprint. 2019\n\n[4] Versatile offline imitation from observations and examples via regularized state-occupancy matching. ICML. 2022\n\n[5] Demodice: Offline imitation learning with supplementary imperfect demonstrations. ICLR 2022.\n\n[6] Mind the gap: Offline policy optimization for imperfect rewards. ICLR 2023.\n\n[7] OptiDICE: Offline Policy Optimization via Stationary Distribution Correction Estimation. ICML 2021."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233817602,
                "cdate": 1700233817602,
                "tmdate": 1700233817602,
                "mdate": 1700233817602,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XH07jVZd86",
                "forum": "fTiU8HhdBD",
                "replyto": "44dmORkyHc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We sincerely look forward to your reply."
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe appreciate your comments. We were wondering if our responses and revision have resolved your concerns since only two days are left for the discussion phase. We have conducted experiments with longer environmental steps, and revised our implementation details according to your suggestions. If you have any other questions, we are also pleased to respond. We sincerely look forward to your response.\n\nBest wishes!\n\nThe authors."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700566083834,
                "cdate": 1700566083834,
                "tmdate": 1700566083834,
                "mdate": 1700566083834,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RF6o7buQGe",
                "forum": "fTiU8HhdBD",
                "replyto": "44dmORkyHc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Awaiting your valuable feedback before deadline"
                    },
                    "comment": {
                        "value": "Dear Reviewer 9mJF,\n\nWe sincerely appreciate the time and effort you have dedicated to reviewing our work, especially during this busy period. As we approach the final 24 hours of the discussion stage, we kindly seek your feedback on our responses.\n\nIn our response, we have provided the experimental results with 2.5M environment steps, clarified the performance improvement of OMPO in stationary environments, detailed the implementation of OMPO. Should you have any further questions or require additional insights, please do not hesitate to contact us. We are fully committed to supplying any necessary information to support your assessment.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660668913,
                "cdate": 1700660668913,
                "tmdate": 1700660668913,
                "mdate": 1700660668913,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "izNBdosGXB",
            "forum": "fTiU8HhdBD",
            "replyto": "fTiU8HhdBD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3398/Reviewer_kdm5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3398/Reviewer_kdm5"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a unified framework, called Occupancy-Matching Policy Optimization (OMPO), for reinforcement learning under policy and dynamics shifts. The authors identify the challenges posed by these shifts and propose a surrogate policy learning objective that captures the transition occupancy discrepancies. They then formulate the objective as a tractable min-max optimization problem through dual reformulation. The proposed method is evaluated on benchmark environments and compared with several baselines, demonstrating its superior performance in all settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper addresses an important problem in reinforcement learning, namely handling policy and dynamics shifts, which are common in real-world scenarios.\n- The proposed OMPO framework provides a unified strategy for online RL policy learning under diverse settings of policy and dynamics shifts. The derivation of the algorithm is clear and comprehensive.\n- The experimental results demonstrate that OMPO outperforms specialized baselines in various settings, showcasing its effectiveness in handling policy and dynamics shifts."
                },
                "weaknesses": {
                    "value": "1. The proposed implementation is complicated. It introduces modules such as estimating the density ratio $\\rho_T^\\pi\\left(s, a, s^{\\prime}\\right) / \\rho_{\\widehat{T}}^{\\widehat{\\pi}}\\left(s, a, s^{\\prime}\\right)$ and performing min-max optimization. This can make the training unstable.\n2. There is a lack of discussions on some related papers. See Q2.\n3. The experiments are not thorough enough. See Q3 and Q4."
                },
                "questions": {
                    "value": "1. In the related work, why do algorithms that modify the reward function require policy exploration in the source domain can provide broad data coverage? Is it due to the likelihood ratio that serves as the reward modification term? But OMPO also uses the ratio term and requires that the denominator is larger than zero.\n2. Papers [1,2] also deals with the issue of dynamics shift and should be included as related works. What is the advantage of OMPO compared with these two algorithms?\n3. Regarding the experiments, the change in environment parameters is limited. For example, the gravity in the target dynamics is only twice larger than that in the source dynamics. Is it possible to evaluate the algorithms with a more severe shift in dynamics?\n4. How are the experiment settings related to policy shifts? It seems that all changes are made in environment parameters and related to dynamic shifts.\n\n[1] Xue Z, Cai Q, Liu S, et al. State Regularized Policy Optimization on Data with Dynamics Shift. arXiv preprint arXiv:2306.03552, 2023.\n\n[2] Cang C, Rajeswaran A, Abbeel P, et al. Behavioral priors and dynamics models: Improving performance and domain transfer in offline rl. arXiv preprint arXiv:2106.09119, 2021."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3398/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3398/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3398/Reviewer_kdm5"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3398/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809075716,
            "cdate": 1698809075716,
            "tmdate": 1699636290696,
            "mdate": 1699636290696,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "auTDYDb2KL",
                "forum": "fTiU8HhdBD",
                "replyto": "izNBdosGXB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kdm5 (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments. We have conducted experiments and made modifications in our paper to address your concerns. We hope this improves our paper.\n\n> **W1: The proposed implementation is complicated. It introduces modules such as estimating the density ratio $\\rho_T^\\pi\\left(s, a, s^{\\prime}\\right) / \\rho_{\\widehat{T}}^{\\widehat{\\pi}}\\left(s, a, s^{\\prime}\\right)$ and performing min-max optimization. This can make the training unstable.**\n\n- First, the instantiation of OMPO actually is not very complicated, the only additions in OMPO as compared to typical actor-critic frameworks are a local replay buffer and a distribution discriminator. \n- Second, learning an extra discriminator for policy learning is actually quite common in methods like GAIL [1] as well as lots of DICE-based RL/IL methods [2,3,4,5]. To estimate the density ratio $\\rho_T^\\pi\\left(s, a, s^{\\prime}\\right) / \\rho_{\\widehat{T}}^{\\widehat{\\pi}}\\left(s, a, s^{\\prime}\\right)$ stably, we extend the implementation of GAIL [1] from $\\rho(s,a)$ to $\\rho(s,a,s')$, whose training stability has been well verified.\n- Empirically, we find the benefit of correcting policy/dynamics-shift using the extra discriminator significantly outweighs the possible instability in the discriminator. As shown in our main results in the paper, OMPO is stable and obtains superior performance, and in most cases, achieves even lower variance during policy learning.\n\n\n\n> **Q1: In the related work, why do algorithms that modify the reward function require policy exploration in the source domain can provide broad data coverage? Is it due to the likelihood ratio that serves as the reward modification term? But OMPO also uses the ratio term and requires that the denominator is larger than zero.**\n\n- We appologise for this potentially inaccurate statement in the related work. The correct sentence should be \"**However, these methods often require the source domain dynamics can cover the target domain dynamics, ...**\". Reward modification method like DARC [6] has an assumption that every transition with non-zero probability in the target domain will have non-zero probability in the source domain (i.e., $P_{target}(s'|s,a)>0 \\Rightarrow P_{source}(s'|s,a)>0$, Eq. (1) in the DARC paper), as the computing the reward correction term $\\Delta r(s,a,s')$ involves evaluating $\\log(P_{target}(s'|s,a)/P_{source}(s'|s,a))$, hence require the source dynamics to be sufficiently stochastic to \"cover\" transitions in the target domain. This is also discussed in the Limitations section in the DARC paper.\n- Note that this problem is less severe in OMPO, as it is performing transition occupancy matching between source and target domain (i.e., between $\\rho^{\\pi}\\_T$ and $\\rho^{\\hat{\\pi}}\\_{\\hat{T}}$), which reflect long-term stationary data distribution, rather than matching at the transition dynamics level (i.e., between $P_{target}(s'|s,a)$ and $P_{source}(s'|s,a)$), thus is less restrictive.\n- We have revised this sentence in the related work in our revised paper. Thank you for pointing out this inaccurate statement.\n\n\n\n> **W2&Q2: Papers [1,2] also deals with the issue of dynamics shift and should be included as related works. What is the advantage of OMPO compared with these two algorithms?**\n\n* Thanks for the suggested papers. We have discussed them in Related Works in our revision.\n* Our proposed OMPO differs from these works, as OMPO provides a unified framework that addresses not only dynamics shifts, but also policy shifts as well as their combination. Particularly, our proposed surrogate learning objective Eq. (5) and its dual reformulation make OMPO a versatile solution that can adapt to various settings of shift scenarios."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234090321,
                "cdate": 1700234090321,
                "tmdate": 1700234286451,
                "mdate": 1700234286451,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xq3JPrsIDn",
                "forum": "fTiU8HhdBD",
                "replyto": "izNBdosGXB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kdm5 (2/2)"
                    },
                    "comment": {
                        "value": "> **W3&Q3: Regarding the experiments, the change in environment parameters is limited. For example, the gravity in the target dynamics is only twice larger than that in the source dynamics. Is it possible to evaluate the algorithms with a more severe shift in dynamics?**\n\n* We thank the reviewer's valuable suggestion. In our original paper, we set the environmental parameters comparable to those in the baseline papers ($0.5\\sim2$ times the original parameters). Besides, our considered dynamics shifts encompass multifaceted variations, such as simultaneous changes in wind and gravity.\n* To fully address the reviewer's concern, we have conducted additional experiments in Non-stationary environments with a broader range of $0.5\\sim3$ times gravity changes. We have included the results in Figures 19 and 20, Appendix F.6 in our revision. The results show, even with a more severe shift in dynamics, OMPO can still achieve much better performance than the baselines. Here are the brief results on the average return of the Ant and Humanoid tasks in non-stationary dynamics.\n\n|                                  | Ant    | Humanoid |\n| -------------------------------- | ------ | -------- |\n| OMPO ($0.5\\sim2$ times gravity)  | $2133$ | $3185$   |\n| CEMRL ($0.5\\sim2$ times gravity) | $1278$ | $1546$   |\n| OMPO ($0.5\\sim3$ times gravity)  | $1950$ | $2848$   |\n| CEMRL ($0.5\\sim3$ times gravity) | $1265$ | $1256$   |\n\n\n\n> **W3&Q4: How are the experiment settings related to policy shifts? It seems that all changes are made in environment parameters and related to dynamic shifts.**\n\nActually, all our methods more or less involve policy shifts, since our method as well as most baselines are off-policy algorithms, the training data sampled from the replay buffer always have some gaps with the current on-policy distribution. In Figure 2, we specifically compared our method on stationary environments (no dynamics shift) against other mainstream off-policy RL algorithms like SAC, TD3 as well as the DICE-based method AlgeaDICE, and OMPO achieves consistently better performance due to better handling of the policy shifts. In Figure 7, we also visualized the distribution discrepancy caused by policy shifts in different training stages, which shows that even in the stationary environment, the impacts of policy shifts can be clearly observed.\n\n---\n\nWe hope the explanations and additional experiments could resolve the reviewer's concerns. If the reviewer have further questions, we are happy to address them.\n\n**Reference**\n\n[1] Generative adversarial imitation learning. NeurIPS. 2016.\n\n[2] Versatile offline imitation from observations and examples via regularized state-occupancy matching. ICML 2022.\n\n[3] Offline Goal-Conditioned Reinforcement Learning via f-Advantage Regression. NeurIPS 2022.\n\n[4] Demodice: Offline imitation learning with supplementary imperfect demonstrations. ICLR 2022.\n\n[5] Mind the gap: Offline policy optimization for imperfect rewards. ICLR 2023.\n\n[6] Off-Dynamics Reinforcement Learning: Training for Transfer with Domain Classifiers. ICLR 2021."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234193779,
                "cdate": 1700234193779,
                "tmdate": 1700235603622,
                "mdate": 1700235603622,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L1tWfhAQE7",
                "forum": "fTiU8HhdBD",
                "replyto": "izNBdosGXB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We sincerely look forward to your reply."
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe appreciate your comments. We were wondering if our responses and revision have resolved your concerns since only two days are left for the discussion phase. We hope our last reply and experiments have resolved all your concerns. If you have any other questions, we are also pleased to respond. We sincerely look forward to your response.\n\nBest wishes!\n\nThe authors."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565924056,
                "cdate": 1700565924056,
                "tmdate": 1700565924056,
                "mdate": 1700565924056,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LgmPTRvm2P",
                "forum": "fTiU8HhdBD",
                "replyto": "izNBdosGXB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Awaiting your valuable feedback before deadline"
                    },
                    "comment": {
                        "value": "Dear Reviewer kdm5,\n\nWe sincerely appreciate the time and effort you have dedicated to reviewing our work, especially during this busy period. As we approach the final 24 hours of the discussion stage, we kindly seek your feedback on our responses.\n\nIn our response, we have provided clarification on the implementation of OMPO and the existence of policy shifts in our experiments, discussed the recommended references, and conducted additional experiments on more severe dynamics shifts. Should you have any further questions or require additional insights, please do not hesitate to contact us. We are fully committed to supplying any necessary information to support your assessment.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660252619,
                "cdate": 1700660252619,
                "tmdate": 1700660252619,
                "mdate": 1700660252619,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IHlTTUtho8",
            "forum": "fTiU8HhdBD",
            "replyto": "fTiU8HhdBD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3398/Reviewer_UYp5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3398/Reviewer_UYp5"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, a policy learning algorithm is proposed, that is intended to learn from off policy observations, which also were collected in environments that are different from the target environment, and in particular in non-stationary environments.  \n\nIt is argued that differences between environments can be expressed via  the stationary (s,a,s') distribution, which extends the standard state-action occupancy distribution (s,a) by adding the next-sate s'.  A \"surrrogate\" objective is proposed, that involves averaging over the (s,a,s') distribution given in the data,\nwhich is intended to be analogous to the various DICE type off-policy methods (AlgaeDICE and others) where averaging is over the empirical occupancy (s,a). \n\nExperiments are performed comparing the derived algorithm  to a number of current model free methods."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problems of off-policy learning, domain adaptation and non-stationary environments are impotrtant. \nThe experiments presented in the paper suggest  performance improvements over the copmeting algorithms."
                },
                "weaknesses": {
                    "value": "This paper lacks any theoretical justification or proper motivation of the proposed objective. \n\nIt is completely not clear why optimising the proposed objective (5) should yield good performance \non a new unseen environment, or even on the same env. off policy.  In fact, there are several logical errors in the arguments. \n\nIn more detail:\n\n* The authors propose to construct the occupancy measure $\\rho_{T}^{\\pi}(s,a,s')$ and assume that observations data has such distribution. However, stantionary measures are generally ill-defined for  \nnon-stationary envirnoments and it is not clear what this means.  Thus when the authors write $\\rho_{\\hat{T}}^{\\pi}(s,a,s')$, it appears that they must assume the data generating env. $\\hat{T}$ is a regular env, contradicting the premise of the paper. \n\n* Even if $\\rho_{\\hat{T}}^{\\pi}(s,a,s')$ is just computed from a finite data, it is not clear why information in it should be relevant to performance in a new environment. This might be true under some strong assumptions that are implicit, but such assumptions must be discussed. It trivially not true in the general case. \n\n\n* Further, even if we only have one environment, it is not clear why the objective (5) should be related to \nthe standard objective $\\mathcal{J}(\\pi)$.  The inequlities in (2) are not tight, except in very degenerate cases. I.e. that gap between the right handside and left handside can be huge even for the optimal policy $\\pi^*$. \n\n\n* It is generally not clear how specifically the introduction of $s'$ helps performance on the target environment. \n\n\n\n\nThe encouraging experimental results do seem to indicate that there is something intersting about the proposed algorithm. However, a finished paper must provide an understanding of why the improvement happens,  or at least provide minimal theoretical grounding of the methods, both of which are absent from the current paper."
                },
                "questions": {
                    "value": "Please see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3398/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3398/Reviewer_UYp5",
                        "ICLR.cc/2024/Conference/Submission3398/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3398/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699550705916,
            "cdate": 1699550705916,
            "tmdate": 1700740640517,
            "mdate": 1700740640517,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6EadKxu8UJ",
                "forum": "fTiU8HhdBD",
                "replyto": "IHlTTUtho8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UYp5 (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your comments. While we respectfully disagree with some of your key points, we do appreciate the review and provide clarification to your questions and concerns below.\n\n> **W1: It is completely not clear why optimising the proposed objective (5) should yield good performance on a new unseen environment, or even on the same env.**\n\nUsing off-policy [1,2] or off-dynamics [3,4] data to enhance policy learning performance and sample efficiency is widely studied in the literature. Although these samples are not from the target environment or current policy, they can transfer useful information to facilitate policy learning. Our work follows similar intuition but provides a unified and theoretical grounding solution. Compared to general RL objective (1), our proposed objective (5) penalizes policy & dynamics shifts by the terms of $\\log\\left(\\rho^\\pi_T/\\rho^{\\widehat{\\pi}}_{\\widehat{T}}\\right)$ and $D_f\\left(\\rho^\\pi\\_{\\widehat{T}}\\|\\rho^{\\widehat{\\pi}}\\_{\\widehat{T}}\\right)$. In short, we penalize more on samples that have high discrepancies with the occupancy measure of the current policy $\\pi$ and the target dynamics $T$, but allow to learn from samples if their discrepancies are low. For a detailed discussion, \n\n- **Same env case:** objective (5) reduces to \n\n$\\widehat{\\mathcal{J}}(\\pi)=\\mathbb{E}_{(s,a,s')\\sim\\rho^\\pi_T}\\left[\\log r(s,a)-\\alpha\\log(\\rho^\\pi_T/\\rho^\\widehat{\\pi}_T)\\right]-\\alpha D_f\\left(\\rho^\\pi_T\\Vert\\rho^\\widehat{\\pi}_T\\right)$ \n\n$=\\mathbb{E}_{(s,a)\\sim\\rho^\\pi}\\left[\\log r(s,a)-\\alpha\\log(\\rho^\\pi/\\rho^\\widehat{\\pi})\\right]-\\alpha D_f\\left(\\rho^\\pi\\Vert\\rho^\\widehat{\\pi}\\right)$\n\n$=\\mathbb{E}\\_{(s,a)\\sim\\rho^\\pi}[\\log r(s,a)]-\\alpha\\left[D_{KL}(\\rho^\\pi\\Vert\\rho^\\widehat{\\pi})+D_f(\\rho^\\pi\\Vert\\rho^\\widehat{\\pi})\\right]$\n\nwhich essentially regularizes the discrepancy between on-policy occupancy $\\rho^{\\pi}$ and the occupancy $\\rho^\\widehat{\\pi}$ induced by off-policy samples from the replay buffer, which helps to alleviate potential instability caused by off-policy learning [5,6].\n\n- **New unseen env case:** As mentioned in Section 4.2 and 4.3 of our paper, our proposed OMPO actually need a small number of samples collected from the target env. A key advantage of OMPO is that it can maximally leverage easily accessible data from the off-dynamics source domain environment to facilitate learning in the target domain, by matching the occupancy measure in the source domain to be close to the occupancy measure implied in the small amount of target domain data. As illustrated in Figure 6, without such regularization in SAC, the dynamics gap would negatively impact policy learning, leading to deteriorated performance.\n\n\n\n> **W2: The authors propose to construct the occupancy measure $\\rho\\_{T}^{\\pi}(s,a,s')$ and assume that observations data has such distribution. However, stantionary measures are generally ill-defined for non-stationary envirnoments and it is not clear what this means. Thus when the authors write $\\rho\\_{\\hat{T}}^{\\pi}$, it appears that they must assume the data generating env. $\\hat{T}$ is a regular env, contradicting the premise of the paper.** \n\nWe think there are some misunderstandings here. As clearly illustrated in Figure 1, the non-stationary dynamics considered in our paper involve a series of different dynamics $\\widehat{T}_1,\\widehat{T}_2,...,\\widehat{T}_h$ at different training stages, each representing a specific environment dynamics. This is a common setting considered by previous non-stationary RL studies [7, 8, 9]. We use $\\widehat{T}$ to represent the transition dynamics that we encountered in the samples from the replay buffer of the **current training stage**. $\\widehat{T}$ actually correspond to $\\widehat{T}_1,\\widehat{T}_2,...,\\widehat{T}_h$ during the course of training. $\\widehat{T}$ **does not** correspond to a stationary env, and we use the symbol of $\\widehat{T}$ simply to keep the notations uncluttered and also provide a unified view of RL problem under policy/dynamics shifts."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234633497,
                "cdate": 1700234633497,
                "tmdate": 1700234998759,
                "mdate": 1700234998759,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b7poGVhrMm",
                "forum": "fTiU8HhdBD",
                "replyto": "IHlTTUtho8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UYp5 (2/3)"
                    },
                    "comment": {
                        "value": "> **W3: Even if $\\rho_{\\hat{T}}^{\\pi}(s,a,s')$ is just computed from a finite data, it is not clear why information in it should be relevant to performance in a new environment. This might be true under some strong assumptions that are implicit, but such assumptions must be discussed. It trivially not true in the general case.**\n\n* First, we would like to politely remind that, $\\rho_{\\widehat{T}}^{\\pi}(s,a,s')$ actually does not need to be explicitly computed in OMPO. If the reviewer checks Step 3 of Section 4.2, as well as its proof in Appendix A.3, by leveraging Fenchel conjugate and the change of variable trick similar to DualDICE [10] and AlgaeDICE [11], we can avoid evaluating $\\rho_{\\widehat{T}}^{\\pi}(s,a,s')$ but solve a tractable formulation Eq. (12) that only involve samples from $\\rho_{\\widehat{T}}^{\\widehat{\\pi}}$ (i.e., the off-policy/off-dynamics samples from the replay buffer).\n* Second, as we have discussed in the response to W1, using off-dynamics data to facilitate policy learning in the target domain is widely studied and acknowledged in literature [3, 4]. If the reviewer still thinks using off-dynamics samples is not relevant to policy learning in a new environment, we are happy to provide a comprehensive list of related references.\n* Lastly, as demonstrated in our empirical results in Section 5.1, OMPO achieves strong performance under environments with shifted dynamics and substantially outperforms algorithms that are specifically designed for such settings, even in some highly non-stationary environments.\n\n\n> **W4: Further, even if we only have one environment, it is not clear why the objective (5) should be related to the standard objective $\\mathcal{J}(\\pi)$. The inequlities in (2) are not tight, except in very degenerate cases. I.e. that gap between the right handside and left handside can be huge even for the optimal policy $\\pi^{*}$.**\n\n\n* Even in a single environment, if one uses off-policy RL algorithms, the policy shifts in the off-policy training data from the replay buffer could also negatively impact performance [5, 6]. Note that, the original RL objective $\\pi^*=\\arg\\max_{\\pi}\\mathbb{E}_{(s,a)\\sim\\rho^\\pi}[r(s,a)]$ assumes the samples are from on-policy distribution $(s,a)\\sim\\rho^\\pi$, not the off-policy distribution $(s,a)\\sim\\rho^{\\widehat{\\pi}}$. Under policy shifts, the training samples $(s,a)\\sim\\rho^{\\widehat{\\pi}}$ from a previous policy $\\hat{\\pi}$ have a mismatch to on-policy samples $(s,a)\\sim\\rho^\\pi$, resulting in suboptimal performance. Thus, objective (5) penalizes the policy shifts to handle the mismatch.\n* Regarding the tightness of the inequality (2), note that under a single environment, i.e., $\\widehat{T}=T$, the second term in Eq. (2) satisfies $D_{KL}\\left(\\rho^\\pi_{\\widehat{T}}\\Vert\\rho^\\pi_{T}\\right)=D_{KL}\\left(\\rho^\\pi_T\\Vert\\rho^\\pi_{T}\\right)=0$ and right-hand side of Eq. (2) reduces to $\\mathbb{E}_{(s,a,s')\\sim\\rho^\\pi_T}[\\log r(s,a)]$. This actually corresponds to solving an MDP with reward shaping using the log function. Since the log function is monotonically increasing, it does not largely change the nature of the original task. \n\n\n\n> **W5: It is generally not clear how specifically the introduction of $s'$ helps performance on the target environment.**\n\nThe introduction of $s'$ is to explicitly capture the dynamics shifts between source dynamics $\\widehat{T}$ and target dynamics $T$, since we need to have transition $(s,a,s')$ triples to fully specify the transition dynamics $T(s'|s,a)$. Note the transition occupancy distribution $\\rho_T^{\\pi}(s,a,s')$ will be different if $T$ is different, which is exactly how we our method can penalizes the dynamics shifts as in Eq.(2) (i.e., penalize with term $D_{KL}(\\rho_{\\widehat{T}}^\\pi(s,a,s')\\|\\rho_T^\\pi(s,a,s'))$)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234669745,
                "cdate": 1700234669745,
                "tmdate": 1700235278991,
                "mdate": 1700235278991,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9nTjSNXthY",
                "forum": "fTiU8HhdBD",
                "replyto": "IHlTTUtho8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UYp5 (3/3)"
                    },
                    "comment": {
                        "value": "> **W6: The encouraging experimental results do seem to indicate that there is something interesting about the proposed algorithm. However, a finished paper must provide an understanding of why the improvement happens, or at least provide minimal theoretical grounding of the methods, both of which are absent from the current paper.**\n\n- As discussed above, our proposed objective (5) extends the general RL objective under policy & dynamics shifts, and the subsequent derivations are built upon the well-established DICE-related theories. Specifically, general RL objective $\\arg\\max\\_{\\pi}\\mathbb{E}\\_{(s,a)\\sim\\rho_\\pi}[r(s,a)]$ has no treatment for policy and dynamics shifts, thus will be heavily impacted if encountering scenarios involving such shifts. In contrast, OMPO can handle these shifts carefully from our theoretical derivation using regularization terms $\\log\\left(\\rho^\\pi_T/\\rho^{\\widehat{\\pi}}\\_{\\widehat{T}}\\right)$ and $D_f(\\left(\\rho^\\pi\\_T\\|\\rho^{\\widehat{\\pi}}\\_{\\widehat{T}}\\right))$, resulting in improved performance.\n- We have added a logical flow for the theoretical derivation of our method in Appendix A of our revision paper, as well as extra discussions to explain the source of performance improvement in Appendix B. Please let us know if you have any specific comments on our theoretical derivation.\n\n---\n\nThanks again for your comments. We hope the reviewer can reassess our work in light of these clarifications.\n\n**Reference**\n\n[1] Safe and efficient off-policy reinforcement learning. NeurIPS. 2016.\n\n[2] Data-efficient off-policy policy evaluation for reinforcement learning. ICML. 2016.\n\n[3] Domain randomization for transferring deep neural networks from simulation to the real world. IROS. 2017.\n\n[4] Transfer learning for reinforcement learning domains: A survey. JMLR. 2009.\n\n[5] Off-Policy Actor-Critic. ICML 2012.\n\n[6] Trust region policy optimization. ICML 2015.\n\n[7] Context-aware dynamics model for generalization in model-based reinforcement learning. ICLR 2022.\n\n[8] Non-stationary reinforcement learning without prior knowledge: An optimal black-box approach. COLT 2021.\n\n[9] Meta-reinforcement learning in nonstationary and dynamic environments. TPAMI, 2022.\n\n[10] Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. NeurIPS 2019.\n\n[11] Algaedice: Policy gradient from arbitrary experience. arXiv 2019."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234713959,
                "cdate": 1700234713959,
                "tmdate": 1700235200164,
                "mdate": 1700235200164,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "69tumAerXD",
                "forum": "fTiU8HhdBD",
                "replyto": "IHlTTUtho8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We sincerely look forward to your reply."
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe appreciate your comments. We hope our last reply has resolved all your concerns. If you have any other questions, we are also pleased to respond. We sincerely look forward to your response.\n\nBest wishes!\n\nThe authors."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488241724,
                "cdate": 1700488241724,
                "tmdate": 1700488241724,
                "mdate": 1700488241724,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "APZvCs5zdC",
                "forum": "fTiU8HhdBD",
                "replyto": "IHlTTUtho8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Your feedback is critical to us"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe were wondering if our responses and revision have resolved your concerns since only two days are left for discussion. We have added the discussion about performance improvement of OMPO compared to general RL objective, and the theoretical sketch of our derivation. Please let us know if these changes can resolve your concerns. We are eager to engage in further discussions and continue improving our work.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565737440,
                "cdate": 1700565737440,
                "tmdate": 1700566165410,
                "mdate": 1700566165410,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1GSwsH1PVl",
                "forum": "fTiU8HhdBD",
                "replyto": "IHlTTUtho8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Awaiting your valuable feedback before deadline"
                    },
                    "comment": {
                        "value": "Dear Reviewer UYp5,\n\nWe sincerely appreciate the time and effort you have dedicated to reviewing our work, especially during this busy period. As we approach the final 24 hours of the discussion stage, we kindly seek your feedback on our responses. Your insights are crucial in addressing any remaining concerns you may have regarding our submission.\n\nIn our response, we have delved into the benefits of our proposed objective (5), clarified the definition of the occupancy measure, explained the inequalities (2) and the introduction of $s'$. Additionally, in our revision, we have incorporated a theoretical analysis of performance comparison with the general RL objective in practice, along with a theoretical sketch in our derivation.\n\nIt is important to note that post-November 22nd, while we welcome any further reassessment or comments on our work at your convenience, our ability to respond may be limited. Hence, we are particularly eager to engage in a constructive discussion with you before the impending deadline. Your feedback is not only valuable to us but also essential for the final evaluation of our work.\n\nBest regards!\n\nThe Authors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656693798,
                "cdate": 1700656693798,
                "tmdate": 1700660278049,
                "mdate": 1700660278049,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YeYgr4PKqf",
                "forum": "fTiU8HhdBD",
                "replyto": "1GSwsH1PVl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Reviewer_UYp5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Reviewer_UYp5"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors, \n\n Thank you for your responses.  Unfortunately, it is still not clear what is the meaning of the distribution on (s,a,s') in the case it is computed from multiple environments. \n(by computed I do not mean one has to compute it explicitly, just defined in a way the authors prefer to define it [*]).   \n\nIn a such a case, it may correspond to a certain mixture of (s,a,s') distributions from the statinoary environments composing the non-stationary one (although not even this, in the general case). As such, there may be no policy that can approximate this mixture  in a single envirnment, and it is not clear why attempting to approximate such an average case would be useful for each envirnment indivdually. \nNote this has nothing to do with on/off policy learning. \n\nI am not saying that it always unreasonable to learn such a missspecifed model as above. I am saying that the costs must be well defined, the misspecification must be stated explicitly  and analysed to understand when it is helpful.  \n\n\n\n[*] The notion of \"non-statinary environment\" $\\hat{T}$ was not formally defined in the paper, and thus also the  associated notion of  \"\\rho^{\\pi}_{\\hat{T}}\" does not have a fully clear meaning."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740474536,
                "cdate": 1700740474536,
                "tmdate": 1700740474536,
                "mdate": 1700740474536,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aU91KVXpjW",
                "forum": "fTiU8HhdBD",
                "replyto": "IHlTTUtho8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3398/Reviewer_UYp5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3398/Reviewer_UYp5"
                ],
                "content": {
                    "comment": {
                        "value": "This is not a question of notation. In any case, I will discuss the situation with other reviewers."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741054966,
                "cdate": 1700741054966,
                "tmdate": 1700741128450,
                "mdate": 1700741128450,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]