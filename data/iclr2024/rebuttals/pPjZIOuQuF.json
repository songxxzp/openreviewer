[
    {
        "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems"
    },
    {
        "review": {
            "id": "nDANDlWgVz",
            "forum": "pPjZIOuQuF",
            "replyto": "pPjZIOuQuF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8936/Reviewer_mVr1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8936/Reviewer_mVr1"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new benchmark called RepoBench for evaluating repository-level code completion systems. RepoBench consists of three interconnected evaluation tasks: RepoBench-R for retrieving the most relevant code in the repository, RepoBench-C for code completion using both in-file and cross-file context, and RepoBench-P for the entire pipeline of both retrieval and code completion. The authors carry out a series of experiments on RepoBench, analyzing the efficacy of various retrieval methods and code completion models of different magnitudes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- A nice idea of benchmarking repository-level code completion\n - The paper is generally well-written and easy to follow"
                },
                "weaknesses": {
                    "value": "To me, the novel research contributions of the paper are a bit limited, especially for an AI conference. The paper could better fit a software engineering/programming conference.  The significance of the work could be more clearly stated.\n\nTo evaluate RepoBench-R, the authors selected three baseline strategies for the retrieval task, namely, random retrieval, lexical retrieval, and semantic retrieval. The selection of baseline strategies for RepoBench-R, particularly the inclusion of random retrieval and lexical retrieval, are weak baselines, which may not effectively demonstrate the distinctive capabilities of the proposed benchmark. In that sense, the results presented in Section 4.1 are under expectation and I think that previous benchmarks may also demonstrate the ability of these strategies. A more competitive baseline selection including LLMs would enhance the work.\n\nThe paper lacks a comprehensive comparison with previous benchmarks about code completion. Although RepoBench is the first benchmark on repository-level code completion, it would still benefit from comparisons with prior benchmarks. Such comparisons could involve RepoBench-R versus existing code retrieval benchmarks and RepoBench-C versus traditional benchmarks for function-level code completion. \n\nThe metrics used for code completion, i.e., EM and Edit Similarity, are unusual. The authors could consider more widely used metrics such as pass@k and CodeBLEU? \n\nThe evaluation of RepoBench-C is conducted using only three Language Model Models (LLMs), specifically CodeGen, StarCoder, and Codex. As a benchmark paper, the inclusion of only three LLMs may not fully represent the diverse capabilities of available models. To enhance the benchmark's applicability, additional LLMs, including recently proposed ones, could be considered for comparison. For example: Shi et al., SoTaNa: The Open-Source Software Development Assistant, https://arxiv.org/abs/2308.13416"
                },
                "questions": {
                    "value": "- Why not using widely used metrics such as pass@k and CodeBLEU? \n- How is the proposed benchmark compared to previous benchmarks for code completion?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8936/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674697171,
            "cdate": 1698674697171,
            "tmdate": 1699637125273,
            "mdate": 1699637125273,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yzVmBplg97",
                "forum": "pPjZIOuQuF",
                "replyto": "nDANDlWgVz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8936/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8936/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mVr1 (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer mVr1,\n\nWe appreciate the time you invested in reviewing our paper. Your comments have provided valuable perspectives that we have carefully considered in this response.\n\n> The novel research contributions of the paper are a bit limited, especially for an AI conference.\n\nWe would like to highlight the significance of our work:\n\n1. We hope that RepoBench can address the evaluation of code generation in LLMs at current stage. This is a key area in the field of AI, as understanding and improving code generation capabilities are vital.\n2. RepoBench-C can be used to facilitate tasks involving long-context generation. The customizable token number feature of RepoBench-C allows the construction of 2k, 8k, 16k and even 128k versions, making it a benchmark for evaluating the efficacy of LLMs in handling long context length. This flexibility is particularly important in the current landscape of AI research, where handling longer contexts is becoming increasingly crucial.\n3. We are encouraged by the interest RepoBench has already garnered in the LLM community. The StarCoder team has noticed the performance of StarCoder here, leading to a collaboration study focused on repo-level model training for next version. You can check the preliminary ablation study we already have shown in the global response.\n\n>  Weak baselines selection of RepoBench-R.\n\nFirst, we would like to emphasize the relevance of Jaccard similarity as a baseline. It has been revealed through reverse engineering studies online that GitHub Copilot uses Jaccard similarity backend for retrieval.\n\nIn our initial experiments, we evaluated both encoder-only and decoder models, such as CodeGPT and CodeGen. While encoder-only models demonstrated superior performance for retrieval tasks, we initially omitted these results due to page constraints. In response to your feedback, we plan to include these findings to present a more comprehensive evaluation.\n\nWe are also excited to share that we have conducted additional experiments with more recent encoding models, specifically CodeT5+ and Intruct-XL. The results from these experiments have shown **Instruct-XL demonstrates similar performance to UnixCoder**.\n\nTable 4: Results of CodeGPT, CodeGen, CodeT5+ and Instruct-XL as retriever on RepoBench-R.\n\n|       Language |  Model     | acc@1 (Easy XF-F) | acc@3 (Easy XF-F) | acc@1 (Easy XF-R) | acc@3 (Easy XF-R) | acc@1 (Hard XF-F) | acc@3 (Hard XF-F) | acc@5 (Hard XF-F) | acc@1 (Hard XF-R) | acc@3 (Hard XF-R) | acc@5 (Hard XF-R) |\n|------------------|-----------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|\n| **Python**       |     CodeGPT       | 15.86 | 47.12 | 16.83 | 46.92 | 8.46 | 23.15 | 36.71| 7.90 | 22.07 | 35.62 |\n| | CodeGen | 20.15 | 53.90 | 22.78 | 55.02 | 12.81 | 30.89 | 45.96| 13.98 | 33.15 | 46.73 |\n| | CodeT5+ | 17.86 | 50.61 | 19.25 | 51.63 | 8.39 | 22.53 | 36.02| 8.97 | 24.02 | 36.68 |\n| | Instruct-XL | 26.37 | 61.64 | 31.92 | 65.03 | 18.12 | 39.12 | 52.62| 21.07 | 41.50 | 55.38 |\n| **Java**       |     CodeGPT       | 15.89 | 48.09 | 17.60 | 49.20 | 7.81 | 23.31 | 37.59| 7.98 | 22.30 | 37.40 |\n| | CodeGen | 18.49 | 51.84 | 23.28 | 54.12 | 9.99 | 28.30 | 43.06| 13.30 | 31.35 | 46.55 |\n| | CodeT5+ | 15.73 | 46.75 | 16.90 | 49.52 | 6.22 | 17.93 | 29.66| 6.95 | 19.65 | 32.17 |\n| | Instruct-XL | 16.88 | 50.58 | 26.05 | 60.57 | 10.69 | 28.09 | 41.58| 17.83 | 37.67 | 50.40 |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8936/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634619588,
                "cdate": 1700634619588,
                "tmdate": 1700634619588,
                "mdate": 1700634619588,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zlXqukCPca",
            "forum": "pPjZIOuQuF",
            "replyto": "pPjZIOuQuF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8936/Reviewer_zCY2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8936/Reviewer_zCY2"
            ],
            "content": {
                "summary": {
                    "value": "The authors address a need for repository wide benchmarks for code-prediction and code-retrieval tasks. They do so by creating two datasets, in the test set, they recover repository information for the github-code dataset and create two variants, a 2K and an 8K variant. For the test set, they crawl permissively licensed Java and Python projects after the The Stack cut-off date. To better mimic real-world scenarios, the test set is not separated by prompt length. As for the benchmark itself, it focuses on three tasks that should exercise both cross-file and in-file context requirements. The tasks are code auto-completion, code-retrieval, and a join task where the relevant cross-file information should be retrieved before it is used for completion (pipeline)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The paper addresses the need for a repository wide benchmark that better aligns with real-world usecases in software projects. (2) It addresses data leakage issues* by crawling new data for the test set and (3) provides fine-tuning data for models that may require it. \n(4) The StarCoder overfitting to file-level use-cases provides interesting additional insight."
                },
                "weaknesses": {
                    "value": "The main concerns with the paper are two-fold. \n\nThe usefulness of the benchmark relies on a gentleman agreement to not use data from the collection dates during training or fine-tunning.\n\nAnother concern is the opt-out possibility. While not strictly necessary, a nice-to-have would be an opt-out mechanism similar to the The Stack one for authors that may want to remove their code from.the data."
                },
                "questions": {
                    "value": "Is there an intention to make the bechmark a \"living\" benchmark where the test set is periodically refreshed to be past the training set horizon date?\n\nAlternatively, is there an intention to check and disqualify models that have trained on test set data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The opt-out is more-so a nice-to-have rather than necessary since the authors have taken care to respect code licenses during crawling."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8936/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8936/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8936/Reviewer_zCY2"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8936/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698718844057,
            "cdate": 1698718844057,
            "tmdate": 1699637125161,
            "mdate": 1699637125161,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FZ7Qo7oVzh",
                "forum": "pPjZIOuQuF",
                "replyto": "zlXqukCPca",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8936/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8936/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zCY2"
                    },
                    "comment": {
                        "value": "Dear Reviewer zCY2,\n\nThank you for your appreciation and constructive feedback on our paper. \n\n> Opt-Out Possibility and Living Benchmark\n\nWe are grateful for your suggestion about the opt-out mechanism and the idea of a living benchmark. Interestingly, we are already moving in this direction and the living benchmark is exactly what we want to present since we have the pipeline for construction and we can always crawl the newest data for benchmarking.\n\nSpecifically, we are collaborating with the StarCoder team, and we will present the newest version of RepoBench shortly with all the repositories are created in September, 2023, aligning with the next release of \"The Stack.\" As for the opt-out mechanism, the StarCoder team has implemented a rigorous deduplication process on their new training data, which will also be open-sourced soon.\n\n> Alternatively, is there an intention to check and disqualify models that have trained on test set data?\n\nThanks for pointing out this. RepoBench as a living benchmark as you expected, it allows us the flexibility to continuously update and release new versions (e.g., September version, October version, November version, etc.). So, if a model has been trained and released, we can simply validate it against the latest version of RepoBench. This method ensures an effective check against any potential training on test set data, as models will be evaluated on newly updated benchmarks that they have not encountered before, ensuring the integrity and fairness of the evaluation process."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8936/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634604071,
                "cdate": 1700634604071,
                "tmdate": 1700634604071,
                "mdate": 1700634604071,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2vllTcSqTp",
                "forum": "pPjZIOuQuF",
                "replyto": "FZ7Qo7oVzh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8936/Reviewer_zCY2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8936/Reviewer_zCY2"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the answers and confirming the move to a living benchmark as well as the global preliminary results response above. \n\nIf the opt-out mechanism is shared with \"The Stack\", I am more than satisfied with the process as last I tested it, it was fairly smooth and simple to follow.\n\nAs for data leakage, the cat-and-mouse game remains, but the move to a living benchmark alleviates my concerns as new cuts of the benchmark should reveal models that may have cheated on previous iterations."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8936/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635141631,
                "cdate": 1700635141631,
                "tmdate": 1700635141631,
                "mdate": 1700635141631,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VqI35GzDZH",
            "forum": "pPjZIOuQuF",
            "replyto": "pPjZIOuQuF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8936/Reviewer_V3UP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8936/Reviewer_V3UP"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose RepoBench - a benchmark for repository level code auto-completion evaluation. They propose three evaluation tasks: retrieval, code completion, and pipeline. Authors perform experiments using RepoBench"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Significant work on RepoBench construction.\n- Extensive experiments with RepoBench with existing models and retrieval techniques."
                },
                "weaknesses": {
                    "value": "- It is not clear what new insights RepoBench and experiments on it contribute to the field. Were the results previously unknown or unexpected?\n\n- This might not be a weakness of the paper per se, but it concerns me a bit that random retrieval is close to or even outperforms some non-random retrieval methods.\n\n\nI increased the rating based on authors' answer to my questions."
                },
                "questions": {
                    "value": "- What is exactly \"the first appearance of a cross-file line within a file\"? Is this the import line? Is this the first line that uses cross-file function?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8936/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8936/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8936/Reviewer_V3UP"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8936/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730575313,
            "cdate": 1698730575313,
            "tmdate": 1700700670421,
            "mdate": 1700700670421,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QoovnM61YO",
                "forum": "pPjZIOuQuF",
                "replyto": "VqI35GzDZH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8936/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8936/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer V3UP"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review and valuable comments on our paper. We appreciate the opportunity to clarify and elaborate on the aspects you've highlighted.\n\n> It is not clear what new insights RepoBench and experiments on it contribute to the field. Were the results previously unknown or unexpected?\n\nWe understand your concern regarding the novel insights provided by RepoBench. Our primary contribution through RepoBench is in addressing the need for benchmarks that evaluate repository-level code auto-completion, a gap that exists in current benchmarks focused mostly on single-file tasks. This focus is particularly relevant given that real-world applications of code models often require a comprehensive repository-level approach for optimal functionality.\n\nAdditionally, RepoBench has the ability to construct generation tasks with customizable context lengths (e.g., 2k, 8k, 16k, 64k, etc.). This flexibility allows for an in-depth analysis of model performance under varying long-context scenarios\n\n\nDuring our experiments, we did encounter several intriguing findings that were unexpected and might be insightful:\n\n1. **Multilingual Code Generation**: While current models show promising results in Python, matching Codex in some cases, we observed a noticeable performance gap in Java code generation\n2. **Long Context Generation**: On repobench-c with 8k max length, StarCoder is still trailing behind the capabilities of Codex, highlighting a crucial area for future development.\n3. **Repo-Level performance**: Our initial assumption was that repo-level code generation would be generalizable. However, we found that models like StarCoder did not perform well at this level, suggesting they might need to be trained with repository-level data.  This insight led to a collaborative effort with the StarCoder team and our subsequent contribution to the model improvement. We show some of the preliminary ablation study in a global response, which demonstrates that repository-level training can improve the performance dramatically.\n4. **Retrieve as much as possible**: Merely retrieving the gold code snippet is not always the most effective approach. Instead, filling the context with as much 'relevant' code as possible from the repository tends to improve performance.\n5. **Positioning of Cross-File Context**: Unlike long context generation in textual documents where models pay more attention to the beginning of the prompt, in repository-level code generation we found that placing more relevant cross-file snippets towards the end of the prompt (closer to the in-file context) can improve performance.\n\n> It concerns me a bit that random retrieval is close to or even outperforms some non-random retrieval methods.\n\nThe observation regarding random retrieval occasionally outperforming strategized retrieval, particularly in Java, is indeed intriguing. We speculate that this phenomenon is influenced by the inherent characteristics of the language. In Java, cross-file references often involve class definitions (e.g., `public class DerivedClass extends BaseClass {`). In this scenario, previous lines might not provide contextual clues (meaningful information) for retrieval. While in Python, cross-file references may often involve more direct contextual clues such as function calls with parameters, making strategized retrieval more effective. For example, if you try to use `calculate(left, right)`, you may define `left=?` and `right=?` in the previous lines. Thus, our findings suggest that effective retrieval strategies may vary between different programming languages.\n\n> What is exactly \"the first appearance of a cross-file line within a file\"? Is this the import line? Is this the first line that uses cross-file function?\n\nWe acknowledge for the ambiguity in our description. The \"first appearance of a cross-file line within a file\" refers to the first instance where a cross-file function/class is used in the code, not the import line. So strictly speaking, it should be the second appearance (the first one will always be the import line)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8936/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634586606,
                "cdate": 1700634586606,
                "tmdate": 1700634586606,
                "mdate": 1700634586606,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pkzR3MCPaH",
                "forum": "pPjZIOuQuF",
                "replyto": "QoovnM61YO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8936/Reviewer_V3UP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8936/Reviewer_V3UP"
                ],
                "content": {
                    "title": {
                        "value": "Thanks to authors for the answers"
                    },
                    "comment": {
                        "value": "Thank you for responses and elaboration on your work. I have increased the rating of the paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8936/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700606650,
                "cdate": 1700700606650,
                "tmdate": 1700700606650,
                "mdate": 1700700606650,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]