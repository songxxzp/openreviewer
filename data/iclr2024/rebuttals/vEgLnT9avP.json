[
    {
        "title": "ResolvNet: A Graph Convolutional Network with multi-scale Consistency"
    },
    {
        "review": {
            "id": "yFCvRumejf",
            "forum": "vEgLnT9avP",
            "replyto": "vEgLnT9avP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4788/Reviewer_2iPY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4788/Reviewer_2iPY"
            ],
            "content": {
                "summary": {
                    "value": "This paper first points out that the presence of strongly connected sub-graphs may severely restrict information flow in common GNN architectures. Then, it introduces the concept of multi-scale consistency, which can fit both the node-level and graph-level scenarios. In light of this, the authors introduce ResolvNet, a flexible graph neural network based on the mathematical concept of resolvents. Finally, it conducts some experiments to evaluate the proposed method, showing that the proposed method outperforms state-of-the-art baselines on several tasks across multiple datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tIt provides some theorical support for the proposed model.\n2.\tIt tests on several widely-used datasets, and the proposed method can sometimes beat the existing methods.\n3.\tThe authors provide their codes."
                },
                "weaknesses": {
                    "value": "1.\tSOTA baselines are largely ignored. On three famous datasets Cora, Citeseer and Pubmed, there are only two baselines are considered (in Table 1). Few baselines (like GCNII and GraphMAE2), which after 2022, are considered. As far as I know, GCNII (which is open source) can beat the proposed method on Cora and Pubmed. Moreover, even this, the proposed method cannot get the best performance in Table 3.\n2.\tThe work is some kind of hard to follow. Although providing lots of theories will enhance the paper, the readability is also should be considered.\n3.\tSome grammatical errors, like 1) satisfied by poular graph -> \u201cpopular\u201d; 2) severly restricts - > \u201cseverely\u201d. 3) degree occuring -> \u201coccurring\u201d"
                },
                "questions": {
                    "value": "1.\tWhy the reported results cannot beat SOTA baselines (like GCNII) in Table 1?\n2.\tHow many hyper-parameters are there in your method? If the proposed method contains too many hyper-parameters, it will be hard to reproduce.\n3.\tSee the weakness in the \u201c*Weaknesses\u201d part.\n4.\tThe work can be largely improved by enhancing its experiments and fixing gram errors."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4788/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698645418713,
            "cdate": 1698645418713,
            "tmdate": 1699636461339,
            "mdate": 1699636461339,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DQIpZzNWes",
                "forum": "vEgLnT9avP",
                "replyto": "yFCvRumejf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "content": {
                    "title": {
                        "value": "ResolvNet performs better than GCNII. It also already performs best (not worst) in Table 3 since here \u201clower-is-better\u201d (the metric being mean-absolute-error)."
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for reviewing our paper and providing us with their feedback.\n\nAs we understand it, the main points of criticism expressed by the reviewer are that 1) our method supposedly does not perform best in Table 3, and 2) the reviewer hypothesized that our ResolvNet method would not perform better than GCNII in Table 1.\n\n**Both points however  stem from misconceptions:**\n\n1) **In Table 3** the metric is \u201cmean-absolute-error\u201d, so that **lower numbers correspond to better performance**. Our  method actually performs best in Table 3 by a large margin (i.e. by an error that is lower by factor of $10$ to $40$ compared to baselines) as discussed in the main-text of our submission.\n2) Accuracies reported in the GCNII-paper [2] and Table 1 of our submission can not simply be compared, as the **experimental setups differ significantly**: Our paper uses the experimental setup and (class-balanced) random splits of [1] (i.e. \u201cpredict then propagate\u201d). Instead, [2]  uses the experimental setup and splits of [3].\n**Evaluating ResolvNet and GCNII in a common evaluation setup clearly shows that ResolvNet performs better** (details provided below).\n\nLet us address these and all other points raised by the reviewer in detail individually below:"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574819283,
                "cdate": 1700574819283,
                "tmdate": 1700574819283,
                "mdate": 1700574819283,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mVosxOCqyZ",
                "forum": "vEgLnT9avP",
                "replyto": "yFCvRumejf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Experimental Results"
                    },
                    "comment": {
                        "value": "* Why the reported results cannot beat SOTA baselines (like GCNII) in Table 1?\n\nThe results in Table 1 were obtained following the experimental setup of [1] (i.e. \u201cPredict then Propagate\u201d) which evaluates on class-balanced  random splits. In contrast the GCNII-paper followed the experimental setup of  [2], so that reported numerical values for accuracies can not simply be compared.\n\nTo allow for a comparison of GCNII with the other baselines of Table 1 as well as ResolvNet, we evaluated GCNII within the experimental Setup of Table 1 (i.e. following the setup of  [1]).\nWe used the best-performing hyperparameters whenever they were reported in the original GCNII paper [3] and otherwise performed a full hyperparameter sweep.\n\nAs is evident from Table 1 in our updated manuscript, our method actually performs better than GCNII on the vast majority of datasets. \n\nRegarding the performance in the homophilic regime the reviewer inquired about\n> (As far as I know, GCNII (which is open source) can beat the proposed method on Cora and Pubmed)\n\nwe note the following result:\n|[\\%]  | MS. Acad. | Cora  | Pubmed  | Citeseer  |   \n|---|---|---|---|---|\n|  GCNII | 88.37 $\\pm$ 0.16  |  84.41 $\\pm$ 0.26 | 78.45 $\\pm$ 0.31  | 73.12 $\\pm$ 0.35  |\n|  ResolvNet| 92.73 $\\pm$ 0.08 |  84.16 $\\pm$ 0.26| 79.26 $\\pm$ 0.36  | 75.03 $\\pm$ 0.29 |\n\nOn Cora, there is a marginal difference between results for ResolvNet and GCNII (84.16\\% vs. 84.41 \\%). ResolvNet clearly performs better on the other datasets.\n\n\n\n\n\nWe would also like to thank the reviewer for bringing the recent and  interesting GraphMAE2 paper to our attention. \nThis paper focusses on the self-supervised setting, the training procedure of the GraphMAE2 model involves pre-training and the backbones for its encoder and decoder might be constituted by any GNN-architecture (including ResolvNet).\nIn contrast, Table 1 focusses on benchmarking the performance of different _convolutional_ - _layers_ against each other; all under the same (semi-)supervised training procedure and using the same loss.\nNevertheless, we found both GraphMAE2 and its precursor GraphMAE to be interesting works and are happy to now cite them in the introduction to our paper.\n\nFinally, we would like to point out that \u2013 while ResolvNet generally performs better than baselines in the homophilic setting -- we are not concerned with designing another network that incrementally improves the state of the art on standard node-classification datasets.\n\nInstead we are concerned with introducing the concept of multi-scale consistency. We  explain its importance for the transferability of models between graphs describing the same object at different resolutions, as well as for the information-flow within multi-scale graphs. We then introduce ResovNet as a method to address the lack of multi-scale consistency of previous architectures.\n\n\n\n* On three famous datasets Cora, Citeseer and Pubmed, there are only two baselines are considered (in Table 1).\n\nWe clearly compared against $10$ baselines on all datasets of Table 1 (including Cora, Citeseer and Pubmed) in our initial manuscript. We are  unsure how to interpret this comment; could the reviewer clarify?\n\nIn our updated manuscript, we now additionally also compare against  the recent ChebNetII (He et al. (2022)) as an additional  baseline. Thus we now compare in total against $12$ (as opposed to the previous $10$) baselines in this table. Our conclusions  remain the same.\n\n* the proposed method cannot get the best performance in Table 3.\n\nAs stated in the header of Table 3, this Table lists Mean Absolute Errors (MAEs). Here, lower numbers thus correspond to better performance. Thus our method is clearly the best performing one in Table 3.\n\nAs already discussed in the corresponding section of the main-text, our method in fact not only performs best, but it does so by a large margin (MAEs are lower by factor of between 10 to 40 compared to baselines).\n\nWhile we had provided the evaluation metric in the header of Table 3,and marked our method via \u201c\\textbf\u201d as the best-performing method in this Table, we have -- following this comment by the reviewer --  now added a \u201c($\\downarrow$)\u201d symbol to the header of Table 3 for additional clarity."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575018136,
                "cdate": 1700575018136,
                "tmdate": 1700575303985,
                "mdate": 1700575303985,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1BOSUa2BTI",
            "forum": "vEgLnT9avP",
            "replyto": "vEgLnT9avP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4788/Reviewer_ZGZ9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4788/Reviewer_ZGZ9"
            ],
            "content": {
                "summary": {
                    "value": "This paper study multi-scale consistency (distinct graphs describing the same object at different resolutions should be assigned similar feature vectors) of node representation in graph neural network, which is indeed an important topic that is less well explored. \n\nThe authors show existing GNN method lack of multi-scale consistency, then they propose ResolvNet to solve this issue. Experiment shows improvement on GNN performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper study multi-scale consistency (distinct graphs describing the same object at different resolutions should be assigned similar feature vectors) of node representation in graph neural network, which is indeed an important topic that is less well explored. \n\n2. This paper provide a very clear definition on multi-scale consistency in Definition 2.1, and explain in great details (using both figures, text, and examples) to help readers understand why it is important.\n\n3. The proposed method capture the intuition of multi-scale consistency."
                },
                "weaknesses": {
                    "value": "1. Experiment dataset is small. This is potentially because the proposed method has very high complexity due to matrix inverse (see feed-forward rule in paragraph **The ResolvNet Layer**. The authors need to conduct experiment on larger datasets (e.g., OGBN) and report complexity in terms of FLOP/Wall-clock time.\n\n2. Part of the discription is not very clear, please refer to Questions."
                },
                "questions": {
                    "value": "1. I understand the definition of $G_\\text{high}$ and $G_\\text{reg}$, but I am very clear how two split an original graph into this two graph. This is related to Definiton 2.1.\n\n2. Please elaborate on \"we would have a Lipschitz continuity relation that allows to bound the difference in generated feature vector in terms of a judiciously chosen distance\". This is the sentense above Eq. 1. I don't understand why."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4788/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731827664,
            "cdate": 1698731827664,
            "tmdate": 1699636461241,
            "mdate": 1699636461241,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L8kspnC1ns",
                "forum": "vEgLnT9avP",
                "replyto": "1BOSUa2BTI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Performed complexity analysis and answered questions"
                    },
                    "comment": {
                        "value": "We would like to sincerely thank the reviewer for the careful review of our paper.\n\n We were especially happy to read that we consider \n\n> indeed an important topic,\n\nthat \n> the paper provide a very clear definition on multi-scale  consistency\n\nand that our various explanations using different modalities (text, examples, figures)\n> help readers understand why [multi-scale consistency] is important.\n\nLet us address the raised points individually:\n\n* Experiment dataset is small. This is potentially because the proposed method has very high complexity due to matrix inverse (see feed-forward rule in paragraph The ResolvNet Layer. The authors need to conduct experiment on larger datasets (e.g., OGBN) and report complexity in terms of FLOP/Wall-clock time.\n\nFollowing this important point by the reviewer, we have included an additional section (Appendix J: Scaling ResolvNet to large Graphs) that discusses in detail how ResolvNet is scaled to large graphs.\n\nAs detailed in Appendix J, the key ingredient here is a Neumann-Representation of the resolvent that avoids an explicit matrix-inversion.\n\nThis replaces the hypothetical $\\mathcal{O}(N^3)$ complexity during preprocessing and  $\\mathcal{O}(N^2)$ complexity during training and inference that a na\u00efve implementation of ResolvNet would constitute with a  $\\mathcal{O}(EFN)$ complexity (i.e. the complexity of sparse-dense matrix multiplication). Here $N$ is the number of nodes,  $E$ is the number of edges and $F$ is the feature dimension.\n\nBeyond these theoretical considerations and in addition to the empirical complexity analysis already provided in Appendix G, we also followed the recommendation of the reviewer and evaluated complexity experimentally  (reporting  (reliably measurable) MACS (i.e. multiply-accumulate operations) in-lieu of FLOPS):\n\nWe trained  ResolvNet, ChebNet (both with $K = 1$) and GAT in a two-layer deep configuration with $64$ hidden units in each layer for $100$ epochs on the OGBN-Arxiv and OGBN-Products datasets, which are comprised of $169,343$ Nodes and more than 2M nodes respectively.\n\nResults are collected in the tables below:\n\nResults are collected in the tables below:\n\nOGBN-Arxiv:\n|\t    \t |  trainable parameters  |   max-memory-allocated [GB] | Training-time per epoch [ms]  | MACS|\n| ----------- | ----------- | ----------- | ----------- | ----------- |\n| ResolvNet  | 39848 \t| 0.68  \t| 634.71| 4,595,291,648 |\n|GAT  \t| 351272 |  25.53   |  1089.96\t| 3,468,144,640      |\n|ChebNet  |  15016\t| 0.42\t| 64.47 \t| 433,518,080        |\n\n\nOGBN-Products:\n\n| |  trainable parameters |   max-memory-allocated [GB] | Training-time per epoch [ms]  | MACS |\n| ----------- | ----------- | ----------- | ----------- | ----------- |\n| ResolvNet  | 39848 | 19.23\t|  37818.73 |   57,261,648,000  |\n| GAT  | 351272 | OOM | OOM | OOM |\n| ChebNet   | 15016\t| 8.48 | 548.78\t|   7,176,793,216 |\n\n\n\nAs can be inferred from the tables above, ResolvNet takes significantly less time to train per epoch on OGBN-Arxiv, when compared to GAT. It also needs considerably less GPU memory and multiply-accumulate operations compared to GAT.\n\nCompared to ChebNet, our method needs to compute an order of magnitude more sparse-dense matrix multiplication operations (as detailed in Appendix J). This is reflected in the number of performed MACs, which for ResolvNet is roughly an order of magnitude larger than that for ChebNet.\n\nOn OGBN-Products, our method remains more complex than ChebNet. In contrast to GAT however, ResolvNet remains trainable."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573416935,
                "cdate": 1700573416935,
                "tmdate": 1700573416935,
                "mdate": 1700573416935,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W4ii2MLlCa",
            "forum": "vEgLnT9avP",
            "replyto": "vEgLnT9avP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4788/Reviewer_MwCo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4788/Reviewer_MwCo"
            ],
            "content": {
                "summary": {
                    "value": "This paper points out a problem in graph neural networks where certain strongly connected parts, like cliques, can limit the spread of information in the graph. To solve this, the authors introduce the idea of multi-scale consistency. This means keeping a connected way of spreading information even if the connection density in the graph changes for the node level tasks. For the graph level tasks, it means graphs generated from the same ground truth, which are at different resolutions,  should be assigned similar feature vectors. The research shows that many popular GNN designs don't have this feature. To fix this, the authors of this work propose ResolvNet, a new Spectral-based GNN design based on a math concept called resolvents. By applying resolvent of the Laplacian, \tResolvNet is able to have the same effect of projecting the dense connected components in the original graph to a coarsened graph, then efficiently propagating information and finally projecting the embedding back to the original graph node level. Authors have theoretically proved that the proposed method is able to consistently integrate multiple connectivity scales occurring within graphs. Also , extensive experiments have shown that ResolvNet has multi-scale consistency and does better than other baselines in many tasks using various datasets. It is also shown that the proposed method is more stable than the baselines under different resolution scales."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "*Originality*: The paper identifies a novel issue in graph neural networks and introduces an effective framework, ResolvNet, to address it. This represents a significant and innovative contribution to the field.\n\n*Quality*: The investigative experiments and primary results presented in the paper are persuasive. Supported by solid theoretical proofs, this work stands out as a high-quality piece of research.\n\n*Clarity*: The paper is exceptionally well-organized. Its straightforward and lucid presentation of both the problem and the proposed solution allows readers to grasp the content quickly and comprehensively.\n\n*Significance*: By highlighting a new issue and offering an effective framework to tackle it, this work holds substantial impact potential for the broader community."
                },
                "weaknesses": {
                    "value": "*Insufficient Analysis*: The paper could benefit from more extensive ablation studies and parameter analyses. Understanding how variations in parameters like $\\omega$ and $k$, as defined in the ResolvNet Layer, impact the final results would provide deeper insights.\n\n*Complexity of Concepts*: The concept of \"resolvents\" is not a commonly understood mathematical idea. Providing more explanations, along with practical application cases, would greatly aid readers in grasping this concept and its significance in the proposed framework.\t\n\nMinor issue:\n\n*Notation Introduction*: The paper occasionally lacks a comprehensive introduction to certain notations. For instance, the notation $T$ in section 3.2 is introduced without adequate context or explanation."
                },
                "questions": {
                    "value": "The datasets utilized in this study are primarily small to medium-sized. How would ResolvNet perform in terms of accuracy and computational time when applied to larger datasets?\n\nHow do learnable filters as polynomials in resolvents achieve similar effects of up-projection operator and down-projection operator. It may need more illustrations and explanations for this in Sec 3.2."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4788/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782961968,
            "cdate": 1698782961968,
            "tmdate": 1699636461162,
            "mdate": 1699636461162,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zoSRmzwmUF",
                "forum": "vEgLnT9avP",
                "replyto": "W4ii2MLlCa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Added Ablation Study and corrected notational Typo"
                    },
                    "comment": {
                        "value": "We would like to sincerely thank the reviewer for the careful evaluation of our paper and appreciation of our results. We were especially happy to read that\n\n> [our paper] represents a significant and innovative contribution to the field,\n\n> this work stands out as a high-quality piece of research\n\nand that\n\n>  The paper is exceptionally well-organized\n\nLet us address the raised points individually:\n\n* The paper could benefit from more extensive ablation studies and parameter analyses. \n\nFollowing this comment, we have conducted ablation studies on both $\\omega$ and $K$. The results are collected in Appendix K in our updated manuscript.\n\nAs a rough summary, we see that increasing $K$ beyond $K = 1$ does not significantly improve performance. This is reminiscent of similar results for e.g. Chebnet, whose polynomial filtering operation generically does not perform better if the polynomial dimension is increased. \n\nFor $\\omega$, we see that a too small value has  negative implications as far as performance is concerned. For sufficiently large $\\omega$ (small) variations of $\\omega$ do not affect performance. I.e. ResolvNet is stable under (sufficiently small) variations in $\\omega$.\n\n* Complexity of Concepts: The concept of \"resolvents\" is not a commonly understood mathematical idea. Providing more explanations, along with practical application cases, would greatly aid readers in grasping this concept and its significance in the proposed framework. \n\nWe will be very glad to do so! For a camera ready version, we will include an additional section in the Appendix, detailing the significance of resolvents in other fields such as Mathematics and Physics, as well as a detailed mathematical introduction, (following the pedagogical structure of  (Teschl (2014)).\n\n* Minor issue: Notation Introduction: The paper occasionally lacks a comprehensive introduction to certain notations. For instance, the notation $T$ in section 3.2 is introduced without adequate context or explanation.\n\nThis is indeed a typo, which we thank the reviewer for spotting. We corrected it and now write the Laplacian as $\\Delta$ instead of $T$ also in Section 3.2."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572559955,
                "cdate": 1700572559955,
                "tmdate": 1700737492817,
                "mdate": 1700737492817,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7mxLkY8Ga9",
                "forum": "vEgLnT9avP",
                "replyto": "W4ii2MLlCa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "content": {
                    "title": {
                        "value": "ResolvNet on large graphs"
                    },
                    "comment": {
                        "value": "* The datasets utilized in this study are primarily small to medium-sized. How would ResolvNet perform in terms of accuracy and computational time when applied to larger datasets?\n\nThis is indeed an important question. Following this raised point, we now include a new and additional section (Appendix J: Scaling ResolvNet to large Graphs) in the Appendix that discusses in detail how ResolvNet is scaled to large graphs.\n\nThe key ingredient here is a Neumann-representation of the resolvent that avoids an explicit matrix-multiplication:\n$$ (T - zId)^{-1} \\approx \\frac{1}{z} \\sum\\limits_{k=1}^K (-T/z)^k $$\nThis approximation becomes exact as $K$ is taken o infinity. A similar approximation was already used to scale the PPNP method of \u201cPPNP Paper\u201d to large graphs.\nThis replaces the hypothetical $\\mathcal{O}(N^3)$ complexity during preprocessing and  $\\mathcal{O}(N^2)$ complexity during training and inference that a na\u00efve implementation of ResolvNet would constitute with a  $\\mathcal{O}(EFN)$ complexity (i.e. the complexity of sparse-dense matrix multiplication). Here $N$ is the number of nodes,  $E$ is the number of edges and $F$ is the feature dimension.\n\nBeyond these theoretical considerations and in addition to the empirical complexity analysis already provided in Appendix G, we also evaluated complexity on large graphs experimentally: \nWe trained  ResolvNet, ChebNet (both with $K = 1$) and GAT in a two-layer deep configuration with $64$ hidden units in each layer for $100$ epochs on the OGBN-Arxiv and OGBN-Products datasets, which are comprised of $169,343$ Nodes and more than 2M nodes respectively.\n\nResults are collected in the tables below:\n\nOGBN-Arxiv:\n|\t    \t |  trainable parameters  |   max-memory-allocated [GB] | Training-time per epoch [ms]  | MACS|\n| ----------- | ----------- | ----------- | ----------- | ----------- |\n| ResolvNet  | 39848 \t| 0.68  \t| 634.71| 4,595,291,648 |\n|GAT  \t| 351272 |  25.53   |  1089.96\t| 3,468,144,640      |\n|ChebNet  |  15016\t| 0.42\t| 64.47 \t| 433,518,080        |\n\n\nOGBN-Products:\n\n| |  trainable parameters |   max-memory-allocated [GB] | Training-time per epoch [ms]  | MACS |\n| ----------- | ----------- | ----------- | ----------- | ----------- |\n| ResolvNet  | 39848 | 19.23\t|  37818.73 |   57,261,648,000  |\n| GAT  | 351272 | OOM | OOM | OOM |\n| ChebNet   | 15016\t| 8.48 | 548.78\t|   7,176,793,216 |\n\n\n\nAs can be inferred from the tables above, ResolvNet takes significantly less time to train per epoch on OGBN-Arxiv, when compared to GAT. It also needs considerably less GPU memory and multiply-accumulate operations compared to GAT. \n\nCompared to ChebNet, our method needs to compute an order of magnitude more sparse-dense matrix multiplication operations (as detailed in Appendix J). This is reflected in the number of performed MACs, which for ResolvNet is roughly an order of magnitude larger than that for ChebNet.\n\nOn OGBN-Products, our method remains more complex than ChebNet. In contrast to GAT however, ResolvNet remains trainable.\n\nAs for accuracy on large graphs, we again expect better performance on homophilic graphs as ResolvNet\u2019s (necessary) inductive bias towards homophily is independent of graph-size. A full experimental investigation of this question is still ongoing and will be included in  a camera ready version of this submission."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572810758,
                "cdate": 1700572810758,
                "tmdate": 1700573430640,
                "mdate": 1700573430640,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l7DXf2Zv8l",
                "forum": "vEgLnT9avP",
                "replyto": "W4ii2MLlCa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The relation of projection operators and resolvents"
                    },
                    "comment": {
                        "value": "* How do learnable filters as polynomials in resolvents achieve similar effects of up-projection operator and down-projection operator. It may need more illustrations and explanations for this in Sec 3.2.\n\nThe key insight here is provided by Theorem 3.3. From this Theorem, we infer that for multi-scale graphs we have\n$$R_z(\\Delta) \\approx J^{\\uparrow}R_z(\\Delta_{\\text{collapsed}}) J^{\\downarrow}. $$\nAdditionally, we have $ J^{\\downarrow} \\cdot J^{\\uparrow} = Id_{G_{\\text{collapsed}}}$, as detailed in Appendix B.\nCombining these two facts yields \n$$(R_z(\\Delta))^k \\approx J^{\\uparrow}(R_z(\\Delta_{\\text{collapsed}}))^k J^{\\downarrow}. $$\n\nThus also polynomials in resolvents effectively first project down to $G_{\\text{collapsed}}$, then propagate there and subsequently interpolate pack up to the original graph $G$. \n\nWe provide a full discussion of this in Appendix B (c.f. especially Theorem B.4).\nFollowing this comment by the reviewer, we now highlight this even stronger in the main body of our submission."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573079523,
                "cdate": 1700573079523,
                "tmdate": 1700573103820,
                "mdate": 1700573103820,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5d230zsJDQ",
            "forum": "vEgLnT9avP",
            "replyto": "vEgLnT9avP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4788/Reviewer_nd7U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4788/Reviewer_nd7U"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers graphs with two scales, one in which nodes are strongly connected into clique-like communities and a another scale in which the connections are weaker and uniform over the graph. A distinction is based between the two communities based on spectral analysis: the second eigenvalue of the first scale is much higher than all the eigenvalues of the second scale. The idea of resolvents is proposed to deal with such graphs and two types of filters, type-0 and type-1 are defined to propagate information in a GNN. The ideas are validated empirically. It is shown that the proposed method works well on graphs with high homophily."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea of separating a network into multiple scales is nice. The problem is well defined and motivated\n\nThe use of resolvents to design filters is novel. A theory is developed to justify the methods.\n\nThe experimental results show the usefulness of the method."
                },
                "weaknesses": {
                    "value": "1. It would be good if the authors could demonstrate the performance of their methods on synthetically generated graphs, say using stochastic block models. That would allow all parameters to be controlled.\n\n2. It is not clearly defined how the two kinds of filters are combined: does a node learn which filter to use?\n\n3. There are some other obvious baselines with which the authors could compare their methods: \na. Apply pooling to learn the clusters (say using diffpool, gpool, eigenpooling among others) and then use a generic GNN on the coarsened graph.\nb. Separate the two networks using Gaussian Mixture Models, have two different GNNs for the two scales, and combine the two representations for node/graph prediction.\n\n4. The abstract is not clear, especially the sentence: At the graph level, multi-scale ..\" The last sentence also seems to make bolder claims than what the experiments show."
                },
                "questions": {
                    "value": "Please look at the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4788/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698880625463,
            "cdate": 1698880625463,
            "tmdate": 1699636461094,
            "mdate": 1699636461094,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Xr4veeIuB2",
                "forum": "vEgLnT9avP",
                "replyto": "5d230zsJDQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Conducted additional synthetic experiments and compared with suggested baselines"
                    },
                    "comment": {
                        "value": "We would like to sincerely thank the reviewer for the careful review of our paper and the insightful advice, which we have followed diligently.\nWe were especially happy to read that our\n> Idea of separating a network into multiple scales is nice\n\nand that our\n\n> experimental results show the usefulness of the method\n\nLet us address each raised point individually:\n\n* It would be good if the authors could demonstrate the performance of their methods on synthetically generated graphs, say using stochastic block models. That would allow all parameters to be controlled.\n\nWe think that this is a great idea as stochastic block models indeed allow to generate graphs with strongly connected communities. \n\nFollowing this comment by the reviewer, we have thus conducted  additional experiments and ablation studies on graphs drawn from stochastic block models. We provide precise descriptions of our experimental setup, as well as full experimental results in the newly created section \u201cAppendix H: Additional Experiments on synthetic Data\u201d.\n\nHere we provide a brief summary of the main result:\n\nWe generate graphs using a custom stochastic block model: Apart from **cluster-size** and **number-of-clusters**, our model takes as input two probabilities: \n\nThe probability $p_{\\text{inner}}$ determines the **likelihood of two nodes within the same cluster being connected** by an edge. \n\nThe probability $p_{\\text{inter}}$ determines the **likelihood for two nodes in different clusters to be connected** by an edge.\n\nTo numerically verify Theorem 4.2 and showcase  the intuition behind Figure 2, we first keep all parameters  except for $p_{\\text{inner}}$ constant.\nWe then vary $p_{\\text{inner}}$ from $p_{\\text{inner}} = 0$ (no edges within the clusters) to $p_{\\text{inner}} = 1$ (fully connected clusters).\n\nIn this setup, we compare the feature vectors that ResolvNet and baselines generate for such graphs with the feature vectors generated by these models for coarse-grained versions of the original graphs where clusters are collapsed to single nodes.\n\n**Figure 14 (b-d) in Appendix H* plots corresponding results: As the connectivity within the clusters increases towards full connectivity, the norm difference between features of the original graph and that of its collapsed version decreases for ResolvNet. \nThis is what we desire, as discussed in Section 2 and Section 4: Very strongly connected sub-graphs are supposed to be treated in a similar way that single (collapsed) nodes are.\n\nAs is also evident from **Figure 14 in Appendix H**, baselines (in contrast to ResolvNet) do not exhibit this behaviour. This implies that these baselines are not able to consistently integrate multiple scales and hence are not transferable between different graphs describe the same underlying object at different resolution scales.\n\nBeyond these results Appendix H contains additional investigations and experiments (c.f. e.g. **Figure 15** and **Figure 16** in **Appendix H**)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571012845,
                "cdate": 1700571012845,
                "tmdate": 1700572171631,
                "mdate": 1700572171631,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dVwWIu9QkP",
                "forum": "vEgLnT9avP",
                "replyto": "5d230zsJDQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Experiments for additional baselines I: Pooling"
                    },
                    "comment": {
                        "value": "* There are some other obvious baselines with which the authors could compare their methods: a. Apply pooling to learn the clusters (say using diffpool, gpool, eigenpooling among others) and then use a generic GNN on the coarsened graph. \n\nWe had indeed not thought of this, as our main intention is to design a model that might be deployed at any scale, and does not need to coarse-grain given graphs to a predetermined scale, like pooling does. \n\nNevertheless, we agree with the reviewer that this is an interesting comparison. We thus followed the reviewers recommendation and combined GCN as a representative baseline with Self-Attention Graph Pooling (SAG-pooling) [1]. We chose SAG-pooling as the pooling method since its implementation does not need data to be loaded in a dense format and hence is compatible with experimental setup.\n\nWe considered two implementations:\n\n\u201cSAG\u201d simply pools first and then runs GCN on the coarsened graph. \n\n\u201cSAG-Multi\u201d instead generates graph representations first at the original scale via GCN, then pools, runs GCN on the coarsened graph and generates a second (coarser) Graph representation. This procedure is iterated again and all three representations are combined into a single representation.\n\n\n\n\nWe then reran all experiments on QM$7$ data described in Section 5 again also for these two additional baselines.\n\n\n\n\n\n\nWe here first provide an updated version of Table 2 (listing prediction errors for atomic energy prediction on original QM$7$) with these two baselines added:\n\n| QM$7$  | MAE [kcal/mol] | \n|---|---|\n|BernNet   |113.57 $\\pm$  62.90  | \n| GCN  | 61.32 $\\pm$ 1.62 | \n|  ChebNet | 59.57 $\\pm$ 1.58  | \n|  ARMA |  59.39 $\\pm$ 1.79| \n|  SAG |  64.69 $\\pm$  3.24| \n|  SAG-Multi|  61.36 $\\pm$ 4.47| \n|  ResolvNet |  16.52 $\\pm$0.67| \n\n\n\nAs can be inferred from this  Table , either way of combining GCN (as the prototypical message passing network) with pooling does not lead to a performance increase on the multi-scale data of the QM$7$-dataset. \n\n\n\nWe also investigated the graph-level multi-scale stability of these models, updating Table 3 (which evaluated the models trained for Table 2 on coarse-grained/collapsed versions of the molecular graphs in QM$7$):\n\n|  QM$7$-coarse | MAE [kcal/mol] |\n|---|---|\n|BernNet   |580.67 $\\pm$99.27 |   \n| GCN  |124.53 $\\pm$ 34.58 |   \n|  ChebNet | 645.14 $\\pm$ 34.59  |   \n|  ARMA |   248.96 $\\pm$ 15.56  |   \n|  SAG |  550.42 $\\pm$ 24.43| \n|  SAG-Multi|  246.95 $\\pm$ 68.49| \n|  ResolvNet |  16.23 $\\pm$ 2.74| \n\nFrom this table with the previous one, we may infer that including a learned pooling operations  has a drastically negative effect when aiming to transfer models between graphs describing the same object (in this case a molecule) at different resolution-scales:\n\nAdditional discussions and an investigation of the behaviour of the (GNN + Pooling)-baselines in comparison with ResolvNet under continuously varying the scale-imbalance within graphs is discussed in Appendix G.1 (c.f. also Fig. 13)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571703325,
                "cdate": 1700571703325,
                "tmdate": 1700571727921,
                "mdate": 1700571727921,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W3lJa7wQWf",
                "forum": "vEgLnT9avP",
                "replyto": "5d230zsJDQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Experiments for additional baselines II: Manual Scale Separation"
                    },
                    "comment": {
                        "value": "*    b. Separate the two networks using Gaussian Mixture Models, have two different GNNs for the two scales, and combine the two representations for node/graph prediction.\n\nWe agree that this is an interesting point for comparison. To implement the corresponding experiment, we considered the QM$7$ dataset.\n\nAs described in Section 5, weights here essentially correspond to inverse distance between atoms. We hence separate the scales by fixing a cut-off distance: Nodes that are within the cut-off distance of each other are considered to be connected via an edge in $G_{\\text{high}}$. Pairs of nodes that are further apart from each other provide an edge in $G_{\\text{reg}}$.\n\n The resulting prediction accuracies are provided below:\n| MAE[kcal/mol]  | Standard  |Mixture   | \n|---|---|---|\n|BernNet   |113.57 $\\pm$  62.90  |  85.24 $\\pm$9.60 |   \n| GCN  | 61.32 $\\pm$ 1.62 | 64.16 $\\pm$ 1.89 |   \n|  ChebNet | 59.57 $\\pm$ 1.58  | 47.11 $\\pm$ 1.89  |   \n|  ARMA |  59.39 $\\pm$ 1.79| 30.88 $\\pm$ 2.82  |   \n|  **ResolvNet** | **16.52  $\\pm$ 0.67** | [**16.52  $\\pm$ 0.67**] |  \n\nAs can be inferred from this table, the idea of the reviewer significantly improves accuracy for most baselines. However, mean absolute errors of baselines, even after the two-scale-mixture modification, is still at least roughly twice as large as that of Resolvnet (16.52 $\\pm$ 0.67 kcal/mol).\n\nAdditionally, in order to combine baselines with such a scale-separation procedure, we had to manually select graph decomposition into the respective scales, while ResolvNet automatically takes care of this.\n\nFull details on the considerations above are provided in Appendix G.2."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571848119,
                "cdate": 1700571848119,
                "tmdate": 1700664610733,
                "mdate": 1700664610733,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rOGrnznr9T",
                "forum": "vEgLnT9avP",
                "replyto": "5d230zsJDQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Adressing additional points and References"
                    },
                    "comment": {
                        "value": "* It is not clearly defined how the two kinds of filters are combined: does a node learn which filter to use?\n\nIn principle, any Type-0 filter can represent any fixed Type-I filter, so that the type of filter may indeed be learned.\nIn practice, however it is beneficial to equip models with a stronger inductive bias towards the desired type of transferability (c.f. the discussion after Theorem 4.1). Thus in practice we treat the type-of-filter-choice as a binary hyperparameter. We have made this clearer in our updated manuscript now (c.f. the updated discussion after eq. (2)).\n\n* The abstract is not clear, especially the sentence: At the graph level, multi-scale ..\" The last sentence also seems to make bolder claims than what the experiments show.\n\nFollowing this comment, we  have amended the last sentence of the abstract and changed the \u201cAt the graph level, multi-scale ..\"- sentence  to \u201cAt the graph-level, a multi-scale consistent graph neural network assigns similar feature vectors to distinct graphs.\" describing the same object at different resolutions\".\nWe hope that the new formulation is clearer now; but would be happy to change the sentence again.\n\nReferences:\n\n[1]: Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,  California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 3734\u20133743. PMLR, 2019. URL http://proceedings.mlr.press/v97/lee19c.html"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572055599,
                "cdate": 1700572055599,
                "tmdate": 1700572249066,
                "mdate": 1700572249066,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zzePhHRtnW",
                "forum": "vEgLnT9avP",
                "replyto": "rOGrnznr9T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4788/Reviewer_nd7U"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4788/Reviewer_nd7U"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response."
                    },
                    "comment": {
                        "value": "Thanks for conducting extensive experiments based on the feedback. I am not sure of your commentary on the performance of the mixture model. The gap between the two approaches is quite significant."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627383439,
                "cdate": 1700627383439,
                "tmdate": 1700627383439,
                "mdate": 1700627383439,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pBWuXr74gx",
                "forum": "vEgLnT9avP",
                "replyto": "5d230zsJDQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "content": {
                    "title": {
                        "value": "ResolvNet performs significantly better than mixture models and mixture models are not transferable between different resolutions"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the continued engagement and are very happy to extend our discussion of mixture models: \n\n\n\n\n| QM$7$-MAE [kcal/mol]  | |\n|---|---|\n|BernNet-mixture   |  85.24 $\\pm$9.60 |   \n| GCN-mixture  | 64.16 $\\pm$ 1.89 |   \n|  ChebNet-mixture |  47.11 $\\pm$ 1.89  |   \n|  ARMA-mixture |  30.88 $\\pm$ 2.82  |   \n| **ResolvNet** |  **16.52 $\\pm$ 0.61** |\n\n\nPerforming a manual scale-separation and running simultaneous networks on the different graph-structures improved the relative performance of baselines. This is most pronounced for ARMA, where the performance gap between the standard- and mixture-implementations is roughly a factor of two, which is quite significant; as the reviewer points out.\n\nHowever, **ResolvNet performs better** than this best performing micture-model (ARMA-mixture) **by an additional factor of two**; i.e. with a performance improvement of exactly the same significance.\n\nThus ResolvNet is still able to incorporate multi-scale data a lot better than mixture-models.\n\n\nAs an important additional point we note the following:\n\n**Considering mixture-models (as opposed to ResolvNet) does not lead to graph-level multi-scale consistency.**\n\nMixture-models (as opposed to ResolvNet) are unable to consistently handle graphs describing the same object at different resolution scales:\n\nTo showcase this, we have now investigated the graph-level multi-scale stability of mixture-models, by rerunning the experiment corresponding to Table 3 (which evaluated the models trained for Table 2 on coarse-grained/collapsed versions of the molecular graphs in QM$7$) for mixture-models:\n\n\n\n\n| QM$7_{\\text{coarse}}$MAE[kcal/mol] | |\n|---|---|\n|BernNet-mixture    |300.02 $\\pm$  18.13|   \n| GCN-mixture   | 198.44 $\\pm$ 35.55|   \n|  ChebNet-mixture  |  821.10 $\\pm$ 41.62  |   \n|  ARMA-mixture  |  597.12 $\\pm$ 35.54  |   \n| **ResolvNet** |  **16.23 $\\pm$ 2.74** |\n\nWhen confronted with such different resolution scales, the performance difference between ResolvNet and mixture-models becomes even larger:\n\n\nWhile ResolvNet performed better than mixute models by a factor of at least two on same-resolution multi-scale data, the performance gap now varies between a factor of $12$ to $50$ if multiple graph-resolutions are considered.\n\nThus:\n1) For single-resolution multi-scale data, ResolvNet performs better than mixture-models by a factor of two and more\n2) In multi-resolution, multi-scale settings ResolvNet performs better than mixture-models by a factor of $12$ to $50$\n\nWe hope our extended discussion was able to clarify our earlier commentary. We of course stand by, should further questions arise."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664492065,
                "cdate": 1700664492065,
                "tmdate": 1700664717897,
                "mdate": 1700664717897,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sqILz60JgJ",
                "forum": "vEgLnT9avP",
                "replyto": "pBWuXr74gx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4788/Reviewer_nd7U"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4788/Reviewer_nd7U"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the details"
                    },
                    "comment": {
                        "value": "1. What was the effect of manual separation on the node classification datasets? \n2. If I understand the authors' response, the performance of the method should be judged more on the regression task than the node classification task. Why then there is so much emphasis being placed on node classification in the experimental results? Are there any other tasks where the method does well?"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724586836,
                "cdate": 1700724586836,
                "tmdate": 1700724586836,
                "mdate": 1700724586836,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "euK6se4AXt",
                "forum": "vEgLnT9avP",
                "replyto": "5d230zsJDQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4788/Authors"
                ],
                "content": {
                    "title": {
                        "value": "ResolvNet is versatile and performs well in multiple settings"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the continued engagement with our work.\n\n* What was the effect of manual separation on the node classification datasets? \n\nOne of the inherent drawbacks of the suggested manually-scale-separated mixture-model baseline is that any given graph needs to be manually separated into distinct graph structures (corresponding to the different scales) before such a baseline may be run.\n\nThe node classification datasets (i.e. Citeseer, Cora, etc.) are not inherently multi-scale.\nIt is thus not clear how such a manual separation into distinct scales as suggested  should proceed on these datasets. \n\nShould the reviewer have a separation method in mind, we would be happy to combine this separation-method with the already suggested mixture-models and conduct further experiments with this additional baseline in a camera ready version of our submission.\n\n* If I understand the authors' response, the performance of the method should be judged more on the regression task than the node classification task. Why then there is so much emphasis being placed on node classification in the experimental results? Are there any other tasks where the method does well?\n\nThe focus of our paper is to introduce the concept of multi-scale consistency for graph neural networks. As we detail in the paper, this concept has two aspects:\n\n1)\tFor a fixed multi-scale graph, multi-scale consistency refers to an unrestricted flow of information over the entire graph, even if connectivity varies within said graph\n\n2)\tWhen confronted with graphs describing the same object at different resolution scales, a multi-scale consistent network assigns similar feature vectors to graphs describing this same object at different resolution scales.\n\nWe showcase the lack of multiscale consistency of baselines and how the proposed ResolvNet is able \u2013 in contrast to baselines \u2013  to consistently incorporate varying  scales in different experiments:\n\n1)\tTo showcase the first point 1) above, we e.g. conduct experiments on QM$7$ data. Here edge weights essentially correspond to inverse distances. Atoms that are far-away from each other are thus connected via edges with far lower weights. However, long-range communication (i.e. along the weight suppressed lower connectivity scale) within such molecular data is important for prediction tasks (c.f. the references provided in our paper). Since ResolvNet is able to better incorporate such varying connectivity scales than baselines (as established in extensive theoretical discussions, in our experiments and further cemented via additional experiments in the discussion phase), ResolvNet is able to better predict molecular properties than baselines.\n\n2)\tTo showcase the second point 2) above, we e.g. confronted ResolvNet and baselines with coarse-grained descriptions of molecules during inference. ResolvNet\u2019s prediction capabilities remained the same, while those of baselines severely declined (c.f. Table 3, or the discussion in an earlier comment).\n\nWhile these are the settings that our paper is concerned with and indeed the type of tasks ResolvNet was designed for, we find it important to also evaluate ResolvNet against other baselines on the standard banchmark of node classification. We make this clear by writing \n\n> To establish that the proposed ResolvNet architecture not only performs well in multi-scale settings, we conduct node classification experiments [\u2026]\n\nwhen discussing the corresponding node-classification results of Table 1.\n\nAn inductive-bias towards homophily is necessary, to achieve multiscale consistency, as was discussed in the paper. We thus expect ResolvNet to perform well on homophilic graphs. \nThis is indeed what our experiments show, with ResolvNet being by far the best performing method in this realm (c.f. Table 1).\n\nIn total we thus established ResovNet\u2019s superior performance in three different real-world settings:\n\n1)\tFor node classification on homophilic graphs, ResolvNet was the best performing method (c.f. Table 1)\n\n2)\tOn muli-scale data ResolvNet performed significantly better than baselines (c.f. e.g. Table 2 for experiments on corresponding real world data)\n\n3)\tIn the multi-resolution-scale setting, ResolvNet also performed significantly better than baselines (c.f. e.g. Table 3 or Figure 7)\n\nWe hope this discussion has clarified the versatility aspect of ResolvNet.\n\nWe of course stand by should additional questions arise and would be happy to include even more experiments in a camera ready version of our paper."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731992778,
                "cdate": 1700731992778,
                "tmdate": 1700732632612,
                "mdate": 1700732632612,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]