[
    {
        "title": "Enable Lanuguage Models to Implicitly Learn Self-Improvement From Data"
    },
    {
        "review": {
            "id": "HFdNgbZxpP",
            "forum": "2tVHNRZuCs",
            "replyto": "2tVHNRZuCs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4154/Reviewer_LeAH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4154/Reviewer_LeAH"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a framework, PIT, that trains an additional model that improves the output of an LLM with RLHF. The reward is set to be the preference gap between the improved output and the original LLM output, i.e. implicitly trained with the improvement goal of better aligning with human preferences."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper introduces an interesting framework, PIT, that can learn how to improve the LLM output in a RLHF way, which does not require a significant amount of human annotation and prompt design.\n2. Instead of updating the original LLM as the original RLHF, PIT proposes to update the model that targets improving the output for the original LLM, which is a novel technique.\n3. The paper evaluates the PIT technique with the previous self-refine method that also tries to improve the LLM output, and demonstrates a better result. It also shows a set of ablation studies that show the effectiveness of each component."
                },
                "weaknesses": {
                    "value": "1. The paper motivates why using RLHF in output improvement is better than just prompting the LLM to give feedback, but it is unclear why we should apply RLHF for the improvement, instead of directly applying RLHF to the original LLM. It would be better to see the comparison in the evaluation.\n2. Comparing PIT with Self-Refine seems unfair as PIT requires more human annotation as it requires additional human preference labels.\n3. The description of PIT is a bit unclear as the model used in each component of the method is not clearly stated."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4154/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4154/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4154/Reviewer_LeAH"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4154/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698560782297,
            "cdate": 1698560782297,
            "tmdate": 1700732188527,
            "mdate": 1700732188527,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iLXKVfovle",
                "forum": "2tVHNRZuCs",
                "replyto": "HFdNgbZxpP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer LeAH"
                    },
                    "comment": {
                        "value": "Thank you for the thoughtful feedback! We are happy that you find our approach interesting, novel, and effective! We understand you may have some confusion and concerns, which we address as follows.\n\n> ### **W1: The paper motivates why using RLHF in output improvement is better than just prompting the LLM to give feedback, but it is unclear why we should apply RLHF for the improvement, instead of directly applying RLHF to the original LLM. It would be better to see the comparison in the evaluation.**\n\nThe original LLM (policy model in Section 3 and \u201cLLMs\u201d in Figure 1) that generates the initial response is already RLHF-finetuned. PIT takes initial responses generated by the original LLM as input, and generates improved responses. PIT and the original LLM were finetuned with the same starting pre-trained checkpoint and data, but PIT is not a continual training of the original LLM (Continual training may not be suitable since their input formats are different. Appendix A illustrates more details). PIT only utilizes the output of the original LLM during training and inference time and does not use its weights. We compare PIT with (1) original responses given by the original LLM and (2) Improved responses given by Self-Refine on the original LLM to show PIT is better than the original LLM with Self-Refine. \n\nWe apologize for not making ourselves clearer and causing you some potential confusion. We demonstrate our workflow in Appendix A in the revised paper to make our approach clearer and more organized. We hope these extra demonstrations are helpful for a better understanding of our approach and can clarify this concern. \n\n> ### **W2: Comparing PIT with Self-Refine seems unfair as PIT requires more human annotation as it requires additional human preference labels.**\n\nThe training data of PIT is exactly the same as the data we use to train the RLHF-finetuned model. Thus, PIT needs **no extra data**. A more detailed workflow demonstration can be found in Appendix A in the revised paper. Again, we apologize for the confusion and for not making this point clearer.\n\n> ### **W3: The description of PIT is a bit unclear as the model used in each component of the method is not clearly stated.**\n\nWe make a more detailed workflow demonstration containing the input format training algorithm blocks in Appendix A in the revised paper. We believe general response GR2 will also be helpful for understanding. We hope now the framework is clearer.\n\n\nWe hope our reply addresses your questions, and please let us know if you have further questions. We would be happy to continue the discussion if needed."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699855920752,
                "cdate": 1699855920752,
                "tmdate": 1699856101043,
                "mdate": 1699856101043,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TrkqvXKDbh",
                "forum": "2tVHNRZuCs",
                "replyto": "iLXKVfovle",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4154/Reviewer_LeAH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4154/Reviewer_LeAH"
                ],
                "content": {
                    "title": {
                        "value": "RE: Response to reviewer LeAH"
                    },
                    "comment": {
                        "value": "Thanks to the authors for the responses! They solved most of my concerns. Thus, I increased the score to 6."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732239634,
                "cdate": 1700732239634,
                "tmdate": 1700732239634,
                "mdate": 1700732239634,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oNhkijDosL",
            "forum": "2tVHNRZuCs",
            "replyto": "2tVHNRZuCs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4154/Reviewer_tfvt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4154/Reviewer_tfvt"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an RLHF-style fine-tuning routine to allow large language models (LLMs) to self-improve. The authors propose PIT that modifies the RLHF routine in all three stages: at the supervised fine-tuning stage, PIT maximises the likelihood of the better response conditional on both the instruction and the worse outputs (instead of the instructions only). In the reward model training stage, PIT encodes 4 pairwise relations between good and worse outputs instead of simply optimising the reward gap between better and worse responses. In the RL stage, instead of defining reward only in terms of supervised finetuned models and the RL model, PIT utilises multiple stages of RL to improve both over the annotated examples and iteratively improve over its previous responses (thereby achieving self-improvement). Experiments are done on 3 RLHF datasets, and the authors show that their proposed methods compare favourably both over the original response and over self-refine, a prompt-based self-improvement method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- I found the paper\u2019s motivation of implicitly self-improving from preference data instead of explicit rubrics construction to be well thought out, convincing and likely of interest to the community. \n- The execution is largely reasonable and intuitive and I like the pairwise relation encoding in the methodology, although there is room for improvement in this area; see weaknesses. The idea of curriculum RL to iteratively and continuously improve on LLM outputs is also intriguing.\n- The experimental section is largely thorough, and the improvement over baseline and prompting-based self-refine seems largely convincing. Experimental support is also provided for some (but not all) of the critical designs, such as the use of two (or more) stages of reinforcement learning and the indispensability of each stage. \n- The paper is generally well-written and clear."
                },
                "weaknesses": {
                    "value": "- The computational cost and the execution difficulty should be more clearly stated: while the method seems to lead to a stronger gain than self-refine, such an improvement is not always significant and sometimes self-refine seems to be stronger by some metric (e.g., in terms of GPT-4 evaluation), although the authors have given possible explanations as to why they occur. On the other hand, self-refine as a prompting-based method is much easier to execute and cheaper I think the paper would benefit if the authors would give more detailed account on the computational cost, including a comparison with the baseline methods. \n- Some claims are qualitatively argued rather than empirically validated. An example is the use of pairwise relation training in Eq (2), a key component of the algorithm\u2019s design. The authors largely provide intuitive explanations against the simpler alternative in favour of the more complicated design the paper adopted, but no empirical validation is provided.\n\n## Minor\n- Typo in openreview title: Lanuguage -> Language\n- First line beneath caption of Fig 1: rubics -> rubrics"
                },
                "questions": {
                    "value": "- Address my concerns in weaknesses.\n- It seems to me that PIT is run on top of an RLHF-finetuned model (i.e., the model generating \u201coriginal\u201d outputs in experiments)? if so, this point should be more clearly stated. If not, why not? In that case, RLHF should be an obvious baseline to compare against given that PIT is closely formulated based on the original RLHF (I gave the benefit of the doubt in the preliminary review on this point, pending clarification in rebuttal).\n- Do you observe that the reward model remains discriminative after multiple rounds of improvements? It seems to me that the reward model is trained on the original y_w and y_l annotated responses only, but the new outputs should be even better than y_w after a few iterations.\n\n-- **Post-rebuttal** --\n\nI thank the authors for the detailed feedback, which largely addressed my concerns. I also read other reviews, and I will stick to my rating that recommends acceptance. I think the discussion regarding prompting methods might be better qualified, though, as even if the proposed method uses fewer tokens, prompting-based methods have the strength that they do not require model weight adjustments, and only forward passes are required. As discussed in the original review, the results compared to baselines can be occasionally somewhat mixed, so I am unable to give an even higher score. Nonetheless, I believe this paper is of value to the community and can be accepted at ICLR."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4154/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4154/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4154/Reviewer_tfvt"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4154/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698652673748,
            "cdate": 1698652673748,
            "tmdate": 1700658330927,
            "mdate": 1700658330927,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fPuIo5UkMH",
                "forum": "2tVHNRZuCs",
                "replyto": "oNhkijDosL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer tfvt (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed and insightful feedback! We are excited that you think our method is convincing, interesting, reasonable, intuitive, and intriguing! Here are our responses to clarify your questions:\n\n> ### **W1.1: The computational cost and the execution difficulty should be more clearly stated: while the method seems to lead to a stronger gain than self-refine, such an improvement is not always significant and sometimes self-refine seems to be stronger by some metric (e.g., in terms of GPT-4 evaluation), although the authors have given possible explanations as to why they occur.**\n\nWe understand the reviewer\u2019s concern about GPT-4 metrics, so we introduce human evaluation in this case. GPT-4 is well-known for many biases, such as position bias [1,2], format bias [3], and knowledge bias [3]. Therefore, we introduce human evaluation when GPT-4 disagrees with the reward model. Our human evaluation detects several false negative cases in GPT-4, like long-content preference, as stated in Section 4.4. Therefore, we trust human evaluation in this case, which shows PIT is still apparently better than Self-Refine (Table 1).\n\n[1] Wang, Peiyi, et al. \"Large language models are not fair evaluators.\" arXiv preprint arXiv:2305.17926 (2023).\n\n[2] Zheng, Lianmin, et al. \"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.\" arXiv preprint arXiv:2306.05685 (2023). \n\n[3]Zhu, Lianghui, Xinggang Wang, and Xinlong Wang. \"JudgeLM: Fine-tuned Large Language Models are Scalable Judges.\" arXiv preprint arXiv:2310.17631 (2023).\n\n\n> ### **W1.2 On the other hand, self-refine as a prompting-based method is much easier to execute and cheaper I think the paper would benefit if the authors would give more detailed account on the computational cost, including a comparison with the baseline methods**\n\nThanks for pointing out this important discussion. A short answer to the computational cost concern is that our method does not bring extra computational cost compared to Self-Refine during inference and, in principle, will even have a bit lower computational cost due to fewer token inputs (without rubrics). A more detailed explanation can be found in the general response GR2.\n\nFor the execution difficulty, we would like to address that Self-Refine is more challenging to execute than it looks since it needs careful prompt/rubric design, which is one of our motivations. From the end-user perspective, our method is more friendly since the end-user only needs to provide the query and a candidate response without putting effort into designing prompts.\n\nOverall, we appreciate the reviewer pointing out this important discussion, and we already added this to Appendix B of the revised paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699855423287,
                "cdate": 1699855423287,
                "tmdate": 1699855629423,
                "mdate": 1699855629423,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "40Z1zQM967",
                "forum": "2tVHNRZuCs",
                "replyto": "oNhkijDosL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer's post-rebuttal comment"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback. We sincerely appreciate your time in reviewing our paper. \n\nA potential method to tackle the training computational cost issue is to incorporate PIT into the original model by jointly training the original model on both the PIT training objective and RLHF objective, so that we can integrate PIT functionality into the original model without training a separate model. We thank the reviewer for bringing this discussion; this could be an interesting future work :)"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731473010,
                "cdate": 1700731473010,
                "tmdate": 1700731473010,
                "mdate": 1700731473010,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I7PSmHy64E",
            "forum": "2tVHNRZuCs",
            "replyto": "2tVHNRZuCs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4154/Reviewer_c4bA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4154/Reviewer_c4bA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel framework, implicit self-improvement, to learn from human preference data. Instead of optimizing the response quality, their method maximizes the gap of the pair of responses. The experiments on three datasets demonstrate the effectiveness of their method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper proposes a novel method to implicitly self-improve from data. In this way, PIT can iteratively improve responses by repeating the self-improvement process.\n2. The authors find a practical way to implement their ideas for maximizing the gaps between responses for implicit self-improvement. They conduct experiments and analyses to verify PIT's effectiveness."
                },
                "weaknesses": {
                    "value": "1. In the experiment, the authors compare PIT with prompt-based methods (self-refine), while there is a lack of comparison with other reinforcement learning related methods like [1]. Can other RL methods help to self-improve the response? \n[1] Song F, Yu B, Li M, et al. Preference ranking optimization for human alignment[J]. arXiv preprint arXiv:2306.17492, 2023."
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4154/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4154/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4154/Reviewer_c4bA"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4154/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825383014,
            "cdate": 1698825383014,
            "tmdate": 1699636381154,
            "mdate": 1699636381154,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BSVwqMaYbt",
                "forum": "2tVHNRZuCs",
                "replyto": "I7PSmHy64E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4154/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer for the helpful feedback. We are happy to see that you find our method novel and practical! Here are our responses to your concerns:\n\n> ### **W1: In the experiment, the authors compare PIT with prompt-based methods (self-refine), while there is a lack of comparison with other reinforcement learning related methods like [1]. Can other RL methods help to self-improve the response? [1] Song F, Yu B, Li M, et al. Preference ranking optimization for human alignment[J]. arXiv preprint arXiv:2306.17492, 2023.**\n\n\nWe thank the reviewer for pointing out related resources. We added a discussion comparing our algorithm with the mentioned PRO paper in Section 3 (highlighted with green color). In short: PRO and similar papers such as DPO [1], focus on LLM alignment which is not what our work aims for. We focus on triggering the self-improvement capability implicitly from data. Therefore, PIT is not directly comparable to these methods.\n\nMore specifically, RLHF, DPO, and PRO focus on \u201cHow to make LLM generate responses preferred by humans given an input $x$\u201d, whereas PIT focuses on \u201cHow to get a better response preferred by humans given an input $x$ and a reference response $y$, where the reference response is generated by policy models (LLMs) trained with RLHF, DPO or PRO.\u201d Appendix A shows more details of PIT, which may help understand that PIT has different optimization goals compared to PRO, etc.\n\n[1] Rafailov, Rafael, et al. \"Direct preference optimization: Your language model is secretly a reward model.\" arXiv preprint arXiv:2305.18290 (2023).\n\nWe hope our reply addresses your concerns, and please let us know if you need further clarification. We are happy to continue discussions if needed!"
                    },
                    "title": {
                        "value": "Response to reviewer c4bA"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699855218325,
                "cdate": 1699855218325,
                "tmdate": 1699855289013,
                "mdate": 1699855289013,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xP4gB6llbH",
            "forum": "2tVHNRZuCs",
            "replyto": "2tVHNRZuCs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4154/Reviewer_xn4T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4154/Reviewer_xn4T"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the PIT framework, an innovation aimed at enhancing the quality of text generation in LLMs. PIT offers a novel twist on RLHF by reformulating its objectives towards increasing the gap in quality between a generated response and a original reference response. This work bypasses the need for costly and complex explicit rubrics. The paper demonstrates the superiority of PIT over conventional self-refinement methods across various datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper's main contribution lies in its innovative approach to reformulating the objectives of RLHF, particularly within the context of supervised fine-tuning reward model training, and reinforcement learning. It is particularly notable for introducing a novel reward model that prioritizes the quality gap between two responses from LLMs. This unique angle encourages the model to continually refine its output by comparing it to a reference response. The method has the potential to shift the paradigm in how LLMs are self-improved for better alignment with human preferences without the designing rubrics or prompts."
                },
                "weaknesses": {
                    "value": "1. The framework may not incorporate new information or data during the self-improvement cycle, potentially limiting the scope of learning to alignment with initial human preferences.\n1. The iterative nature of the proposed self-improvement process could result in increased computational time and cost in inference."
                },
                "questions": {
                    "value": "1. Can you clarify if the three models depicted in Figure 1, i.e. LLMs, PIT, and PIT Reward Model? Are they distinct or the same model?\n1. How to train with objectives (4) and (5)? An algorithm block is easy to illustrate. \n1. The paper lacks a detailed algorithmic flow for training with objectives (4) and (5). Could you provide an algorithmic block to illustrate the training process?\n1. How to determine the number of iterations for the self-improvement process for inference?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4154/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699015091885,
            "cdate": 1699015091885,
            "tmdate": 1699636381068,
            "mdate": 1699636381068,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WVXLJsuyVX",
                "forum": "2tVHNRZuCs",
                "replyto": "xP4gB6llbH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4154/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback! We are glad you find our method novel and the potential to guide future directions. We address your concerns in the following paragraphs.\n\n> ### **W1: The framework may not incorporate new information or data during the self-improvement cycle, potentially limiting the scope of learning to alignment with initial human preferences.**\n\nWe understand the reviewer's concern about the scenario of new incoming preferences. This is a challenging question for the conventional RLHF, too, not only our approach. In the conventional RLHF setting, one needs to re-train the reward model or ensemble different reward models. PIT can adopt the same technique to handle new incoming preferences as conventional RLHF. \n\n\n>### **W2: The iterative nature of the proposed self-improvement process could result in increased computational time and cost in inference.**\n\nIn fact, the inference cost is the same as Self-Refine and we apologize for not directly pointing this out in the paper. Detailed discussions can be found in the general response GR2.\n\nBoth Self-Refine and PIT iteratively replace candidate responses with improved responses (See Algorithm 2 in Appendix A in the revised version for more details). We evaluate the self-improvement of both methods under one iteration (Table 1, 2, and Figure 3) and multiple iterations (Table 3). Both methods have identical computational costs given the same number of iterations.\n\n\n>### **Q1: Can you clarify if the three models depicted in Figure 1, i.e. LLMs, PIT, and PIT Reward Model? Are they distinct or the same model?**\n\nWe apologize for the confusion. In our experiments, all models start with the same pre-trained language model, i.e., PaLM 2 (Bison). \u201cLLM\u201d (or the policy model $\\text{M}\\_\\text{P}^\\text{RL}$ in Section 3) is the model fine-tuned on the dataset (e.g., HH-RLHF) following the normal RLHF pipeline. \u201cPIT\u201d (i.e., $\\text{M}\\_\\text{PIT}^\\text{RL}$ in Section 3) is the model fine-tuned with our reformulated RLHF pipeline (Equation 4 and 5) on the same dataset (e.g. HH-RLHF). \u201cPIT Reward Model\u201d (i.e., $\\text{R}\\_\\text{PIT}$ in Section 3) is the model fine-tuned with our reformulated reward model training objective (equation 2) on the same dataset (e.g., HH-RLHF). \u201cPIT reward model\u201d offers the reward signal in equations 4 and 5 when training \u201cPIT\u201d.\n\nDuring inference, we first get a candidate response from \u201cLLM\u201d, then self-refine will design prompts and let \u201cLLM\u201d self-improve the candidate response. Differently, our approach uses \u201cPIT\u201d to improve the candidate response.\n\nA more formal and detailed demonstration including input formats, training, and inference details can be found in Appendix A in the revised paper. We also added some clarifications in Section 3 to help readers understand their differences (modifications are highlighted with green color).\n\n\n>### **Q2 and Q3: How to train with objectives (4) and (5)? An algorithm block is easy to illustrate. The paper lacks a detailed algorithmic flow for training with objectives (4) and (5). Could you provide an algorithmic block to illustrate the training process?**\n\nWe have provided these details in our revised paper Appendix A. In short, to train with objective (4), we sample $x$ and $y\\_\\text{ref}$ from data $\\mathcal{D}\\_\\text{RL}$ and use RL for optimization. To train with objective (5), we sample $x$ from data $\\mathcal{D}\\_\\text{RL}$ and $y\\_\\text{ref}$ from $\\text{M}\\_\\text{P}^\\text{RL}(\\cdot | x)$, and then use RL for optimization. \n\n\n>### **Q4: How to determine the number of iterations for the self-improvement process for inference?**\n\nThis is a great question! It is a common problem for all self-improvement methods during inference. It is challenging to determine the stopping criteria for both Self-Refine and PIT. We show preliminary analysis in the paper (Section 4.7) -- there is no guarantee that every iteration can lead to improvement. Table 3 shows that more iterations do not denote better response qualities for both Self-Refine and our method. Nevertheless, our method is better than Self-Refine under all iterations, showing its effectiveness.Further research is needed to investigate the stopping criteria of self-improvement methods. One potential method is gradually reducing the temperature for the next iteration until the response is repeated.\n\n\n\n\nWe hope we clarify your concerns and questions, please let us know if you have further questions and we will be happy to discuss further."
                    },
                    "title": {
                        "value": "Response to reviewer xn4T"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699855147023,
                "cdate": 1699855147023,
                "tmdate": 1699855303875,
                "mdate": 1699855303875,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]