[
    {
        "title": "Towards human-like spoken dialogue generation between AI agents from written dialogue"
    },
    {
        "review": {
            "id": "5YdY7AKDmz",
            "forum": "0AYosSFETw",
            "replyto": "0AYosSFETw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3338/Reviewer_Kha2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3338/Reviewer_Kha2"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes CHATS (CHatty Agents Text-to-Speech), a system for transforming written dialogue into spoken dialogue, whose content is coherent with the input written dialogue but generated with backchannels, laughter, and smooth turn-taking. Several contributions are announced: a method to prepare written dialogue by excluding backchannels, a mechanism for taking turns in conversation, and a Multi-Stream Dialogue Transformer Language Model. Paper builds upon previous work, such as dGSLM and Dialogue Transformer Language Model by Nguyen et al. in 2023 and it provides evaluations for different parts of the proposed system, including the dialog model, turn-taking model, and back-channeling model. \n\nWhen I take a closer look, it's clear that paper has too much stuff in it. There are many models and evaluations crammed together in one document w/o enough details of each of them. This makes it hard to read and understand the paper. It's unfortunate because this is an ambitious and relevant research objective that is described here. Current version of the paper needs a big re-organization to make it clearer and maybe each problem addressed should correspond to a single paper with deeper / more detailed description and evaluation; that would allow reader/reviewer to better understand and appreciate the valuable insights it offers."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-This research is ambitious because it explores how people talk in real conversations, not just in written text.\n\n-It introduces a Turn-Taking and Backchanneling Mechanism, which is important for making better autonomous spoken conversational agents.\n\n-The Multi-Stream Dialogue Transformer Language Model (MS-DLM) seems the main contribution and is definitely an interesting architecture"
                },
                "weaknesses": {
                    "value": "* It's unfortunate that the spoken dialog examples on GitHub are not in English. This makes it challenging for me to evaluate, and it limits its accessibility since only Japanese speakers can understand it. English examples would have been more universal.\n\n- In the contributions mentioned in the paper, it is not clear why \"Conversion from Spoken to Written Dialogue\" is valuable or innovative. Authors mention using both rule-based and machine learning approaches to identify backchannels and exclude them from written dialogues, but the paper lacks detail on the challenges this addresses. Is it mainly about data preprocessing?\n\n- As said above, the paper's structure needs improvement, as it tries to cover too many topics in one document.\n\n- The paper builds on the work of dGSLM (Nguyen et al., 2023) and the Dialogue Transformer language model (DLM) (Nguyen et al., 2023), but it doesn't provide enough information about these previous models to make this paper self-understandable\n\n- Section 3.1.2 seems to be a core part, but it's too brief to fully understand its significance."
                },
                "questions": {
                    "value": "see main remarks above +\n not clear was is the challenge in the part \"Conversion from Spoken to Written Dialogue\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3338/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3338/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3338/Reviewer_Kha2"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3338/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698425008031,
            "cdate": 1698425008031,
            "tmdate": 1700410126275,
            "mdate": 1700410126275,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8WpawEiReQ",
                "forum": "0AYosSFETw",
                "replyto": "5YdY7AKDmz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3338/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Kha2 (1/2)"
                    },
                    "comment": {
                        "value": "We thank Reviewer Kha2 for the insightful feedback on our paper. We are currently revising our manuscript to incorporate your suggestions, and will inform you once the updated manuscript is ready. The revised version will include (1) a clearer statement on the importance of \u201cConversion from Spoken to Written Dialogue\u201d in Section 2, (2) a detailed explanation of Generative Spoken Language Model (GSLM) ([Lakhotia et al., 2021](https://arxiv.org/abs/2102.01192)) pipeline in a new section or Appendix, and (3) an expanded discussion on the differences between conventional DLM and our MS-DLM in Section 3 or Appendix. Below are our specific responses to each of your comments.\n\n**Response to Complexity and Organization Concerns**:\n> When I take a closer look, it's clear that paper has too much stuff in it. There are many models and evaluations crammed together in one document w/o enough details of each of them. This makes it hard to read and understand the paper. It's unfortunate because this is an ambitious and relevant research objective that is described here. Current version of the paper needs a big re-organization to make it clearer and maybe each problem addressed should correspond to a single paper with deeper / more detailed description and evaluation; that would allow reader/reviewer to better understand and appreciate the valuable insights it offers.\n\nThank you for your constructive feedback and the recognition of our research's ambition. Our approach integrates various elements critical to naturalistic spoken dialogue generation, such as backchannels, laughter, and turn-taking. We believe segmenting these elements into separate papers could detract from the integrated nature of these components. To improve readability, we will clarify the conventional GSLM pipeline including s2u/u2s modules and uLM as well as its adaptations in our system, enhancing the manuscript's comprehensibility.\n\n**On Language Accessibility of Examples**:\n> It's unfortunate that the spoken dialog examples on GitHub are not in English. This makes it challenging for me to evaluate, and it limits its accessibility since only Japanese speakers can understand it. English examples would have been more universal.\n\nThank you for your feedback regarding the language accessibility of our examples. To address your concern, we plan to add figures like Figure E.1 and E.2 to our demo page. These figures will visually indicate the generated backchannels, laughter, and overlaps, making the content more accessible to non-Japanese readers\n\nWe acknowledge the importance of including English examples and intend to incorporate them in the future. However, currently, we face a limitation due to the lack of a suitable dataset.\nOur system requires two-channel audio, where the voices of each speaker are separately recorded in each channel. However, such public datasets are currently rare, which led us to decide to record our own high-quality spoken dialogue dataset.\nFor example, the Fisher dataset, though used in the previous study, is not ideal for demonstrating our system's capability of high-quality spoken dialogue generation, due to its limited bandwidth (8 kHz sampling rate, 4kHz bandwidth).\n\nIn addition, we would like to highlight our system\u2019s language-independent design. \nAlthough backchannels and laughter vary across languages (e.g. \"un\", \"hai\" in Japanese, \"uh-huh\", \"yeah\" in English, and \"shi\", \"dui\" in Mandarin), our system learns the pronunciation and timing of them in a data-driven manner using language-independent \"units\". Therefore, we believe our system is also applicable to languages other than Japanese. More specifically, we just have to replace the grapheme-to-phoneme module with one tailored to the target language to obtain the correct phoneme sequence $p_{n,1}^c, \u2026, p_{n, M_n}^c$."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699958654739,
                "cdate": 1699958654739,
                "tmdate": 1699958654739,
                "mdate": 1699958654739,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Np022DtTnd",
                "forum": "0AYosSFETw",
                "replyto": "Pbam3Xga55",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3338/Reviewer_Kha2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3338/Reviewer_Kha2"
                ],
                "content": {
                    "title": {
                        "value": "after authors' answers"
                    },
                    "comment": {
                        "value": "tks for your answers, i acknowledge authors' amibition to improve paper structure / readability and add description on dGSLM in the background +  as well as providing english samples on their demo page; i'll slightly increase my score accordingly"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410086115,
                "cdate": 1700410086115,
                "tmdate": 1700410086115,
                "mdate": 1700410086115,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0Ubl1s0DWb",
            "forum": "0AYosSFETw",
            "replyto": "0AYosSFETw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3338/Reviewer_yc8m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3338/Reviewer_yc8m"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method to generate natural overlapping spoken dialogue with the listener cues like backchannels and laughter only using the written transcripts (that lack the rich spoken dialog modes). This system generates speech for both the speaker and the listener simultaneously, using only the transcription from the speaker side by finetuning the modified dGSLM model with careful curation and pre-processing of natural dialog. The overall pipeline is similar to the one used by the dGSLM system; however using the careful finetuning process delivers very strong results and a practical tool for enriching the dialogs with natural spoken dialog properties. \n\nThe model has extensive experiments to show that the utterance quality is good, the dialog segments contain high quality of close to ground truth backchannels and pauses and the turn taking events also resemble the ground truth. Most important, the qualitative human evaluation experiments also show very good naturalness, meaningfulness and sound quality."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- There are many Dialog generation LLMs available today. These are currently not very natural generation systems, meaning, they cannot mimic human-to-human conversations that contain rich elements like laughter, backchannel, fluid turn-taking, etc. This paper aims to solve this problem and generates natural spoken dialog and presents methods including how to prepare datasets, create context properly in the training data and predict turn-taking events using the dual-transformer architecture (dGSLM). \n- The methods also shows how smaller datasets (74 hr of 2 channel speech) can be used to train a high quality spoken dialog generator (using a pre-trained uLM model). \n- Ablations show that data augmentation, next sentence objectives, turn-taking mechanism were all important pieces of the architecture and pipeline are all important for getting the overall natural dialog output."
                },
                "weaknesses": {
                    "value": "- the paper presents the overall system very well, however, it is not clear if the original contribution of the work is significant. It seems like a straightforward extension of the dGSLM model where it has been fine-tuned to create this improved version of natural dialog corpora. \n- there is no comparison to any other baseline system that is described in the paper. \n- human evaluation does not try to assess the content and quality of generated backchannels. \n- Also, it is not clear how the generation will transfer to various other data domains."
                },
                "questions": {
                    "value": "- it is not clear how many backchannel tokens are in the vocabulary (like laughter, ums, etc)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3338/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699070181245,
            "cdate": 1699070181245,
            "tmdate": 1699636282992,
            "mdate": 1699636282992,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "p3tK0ldoIV",
                "forum": "0AYosSFETw",
                "replyto": "0Ubl1s0DWb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3338/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yc8m (1/3)"
                    },
                    "comment": {
                        "value": "We thank Reviewer yc8m for the insightful comments regarding our paper. We are in the process of revising our manuscript to address the concerns raised and will inform you once the updated manuscript is ready. Below, we provide our specific responses to each of your comments.\n\n**On Summary and Strengths**:\n\nWe deeply appreciate your insightful and precise summary of our research's novelty and strengths. Your acknowledgment of the significant aspects and contributions of our work is invaluable and reinforces the importance of our efforts in advancing spoken dialogue generation. Thank you for your thorough and thoughtful analysis.\n\n**On Original Contribution and Significance**:\n\n> the paper presents the overall system very well, however, it is not clear if the original contribution of the work is significant. It seems like a straightforward extension of the dGSLM model where it has been fine-tuned to create this improved version of natural dialog corpora.\n\nAs you rightly pointed out, our CHATS system builds upon the foundations of existing models such as GSLM([Lakhotia et al., 2021](https://arxiv.org/abs/2102.01192)) and dGSLM([Nguyen et al., 2023](https://arxiv.org/abs/2203.16502)).\nHowever, it is crucial to recognize that the advancement of deep learning often involves the innovative adaptation of powerful existing models for new tasks. Successful examples include VALL-E ([Wang et al., 2023](https://arxiv.org/abs/2301.02111)) and AudioPaLM ([Rubenstein et al., 2023](https://arxiv.org/abs/2306.12925)), which utilize GPT-like Transformer decoder architecture for TTS and ASR/ST, respectively.\n\nCHATS distinguishes itself from dGSLM by offering a unique input/output structure specifically designed for spoken dialogue generation. This design allows for the control of spoken content through textual input and enables the generation of contextually accurate and coherent utterances from written dialogues, a capability absent in dGSLM. This advancement, we believe, is significant both academically and in practical applications across various domains.\n\n**On Lack of Comparative Baseline Systems:**:\n\n> there is no comparison to any other baseline system that is described in the paper.\n\nWe appreciate your observation regarding the comparative baselines. Given that our study explores AI-to-AI spoken dialogue generation, a relatively novel area, we faced challenges in identifying appropriate conventional baselines. Traditional TTS systems, including recent state-of-the-art ones, do not incorporate elements like backchannels, laughter, or turn-taking management. Therefore, we chose a standard TTS system whose architecture is similar to the proposed system and dGSLM as our baselines to cover a spectrum from text-driven to textless approaches. Our focus was on conducting in-depth analyses of key components through ablation studies, which we believe will more effectively advance the field of spoken dialogue generation."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700029735264,
                "cdate": 1700029735264,
                "tmdate": 1700029735264,
                "mdate": 1700029735264,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UHXxXOCBHn",
                "forum": "0AYosSFETw",
                "replyto": "0Ubl1s0DWb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3338/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yc8m (2/3)"
                    },
                    "comment": {
                        "value": "**On Evaluation of Generated Backchannels:**\n\n> human evaluation does not try to assess the content and quality of generated backchannels.\n\nWe recognize the importance of evaluating the content and quality of generated backchannels and appreciate your suggestion. In fact, we included the naturalness of backchannels as one of the criterion of dialogue naturalness evaluation as follows: \n*\"Dialogue Naturalness: Are backchannel and laughter appropriately integrated to create a human-like interaction? Is there a seamless transition between the speaker and listener at the right moments? Does the conversation flow smoothly?\"*\nTherefore, we conjecture that if the backchannel content was unnatural or its quality was quite low, the dialogue naturalness score of the Proposed system could be lower than the Baseline system in Table 5. For greater clarity and reproducibility, we will include the exact wording used in our human evaluation in the Appendix.\n\nAdditionally, since we segmented the generated backchannels from the entire spoken dialogue in Section 4.3.1, we applied Whisper to each segment to obtain their transcription and listed the top-20 frequently used backchannels (Please refer to the two tables below for the results). While the order and frequency differ, many backchannels are common between the Ground Truth dialogue and the generated ones, indicating that the CHATS system is capable of appropriately generating backchannels used in actual conversations. We plan to add these results in Appendix.\n\nTable: Top-20 Frequently Used Backchannels in Ground Truth (left four columns) and Generated (right four columns) Spoken Dialogues. Each Japanese transcripts were translated into English to match the meaning as closely as possible.\n\n| Frequency | Transcript | Pronunciation | Translation | Frequency | Transcript | Pronunciation | Translation |\n|-------|------------|---------------|-------------|-------|------------|---------------|-------------|\n| 261 | \u3046\u3093 | un | Uh-huh | 148 | \u3046\u3093 | un | Uh-huh |\n| 87 | \u3093\u30fc | n- | Mm-hm | 117 | \u3093 | n | Mm |\n| 58 | \u306f\u3044 | hai | Yes | 77 | \u3093\u3093\u3093 | nnn | Mmm |\n| 47 | \u305d\u3046 | sou | I see | 62 | \u3093\u3093\u3063 | nn | Mm! |\n| 43 | \u3093\u3093\u3093 | nnn | Mmm | 26 | \u3093\u3093\u3093\u3093 | nnnn | Mm-hmm |\n| 43 | \u3093 | n | Mm | 25 | \u3093\u3093 | nn | Mm-mm |\n| 32 | \u3046\u3093\u3046\u3093 | unun | Yeah yeah | 24 | \u3093\u30fc | n- | Mm-hm |\n| 25 | \u3093\u3093\u3093\u3093 | nnnn | Mm-mm | 24 | \u306f\u3044 | hai | Yes |\n| 23 | \u3042\u30fc\u30fc | a-- | Ah | 14 | \u3075\u3045 | fuu | (sigh) |\n| 21 | \u3046\u30fc\u3093 | u-n | Hmm | 12 | \u305d\u3046 | sou | I see |\n| 20 | www | (laugh) | (laugh) | 11 | \u306f\u3044\u306f\u3044 | haihai | Yes yes |\n| 17 | \u306f\u3041 | ha | Oh | 11 | \u3046\u3093\u3046\u3093 | unun | Yeah yeah |\n| 16 | \u305d\u3046\u305d\u3046\u305d\u3046 | sousousou | Exactly | 10 | \u3042\u3001\u305d\u3046\u306a\u3093\u3060 | a, sounanda | Oh, is that so? |\n| 14 | \u3075\u3075\u3063 | fufu | (chuckle) | 8 | \u30d5\u30d5\u30d5\u30d5\u30d5\u30d5\u30d5 | fufufufufufu | (laugh) |\n| 11 | \u306d\u3048 | nee | Hey | 7 | \u306f\u3041 | ha | (sigh) |\n| 11 | wwww | (laugh) | (laugh) | 6 | \u305d\u3046\u305d\u3046\u305d\u3046 | sousousou | Exactly |\n| 11 | \u3093\u3093\u3063 | nn | Mm! | 6 | \u305d\u3046\u306a\u3093\u3060 | sounanda | Oh, really? |\n| 10 | \u3093\u3075\u3075\u3075 | nfufufu | (giggle) | 6 | \u3093\u3075\u3075\u3075 | nfufufu | (giggle) |\n| 9 | \u306f\u3044\u306f\u3044\u306f\u3044 | haihaihai | Yes yes yes | 6 | \u305d\u3046\u3060\u306d | soudane | That's right |\n| 9 | \u3093\u30fc\u30fc | n-- | Mm-hmm | 5 | www | (laugh) | (laugh) |\n\n**On Applicability to Various Data Domains**:\n\n> Also, it is not clear how the generation will transfer to various other data domains.\n\nWe understand that the transferability of our model to diverse data domains is an important aspect of its applicability. \nAlthough we would like to apply our system to languages other than Japanese (e.g. English, Mandarin, etc.) and domains other than chit-chat (e.g. interview, consulting, etc.), we leave it as future work due to the lack of a suitable dataset. Our system requires two-channel audio where each speaker\u2019s voices are separately recorded in each channel, but such a public dataset is rare for now. For example, the Fisher dataset, used in the previous study, is not ideal for demonstrating our system's capability of high-quality spoken dialogue generation, due to its limited bandwidth (8 kHz sampling rate, 4kHz bandwidth).\nThis limitation prompted us to begin our research by recording a high-quality spoken dialogue dataset.\n\nIn addition, we would like to emphasize that our system learns when and how to generate backchannels or laughter in a data-driven manner by using \"units\". \nTherefore, we believe that our system is applicable to other languages by simply substituting the grapheme-to-phoneme module. Conditioning our system on domain information to control the content and frequency of backchannels and laughter might also present an interesting research direction.\""
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700029821998,
                "cdate": 1700029821998,
                "tmdate": 1700101557698,
                "mdate": 1700101557698,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "H54C4RTQJE",
                "forum": "0AYosSFETw",
                "replyto": "0Ubl1s0DWb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3338/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yc8m (3/3)"
                    },
                    "comment": {
                        "value": "**On Vocabulary of Backchannel Tokens**:\n\n> it is not clear how many backchannel tokens are in the vocabulary (like laughter, ums, etc).\n\nWe apologize for any confusion caused regarding the backchannel tokens in our vocabulary. To clarify, our vocabulary does not contain specific tokens for each type of backchannel, such as \"umm\" or \"uh-huh\". Instead, all backchannels and laughter from the listener side are generated using a single special token, \"LIS\", as described in Section 3.1.2. This approach allows for a more dynamic and contextually appropriate generation of listener responses without being constrained by a predefined set of backchannel types. \n\nWe have listed all special tokens in the Appendix A.2.\nThe inclusion of the \"LAU\" token in our vocabulary is specifically for cases where explicit laughter generation is desired on the speaker side. An example of this can be seen in Table E.2 of Appendix E, where the \"LAU\" token is used to generate laughter in the speaker side. This method provides a balance between the flexibility of spontaneous listener interactions and the ability to explicitly control certain aspects of the dialogue, like speaker-initiated laughter."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700029853198,
                "cdate": 1700029853198,
                "tmdate": 1700029853198,
                "mdate": 1700029853198,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2qgRBrOuYh",
            "forum": "0AYosSFETw",
            "replyto": "0AYosSFETw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3338/Reviewer_FEtd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3338/Reviewer_FEtd"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the task of generating spoken dialogues between 2 parties using autoregressive models. It follows the earlier work on DLM (dialogue language model), and tries to extend it for better turn-taking and pause modeling. This results in more natural generated dialogs. The authors train all models from scratch."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It makes sense to incorporate pitch and content units in a multi-stream dialog language model for spoken dialog generation. The authors also build secondary models for turn taking and pause modeling. These are very critical for a more natural sounding dialog generation, and are lacking in textual dialogs. Especially the audio samples with overlapping speech are impressive."
                },
                "weaknesses": {
                    "value": "I had a hard time to understand the concept of \"units\" and has to read Kharitonov. The paper should do a better job explaining what they are with motivation. Furthermore I had a hard time understanding uLM and had to read the DLM paper. The authors should first explain DLM. But after reading these 2 papers, it is clear that the contribution is actually not that significant, but still very creative idea, applied to Japanese data."
                },
                "questions": {
                    "value": "dGSLM is trained with 2000 hours of English data. In this paper authors use only 74 hours of Japanese data. And they train the dGSLM models from scratch using that 74 hours. The experimental results show inferior comprehensiveness compared to the original dGSLM paper. This begs the question of authors replicating the experiments for English with larger training set. In other words, we do not know whether their improvements will disappear with more data."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3338/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699574573346,
            "cdate": 1699574573346,
            "tmdate": 1699636282884,
            "mdate": 1699636282884,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5QXQiLaCVk",
                "forum": "0AYosSFETw",
                "replyto": "2qgRBrOuYh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3338/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FEtd"
                    },
                    "comment": {
                        "value": "We thank Reviewer FEtd for the insightful comments and questions regarding our paper. We are in the process of revising our manuscript to incorporate your suggestions and will inform you once the updated manuscript is ready. Below are our specific responses to each of your comments.\n\n**On Need for Better Explanation of Units and DLM**:\n\n> I had a hard time to understand the concept of \"units\" and has to read Kharitonov. The paper should do a better job explaining what they are with motivation. Furthermore I had a hard time understanding uLM and had to read the DLM paper. The authors should first explain DLM.\n\nWe acknowledge that the original manuscript lacks sufficient explanation of the necessary background knowledge. To rectify this, we will include a more thorough explanation of the Generative Spoken Language Model (GSLM) ([Lakhotia et al., 2021](https://arxiv.org/abs/2102.01192)), which will provide foundational knowledge on \"units\" and the pipeline of speech-to-unit (s2u) module, unit language model (uLM), and unit-to-speech (u2s) module. Additionally, in Section 3, we will summarize the original DLM ([Nguyen et al., 2023](https://arxiv.org/abs/2203.16502)) and discuss how it differs from our MS-DLM. \n\n**On Significance of Contribution and Creativity**:\n\n> But after reading these 2 papers, it is clear that the contribution is actually not that significant, but still very creative idea, applied to Japanese data.\n\nWe extend our sincere gratitude to the reviewer for the effort in reading the cited papers and acknowledging the creative aspect of our work.\nWhile we agree that our paper builds upon existing models, we believe our contribution lies in the integration of multiple spoken dialogue characteristics such as backchannels, laughter, and turn-taking. These characteristics have typically been addressed separately in previous research.\n\nWhile we currently focus on Japanese data, we believe that our system is adaptable to other languages due to the language-independent design of the GSLM pipeline.\nConcretely, we just have to substitute the grapheme-to-phoneme module with that of the target language. We plan to extend our system to additional languages as suitable datasets become available.\n\n**On Comparison with dGSLM and Data Volume**:\n\n> dGSLM is trained with 2000 hours of English data. In this paper authors use only 74 hours of Japanese data. And they train the dGSLM models from scratch using that 74 hours. The experimental results show inferior comprehensiveness compared to the original dGSLM paper. This begs the question of authors replicating the experiments for English with larger training set. In other words, we do not know whether their improvements will disappear with more data.\n\nThis observation is quite perceptive. Although we are currently unable to conduct additional experiments using the Fisher dataset, we can refer to the original dGSLM paper, which reported that the dGSLM's meaningfulness score was 2.48\u00b10.49, while the Ground Truth conversation achieved 4.21\u00b10.25. These results suggest that even with 2,000 hours of data, it is challenging for the original dGSLM to generate comprehensible content. We hypothesize that a larger dataset narrow the gap between the meaningfulness of dGSLM and our system. However, we believe our system will remain valuable with a massive dataset because it allows for controlled spoken content through text."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699958937471,
                "cdate": 1699958937471,
                "tmdate": 1699958937471,
                "mdate": 1699958937471,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]