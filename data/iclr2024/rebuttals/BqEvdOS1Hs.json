[
    {
        "title": "Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain"
    },
    {
        "review": {
            "id": "PkPCheWpFA",
            "forum": "BqEvdOS1Hs",
            "replyto": "BqEvdOS1Hs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7798/Reviewer_qZtz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7798/Reviewer_qZtz"
            ],
            "content": {
                "summary": {
                    "value": "This work focusses on fine-tuning high-performing AI agents trained for commercial games to be more enjoyable as teammates for human players.  It has been observed that strong AI policies for team vs. team games can perform well as members of human-AI teams, but nonetheless be unenjoyable for human players due to a lack of perceived cooperation, or opportunities for the human to lead.  They propose to address this issue using \"RL from Human Gain\" or RLHG, a framework in which the AI's objective is to maximize the impact of the human's own actions on their reward (which may differ from the task-level reward).  They present a policy gradient update that maximizes a mixture of the one-step RLHG objective with the task-level reward.\n\nThey evaluate RLHG by fine-tuning the existing Wukong agent developed for the game \"Honor of Kings\".  Through a user-study with expert players they show that these players generally preferred to play on teams with RLHG-tuned agents rather than the high-performing Wukong agent."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I see two main contributions from this paper:\n1. Highlighting the importance, in the use of RL for game AI, of maximizing human agency and enjoyment, rather than absolute performance\n2. The RL from human gain algorithm for learning policies that maximize human agency itself, rather than\n\nWhile conceptually simple, they are able to demonstrate empirically that the RLHG algorithm can fine-tune high-performing models into ones that achieve high player satisfaction, while retaining much of their original performance."
                },
                "weaknesses": {
                    "value": "My main concern with this work is that its technical presentation is currently unclear to the point that it is not possible to determine which algorithm is actually being evaluated, yielding two very different possible explanations for the empirical results.\n\nThe way RLHG is currently presented suggests that the algorithm attempts to select AI actions that maximize the difference between the advantage of the human's own actions under the AI's action, and their advantage under some baseline AI policy (this advantage captured by the term $\\hat{A}_H (s, a)$).  Working through the derivation of the RLHG update, however, it appears that RLHG may essentially be equivalent to optimizing the weighted combination of the human and task rewards $R + \\alpha R^H$.  This may simply be a misunderstanding of the notation used, but in either event the presentation needs to be improved for a final version of the paper.\n\nLooking at the RLHG policy gradient in equation 4, it appears that we can express the human gain advantage as: \n\n$$\n     \\hat{A}_H (s, a) = E_{\\pi^H} [ G^{H}_t \\vert s_t = s, s_t = a ] - V^{\\pi^{e}_{\\theta}, \\pi^{H}}_H\n$$\n\nThis is where the notation makes things unclear.  As I understand it, $V^{\\pi^{e}_{\\theta}, \\pi^{H}}_H$ is to be interpreted as a function of the state $s$ alone, in which case it would have no impact on the expectation of the policy gradient (though it would potentially reduce the variance).\n\nThe other interpretation, which is more consistent with the intuition behind the algorithm (and with equation 5), is that $V^{\\pi^{e}_{\\theta}, \\pi^{H}}_H$ is really a function of both $s$ and the AI's action $a$ (or equivalently, $V : S \\mapsto \\Re^A$).\n\nSome improvements to the presentation that could help clear things up:\n1. Specify the domain and range of the value functions in question\n2. Always condition value functions on their inputs ($V(s)$ instead of $V$)\n3. clearly distinguishing in the definitions between the AI actions and those taken by the human\n\nThis also raises another question about the relative importance of the \"human-gain\" objective vs. the distinct human reward function $R^H$ in the empirical results.  My understanding is that the baseline HRE fine-tuning process corresponds to simply maximizing the human's reward function, starting from the pre-trained Wukong agent.  The fact that this performs so poorly would suggest that the human-gain objective is really the critical factor.  Unfortunately the HRE agent is not sufficiently well-defined.  Explicitly writing down the gradient update or objective function for HRE agent would further improve the reader's understanding of RLHG.\n\nIf these issue could be clarified it would significantly impact my recommendation."
                },
                "questions": {
                    "value": "1. Were an experiments conducted using RLHG with the task reward and human reward being equivalent (solely optimizing human agency)?\n2. Do the HRE experiments correspond to maximizing the $\\alpha$-weighted mixture of human and task reward?\n3. Was the human-centric reward a pre-existing score in the game environment, or was it derived based on the survey of player preferences?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7798/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7798/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7798/Reviewer_qZtz"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7798/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699020780655,
            "cdate": 1699020780655,
            "tmdate": 1700680795620,
            "mdate": 1700680795620,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "HBde5CUR1A",
            "forum": "BqEvdOS1Hs",
            "replyto": "BqEvdOS1Hs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7798/Reviewer_uNSX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7798/Reviewer_uNSX"
            ],
            "content": {
                "summary": {
                    "value": "* This paper proposes a very interesting problem in multi-agent RL that designing robot policies with the goal of providing the human a good experience. The paper proposes a new RL training procedure to obtain such policies and deployed them in the game of \"Honor of Kings\", in both simulation (4 robot + 1 simulated human playing against 4 robot + 1 simulated human), and in real game play (4 robot + 1 real human playing against 4 robot + 1 simulated human).\n\n* This paper separate the task reward from the human reward, which captures the human experience in the game. Both rewards are known to the system. The training procedure optimizes the robot policies to maximize the difference between the total human reward obtained compared to the total human reward obtained if the robot is just maximizing the task reward."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* The formulation and algorithm are sound.\n* The result is rich. In both simulation and real game play, the authors demonstrate that the proposed algorithm achieved higher human experience, without sacrificing a lot of task performance."
                },
                "weaknesses": {
                    "value": "* I have some questions about the notations. I think the notation in Sec.2.2 and Sec.3.1, 3.2 are a bit confusing and hard to follow. See my questions in the \"questions\" box.\n* I think Dec-POMDP usually assumes that all agents maximize the same reward function. However, in this paper, the human and robot are not just optimizing the same reward function. Instead, it seems that the human is just a fixed policy that is not optimizing anything. The robot is a policy that optimizes the human gains \u2206\u03c0H in Sec.3.2. Maybe it is worthy to clarify this.\n    * Due to this conflict, it did confuse me in the beginning and took me some time to figure out that in this paper, the human policy is fixed (e.g., learned from data), while the robot policy is learned as the \"best-response\" to the human policy. In other words, the human might not adapt to or might not be the best respond to the robot policy. It might be useful to make it clear."
                },
                "questions": {
                    "value": "* I think in multi-agent setting, the value function depends on both the robot policies, denoted by \u03c0 or \u03c0\u03b8, in this paper, and also the human policy \u03c0H. However, some notations in Sec.2.2 and Sec.3.1, 3.2 are only explicitly associated with one policy, rather than both the robot and human policies, which makes me confused.\n    * E.g., in the 2nd line of the paragraph before Eq.1, it says V^\u03c0\u03b8, where I am confused that what is the human policy inducing this value function.\n    * E.g., in Eq.1's last term - expectation under \u03c0H, the total return Gt should depend on the robot policy and the human policy, but the expectation is only taken under \u03c0H, not (\u03c0H,\u03c0\u03b8).\n        * Similar to Eq.2's last term - E_\u03c0H[...].\n        * Similar to the line below Eq.3: E_\u03c0H[...].\n        * Simlar to Sec.3.2's 2nd paragraph's \u2206\u03c0(s, a)=E_\u03c0H[...].\n* I am also not sure how to get the last inequality in page number 4."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7798/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699045543955,
            "cdate": 1699045543955,
            "tmdate": 1699636952945,
            "mdate": 1699636952945,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "G262cxjdff",
            "forum": "BqEvdOS1Hs",
            "replyto": "BqEvdOS1Hs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7798/Reviewer_zdWR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7798/Reviewer_zdWR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a human-centered modeling scheme to improve the design of human experiments in cooperative human-AI games. Specifically, the authors train two networks: a value network to estimate the expected human return in achieving human goals and a gain network to estimate the expected positive gain of human return. Empirically, they evaluate their proposed method in the game Honor of Kings, and find that the RLHG agent provides participants with a better gaming experience, both in terms of objective performance and subjective preference."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper's lies in its focus on human experiments in human-AI collaborative games, rather than the current popular AI-centered training scheme. This is a very important area of research, as human experience is essential in human-AI collaboration. Additionally, the paper conducts a comprehensive experimental evaluation of the proposed algorithms, and shows that they can provide a better human experience in terms of both objective performance and subjective preference."
                },
                "weaknesses": {
                    "value": "Although the reviewer agrees that human experience is important in human-agent collaboration, it seems the novelty of this paper is limited. The proposed approach seems to simply add a human modeling network to the existing RL framework. \nThe details of the algorithms are also not very clear in the paper, and the reviewers suggest that Algorithm 1 should be moved to the main body of the paper to make it easier for readers to follow."
                },
                "questions": {
                    "value": "As weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7798/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699093803104,
            "cdate": 1699093803104,
            "tmdate": 1699636952847,
            "mdate": 1699636952847,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]