[
    {
        "title": "Generation of Geodesics with Actor-Critic Reinforcement Learning to Predict Midpoints"
    },
    {
        "review": {
            "id": "Zvj7pLQUso",
            "forum": "9SwObx9Jdn",
            "replyto": "9SwObx9Jdn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3381/Reviewer_8vLp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3381/Reviewer_8vLp"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors study the problem of finding geodesics in general manifolds via reinforcement learning. The main idea is to divide the discovery task into smaller ones by predicting midpoints recursively. An actor-critic algorithm learns a policy to generate midpoints. Two empirical evaluations are provided to demonstrate the efficacy of the proposed algorithm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Geodesics generation with reinforcement learning is a relatively under-explored research area. This work contributes by studying an actor-critic formulation and shows its effectiveness.\n\n2. A few design choices are explored, such as different variants of the actor loss. These results help illustrate some properties of the proposed algorithm."
                },
                "weaknesses": {
                    "value": "1. The empirical evaluation environments are relatively artificial. I would expect some more practical tasks such as robotic motion planning to be more effective in demonstrating the significance of the contribution. \n\n2. The current baselines are all RL based. I think some classical motion planning algorithms should be included too, such as RRT (RRT*) and A* search. \n\n3. This is more of a clarification of the problem setting. It seems that the end goal of the learned policy is not necessarily finding the shortest path. The success criterion is stated as \u201call values of C(4) for two consecutive points are not greater than $\\epsilon$\u201d, which does not imply that a path is the shortest. Is this correct? If so, this should be stated more clearly."
                },
                "questions": {
                    "value": "1. Please provide some motivations for the definition of $C$ (Equation (4)). Also please explain what $df_x$ is in this definition.\n\n2. Why is Equation (5) hard to compute efficiently?\n\n3. How does one decide the depth parameter $D$ on Line 18 of Algorithm 1?\n\n4. In Equation (11), should the right-hand side be $d(x, y)$, the true distance rather than the local approximation? Either way, Equation (11) could use a more expanded explanation.\n\n5. In Proposition 2, what is $V_i$?\n\n6. In the Sequential Reinforcement Learning (Seq) baseline, why is the reward function (Equation (16)) scaled by $\\epsilon$? How does this decision affect the learning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3381/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697742831106,
            "cdate": 1697742831106,
            "tmdate": 1699636288979,
            "mdate": 1699636288979,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "owcA2hETnf",
                "forum": "9SwObx9Jdn",
                "replyto": "Zvj7pLQUso",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3381/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3381/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the review!\n\n> This is more of a clarification of the problem setting. It seems that the end goal of the learned policy is not necessarily finding the shortest path. The success criterion is stated as \u201call values of C(4) for two consecutive points are not greater than $\\epsilon$\u201d, which does not imply that a path is the shortest. Is this correct? If so, this should be stated more clearly.\n\nIndeed, judged as a success does not mean being the shortest, but success rates should increase if shorter paths are chosen for all pairs. The reason we do not use the lengths of paths for evaluation is that, since the metric can only be calculated locally, lengths cannot be calculated when the success condition is satisfied. Moreover, it is needed to compare fairly with the sequential baseline method, which has a different generation method. For these reasons, we chose this evaluation scheme.\n\n> Please provide some motivations for the definition of $C$\n (Equation (4)). Also please explain $df_x$ what \n is in this definition.\n\nSorry for the missing explanation. $df_x$ is the differential (pushfoward) of $f$ at $x$.\n\nSince the Finsler distance is defined by (1) and the geodesics can be approximated by the segment in the coordinate space,\nwe approximate the distance between the closed points by (5). Furthermore, since $F$ is continuous and the points are closed, this integral can be approximated by the value at $t=0$. This is (4).\n\n> Why is Equation (5) hard to compute efficiently?\n\nOf course, there are various methods of numerical calculation of integrals, but they take more time than calculating a function once\nand it is difficult to know in advance the required accuracy.\n\n> In Equation (11), should the right-hand side be $d(x,y)$, the true distance rather than the local approximation? Either way, Equation (11) could use a more expanded explanation.\n\nUnlike the proposed method, different policies and value functions are considered here for different depths.\n$V_i$ represents the sums of the values of $C$ after the policies $\\pi_{i-1}, \\ldots ,\\pi_0$ is applied.\nIt is therefore correct that the right-hand side of (11) is $C$.\n\n> In Proposition 2, what is $V_i$ ?\n\nThis is a typo of $V_i^*$. Thank you for pointing this out.\n\n> In the Sequential Reinforcement Learning (Seq) baseline, why is the reward function (Equation (16)) scaled by $\\epsilon$ ? How does this decision affect the learning?\n\nThis motivates agents to reach goals in as few steps as possible."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3381/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699876530237,
                "cdate": 1699876530237,
                "tmdate": 1699876530237,
                "mdate": 1699876530237,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3aVTjkrUJn",
                "forum": "9SwObx9Jdn",
                "replyto": "Zvj7pLQUso",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3381/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3381/Authors"
                ],
                "content": {
                    "title": {
                        "value": "About baselines"
                    },
                    "comment": {
                        "value": "The first concern that the experimental environments were artificial could have been answered by conducting an additional experiment.\n\nOne of the reasons we focused the baselines on reinforcement learning is that it is difficult to make a fair comparison with searching methods.\nWhile searching methods can find good solutions if enough time is spent, prediction by trained policy uses very little time.\nTherefore, we decided to limit ourselves to RL methods for situations where we want to generate paths or motions in a short time."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3381/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642640863,
                "cdate": 1700642640863,
                "tmdate": 1700642640863,
                "mdate": 1700642640863,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "egbhZg5OcJ",
                "forum": "9SwObx9Jdn",
                "replyto": "3aVTjkrUJn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3381/Reviewer_8vLp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3381/Reviewer_8vLp"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I want to thank the authors for their responses. If the argument about classical planning methods such as RRT is the computation time, the authors should provide relevant evaluations to back it up. In particular, there are accelerated versions of those methods which can be made very fast as well. I will maintain my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3381/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681724038,
                "cdate": 1700681724038,
                "tmdate": 1700681724038,
                "mdate": 1700681724038,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mnVcO6btgi",
            "forum": "9SwObx9Jdn",
            "replyto": "9SwObx9Jdn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3381/Reviewer_4ppY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3381/Reviewer_4ppY"
            ],
            "content": {
                "summary": {
                    "value": "This work focuses on path planning to generate geodesics in some manifold. It extends sub-goal tree framework (Jurgenson et al., 2020) to generate midpoints (equal distances to two given points), instead of any intermediate points. They train an actor to predict the midpoints, and a critic to predict the distance of two given points (s,g). It is also shown to converge to a unique optimal solution,  where the distance is given by some continuous approximation. The method is evaluated on two toy tasks to showcase its effectiveness over RL and previous planning approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The overall writing is rigorous, principled and looks solid work. But I am not sure of its significance."
                },
                "weaknesses": {
                    "value": "Perhaps the motivation of this work can be better written. As the authors pointed out in their experiments, generating geodesic (path planning) can be simply tackled by RL by specifying a reward function related to the difference in distance. But it may have instability or other issue compared to path planning approaches. \n\nCould you give some explanation why Car-like task favors your approach, while Matsumoto task not? \n\nThe experiment scope is a bit narrow as only two toy tasks are evaluated. \n\nMinor: The description of methods in the experiments can be more complete \u2013 add a line of \u201cours\u201d using Eq. 8 before \u201cthe following variants of our methods\u201d.  The name \u201csequential RL\u201d is a bit confusing as RL is sequential in nature. Perhaps \u201cvanilla RL\u201d or just \u201cRL\u201d, because your approach uses a non-conventional actor loss."
                },
                "questions": {
                    "value": "I\u2019m not familiar with path planning and differential manifold, so some of these comments are my educational guess.\n\n---- Post-rebuttal\n\nAfter reading the authors' response and other reviews, I think this work still requires more empirical evaluation on their approach. Thus, I lower my rating."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3381/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3381/Reviewer_4ppY",
                        "ICLR.cc/2024/Conference/Submission3381/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3381/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698273349070,
            "cdate": 1698273349070,
            "tmdate": 1700454488271,
            "mdate": 1700454488271,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PAHpkrfx2C",
                "forum": "9SwObx9Jdn",
                "replyto": "mnVcO6btgi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3381/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3381/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the review!\n\n> Perhaps the motivation of this work can be better written. As the authors pointed out in their experiments, generating geodesic (path planning) can be simply tackled by RL by specifying a reward function related to the difference in distance. But it may have instability or other issue compared to path planning approaches.\n\nWe are grateful for your useful comments. Indeed, we felt that sequential RL is unstable for both random seed and hyperparameters compared to the proposed methods. It may be due to the fact that sequential RL have long horizons compared to the proposed method.\n\n> Could you give some explanation why Car-like task favors your approach, while Matsumoto task not?\n\nI think that sequential RL is not good at CarLike rather than our method is good at CarLike.\nSince sequential RL uses ad-hoc reward function defined in (16), there is no guarantee that it works well.\nPerhaps the defined reward is far from the actual distance in CarLike.\n\n> Minor: The description of methods in the experiments can be more complete \u2013 add a line of \u201cours\u201d using Eq. 8 before \u201cthe following variants of our methods\u201d. The name \u201csequential RL\u201d is a bit confusing as RL is sequential in nature. Perhaps \u201cvanilla RL\u201d or just \u201cRL\u201d, because your approach uses a non-conventional actor loss.\n\nWe will add a little explanation of what \"Ours\" means.\n\nThe name \"sequential\" may be somewhat inappropriate, but I wouldn't dare change it because previous work by Jurgenson et al. have used this word."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3381/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699873740088,
                "cdate": 1699873740088,
                "tmdate": 1699873740088,
                "mdate": 1699873740088,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "S3w3IKocGY",
            "forum": "9SwObx9Jdn",
            "replyto": "9SwObx9Jdn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3381/Reviewer_ZvgN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3381/Reviewer_ZvgN"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a modification of the sub-goal tree framework to use midpoints instead of arbitrary intermediate points and actor-critic instead of policy gradient for goal-conditioned reinforcement learning problems. With the two changes, the proposed method is able to generate equally divided waypoints and with better sample efficiency on deep trees. Theoretical proofs are given for the convergence of the proposed method. The proposed method shows comparable performance to baselines on several tasks with advantage of generating equally divided waypoints."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written and the method is well-motivated. The effectiveness of the proposed method is supported both theoretically and empirically. The generated waypoints with equal distances would be more useful than that of the previous method."
                },
                "weaknesses": {
                    "value": "The novelty of the paper is not prominent compared to its base methods. \nThe experimental setting is a bit simplified. In section 6, the authors propose a penalty term to be added to deal with obstacles. Wondering how easy is it to generalize the proposed method to environments with obstacles.\nThe experiment results do not show clear performance improvements of the proposed method."
                },
                "questions": {
                    "value": "Can we add some more explanation and justification on why the midpoint is not just a trivial extension of the existing method using arbitrary waypoints?\n\nCan we add more analysis on how the proposed method could be generalized to environments with obstacles?\n\nIn Figures 2 and 3, the proposed method does not show clear improvements compared to baselines. Is this expected? Can we add more explanations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3381/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699041214388,
            "cdate": 1699041214388,
            "tmdate": 1699636288833,
            "mdate": 1699636288833,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q7JFKmgFf0",
                "forum": "9SwObx9Jdn",
                "replyto": "S3w3IKocGY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3381/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3381/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answers to questions"
                    },
                    "comment": {
                        "value": "Thanks for the review!\n\n> Can we add some more explanation and justification on why the midpoint is not just a trivial extension of the existing method using arbitrary waypoints?\n\nWe think that he most non-trivial point for midpoint generation compared to the sub-goal tree generation is the theoretical soundness proved in the subsection 4.3. While the existing method using arbitrary waypoints can perform badly in our setting as in the situation of Remark 6, our midpoint generation method avoids this.\n\n> Can we add more analysis on how the proposed method could be generalized to environments with obstacles?\n\nPlease see the papers cited in our response to Reviewer etxs.\n\n> In Figures 2 and 3, the proposed method does not show clear improvements compared to baselines. Is this expected? Can we add more explanations?\n\nIn CarLike environments, the proposed method clearly outperformed other methods except the intermediate point variant of the proposed method.\nIn Matsumoto environments, the proposed method outperformed other methods except the sequential RL baseline.\nIndeed, the superiority of the proposed method over the intermediate point variant may not be so obvious.\nHowever, it is noted that the intermediate point variant can perform badly in Remark 6.\nThus, our method has been shown to be theoretically or experimentally superior to all other methods."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3381/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699871999828,
                "cdate": 1699871999828,
                "tmdate": 1699871999828,
                "mdate": 1699871999828,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "a2gVtoGXbG",
            "forum": "9SwObx9Jdn",
            "replyto": "9SwObx9Jdn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3381/Reviewer_etxs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3381/Reviewer_etxs"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel reinforcement learning framework, termed as 'midpoint tree', designed to recursively generate geodesics for path planning. The approach introduces an actor-critic learning method tailored to predict midpoint waypoints, facilitating the construction of paths in complex environments. The paper details both the theoretical underpinnings and the practical implications of the method, demonstrating its application to two distinct metrics, the Matsumoto metric and a car-like metric, and discussing its potential in fields such as image processing and physical systems modeling."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper presents a distinct approach to generating geodesics in reinforcement learning environments via a \"midpoint tree\" algorithm. The theoretical underpinnings are robust, complemented by a thorough experimental evaluation. The articulation is commendable, with the authors elucidating complex ideas succinctly. This work's originality and potential applicability are clear, indicating its prospective value in advancing research within reinforcement learning and robotics."
                },
                "weaknesses": {
                    "value": "The paper lacks a broader range of examples to demonstrate the applicability of the method to more common robotic tasks like locomotion and manipulation planning. The experimental results, while encouraging, do not showcase a significant advantage over existing methods, raising questions about the practical benefits of the proposed approach. It requires certain assumptions that may not be present in typical robotic environments, such as the need for global coordinate systems and uniform sampling. The method might not be readily applicable to more complicated, dynamic environments.\n\nMore concretely:\n- The algorithm requires additional assumptions that may not be readily available or applicable in common robotic tasks, such as locomotion and manipulation planning. These assumptions include the need for global coordinate systems, obstacle-free environments, and environment-specific policy learning. The method's effectiveness is contingent on these conditions, which are not always present in more complex or dynamically changing real-world scenarios. Additionally, the challenge of generating globally optimal paths and dealing with the complexity of Finsler geodesics further limits its applicability to standard reinforcement learning tasks.\n\n- In the original wording, the paper mentions that the method \"only works well locally since we assume that manifolds have global coordinate systems and the continuous midpoint property may be satisfied only locally. For the generation of globally minimizing geodesics, we may have to divide manifolds, train policies for each local region and connect locally generated geodesics.\" It also states that \"the policy has to be learned for each environment. By modifying our method so that the actor and critic input information on environments, it may be possible to learn a policy applicable to different environments.\" These statements highlight the limitations regarding the need for specific geometric and topological assumptions that may not hold in typical RL tasks in robotics.\n\nA line of work on quasimetric distance for goal-conditioned RL seems related, which could provide important context and benchmarking. I'd be curious whether the proposed approach is related to them.\n- Tongzhou Wang et al., Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning, ICML 2023\n- Tongzhou Wang et al., On the Learning and Learnability of Quasimetrics, ICLR 2022"
                },
                "questions": {
                    "value": "- Can the authors provide additional examples where their method might be applicable, specifically within the realm of robotics tasks like locomotion and manipulation?\n- How does the proposed approach compare in terms of benefits and applicability to other realistic tasks, beyond what has been demonstrated in the paper?\n- Could the authors discuss the relationship and distinctions between their work and recent research on quasimetric learning for goal-conditioned RL?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3381/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699248533447,
            "cdate": 1699248533447,
            "tmdate": 1699636288767,
            "mdate": 1699636288767,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jJsawcYz0a",
                "forum": "9SwObx9Jdn",
                "replyto": "a2gVtoGXbG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3381/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3381/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answers to questions"
                    },
                    "comment": {
                        "value": "Thanks for the review!\n\n> Can the authors provide additional examples where their method might be applicable, specifically within the realm of robotics tasks like locomotion and manipulation?\n\nSome robotics tasks including manipulation are formulated by Riemannian geometry in the context of Riemannian Motion Optimization.\nSee the following papers:\n\nRatliff, Nathan, Marc Toussaint, and Stefan Schaal. \"Understanding the geometry of workspace obstacles in motion optimization.\" 2015 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2015.\n\nMainprice, Jim, Nathan Ratliff, and Stefan Schaal. \"Warping the workspace geometry with electric potentials for motion optimization of manipulation tasks.\" 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2016.\n\nTherefore, our method is potentially appliable to these tasks.\nEspecially, since the technique to warp metrics for avoiding obstacles is already studied, our method can be appliable to environments with obstacles.\n\nThe sentence \"only works well locally since we assume that manifolds have global coordinate systems and the continuous midpoint property may be satisfied only locally\" we wrote is maybe misleading. We assumed the continuous midpoint property for the theoretical result, it seems not to be practically necessary. Indeed, even environments we used for experiments seem not to have the property.\nTherefore, this assumption is a limitation of our theoretical contribution and not of our method.\n\n> How does the proposed approach compare in terms of benefits and applicability to other realistic tasks, beyond what has been demonstrated in the paper?\n\nIn the context of aforementioned Riemannian motion optimization, the practical advantage of RL methods compared to the optimization method is the computational cost. While optimization is computationally costly, predictions by learned agents are fast. The advantage of our method over sequential RL method is that there is no need to reward engineering. In addition, the fact that the proposed method has a very short \"horizon\" compared to sequential RL may stabilize learning.\n\n> Could the authors discuss the relationship and distinctions between their work and recent research on quasimetric learning for goal-conditioned RL?\n\nThank you for sharing related works!\nThe main difference between our baseline sequential RL and the quasimetric learning method is architectures of value function approximation models. \nSince we do not focus on architectures of critics and used simple MLPs for both the proposed method and the baseline methods, the direction of our research and the direction of this research are orthogonal. The embedding method for quasimetric learning can be used for the critic in our method."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3381/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699870692028,
                "cdate": 1699870692028,
                "tmdate": 1699870692028,
                "mdate": 1699870692028,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]