[
    {
        "title": "Can Language Agents Approach the Performance of RL? An Empirical Study On OpenAI Gym"
    },
    {
        "review": {
            "id": "R2rJ0pLq3J",
            "forum": "F0q880yOgY",
            "replyto": "F0q880yOgY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7636/Reviewer_tsqs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7636/Reviewer_tsqs"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript introduces a novel benchmark, `TextGym`, which is based on the `OpenAI Gym` RL environments and can provide insightful assessment for the decision-making capabilities of language models.  The authors also compared various existing language agents based on `GPT-3`, and presented an `EXE` agent that strategically balances exploration and exploitation."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "In general, this is a very well-organized and well-written paper, with solid contributions.\n\n- The problem that the authors are investigating is a hot topic among LLM researchers, and people are in need of a unified benchmark to evaluate the decision-making capabilities of language models.\n- The benchmark is novel, and has the potential to generate impact.\n- The authors provided comprehensive and detailed empirical results."
                },
                "weaknesses": {
                    "value": "- I would encourage the authors to avoid assertive and absolute claims, such as the first sentence \"large language models lack grounding in external environments\", and at the bottom of page 1, \"we adopt `OpenAI Gym`, the most classic and prevalent...\".  These arguments, though eye-catching, are debatable, and such debates are irrelevant to the central points that the authors are trying to make, so why not avoid them?\n-  In the final paragraph on page 2, \"after *sufficient* numerical experiments...\"  I suppose that it should be left for the readers to decide whether the experiments are sufficient.\n- Point 3) at the bottom of page 2 is confusing.  Which baseline do the language agents improve upon?  I suppose that the authors wanted to say that the performance can be improved *via* incorporating XXX, but it would be helpful to reorganize the sentence and clarify the point.\n- The Problem Formulation section is somewhat redundant, as none of the math symbols are reused anywhere in the main text."
                },
                "questions": {
                    "value": "- What is the main difference between the `EXE` language agent and other language agents, besides that the learner module provides exploration-exploitation balancing strategies in addition to how to exploit?  The reason that I am asking this question is that, if the actor and critic modules are identical to the other agents, the authors don't have to reiterate the descriptions on page 7.  The saved space can be dedicated to explaining how the learner module provides guidance on exploration and exploitation.\n- It remains unclear to me (after reading the Appendix) how different levels of agents are implemented.  It would be useful to provide at least a brief description."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7636/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698637335121,
            "cdate": 1698637335121,
            "tmdate": 1699636928217,
            "mdate": 1699636928217,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Gl7SsxIB3C",
                "forum": "F0q880yOgY",
                "replyto": "R2rJ0pLq3J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7636/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7636/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer tsqs"
                    },
                    "comment": {
                        "value": "Thank you very much for recognizing our work, and your constructive comments and suggestions! We have revised our paper accordingly. Below, we will provide detailed responses to each point.\n\n> `Q1: I would encourage the authors to avoid assertive and absolute claims, such as the first sentence \"large language models lack grounding in external environments\", and at the bottom of page 1, \"we adopt OpenAI Gym, the most classic and prevalent...\". These arguments, though eye-catching, are debatable, and such debates are irrelevant to the central points that the authors are trying to make, so why not avoid them? In the final paragraph on page 2, \"after sufficient numerical experiments...\" I suppose that it should be left for the readers to decide whether the experiments are sufficient. Point 3) at the bottom of page 2 is confusing. Which baseline do the language agents improve upon? I suppose that the authors wanted to say that the performance can be improved via incorporating XXX, but it would be helpful to reorganize the sentence and clarify the point.`\n\nThank you very much for your valuable comments! We have polished our text and avoided the over-claims.\n\n> `Q2: The Problem Formulation section is somewhat redundant, as none of the math symbols are reused anywhere in the main text.`\n\nThanks for your advice! We have moved it to Appendix.\n\n> `Q3: What is the main difference between the EXE language agent and other language agents, besides that the learner module provides exploration-exploitation balancing strategies in addition to how to exploit? The reason that I am asking this question is that, if the actor and critic modules are identical to the other agents, the authors don't have to reiterate the descriptions on page 7. The saved space can be dedicated to explaining how the learner module provides guidance on exploration and exploitation.`\n\nThanks you for your comments! We have added pseudocode of EXE and $5$ scenarios in our paper.\n\n> `Q4: It remains unclear to me (after reading the Appendix) how different levels of agents are implemented. It would be useful to provide at least a brief description.`\n\nWe are apology for the missing description. We have added pseudocode for them in the revised version."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7636/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739555777,
                "cdate": 1700739555777,
                "tmdate": 1700739555777,
                "mdate": 1700739555777,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AmwkowBgZD",
            "forum": "F0q880yOgY",
            "replyto": "F0q880yOgY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7636/Reviewer_i4oJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7636/Reviewer_i4oJ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors investigate whether LLMs can achieve the same level of performance as agents trained with RL in classical RL benchmarks. The authors consider various algorithms and settings for training Language agents in Gym environments. They introduce a pipeline for creating text-based versions of Gym environments, as well as 5 training scenarios. Finally, the authors propose their own algorithm, EXE, which improves on recent algorithms like Reflexion in solving their text-based versions of the Gym environments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper asks an interesting and timely question. They introduce challenges around training Language Agents and discuss various levels of domain knowledge that can be provided to the agents. The proposed algorithm, EXE, can solve more tasks than the baselines, and comes close to PPO performance on some tasks, which is nice."
                },
                "weaknesses": {
                    "value": "While the idea is timely, the methodology of the paper is unfortunately not very clear. The algorithm the authors propose is not introduced formally, but in charts. This makes it challenging to understand the algorithm, or why it works. Furthermore, the five training scenarios are not described in enough detail either. As a result, it\u2019s not clear to me why L3 does the best out of the setups. It would also help if the authors motivate their algorithm better, as well as the reason why they expect it to work better than previous approaches.\n\nIt is also strange to me (if I understood this correctly) that the authors used GPT-4 to generate the python code that translates the Gym observations into text. This method seems error prone, and the authors do not demonstrate that this works, or why it\u2019s even necessary in the first place.\n\nLastly, the experiments are not presented with enough detail. For instance:\n* the number of seeds used to evaluate the agents is not reported.\n* The learning curves or the converged performance of the PPO/SAC agents are not reported in the main text, but the Appendix. \n* The threshold for saying that a task is solved is not defined in the main text either.\n\n\nMinor points:\n* The radar plot in figure 3B is very crowded.\n* Parts of the paper are not so easy to follow. Consider simplifying language."
                },
                "questions": {
                    "value": "* If SAC can solve tasks that PPO cannot solve in your setup, why are the PPO scores reported for the majority of the tasks instead of the SAC scores? \n* I don\u2019t see how some of these environments are partially observable (for instance Acrobot  and Cartpole) - this depends on how the states are translated into text. If they are indeed partially observable, it would be good to clarify how in the text.\n* The authors say that Language agents are better in more intuitive and deterministic environments. Aren\u2019t environments like Acrobot and LunarLandar deterministic (after the random first state)? And Blackjack, which is stochastic, can be solved quite well by EXE according to the Radar plot? What do the authors mean when they say an environment is intuitive?\n\nWhile I like the idea of designing algorithms that make Language Agents better at solving RL tasks, I think the experiments and ideas of the paper need to be presented differently. The motivation for their algorithm is not clear, and the evaluation of the different agents does not seem very thorough. I therefore recommend that the paper is rejected as it currently is. I think the paper could be improved a lot by motivating the algorithm and a specific training scenario they recommend better. The evaluation of the standard RL algorithms like PPO and SAC could also be done more thoroughly. Finally, it would be great to understand why EXE works better than the Language Agents."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7636/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698664051731,
            "cdate": 1698664051731,
            "tmdate": 1699636928113,
            "mdate": 1699636928113,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UEqPbtJ6wV",
                "forum": "F0q880yOgY",
                "replyto": "AmwkowBgZD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7636/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7636/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer i4oJ"
                    },
                    "comment": {
                        "value": "Thank you very much for your constructive comments and suggestions. We have revised our paper accordingly. Below, we will provide detailed responses to each point.\n\n> `Q1: While the idea is timely, the methodology of the paper is unfortunately not very clear. The algorithm the authors propose is not introduced formally, but in charts. This makes it challenging to understand the algorithm, or why it works. Furthermore, the five training scenarios are not described in enough detail either. As a result, it\u2019s not clear to me why L3 does the best out of the setups. It would also help if the authors motivate their algorithm better, as well as the reason why they expect it to work better than previous approaches.`\n\nThank you very much for your valuable advices! We have improved our presentation of the algorithm and $5$ levels. \n\n> `Q2: It is also strange to me (if I understood this correctly) that the authors used GPT-4 to generate the python code that translates the Gym observations into text. This method seems error prone, and the authors do not demonstrate that this works, or why it\u2019s even necessary in the first place.`\n\nThanks for your helpful comments! We have moved it to the appendix. \n\n> `Q3: Lastly, the experiments are not presented with enough detail. For instance: the number of seeds used to evaluate the agents is not reported. The learning curves or the converged performance of the PPO/SAC agents are not reported in the main text, but the Appendix. The threshold for saying that a task is solved is not defined in the main text either.`\n\nThank you very much for your questions! \n\n1. we have extended our evaluations to $5$ seeds. \n2. PPO and SAC performances are added. \n3. The threshold description is moved to the main text.\n\n> `Q4: If SAC can solve tasks that PPO cannot solve in your setup, why are the PPO scores reported for the majority of the tasks instead of the SAC scores?`\n\nThank you very much for your questions! SAC performances are added. There are also tasks that SAC can not solve. \n\n> `Q5: I don\u2019t see how some of these environments are partially observable (for instance Acrobot and Cartpole) - this depends on how the states are translated into text. If they are indeed partially observable, it would be good to clarify how in the text.\nThe authors say that Language agents are better in more intuitive and deterministic environments. Aren\u2019t environments like Acrobot and LunarLandar deterministic (after the random first state)? And Blackjack, which is stochastic, can be solved quite well by EXE according to the Radar plot? What do the authors mean when they say an environment is intuitive?`\n\nThis is a very interesting and important question! It is true that not all of the environments are partially observable. We have summarized the challenges for each environment in Appendix C.1 We have altered the informal description \"non-intuitive\" to \"complex dynamic\". For instance, it is easy to reason that the cart will move right if we push right when the cart speed is $0$. We call this dynamic simple. For `Acrobot`, it is hard to reason how the two links will behave when applying $-1$ torque to the actuated joint. \n\n> `Q6: I think the paper could be improved a lot by motivating the algorithm and a specific training scenario they recommend better. The evaluation of the standard RL algorithms like PPO and SAC could also be done more thoroughly. Finally, it would be great to understand why EXE works better than the Language Agents.`\n\nThank you very much, it is a really good suggestion. In the revised version:\n1. We take `CliffWalking` as our motivating example. \n2. We make further analysis on EXE, especially comparing it to Reflexion."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7636/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739128285,
                "cdate": 1700739128285,
                "tmdate": 1700739128285,
                "mdate": 1700739128285,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "t7mZ99sTMW",
            "forum": "F0q880yOgY",
            "replyto": "F0q880yOgY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7636/Reviewer_9WGb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7636/Reviewer_9WGb"
            ],
            "content": {
                "summary": {
                    "value": "The papers compares the performance of PPO and LLM-based AI agents on text-based version of Gym environments. The performance is evaluated by introducing different levels of domain knowledge into the LLM-based agent, and using a peculiar architecture (called EXE) for the LLM-based AI agent to interact with the environment."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Despite my generally negative judgement on the paper, it presents some positive features in terms of positioning and perspective. In particular, these are, in my opinion, the strengths of the paper:\n- The study of how LLM-based AI agents should be evaluated, and how their performance can be compared to other classes of agents, is becoming an important research problem worth investigating.\n- The principle of evaluating an LLM-based agent's performance when prompted with varying amounts of task-related information is sound and promising."
                },
                "weaknesses": {
                    "value": "The paper presents many issues and limitations, both in terms of experimental protocols and rigor, and in terms of presentation. These are the main issues that I have identified:\n- First and most importantly, the level of rigor in the empirical evaluation is far from being satisfactory. Performance comparisons are executed using a single seed only and not showing any form of confidence or error bars anywhere in the plots. A comparison of this kind is simply meaningless from the empirical standpoint, regardless of the funding constraints one might have to comply to, and invalidates all the claims and supposed findings contained in the paper. The sequential decision-making scientific community has made significant progress on evaluation rigor in the last few years, and I encourage the author to look at recent work (e.g., https://arxiv.org/abs/2304.01315 , https://arxiv.org/abs/2108.13264) and comply to the standards proposed there. The presence of LLMs or product-oriented APIs does not change the importance of rigorous scientific evaluation.\n- Some details in the presentation are confusing or unnecessary. As a significant example, why should one highlight that GPT-4 generated the text version of the environment, if that one is not a feature that is being scientifically evaluated? As of now, it reads an attempt to yield to GPT-4 the responsibility concerning the correctness of the new environments that are being proposed. But this is totally irrelevant for the matter of the presentation of the scientific evaluation of a system: regardless of whether the code for evaluating the AI agents was generated by GPT-4 writing code, humans writing code, or copied from a random repository on the web, the only thing that matters is its correctness and alignment with the claims that the rest of the paper is trying to make.\n- I believe a meaningful comparison of pretrained LLM-based systems with RL algorithms trained from scratch might not be as straightforward as the paper is trying to imply. LLMs are pretrained on all the internet, and the neural networks trained with RL start from scratch. It is still interesting to compare the performance of these different approaches, but saying that one approach is more sample efficient than another one does not do justice about their different nature and tradeoffs."
                },
                "questions": {
                    "value": "- How are the claims in the paper statistically significant with a single seed and no error bars?\n- Can you remove any mention to GPT-4 automatically generating environments?\n- Why did you use PPO for some environments and SAC for some others?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7636/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762271072,
            "cdate": 1698762271072,
            "tmdate": 1699636927982,
            "mdate": 1699636927982,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8aqEG3KeBW",
                "forum": "F0q880yOgY",
                "replyto": "t7mZ99sTMW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7636/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7636/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 9WGb"
                    },
                    "comment": {
                        "value": "Thank you very much for your constructive comments and suggestions. We have revised our paper accordingly. Below, we will provide detailed responses to each point.\n\n> `Q1: First and most importantly, the level of rigor in the empirical evaluation is far from being satisfactory. Performance comparisons are executed using a single seed only and not showing any form of confidence or error bars anywhere in the plots. A comparison of this kind is simply meaningless from the empirical standpoint, regardless of the funding constraints one might have to comply to, and invalidates all the claims and supposed findings contained in the paper. The sequential decision-making scientific community has made significant progress on evaluation rigor in the last few years, and I encourage the author to look at recent work (e.g., https://arxiv.org/abs/2304.01315 , https://arxiv.org/abs/2108.13264) and comply to the standards proposed there. The presence of LLMs or product-oriented APIs does not change the importance of rigorous scientific evaluation.`\n\nThank you very much for your comments. Please refer to **Response to all reviewers: Q1 and Q3.**\n\n> `Q2: Some details in the presentation are confusing or unnecessary. As a significant example, why should one highlight that GPT-4 generated the text version of the environment if that one is not a feature that is being scientifically evaluated? As of now, it reads an attempt to yield to GPT-4 the responsibility concerning the correctness of the new environments that are being proposed. But this is totally irrelevant for the matter of the presentation of the scientific evaluation of a system: regardless of whether the code for evaluating the AI agents was generated by GPT-4 writing code, humans writing code, or copied from a random repository on the web, the only thing that matters is its correctness and alignment with the claims that the rest of the paper is trying to make.`\n\nThank you very much for your advices. We have moved it to the appendix and taken a more compact way to present it. \nHowever, we also want to clarify that the results that GPT-4 is able to generate a translator based on the documentation and template is very helpful for researchers who want to use language agents as alternatives to PPO. \n\n> `Q3: I believe a meaningful comparison of pretrained LLM-based systems with RL algorithms trained from scratch might not be as straightforward as the paper is trying to imply. LLMs are pretrained on all the internet, and the neural networks trained with RL start from scratch. It is still interesting to compare the performance of these different approaches, but saying that one approach is more sample efficient than another one does not do justice about their different nature and tradeoffs.`\n\nThank you for your valuable suggestions! We have repositioned our paper as in **Response to all reviews: Q1** and motivated our comparisons as in **Response to all reviews: Q2**.\n\n> `Q4: Why did you use PPO for some environments and SAC for some others?`\n\nThank you very much for your question! For `MountainCarContinuous`, PPO fails to generate expert-level performance trajectories due to the exploration challenge in `MountainCarContinuous`. PPO gets easily to stuck in sub-optimal strategy (not move) even with grid searching hyper-parameters. Although our main focus is the comparison of PPO, the infeasible expert trajectory will make the `Lv4` scenario infeasible. Thus we take SAC to generate the trajectories and we take it to construct the L4 scenario. See also in the **Response for reviewer DgbS.**"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7636/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738522134,
                "cdate": 1700738522134,
                "tmdate": 1700738531720,
                "mdate": 1700738531720,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bEOZJljNoy",
            "forum": "F0q880yOgY",
            "replyto": "F0q880yOgY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7636/Reviewer_CAf4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7636/Reviewer_CAf4"
            ],
            "content": {
                "summary": {
                    "value": "The authors create a TextGym benchmark conforming to the popular OpenAI Gym interface to evaluate LLM agents on some common RL environments, including CartPole, MountainCar variants, BlackJack and CliffWalking (see Fig. 3a for names of others). \n\nThe environments are converted into textual form to be able to be parsed by the language agents. This is done by using the official documentation of the environments and prompting GPT-4 with the info (the process is described in Fig. 5 in the Appendix). \n\nThe authors further design 5 scenarios with different amounts of prior knowledge that is provided to the language agents to be able to solve the environments - levels 1 through 5 with a higher number corresponding to more prior knowledge provided to the agents. Level 1 corresponding to zero prior knowledge, level 2 to providing non-optimal policies' trajectories to learn from (inspired by offline RL) - they use random policy rollouts for this scenario, level 3 to letting the agent interact with the environment for some time (inspired by RL) - they let the language agent interact with the environment for 5 episodes, level 4 to providing expert trajectories (inspired by imitation learning) - they use optimised RL agents from the Tianshou library for this scenario, level 5 to be provided expert human guidance in natural language to help the agent solve the environment.\n\nThey also come up with a unified architecture for such language agents with 3 components: actor, critic and learner (inspired by RL). They design a novel agent (the explore-exploit-guided language (EXE) agent) conforming to this architecture, with the learner and critic having long term memories of episodic transitions and the actor having a short term memory of the current episode's transitions. The learner guides the actor to explore or exploit based on criticism from the critic.\n\nExperimentally, various language agents (Table 1) are benchmarked on their TextGym environments with performances normalized between -1 and 1. They show that the language agents can solve 5/8 environments and provide some inisghts into why and what agents and what scenarios performed better in the end."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea behind the benchmark and the various scenarios and unified architecture are well developed. The process of converting RL environments to a text interface and evaluating language agents on them is likely to lead to useful inisghts. I also find the different scenarios that have been designed intuitive. The paper is generally well presented  in terms of motivation and the problem tackled seems timely given the advent of LLMs."
                },
                "weaknesses": {
                    "value": "I would say that lack of many seeds for a more thorough evaluation are the main weakness. The paper mentions in Appendix B.3 that only one seed is used for evaluation (except 100 for BlackJack). That feels too little given the kinds of environments in there do not seem expensive to me. Additionally, a few more environments could be evaluated given what seems to me to be not so compute-heavy experiments. Finally, the analyses in sections 5.2 and 5.3 seemed to lack in detail.\n\nMinor:\nOpenAI Gym is not an environment, it is library that contains an API for enviroments."
                },
                "questions": {
                    "value": "Could the authors elaborate on the compute costs?\n\nGiven the fact that a lot of training is not needed for these experiments (as I understand it the authors use existing trained models for the LLM part and only train the RL agents with default hyperparameters in Tianshou, so no hyperparameter tuning is needed), I believe more seeds could be run and a wider range of environments could be included, a couple of Mujoco or Atari environments, for example. Running only one seed could also lead to noisier insights and make it harder to analyse the experiments. Could the authors clarify?\n\nWhile I feel the paper is generally well written, the insights and analyses in sections 5.2 and 5.3 seem a bit high-level to me. For example in Fig. 3b: L3 agents actually did better than L5 and L4 agents, L4 was actually very bad even though it is supposed to be the second easiest scenario. L2 and L1 are sometimes better than L3 and L5 agents even though these are harder.  The reasons for such unintuitive results could be discussed in much more detail. The reason for lack of depth of analyses could be that a substantial portion of the paper is dedicated to presenting the benchmark and even Fig. 5 and the following code template in listing-1 are in Appendix even though in my opinion these are key to understanding the work. This makes me feel that maybe a journal would lend itself better to detailed presentations for a promising benchmark such as this. Another reason could be the lack of running many seeds which as mentioned above leads to noisier insights and analyses. Another example of a superfluous statement for me is the first key finding at the end of section 5.3. It seems to be a claim without proper evidence that EXE's exploration and exploitation capabilities helped it perform better. Could the authors say more on this?\n\nIn the problem formulation section 2.1, it is unclear to me what the index i is for?\n\nThe first case study in Appendix C was already hard for me to understand, so I did not try reading further here. I am not sure what agent was used here. Also, why was a part of the prompt hallucinated (\"cliffs(Transversal interval from (3, 1) to (3, 10)\")? It was provided by the authors I suppose and could not be hallucianted.\n\nCould you provide a Code / Jupyter example for the work?\n\nWhy not show SAC in Fig. 3a for Taxi and MountainCar continuous environments as default PPO cannot solve them?\n\nMaybe a table showing how many environments were solved by each agent in each scenario would be useful?\n\nCould you also please explain \"Specifically, we allocate 1 hour of effort to develop scenarios for a single agent in each environment, randomizing the sequence of language agents.\" in more detail?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7636/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7636/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7636/Reviewer_CAf4"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7636/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785113055,
            "cdate": 1698785113055,
            "tmdate": 1699636927873,
            "mdate": 1699636927873,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MpBhvaq1XE",
                "forum": "F0q880yOgY",
                "replyto": "bEOZJljNoy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7636/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7636/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your detailed comments! Response to Reviewer CAf4"
                    },
                    "comment": {
                        "value": "> `Q1: Compute and Time Costs`\n\nFor detailed information on the compute and time costs, please refer to our **Response to all reviewers: Q4**.\n\n> `Q2: Number of Seeds and Range of Environments`\n\nWe have used 5 seeds to make evauation of Language Agents in our revised manuscript. While hyperparameter tuning is not a concern for our experiments, the decision-making process with LLM-based agents is inherently resource-intensive. This restricts the scale of our experiments. For additional context, see **Response to all reviewers: Q3**.\n\n> `Q3: Depth of Analyses in Sections 5.2 and 5.3`\n\nWe have streamlined the presentation for code generation and relocated the GPT-related content to the appendix to allow for a more concise main text. In Experiment section, we have added more analysis and many case studies are provided for each environment in the Appendix D. \n\n> `Q4: Problem Formulation Clarification`\n\nThe issue with the index \\( i \\) in the problem formulation section will be clarified and we now move the formulation section to the appendix. \n\n> `Q5: Explanation of Scenario Development Effort`\n\nWe have refined our explanation regarding the design of expert prompts for each environment and language agent. To minimize bias in comparative analysis, we randomized the sequence of language agent prompt design. This approach also addresses potential concerns about the fairness and adequacy of prompt design. However the expert guidance can hard to be controlled abosolutely fair and we can hard to predict how good it will be for a novel task. Thus our paper proposes `Lv2`, `Lv3`, and `Lv4` scenarios to control the domain knowledge and reduce human efforts."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7636/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740458661,
                "cdate": 1700740458661,
                "tmdate": 1700740641544,
                "mdate": 1700740641544,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Cg7bkdwISH",
            "forum": "F0q880yOgY",
            "replyto": "F0q880yOgY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7636/Reviewer_DgbS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7636/Reviewer_DgbS"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a text interface that uses GPT-4 to translate eight OpenAI-Gym environments into text-based games, then tests a particular LLM, gpt-3.5-turbo0301, on the eight text-based games. Besides necessary verbal description of the current observation in the game, the LLM is also prompted with verbal suggestions/tips for game playing. The suggestion can be either hand-crafted by the authors (called Level-5 scenario in the paper), or from a text-based in-context learning method presented in the paper. In this method, we collect a few episodes of playing experience as input, then instruct a LLM to summarize and evaluate the playing experience as well as the decision policy behind it, and then instruct the LLM again to turn the summary/evaluation into a verbal suggestion for better game playing. The game-playing data used in this method can further be either from a random policy (called Level-2 scenario in the paper), or from self-playing in the RL manner (called Level-3 scenario), or from an expert policy obtained by PPO/SAC (called Level-4 scenario). \n\nThe paper calls the above method, EXE, if the LLM is instructed to make suggestion that encourages some exploration behavior in the game. The paper reports that, without the EXE trick, the GPT3.5-based language agents can barely solve any of the text-based games except for Blackjack-v1. With the EXE trick, the language agent achieves reasonable performance on 5 out of 8 environments under the Level-3 scenario, i.e. when the data for suggestion learning is collected in RL manner. Other suggestion-learning scenarios (L2,4,5) does not seem to work even with the EXE trick."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "LLMs exhibit potential in general problem solving, as well as in some new learning forms such as learning from instruction and few-shot in-context learning. Utilizing LLMs in various problem domains beyond NLP is an interesting and hot topic in general. This paper reports some informative results to this end. It is especially intriguing to see that current LLMs can learn more effectively from experience data of its own, compared with learning from experience data or verbal instructions of better decision agents (corresponding to the superiority of L3 agents over L4 and L5 agents in Figure 4a)."
                },
                "weaknesses": {
                    "value": "**(a)** I think the general perspective of the paper is confusing. This is reflected in the paper title already: The authors ask if language agents is competitive to RL, but language-based agent and RL-based agent are not competing methods at all. In fact, the Lv3 language agent discussed in this paper -- which seem to be the only working language agents, according Fig. 4a -- is exactly a reinforcement learning procedure that improves decision policy from evaluative feedbacks collected from environmental interaction. You can't beat \"RL\" with a RL agent.\n\nMore importantly, as \"an empirical study on OpenAI-Gym\", the experiment includes no Mujoco or Atari task at all, which is a notable drawback as many representative tasks of OpenAI-Gym are in these two categories. Even for the chosen task categories, the tasks are still selected, with 4 out 12 missing in the paper. I am afraid the simple tasks considered in this paper cannot represent \"the performance of RL\" which is the target of the paper. \n\n**(b)** Even if we re-position this paper as just a comparison between two specific methods (EXE vs PPO) on some specific simple tasks as a preliminary study, the current experiment setup may be still a bit biased. For example, I suspect the model size in PPO is orders-of-magnitude smaller than the GPT-3.5 in the language agents. Task-specific heuristics is provided (only) to the language agents. The chosen PPO implementation seems to have weird result on some tasks such as MountainCarContinuous. Results of some tasks in the same category are missing. Finally, when it comes to sample complexity, we should keep in mind that the LLMs are pretrained on huge data, thus it's not really a \"5 vs 20K episodes\" game; perhaps the paper can compare also with meta-RL agents such as Gato (Reed et al. 2022). See my Question 1~4 below for the detailed concerns.\n\n**(c)** In terms of novelty, the paper may need to better contrast with prior works, especially with Reflexion (Shinn et al. 2023) which is similar to the proposed EXE method. Current appendix A.1 is not enough in this regard.\n\n**(d)** The current presentation leaves many things unclear. See my Question 5~10 below for the detailed concerns here. Experimentation code is not released, which is not ideal as an \"empirical study\" that proposes a new \"benchmark\"."
                },
                "questions": {
                    "value": "1. In your experiment, what's the architecture of the policy model at the PPO side? How large is the model for PPO? \n\n2. Did you try not prompting the LLM with the game description info and what's the performance? Although game description is indeed readily available for the particular tasks you considered, as you argued in Remark 1, the description is not necessarily available for other tasks in general. In the end, our goal is not to solve the particular tasks in Gym. We only use them as a benchmark to find method that hopefully works in the general case. If the game description is domain-specific knowledge crafted by human, prompting LLM with such task-specific descriptions is, in its essence, not much different from equipping LLM with gaming expertise from human. It thus feels a bit unfair to compare LLMs equipped with human-generated game description against RL agents that are autonomously learning fully on its own.\n\n3. Why didn't you use SAC as the RL baseline for all the eight environments? Also, have you tried other PPO implementations on MoutainCarContinueous-v0?\n\n4. Have you tried Pendulum (classic control), Frozen Lack (toy text), Bipedal Walker (Box 2D), and Car Racing (Box 2D), which are tasks in the same categories with the ones the paper studies? Since the TextGym code is generated automatically by GPT-4, and your experiment does not involve massive training, I guess encompassing these tasks should not be a huge burden?\n\n5. How does the TexGym interface translate the termination condition? It seems the example code in Appendix B.1 does not process the termination information at all.\n\n6. Is the actor-critic-learner framework in Section 3.3 used in all the 5 scenarios in Section 3.2? If so, how do the critic and learner work in L1 (no guidance) and L5 (expert guidance) scenarios which seem to involve no learning at all?\n\n7. In L2, L3, and L4 scenarios, will the actor receives the 5-episode data as part of its prompt for action selection (in that case the actor will repeatedly receive the data in every gaming step even at testing time), or the 5-episode data is only used by the critic and the actor never sees the 5-episode data but only receives the decision suggestion extracted from that data?\n\n8. What's the default learner? Appendix B.2 only gives the default critic.\n\n9. In the first paragraph of Page 8, how are the scores between (0,1) mapped to the raw performance? A linear mapping? For reproducibility concern, please give the raw performance scores of the solvability threshold and sota thresholds used in your experiment.\n\n10. Figure 3b and last paragraph of Page 8: from the picture it is not evident at all that \"L3-Agents outperform their counterparts\". All colored regions are stacked together and it's hard to tell. Please give data table for the exact performance scores. \n\n11. Any explanation for why L4 result is worse than L3? Intuitively, the training data in L4 agents should be of higher quality (in terms of performance score) than those used in L3 agents. If the LLM can summarize useful principles from the latter, it should be able to do that in the former case too, intuitively.\n\n12. In the paragraph below Figure 4, you said \"for L1, L2, and L4 scenarios, EXE also outperforms other [language agents]\". But to my current understanding, in these three scenarios the agent has no chance to explore the environment at all, isn't it? These three scenarios are pure offline scenarios, while the exploration-vs-exploitation trade-off becomes relevant mostly in online RL setting."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7636/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699441878465,
            "cdate": 1699441878465,
            "tmdate": 1699636927774,
            "mdate": 1699636927774,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wp0TDK8ExH",
                "forum": "F0q880yOgY",
                "replyto": "Cg7bkdwISH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7636/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7636/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your detailed comments! Response to Reviewer DgbS (1/4)"
                    },
                    "comment": {
                        "value": "> `Q1: Scope of the Study`\n\nThank you for your insightful suggestions. We have repositioned our paper to explore the potential of language agents as an alternative to PPO, which is reflected in the updated title: \"Can Language Agents Be Alternatives to PPO? A Preliminary Empirical Study On OpenAI Gym\". This revision also addresses the concern of \"beat RL with RL\".  \nSee details in Common Response Q1.\n\n> `Q2: Omission of Mujoco, Atari Tasks, and More`\n\nPlease refer to **Response to all reviewers: Q2 and Q4**. The evaluation costs are significant and cannot be overlooked. We have also made attempts on FrozenLake, which is challenging due to its stochastic dynamics. Even with optimal actions, performance can be negatively impacted. Therefore, we have not pursued further evaluations in this area for now, considering the cost of evaluation.\n\n> `Q3: A Fairer Comparison Between Language Agents and PPO`\n\nWe direct the reviewer to **Response to all reviewers: Q2** for a more detailed explanation.  \n\nFor details on PPO, please see Appendix C.4 in our revised manuscript. In our experiments, we utilized Tianshou in almost every environment and `Taxi-v3` with stable-baselines3. These were trained and evaluated in OpenAI Gym, not TextGym. Regarding model size, our policy model has 8,964 trainable parameters when there are 3 actions and the input dimension is 1. This figure is the sum of elements across all parameter tensors that require gradient computation.\n\n> `Q4: Additional Comparison with Reflexion`\n\nFirstly, we have provided pseudocode for both EXE and Reflexion in the Appendix to elucidate their differences and the novelty of EXE. There are three main distinctions: \n1. EXE can generate suggestions without prior experience; \n2. The critic in EXE is a language model that evaluates the trajectory based on suggestions guiding the actor's sampling process;\n3. EXE specifically considers insights, exploration, exploitation, and their balance, whereas Reflexion focuses solely on score improvement.\n\nWe also added case studies between them in Experiment section 4.4, with a more detailed analysis presented in the Appendix D."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7636/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738374167,
                "cdate": 1700738374167,
                "tmdate": 1700739853807,
                "mdate": 1700739853807,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "j4L2CjaDTw",
                "forum": "F0q880yOgY",
                "replyto": "Cg7bkdwISH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7636/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7636/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DgbS (2/4)"
                    },
                    "comment": {
                        "value": "> `Q5: Necessity of Game Descriptions`\n  \nWe've included additional scenarios in the Appendix D.2, where EXE's performance in environments such as Cartpole, MountainCar, and MountainCarContinuous was notably lower without the basic game description. This demonstrates that EXE's effectiveness may be constrained when essential game details are not provided.\n\nIt's important to note that the game description is not crafted to enhance agent performance artificially; it typically exists as part of the task's framework. If a simulator for a task has been developed, it is reasonable to assume that a description of the game would be available. Human knowledge at Level 5 (L5) differs significantly from a mere game description; it is often constructed to intentionally boost the performance of Language Agents. For instance, in Cliffwalking, humans might outline the cliff locations, the goal, and even provide demonstrations for Language Agents\u2014information that may not be readily available for a new task.\n\n> `Q6: Release of Code`\n \nOur code has been open-sourced.\n\n> `Q7: Clarity in Figure 3b and Supporting Data`\n \nWe have revised Figure 3b for better visualization, offering a clearer representation of the performance metrics."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7636/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738779805,
                "cdate": 1700738779805,
                "tmdate": 1700739982891,
                "mdate": 1700739982891,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HXnSdidbzf",
                "forum": "F0q880yOgY",
                "replyto": "Cg7bkdwISH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7636/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7636/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DgbS (3/4)"
                    },
                    "comment": {
                        "value": "> `Q8: Explanation for L4 Results Being Worse Than L3`\n\nA8: The GPT model inherently exhibits a degree of stochasticity, which can affect the outcome. When it comes to expert data, a lack of diversity can impede the agents' ability to generalize, a phenomenon observed in contrasts between imitation learning and reinforcement learning. This is supported by our case studies (Appendix 4.4). \n\n> `Q9: Clarification on Scenarios L1, L2, and L4`\n\nA9: Indeed, L1, L2, and L4 are offline scenarios where agents do not actively explore the environment. In L1, EXE is unique in that it engages the learner, potentially aiding the actor with structured suggestions about exploration and exploitation. Supporting this, we observed in CliffWalking that EXE visited the goal in all five trials, compared to Reflexion, which did not reach the goal in any trial within the L1 scenario. For L2 and L4, the improvements in our updated version are modest and can be attributed to the structured suggestions provided by our model. In earlier versions, the notable performance gains in L2 and L4 were likely influenced by stochastic variation.\n\n> `Q10: Peculiar Performance of PPO in MountainCarContinuous`\n\nFor MountainCarContinuous, we conducted a comprehensive grid search covering various hyperparameters:\n\n- Learning rate: `{1e-3, 1e-4, 1e-5}`\n- Discount factor: `{0.99, 0.95, 0.9}`\n- Weight for entropy loss: `{0.01, 0.05, 0.1}`\n- Number of repetitions for policy learning: `{10, 20}`\n\nDespite these efforts, the performance issues persisted. A significant challenge in this environment is the substantial penalty received by the agent when failing to reach the target, which often leads to a strategy of inactivity as a quick convergence solution. We also tested the implementation of PPO using stable baselines3.\n\nIt is worth mentioning that our goal was not to fine-tune PPO to achieve superior performance, as extensive parameter searching can equate to expert guidance akin to the L5 scenario. Achieving excellent performance through extensive effort (akin to prompt designing) is possible but usually not the preferred approach.\n\n> `Q11: SAC Baseline and PPO Implementations on MountainCarContinuous-v0`\n\nWe incorporated SAC results after searching for optimal alpha, learning rate, and gamma using the stablebaselines3 repository. While SAC was successful in `MountainCarContinuous`, it did not perform well in `CliffWalking`.\n\nFor each game, we performed a grid search for:\n\n- Learning rate: `{1e-3, 1e-4, 1e-5}`\n- Discount factor: `{0.99, 0.95, 0.9}`\n- Entropy regularization coefficient: `{0.1, 0.2, 0.5}`\n- Auto-entropy regularization coefficient setting.\n\nBoth PPO and SAC encountered challenges in certain tasks.\n\n_Table: SAC Performance Across Different Games_\n\n| Games | Performances |\n| --- | --- |\n| CartPole-v0 | `194.67 \u00b1 56.89` |\n| LunarLander-v2 | `-7.68 \u00b1 6.72` |\n| Acrobot-v1 | `-74.67 \u00b1 0.22` |\n| MountainCar-v0 | `-89.33 \u00b1 1.56` |\n| Blackjack-v1 | `13.33 \u00b1 14.89` |\n| Taxi-v3 | `-200.00 \u00b1 0.00` |\n| CliffWalking-v0 | `-200.00 \u00b1 0.00` |\n| MountainCarContinuous-v0 | `93.23 \u00b1 2.06` |\n\n---"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7636/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739385981,
                "cdate": 1700739385981,
                "tmdate": 1700740090351,
                "mdate": 1700740090351,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CGfFt47Tgz",
                "forum": "F0q880yOgY",
                "replyto": "Cg7bkdwISH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7636/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7636/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DgbS (4/4)"
                    },
                    "comment": {
                        "value": "> `Q12: Termination Condition in TextGym Interface` \n\n`TextGym` utilizes a translator interface wrapped around OpenAI Gym to handle the termination conditions. The termination flag is managed by the gym simulator, and language agents are informed about the termination conditions within the game descriptions. This is particularly highlighted in Appendix B.1. \n\n\n> `Q13: Actor's Exposure to Episode Data in Scenarios Lv2, Lv3, and Lv4`\n\nIn scenarios `Lv2`, `Lv3`, and `Lv4`, the actor does not receive the $5$-episode data for action selection; it only receives the suggestions derived from the data.\n\n> `Q14: Default Learner Specification`\n\nDetails about the default learner are now specified in Appendix B.2, highlighted in blue for clarity.\n\n> `Q15: Mapping of Scores and Raw Performance Data`\n\nThe raw performance scores are documented in Appendix D.1. The normalized scores used in the experiments are computed as described in the experiment section on page 7."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7636/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739605709,
                "cdate": 1700739605709,
                "tmdate": 1700740204835,
                "mdate": 1700740204835,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]