[
    {
        "title": "Generalizable Deep RL-Based TSP Solver via Approximate Invariance"
    },
    {
        "review": {
            "id": "04ziYSyJxY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1815/Reviewer_dYNB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1815/Reviewer_dYNB"
            ],
            "forum": "oGsR3MJvwS",
            "replyto": "oGsR3MJvwS",
            "content": {
                "summary": {
                    "value": "This paper studies the generalization of neural TSP solvers. The authors propose a Transformer Structured TSP Solver, which reduces the space of decision-making to the K-nearest neighbors of the current node. By further combining with data augmentation and MCTS, the proposed method (i.e., $TS^4$) could achieve superior generalization performance on large-scale TSP instances when only trained on TSP50."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The motivation is clear. The studied topic (i.e., generalization) is important.\n* The proposed method is sound to me. The authors claim three contributions, including (1) the local KNN view, (2) the modified training algorithm with data augmentations, and (3) the combination with MCTS.\n* The empirical results look good. The source code is provided."
                },
                "weaknesses": {
                    "value": "* The scope of this paper is limited to TSP. Popular attention-based models (e.g., [1, 2]) could solve a wide range of VRP variants. It is suggested to (at least) include CVRP.\n\n* Novelty: \n\n  * I like the third contribution. Typically, MCTS mainly works with heat-map-based methods [3]. This paper demonstrates the potential of attention-based solvers [1, 2] with MCTS.\n  * While the first two seem to be incremental since previous works have already explored them (e.g., [4]).\n\n* The related work and baselines are too limited. This paper studies the generalization issue of DRL-based TSP solvers. Based on the ICLR policy, a comprehensive review and experimental comparison of the recent generalization studies (before June, 2023) is expected. Moreover, Concorde should be added in Table 1, and the gap should be computed w.r.t. its result.\n\n* The justification for Figure 1 is weak. \n\n  * Are you using instances following the uniform distribution? Does the conclusion or empirical observation still hold for other cases, e.g., instances with several clusters of customer nodes?\n\n  * I somewhat don't agree with the conclusion of the right panel of Figure 1 (see below). Recent studies find that the adversarial perturbations [5, 6] may significantly change the optimal solutions of TSP instances. What if the added random perturbation coincides with the adversarial one?\n\n    > \"Small perturbations introduce small gaps, which can be regarded as exploiting approximate invariance.\"\n\n* The problem formulation of TSP (with DRL) should be simple and concise, while the notations of this paper are wordy, making it unclear to the readers. \n\n* Minor: \n\n  * P3 of Introduction: \"how (exact) invariance\" -> should be approximate?\n  * Report the total inference time on a test dataset is better.\n\n[1] Attention, learn to solve routing problems! In ICLR 2019.   \n[2] POMO: Policy optimization with multiple optima for reinforcement learning. In NeurIPS 2020.   \n[3] Generalize a small pre-trained model to arbitrarily large TSP instances. In AAAI 2021.   \n[4] Sym-NCO: Leveraging Symmetricity for Neural Combinatorial Optimization. In NeurIPS 2022.   \n[5] Generalization of neural combinatorial solvers through the lens of adversarial robustness. In ICLR 2022.   \n[6] ROCO: A General Framework for Evaluating Robustness of Combinatorial Optimization Solvers on Graphs. In ICLR 2023.\n\n----\n\n**Given all,** I vote for rejection. This paper needs a bit more work before being accepted."
                },
                "questions": {
                    "value": "* The model does not have a normalization layer after the MHA and FF layers. Any explanations?\n* For the baseline function, why not just use a simple average over the objective values of all augmented instances, as done in [2]?\n* Is the proposed method empirically effective for cross-distribution generalization setting?\n* What is the training complexity (e.g., time and GPU memory) of the proposed method, compared with baselines?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1815/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697430976734,
            "cdate": 1697430976734,
            "tmdate": 1699636111085,
            "mdate": 1699636111085,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0mFEVPxycj",
                "forum": "oGsR3MJvwS",
                "replyto": "04ziYSyJxY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1815/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1815/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Review Response"
                    },
                    "comment": {
                        "value": "- weaknesses\n    - (1) Please check (3) in Point c) in our general response.\n    - (2) Please check Point a) in our general response.\n    - (3) Please check Point b) in our general response. \n      Also, Concorde (or Gurobi) is definitely a better heuristic solver for small-sized TSP instances than LKH3, but the relative performances and the final conclusions do not change whether we select Concorde (or Gurobi) or LKH3 as our \"optimality\" baselines.\n    - The observation we made about the kNN distribution in Fig.1 holds for other distributions than the uniform distribution, which may explain our good performance on TSPLib.\n      In contrast to the two mentioned papers, we apply a non-adversarial noise. In that case, on average our perturbed instances have similar or the same solutions, which is the property that we want to exploit with approximate invariance.\n\n- questions\n    - _q1: \"The model does not have a normalization layer after the MHA and FF layers. Any explanations?\"_\n    \n      We follow the standard Transformer architecture and includes normalization layer after the MHA and FF layers. We omit it since we regard it as implementation details. We will clarify in the final version.\n      \n    - _q2: \"For the baseline function, why not just use a simple average over the objective values of all augmented instances, as done in POMO?\"_\n\n      I think you are asking about the baseline function update, where we use this technique to compare the performance of the trained model and the baseline model. Our baseline simply follows how the trained policy is used: for a given instance, the final solution is the best found among the solutions generated for the original instance and some perturbed instances.\n      \n    - _q3: \"Is the proposed method empirically effective for cross-distribution generalization setting?\"_\n    \n      Please check (2) in Point c) in our general responses.\n    \n    - _q4: \"What is the training complexity (e.g., time and GPU memory) of the proposed method, compared with baselines?\"_\n    \n      The time complexity of a single inference procedure is $O(N (\\omega+1) n^2 \\gamma_{\\text{heads}} d_{\\text{emb}}^2 k)$ and the memory complexity is $O(N (\\omega+1) n \\gamma_{\\text{heads}} d_{\\text{emb}}^2 k)$, where $N$ denotes the batch size, $\\omega$ denotes the augmentation size, $n$ denotes the size of TSP, $\\gamma_{\\text{heads}}$ denotes the number of heads, $d_{\\text{emb}}$ denotes the number of dimensions for embedding and $k$ denotes the size of the kNNs. Therefore, the overall time and memory complexity for training is $O(T M n^2 \\gamma_{\\text{heads}} d_{\\text{emb}}^2 k)$ and $O(n \\gamma_{\\text{heads}} d_{\\text{emb}}^2 k)$, where $T$ denotes number of epochs and $M$ denotes steps within one epoch. Since our model is trained on small-sized instances, those costs are affordable. For inference on larger-size TSPs, it is unavoidable to have $O (n^2)$ in the complexity when applying a constructive method involves global information aggregation. In our experiments, we take $N=6$ and $\\omega=7$ for training models with TSP-50 on a single RTX 3060 GPU, which has 12GB memory. It takes about 3 days to finish training with $T=170$ and $M=1600$."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666319259,
                "cdate": 1700666319259,
                "tmdate": 1700666319259,
                "mdate": 1700666319259,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jbHvevBSBm",
                "forum": "oGsR3MJvwS",
                "replyto": "0mFEVPxycj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1815/Reviewer_dYNB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1815/Reviewer_dYNB"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for your response. \n\n* For the presentation issue, it seems that the author does not update the draft. \n\n* The empirical evidence wrt W.4 is not provided as well.\n\nI have also gone through other reviews, and tend to keep my current rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698636652,
                "cdate": 1700698636652,
                "tmdate": 1700698636652,
                "mdate": 1700698636652,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Samo3dP3wy",
                "forum": "oGsR3MJvwS",
                "replyto": "04ziYSyJxY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1815/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1815/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to remind"
                    },
                    "comment": {
                        "value": "Thanks for your kind remind. \n- We have updated our draft for the presentation issue.\n- Due to the limit time, the experiments on the empirical evidence wrt W.4 are still running. We would provide it in the final draft."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736972760,
                "cdate": 1700736972760,
                "tmdate": 1700737422738,
                "mdate": 1700737422738,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rHQfCdWqTT",
            "forum": "oGsR3MJvwS",
            "replyto": "oGsR3MJvwS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1815/Reviewer_tWCL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1815/Reviewer_tWCL"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors extended the construction type of Transformer-based TSP solver for tackling large instances unseen during training, by coupling the original global view with a kNN based local view. They also extend such Transformer into the Heatmap based solver, which couples with the Monte-Carlo Tree Search. The proposed methods are evaluated on both synthetic instances and benchmark instances (TSPLib)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1- The method itself looks reasonable.\n2- The paper is easy to follow."
                },
                "weaknesses": {
                    "value": "While the paper is well motivated, its literature review and experimental comparison are extremely poor, which significantly diminishes the novelty and contribution.\n\n1- Regarding the 'generalizable', it missed,\n    Towards Omni-generalizable Neural Methods for Vehicle Routing Problems, ICML 2023;\n    Learning Generalizable Models for Vehicle Routing Problems via Knowledge Distillation, NeurIPS 2022;\n2- Regarding combining Global embedding and Local embedding, it missed,\n    Towards Generalizable Neural Solvers for Vehicle Routing Problems via Ensemble with Transferrable Local Policy, Arxiv 2023;\n    Multi-View Graph Contrastive Learning for Solving Vehicle Routing Problems, UAI 2023.\n3- Regarding comparison on instances with >= 1000 nodes, it missed,\n    DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization, Arxiv 2023;\n    Unsupervised Learning for Solving the Travelling Salesman Problem, Arxiv 2023;\n    DIMES: A Differentiable Meta Solver for Combinatorial Optimization Problems, NeurIPS 2022;\n    NeuroLKH: Combining Deep Learning Model with Lin-Kernighan-Helsgaun Heuristic for Solving the Traveling Salesman Problem, NeurIPS 2021;\n    Select and Optimize: Learning to solve large-scale TSP instances, AISTATS 2023.\n\nParticularly, the main ideas of this submission is quite similar to some of them. Moreover, it seems the proposed approach is limited to TSP, especially when it is further converted to the heatmap based paradigm, which considerably harmed its applicability to different VRPs. Overall, I think this submission is far away from the standard of an ICLR publication."
                },
                "questions": {
                    "value": "Please refer to the Weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1815/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697959864253,
            "cdate": 1697959864253,
            "tmdate": 1699636110982,
            "mdate": 1699636110982,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KPlIEz2Zlm",
                "forum": "oGsR3MJvwS",
                "replyto": "rHQfCdWqTT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1815/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1815/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Review Response"
                    },
                    "comment": {
                        "value": "Please check Point b) in our general response."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658586465,
                "cdate": 1700658586465,
                "tmdate": 1700658586465,
                "mdate": 1700658586465,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VOteo8jQom",
            "forum": "oGsR3MJvwS",
            "replyto": "oGsR3MJvwS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1815/Reviewer_xEdU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1815/Reviewer_xEdU"
            ],
            "content": {
                "summary": {
                    "value": "The authors present an attention-based approach that solves the TSP using RL based on a well-known existing approach that can improve generalization to larger problem sizes than the ones trained on. They extend the architecture by including additional encodings of the nearest neighbors of the current last node during the generation of the tour. Additionally, they use data augmentation and exploiting invariance and MCTS. However, these techniques are not novel and have been applied to TSP."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "S1 The approach can achieve SOTA performance in a specific setting."
                },
                "weaknesses": {
                    "value": "W1 Two of the three primary contributions in this paper have been previously explored and are not considered novel (augmentation/invariance, MCTS). The remaining contribution, the encoding of the nearest neighbors, seems relatively straightforward.\n\nW2 The experimental evaluation has several shortcomings. First, the generalization is only evaluated with instances trained on the TSP of size 50. It would be valuable to explore the model's performance across a broader range of problem sizes to gain a more comprehensive understanding of its capabilities.\n\nW3 The paper imposes time limits on several close competitors, which raises questions about whether the proposed method is genuinely superior or simply optimized for a specific time constraint. A more thorough examination without the time limit would be valuable.\n\nW4 Most of the ablation study is done with ts3, but it should be done with ts4 (the complete approach including MCTS).\n\n* Furthermore, the ability of the method to achieve SOTA performance in the tsplib evaluation remains less clear (see appendix), especially since the closest competitor in the random tsp (AttGRCN+MCTS) is missing from the comparison."
                },
                "questions": {
                    "value": "Q1. How would be the generalization look like if the approach is not only trained on size 50?\n\nQ2. How would the competitors perform without the time limit?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1815/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698665478319,
            "cdate": 1698665478319,
            "tmdate": 1699636110914,
            "mdate": 1699636110914,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BErbHr9tpG",
                "forum": "oGsR3MJvwS",
                "replyto": "VOteo8jQom",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1815/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1815/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "- weaknesses\n  - (1) About contribution and novelty. Please check Points a) and b) of our general response.\n  - (2) About experiments. We have trained our model on TSP-30 and TSP-100 (see table below). The performance increases when trained on larger instances. Interestingly, even training on TSP30 achieves very competitive generalization capabilities.\n\n    |                     | **TSP20** | **TSP50** | **TSP100** | **TSP200** | **TSP500** | **TSP1000** |\n    | ------------------- | --------- | --------- | ---------- | ---------- | ---------- | ----------- |\n    | $\\texttt{TS}^3$-30  |    $0.01$%       |    $0.56$%       |       $2.02$%     |      $4.55$%      |    $7.58$%        |        $10.10$%     |\n    | $\\texttt{TS}^3$-50  |     $0.03$%      |     $0.51$%      |     $1.82$%       |     $3.98$%       |       $6.68$%     |     $8.10$%        |\n    | $\\texttt{TS}^3$-100 |     $0.04$%      |    $0.52$%        |     $1.02$%      |     $2.59$%       |      $5.25$%      |    $7.76$%         |\n  - (3) About time limit. In our experiments, the other baselines given more time are still not competitive with our methods.\n  - (4) About ablation study. Since our main contribution is $\\texttt{TS}^3$, we did an extensive ablation study of this architecture. Regarding $\\texttt{TS}^4$, to demonstrate the quality of what is learned by $\\texttt{TS}^3$, we compare MCTS using our proposed heatmap (i.e., $\\texttt{TS}^4$) and MCTS with uniform heatmap. Table 3 shows that the former outperforms the latter.\n  - (5) About comparisons in TSPLIB. We have done a comparison that takes the average of TSP results within a given range as PointerFormer has done. AttGCRN+MCTS results are missing since their code implementation is hard to apply on variable-sized TSPs.\n\n- questions\n  - _q1: \"How would be the generalization look like if the approach is not only trained on size 50?\"_\n  \n    Please check our response to point (2).\n    \n  - _q2: \"How would the competitors perform without the time limit?\"_\n  \n    Please check our response to point (3)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666996705,
                "cdate": 1700666996705,
                "tmdate": 1700666996705,
                "mdate": 1700666996705,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1VtM0yQ9tP",
            "forum": "oGsR3MJvwS",
            "replyto": "oGsR3MJvwS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1815/Reviewer_jwE3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1815/Reviewer_jwE3"
            ],
            "content": {
                "summary": {
                    "value": "The paper attempts to solve the generalization issue in TSP. Local and global policies are learned concurrently and the data augmentation is used to enhance the performance of TSP solver. In experiments, the proposed approaches show inferior to some compared approaches on small problem sizes and gain better generalization results on large problem sizes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Generalization issue in TSP is a critical topic that relates to  large problem solving in real-world settings. This work solves TSP from the local and global views to enhance the generalization of TSP solver. At the same time, data augmentation is a commonly used approach to enhance the performance of neural networks and it is empirically effective to make the TSP solver more generalizable."
                },
                "weaknesses": {
                    "value": "The related work is insufficient with too many highly related works not even mentioned. Many literature have applied different approaches to overcome the generalization issue for vehicle routing problems. Please refer to and discuss the highly related ones like\n\"Towards Generalizable Neural Solvers for Vehicle Routing Problems via Ensemble with Transferrable Local Policy\" and \"Towards Omni-generalizable Neural Methods for Vehicle Routing Problems\".\n\nThe compared approaches are too old and can not represent the current best performance for TSP. More recent methods should ought to be compared to render the results more convincing. For instance, the aforementioned two papers already tackled large routing problems with thousands of nodes and achieved small gaps. A comparison to them is recommended to show the reliability of the approach here.\n\nThe data augmentation and different baseline functions have been widely studied in literature, degrading the significance of this work. The local and global policies are seen in aforementioned paper. The total novelty of this work is not noticeable. In this sense, more comparisons with recent approaches to fully validate the performance is important, which I think is inferior in this paper."
                },
                "questions": {
                    "value": "1. What if POMO or PointerFormer are trained with data augmentation like rotations or scalings,  which are not special and may be easily applied for these baselines?\n2. Since different techniques are added to enhance the generalization. Would these extra techniques obviously raise the training time? How long would it take to train a TS3 model?\n3. I don't see any simple ways to extend the approaches to other vehicle routing problems. The authors may explain a bit how the data augmentation and neural network are to be applied to CVRP."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1815/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1815/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1815/Reviewer_jwE3"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1815/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681534588,
            "cdate": 1698681534588,
            "tmdate": 1699636110817,
            "mdate": 1699636110817,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D5u0WI2Eaz",
                "forum": "oGsR3MJvwS",
                "replyto": "1VtM0yQ9tP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1815/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1815/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Review Response"
                    },
                    "comment": {
                        "value": "- weaknesses\n  - (1) About related works. Please check [1] and [5] in Point b) of our general response.\n  - (2) About comparison to other SOTA approaches. Among learn to construct method, we have not found any other method that can perform better in terms of cross-size generalization than $\\texttt{TS}^3$.\n  - (3) About contribution and novelty. Please check Points a) and b) of our general response.\n\n- questions\n  - _q1: \"What if POMO or PointerFormer are trained with data augmentation like rotations or scalings, which are not special and may be easily applied for these baselines?\"_\n  \n    Note that POMO and PointerFormer train independently over a batch of an instance and its augmented ones (using rotations), while our training method is designed to enforce approximate invariance (using noise+rotation+scaling), i.e., the loss of the augmented instances is computed with respect to the original one. We believe that our training technique if applied in POMO or PointerFormer would improve their performance.\n    \n  - _q2: \"Since different techniques are added to enhance the generalization. Would these extra techniques obviously raise the training time? How long would it take to train a $\\texttt{TS}^3$?\"_\n  \n    Data augmentation won't cost too much time since it only involves simple operations like noise, rotation, and reflection, which can be processed in batches. The design of the local and global architecture introduces additional training and inference time. However, those costs are affordable since our model can be trained on small instances with low batch size and still achieve a competitive performance. Currently, our models are trained on a single RTX 3060 GPU card within three days.\n    \n  - _q3: \"I don\u2019t see any simple ways to extend the approaches to other vehicle routing problems. The authors may explain a bit how the data augmentation and neural network are to be applied to CVRP.\"_\n\n    Please check (3) in Point c) of our general response"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658539753,
                "cdate": 1700658539753,
                "tmdate": 1700658539753,
                "mdate": 1700658539753,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]