[
    {
        "title": "Hyper Evidential Deep Learning to Quantify Composite Classification Uncertainty"
    },
    {
        "review": {
            "id": "MPiDNh7xBs",
            "forum": "A7t7z6g6tM",
            "replyto": "A7t7z6g6tM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6833/Reviewer_3vp1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6833/Reviewer_3vp1"
            ],
            "content": {
                "summary": {
                    "value": "The DNN\u2019s uncertainty due to composite set labels (i.e., an example might be labeled as a set of possible classes, but only one class is true) in training data is considered. This work introduces a new type of uncertainty termed vagueness, and propose a framework Hyper-Evidential Neural Network (HENN) to quantify this type of uncertainty. Further, a loss named uncertainty partial cross entropy (UPCE) is proposed. Finally, the proposed method is shown to be effective on four image datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tA novel type of DNN uncertainty is introduced, and the uncertainty calibration is evaluated with the metric Jaccard Similarity.\n2.\tAn extension of the partial cross, uncertainty partial cross entropy (UPCE) is proposed."
                },
                "weaknesses": {
                    "value": "1.\tAlthough the newly introduced uncertainty concept and problem setup sound interesting, I am not sure the empirical evaluation is realistic and convincing enough. Are all the composite labels used in the empirical evaluations synthetically generated? Could you provide more concrete examples in these synthetic datasets? I am curious how realistic these generated composite labels are. If so, is it possible to run evaluations on real-world composite set labels?"
                },
                "questions": {
                    "value": "Please see the comments in the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6833/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698552664689,
            "cdate": 1698552664689,
            "tmdate": 1699636790966,
            "mdate": 1699636790966,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "frsWbNXwcm",
                "forum": "A7t7z6g6tM",
                "replyto": "MPiDNh7xBs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6833/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6833/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3vp1"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback and for recognizing the novel aspects of our work. We appreciate the opportunity to address your concerns regarding the empirical evaluation of our framework. \n\n> Are all the composite labels used in the empirical evaluations synthetically generated? Could you provide more concrete examples in these synthetic datasets? I am curious how realistic these generated composite labels are.  is it possible to run evaluations on real-world composite set labels?\n\nThe composite labels used in our empirical evaluations were indeed synthetically generated. One concrete example could be found in Fig.1(right). A blurring image which is difficult to distinguish between {husk} and {wolf}. We admit that the datasets with Gaussian blurring are semi-synthetic. From a sizable pool of applicants, we selected 23 students from our department and tasked them with annotating images in the CIFAR10 dataset, categorizing each as either a singleton class or a composite set. This effort successfully resulted in a real-world dataset enriched with human-annotated singleton and composite labels. Our method, along with various baseline approaches, was applied to this dataset. The comparative results were in line with those obtained from the synthetic datasets. Additionally, we plan to publicly release the dataset and the labels.  The following table results are also represented in the **Table 15 in the rebuttal revision**. We also show AUROC curves in **Figure 7 in the revision**, which indicates that vagueness is still the best among different uncertainties to distinguish composite examples from singleton examples.\n\nBackbone: ResNet18\n\n| Method | OverJS | CompJS | Acc\n| ---------| ---------| ---------| ---------|\n|DNN|79.73\u00b10.33|40.10\u00b17.06|82.17\u00b10.54|\n|ENN|67.09\u00b10.75|46.80\u00b10.06|82.75\u00b10.19|\n|E-CNN|59.68\u00b10.62|31.84\u00b10.81|66.23\u00b11.47|\n|RAPS|62.60\u00b10.46|33.80\u00b14.86|82.17\u00b10.54|\n|HENN (ours)|**80.74**\u00b10.17| **51.44**\u00b11.02| **83.03**\u00b10.14|\n\n\nBackbone: EfficientNet-b3\n| Method | OverJS | CompJS | Acc\n| ---------| ---------| ---------| ---------|\n|DNN|92.53\u00b10.11| 53.59\u00b13.15| 96.49\u00b10.21|\n|ENN|77.84\u00b13.86| 54.83\u00b10.59| 96.82\u00b10.38|\n|E-CNN|63.65\u00b10.93| 34.74\u00b12.91| 68.98\u00b10.72|\n|RAPS|65.70\u00b10.80 | 39.40\u00b12.29 | 96.49\u00b10.21|\n|HENN (ours)|**93.38**\u00b10.06| **72.87**\u00b11.25| **97.52**\u00b10.04|\n\n\n### Please kindly let us know if you have any concerns you find not fully addressed. We are more than happy to have a further discussion regarding it. Thank you so much for your time!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6833/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739757785,
                "cdate": 1700739757785,
                "tmdate": 1700742487249,
                "mdate": 1700742487249,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v21uhCfSap",
            "forum": "A7t7z6g6tM",
            "replyto": "A7t7z6g6tM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6833/Reviewer_AQu8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6833/Reviewer_AQu8"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel framework for deep learning to deal with composite labels, i.e. labels that couldn't be assigned to a single class due to quality of the input, for example, and were assigned to more than one class. The framework assumes a neural network to output, similarly to the training labels, both singleton classes and composite labels. The paper also proposes a novel uncertainty metric, called vagueness, that is measuring uncertainty related to composite label evidence in training. To train a novel neural network with composite output, the paper proposes a novel loss function."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* A very thorough work has been done with the new framework (and basically the new problem setup). Extensive theoretical evaluation, empirical evaluation including ablation study, a good number of baselines considered for comparison. \n* In addition to the new framework itself, the novel uncertainty metric is made as a proper contribution on its own with the thorough theoretical and empirical comparison with other uncertainty metrics. \n* The paper is mostly well-written and presents the context of prior work. The only drawbacks I can see are obviously due to the lack of space.\n\nOriginality: I am not familiar well with the related works, but both the framework and the uncertainty metric appears to be novel. \n\nQuality: Very well done and thought through piece of work. See above.\n\nClarity: Mostly well-written and easy to follow. \n\nSignificance: Empirical evaluation demonstrates that even in terms of single class classification (i.e. output of a NN is a single class) the proposed framework demonstrates improved accuracy in comparison to all considered baselines. Moreover, the proposed uncertainty metric demonstrates significantly superior performance in terms of distinguishing between composite and single labels examples."
                },
                "weaknesses": {
                    "value": "The main weakness that I see in the paper is the lack of motivation how severe is the problem of having composite labels in practice for training and why we need a NN to output us a composite label in practice. The main conceptual motivation that I gathered from the paper is that the new framework allows to estimate uncertainty related to composite labels in training data. However, the argument is not too convincing. It seems that we don't have to produce a composite label in order to estimate the effect of composite labels in the training data (not that I know how to do it without). \nEmpirical evidence shows us the practical motivation of using the novel framework: it gives us the higher accuracy when we consider a single class prediction. However, again this is shown in the experiments with the composite labels in training, which is not well motivated how often in practice we would have these labels for training. \nTo this end, I think a small experiment, showing that the proposed framework still works with traditional setup of single labels for training only, would be beneficial. \n\nOriginality: I appreciate that the considered problem formulation is different in that a single class is expected to be true for each sample, but maybe lacking due to the quality of the sample, but still I find it is a related area. The area of research devoted to multiclass classification is missing in the related works (e.g., Augustin, A.; Venanzi, M.; Hare, J.; Rogers, A.; and Jennings, N. 2017. Bayesian aggregation of categorical distributions with applications in crowdsourcing. AAAI Press/International Joint Conferences on Artificial Intelligence, but there are lots of others).\n\nQuality: The code is provided which should elevate this weakness, but based on the text not all implementation details are provided sufficiently to reproduce the results. See details below. \nDifferent types of image corruption would be interesting to see in the experiments in addition to the Gaussian blur. \n\nClarity: A lot of theory is packed in a very small space with lack of illustrative examples. See details below. \n\nSignificance: As per the main drawback mentioned above, I am not sure about the significance of the paper due to very specific problem setup, which was not too convincing for me. \n\nSpecific comments/suggestions:\n1. First two sentence of the text are unclear how to connect to the rest of the text. The problem considered is not related to missing data but rather ambiguous data due to the quality of the input. \n2. Missing reference in third paragraph of Section 1. \n3. Section 3. Illustrative examples of what this means in practice would be much appreciated. Note that example from Table 1 is not sufficient. For example, what does evidence 24 mean in practice? 24 annotators voted for this category? \n4. Figure 1 right is not referred to in the text. \n5. UCE (page 6) is not defined.\n6. Propositions 1 and 2. How reasonable is the assumption of universal approximation property? \n7. Eq. 15 appears without any introduction, connection to the previous text. \n8. Preprocessing of the dataset to create composite labels is unclear (including the text in the appendix). \n* \"Several random subclasses for each selected superclass will be chosen\" - for what? How these subclasses are used further?\n* Which images are selected to be blurred? \n9. How CompJS is computed for baselines not producing composite output is unclear until explanation of what cutoff is in Appendix. Also mentioning of cutoffs in the main text is unclear until this explanation in Appendix (no reference in the main text). \n10. It is unclear exactly how superclasses are extracted for Living17 and Nonliving26 datasets.\n11. The full list of data augmentations is required for reproducibility.\n12. Up until the last paragraph in page 26 in Appendix, it is not clear how the process of duplicating samples with composite labels is working (there is no reference to this paragraph in the earlier text). \n13. Section F.4 There are SinglJS, SingleAcc and Acc mentioned in different places. What is the correct one?\n \n\nMinor:\n1. Page 7. The paragraph before Section 5. \"As a generalized framework of ENN, The HENN\" -> \"the\"\n2. Section F.4 title. \"regularier\" -> \"regulazier\""
                },
                "questions": {
                    "value": "My main question to the authors, the answer to which hopefully will clarify any doubts from my side, is the question of motivation of the problem setup. I.e. how important is the problem of having composite labels during training and why do we need to also output composite labels by a NN?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6833/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698635344221,
            "cdate": 1698635344221,
            "tmdate": 1699636790800,
            "mdate": 1699636790800,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qVxdQLVEjh",
                "forum": "A7t7z6g6tM",
                "replyto": "v21uhCfSap",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6833/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6833/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AQu8 [1/4]"
                    },
                    "comment": {
                        "value": "We appreciate your thorough review of our paper and the opportunity to address your concerns, particularly regarding the motivation and practical implications of our work.\n\n> The question of motivation of the problem setup. I.e. how important is the problem of having composite labels during training and why do we need to also output composite labels by a NN?\n\nWe added **two more examples in the first paragraph of the Introduction section** to motivate the problem setup. \n\n\u201cIn various applications, particularly those dependent on data from low-quality sensors or high-quality data with insufficiently distinct features to separate some individual classes, the resulting data often exhibits significant vagueness and ambiguity (Allison, 2001; Ng et al., 2011). For example, in security surveillance, grainy images from store cameras may not provide clear enough resolution to accurately distinguish between different individuals or activities, necessitating the use of composite class labels to address this uncertainty (Allison, 2001). Similarly, in the field of medical imaging, a radiograph displaying features suggestive of multiple possible diagnoses may require composite labels to capture this uncertainty (Allison, 2001) effectively.\u201d\n\n> The area of research devoted to multiclass classification is missing in the related works (e.g., Augustin, A.; Venanzi, M.; Hare, J.; Rogers, A.; and Jennings, N. 2017. Bayesian aggregation of categorical distributions with applications in crowdsourcing. AAAI Press/International Joint Conferences on Artificial Intelligence, but there are lots of others).\n\nThe AAAI paper by Augustin et al. considers the problem of aggregating judgments (labels) of proportions from crowdsourcing annotators that are skewed and may provide judgments randomly (i.e., they are spammers). This is different from our work. We are concerned with images collected in challenging environments where weather, poor lighting, smoke, fires, poor focusing, etc., lead to less-than-ideal images. Clean images lead to precise annotations, and degraded images lead to vague annotations.  We expect the classifier to make the best classification effort and provide a composite label when the image is degraded to the point that a human cannot discern a singleton. In short, we are concerned with composite and precise labels for possibly poor-quality images. The classifier needs to make a best-effort prediction for the degraded images as opposed to a bad singleton prediction.    \n\n> the experiments with the composite labels in training, which is not well motivated how often in practice we would have these labels for training. To this end, I think a small experiment, showing that the proposed framework still works with traditional setup of single labels for training only, would be beneficial. \n\nWe added extra experiments on singleton class only datasets and evaluated the performance on traditional classification tasks with Accuracy for ENN and our HENN. Please refer to the result shown **in Appendix F6.4. Tab.18** presents the accuracy results from the ENN and HENN methods trained and evaluated on CIFAR100 with only singleton class data (without Gaussian blurring and label replacement) across 5 trials each. The mean accuracy and the standard deviation are reported. Under a traditional singleton classification setting, HENN still shows comparable performance in terms of accuracy compared to ENN model. A notable advantage of HENN is its ability to quantify an additional type of uncertainty compared to ENN with minimal performance degradation observed even if the training data consists of exclusive singleton ones.\n\n|Method | ENN | HENN |\n|----------|------|--------|\n|Acc(%)|85.82 \u00b1 1.0| 85.81 \u00b1 2.4|"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6833/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737931724,
                "cdate": 1700737931724,
                "tmdate": 1700742569987,
                "mdate": 1700742569987,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2wn4oQSnr3",
                "forum": "A7t7z6g6tM",
                "replyto": "v21uhCfSap",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6833/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6833/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AQu8 [4/4]"
                    },
                    "comment": {
                        "value": "> Specific Q10: It is unclear exactly how superclasses are extracted for Living17 and Nonliving26 datasets.\n\nThe extraction of superclass and class hierarchy for these two datsaets are explored by Santurkar et al. (2021). We **add one sentence in the Datasets&Preprocessing (Section 5.1)**:\n\n\u201cTheir superclasses and hierarchy information have been extracted by Santurkar et al. (2021) based on visual similarities.\u201d\n\nWe also introduce **more detail in Appendix E.1** to explain why the Living17 and Nonliving26 are considered:\n\n\"Additionally, distinguishing between different classes can often be challenging due to their similar visual features. Unlike WordNet, which organizes its hierarchy based on semantic relationships between words, the class hierarchies of Living17 and Nonliving26 are constructed considering both visual and semantic similarities. These two subsets, part of the broader ImageNet dataset (Deng et al., 2009), consist of images with a resolution of 224\u00d7224 pixels. For detailed information about these subsets, please refer to Tables 5 and 6 in the original paper.\"\n\n> Specific Q11: The full list of data augmentations is required for reproducibility.\n\nData augmentation has been included **in Appendix E.2** actually. To make this more clear, we add more sentences to explain what data augmentation we used at the end of Appendix E.2:\n\n\u201cWe used 2 methods for data augmentation following a typical computer vision setting. First, each image is applied to a random horizontal flip with the flipping probability of 0.5. After that, a random corp is introduced for each image with a size of 32\u00d732 and padding of 4.\u201d\n\n> Specific Q12: Up until the last paragraph in page 26 in Appendix, it is not clear how the process of duplicating samples with composite labels is working (there is no reference to this paragraph in the earlier text).\n\nWe add detailed process to explain this in **Appendix E.3 Implementation**:\n\nDNN and ENN are typically designed to process examples with singleton labels and cannot deal with composite class labels during training.  Given that our training dataset includes vague images with composite class labels, we have devised a strategy to enable these baseline models to handle such examples without eliminating training data. This involves duplicating composite examples and assigning them singleton labels derived from their composite set labels, ensuring that all class labels during training remain exclusive. For instance, if there is an image x with a composite label A, B during training, we create two duplicates of x \u2013 one labeled as singleton A and the other as singleton B \u2013 and use these as inputs for model training.\n\n> Specific Q13: Section F.4 There are SinglJS, SingleAcc and Acc mentioned in different places. What is the correct one?\n\nWe united these terms to Acc in F.4. Basically it means the top-1 accuracy for singleton label prediction.\n\n\n> Quality: The code is provided which should elevate this weakness, but based on the text not all implementation details are provided sufficiently to reproduce the results. See details below. Different types of image corruption would be interesting to see in the experiments, in addition to the Gaussian blur.\n\nWe have added more detail which is necessary to reproduce the results according to the above questions. In addition, we also wrote a README file in our code repo to explain how to use our code.\n\nFor different type of image corruption in addition to Gaussian blur, use another data corruption method: bicubic interpolation, which is popular in the super-resolution research community. For example, in paper [1], the low resolution (LR) input is generated based on the high resolution images. Also, the same method is also used in paper [2]: \u201cThe LR counterparts are downsampled using bicubic interpolation\u201d. The first column is LR image after Bicubic interpolation in  the following figure (figure 6 from paper [1]):\n\nwe conduct experiments on Living17dataset. The results are shown below and also represented in **Appendix Section F.6.3**. It indicates the consistence with results using Gaussian Blur.\n\n| Method | OverJS | CompJS | Acc\n| ---------| ---------| ---------| ---------|\n|DNN|87.28\u00b10.23| 74.61\u00b12.57| 84.35\u00b10.36|\n|ENN|87.46\u00b10.34 | 69.44\u00b13.25 | 85.38\u00b10.28|\n|RAPS|85.38\u00b10.32 | 62.10\u00b10.26 | 84.35\u00b10.36|\n|HENN (ours)| **88.09**\u00b10.21 | **96.33**\u00b13.54 | **86.12**\u00b10.37|\n\n**Reference:**\n\n- [1] Image Super-Resolution via Iterative Refinement, TPAMI 2022\n- [2] Super-Resolution Neural Operator, CVPR 2023\n\n> Other Minor issues\n\nWe have updated them.\n\n### Please kindly let us know if you have any concerns you find not fully addressed. We are more than happy to have a further discussion regarding it. Thank you so much for your time!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6833/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739527990,
                "cdate": 1700739527990,
                "tmdate": 1700742739686,
                "mdate": 1700742739686,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Y5Gwf9XJKC",
            "forum": "A7t7z6g6tM",
            "replyto": "A7t7z6g6tM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6833/Reviewer_Gzpb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6833/Reviewer_Gzpb"
            ],
            "content": {
                "summary": {
                    "value": "A hyper-evidential neural network is presented in this paper, for classification of data, modelling predictive uncertainty based on training data with composite set labels. The uncertainty is measured by introducing the vagueness type of measure. Results are presented where HENN performance is compared favorably with other methods over four image datasets. Detailed analysis is included in Appendices."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper deals with a significant problem, i.e., to train DNNs when (some) training data have composite labels, being able to predict these labels and quantify the predictive uncertainty due to these labels. It defines a related measure, vagueness, to do so and extends neural network structures to model this uncertainty for classification problems; it uses an uncertainty partial cross entropy loss function extending the normal UCE function.  An experimental study is presented which illustrates a good performance over four image datasets, when compared to five other methodologies that can be applied in this context."
                },
                "weaknesses": {
                    "value": "The paper defines vagueness as a measure of the predictive uncertainty due to composite labels of training data. It then uses gaussian blurring in the experiments to create such data cases and perform the experimental verification. However, this is a rather specific synthetically generated experiment, which can not justify the significance of the results over real world applications. Such applications could, for example, include classification of facial images showing compound and primary emotions in-the-wild (where compound emotions two or more primary ones), or in image2image translation tasks. Moreover, the comparison shown in Table 2 does not seem fair, since - as also discussed in the 'Classification' results subsection (in 5.2) - the other methods are not designed to handle this type of vague images."
                },
                "questions": {
                    "value": "Following the above, would it be possible to apply the method in real world applications involving composite labels? \nMoreover, performance seems to heavily depend on regularization hyperparameter, which is selected in an ad-hoc manner. Is this a pitfall for method's robustness over different datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6833/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698741568380,
            "cdate": 1698741568380,
            "tmdate": 1699636790666,
            "mdate": 1699636790666,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B9rReLG0G0",
                "forum": "A7t7z6g6tM",
                "replyto": "Y5Gwf9XJKC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6833/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6833/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Gzpb [1/2]"
                    },
                    "comment": {
                        "value": "We thank you for your insightful review and the opportunity to address the concerns raised regarding our work on our work!\n\n> The dataset used in the paper is a rather specific synthetically generated experiment, which can not justify the significance of the results over real-world applications. would it be possible to apply the method in real-world applications involving composite labels?\n\nWe employed synthetic datasets generated through Gaussian blurring (as per Richard Webster et al. 2018), due to the absence of public benchmarking datasets with composite set labels from annotators. Annotators of commonly used benchmarking datasets, such as CIFAR100 and TinyImageNet, are typically restricted to providing only singleton class labels, lacking the option for composite class labeling. As a result, these datasets do not include any composite class labels. Gaussian blurring is commonly used to simulate images collected from low-quality sensors (e.g., security and surveillance cameras) (add references) that human annotators will more likely label as composite sets of classes, as these images lack fine details annotators can spot to label singleton classes.\nHowever, we admit that the datasets with Gaussian blurring are semi-synthetic. From a sizable pool of applicants, we selected 23 students from our department and tasked them with annotating images in the CIFAR10 dataset, categorizing each as either a singleton class or a composite set. This effort successfully resulted in a real-world dataset enriched with human-annotated singleton and composite labels. Our method, along with various baseline approaches, was applied to this dataset. The comparative results were in line with those obtained from the synthetic datasets. Additionally, we plan to publicly release the dataset and the labels. The following table results are also represented in the **Table 15 in the rebuttal revision**. We also show AUROC curves in **Figure 7 in the revision**, which indicates that vagueness is still the best among different uncertainties to distinguish composite examples from singleton examples.\n\n\n\nBackbone: ResNet18\n\n| Method | OverJS | CompJS | Acc\n| ---------| ---------| ---------| ---------|\n|DNN|79.73\u00b10.33|40.10\u00b17.06|82.17\u00b10.54|\n|ENN|67.09\u00b10.75|46.80\u00b10.06|82.75\u00b10.19|\n|E-CNN|59.68\u00b10.62|31.84\u00b10.81|66.23\u00b11.47|\n|RAPS|62.60\u00b10.46|33.80\u00b14.86|82.17\u00b10.54|\n|HENN (ours)|**80.74**\u00b10.17| **51.44**\u00b11.02| **83.03**\u00b10.14|\n\n\nBackbone: EfficientNet-b3\n\n| Method | OverJS | CompJS | Acc\n| ---------| ---------| ---------| ---------|\n|DNN|92.53\u00b10.11| 53.59\u00b13.15| 96.49\u00b10.21|\n|ENN|77.84\u00b13.86| 54.83\u00b10.59| 96.82\u00b10.38|\n|E-CNN|63.65\u00b10.93| 34.74\u00b12.91| 68.98\u00b10.72|\n|RAPS|65.70\u00b10.80 | 39.40\u00b12.29 | 96.49\u00b10.21|\n|HENN (ours)|**93.38**\u00b10.06| **72.87**\u00b11.25| **97.52**\u00b10.04|\n\n\n**References**: \nRichardWebster, Brandon, Samuel E. Anthony, and Walter J. Scheirer. \"Psyphy: A psychophysics driven evaluation framework for visual recognition.\" IEEE transactions on pattern analysis and machine intelligence 41.9 (2018): 2280-2286."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6833/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736900247,
                "cdate": 1700736900247,
                "tmdate": 1700742239514,
                "mdate": 1700742239514,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UaR72L9sqN",
                "forum": "A7t7z6g6tM",
                "replyto": "Y5Gwf9XJKC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6833/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6833/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Gzpb [2/2]"
                    },
                    "comment": {
                        "value": "> Moreover, performance seems to heavily depend on regularization hyperparameter, which is selected in an ad-hoc manner. Is this a pitfall for method's robustness over different datasets?\n\nWe proposed a KL-based regularization term in Equation (14) to address the limitations of the UPCE loss function analyzed in our Propositions 1 and 2, discussed in Section 4.1. The best hyperparameter ($\\lambda$) of this regularization term was selected based on a grid search on the validation set for each dataset, a standard procedure for hyperparameter selection. We observed consistent performance results across our datasets, so our method is robust over different datasets. We also analyzed other variants of the regularization term, and the results are reported in **Table 3 shown in Section 5.2**. The results demonstrate that the HENN learned based on the KL-based regularization term or its variants consistently outperforms other baselines in different settings, indicating its robustness. \n\nWe note that the traditional ENN incorporates a different KL regularization term primarily to increase the vacuity for samples likely to be misclassified. This term can be considered a special instance of our proposed  KL-based regularization term in Equation (14) when the prediction is Dirichlet distribution for singleton classes instead of hyperDirichlet distribution for singleton class or composite set labels. We will discuss this relation in our revised version. To summarize, our proposed HENN does not have more hyperparameters than ENN, but the regularization term is designed differently to address the limitations of UPCE for hyper-dirichlet predictions.\n\n### Please kindly let us know if you have any concerns you find not fully addressed. We are more than happy to have a further discussion regarding it. Thank you so much for your time!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6833/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737122985,
                "cdate": 1700737122985,
                "tmdate": 1700742801841,
                "mdate": 1700742801841,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oyH6N2bVVZ",
            "forum": "A7t7z6g6tM",
            "replyto": "A7t7z6g6tM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6833/Reviewer_RL9p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6833/Reviewer_RL9p"
            ],
            "content": {
                "summary": {
                    "value": "The paper suggests a variant of Dempster-Shafer Theory to arrive at decisions that allow for uncertainty quantification for composite classification. In the proposed HENN framework the uncertainty from composite annotations during training is leveraged for quantification of classification uncertainty. Experiments on image datasets are used to demonstrate the effectiveness of HENN which the authors support by a theoretical analysis as well."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Undoubtedly, annotation uncertainty is a big challenge in today's machine-learning models. Thus, this paper tackles a highly relevant topic. In addition, uncertainty in the output of DNN models is the focus of many papers, e.g. to find the right calibration of the values to allow interpretation and decision-making. The authors suggest DST for solving this issue, which I did not see before for DNN. If the theoretical analysis (Sec. 4.1) can be confirmed to be correct and relevant in practice (which I could not!), I would see a relevant contribution by this work, at least to inspire others to look into hyper-opinions."
                },
                "weaknesses": {
                    "value": "I see two significant shortcomings:\n1. composite classification is not new, and solutions exist in a different context and with different methods (i.e. no DST). See, for example, Brust et al.: Making Every Label Count: Handling Semantic Imprecision by Integrating Domain Knowledge. ICPR 2020. The authors did not make clear what the advantage of their framework is compared to such work. This referenced work might differ, but the overall modelling procedures look similar to the one in the submission: annotations are at a different level of a concept hierarchy (dog->husky, dog->wolf, might generate label \"dog\" if uncertain, or \"husky/wolf\" if certain). \n2. the selected datasets are not suitable to demonstrate a (at least for me) complex theory behind uncertainty and how to include it into deep learning loss functions and regularization. For example, I am not sure what Gaussian blurring will make with tiny-images, i.e. what remains as information after blurring. \n\nI am also not happy with the theoretical part of the paper, although I have to admit that I am not that familiar with DST, and this hindered me from diving deeper into the derivations done in Sec. 4.1 The notation and style of presentation is probably only proper for an expert in this area."
                },
                "questions": {
                    "value": "I have only two questions:\n- do you see any relation to Brust et al.: Making Every Label Count: Handling Semantic Imprecision by Integrating Domain Knowledge. ICPR 2020 and which concepts of this work is related to yours? \n- did you perform experiments on more relevant benchmark datasets, like NABirds (https://dl.allaboutbirds.org/nabirds). For this dataset, hierarchies exists and benchmarks in fine-grained recognition which most likely would benefit most from your suggested idea."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6833/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827494667,
            "cdate": 1698827494667,
            "tmdate": 1699636790506,
            "mdate": 1699636790506,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ubCfyJ7GZF",
                "forum": "A7t7z6g6tM",
                "replyto": "oyH6N2bVVZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6833/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6833/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RL9p [1/4]"
                    },
                    "comment": {
                        "value": "We appreciate your insightful feedback and the opportunity to clarify aspects of our work. Your comments have given us a chance to articulate the novelty and strengths of our approach more clearly.\n\n> Do you see any relation to Brust et al.: Making Every Label Count: Handling Semantic Imprecision by Integrating Domain Knowledge. ICPR 2020 and which concepts of this work is related to yours?\n\nThe problem setting of the work by Brust et al. is stated in Section III of the ICPR 2020 paper: \u201cWe require the classifier to predict only precise labels from $Y$. At the same time, it needs to be able to learn from training data with labels from both $Y$ and $Y^+$, which we refer to as semantically imprecise data\u201d. Here, the authors refer to \u201cprecise labels\u201d as singleton labels. The Experimental Section V only evaluates the accuracies of the proposed method, named CHILLAX, and baselines on this problem setting. It is unclear if this method can be effectively adapted to predict composite set labels.   \n\nIn our proposed work, we are training a classifier to perform on par with human annotators where the training data is annotated by humans, so there is no ground truth.  With that said, we expect to be concerned with images collected in challenging environments where weather, poor lighting, smoke, fires, poor focusing, etc., lead to less-than-ideal images. Clean images lead to precise annotations, and degraded images lead to vague annotations.  We expect the classifier to make the best classification effort and provide a composite label when the image is degraded to the point that a human cannot discern a singleton. In short, we are concerned with composite and precise labels for possibly poor-quality images. The classifier needs to make a best-effort prediction for the degraded images as opposed to a bad singleton prediction. This differs from CHILLAX, which assumes pristine images, but subsets of annotators do not have the expertise to provide singleton labels.\n\nBased on the above motivation, our proposed HENN model is designed to quantify a new type of uncertainty, called 'vagueness', for each singleton or composite prediction. This vagueness measure refers to the degree of predictive uncertainty caused by evidence in training samples with composite labels. This offers useful insights for decision-making in safety-critical applications, such as medical disease classifications. \n\nTo explain vagueness, suppose we have two singleton classes: 'cat' and 'dog'. Suppose the evidence supporting a prediction for an input image includes 6 training images labeled as 'cat' (a singleton class) and 4 images labeled as the composite set {cat, dog}. HENN compares these quantities (6 > 4) and predicts the singleton class {cat}. However, it also assigns a non-zero vagueness score 0.33, reflecting the presence of evidence from the training samples with composite labels. Let \u201cc\u201d and \u201cd\u201d denote cat and dog, respectively. The evidence of cat $e_c = 6$ and $e_{c,d} = 4$. According to Equations (2) and (5), the vagueness $vag = b_{c,d} =4/(e_c + e_{c,d} + K) = 4/12=0.33$, where $K = 2$ refers to the number of singleton classes. This vagueness score is important as it informs the user that the prediction is partly supported by training samples with composite set labels (e.g., {dog, cat}) while the ground truth is a singleton class (e.g., {cat} or {dog})."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6833/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734642810,
                "cdate": 1700734642810,
                "tmdate": 1700734642810,
                "mdate": 1700734642810,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RHCk0ZeeKc",
                "forum": "A7t7z6g6tM",
                "replyto": "oyH6N2bVVZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6833/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6833/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RL9L [3/4]"
                    },
                    "comment": {
                        "value": "> did you perform experiments on more relevant benchmark datasets, like NABirds \n\nAs the NABirds dataset does not have composite class labels from annotators, similar to the datasets that we used, we degraded the images in NABirds synthetically based on Gaussian blurring in the same way used in our other synthetic datasets and compared our proposed HENN and other baselines. The results are shown below. It indicates that \nHENN outperforms DNN and ENN for a large margin in terms of CompJS. And HENN also performs\nbetter in terms of OverJS and Acc, which is consistent with previous experiments on reported four datasets in the paper.\n\n| Method | OverJS | CompJS | Acc\n| ---------| ---------| ---------| ---------|\n| DNN|77.38\u00b10.19| 35.24\u00b13.52|78.04\u00b10.27|\n|ENN|76.72\u00b10.56|37.46\u00b12.39|78.45\u00b10.31|\n|HENN (ours)|**80.01**\u00b10.37|**71.42**\u00b11.43|**80.14**\u00b10.35|\n\nHowever, we admit that the datasets with Gaussian blurring are semi-synthetic. From a sizable pool of applicants, we selected 23 students from our department and tasked them with annotating images in the CIFAR10 dataset, categorizing each as either a singleton class or a composite set. This effort successfully resulted in a real-world dataset enriched with human-annotated singleton and composite labels. Our method, along with various baseline approaches, was applied to this dataset. The comparative results were in line with those obtained from the synthetic datasets. Additionally, we plan to publicly release the dataset and the labels. The following table results are also represented in the **Table 15 in the rebuttal revision**. We also show AUROC curves in **Figure 7 in the revision**, which indicates that vagueness is still the best among different uncertainties to distinguish composite examples from singleton examples.\n\nBackbone: ResNet18\n\n| Method | OverJS | CompJS | Acc\n| ---------| ---------| ---------| ---------|\n|DNN|79.73\u00b10.33|40.10\u00b17.06|82.17\u00b10.54|\n|ENN|67.09\u00b10.75|46.80\u00b10.06|82.75\u00b10.19|\n|E-CNN|59.68\u00b10.62|31.84\u00b10.81|66.23\u00b11.47|\n|RAPS|62.60\u00b10.46|33.80\u00b14.86|82.17\u00b10.54|\n|HENN (ours)|**80.74**\u00b10.17| **51.44**\u00b11.02| **83.03**\u00b10.14|\n\n\nBackbone: EfficientNet-b3\n| Method | OverJS | CompJS | Acc\n| ---------| ---------| ---------| ---------|\n|DNN|92.53\u00b10.11| 53.59\u00b13.15| 96.49\u00b10.21|\n|ENN|77.84\u00b13.86| 54.83\u00b10.59| 96.82\u00b10.38|\n|E-CNN|63.65\u00b10.93| 34.74\u00b12.91| 68.98\u00b10.72|\n|RAPS|65.70\u00b10.80 | 39.40\u00b12.29 | 96.49\u00b10.21|\n|HENN (ours)|**93.38**\u00b10.06| **72.87**\u00b11.25| **97.52**\u00b10.04|"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6833/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736360354,
                "cdate": 1700736360354,
                "tmdate": 1700742225659,
                "mdate": 1700742225659,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lMVqXcjqDH",
                "forum": "A7t7z6g6tM",
                "replyto": "oyH6N2bVVZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6833/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6833/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RL9L [4/4]"
                    },
                    "comment": {
                        "value": "> the selected datasets are not suitable to demonstrate a (at least for me) complex theory behind uncertainty and how to include it into deep learning loss functions and regularization. For example, I am not sure what Gaussian blurring will make with tiny-images, i.e. what remains as information after blurring.\n\nGaussian blurring is commonly used to simulate images collected from low-quality sensors (e.g., security and surveillance cameras) (Richard Webster et al. 2018) that human annotators will more likely label as composite sets of classes, as these images lack fine details annotators can spot to label singleton classes. Although the blurred images may not have edges and fine details, the annotators can still rule out some classes based on the shapes and other features of the objects within the images (e.g., horses can be easily distinguished from cats and dogs in their traits).  \n\nWe have generated synthetic datasets based on four base datasets, including CIFAR100, TinyImagenet, living-17 and non-living-26. Now all the datasets are tiny images. The first two datasets are small in image sizes (32 x 32). The images in living-17 and non-living-26 are greater than 224 x 224, but we rescaled them to 224 x 224. \n\nAs detailed in our response to the preceding item 2, we also conducted the empirical comparison on a real-world dataset CIFAR10 enriched with human-annotated singleton and composite labels. The results are consistent with the results on the five synthetic datasets. \n\n**References**: \nRichardWebster, Brandon, Samuel E. Anthony, and Walter J. Scheirer. \"Psyphy: A psychophysics driven evaluation framework for visual recognition.\" IEEE transactions on pattern analysis and machine intelligence 41.9 (2018): 2280-2286.\n\n> The authors suggest DST for solving this issue, which I did not see before for DNN. If the theoretical analysis (Sec. 4.1) can be confirmed to be correct and relevant in practice (which I could not!), I would see a relevant contribution by this work, at least to inspire others to look into hyper-opinions. \n\nIn essence, our proposed HENN is the GDD extension of evidential deep learning (Ulmer et al. (2023)) (which was based upon Dirichlet distributions). As demonstrated in the detailed proofs in Sections C.2 and C.3 in Appendix, the two propositions are correct and demonstrate the need for the KL term so that only the evidence for the ground truth class tends to infinity. The issues of the UPCE identified by our two proposition 1 are also empirically verified by **our case study on CIFAR100 in Appendix F.7**: (1) the HENN learned based on UPCE and a training set consisting of only singleton class labels predicts non-zero evidence on composite set labels for 14.4% of the training samples even that the training set does not have evidence of composite class labels to accumulate, and (2) the HENN learned based on UPCE and a training set consisting of only composite class labels predicts non-zero evidence on singleton class labels for 100.0% of the training samples even that the training set does not have evidence of singleton class labels to accumulate. Our proposed regularization can avoid these unexpected behaviors. \n\n**References**: \nDennis Thomas Ulmer, Christian Hardmeier, and Jes Frellsen. Prior and posterior networks: A survey on evidential deep learning methods for uncertainty estimation. Transactions on Machine Learning Research, 2023.\n\n> I am also not happy with the theoretical part of the paper, although I have to admit that I am not that familiar with DST, and this hindered me from diving deeper into the derivations done in Sec. 4.1 The notation and style of presentation is probably only proper for an expert in this area.\n\nIn our updated submission, we revised Section 4.1 to explain the proposed regularization term in more detail. We added a paragraph at the end of this section to discuss the motivations and limitations of our theoretical analysis. An ablation study is discussed at the end of Section5.2 to empirically demonstrate the need for the regularization term. We also added more explanations in the proofs of the propositions in Appendix C.2 to make the proofs more self-contained.\n\n### Please kindly let us know if you have any concerns you find not fully addressed. We are more than happy to have a further discussion regarding it. Thank you so much for your time!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6833/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736624866,
                "cdate": 1700736624866,
                "tmdate": 1700742744261,
                "mdate": 1700742744261,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]