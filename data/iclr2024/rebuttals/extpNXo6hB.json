[
    {
        "title": "SweetDreamer: Aligning Geometric Priors in 2D diffusion for Consistent Text-to-3D"
    },
    {
        "review": {
            "id": "2jhKYpeIhe",
            "forum": "extpNXo6hB",
            "replyto": "extpNXo6hB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3384/Reviewer_UgSj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3384/Reviewer_UgSj"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the author present SweetDreamer, a text-to-3D generation model. They address the limitations of existing 2D diffusion models by aligning the 2D geometric priors with well-defined 3D shapes from a 3D dataset. To resolve the multi-view inconsistency, SweetDreamer incorporates fine-tuning a pre-trained diffusion model to produce view-specific coordinate maps of canonically oriented 3D objects. Then these aligned geometric priors are integrated into different text-to-image pipelines to generate geometry and appearance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problem addressed in this paper is a realistic problem that current diffusion models struggle to handle effectively.\n\n2. The incorporation of 3D geometric priors into the generation process is a compelling and intriguing approach."
                },
                "weaknesses": {
                    "value": "1. The novelty of this paper is kind of incremental. Most components (such as Stable Diffusion, loss functions) of the proposed method come from previous works, lacking technical novelty.\n\n2. The quantitative results are not satisfying. The evaluation scores reported in Table 1 only show the performance of inconsistency. Did the author compare with other methods by use of other metrics (e.g., R-Precision in DreamFusion)? \n\n3. A simple baseline is missing. Have the authors considered directly replacing the original SDS loss with the aligned geometric SDS loss and calculated the inconsistencies in Table 1?\n\n4. The ablation discussion is absent."
                },
                "questions": {
                    "value": "Please see Weaknesses.\n\nOther questions:\n\n1. How long does the fine-tuning for a stable diffusion model take?\n2. How does the efficiency (inference time) of this model compare to other methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3384/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3384/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3384/Reviewer_UgSj"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3384/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698642069709,
            "cdate": 1698642069709,
            "tmdate": 1699636289226,
            "mdate": 1699636289226,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "InXbZRaMb3",
                "forum": "extpNXo6hB",
                "replyto": "2jhKYpeIhe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3384/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3384/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q: Novelty**\n\nA: Please refer to the common response for our reply.\n\n**Q: A simple baseline (AGP-only) is missing.**\n\nA: Thanks for pointing this out. When generating the 3D results shown in the submission, AGP has actually dominated the geometry modeling in both NeRF- and DMTet-based pipelines, as we set a large weight  (5x \\lambda_ori) for \\lambda_align at the beginning of the optimization. As demonstrated in Figure 9 of the revised version, the overall shape generated by AGP-only is almost the same as the AGP + DeepFolyed IF SDS, except for minor details. As a result, the qualitative results are the same as the Ours NeRF-based IF. Please refer to Figure 9 in the revision paper for further visual results at each stage in the generation pipeline.\n\n**Q: The metric of R-Precision.**\n\nA\uff1a We thank the reviewer for this insightful suggestion.  For more discussion, please refer to the feedback of Reviewer nptq. During the rebuttal, we obtained the numerical results of appearance-related metrics. We report the R-precision as your suggestion. The results are presented below and in Section A.7 and Table 3 in the revised paper. \n\n\n**Q: The ablation discussion is absent.**\n\nA: Thanks for this suggestion. Please refer to the common response for more details on this point.\n\n\n**Q: How long does the fine-tuning for a stable diffusion model take?**\n\nA: Since we only use coarse geometries, i.e., low-resolution 64x64 CCM maps, to fine-tune the SD, the training process is highly efficient. The whole fine-tuning process takes about one day on 8 A100 GPUs for reasonably good results, and the final model used in the submission was trained for two days.\nWe added more details about finetune in the revision A.2.\n\n\n**Q: How does the efficiency (inference time) of this model compare to other methods?**\n\nA: Our AGP model is designed to be as lightweight as possible, with the goal of improving the consistency of generated results. The inference time of the fine-tuned SD model is almost identical to that of the original model. And the time for the whole generation depends on the base text-to-3D model, but our model has introduced almost no additional computational time overhead. Specifically, it takes 0.5 hours for our DMTet-based pipeline and 1 hour for our NeRF-based pipeline, using 4 V100 GPUs."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654081296,
                "cdate": 1700654081296,
                "tmdate": 1700654081296,
                "mdate": 1700654081296,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jQ6mdVRlRK",
                "forum": "extpNXo6hB",
                "replyto": "2jhKYpeIhe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3384/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3384/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Your feedback is greatly appreciated! We've strived to address your concerns in our response. As the deadline approaches, should further queries arise, we are fully prepared to answer. Additionally, we respectfully request your kind consideration regarding a potential adjustment in the rating. Your insights are invaluable to us. Thank you sincerely for your time and support!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714059736,
                "cdate": 1700714059736,
                "tmdate": 1700714059736,
                "mdate": 1700714059736,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LBLB7hvLdS",
            "forum": "extpNXo6hB",
            "replyto": "extpNXo6hB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3384/Reviewer_uDHT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3384/Reviewer_uDHT"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes SweetDreamer, a new solution to the multi-face Janus problem encountered in the text-to-3D generation pipeline. The core strategy revolves around integrating a Canonical Coordinates Map (CCM) SDS loss. SweetDreamer fin-tunes the Stable Diffusion model to predict Canonical Coordinates Maps (CCM) on the Objaverse dataset, offering Aligned Geometric Priors (AGP). Furthermore, the authors incorporated camera information into the diffusion model. Both quantitative and qualitative assessments show that SweetDreamer successfully mitigates the issues of geometry and appearance inconsistency."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of introducing view-conditioned information into 2D foundation models like Stable Diffusion to address the multi-face problem is sound. By fine-tuning the 2D diffusion model to predict the Canonical Coordinates Map, it gains an inherent understanding of the basic viewpoint of an object, thus providing additional aligned geometric priors for the text-to-3D pipeline. Additionally, the inclusion of camera information to speed up convergence is a reasonable choice.\n2. Human evaluation results indeed validate that SweetDreamer alleviates the multi-faced problem. Furthermore, a user study accentuates SweetDreamer's superiority over previous methods.\n3. The presentation is clear and easy to follow. The author gives a thoughtful analysis of the multi-face problem, including \u201cthe geometry inconsistency is the key problem compared with appearance inconsistency\u201d."
                },
                "weaknesses": {
                    "value": "1. There are no ablation studies in this paper. For example, the author claims that the SDS loss from the Canonical Coordinates Map can avoid overfitting the 3D training data. However, there are no experiments to support this idea. Moreover, all the design choices in this paper including how to inject the camera information into the diffusion model are not ablated. \n2. In fact, introducing a view-dependent SDS loss to tackle the multi-face issue has been proven in Zero-123 and Magic-123, which use a view-aware SDS loss to generate a 3D object. SweetDreamer uses a Canonical Coordinates Map SDS loss rather than RGB SDS loss, the novelty and technique contribution is limited.\n3. SweetDreamer uses Objaverse data to fine-tune the Stable Diffusion model to predict the Canonical Coordinates Map. However, defining a canonical coordinate system uniformly across diverse categories is challenging. The authors' choice to whittle down the Objaverse dataset from 800k to 270k, in effect, results in significant data attrition. When compared with techniques that fine-tune based on multi-view images, this could constrain the method's generalizability."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3384/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698776153135,
            "cdate": 1698776153135,
            "tmdate": 1699636289128,
            "mdate": 1699636289128,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zcovtSHYJm",
                "forum": "extpNXo6hB",
                "replyto": "LBLB7hvLdS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3384/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3384/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q: Ablation studies and the generalization ability of the proposed method.**\n\nA: Please refer to the \u201cMore ablation studies\u201d and \u201cTechnical novelty\u201d in the common response for the ablation studies and generalization ability,\nrespectively.\n\n\n**Q: Novelty.**\n\nA: Please refer to the \u201cTechnical novelty\u201d in the common response.\n\n\n**Q: Filtering the data and not using all may constrain the generalization ability.**\n\nA: Our filtering process mainly filters out noisy 3D data, such as low-poly shapes, point clouds, unbounded scenes, etc, using some simple and heuristic rules. In addition, we also filtered out some obviously wrongly oriented samples in certain categories (e.g., characters) based on the orientation of their Axis-Aligned Bounding Box (we missed this filtering in the submission and have revised accordingly). While we agree that the data can potential affect the fine-tuning process, we also believe that inherently the key is the diversity and quality of the data rather than simply the absolute amount of data. Moreover, we only use the coarse geometries in the dataset for aligning the diffusion priors at the coarse geometry level, such \u201cless is more\u201d way of using the data has demonstrated its efficacy in alleviating the multi-view inconsistency issues without compromising significantly the generalizability of powerful diffusion priors in terms of highly varied appearance and geometric details, as evidenced by extensive successful results in the submission.\n\nIn addition, recent works have also found that 2D diffusion models are already very powerful, and the results of lightweight fine-tuning using a small number of high-quality 3D data may even be better than training with a larger amount of data (such as Wonder3D and Instant3D). This is consistent with our experimental experience, and we have added more discussions on this in the revision (See Appendix A.5). \n\nLast, we have also added a discussion on the generalizability of our method in Appendix A.4. More results, that showcase the ability to generalize to highly varied objects unseen in the dataset, can also be found on this anonymous website. http://3dsweetdreamer.github.io/nerf-based-gallery_0.html."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654145509,
                "cdate": 1700654145509,
                "tmdate": 1700654145509,
                "mdate": 1700654145509,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8f1ZGhjMqD",
                "forum": "extpNXo6hB",
                "replyto": "LBLB7hvLdS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3384/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3384/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Your feedback is greatly appreciated! We've strived to address your concerns in our response. As the deadline approaches, should further queries arise, we are fully prepared to answer. Additionally, we respectfully request your kind consideration regarding a potential adjustment in the rating. Your insights are invaluable to us. Thank you sincerely for your time and support!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714020332,
                "cdate": 1700714020332,
                "tmdate": 1700714020332,
                "mdate": 1700714020332,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FYxaxqHtla",
            "forum": "extpNXo6hB",
            "replyto": "extpNXo6hB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3384/Reviewer_nptq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3384/Reviewer_nptq"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes aligned geometric priors (AGP) to address the multiview inconsistency problem in diffusion models. In detail, the method finetunes a pre-trained 2D diffusion model to generate viewpoint-specific coarse geometric maps of canonically oriented\n3D objects to ensure 3D awareness. Qualitative experiments and human evaluation results show great geometry consistency of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed aligned geometric priors (AGP) show effectiveness in improving multiview consistency.\n\n2. The qualitative results show good consistency compared with other methods.\n\n3. The paper is well-written and the core contributions are clear."
                },
                "weaknesses": {
                    "value": "1. Using Canonical Coordinates Map (CCM) in diffusion model to preserve 3D consistency is one of the core contributions of this paper. However, this coordinate map representation is not something new, and the advantage of using CCM instead of a depth map, normal map, or point cloud is not clear. Experiments to show the superiority of such representation are desired.\n\n2. The experiments have some weaknesses. Only 80 results from 80 text prompts are generated using each method. The results can be noisy based on the small amount of data.\n\n3. The definition of \"3D consistency\" in the quantitative experiment is vague (\"e.g., multiple heads, hands, or legs\"). For example, when the generated object is blurred, or does not have the concept of \"head, hand, leg\" (e.g., building), or some part of the generated object is twisted but not duplicated or missing, how can we determine the 3D consistency is correct or not?\n\n4. It is better to also report the appearance or texture quality measurements, even though this paper mainly focuses on improving the 3D consistency. It can show whether (and if yes, to what extent) the proposed method will influence the appearance or texture quality of the generated objects.\n\n5. The method is based on the assumption that all objects within the same category adhere to a canonical orientation in the training data. However, Objaverse does not align the zero poses of the objects. It is interesting to see whether only using a smaller subset, where objects have a canonical orientation, can lead to better results or not."
                },
                "questions": {
                    "value": "1. The questions in the Weaknesses section.\n\n2. \"Ours (NeRF-based full) using DeeFloyd IF first and then Stable Diffusion\". How is it implemented in detail? What is the method (Ours (NeRF-based)) used in user study (NeRF-based IF or NeRF-based full)?\n\n3. How are $\\lambda^{ori}$ and $\\lambda^{align}$ chosen?\n\n4. The user study interface can be shown in the appendix (How did the interface show users to \"consider only the 3D consistency\"? How are the users chosen? Do they have the concept of 3D consistency?)\n\nI will consider raising the rating if the authors can respond to the weaknesses and questions well in the rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3384/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3384/Reviewer_nptq",
                        "ICLR.cc/2024/Conference/Submission3384/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3384/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816806542,
            "cdate": 1698816806542,
            "tmdate": 1700765366550,
            "mdate": 1700765366550,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2Db5x9yNW9",
                "forum": "extpNXo6hB",
                "replyto": "FYxaxqHtla",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3384/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3384/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q: Ablation studies between CCM and other representations.**\n\nA: Thanks for the valuable comment, please refer to the common response about  \u201cMore ablation studies\u201d for more details.\n\n\n**Q: Limited experiments.**\n\nA: Almost all existing text-to-3D methods, including ours, share a common limitation that the optimization-based generation process could take hours for obtaining a single 3D result. For example, DreamFusion takes 1.5 hours on 4 TPUs, Magic3D 40 minutes on 8 GPUs, Fantasia3D 1 hour on 4 GPUs, and ProlificDreamer approximately 8 GPU hours. Due to this very reason, while DreamFusion managed to present results obtained using 398 prompts, the number of results presented in most of these works is indeed limited, such as 40 prompts in MVDream and a dozen in Fantasia3D and ProlificDreamer. \n\nYet, in our submission, we have presented around 100 results obtained using various arbitrary prompts in the video and supplementary, and compare to baseline methods on 80 prompts. Therefore, we would like to emphasize that our experimental results are sufficient and should not be considered as a weakness, as these prompts are several times more than those used for evaluating previous methods. During the rebuttal, we have increased the number of text prompts up to 150 (see anonymous links http://3dsweetdreamer.github.io/nerf-based-gallery_0.html and https://3dsweetdreamer.github.io/dmtet-based-gallery_0.html) but are indeed limited by the computing resources for more, we are happy to add more results using more prompts to the revision in the next stage.\n\n\n**Q: Criteria for determining 3D consistency in quantitative evaluations.**\n\nA: We agree that the definition of the 3D consistency can be vague to describe, and currently there is a lack of objective metrics that can be computed automatically to determine if a 3D object is multi-view inconsistent or not. This is why we rely on human users for quantitative evaluation of the 3D consistency of the generated results. Specifically, the human participants involved in Table 1 of the paper are researchers from academic laboratories, with highly relevant research backgrounds such as Computer Vision, Computer Graphics and Visual Computing, while the users involved in obtaining the preference rate charts are from loosely related backgrounds such as CS and EE (a more noisy evaluation process, yet our method consistently outperforms other baselines). Furthermore, we have included the user study interface in the appendix of the revised paper for clarification.\n\n\n**Q: Report the appearance quality.**\n\nA: We thank the reviewer for this insightful suggestion. As also mentioned in the common response, our method can preserve the high realism of the 2D results of the powerful text-to-image foundation model, as we only align the geometric priors at the coarse geometry level. This is particularly attractive, as the 3D dataset (e.g., Objaverse) in its current form falls short of covering samples as diverse as the 2D text-image dataset, particularly in terms of the appearance. During the rebuttal, we have obtained the numerical results of appearance-related metrics. As suggested by Reviewer UgSj, we use the R-precision, which is also used in related work. The results are presented below, and in Section A.7 and Table 3 in the revised paper. It is obvious that our method achieves the best performance on this metric over competing methods.\n| Method           | CLIP B/32 R@1 | CLIP B/32 R@5 | CLIP B/16 R@1 | CLIP B/16 R@5 |\n|------------------|---------------|---------------|---------------|---------------|\n| Magic3D          | 60.1          | 71.5          | 62.7          | 77.7          |\n| TextMesh         | 51.7          | 65.1          | 55.1          | 78.4          |\n| SJC              | 40.2          | 51.2          | 52.5          | 62.5          |\n| DreamFusion      | 59.7          | 70.2          | 61.6          | 74.3          |\n| Ours (NeRF-based)| **77.5**      | **84.9**      | **88.7**      | **92.3**      |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654403707,
                "cdate": 1700654403707,
                "tmdate": 1700654403707,
                "mdate": 1700654403707,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0xcfnipQtw",
                "forum": "extpNXo6hB",
                "replyto": "FYxaxqHtla",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3384/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3384/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q: Whether using a small amount of high-quality data for fine-tuning will improve the performance.**\n\nA:  In the submission, our model is fine-tuned using 3D objects obtained by filtering the Objaverse via some simple and heuristic rules. In addition, we also filtered out some samples in certain categories (e.g., humans) simply based on the orientation of their Axis-Aligned Bounding Box (we missed this filter in the submission and have revised accordingly). After these filterings, most of the data have been canonically aligned (around 80%, estimated based on statistical random samplings), and the rest wrongly oriented objects have only a minor impact on the performance of our model, as manifested by extensive successful results in the submission.\n\nIn addition, as suggested, we have also trained our model on a carefully and manually selected dataset, which consists of 20,000 objects. The resulting model is still able to generate highly varied 3D results, with detailed appearances (See A.5 and Figure 10 of the revised paper). While we have not observed significant improvement on this setting, we believe that using more well-structured data could potentially benefit the 3D generation and will perform a more thorough study in the next stage.\n\n**Q: More implementation details.**\n\nA: Ours (NeRF-based full) uses the DeepFloyd IF model first and then using SD. While DeepFloyd IF can be a more powerful model for text-to-image, it can only perform at the 64 x 64 resolution on our side due to its use of much more computational resources. For a fair comparison, Ours (NeRF-based) in the user study is conducted using only DeepFloyd IF, which is the same as other competing methods.\n\n\n**Q: Hyperparameters.**\n\nA: All results presented in the paper were generated using the same hyperparameters, without the need for adjusting individually for each generation. More specifically, at the beginning of the generation we guide the generation to focus more on the geometry, so the $\\lambda^{align}$ is set to a larger value (5x $\\lambda^{ori}$) and then decreased to 0.5x for higher-quality appearance. \n\n\n**Q: More details about the interface of the user study and the criteria for user selection.**\n\nA: We added more details and the figure of the interface for user study in the appendix. Please kindly see the feedback in the revision."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654516216,
                "cdate": 1700654516216,
                "tmdate": 1700654652599,
                "mdate": 1700654652599,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bdW8Iiqyaw",
                "forum": "extpNXo6hB",
                "replyto": "FYxaxqHtla",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3384/Reviewer_nptq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3384/Reviewer_nptq"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the detailed reply.\n\nThe ablation studies between CCM and other representations show the advantage of CCM. The new results measuring CLIP R-Precision show the advantage of the proposed method on image quality. Moreover, the added details would be helpful for readers to have a better understanding. \n\nOne minor question: in the user study interface, have the generated objects been shuffled in each row? It seems that in the figure of the NeRF-based method, the objects in the 4th row always have the largest size."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684429718,
                "cdate": 1700684429718,
                "tmdate": 1700684485338,
                "mdate": 1700684485338,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g1L4eDD4c8",
                "forum": "extpNXo6hB",
                "replyto": "FYxaxqHtla",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3384/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3384/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Hi, we hope our responses have effectively addressed all your concerns. Please be informed that the author-reviewer discussion period ends today. Should there be any unresolved issues, we would appreciate your prompt feedback. If your concerns have been addressed, please kindly consider updating your score. Thank you!"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733788492,
                "cdate": 1700733788492,
                "tmdate": 1700736610275,
                "mdate": 1700736610275,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7nQm6nXU8g",
            "forum": "extpNXo6hB",
            "replyto": "extpNXo6hB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3384/Reviewer_Q6rV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3384/Reviewer_Q6rV"
            ],
            "content": {
                "summary": {
                    "value": "Text-to-3D is now a very very hot topic, starting from Dreamfushion. Nowadays, there are already many works which demonstrate very good quality of results. However, existing methods undergo a severe issue: the SD lacks view information of the 3D objects, making \"multi-face\" issues are usually caused. This work presented Canonical Coordinates Map as a geometric representation and train a geometric-aligned prior resorting to the recent public 3D dataset-Objaverse. The results are of very high quality."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- First, the results are of very high quality. \n- The paper writing is good and the motivation is strong. \n- The proposed method is novel."
                },
                "weaknesses": {
                    "value": "- My major concern is about the necessity of the proposed CCM. It seems CCM is one of the geometric prior representation. I am curious what will happen if we replace CCM by just using normal map or depth map as a geometric representation? More specifically, an extra experiment is needed: using objaverse to train a normal/depth map (instead of CCM) generative model and also include camera condition.  \n- My another question is: if objaverse is used to finetune SD for CCM generation, will the generalization ability be decreased? As objaverse is lacking diversity especially on complicated scenarios. More discussions are encouraged.\n- In Fig 1, it seems the normal map owns many bumps, what are the details about the normal rendering?\n- It is said in the implementation details \"a significant portion of the 3d objects in canonical orientation and a few misoriented\". The authors mentioned it will not affect the results. I think more detailed explanations are needed, for example, what does it mean about mis-oriented? how many objects are mis-oriented\uff1f if the mis-oriented cases are corrected manually,  will the performance be improved further?"
                },
                "questions": {
                    "value": "see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3384/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698897726681,
            "cdate": 1698897726681,
            "tmdate": 1699636288968,
            "mdate": 1699636288968,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JmtiLnhTuv",
                "forum": "extpNXo6hB",
                "replyto": "7nQm6nXU8g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3384/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3384/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q: Ablation studies among CCM, depth map, and normal map.**\n\nA: We appreciate your feedback. Please refer to the common response for more details.\n\n\n**Q: The generalization ability of the model after fine-tuning.**\n\nA: Since we only finetune and constrain the geometric prior at a coarse resolution, the powerful generalization ability of the image diffusion models is largely preserved. The appearance modeling totally relies on the original diffusion model trained on billions of high-quality images. During the geometric modeling process, we were surprised to find that the finetune model still has strong generalization ability and can generate prompts such as Einstein wearing a gray suit and riding a bicycle. This is difficult to achieve with other existing methods. We have added more discussion on the generalizability in Appendix A.4.\n\n\n**Q: More details about normal rendering.**\n\nA: The bumps come from the tangent space normal. For appearance modeling, we follow Fantasia3D and optimize the tangent space normal, which is a crucial component\n of Physically Based Rendering (PBR) materials. The normal map showcased in our paper emerges from offsetting the original geometric normal through the application of\n tangent space normal. This procedure generates multiple bumps in visual, thereby enriching both the hierarchical structure and realism of the appearance. It allows the\n representation of intricate surface details without increasing the geometric complexity of the underlying model. This efficiency is particularly useful for representing\n high-resolution details on low-resolution geometry.\n\n**Q: More details about the orientation.**\n\nA: Let us illustrate the concept of 'mis-oriented' with an example. In the objaverse dataset, imagine 3D asset labeled as a 'cute panda.' When this asset is rendered from an azimuth=0 perspective, deviating from the front view of the panda, we classify this as the object being mis-oriented. Conversely, if the front view aligns with azimuth=0, we deem this as the correct or consistent orientation. Following the application of a specific set of rules to filter around 270k objects from the initial dataset, we conducted random sampling and manually assessed the percentage of objects exhibiting the accurate orientation. Our findings revealed that roughly 80% of the data exhibited consistent orientations. We reveal that the orientation of objects generated by the proposed model trained with such dataset is almost always correct and has a strong generalization ability. In order to further investigate the impact of data with a higher proportion of correct orientation on the generated results, We then manually screened the top 20k data, resulting in a dataset of 18,488 instances with a uniform orientation and high correspondence with their descriptions. From the experimental results obtained by training on such a smaller dataset, we are surprised to find that even with a smaller dataset of only 20k instances for fine-tuning, our text-to-3D pipeline still exhibits strong generalization and is capable of generating diverse 3D objects. Please refer to Appendix A.5 in the revised version of our paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653890670,
                "cdate": 1700653890670,
                "tmdate": 1700654632437,
                "mdate": 1700654632437,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cZN4hD6brE",
                "forum": "extpNXo6hB",
                "replyto": "7nQm6nXU8g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3384/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3384/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Your feedback is greatly appreciated! We've strived to address your concerns in our response. As the deadline approaches, should further queries arise, we are fully prepared to answer. Additionally, we respectfully request your kind consideration regarding a potential adjustment in the rating. Your insights are invaluable to us. Thank you sincerely for your time and support!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714170512,
                "cdate": 1700714170512,
                "tmdate": 1700714170512,
                "mdate": 1700714170512,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]