[
    {
        "title": "Towards the Characterization of Representations Learned via Capsule-based Network Architectures"
    },
    {
        "review": {
            "id": "F8BzRlUmcW",
            "forum": "irorVob9Eq",
            "replyto": "irorVob9Eq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2884/Reviewer_nJf8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2884/Reviewer_nJf8"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on Capsule Networks (CapsNets), which have been reintroduced as a compact and interpretable alternative to deep neural networks. While previous research has highlighted their compression capabilities, this paper aims to conduct a systematic and principled study to assess the interpretability properties of CapsNets, specifically examining the encoding of part-whole relationships within learned representations.\n\nTo evaluate interpretability, the authors analyze several capsule-based architectures using MNIST, SVHN, PASCAL-part, and CelebA datasets. The findings suggest that the representations encoded in CapsNets may not be as disentangled or strictly related to part-whole relationships as commonly claimed in the literature.\n\nThe contributions of the paper lie in conducting a thorough and rigorous investigation into the interpretability of CapsNets. By challenging the prevailing notion of highly disentangled representations and strict part-whole relationships, the authors provide valuable insights that facilitate a better understanding of the limitations and characteristics of CapsNets as interpretability-focused models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper stands out for conducting a systematic and principled study to evaluate the interpretability properties of Capsule Networks (CapsNets).\n\n+ The authors analyze multiple datasets (MNIST, SVHN, PASCAL-part, CelebA) and employ various capsule-based architectures, providing a comprehensive evaluation of interpretability in CapsNets. This extensive analysis strengthens the robustness of their conclusions and allows for a broader understanding of the limitations in terms of part-whole relationships."
                },
                "weaknesses": {
                    "value": "- The study focuses on a specific type of neural network architecture (CapsNets) and evaluates interpretability properties on a limited set of datasets (MNIST, SVHN, PASCAL-part, CelebA). Maybe large-scale datasets should be also considered, such as ImageNet."
                },
                "questions": {
                    "value": "Please refer to paper weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2884/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698049936358,
            "cdate": 1698049936358,
            "tmdate": 1699636231889,
            "mdate": 1699636231889,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Xb9OvX4grl",
                "forum": "irorVob9Eq",
                "replyto": "F8BzRlUmcW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Experiments on the small-scale dataset"
                    },
                    "comment": {
                        "value": "In the literature, MNIST, CIFAR10, affNIST, or SVHN are the datasets frequently used by the community for conducting research on CapsNets-based architectures. Therefore, we opted to conduct our analysis on a variety of these datasets so that our method and reported results could compared against by future methods. Beyond these standard datasets, we conducted experiments on CelebA and PASCAL datasets which are considered much complex in comparison with MNIST and CIFAR10.  \n\nAt this point, conducting experiments on ImageNet or similar large-scale datasets will only contribute to indicate whether the observations made in the paper remain on such big data settings. Due to the large scale of these datasets, the density of our analysis, our reduced computational resources and the limited discussion period, conducting an experiment on ImageNet might not be feasible (even partially). We will attempt to conduct an experiment on the CIFAR100 dataset, which possesses a significantly larger number of classes when compared to the datasets considered in the paper and do our best to provide results during the discussion period. We believe this should provide insights on the observations made in our work under a larger scale dataset."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245079089,
                "cdate": 1700245079089,
                "tmdate": 1700245079089,
                "mdate": 1700245079089,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LIgecrku4m",
                "forum": "irorVob9Eq",
                "replyto": "Xb9OvX4grl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2884/Reviewer_nJf8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2884/Reviewer_nJf8"
                ],
                "content": {
                    "title": {
                        "value": "Experiments on the small-scale dataset"
                    },
                    "comment": {
                        "value": "Thank your for the response. Please provide the experimental results on CIFAR100 and I will keep the original score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487822222,
                "cdate": 1700487822222,
                "tmdate": 1700487822222,
                "mdate": 1700487822222,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LcwhuYIsLv",
            "forum": "irorVob9Eq",
            "replyto": "irorVob9Eq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2884/Reviewer_66ge"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2884/Reviewer_66ge"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a principled approach for assessing the properties of capsule networks, focused around the investigation of relevant units in the network. This serves as an initial investigation in the lacking related literature concerning CapsNets. Qualitative and quantitative evaluations on benchmark datasets (MNIST, SVHN, PASCAL-Part, CelebA and CelebAMask-HQ) reveal a potential entanglement of the emerging representations, contradicting previous findings/claims in the related CapsNet literature."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work considers an analysis of the disentanglement of the representations (features) in the context of CapsNets. To the best of my knowledge, this constitutes one of the first attempts towards analysis of CapsNets in this setting. The motivation is clear and the considered setting is sound. This approach introduces an appropriate interval for pertubation analysis via first-order statistics The authors consider a variety of standard benchmark architecture, similar to related recent methods, and assess different configurations and settings."
                },
                "weaknesses": {
                    "value": "Due to the fact that the authors consider multiple settings and configurations, some parts of the paper are not easy to read; some missing definitions and notation inconsistency further interrupt the flow. \n\nStarting with the definition of the first-order statistics in the perturbation analysis section, the authors introduce $A_c = [a_{1,c}^l, a_{2,c}^l, \\dots, a_{s,c}^l]$, where $A_c \\in \\mathbb{R}^{S' \\times A'}$. However: (i) $A_c$ seems to be layer-specific; omitting this breaks the flow of the paper, and (ii) at the same time, $S'$ and $A'$ are never defined. Is $S'$ the set of examples corresponding to class $c$? and what is $A'$? I would assume that $A' = w \\times h \\times d$, but it is never defined. \n\nIn a similar vein, why are the first order statistics again $\\in \\mathbb{R}^{S'\\times A'}$? Since a reduction takes places, the dimensionality should be different. The authors then introduce $A_{all} = [A_1;A_2;\\dots ; A_1^M] \\in \\mathbb{R}^{D \\times A'}$. I am assuming that $A_1^M$ is a typo that should be $A_M$. Again this is layer specific and misses the $l$ superscript (which appears in the $\\alpha$ definition afterwards). Does this matrix comprise the first order statistics or $A_c$ themselves? The authors define $A_{all}$ as the concatentation of the $A_c$ matrices, but note that this is composed of the first order statistics. In this context, wouldn't an $\\alpha$ value based only on $A_c$ make more sense for the sensitivity analysis? Different classes activate $v_j$ with different magnitudes; when an entry for one example is small and is altered with a $\\xi$ that is very larger, it can easily lead to massive changes in the reconstruction. I fully understand the need for a more principled definition of $A_c$; I am not $100$% certain that this formulation captures the subtle differences in the activations between the examples. What are the values that the other works consider?\n\nMoving on to the experimental section, for the perturbation analysis, I re-iterate my point about the magnitude of the perturbation. Even though the argument that the authors could easily hold, I am still concerned about the impact of the perturbation magnitude is the decoding process. Since the decoder it's not just a simple linear layer, a large change (compared to the original magnitudes of the vector) in the entry of vector $v_j$ can lead to misleading results. \n\nThe visualization in Fig. 2 is not clear, a description of what each color represents is important for understanding what is happening. \n\nFurther details are also necessary in the caption of Fig. 4. Without running back and forth to the relevant section, it is not clear what the different plots depict. What are the $D_0-D_9$ legends? I suppose they are the digits of the datasets. \n\nI can't see how the Relevance Mass Accuracy metric is an appropriate  proxy for measuring the part-whole relationship. This is a metric for measuring the spatial overlap between ground truth masks and a 2D positive valued image with a single channel.  How does this relate to heatmaps arising from spatially re-arranged responses of capsules is not clear to me. It is possible that I am missing the intuition and the formulation behind this construction. I could understand the approach for activation maps for convolutional maps but not for capsules. \n\nFigure 6 is not very clear. A more detailed caption can help clarify what each column depicts without the need to re-look at the text. \n\nThe authors note that \"Overall, the mean of RMA is lower than what was anticipated\". What was anticipated and how this conclusion was reached? Potentially this ties to the previous point. After the analysis, the authors themselves note that \"the observed low overlap may have its origin in other sources\". I personally don't find this particular analysis to be adequate to draw conclusions about the part-whole relationship of CapsNets. It may very well be true as the authors claim, but further investigation is needed."
                },
                "questions": {
                    "value": "Please see the Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2884/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2884/Reviewer_66ge",
                        "ICLR.cc/2024/Conference/Submission2884/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2884/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698183899385,
            "cdate": 1698183899385,
            "tmdate": 1700651964731,
            "mdate": 1700651964731,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4eB7G75Azn",
                "forum": "irorVob9Eq",
                "replyto": "LcwhuYIsLv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarity on the Perturbation Analysis (Sec. 4.1)"
                    },
                    "comment": {
                        "value": "Regarding the clarity of presentation of Section 4.1, We checked the section and we agree with you regarding the following; $A\\prime$  where indeed refers to the flattened activation obtained from all examples of a given layer.  Where its dimension $S\\prime$ refers to the number of examples corresponding to a class $c$ and $A\\prime$ refers to the total number of flattened activations. This matrix represents all the activations of all the units in layer l across all the examples of class c. We agree that to preserve the flow the superindex l should be in place ($A_c^l$) in order to preserve its connection to its corresponding layer. From there we compute a matrix $\\eta_c^l = [min(A^l_c) ; max(A^l_c) ; mean(A^l_c); std(A^l_c)] \\in\\mathbb{R}^{[4 {\\times} A\\prime]}$ is computed which represents the first-order statistics for the activations of every unit within layer l for class $c$. A similar approach is conducted with a larger matrix $A^l_{all}$ containing activations from examples of all classes in the dataset. It is from these $A$ that the empirical activation range ($\\alpha$) is computed at the unit level. In the text of the paper have introduced an additional matrix $\\eta$ to refer to the first-order statistics computed from their corresponding $A$ matrices. We are considering droping these $\\eta$ matrices for clarity. In the revised version of the paper, we have addressed these notation unclarities plus other typos highlighted by the reviewer. We thank the reviewer for the meticulous revision of this part of the manuscript we believe the provided feedback improves the clarity and flow of the content."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244195783,
                "cdate": 1700244195783,
                "tmdate": 1700244286360,
                "mdate": 1700244286360,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oQnmKQrxza",
                "forum": "irorVob9Eq",
                "replyto": "LcwhuYIsLv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Magnitude of the applied perturbation"
                    },
                    "comment": {
                        "value": "We thank the reviewer for highlighting this very important aspect, we are currently conducting some experiments to further elaborate on this aspect. We hope to provide more insights on this before the end of the discussion period."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244516691,
                "cdate": 1700244516691,
                "tmdate": 1700244516691,
                "mdate": 1700244516691,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "teKAOQVhPE",
                "forum": "irorVob9Eq",
                "replyto": "LcwhuYIsLv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarity/Intelligibility of the presentation of some figures and tables."
                    },
                    "comment": {
                        "value": "Thanks for bringing the issue with the missing color bar (Fig.2/Sec. 6.1) to our attention, we already updated the color bar where the yellow indicates to higher classification accuracy. Similarly, we updated Fig.4 where $D0$ ${\u2212}D9$ represents the classes as you stated. For clarity, we also updated the title of both Fig.4 and Fig.6."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244607495,
                "cdate": 1700244607495,
                "tmdate": 1700244630770,
                "mdate": 1700244630770,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kOBDfSDo89",
                "forum": "irorVob9Eq",
                "replyto": "LcwhuYIsLv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Assessing Part-Whole Relationships via Relevance Mass Accuracy (RMA)"
                    },
                    "comment": {
                        "value": "The description that the reviewer provided for the operation of RMA is accurate. It indeed measures the spatial coverage between a heatmap (matrix of continuous values) and a matrix of positive values. (reference) In our work, we estimate the level to which features encoded in a trained CapsNet encode part-whole relationships by measuring the spatial overlap between the responses of two units from different levels in the architecture that have a strong connection between them. The main intuition is that given the hierarchical representation that a CapsNet is expected to encode across layers, it is expected that more granular elements (parts) are encoded at earlier layers of the architecture and coarser elements (whole) are expected to be encoded at latter layers as a composition of the part elements. Therefore, spatially-speaking, features coming from a part element is expected to lie within the extent the whole element it contributes to compose. Hence, we measure the level to which this type of encoding is present by measuring the spatial coverage that responses at earlier capsule layers in the architecture have wrt. latter layers.  Following the formulation of RMA, the $2D$ positive matrix is obtained by thresholding the response of the latter layers (whole) and the continuous matrix is defined by the response of the earlier layer (parts).\nIn order to obtain the $2D$ response of a given layer, we apply the methods from [Simoyan et al., 2014] which, given a unit of interest,  an input $x$ and a predicted class label $y$, generates a heatmap highlighting the spatial regions in the input $x$ which determine the prediction $y$."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244808640,
                "cdate": 1700244808640,
                "tmdate": 1700244882983,
                "mdate": 1700244882983,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z0eUCZ1PmS",
                "forum": "irorVob9Eq",
                "replyto": "LcwhuYIsLv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Expected results from the  Relevance Mass Accuracy (RMA) analysis (Sec. 6.3)"
                    },
                    "comment": {
                        "value": "The output of the RMA metric is a value in the range $[0,1]$ indicating the coverage between the two input matrices with $1$ indicating a high coverage. The results reported in our analysis (Table 3) indicate that, when a threshold of $0.5$ is used to produce the positive reference matrix,  the RMA score is on average $~0.04$ among the considered datasets. We notice this score increases as we use less strictive threshold values (e.g. $0.1$), however at this level the spatial extent of the candidate part unit is so coarse that hinders a local analysis. The lower layer did encoded parts of the elements encoded at deeper layers we would have expected a layer spatial coverage/overlap."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244957816,
                "cdate": 1700244957816,
                "tmdate": 1700244957816,
                "mdate": 1700244957816,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7BxDgC2YKq",
                "forum": "irorVob9Eq",
                "replyto": "aURWaRhAYR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2884/Reviewer_66ge"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2884/Reviewer_66ge"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response and particularly the analysis on the applied perturbation magnitude. Even though I still feel that there is some potential missing from this work, especially with respect to using larger datasets, I will raise my score to 6."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651940887,
                "cdate": 1700651940887,
                "tmdate": 1700651940887,
                "mdate": 1700651940887,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nZnP247F22",
            "forum": "irorVob9Eq",
            "replyto": "irorVob9Eq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2884/Reviewer_FL72"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2884/Reviewer_FL72"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the interpretability properties of Capsule Neural Networks. It mainly focuses on the part-whole relationships encoded within the learned representations. The analysis results point out that capsule-based networks may not be related to parts-whole relationships as stated in the literature."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "i) The paper conducts extensive analysis and visualization of the capsule network. \n\nii) The proposed permutation-based analysis and relevant unit selection are reasonable, and the analysis results support the paper's conclusion."
                },
                "weaknesses": {
                    "value": "i) The experiments are mostly conducted on the small-scale dataset, such ass MINIST and SVHN, and the image resolution is also relatively small, which makes the results not convincing, and the visual difference between the baseline method and the proposed method is not obvious.\n\nii) The experiments are all conducted based on ConvNets. Does the conclusion hold based on a transformer-based network?\n\niii) The discussed related works are mostly before 2020. There have been many works about capsule networks in recent years that have not been discussed."
                },
                "questions": {
                    "value": "Refer to the weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2884/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698661675692,
            "cdate": 1698661675692,
            "tmdate": 1699636231712,
            "mdate": 1699636231712,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "90YAzgCbUM",
                "forum": "irorVob9Eq",
                "replyto": "nZnP247F22",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Experiments on ConvNets/ Transformer based models"
                    },
                    "comment": {
                        "value": "Regarding conducting experiments based on other types of networks, while our methodology is applicable to other architectures, the aim of this paper lies on the interpretability of CapsNets-based architectures."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243835078,
                "cdate": 1700243835078,
                "tmdate": 1700519036725,
                "mdate": 1700519036725,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zbcqxJF43c",
                "forum": "irorVob9Eq",
                "replyto": "nZnP247F22",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Input size of the considered dataset"
                    },
                    "comment": {
                        "value": "Regarding the input size of the datasets used in our analysis, on the one hand, given our limited computational resources, we opted to conduct our analysis on a smaller input size. On the other hand, we believe that using larger input sizes could have helped us improve the level of detail of the provided qualitative results."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243883610,
                "cdate": 1700243883610,
                "tmdate": 1700243883610,
                "mdate": 1700243883610,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e1ZyWEnXdG",
                "forum": "irorVob9Eq",
                "replyto": "nZnP247F22",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Experiments on the small-scale dataset"
                    },
                    "comment": {
                        "value": "In the literature, MNIST, CIFAR10, affNIST, or SVHN are the datasets frequently used by the community for conducting research on CapsNets-based architectures. Therefore, we opted to conduct our analysis on a variety of these datasets so that our method and reported results could compared against by future methods. Beyond these standard datasets, we conducted experiments on CelebA and PASCAL datasets which are considered much complex in comparison with MNIST and CIFAR10.  \n\nAt this point, conducting experiments on ImageNet or similar large-scale datasets will only contribute to indicate whether the observations made in the paper remain on such big data settings. Due to the large scale of these datasets, the density of our analysis, our reduced computational resources and the limited discussion period, conducting an experiment on ImageNet might not be feasible (even partially). We will attempt to conduct an experiment on the CIFAR100 dataset, which possesses a significantly larger number of classes when compared to the datasets considered in the paper and do our best to provide results during the discussion period. We believe this should provide insights on the observations made in our work under a larger scale dataset."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245039473,
                "cdate": 1700245039473,
                "tmdate": 1700245039473,
                "mdate": 1700245039473,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jCQiTGt3yA",
                "forum": "irorVob9Eq",
                "replyto": "nZnP247F22",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2884/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Regarding related recent efforts"
                    },
                    "comment": {
                        "value": "In [1], interpretation capabilities of CapsNets were investigated by analyzing coupling coefficients that link the connections between capsule layers. This analysis helps understand how CapsNet predicts amyotrophic lateral sclerosis (ALS) or Healthy, it reveals which primary capsules were activated when the model predicts ALS, similar to how we interpret predictions in our methodology. In addition, coupling coefficients were averaged, and the results are presented in heatmaps/histograms, highlighting the differences in capsule activation between both classes. In our case, we produce heatmaps that highlight the spatial regions in the input which determine the prediction. Similar to our work, [2] conducted a perturbation analysis of the predicted vector which was used to link learned representation to visual attributes such as thickness, orientation, etc. by reconstructing the perturbed vector using handwritten-digit images. Similar to us, this work was also conducted using a perturbation analysis ([-0.2,0.2] with steps of 0.06).  Different from our work, where a methodology is put forward for the extraction of this perturbation range, the range considered in [2] seems to have been selected in a rather arbitrary manner. [3] proposed an interpretable multimodal fusion method (IMCF) aiming at getting insights on how modalities interact and which ones are significant for predictions made by the model. Similar to our work, routing coefficients are considered to guide the interpretation process. In contrast, [3] introduces this interpretation capability by the modification of the CapsNet architecture via the integration of  BiLSTM/LSTM components. [4] proposes a CapsNet for the classification of imagined phonemes and words in EEG signals. This effort differs from ours on that it uses the learned representation of the output vectors in the class capsule layer to construct activity maps (similar to heatmaps) of brain activity based on different categories. These maps explain the most active parts of the brain during the process and they are produced based on statistical features obtained from EEG speech imagery signals. However, in our methodology, we produce gradient-based heatmaps that explain the predictions made by the model.\n\n\n[1] Predicting the prevalence of complex genetic diseases from individual genotype profiles using capsule networks. \"Nature Machine Intelligence\" (2023)   \n\n[2] Quantum Capsule Networks. Liu, Zidu, et al. \"Quantum Science and Technology\" (2022)  \n\n[3] Interpretable Multimodal Capsule Fusion. Wu, Jianfeng, Sijie Mai, and Haifeng Hu. \"IEEE/ACM Transactions on Audio, Speech, and Language Processing\" (2022) \n\n[4] Interpretation of a deep analysis of speech imagery features extracted by a capsule neural network. Mac\u00edas-Mac\u00edas, Jos\u00e9 M., et al. \"Computers in Biology and Medicine\" (2023)"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245426151,
                "cdate": 1700245426151,
                "tmdate": 1700245426151,
                "mdate": 1700245426151,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ijQVdI9mze",
                "forum": "irorVob9Eq",
                "replyto": "jCQiTGt3yA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2884/Reviewer_FL72"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2884/Reviewer_FL72"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's response. After reading the author's response, the experiment results still can not convince me. So I keep my original score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700486860038,
                "cdate": 1700486860038,
                "tmdate": 1700486860038,
                "mdate": 1700486860038,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]