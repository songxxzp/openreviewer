[
    {
        "title": "Latent 3D Graph Diffusion"
    },
    {
        "review": {
            "id": "lttcpgNuDi",
            "forum": "cXbnGtO0NZ",
            "replyto": "cXbnGtO0NZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3853/Reviewer_HVJQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3853/Reviewer_HVJQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper \"Latent 3D Graph Diffusion\" explores using generative AI for 3D graph generation with symmetry-group equivariance. It emphasizes the importance of choosing the right latent space for diffusion, proposing a compact and symmetry-preserving space called \"latent 3D graph diffusion.\" They extend this to conditional generation, showing its potential in molecular discovery with improved speed and quality compared to existing methods. The paper's contributions include motivational analysis, latent space construction, and extensions to conditional generation, all supported by experimental results. The conclusion suggests future research areas, including semantics-specific regularization of the latent space."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths:\n1.\tNovel Approach: The paper addresses an important and under-investigated question regarding the latent space for 3D graph diffusion, introducing a novel concept of latent 3D graph diffusion. This approach is innovative and could lead to significant advancements in the field.\n\n2.\tTheoretical Foundation: The paper provides a theoretical analysis to motivate the use of latent spaces for 3D graph diffusion. The performance bound of 3D graph diffusion in a latent space is discussed, which adds a valuable theoretical perspective to the work.\n\n3.\tComprehensive Contributions: The paper offers a multi-faceted approach to 3D graph generation, addressing the choice of latent space, the construction of compact and informative spaces, conditional generation, and regularization of the latent space. This comprehensive approach demonstrates the authors' commitment to advancing the field.\n\n4.\tEmpirical Validation: The paper supports its claims with empirical results, showing that the proposed method outperforms existing techniques. Various settings are adopted, including unconditional generation, invariant generation conditioned on quantum properties, and equivariant generation conditioned on protein targets."
                },
                "weaknesses": {
                    "value": "I have several questions:\n\n1. What's the difference between a one-shot AE and a Cascaded AE? The paper claims that \"a one-shot AE embeds and reconstructs molecule data, evaluating both structure topology and geometry simultaneously.\" How does a Cascaded AE differ? Are topology and geometry independent from each other in a Cascaded AE?  \n\n2. I would recommend adding an algorithm section to improve clarity. Based on my current understanding, a graph is initially encoded by an encoder in the latent space, then it undergoes diffusion in the latent space, and finally, it is transformed back to the graph space. Please correct me if I'm mistaken. Could you also clarify which diffusion model is used for the diffusion process in the latent space?  \n\n3. The paper asserts that \"a latent space should possess (i) low dimensionality, (ii) low reconstruction error, and (iii) preserve group symmetry.\" How do you ensure that the autoencoder can meet these objectives? Have you conducted any analysis in this regard?"
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3853/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780328791,
            "cdate": 1698780328791,
            "tmdate": 1699636343798,
            "mdate": 1699636343798,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eQOgKuCA2v",
                "forum": "cXbnGtO0NZ",
                "replyto": "lttcpgNuDi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3853/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3853/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HVJQ"
                    },
                    "comment": {
                        "value": "**Q1.** What's the difference between a one-shot AE and a Cascaded AE? The paper claims that \"a one-shot AE embeds and reconstructs molecule data, evaluating both structure topology and geometry simultaneously.\" How does a Cascaded AE differ? Are topology and geometry independent from each other in a Cascaded AE?\n\n**Answer.**\n- The reviewer understands correctly that the topological and geometric AEs encode and sequentially decode, independent from each other in the cascaded version, and in the one-shot version the two AEs jointly encode/decode. Please also refer to Appendices B.1 & B.2 for more information.\n\n\n**Q2.** I would recommend adding an algorithm section to improve clarity. Based on my current understanding, a graph is initially encoded by an encoder in the latent space, then it undergoes diffusion in the latent space, and finally, it is transformed back to the graph space. Please correct me if I'm mistaken. Could you also clarify which diffusion model is used for the diffusion process in the latent space?\n\n**Answer.**\n- The reviewer understands our pipeline correctly.\n- Following the suggestion, we add the detailed algorithm sections (Algorithms 1 & 2) to describe our latent 3D graph diffusion, beyond the overview Figures 2 & 3. They describe the processes of (conditional) in/equivariant latent encoding, latent diffusion, and sampling.\n- The adopted diffusion model is DDPM (https://arxiv.org/abs/2006.11239).\n\n\n**Q3.** The paper asserts that \"a latent space should possess (i) low dimensionality, (ii) low reconstruction error, and (iii) preserve group symmetry.\" How do you ensure that the autoencoder can meet these objectives? Have you conducted any analysis in this regard?\n\n**Answer.**\n- The factors of (i) low dimensionality and (iii) preserving group symmetry are by design in the AE architecture, which is guaranteed.\n- The factor of (ii) low reconstruction error is achieved by building the 3D graph AE in a cascaded manner, as one of our contributions described in Section 3.1. The numerical comparison of different AEs is shown in Tables 1 & 11."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3853/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542858742,
                "cdate": 1700542858742,
                "tmdate": 1700542858742,
                "mdate": 1700542858742,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Luk7m2AdI4",
                "forum": "cXbnGtO0NZ",
                "replyto": "eQOgKuCA2v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3853/Reviewer_HVJQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3853/Reviewer_HVJQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your rebuttal. Most of my concerns have been solved. I will keep my overall score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3853/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710143717,
                "cdate": 1700710143717,
                "tmdate": 1700710143717,
                "mdate": 1700710143717,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "j0FdiDgZbH",
            "forum": "cXbnGtO0NZ",
            "replyto": "cXbnGtO0NZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3853/Reviewer_N36h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3853/Reviewer_N36h"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce the generation of 3D graphs characterized by symmetry group equivariances, a feature with useful implications for machine vision and molecular discovery.The paper focuses on the optimal latent space for 3D graph diffusion, emphasizing its benefits over traditional methods. By strategically using cascaded 2D-3D graph autoencoders, the authors reveal a model called \"latent 3D graph diffusion\". Notably, this innovative approach demonstrates remarkable efficacy when adapted to the molecular context and enhanced by graph self-supervised learning. The experimental results highlight its ability to rapidly generate remarkable 3D molecular conformations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This study provides a detailed analysis of the underlying methodology. It illustrates the overall relationship between diffusion performance, latent space reconstruction quality, symmetry conservation, and data dimensionality. The depth of this analysis demonstrates the thoroughness and precision of the research.\n\n2. In this work, researchers adopt graph contrastive learning as a strategy to refine and enhance latent space representations in 3D graph autoencoders (AEs). The approach stands out for its innovative nature.\n\n3. In terms of application evaluation, the authors conduct an exhaustive evaluation of the model for various scenarios. This includes unconditional generation of 3D molecules, conditional generation based on (invariant) quantum properties, and conditional generation in relation to (equivariant) protein targets. These diverse evaluations reinforce the model's laudable versatility and adaptability."
                },
                "weaknesses": {
                    "value": "1. For the objective of unconditional 3D molecular generation, I noticed the absence of results from \"Ours-GSSL\". Including these would be crucial to validate the effectiveness of GCL.\n\n2. Regarding the unconditional 3D molecule generation, the 'molsta' metric, which gauges molecular integrity, seems paramount. From the data presented, your findings exhibit significant variances when juxtaposed against the previously introduced 3D latent diffusion model, GeoLDM. Additionally, the AtomSta and MolSta metrics for the Drug dataset appear to be omitted.\n\n3. In the context of conditional generation based on (invariant) quantum properties, it's essential to evaluate both ID and OOD. Yet, the \"Ours-GSSL\" performance doesn\u2019t exhibit a noteworthy distinction compared to \"ours\", \"Random\", and similar methods. In some instances, it even underperforms. This might cast a shadow on the robustness of conclusions articulated in Section 4.2, especially point (v).\n\n4. Concerning the experiments related to conditional generation associated with (equivariant) protein targets, I'd recommend broadening the evaluation scope by incorporating metrics such as the Vina Score and Vina Dock in your findings. Furthermore, beyond Targetdiff, newer techniques like DiffSBDD[1] and DecompDiff[2] have emerged. Comparing your approach with these could provide a more holistic view. As per the metrics currently displayed, the method delineated in your manuscript doesn't seem to lead the pack.\n\nMinor:\n\n1. In Figure 4, there appear to be some omission errors; the circled references seem to be missing.\n\n2. The structure of \"Proofs for Analysis\" in Appendix A could be refined for better readability.\n\n[1] Schneuing, Arne, et al. \"Structure-based drug design with equivariant diffusion models.\" arXiv preprint arXiv:2210.13695 (2022).\n\n[2] Guan, Jiaqi, et al. \"DecompDiff: Diffusion Models with Decomposed Priors for Structure-Based Drug Design.\" (2023)."
                },
                "questions": {
                    "value": "1. The design of latent diffusion appears to be complex. Have you considered simplifying it by merely applying KL divergence to $z^{{0}}$ and deriving a VAE-like model using the encoder and decoder as proposed in this paper? An ablation study should be added here.\n\n2. Could there be a more comprehensive validation of GCL's efficacy in Section 4?\n\n3. Given your assertion that your model \"produces superior 3D molecules faster\" owing to latent diffusion, could you perhaps contrast it with GeoLDM, a previously introduced 3D latent diffusion model?\n\n4. Is it possible to conduct a more in-depth evaluation and comparative study of experiments related to the generation of conditions related to equivariant protein targets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3853/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830226658,
            "cdate": 1698830226658,
            "tmdate": 1699636343718,
            "mdate": 1699636343718,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XXcFyq0u5f",
                "forum": "cXbnGtO0NZ",
                "replyto": "j0FdiDgZbH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3853/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3853/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer N36h (1)"
                    },
                    "comment": {
                        "value": "**Q1.** For the objective of unconditional 3D molecular generation, I noticed the absence of results from \"Ours-GSSL\". Including these would be crucial to validate the effectiveness of GCL. Could there be a more comprehensive validation of GCL's efficacy in Section 4?\n\nIn the context of conditional generation based on (invariant) quantum properties, it's essential to evaluate both ID and OOD. Yet, the \"Ours-GSSL\" performance doesn\u2019t exhibit a noteworthy distinction compared to \"ours\", \"Random\", and similar methods. In some instances, it even underperforms. This might cast a shadow on the robustness of conclusions articulated in Section 4.2, especially point (v).\n\n**Answer.**\n- We provide additional results in Tables 15 & 16, on applying GSSL for unconditional generation and conditional generation with less training data.\n- Based on our experiments, we observed that the benefit of GSSL is mainly on:\n    - OOD scenarios when the GSSL prior is aligned with the condition semantics, as we stated in Section 4.2;\n    - Unconditional or ID scenarios when training data are insufficient. This is consistent with the role of GSSL in discriminative modeling (https://arxiv.org/abs/2010.13902).\n- For the OOD scenarios, though the improvement does not appear for all datasets, we know when it is improved: Specifically, we can measure the semantics-alignment by computing the homogeneity ratio as detailed in Appendix E.2. This indicates a future potential to design semantics-specific GSSL tasks for generative modeling.\n\n**Q2.** Regarding the unconditional 3D molecule generation, the 'molsta' metric, which gauges molecular integrity, seems paramount. From the data presented, your findings exhibit significant variances when juxtaposed against the previously introduced 3D latent diffusion model, GeoLDM. Additionally, the AtomSta and MolSta metrics for the Drug dataset appear to be omitted.\n\n**Answer.**\n- Our method is different from GeoLDM. We summarize the difference across different methods in Table 10, mainly lying in two perspectives: (i) latent dimension and (ii) the modeling of topological and geometric features.\n    - Latent dimension (LD). Assuming the feature dimension of data is $N \\times (D+3)$. GeoLDM, diffusing at the latent space of nodes is of LD $N \\times (D\u2019+3)$. Guided by our theory in Section 3.2, we chase a lower dimensional latent space by compressing the node factor $N$, that our method at the other end of the spectrum of the minimum LD $D\u2019$.\n    - Modeling of topological and geometric features. EDM, GeoLDM and our one-shot model jointly model the topological and geometric features for all $N$ nodes. However, our one-shot model cannot manage to reconstruct the feature per node for its overly low LD, that the node factor $N$ is compressed. Guided by our theory in Section 3.2, we develop the cascaded model, achieving satisfactory reconstruction performance while maintaining symmetry and sufficiently low LD.\n- The above aspects lead to different results: GeoLDM performs better in generating geometrically stable molecules, and our method performs better in generating topologically valid molecules. Both of them are important in real-world applications.\n- The evaluation is standardized as in EDM and GeoLDM, and we do report AtomSta for Drugs. Following EDM and GeoLDM, the MolSta metric is less informative to be evaluated in Drugs, since the ground-truth molecules in Drugs have 86.5% atom-level and nearly 0% molecule-level stability, due to which contain larger and more complex structures, creating errors during bond type prediction based on pair-wise atom types and distances."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3853/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542213183,
                "cdate": 1700542213183,
                "tmdate": 1700542213183,
                "mdate": 1700542213183,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "f00T6xzCHB",
            "forum": "cXbnGtO0NZ",
            "replyto": "cXbnGtO0NZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3853/Reviewer_7uXM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3853/Reviewer_7uXM"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to perform 3D Graph (molecule) generation in latent space instead of during it directly in the 3D space as usually done now. The author show that diffusion in a lower dimensional space should be more efficient for diffusion. They also propose to use a cascaded auto encoder (AE) instead of a one-shot AE to build such latent space. These changes are shown to help improve the quality of generated samples and are also applicable in the conditioned case."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors provide a nice and sensible motivation, that smaller latent spaces should be more efficient for diffusion. The proposed approach also outperforms the chosen baselines. The proposed formulation is nicely extended to include conditioning on both scalar and 3D properties, which allows for a wider range of applications."
                },
                "weaknesses": {
                    "value": "The novelty of the work is somewhat limited. It takes existing 3D molecule genration setup with diffusion models and transforms it into latent diffusion, which is a known concept in general and some latent 3D models do exist (e.g. GEOLDM), using mostly off the shelf architectures.\n\nOne of the main contributions of this work is the cascaded auto encoder and main motivation for why it is needed stems from the fact that one-shot AE effectively failed to train. I find it a bit strange that the one-shot AE fails to train completely as there are 3D molecule generative models that generate 2D and 3D graph features jointly (e.g. MIDI or the example I reference in the paragraph below). They do usually re-weight the losses for 2D and 3D terms, but they do work. So I would like to see a more detailed analysis on why it doesn't work and potentially an ablation on the one-shot AE architecture and losses. I can understand one-shot AE working worse than the proposed cascaded AE with teacher forcing, but it essentially performing like a random initialization makes me wonder if it was tested sufficiently carefully.\n\nAs I also point out below, it would make sense to use for example https://arxiv.org/pdf/2309.17296.pdf as the baseline instead of an older EDM model, even though that paper is somewhat recent, so I understand its exclusion for the initial version of the paper. Still it would be nice to have for the rebuttal."
                },
                "questions": {
                    "value": "In section 3.1 it is stated that graph matching is used for the AE training loss to ensure permutation and SE3 invariance. This can be computationally expensive. How fast is the AE training? In terms of wall time, but also asymptotically (in O notation)? Without knowing this the comparison of training time of the proposed method vs EDM is also a bit complicated. As I understand Table 4. does not account for the AE training time (yes, it can be trained once, but it still needs training).\n\nAlso, since AE is trained on a much larger dataset, it might be fair to also pre-train the EDM on general molecule genration. It has been shown that pretraining helps molecule generation (https://arxiv.org/pdf/2309.17296.pdf). While this paper is quite recent it would still make more sense to compare against such properly tuned state-of-the-art molecular diffusion setup instead of vanilla EDM, which is an older baseline (first ever diffusion model for 3D molecule generation).\n\nWhy is MIDI, which is cited numerous times in the paper is not compared against in the experiments? It does in certain metrics perform a lot better than the EDM, which is used as the main baseline here.\n\nAlso, in 3.2 Setup sections authors say that connectivity is commonly later determined based on domain rules and cite MIDI among others. Its true, that such domain-based rules are used to recover connectivity in e.g. EDM, but in MIDI connectivity is modeled explicitly if I remember correctly?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3853/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3853/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3853/Reviewer_7uXM"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3853/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832241206,
            "cdate": 1698832241206,
            "tmdate": 1699636343619,
            "mdate": 1699636343619,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OI2ffKJ9ya",
                "forum": "cXbnGtO0NZ",
                "replyto": "f00T6xzCHB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3853/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3853/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7uXM (1)"
                    },
                    "comment": {
                        "value": "**Q1.** The novelty of the work is somewhat limited. It takes existing 3D molecule genration setup with diffusion models and transforms it into latent diffusion, which is a known concept in general and some latent 3D models do exist (e.g. GEOLDM), using mostly off the shelf architectures.\n\n**Answer.**\n- **Our work is not a direct extension of latent diffusion into 3D graphs.** We aim at generating 3D graphs of symmetry-group equivariance through diffusion. In other words, the quality of the generated graphs of topology and geometry is invariant / equivariant to a set of transformations (permutation group and Lie group E(3) of rotations and translations in our study).  This aim has deep roots in 3D molecular generation as demonstrated in our numerical experiments for desired quantum properties (invariant to E(3) perturbation of the 3D molecules) or desired protein binding (equivariant to E(3) perturbation of the 3D molecules); and it has broader application scopes for 3D graphs of symmetry-group equivariance in general.\n- **Contribution (i).** We provided theoretical analysis (Section 3.2) on what space should diffusion generative models operate on for generating 3D graphs of non-Euclidean structure and group-symmetry equivariance.  We addressed group-symmetry equivariance while deriving model performance bounds.  Specifically, Proposition 1 gave the upper bound of the diffusion model performances (as measured in total variation distance) operating in the original 3D graph domain ($D\u2019$-dimensional); whereas Proposition 2 gave the upper bound in the latent space ($D\u2019\u2019$-dimensional). The latter led to a better upper bound of the diffusion model performances if reconstruction errors are low and the latent dimension is low, which motivates our approach of latent diffusion.\n- **Contribution (ii).** In pursuing the latent space as guided by our theoretical, motivational analyses, we constructed a 3D graph autoencoder of the following properties: (1) permutation and E(3) invariant and (2) sufficiently low reconstruction errors despite low latent dimension (these two present nontrivial trade-offs), which is under-explored.  We accomplished this by combining 2D graph autoencoder (for topology features) and point cloud autoencoder (for geometry features given topologies) in a cascade manner (Section 3.1).\n- **Contribution (iii).** To improve the out-of-domain (OOD) generation, we further incorporate graph self-supervised learning into learning the 3D graph autoencoder and the resulting latent space for diffusion (Section 3.1).  Our self-supervised latent diffusion has been largely unexplored for 3D graphs of group-symmetry equivariance.  Our numerical experiments demonstrated its effectiveness in improving OOD performances, which is a critical barrier for designing molecules with much improved properties or new properties.\n- **Contribution (iv).** We further extend our work into conditional generation of 3D graphs, which stems from practical applications and presents a new challenge.  Namely the new challenge is to maintain conditional equivariance, that is, the generated 3D graphs should be E(3)-invariant to the property condition and E(3)-equivariant to the protein binder\u2019s structure condition. We addressed this challenge by introducing conditional equivariant geometric autoencoder in our latent diffusion pipeline (Section 3.3).\n- **Difference from GeoLDM.** We summarize the difference across different methods in Table 10, mainly lying in two perspectives: (i) latent dimension and (ii) the modeling of topological and geometric features.\n    - Latent dimension (LD). Assuming the feature dimension of data is $N \\times (D+3)$. At  one end of the spectrum, EDM diffusing in the original space is of the maximum LD $N \\times (D+3)$. The followup work, GeoLDM, diffusing in the latent space of nodes is of LD $N \\times (D\u2019+3)$. Guided by our theory in Section 3.2, we chase an even lower dimensional latent space by compressing the node factor $N$, that our method at the other end of the spectrum of the minimum LD $D\u2019$.\n    - Modeling of topological and geometric features. EDM, GeoLDM and our implemented one-shot model jointly model the topological and geometric features for all $N$ nodes. However, our one-shot model cannot manage to reconstruct the feature per node for its overly low latent dimension, that the node factor $N$ is compressed. Guided by our theory in Section 3.2, we develop the cascaded model, achieving satisfactory reconstruction performance while maintaining symmetry and sufficiently low LD."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3853/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541655133,
                "cdate": 1700541655133,
                "tmdate": 1700541655133,
                "mdate": 1700541655133,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mNQkvvjXJR",
            "forum": "cXbnGtO0NZ",
            "replyto": "cXbnGtO0NZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3853/Reviewer_kaPc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3853/Reviewer_kaPc"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an algorithm for diffusion-based 3D graph generation through latent space. This paper claims that low-dimensional projection is the key factor of the graph generation due to satisfy the equivariant property. The experiments are conducted using 3D molecules datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The writing is clear. The introduction section is wonderful. I clearly understand the scope of this work and what factors that the authors mainly addresses about. The meaning of the latent space and the 2D-3D autoencoder is well aligned in that the latent space could be understood as down-dimensional projection from 3D to 2D. Also, the experiments look okay and outperforms the previous works. \n\nHowever, I am not sure of the clear contribution of this work. Let me write down my questions and worries in the following sections."
                },
                "weaknesses": {
                    "value": "# W-1. Why latent space?\n\nAccording to this paper, I did not clearly catch how the authors project the 3D molecules into the 2D latent space. The technical details are missing. It naively mentioned that the authors borrow the architectures from previous works and such simple comment is not self-contained, in my opinion.\n\nHowever, let's assume that this issue is okay with me. Then, the following question is ... why the latent space is typically important to the graph generation? I know that the several recent studies utilize the latent space for the representation of the diffusion-based generative models. Nonetheless, I could not find any theoretical analysis behind the bridge between \n\n- _'the necessity of the latent space'_  and \n- _'the properties of the 3D graph representation, typically for the 3D molecules'_. \n\nFor me, this is naive extension of latent diffusion models into graph representation.\n\n# W-2. Overclaim.\n\nCan the authors exactly prove this equation in Sec 3 of this manuscript?\n\n_3D Graph Diffusion Performance <= Latent Space Reconstruction Quality + Symmetry Preservation * Data Dimensionality_\n\n # W-3. Lack of experiments\n\nIf the main contributions of this paper is about the analysis of the latent space into the 3D graph representation, I think that the authors should have provided the clear results or ablation study about the this contribution. \n\nFor instance, using the same baseline model from (Hoogeboom et al., 2022), the authors could slightly modify the architecture while maintaining the number of parameters for fair comparison. However, I could not find such kind of analysis or experiment."
                },
                "questions": {
                    "value": "Please address my concerns listed in Weakness section.\n\n# Q-1. Proposition 1\n\nWhile the authors describe full of equations with comments, the concept itself is highly straightforward. For instance in the manuscript, the authors said that '__Proposition 1.__ _Performance bound of 3D graph diffusion is related to feature dimensionality_'.\nIn my opinion, simply if we increase the network capacity by adding more layers or increasing the channel-length with lots of training data, the diffusion models surely have the performance gain. Honestly, I cannot catch the authors' intension from this propositions.\n\n# Q-2. Ablation study\n\nWhat if the authors intentionally omits the latent space encoding? I mean instead of using 2D-3D autoencoders, what happens if the network only consists of the 3D autoencoders? How much gain could we obtain if we adopt the latent space encoding?\n\n# Q-3. Clear difference between the previous works.\n\nCan the authors create one table to clearly demonstrate the difference? I read the paper (Hoogeboom et al., 2022) and it seems like this paper also split the geometry and topology for the diffusion process. Not just this factor itself, I hope to know the clear footsteps that this paper newly takes."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review required"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3853/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699261876702,
            "cdate": 1699261876702,
            "tmdate": 1699636343534,
            "mdate": 1699636343534,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WmfUR99UpS",
                "forum": "cXbnGtO0NZ",
                "replyto": "mNQkvvjXJR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3853/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3853/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kaPc (1)"
                    },
                    "comment": {
                        "value": "**Q1.** According to this paper, I did not clearly catch how the authors project the 3D molecules into the 2D latent space. The technical details are missing. It naively mentioned that the authors borrow the architectures from previous works and such simple comment is not self-contained, in my opinion.\n\n**Answer.**\n- We are sincerely grateful to the reviewer\u2019s feedback. We are committed to improving our clarity, and also would like to point out some aspects missed by the reviewers.\n- We add the detailed algorithm sections (Algorithms 1 & 2) to describe our latent 3D graph diffusion, beyond the overview Figures 2 & 3. They describe the processes of (conditional) in/equivariant latent encoding, latent diffusion, and sampling.\n- More specifically, the details of invariant latent encoding are further presented in Section 3.1.\n- The details of conditional in/equivariant latent encoding are presented in Section 3.3.\nRegarding latent encoding architectures, we supplement more detailed descriptions of architecture design in Appendix B.2 & C.1\n- The details of diffusion model are presented Section 2.\n\n**Q2.1.**  However, let's assume that this issue is okay with me. Then, the following question is ... why the latent space is typically important to the graph generation? I know that the several recent studies utilize the latent space for the representation of the diffusion-based generative models. Nonetheless, I could not find any theoretical analysis behind the bridge between\n'the necessity of the latent space' and\n'the properties of the 3D graph representation, typically for the 3D molecules'.\nFor me, this is naive extension of latent diffusion models into graph representation.\n\n**Answer.**\n- **Our work is not a naive extension of latent diffusion into 3D graphs.** We aim to generate 3D graphs of symmetry-group equivariance through diffusion. In other words, the quality of the generated graphs of topology and geometry is invariant / equivariant to a set of transformations (permutation group and Lie group E(3) of rotations and translations in our study).  This aim has deep roots in 3D molecular generation as demonstrated in our numerical experiments for desired quantum properties (invariant to E(3) perturbation of the 3D molecules) or desired protein binding (equivariant to E(3) perturbation of the 3D molecules); and it has broader application scopes for 3D graphs of symmetry-group equivariance in general.\n- Thus, directly related to the reviewer\u2019s question, **one contribution of our work is to demonstrate the necessity of building the 3D graph latent space** to achieve a potential generation quality. We provided theoretical analysis (Section 3.2) on what space should diffusion generative models operate on for generating 3D graphs of non-Euclidean structure and group-symmetry equivariance.  We addressed group-symmetry equivariance while deriving model performance bounds. \n    - Specifically, Proposition 1 gave the upper bound of the diffusion model performances (as measured in total variation distance) operating in the original 3D graph domain ($D\u2019$-dimensional);\n    - whereas Proposition 2 gave the upper bound in the latent space ($D\u2019\u2019$-dimensional). The latter led to a better upper bound of the diffusion model performances if reconstruction errors are low and the latent dimension is low, which motivates our approach of latent diffusion.\n- **Another contribution is to illustrate how to construct such 3D graph latent space (autoencoder), guided by our theoretical, motivational analyses**, of the following properties: (1) permutation and E(3) invariant and (2) sufficiently low reconstruction errors despite low latent dimension (these two present nontrivial trade-offs), which is under-explored.  We accomplished this by combining 2D graph autoencoder (for topology features) and point cloud autoencoder (for geometry features given topologies) in a cascade manner (Section 3.1).\n\n**Q2.2.**  While the authors describe full of equations with comments, the concept itself is highly straightforward. For instance in the manuscript, the authors said that 'Proposition 1. Performance bound of 3D graph diffusion is related to feature dimensionality'. In my opinion, simply if we increase the network capacity by adding more layers or increasing the channel-length with lots of training data, the diffusion models surely have the performance gain. Honestly, I cannot catch the authors' intension from this propositions.\n\n**Answer.**\n- We are sorry for the confusion. The feature dimensionality in Proposition 1 refers to the feature dimensionality of data, which does not relate to the network capacity. We revised the comment for better clarity. Please also see our last answer for the intention behind Proposition 1."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3853/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541174802,
                "cdate": 1700541174802,
                "tmdate": 1700541468201,
                "mdate": 1700541468201,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ejiBkWChaS",
                "forum": "cXbnGtO0NZ",
                "replyto": "k8iufIVj9n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3853/Reviewer_kaPc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3853/Reviewer_kaPc"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the rebuttals."
                    },
                    "comment": {
                        "value": "Overall, I understand the author's analysis of the latent graph design. Also, the authors responded to most of my questions. However, I still feel like this is a naive extension of latent diffusion into a graph structure. While there are so many analyses in terms of this topic, there are not that many technical contributions that bridge the authors' analysis and their model design. I think that this is quite a controversial topic, however, as a reviewer, I am not that positive with this paper. \n\nMoreover, I still disagree with the authors' opinion on \n\n- _3D Graph Diffusion Performance <= Latent Space Reconstruction Quality + Symmetry Preservation * Data Dimensionality_\n\nWhile the authors said that there is an exact proof in Appendix A, I cannot find any conclusion that mathematically proves the exact equation that I am concerned about. Even though I can guess the authors' intentions, this is an overclaim. For me, once the authors write down the equation within the paper, the equation itself should be formed in an exact and self-contained manner. For me, this is not a general talk or a simple presentation. This is a theoretical paper. In my opinion, such an equation is not a theoretical claim, which makes me unsatisfactory with this submission."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3853/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717467903,
                "cdate": 1700717467903,
                "tmdate": 1700717467903,
                "mdate": 1700717467903,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DJASRSOuJB",
                "forum": "cXbnGtO0NZ",
                "replyto": "mNQkvvjXJR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3853/Reviewer_kaPc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3853/Reviewer_kaPc"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the rebuttals."
                    },
                    "comment": {
                        "value": "I thank the authors for their endeavor in preparing the rebuttal. Through the rebuttal, I can further understand the exact meaning and insights that the authors proposed. However, __I am not that satisfied with several issues: (1) lack of technical contributions, (2) overclaim in \"The plain language equation\"(Q3), and (3) naive extension of latent diffusion into a graph structure. For some issues, the authors provide their opinions. However, it looks trivial for me. Also, motivation itself is not that easy to understand.__\n\nLet me keep my score as _\"5: marginally below the acceptance threshold\"_. Though I am not that positive with this paper, if the other reviewers think this is okay, let me follow their opinions. However, the manuscript needs lots of modification and needs to deal with the exact problem and novelty. Though the authors updated the algorithm 1, I could hardly find clear insight from this paper."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3853/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717922601,
                "cdate": 1700717922601,
                "tmdate": 1700718043279,
                "mdate": 1700718043279,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G3FZaDlvmz",
                "forum": "cXbnGtO0NZ",
                "replyto": "mNQkvvjXJR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3853/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3853/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Clarification to Contributions and Inequation"
                    },
                    "comment": {
                        "value": "Dear Reviewer kaPc,\n\nWe sincerely thank you again for your time and effort to review our paper and read our response. Here are our further clarifications to your questions:\n\n-------------------------------------\n\n- We respectfully disagree the (1) and (2) points of lacking technical novelty and lacking proof. We will illustrate this below.\n- **For (1) and (2)**: The inequation \"*3D Graph Diffusion Performance <= Latent Space Reconstruction Quality + Symmetry Preservation * Data Dimensionality*\" is **the informal expression of proposition 2**, which is **proved in Appendix A.2**. More specifically in proposition 2 (Eq. (3)):\n    - *3D Graph Diffusion Performance* denotes the left-hand-side of the inequation, measuring the statistical distance between learned distribution and data distribution;\n    - *Latent Space Reconstruction Quality* denotes the first term of the right-hand-side of the inequation, measuring the reconstruction error of the forward and backward mappings (i.e. reconstruction error of AE);\n    - *Symmetry Preservation* denotes the first multiplier of the second term of the right-hand-side of the inequation, measuring how AE preserves symmetry affecting the data-processing factor (see Appendix A.1 for details);\n    - *Data Dimensionality* denotes the second multiplier of the second term of the right-hand-side of the inequation, proportional to the dimension of data.\n\n\n-------------------------------------\n\n- We respectfully disagree for **the point (3)** of lacking novelty. Our innovations lie in four aspects, which are detailed in previous responses and briefly summarized below,\n    - Contribution (i) is exactly the technical rationale as we responded (Sec. 3.2);\n    - Contribution (ii) is also guided by the analysis, of how to build a qualified 3D graph AE (Sec. 3.1);\n    - Contribution (iii) is the extension of our latent diffusion pipeline to the **in/equivariant conditional generation** setting (Sec. 3.3). This is an entirely new exploration in the field;\n    - Contribution (vi) is the exploration of graph self-supervised learning for graph generative models, to improve the generalizability of graph generative AI (Sec. 3.1).\n\n-------------------------------------\n\nWe completely respect the dispute here, and also hope that our clarification will assist the reviewer in addressing the confusion more effectively."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3853/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719460372,
                "cdate": 1700719460372,
                "tmdate": 1700719557840,
                "mdate": 1700719557840,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EAlwbmeaO8",
            "forum": "cXbnGtO0NZ",
            "replyto": "cXbnGtO0NZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3853/Reviewer_oqpy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3853/Reviewer_oqpy"
            ],
            "content": {
                "summary": {
                    "value": "This study investigated the generation of 3D graphs from latent space using diffusion models and conditioned on different properties. The model is based on a cascaded 2D-3D graph autoencoders combined with diffusion models. The authors investigated the usefulness of generating 3D graphs from the latent space with symmetry preserved, and explored latent diffusion for conditional 3D graph generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written and the rational is straightforward and convincing. The authors have conducted multiple numerical experiments and the conditional generation based on various properties show great potentials of such method. In general I find this is an interesting paper."
                },
                "weaknesses": {
                    "value": "Please see my questions below."
                },
                "questions": {
                    "value": "The authors trained the topological AE and the geometric AE separately with different constrains. I understand the difficulty in training them in one shot, but I'm wondering the influences from each of the AE, e.g., would it be possible information about the topology is lost while training using the geometric AE? If so, how much is lost/kept?\n\nWhile GSSL improves the results on OOD, it seems it can worse the results on ID. Can the authors provide more insights on this?\n\nConsidering one utility of the model is for generating new drugs, model interpretability could be important. In the latent space, would it be possible for the authors to provide some visualization on the learned features and if available, colored by some topological and geometric features of the graph? If there are certain patterns there then it may be useful to help better understand what features are mostly kept in the latent space and enhance the interpretability of the model."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3853/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699480759429,
            "cdate": 1699480759429,
            "tmdate": 1699636343456,
            "mdate": 1699636343456,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Co5eyaxTXE",
                "forum": "cXbnGtO0NZ",
                "replyto": "EAlwbmeaO8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3853/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3853/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oqpy"
                    },
                    "comment": {
                        "value": "**Q1.** The authors trained the topological AE and the geometric AE separately with different constrains. I understand the difficulty in training them in one shot, but I'm wondering the influences from each of the AE, e.g., would it be possible information about the topology is lost while training using the geometric AE? If so, how much is lost/kept?\n\n**Answer.**\n- In the cascaded model, the geometric AE still preserves topology information, and the topological AE does not have access to geometry information.\n- Specifically, the model (i) performs autoencoding on topological features, and then (ii) given the topological features as inputs, performs autoencoding on geometric features.\n- The information loss in the topological AE is minor to our pipeline, as quantified with the reconstruction performance shown in Table 1 and Appendix B.2.\n\n**Q2.** While GSSL improves the results on OOD, it seems it can worsen the results on ID. Can the authors provide more insights on this?\n\n**Answer.**\n- We provide additional results in Tables 15 & 16, on applying GSSL for unconditional generation and conditional generation with less training data.\n- Based on our experiments, we observed that the benefit of GSSL is mainly on:\n    - OOD scenarios when the GSSL prior is aligned with the condition semantics, as we stated in Section 4.2;\n    - Unconditional or ID scenarios when training data are insufficient. This is consistent with the role of GSSL in discriminative modeling (https://arxiv.org/abs/2010.13902).\n- For the OOD scenarios, though the improvement does not appear for all datasets, we know when it is improved: Specifically, we can measure the semantics-alignment by computing the homogeneity ratio as detailed in Appendix E.2. This indicates a future potential to design semantics-specific GSSL tasks for generative modeling.\n\n**Q3.** Considering one utility of the model is for generating new drugs, model interpretability could be important. In the latent space, would it be possible for the authors to provide some visualization on the learned features and if available, colored by some topological and geometric features of the graph? If there are certain patterns there then it may be useful to help better understand what features are mostly kept in the latent space and enhance the interpretability of the model.\n\n**Answer.**\n- We provided the latent embedding visualization in Figures 11 & 12.\n- Specifically, we perform t-SNE (https://jmlr.org/papers/v9/vandermaaten08a.html) on topological and geometric embeddings, and annotate them with four 2D and six 3D property values of molecules.\n- Our visualization shows information of 2D properties is more preserved in topological embeddings, that data with similar properties tends to cluster. In addition, information of 3D properties is more preserved in geometric embeddings.\n- The observation is also confirmed by quantitative evaluation, by computing the silhouette score of the latent embeddings w.r.t. properties."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3853/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542765830,
                "cdate": 1700542765830,
                "tmdate": 1700542765830,
                "mdate": 1700542765830,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0qUxIZlYUb",
            "forum": "cXbnGtO0NZ",
            "replyto": "cXbnGtO0NZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3853/Reviewer_N14e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3853/Reviewer_N14e"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the appropriate latent space for generating 3D graphs. The authors derive several conclusions through theoretical analysis:\n\n- The lower the dimensionality of the latent space, the higher the performance limit of diffusion.\n\n- Higher quality of the latent space (i.e., lower reconstruction error) corresponds to a higher performance limit of diffusion.\n\n- Preserving symmetry (maintaining graph properties after translation, rotation, etc.) is an inductive bias of the latent space, contributing to an increased performance limit of diffusion.\n\nGuided by these theoretical insights, the authors propose a method named \"latent 3D graph diffusion.\" This approach utilizes cascaded 2D-3D graph autoencoders to learn a latent space with low-error reconstruction (learning topological graphs) and symmetry invariance (learning geometric graphs). Furthermore, the authors extend this method to conditional generation given SE(3) invariant properties (rotation-translation invariance) or equivariant 3D objects.\n\nExperimental results demonstrate that appropriate regularization of the latent space through graph self-supervised learning can further enhance the robustness of conditional generation. The comprehensive experimental findings indicate that, compared to existing competitive methods, this approach can generate 3D molecular graphs with enhanced effectiveness/drug similarity and is at least an order of magnitude faster in diffusion training. The speed advantage increases with the size/complexity of molecules."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Demonstrates superior generation quality, rapid generation capabilities, and conditional generation proficiency in 3D graph generation, while enhancing robustness through regularization."
                },
                "weaknesses": {
                    "value": "- Limited generalization capability in comparison."
                },
                "questions": {
                    "value": "- How to improve generalization capability?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3853/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3853/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3853/Reviewer_N14e"
                    ]
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3853/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699915266253,
            "cdate": 1699915266253,
            "tmdate": 1699915266253,
            "mdate": 1699915266253,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mcKngdNGPu",
                "forum": "cXbnGtO0NZ",
                "replyto": "0qUxIZlYUb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3853/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3853/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer N14e"
                    },
                    "comment": {
                        "value": "**Q1.** Limited generalization capability in comparison. How to improve generalization capability?\n\n**Answer.**\n- We provide additional results in Tables 15 & 16, on applying GSSL for unconditional generation and conditional generation with less training data.\n- Based on our experiments, we observed that the benefit of GSSL is mainly on:\n    - OOD scenarios when the GSSL prior is aligned with the condition semantics, as we stated in Section 4.2;\n    - Unconditional or ID scenarios when training data are insufficient. This is consistent with the role of GSSL in discriminative modeling (https://arxiv.org/abs/2010.13902).\n- Our results indicate a future potential to design semantics-specific GSSL tasks for generative modeling."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3853/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542624448,
                "cdate": 1700542624448,
                "tmdate": 1700542624448,
                "mdate": 1700542624448,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]