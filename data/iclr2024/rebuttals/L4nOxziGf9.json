[
    {
        "title": "Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models"
    },
    {
        "review": {
            "id": "wKRcZg36pl",
            "forum": "L4nOxziGf9",
            "replyto": "L4nOxziGf9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6424/Reviewer_KySW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6424/Reviewer_KySW"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a pipeline framework for improving the accuracy of LVLMs in VQA tasks.\nFirst, details are identified from the question and the image, including entities (using off-the-shelf tools) and rationales (using the LVLM), which are then used to obtain related details from the image (using the LVLM), and descriptions from the image (using the LVLM).\nSecond, the details are fused with the question, generating multiple question candidates (using the LVLM), the results of which are filtered to exclude semantically inconsistent ones (using off-the-shelf tools).\nFinally, the LVLM answers each candidate question; the final answer is selected using the LVLM with the confidence-based method.\n\nOn two benchmark sets, i.e., VQAv2 and A-OKVQA, solid improvements are shown on 3 LVLMs. The with/without kind of ablation is conducted suggesting each element in the pipeline contributes to the improvement. Further analyses are conducted to try to show that the new questions are effective because of less underspecification and ambiguity, i.e., because of the added details that are obvious for humans but not quite so for LVLMs. The asymmetric strength hypothesis seems to suggest that the visual components alone are not quite up to the job of the VQA task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The motivation is straight-forward, clear, and reasonable.\n- The improvements seem solid and the analyses support the improvements that come from the proposed pipeline.\n- The code and the data are provided and the authors promised public release. This is very important because the proposed pipeline is complicated and not easily reproducible."
                },
                "weaknesses": {
                    "value": "- The pipeline seems overcomplicated and involves many steps that are indispensable to the overall performance. Unlike plain CoT, which usually conducts inference once, the proposed pipeline conducts inference multiple times using the LVLM and involves off-the-shelf tools twice. The complexity may affect the reproduction of the method and the incurred (computation and time) cost may hinder the adoption of the method in application.\n- The main results (Table 1) need more explanation. (1) For example, the standard deviation considering the oracle implementation is high. I did not expect that using the optimal candidate would lead to higher variance. Are there results regarding the choice of the number of the question candidates? (2) I would love to see more QA datasets (from diverse sources) tested on. (3) I wonder if the asymmetric strength hypothesis holds, is it possible that stage 2 and stage 3 can be changed to using the original question and the extracted details in texts without the image? (4) All analyses are based on BLIP-2, an encoder-decoder model. I don't think I find a discussion on the effect of model architecture (encoder-decoder or decoder-only).\n- The writing and the organization can be improved. Personally, I would like Section 3 Methodology to be more straightforward. From what I understand, Stage I (ii) adopts different post-processing from (i) and (iii). The paragraph before Section 3.2 states \"we prompt the LVLM to answer each question\" and the first paragraph in Section 3.2 states \"To select which question to answer\", which are contradictory. I had to check the appendix and the footnotes multiple times to guess what's going on."
                },
                "questions": {
                    "value": "Please see the numbered points in weaknesses and comments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6424/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6424/Reviewer_KySW",
                        "ICLR.cc/2024/Conference/Submission6424/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6424/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756444083,
            "cdate": 1698756444083,
            "tmdate": 1700629170441,
            "mdate": 1700629170441,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6CdQxzpy6x",
                "forum": "L4nOxziGf9",
                "replyto": "wKRcZg36pl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6424/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6424/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KySW (Weaknesses and Clarifications)"
                    },
                    "comment": {
                        "value": "We thank you for a detailed review of our paper and helpful suggestions as well as highlighting our \u201csolid\u201d improvements and supporting analysis. We address your comments below.\n\n**On complexity of our pipeline**  \n* **Applicability of CoT:** While our system contains multiple components, we demonstrate through our ablations that (as you point out) each component is necessary. We experimented with CoT approaches, finding that LVLMs generally did not benefit. This is in line with prior work highlighting the limited applicability of in-context learning ([Li et al. 2023](https://arxiv.org/pdf/2301.12597.pdf), Section 5 (1st paragraph) and, [Huang et al. 2023](https://arxiv.org/pdf/2302.14045.pdf), Table 13 shows little to no improvement from CoT across multiple tasks). In our setting, in-context learning is a required ability for CoT, since we need to give the model examples of the types of CoTs we would like it to generate (as is common practice, e.g. [Lu et al. 2023](https://arxiv.org/pdf/2209.09513.pdf), Fig. 5). In our case, the addition of multiple images into the context led the model to answer questions about the in-context images, rather than the test image. \n* **Comparison of token usage of RepARe with CoT:** Disregarding poor performance of CoT (as discussed in Sec 2 paragraph 2 on LVLMs of our original submission), we have additionally collected estimates for the number of tokens shown to models with RePARe and with CoT, using a reasonable number of demonstrations (8). Here, RepARe requires between 430-550 LVLM tokens per task instance on an average, while CoT requires ~460 LVLM tokens on average. Given that some of the modules in RepARe can be parallelized, we contend that RepARe is comparable to CoT with in-context learning in terms of efficiency (measured in LVLM tokens) while having the added benefit of strong performance. We note that the computational cost of the off-the-shelf tools we use is negligible compared to the LVLM (which would be shared by CoT approaches). We also add this clarification to Sec 5\u2019s limitations paragraph.\n\n**Addressing writing clarity**\nWe have updated Sec 3 to improve clarity, focusing on the areas you have mentioned. First, we have split the stages up differently (see new Fig. 2). For 3.1, we have clarified what the output from the stage is. We have reordered \u201csalient question entities\u201d and \u201cinformation from rationales\u201d to be grouped together to improve clarity, and have added details in the last paragraph under \u201cStage I(a)\u201d. We have also added clarification to Stage I(b) and Section 3.2 to indicate that what we are scoring in 3.2 is in fact question-answer pairs, not just questions or answers. Additionally, we provide an exhaustive list of prompts used in this work and describe their usage within each module in detail in updated A.3 for further clarification. We hope that this addresses any lack of clarity in the prior version of the section."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6424/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700268044689,
                "cdate": 1700268044689,
                "tmdate": 1700268044689,
                "mdate": 1700268044689,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XS0IzHAo6Z",
                "forum": "L4nOxziGf9",
                "replyto": "X5IhdX2qRX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6424/Reviewer_KySW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6424/Reviewer_KySW"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "I thank the efforts made by the authors for rebuttal. The rebuttal generally solves my concerns. I raise my score to 6."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6424/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629117756,
                "cdate": 1700629117756,
                "tmdate": 1700629117756,
                "mdate": 1700629117756,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hcbaENDxKB",
            "forum": "L4nOxziGf9",
            "replyto": "L4nOxziGf9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6424/Reviewer_j2wD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6424/Reviewer_j2wD"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an pipeline for using LVLMs to solve VQA that modifies the question using visual information from the image and select the answer with the best confidence. Experiment results on VQAc2 and A-OKVQA show improvement in all question categories. The authors use ablation studies and shown that the visual details and question entity do help improve the performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The performance improvement from the method seem to be solid.\n2. The paper is mostly clear with extensive experiments."
                },
                "weaknesses": {
                    "value": "1. Some of the text requires further clarification.\n2. The underlying hypothesis should be stated more clearly, which in my understanding is that distribution of more specified questions are more aligned with the training data of LVLMs and the answers with higher confidence are more likely to be correct."
                },
                "questions": {
                    "value": "1. Any explanations on why questions with numeric answers benefit the most from RepARe?\n2. What does it mean using `golden` answers for selection? And also `paraphrase oracle`.\n3. In 4.4, why is that \"In all cases, REPARE increases the gap between LLM-only and BLIP-2\"? As it seems that the gap actually decreases. (The numbers should be negative)\n4. The baseline of adding captions alone to the question, i.e. <caption><question>, should be compared."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6424/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6424/Reviewer_j2wD",
                        "ICLR.cc/2024/Conference/Submission6424/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6424/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809997921,
            "cdate": 1698809997921,
            "tmdate": 1700527386745,
            "mdate": 1700527386745,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GDzOgIRHwD",
                "forum": "L4nOxziGf9",
                "replyto": "hcbaENDxKB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6424/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6424/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer j2wD"
                    },
                    "comment": {
                        "value": "Thank you for highlighting our \u201csolid\u201d performance improvements, \u201cextensive experiments\u201d as well your insightful questions and comments. We include our detailed responses below.\n\n**On underlying assumptions.** Thank you for raising this issue. To clarify, our assumptions/hypotheses are regarding the strengths/weaknesses of models, rather than about the alignment to the pre-training data. Our core assumptions are that \n1. LVLMs generally have a stronger LLM component than visual encoder (asymmetric strength)  \n2. LVLMs are better at tasks like captioning than QA (asymmetric abilities)\n3. Finally, we assume that we can use answer confidence to rank answers (and thus, implicitly rank the rewritten questions that generated them). \n\nWe see in our results that these assumptions are borne out (updated Table 5, where we see a large boost to the LLM\u2019s performance from RepARe and from using generated captions, and Table 2, where we ablate the scoring method). We have updated our introduction (Sec 1, page 3) as well as Sec 5 to more clearly state this. Additionally, for the third assumption we experiment with alternate ways of computing answer confidence and find that it does not significantly impact downstream task performance of RepARe (Table 8 updated). \n\n**Q1: On numeric improvement.** We qualitatively examined the best candidate questions for numeric examples. We found that correctly-answered questions often include additional localization. For example, rather than answering *\u201chow many teddy bears are in the photo?\u201d* the model would answer *\u201chow many teddy bears are on the table?\u201d*. We hypothesize that this kind of additional localization helps the LVLM focus on the correct image regions, simplifying the overall task. \n\n**Q2: Clarification of terms.** We apologize for the confusion. By *gold answers*, we are referring to the ground-truth (gold) answers contained in the datasets. The oracle setting (in the original Sec 3.2, now highlighted) refers to using the ground-truth answers to select the question candidate, i.e. choosing the question that yields the correct answer. However, such information is not available at test time. Similarly, the *paraphrase oracle* (in original Section 4.2, now highlighted) uses the ground-truth answers to select a candidate from a pool of paraphrases of the original question. This setting is designed to set an upper-bound on improvements from rephrasing/paraphrasing by directly using the ground-truth answer. \n\n**Q3 & Q4: Revisions to Sec 4.4.** We understand the source of your confusion in interpreting Table 5. In the original table, we had endeavored to show that RepARe\u2019s improvements are largely, but not exclusively, coming from the LLM. We have since overhauled the table and the bulk of Sec 4.4 to convey this message more clearly, highlighting the following key points in our updated paper:\n\n* RepARe improves LLM-only performance showing that the modified questions leverage the asymmetric QA strength of the LLM.\n* Despite the improvement in LLM-only performance, there remains a substantial gap in performance with and without the image (~25% points). Therefore, the modified question is *complementary* to the image and does not make answering the question trivial without the image.\n* Following your suggestion, we include a baseline in which the caption and the original question (excluding the image) are given to the LLM and find that while this setting outperforms the question-only baselines, it still trails behind the image + original question and image + RepARe settings. This shows that RepARe incorporates visual information that is *complementary to the question and the caption* (added as row 5 of updated Table 5).  \n\nWe hope that our changes have satisfactorily addressed your concerns. Please feel free to reach out to us with any additional questions and writing suggestions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6424/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700267990524,
                "cdate": 1700267990524,
                "tmdate": 1700267990524,
                "mdate": 1700267990524,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nF0u8eiTZP",
                "forum": "L4nOxziGf9",
                "replyto": "GDzOgIRHwD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6424/Reviewer_j2wD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6424/Reviewer_j2wD"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response and the new results. I will raise the score to 6. However, I still have doubt on the effectiveness of the proposed method compared to captioning. For the caption baseline, I actually mean the results when picture is included, which is picture+caption+original question."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6424/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527368536,
                "cdate": 1700527368536,
                "tmdate": 1700527368536,
                "mdate": 1700527368536,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ndBQp2mfUP",
            "forum": "L4nOxziGf9",
            "replyto": "L4nOxziGf9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6424/Reviewer_Jt1P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6424/Reviewer_Jt1P"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces \"Repare\", a gradient-free framework that consists of three phases: Visual Details Extraction, Question Rephrasing and Augmentation, and Question Selection. \"Repare\" rephrases questions with more precise information, drawing from image captions, question keywords, and rationale details. Following rephrasing, Repare produces multiple question variations, which are then filterd through an unsupervised quality score provided by a Language Model (LM). The paper concludes with an assessment of Repare's performance on VQAv2 and A-OKQA datasets, showing enhancements when compared to the BLIP2 and MiniGPT4 baseline models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is easy to read and generally well-written\n- Interesting idea of improving VL models in VQA tasks by just modifying one modality (e.g text).\n- Improvements over baselines (BLIP2, MiniGPT4) looks reasonable."
                },
                "weaknesses": {
                    "value": "- Evaluation suite should be improved. For example including: TextVQA, VizWiz. Additionally, authors should consider evaluating tasks such as HatefulMemes which might be more challenging to the proposed approach. Also, consider recent evaluation tasks such as MME.\n- Including recent instruct multimodal models (e.g. LLaVa, Qwen-VL) would be an interesting experiment to see if the gain with Repare is still relevant in these models.\n- Minor:\n    - >One approach involves additional VL pretraining ...\n   \n         In this case, LENS does not involve additional multimodal pretraining.\n  - Why not include BLIP2-XXL as one of your main baselines and improve it with Repare?"
                },
                "questions": {
                    "value": "Please take a look at the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6424/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699504385164,
            "cdate": 1699504385164,
            "tmdate": 1699636716346,
            "mdate": 1699636716346,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xh28liT2kc",
                "forum": "L4nOxziGf9",
                "replyto": "ndBQp2mfUP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6424/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6424/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Jt1P"
                    },
                    "comment": {
                        "value": "Thank you for your detailed feedback and helpful comments. We appreciate that you find our idea of focusing on the text modality to improve VQA performance interesting. In addition to the general response, we have addressed your comments below.\n\n**Improving evaluation suite.** Your comments about expanding the evaluation framework by including more datasets and models are well-taken. Following your suggestion, we have evaluated the VizWiz dataset in Table 1 (updated) and also included performance of LLaVA-1.5 on all 3 datasets in Table 1 (updated) using the Vicuna-7B as the underlying LM. Note that much like TextQA, a considerable portion of VizWiz questions also require the model to utilize the text in the images.  Also taking your feedback into account, we have added BLIP2-XXL to our evaluation.\n\n**Takeaways:** Results in Table 1 (updated) indicate that our method RepARe proves to be effective on VizWiz improving accuracy by up to 7.94% (absolute points) for MiniGPT-4 models and yields 7.61% absolute improvement in accuracy on the A-OKVQA dataset with the LLaVA-1.5 model (with additional tuning experiments running). These improvements show that RepAre is robust to poor image quality, as VizWiz images are often blurred, under/over-exposed, or rotated ([Gurari et al. 2018](https://arxiv.org/pdf/1802.08218.pdf)) since they were collected by visually-impaired people on mobile devices. Lastly, we show that RepARe is also effective at improving performance of BLIP-2 XXL in Table 1 by 3.84%, 5.47%, and 3.46% on VQAv2, A-OKVQA (direct), and VizWiz datasets respectively. In addition to Table 1 (updated), we discuss these promising results in the revised Sec 4.1 (highlighted).\n\n**Addressing \u201cminor\u201d comments.** We understand the confusion caused by citing LENS in the introduction, however our objective was to reiterate LENS\u2019s motivation about additional pre-training costs of Q-former-like models despite frozen image encoders and language models. We have now removed this citation from the sentence in question to avoid confusion.\n\nWe hope that these additional experiments address your concerns. Please let us know if you have any follow up questions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6424/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700267975541,
                "cdate": 1700267975541,
                "tmdate": 1700267975541,
                "mdate": 1700267975541,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]