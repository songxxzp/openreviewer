[
    {
        "title": "LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning"
    },
    {
        "review": {
            "id": "5IKgjCov8i",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3536/Reviewer_wXxt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3536/Reviewer_wXxt"
            ],
            "forum": "RbKThNNFxr",
            "replyto": "RbKThNNFxr",
            "content": {
                "summary": {
                    "value": "This paper provides a minor but meaningful tweak to LoRA, by freezing the A matrix, which has notable benefits in improving training memory. The paper provides both theoretical and empirical justifications for why LoRA-FA achieves close to LoRA performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The method is simple, well-explained, and well-justified\n- The experiments are fairly comprehensive (though as detailed in weaknesses, still leave some crucial questions unanswered)"
                },
                "weaknesses": {
                    "value": "- The writing of the paper at times leans towards over-claiming or making unsuitable comparisons to bolster their method. For example, in section 1 and in other spots, the memory of LoRA-FA is compared to full fine-tuning but not LoRA. Given that LoRA-FA is fundamentally a tweak of LoRA, the natural comparison should be to LoRA, not the full fine-tuning just to make the numbers look more impressive. To the authors: the benefits of LoRA-FA are moderate but clear; there is not need to oversell the method.\n- The crux of the evaluation lies in whether LoRA-FA underperforms LoRA, or performs comparably with reduced memory consumption. While the evaluations in the paper are quite comprehensive (spanning 3 model families), I think the current experiment still fall short of resoundingly answering this crucial question, and I would like to see the authors run a set of experiments to address this. To put this more explicitly: to determine if LoRA-FA underperforms LoRA (or not), we need a setting where LoRA is \"capacity-constrained\", to determine if LoRA-FA has even less \"capacity\" than LoRA. To do this, we need a setting where LoRA meaningfully underperforms full fine-tuning. For both the RoBERTa and T5 experiments, this is not the case. The LLaMA-7B experiments on Alpaca/FLAN -> MMLU come the closest to this, but the margin is still too small to tell (and essentially no gap at all in the case of Alpaca) (more broadly, MMLU is not a fine measure of LM performance). I can recommend the authors run a set of experiments on something like Super-NaturalInstructions, where there is likely to be a bigger gap (since the evaluation is performed on generated sequences rather than simple multiple-choice knowledge questions)."
                },
                "questions": {
                    "value": "My questions are detailed in the weaknesses section above (testing exactly where/how much capacity is lost between LoRA and LoRA-FA). I am willing to update my score given more experiments addressing my question."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3536/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697227473159,
            "cdate": 1697227473159,
            "tmdate": 1699636307723,
            "mdate": 1699636307723,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "baNt1ZhydU",
                "forum": "RbKThNNFxr",
                "replyto": "5IKgjCov8i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3536/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3536/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the positive and valuable comments.\n\n**Limitation 1**: The writing of the paper at times leans towards over-claiming or making unsuitable comparisons to bolster their method.\n\n**Response**: Thank you for your insightful comment, we will refactor the paper to make it more impartial.\n\n**Limitation 2**: The crux of the evaluation lies in whether LoRA-FA underperforms LoRA, or performs comparably with reduced memory consumption. While the evaluations in the paper are quite comprehensive (spanning 3 model families), I think the current experiment still fall short of resoundingly answering this crucial question, and I would like to see the authors run a set of experiments to address this.\n\n**Response**: Thank you for your kind guidance. To address this problem, we conduct the  Llama2-7B fine-tuning on Super-NaturalInstructions dataset. Super-NaturalInstructions is a challenging dataset for LLMs. Specifically, we use the task001_quoref_question_generation.json and task002_quoref_answer_generation.json as the training set, and set the sequence length to 2048, batch size to 4, train epochs to 5. We report eval_loss of 3 learning rates: [7e-5, 1e-4, 4e-4] as follows.\n\n| Learning rate | 7e-5 | 1e-4 | 4e-4 |\n|---------------|------|------|------|\n| LoRA          | 0.45 | 0.53 | 4.27 |\n| LoRA-FA       | 0.45 | 0.47 | 0.51 |\n\nIn the results, both LoRA and LoRA-FA can achieve similar eval_loss. When $lr=4e-4$, LoRA didn't perform well because the learning rate is too large for LoRA. Overall, LoRA-FA shows strong capability similar to LoRA when fine-tuning on generalization tasks."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3536/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507781187,
                "cdate": 1700507781187,
                "tmdate": 1700507781187,
                "mdate": 1700507781187,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9CAk8xLb1V",
                "forum": "RbKThNNFxr",
                "replyto": "baNt1ZhydU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3536/Reviewer_wXxt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3536/Reviewer_wXxt"
                ],
                "content": {
                    "comment": {
                        "value": "- As mentioned in my original comment, the experiment needs to be isolate a case where LoRA underperforms full fine-tuning. As such, we will also need full fine-tuning results to compare to.\n- I don't believe this is the standard application of the Super-NaturalInstructions dataset. The S-NI dataset is intended to be a large multi-task dataset, with many training tasks, and evaluation on separately held-out tasks, to determine the generalization capability of a model. As such, fine-tuning on just two tasks (and it is not clear what the evaluation tasks here are) isn't quite the appropriate setup."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3536/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642966606,
                "cdate": 1700642966606,
                "tmdate": 1700642966606,
                "mdate": 1700642966606,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8cxOrxID6D",
            "forum": "RbKThNNFxr",
            "replyto": "RbKThNNFxr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3536/Reviewer_XvzM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3536/Reviewer_XvzM"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce to reduce for activation memory during fine-tuning, called LoRA-FA. LoRA-FA introduces a memory-efficient approach by freezing certain weights of LoRA layers, significantly reducing activation memory without compromising performance or incurring additional computational costs. Experiments across various model types and scales, including RoBERTa, T5, and LLaMA, demonstrate that LoRA-FA consistently maintains fine-tuning accuracy while reducing overall memory costs by up to 4x and 1.4x compared to full-parameter fine-tuning and LoRA, respectively. Additionally, LoRA-FA is compatible with advanced memory optimization methods like FlashAttention, QLoRA, and ZeRO."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* When comparing activation memory among various existing fine-tuning methodologies, it is evident that LoRA does not significantly reduce activation memory compared to full fine-tuning.\n* In this paper, the authors propose a simple yet effective approach to reduce activation memory size during the fine-tuning phase by selectively freezing certain portions of the LoRA adaptation layer.\n* The experimental findings demonstrate that the proposed LoRA-FA achieves performance comparable to LoRA across various large language models (LLMs), including LLaMA, T5, and RoBERTa, in downstream tasks.\n* The study highlights the compatibility of LoRA-FA with advanced memory optimization techniques such as FlashAttention, QLoRA, and ZeRO."
                },
                "weaknesses": {
                    "value": "* The discussion on the benefits of reduced activation memory through LoRA-FA is lacking. In the finetuning phase, unlike the inference phase, both sequence length and batch size are longer, resulting in high GPU utilization. Therefore, it is not considered practically significant to make LoRA-FA more memory efficient than the existing LoRA.\n* I think efficient finetuning becomes more crucial as the model size increases. Therefore, it is necessary to demonstrate that as the model size grows, it shows performance similar to LoRA. However, the models experimented in the paper were limited to sizes up to 7B, which is relatively small. The trends in models larger than 7B remain unknown.\n* As I consider LoRA to be a comprehensive methodology that encompasses LoRA-FA, it is difficult to anticipate that LoRA-FA would exhibit better accuracy than LoRA.\n* The cost required for inference after finetuning is completed is the same for both LoRA and LoRA-FA."
                },
                "questions": {
                    "value": "* Is there any performance difference based on the initialization method for the LoRA adaptation layer A?\n* When integrating the proposed LoRA-FA into QLoRA, how does it impact the performance of CSR or MMLU in models such as LLaMA, RoBERTa, and T5? While Section 4 of the experiment results demonstrates the approach of freezing adaption layer A in various LLMs' LoRA, including QLoRA, there seems to be a mention of potential application without concrete results on the performance after actual finetuning.\n* When looking at Figure 4 in the Appendix, it appears that LoRA and LoRA-FA exhibit a similar trend in TFLOPS. What are the benefits gained during the Finetuning phase by actually reducing activation memory?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3536/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3536/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3536/Reviewer_XvzM"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3536/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773440215,
            "cdate": 1698773440215,
            "tmdate": 1699636307638,
            "mdate": 1699636307638,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Biu7TSXKGu",
                "forum": "RbKThNNFxr",
                "replyto": "8cxOrxID6D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3536/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3536/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the feedback.\n\n**Question 1**: Is there any performance difference based on the initialization method for the LoRA adaptation layer A?\n\n**Response**: We discussed the performance difference of three different initialization methods for the LoRA adaptation layer A in Appendix B.3.1.\n\n**Question 2**: When integrating the proposed LoRA-FA into QLoRA, how does it impact the performance of CSR or MMLU in models such as LLaMA, RoBERTa, and T5? While Section 4 of the experiment results demonstrates the approach of freezing adaption layer A in various LLMs' LoRA, including QLoRA, there seems to be a mention of potential application without concrete results on the performance after actual finetuning.\n\n**Response**: The fine-tuning with QLoRA is still on-going, we will release the result ASAP.\n\n**Question 3**: When looking at Figure 4 in the Appendix, it appears that LoRA and LoRA-FA exhibit a similar trend in TFLOPS. What are the benefits gained during the Finetuning phase by actually reducing activation memory?\n\n**Response**: Thank you for pointing out the issue! We have corrected the results in Table 5, it was because activation recomputation was turned on when conducting memory benchmark experiments. We ensure all of the configures are right and re-conducted the experiment. Specifically, we fine-tuning Llama2-7B with sequence length = 1024. In the new results, we have several findings:\n\n1. LoRA-FA reduces ~13GB memory compared to LoRA in fine-tuning Llama2-7B when batch size is 16.\n2. In practice, when fine-tuning Llama2-7B on various hardware, LoRA-FA has better capability than LoRA. For example, on RTX4090-24G, LoRA can only fine-tuning Llama2-7B when $batch size \\leq 2$, while LoRA-FA can enlarge the batch size to 4; on A100-40G, LoRA works with batch size = 4, while LoRA-FA works with batch size = 8.\n\nWe also give a runtime memory breakdown analysis of fine-tuning Llama2-7B as follows when batch size is 16, to figure out the memory usage percentage of different modules, such as optimizer state and activation memory.\n\n|                    | LoRA        |                | LoRA-FA     |                |\n|:------------------:|:-----------:|:--------------:|:-----------:|:--------------:|\n|                    | Memory (GB) | Percentage (%) | Memory (GB) | Percentage (%) |\n| Hardware researved | 0.7         | 0.9            | 0.7         | 1.1            |\n| Pytorch researved  | 0.5         | 0.6            | 0.5         | 0.8            |\n| Model in bf16      | 12.6        | 17             | 12.6        | 20.3           |\n| LoRA module        | 0.8         | 1.1            | 0.8         | 1.4            |\n| Cache              | 5.1         | 6.7            | 4.8         | 7.7            |\n| Optimizer state    | 0.7         | 0.9            | 0.4         | 0.4            |\n| Activation         | 55.6        | 73.1           | 42.3        | 68.2           |\n\nThis result shows that LoRA-FA can significantly reduce the activation memory usage compared to LoRA when fine-tuning Llama2-7B (from 55.6 GB to 42.3 GB). This advantage can either help fine-tuning on consumer level GPUs, or can achieve higher throughput by enlarging batch size."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3536/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507618222,
                "cdate": 1700507618222,
                "tmdate": 1700507618222,
                "mdate": 1700507618222,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Gcy7qWzn4k",
                "forum": "RbKThNNFxr",
                "replyto": "8cxOrxID6D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3536/Reviewer_XvzM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3536/Reviewer_XvzM"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed answers and results.\n\nSome of my concerns have been addressed, but i still my concern\u00a0that using LoRA-FA for finetuning can enhance training throughput. I agree with the effectiveness of LoRA-FA in efficiently reducing activation memory, considering it a significant strength of the paper.\n\nHowever, during the fine-tuning phase, it is common to have significantly larger batch sizes and sequence lengths compared to inference, resulting in an exponential increase in time complexity over space complexity. As a result, fine-tuning becomes bound by computation, and in my view, the proposed method does not effectively reduce computation costs during the fine-tuning process, even though it minimizes activation memory.\n\nAs a result, the proposed method, in my understanding, does not reduce computation costs during fine-tuning, even if activation memory is minimized.\n\nHence, I believe the benefits in terms of throughput during the fine-tuning phase, compared to LoRA, may be limited. I consider the experimental results in Figure 4 in the Appendix as evidence supporting this view.\n\nIf there were more convincing explanations for this aspect, I would be willing to reconsider and potentially raise my score. However, lacking sufficient clarity on this matter, I would like to keep my rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3536/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667042314,
                "cdate": 1700667042314,
                "tmdate": 1700667123694,
                "mdate": 1700667123694,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "af6nMT4O4m",
            "forum": "RbKThNNFxr",
            "replyto": "RbKThNNFxr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3536/Reviewer_s6YB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3536/Reviewer_s6YB"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents LoRA-FA, a new parameter-efficient fine-tuning (PEFT) approach for Large Language Models (LLMs). LoRA-FA is an extension of the LoRA method which minimizes memory usage by freezing the A matrix in LoRA layers. This approach alleviates the need to store the input activation of the LoRA layer, leading to a reduction in memory footprint during fine-tuning. The authors provide comprehensive experimental evidence showcasing the efficacy of LoRA-FA across various LLMs including RoBERTa, T5, and LLaMA. The results demonstrate LoRA-FA's memory savings without compromising on fine-tuning performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The proposed LoRA-FA method cuts down GPU memory usage by reducing both the number of trainable parameters and the activation memory compared to traditional full fine-tuning.\n* Comprehensive experimental results on diverse models, including RoBERTa, T5, and LLaMA, demonstrate that LoRA-FA maintains competitive accuracy relative to both full fine-tuning and the original LoRA.\n* The paper lucidly presents the background of parameter-efficient fine-tuning and proposes an expanded method that inherits the advantages of the previous work, LoRA."
                },
                "weaknesses": {
                    "value": "* The proposed method, which involves freezing the LoRA weight A from the existing LoRA, appears incremental in terms of novelty.\n* In the context of \"reducing the number of trainable parameters\", as mentioned in the LoRA paper, the previously proposed PEFT method significantly reduced the number of trainable parameters. This led to a drastic reduction in the memory usage of the optimizer state, bringing it down to megabytes. Thus, using fewer trainable parameters than LoRA does not yield a significant difference.\n* Regarding \"reducing the activation memory\", while LLaMA uses a max sequence length of 2k, the LLaMA2 model employs a 4k length. It's evident that models are gravitating towards longer sequence lengths. As per Table 1, LoRA-FA utilizes '2bsr' of memory (compared to LoRA's '2bsd+2bsr'), but the advantage in memory savings during training becomes less pronounced as the model's sequence length increases relative to LoRA.\n* The proposed methodology primarily targets the reduction of memory usage during training. In section 4.2, Table 5, the variance in memory peak between LoRA and LoRA-FA is minimal, especially for generative models like LLaMA-7B.\n* As stated in the LoRA paper, amplifying the trainable parameters doesn't notably affect accuracy (refer to Figure 2). For a more precise evaluation, I suggest the following comparative experiments with LoRA:\n    * Match LoRA and LoRA-FA at the same level of trainable parameters and then compare their accuracy and memory peaks.\n    * Adjust both LoRA and LoRA-FA to similar peak memory levels and compare their MMLU accuracy.\n    * Examine the peak memory usage of both LoRA and LoRA-FA when using generative models larger than LLaMA-7B to determine if the gap increases as the model size grows."
                },
                "questions": {
                    "value": "Covered in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3536/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3536/Reviewer_s6YB",
                        "ICLR.cc/2024/Conference/Submission3536/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3536/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840382090,
            "cdate": 1698840382090,
            "tmdate": 1700710308269,
            "mdate": 1700710308269,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GvxjjVIPXX",
                "forum": "RbKThNNFxr",
                "replyto": "af6nMT4O4m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3536/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3536/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the detailed comments from this reviewer.\n\n**Movation**: When fine-tuning LLMs, we observed that LoRA takes even more memory than full-parameter fine-tuning when batch size getting larger. This is due to the activation memory of sub-modules, such as attention or MLP. Inside these sub-modules, the low-rank adapter A has much more activation memory usage than adapter B. We hence propose LoRA-FA to address this problem.\n\n**Limitation 1**: In the context of \"reducing the number of trainable parameters\", as mentioned in the LoRA paper, the previously proposed PEFT method significantly reduced the number of trainable parameters. This led to a drastic reduction in the memory usage of the optimizer state, bringing it down to megabytes. Thus, using fewer trainable parameters than LoRA does not yield a significant difference.\n\n**Response**: Thank you for your insightful comment. It is true that using fewer trainable parameters than LoRA does not often yield a significant difference in LoRA-like methods. Our intention was not to imply that using fewer trainable parameters than LoRA would reduce more GPU memory. Rather, our emphasis was to show that LoRA-FA reduces great activation memory by freezing the update of low rank adapter A, which can also reduce the trainable parameters. We will ensure to clarify this in our revised manuscript to avoid any misunderstanding.\n\n**Limitation 2**: Regarding \"reducing the activation memory\", while LLaMA uses a max sequence length of 2k, the LLaMA2 model employs a 4k length. It's evident that models are gravitating towards longer sequence lengths. As per Table 1, LoRA-FA utilizes '2bsr' of memory (compared to LoRA's '2bsd+2bsr'), but the advantage in memory savings during training becomes less pronounced as the model's sequence length increases relative to LoRA.\n\n**Response**: This is a very detailed comment. Regarding to Table 1, in a linear layer, LoRA-FA saves `2bsd` usage compared to LoRA in activation memory, which is linearly related to the coefficient sequence length. This means the memory saving has a linear-like scaling efficiency regarding to sequence length. We further conduct an experiment with longer sequence length in fine-tuning Llama2-13B. Specifically, we set the sequence length from 1024 to 2048, and the batch size to 2. We report the peak GPU memory usage as follows.\n\n| seq_length | 1024  | 2048  |\n|------------|-------|-------|\n| LoRA       | 50 GB | 71 GB |\n| LoRA-FA    | 45 GB | 62 GB |\n\nThis result shows that LoRA-FA saves more GPU memory when using longer sequence length.\n\n**Limitation 3**: The proposed methodology primarily targets the reduction of memory usage during training. In section 4.2, Table 5, the variance in memory peak between LoRA and LoRA-FA is minimal, especially for generative models like LLaMA-7B.\n\n**Response**: Thank you for pointing out the issue! We have corrected the results in Table 5, it was because activation recomputation was turned on when conducting memory benchmark experiments. We ensure all of the configures are right and re-conducted the experiment. Specifically, we fine-tuning Llama2-7B with sequence length = 1024. In the new results, we have several findings:\n\n1. LoRA-FA reduces ~13GB memory compared to LoRA in fine-tuning Llama2-7B when batch size is 16.\n2. In practice, when fine-tuning Llama2-7B on various hardware, LoRA-FA has better capability than LoRA. For example, on RTX4090-24G, LoRA can only fine-tuning Llama2-7B when $batch size \\leq 2$, while LoRA-FA can enlarge the batch size to 4; on A100-40G, LoRA works with batch size = 4, while LoRA-FA works with batch size = 8.\n\nWe also give a runtime memory breakdown analysis of fine-tuning Llama2-7B as follows when batch size is 16, to figure out the memory usage percentage of different modules, such as optimizer state and activation memory.\n\n|                    | LoRA        |                | LoRA-FA     |                |\n|:------------------:|:-----------:|:--------------:|:-----------:|:--------------:|\n|                    | Memory (GB) | Percentage (%) | Memory (GB) | Percentage (%) |\n| Hardware researved | 0.7         | 0.9            | 0.7         | 1.1            |\n| Pytorch researved  | 0.5         | 0.6            | 0.5         | 0.8            |\n| Model in bf16      | 12.6        | 17             | 12.6        | 20.3           |\n| LoRA module        | 0.8         | 1.1            | 0.8         | 1.4            |\n| Cache              | 5.1         | 6.7            | 4.8         | 7.7            |\n| Optimizer state    | 0.7         | 0.9            | 0.4         | 0.4            |\n| Activation         | 55.6        | 73.1           | 42.3        | 68.2           |\n\nThis result shows that LoRA-FA can significantly reduce the activation memory usage compared to LoRA when fine-tuning Llama2-7B (from 55.6 GB to 42.3 GB). This advantage can either help fine-tuning on consumer level GPUs, or can achieve higher throughput by enlarging batch size."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3536/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507577904,
                "cdate": 1700507577904,
                "tmdate": 1700507577904,
                "mdate": 1700507577904,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jp0ry2DoMs",
                "forum": "RbKThNNFxr",
                "replyto": "af6nMT4O4m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3536/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3536/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Limitation 4**:\n\n- Match LoRA and LoRA-FA at the same level of trainable parameters and then compare their accuracy and memory peaks.\n- Adjust both LoRA and LoRA-FA to similar peak memory levels and compare their MMLU accuracy.\n- Examine the peak memory usage of both LoRA and LoRA-FA when using generative models larger than LLaMA-7B to determine if the gap increases as the model size grows.\n\n**Response**:\n\n- We set the rank of LoRA to 64, and the rank of LoRA-FA to 128, to maintain the same level of trainable parameters. We fine-tune Llama2-7B on Super-NaturalInstructions dataset with sequence length = 1024. We report the eval_loss of both approaches. From our result, both LoRA and LoRA-FA can achieve a similar performance (eval_loss = 0.45). LoRA-FA saves 13GB of peak GPU memory compared to LoRA. This means that enlarging rank doesn't yield a great difference in memory consumption.\n- Since enlarging rank doesn't yield a great difference in memory consumption, we choose to adjust the batch size of both methods to maintain a similar peak memory levels. From our result, both LoRA and LoRA-FA achieves a similar MMLU accuracy when fine-tuning Llama2-7B on Super-NaturalInstructions dataset (accuracy = ~46.5).\n- We have checked the peak memory usage when fine-tuning Llama2-7B and Llama2-13B using batch size 4, the results didn't show much increase in GPU memory gap as the model size grows."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3536/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507590488,
                "cdate": 1700507590488,
                "tmdate": 1700507590488,
                "mdate": 1700507590488,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5qPEwo5uA4",
                "forum": "RbKThNNFxr",
                "replyto": "jp0ry2DoMs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3536/Reviewer_s6YB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3536/Reviewer_s6YB"
                ],
                "content": {
                    "comment": {
                        "value": "Dear author,  \nThank you for your response to my queries.   \nHowever, I still have some growing concerns that I would like author to address:  \n* I couldn\u2019t find any results addressing Limitation 4 in author\u2019s response. Could you please direct me to these results?\n* The experimental configurations of the paper may need revision, as it is difficult to locate rank information in all the accuracy report tables.\n* In the experiments shown in Table 4 of the paper, it appears that the rank configuration for LoRA-qv is set at 4, and LoRA-FA is applied to all linear layers. Are there results available for LoRA-qv with a higher rank, like 64 or 128? If so, does LoRA-qv still exhibit lower accuracy in these cases?\n* In your response to Limitation 4, rather than addressing my concerns, it seems they were exacerbated. I believe finding a configuration with similar peak memory and accuracy to LoRA-FA by reducing the layers where LoRA is applied and adjusting the rank should be feasible.\n* Regarding the experiments where you match LoRA and LoRA-FA at the same level of trainable parameters to compare their accuracy and memory peaks: is there a notable difference in memory peak and accuracy when LoRA-FA is applied to all layers, while LoRA is applied only to the attention layers of LLaMA (excluding gate_proj, up_proj, down_proj) with an adjusted rank? Additionally, if LoRA is selectively applied to the combinations of query, key, value, or output projection (o_proj) in the attention layers, does adjusting the rank still result in LoRA-FA having superior accuracy?\n* To convincingly demonstrate that LoRA-FA is superior to LoRA, it seems necessary to show instances where LoRA-FA achieves higher accuracy than LoRA under a limited peak memory constraint. For example, in Figure 2-b of the paper, when the peak GPU memory is limited to 38.5, does LoRA-FA still maintain better accuracy than LoRA, even after sweeping through various configurations (rank, layers applied, etc.) of LoRA?\n* In response of experiments on \u201cAdjusting both LoRA and LoRA-FA to similar peak memory levels and comparing their MMLU accuracy,\u201d author claimed that merely adjusting the rank size does not significantly impact memory consumption. This seems to be an expected result when only the rank size is modified as we already discussed in the Limitation 1. To effectively demonstrate the competitiveness of LoRA-FA, a study showing the effects of reducing the layers where LoRA is applied, thereby lessening the layers calculating gradients and impacting memory consumption, would have been beneficial.\n* Based on the response to \u201cExamine the peak memory usage of both LoRA and LoRA-FA when using generative models larger than LLaMA-7B to determine if the gap increases as the model size grows,\u201d author reported that the memory gap does not widen as the model size increases. Does this suggest that the efficiency of LoRA-FA diminishes with larger models?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3536/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588144696,
                "cdate": 1700588144696,
                "tmdate": 1700588144696,
                "mdate": 1700588144696,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kDJcayafUx",
                "forum": "RbKThNNFxr",
                "replyto": "xerSL5Si8O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3536/Reviewer_s6YB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3536/Reviewer_s6YB"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Author,\n\nThank you for your response.\n\nFrom your experiments, it appears that LoRA-FA has some advantages over LoRA in certain cases. However, I remain unconvinced that LoRA-FA consistently outperforms LoRA. Demonstrating the effectiveness of the proposed methodology is a task for the authors rather than the reviewers.\n\nIn appreciation of your comprehensive and timely reply, I have decided to increase the score by one level. Yet, I must maintain a stance of rejection at this time, with the hope that the following feedback will guide improvements in your paper:\n\n1. The paper does not convincingly demonstrate that LoRA-FA consistently surpasses LoRA.\n2. There is a lack of overall clarity in the paper.\n3. The experimental setup is inadequately detailed, affecting the reliability of the findings."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3536/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710274263,
                "cdate": 1700710274263,
                "tmdate": 1700710274263,
                "mdate": 1700710274263,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]