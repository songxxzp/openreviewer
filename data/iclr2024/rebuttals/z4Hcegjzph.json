[
    {
        "title": "Pre-training with Random Orthogonal Projection Image Modeling"
    },
    {
        "review": {
            "id": "iciYiPh9Lx",
            "forum": "z4Hcegjzph",
            "replyto": "z4Hcegjzph",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6074/Reviewer_zDzQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6074/Reviewer_zDzQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the task of MIM, by designing a new corruption process via the random orthogonal projection. Such a ROP strategy results in a more efficient and effective pre-training method. Another advantage of ROP is the guaranteed bound on the noise variance during the corruption process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This work is based on the sound theory of random orthogonal projection.\n2. ROPIM is able to achieve more superior performance in a shorter pre-training time.\n3. The decoder only contains one linear layer, being more slight comparing with MIM.\n4. The experiments verify its effectiveness on several downstream tasks, including classification and segmentation."
                },
                "weaknesses": {
                    "value": "1. It is a little difficult to understand why does the proposed method is more superior than MIM. According to my understanding, the ROP strategy randomly discards some local (not global) patterns during corruption as shown in Fig. 9. This is very similar to MIM. So, I can't intuitively catch what results in the superiority of ROP in the field of MIM. It would be better to have a discussion in the paper."
                },
                "questions": {
                    "value": "1. Proposition 1 gives a complicated way to generate a projection matrix $P$ that contains only three elements, namely {-1, 0, 1}. This procedure relies on two auxiliary variables of $h$ and $s$. Is it possible to directly sample from {-1, 0, 1} for each entry of $P$?\n\n2. Following question 1, the projection matrix $P$ is composed of {-1, 0, 1}. Generally speaking, \"0\" denotes discarding some embed patch in projection. It is hard to understand the function of \"-1\".\n\n3. In proposition 1, I guess: $h \\in I^d_{K^{'}}$ should be $h \\in I^K_{K^{'}}$?\n\n4. In proposition 2, the notation for $\\phi^{'}$ is missing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6074/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6074/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6074/Reviewer_zDzQ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6074/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698483689366,
            "cdate": 1698483689366,
            "tmdate": 1699636654432,
            "mdate": 1699636654432,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R8SeN09zGQ",
                "forum": "z4Hcegjzph",
                "replyto": "iciYiPh9Lx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6074/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6074/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "# Response to Rev. 3 (Reviewer zDzQ)\n\n*Firstly, **we thank the reviewer** for the constructive review and valuable questions.*\n\n## 1. Little difficult to understand why the proposed method is better than MIM. The ROP strategy randomly discards some local (not global) patterns during corruption as shown in Fig. 9. This is very similar to MIM.\n\nThank you. The simplified answer is that we achieve better results due to different profile/type of noise we implicitly inject via sketching  compared to binary masking:\n* **masking introduces binary patterns** (keep token or completely mask it) resulting in a limited number of masking patterns\n* **Fig. 8 and 9 show that** rather than just removing token, **our ROPIM has this more continuous effect compared to binary masking**, which creates more patterns of randomness (Figure 4 compares the effect of binary masking versus  our strategy).\n\nFor simplicity, **imagine a masking problem with just 2 tokens**:\n* **for standard binary masking, in total there are $2^2$ unique masking patterns**, and assuming 50\\% masking ratio, that just **limits patterns to mere 2**.\n* in contrast, if masking has more \\'continuous\\' nature, **say we can get $\\\\{0\\\\%, 25\\\\%, 50\\\\%, 75\\\\%, 100\\\\%\\\\}$ of original energy preserved per token, that gives $5^2$ unique masking patterns**, and under 50\\% masking ratio (assuming it equals to 50\\% lost information), that gives us (0,100),(25,75), (50,50), (75,25), (100,0) patterns (5 in total). While this is a simplification, ROPIM yields a similar effect but (i) in addition we get complementary \\`unmasking\\' pattern and known variance bound of the implicitly injected noise as per Prop. 1-5.\n\n\n**Importantly, in Figure 7a we show the actual real-data comparison between our ROPIM strategy and binary masking**. Figure 7a illustrates numerically what we argue above, i.e., for the same \\`masking ratio\\':\n* **binary masking makes many tokens at the input to be completely unmodified** (see the first histogram bean in Fig. 7a)\n* in contrast, our **ROPIM leaves fewer tokens \\`untouched\\' by some level of noise - creating richer noise patterns than binary masking**\n* consequently, Figure 7d also shows that our \\`unmasking\\' step works much harder\n\nThus, **ROPIM leads to greater variety of input patterns and \\`unmasking\\' patterns**, which exposes network to greater diversity of patterns to help learn the implicit manifold of the data (think e.g. DAE).\n\\\n\\\nNote that suitable data augmentation strategies during pre-training are an open question:\n> [I]. Masked auto-encoders are scalable vision learner, He *et al.*, CVPR 2022\n\n> [II]. Mixed auto-encoder for self-supervised visual representation learning, Chen *et al.*, CVPR 2023\n\n> [III]. Corrupted image modeling for self-supervised visual pre-training. Fang *et al.*, CVPR 2023.\n\nE.g., color jittering degrades the transfer learning performance of MAE in [III], implying that MIM exhibits a  preference for specific kinds of data augmentation (not all are good). \n\n\n## 2. Proposition 1 gives a complicated way to generate a projection matrix with elements: -1, 0, 1. This procedure relies on h and s. Is it possible to directly sample from -1, 0, 1 for each entry of P ?\n\nThank you. We appreciate the process may look complicated but this is the standard definition of count sketching. Both variables are necessary because hashing function h (producing ${\\bf h}$ ) controls unique locations of non-zero entries per row of ${\\bf P}$ and thus promotes each row vector of ${\\bf P}$ to be orthogonal to other row vectors. **This is essential in order to ensure we deal with subspaces (and that Prop. 2-5 hold).**\n\n## 3. The projection matrix P is composed of -1, 0, 1. \u201d0\u201d denotes discarding some embed patch in projection. It is hard to understand the function of \u201d-1\u201d.\n\nAttributing roles to -1 and 1 at the coefficient level is hard as ${\\bf P}$ is a projection matrix for matrix-vector multiplication, not element-wise tool.\n\nThe meaning of elements 1 and -1 is that they are an output of a hash function $s$ (${\\bf s}$ is specific result from hashing function s).\n\nFor detailed explanation of count-sketching, the following article is a sensible introduction to this advanced topic: \nhttps://en.wikipedia.org/wiki/Count_sketch\n\n Kindly notice **what matters to ROPIM is the resulting properties in Propositions 2-5** of our work:\n* controllable/bounded variance of approximation\n* easy \\`unsketching\\' operation (project to subspace and retract back, notice we use ${\\bf P^\\dagger P \\phi}$ on input)\n* easy complement operation for unmasking (output of decoder) that tries recover exactly what was removed on input\n* fast generation and computation (generating ${\\bf P}$ is fast, multiplying with ${\\bf P}$ too).\n\n## 4. In prop. 1, $h \\in I_{K^{'}}^{d}$ should be $I_{K^{'}}^{K}$?\n\nThank you. We have now fixed it.\n\n## 5. Notation $\\phi'$ missing.\n\nOur apology: should be  $\\phi_y$ not $\\phi'_y$. $\\phi_x$ and $\\phi_y$ reads as \\`take any two vectors\\'."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6074/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505607640,
                "cdate": 1700505607640,
                "tmdate": 1700505607640,
                "mdate": 1700505607640,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6e73x3JIQx",
                "forum": "z4Hcegjzph",
                "replyto": "R8SeN09zGQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6074/Reviewer_zDzQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6074/Reviewer_zDzQ"
                ],
                "content": {
                    "title": {
                        "value": "Response authors"
                    },
                    "comment": {
                        "value": "Thanks for your response. This response addressed all of my concerns. I remain my postive rating.\n\nI suggest to add the discussion in Q1 to the final version."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6074/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532640529,
                "cdate": 1700532640529,
                "tmdate": 1700532640529,
                "mdate": 1700532640529,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pi6KXhxYZ1",
            "forum": "z4Hcegjzph",
            "replyto": "z4Hcegjzph",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6074/Reviewer_xoq6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6074/Reviewer_xoq6"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the self-supervised learning problem, which has attracted much attention recently. While the masked image modeling, e.g. masked autoencoder, has recently shown very promising performance in self-supervised learning for visual pre-training, the authors aim to improve it by proposing a new image modeling. Specifically, unlike the masked image modeling applying random crops to the input and learning to recover the masked inputs with an encoder-decoder network, this paper considers a random orthogonal projection modeling which uses a random subspace for the projection and then learns to recover the complement of that subspace. Provided experiments show that the proposed random orthogonal projection can yield better performance than the crop-based masking."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Though simple and straightforward, to my knowledge the proposed random orthogonal projection modeling for self-supervised learning is novel. Provided experimental results have demonstrated better performance of the proposed random orthogonal projection method in comparison with the masked image modelling using crop-based masking."
                },
                "weaknesses": {
                    "value": "The proposed method is somewhat heuristic."
                },
                "questions": {
                    "value": "Some of the Propositions in Section 3.1 are rather straightforward and would be better not be expressed as Proposition.\n\n$\\ell_1$ loss is used in the reconstruction loss, would the MSE loss yield worse performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6074/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6074/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6074/Reviewer_xoq6"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6074/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739679736,
            "cdate": 1698739679736,
            "tmdate": 1699636654323,
            "mdate": 1699636654323,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eGGExFIep5",
                "forum": "z4Hcegjzph",
                "replyto": "pi6KXhxYZ1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6074/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6074/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "# Response to Rev. 2 (Reviewer xoq6)\n\n*Firstly, **we thank the reviewer** for the constructive review and valuable questions.*\n\n## 1. The proposed method is somewhat heuristic.\n\nThank you. The very standard Masked Auto Encoder (MAE) and related works perform basic binary masking which we agree may be viewed as heuristic. However, MAE can be inspired by the mathematics of Denoising Auto-encoders:\n> What regularized auto-encoders learn from the data-generating distribution., Alain and Bengio, JMLR 2014\n\nThe above study explains how the variance of noise injected into DAE is directly connected with the gradient regularization and manifold learning property, e.g., consider Reconstruction Contractive Auto-encoder (RCAE) (their Eq. 6 and Section 3.1)\n\\\n\\\nFrom that point of view, transformer-based MAE and related works may be harder to analyze but **if we can quantify the variance of the noise we inject, we can lean on the theory of DAE and RCAE to understand how the noise variance governs their data manifold learning property.**\n\\\n\\\nThus, our **Proposition 2 we quantify the upper bound on variance implicitly introduced by our projection in ROPIM**. From that point of view, **our ROPIM enjoys better foundations than existing masked auto-encoders** as we are able to quantify the injected variance while previous works do not seem to notice the importance of noise variance, and they do not provide any guarantee or indications in that sense at all.\n\\\n\\\nWe will ensure to explain that theoretical connection better in our final draft.\n\n## 2. Some of the Propositions in Section 3.1 are rather straightforward and would be better not be expressed as Proposition.\n\nThank you. We are more than happy to simply rephrase propositions 2-5 as a list of **properties** of ROPIM with sketch-based projections. We are more than keen to read further advice from the reviewer.\n\\\n\\\nWhile simple, we include Property 2 as it explains the variance of implicitly injected noise (important due to the conenction to DAE), Propositions 3 and 4 explain how to use such projections in practical terms (e.g., unsketchign is needed to keep tokens in one common space), Proposition 5 is important as it gives \\`the complement of sketching\\' operation required to guide the output of our approach, and ensures it \\`unmasks\\' exactly the information that was removed at the input.\n\n\n\n\n## 3. The $\\ell_1$ loss is used in the reconstruction loss, would the MSE loss yield worse performance?\n\nThank you for spotting this.  We have now conducted experiments with MSE for ViT-T and ImageNet100, and the MSE loss  performs slightly better than the $\\ell_1$ loss. We will include such additional ablations.\n\n\n|Loss | Top 1 acc. (with 300 PT epochs) | Top 1 acc. (with 800 PT epochs)  |\n|-|-|-|\n|ROPIM with $\\ell_1$ | 82.98 | 86.43 |\n|ROPIM with MSE | **83.60** | **86.70** |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6074/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504630927,
                "cdate": 1700504630927,
                "tmdate": 1700504630927,
                "mdate": 1700504630927,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZqdRdaOqup",
            "forum": "z4Hcegjzph",
            "replyto": "z4Hcegjzph",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6074/Reviewer_5QU4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6074/Reviewer_5QU4"
            ],
            "content": {
                "summary": {
                    "value": "The work proposes to use random projections in the place of masked images for pre-training the ViTs. The work is shown to lead to better performance for classification tasks compared to the MIM methods. The work seems novel enough where they replace binary mask with floating point mask."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The use of linear algebraic projection technique in both the method and the loss function for reconstruction.\n\n+ The results are achieved better with a considerably smaller number of epochs.\n\n+ The work is well-motivated from the basics and seems reproducible. \n\n+ The transfer learning results are added value to the work as such. \n\n+ The work should be useful as a pre-trainer for several ViT based applications."
                },
                "weaknesses": {
                    "value": "- I'm not sure if all the recent works on MIM have been compared with. Authors are requested to comment on this."
                },
                "questions": {
                    "value": "Are there any recent works which have been exempted from comparison?\n\nApart from classification and semantic segmentation, do the authors have results on any other applications?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6074/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698748268315,
            "cdate": 1698748268315,
            "tmdate": 1699636654210,
            "mdate": 1699636654210,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "15Pj804KR3",
                "forum": "z4Hcegjzph",
                "replyto": "ZqdRdaOqup",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6074/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6074/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "# Response to Rev. 1 (Reviewer 5QU4)\n\n*Firstly, **we would like to sincerely thank the reviewer** for the constructive review and valuable questions.* Below are point-by-point responses.\n\n## 1. Are there any recent works which have been exempted from comparison? I am not sure if all the recent works on MIM have been compared with.\n\n* Self-supervised learning is an active research area. We have tried our best to cover all related works. Researchers from different fields have been creative in using MIM techniques for various applications such as video:\n>[I]. Videomae v2: Scaling video masked autoencoders with dual masking, Wang *et al.*, CVPR 2023\n\n  and point clouds:\n  >[II]. Geomae: Masked geometric target prediction for self- supervised point cloud pre-training, TIan *et al.*, CVPR 2023\n\n  However, they are not directly related to our work as they research other modality inputs.\n\n* Another stream of research studies merging various pre-training strategies, i.e., combining contrastive learning with MIM, using teacher-student networks or utilizing networks pre-trained  on large multi-modal datasets:\n  >[III]. Towards all-in-one pre-training via maximizing multi-modal mutual information, Su *et al.*, CVPR 2023\n\n  Such ideas can be potentially integrated with ROPIM but they are orthogonal to our work, and training each such a pipeline takes several days if not weeks. Nonetheless, we have equipped the most promising in our modest view such a framework, called **LGP** [IV] (in Fig. 1 we called it **GPL** due to typo), with ROPIM and showed substantial improvement in our Fig. 1.\n  > [IV]. Bridging contrastive learning and masked image modeling for label-efficient representations\n\n  Another very recent pipeline, called Correlational Image Modeling (CorIM) [V], proposes a self-supervised pre-training task leveraging a cropping strategy, a bootstrap encoder, and a correlation decoder. However, performance of CorIM (83.1\\% top 1 acc. for ViT-B) is short of the performance of pure ROPIM's **83.5\\%** with 300 pre-training epochs. Thus, as the most promising new strategy, we investigated GPL with our ROPIM (GPL+ROPIM in Fig. 1 of main submission).\n  >[V].  Correlational image modeling for self-supervised visual pre-training, Li *et al.*, CVPR 2023\n\n  We also reported recent CIM models [VII] which also perform worse than ours despite using a mix of contrastive and masking strategies (e.g., 81.6 vs. ours 82 in Table 1).\n\n\n## 2. Apart from classification and semantic segmentation, do the authors have results on any other applications?\n\nFollowing the very recent works [IV, V, VI, VII], we have reported results on classification and segmentation downstream tasks as each downstream task is time-consuming (e.g., 4 days for segmentation).\n> [VI]. Masked frequency modeling for self-supervised visual pre-training, Xie *et al.*, ICLR 2023\n\n  > [VII]. Corrupted image modeling for self- supervised visual pre-training, Fang *et al.*, ICLR 2023\n\nNonetheless, we have also initiated less often used object detection on COCO for ViT-B. After 5 days of running the code we have  completed 50/100 epochs with 50.01\\% bbox/AP while MAE completed 50/100 epochs at 49.19\\%. We anticipate to reach 100 epochs within another 5 days on our 8 GPU server. We will update our final manuscript accordingly."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6074/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504318316,
                "cdate": 1700504318316,
                "tmdate": 1700504318316,
                "mdate": 1700504318316,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lMIkTGHal3",
                "forum": "z4Hcegjzph",
                "replyto": "15Pj804KR3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6074/Reviewer_5QU4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6074/Reviewer_5QU4"
                ],
                "content": {
                    "comment": {
                        "value": "I am satisfied with the response by the authors. Retain my positive rating."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6074/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540545925,
                "cdate": 1700540545925,
                "tmdate": 1700540545925,
                "mdate": 1700540545925,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]