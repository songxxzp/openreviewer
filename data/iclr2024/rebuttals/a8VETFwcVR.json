[
    {
        "title": "Unveiling Options with Neural Network Decomposition"
    },
    {
        "review": {
            "id": "CnvnW7hQM5",
            "forum": "a8VETFwcVR",
            "replyto": "a8VETFwcVR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1426/Reviewer_voTZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1426/Reviewer_voTZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to distill policies from a learned neural network and use these as options. They provide an algorithm that translates the neural network into a tree, decomposes it into sub-policies and selects their subset. In experiments, they verify the usefulness of the approach in several variants of grid-based domains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written and is original in the way the options are created -- i.e., extracting them from an already trained neural network. However, the suggested approach is limited only to simple NNs."
                },
                "weaknesses": {
                    "value": "First substantial weakness is that the number of sub-policies raises exponentially with the size of the NN, limiting this approach to very small networks and thus the significance of the proposed algorithm. \n\nSecond, given the large number of the subpolicies, it is not clear whether the subset selection (step 3) is better than selecting from a set of random policies. That is, if steps (1) and (2) are ommited and step (3) would select from a random set. In the second example at the end of Experiments, the subpolicies consist of 4 actions, which can be easily randomly generated. This would drastically reduce the complexity of the algorithm, but still would be limited to simle domains. This ablation is crucially missing from the evaluation."
                },
                "questions": {
                    "value": "Please, present a definite argument (e.g., an experiment or a disproval why I am wrong) that the steps (1) and (2) are needed and the set of extracted sub-policies is useful, compared to a set of random sub-policies. I may reconsider the rating based on the arguments presented.\n\nI assume that in the beginning of the section 4.2, the authors meant `where a_t = argmax_a \\pi(s_t , a)`, instead of `where a_t = argmax_a p(s_t , a)`. However, if that is true, why not to sample the actions as `a_t ~ \\pi(s)`? If this is to make a unique sequence per each policy, clearly indicate it for the reader and add a reason.\n\n---\nSuggestions:\n- The graph in Figure 1-right confused me for a while, as I was trying to read it from left-to-right, instead of top-to-down. Indicate this fact for the reader. Also, consider (I don't stress this point) showing the complete graph with the path highlighted.\n\nTypos:\n- p. 7: Since learning a policy [IN] the new set ...\n- p. 7: baseline Dec-Options-Whole[.]"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1426/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1426/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1426/Reviewer_voTZ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1426/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698334520415,
            "cdate": 1698334520415,
            "tmdate": 1700605341008,
            "mdate": 1700605341008,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cOpdAtgd9i",
                "forum": "a8VETFwcVR",
                "replyto": "CnvnW7hQM5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1426/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1426/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Small Neural Networks**\n\nDec-Options do not have to be restricted to small neural networks. Please see our answer to this question in the answer to all reviewers. \n\n**Baseline with Random Policies**\n\nThank you for suggesting this baseline! In this baseline, we ignore steps 1 and 2 and randomly select an option that performs a fixed set of actions of length $M$. We then add $K$ of such options to the agent's action set. Note that this approach requires domain knowledge, as the values of $M$ and $K$ are not known a priori. By contrast, Dec-Options discovers them automatically. \n\nWe ran experiments on the Combo domain where we chose $M$ to be $6$, which is the average number of options Dec-Option chooses and $K$ to be $4$, which is the number of directions the agent can move. Despite having more information than Dec-Options, the latter performs much better as shown in Figure 10 of the Appendix E of the revised version. Even when we manually set the values of $M$ and $K$, there are many options to choose from and it is unlikely that helpful options would be selected this way. \n\nAnother interpretation of the reviewer's suggestion is to use the selection step (step 3) without steps 1 and 2. We note that step 3 cannot be used without step 1. This is because step 3 requires the data generated in step 1 to compute the Levin loss for selection. \n\n**Argmax in Section 4.2**\n\nWell spotted! Thank you! It should be $a_t = \\arg\\max_a \\pi(s_t, a)$ instead of $a_t = \\arg\\max_a p(s_t, a)$. The use of the $\\arg\\max$ operator over $\\pi$ reduces the noise in the selection process of the Dec-Options because the options also act greedily according to the sub-policies extracted from $\\pi$. The added a sentence to explain this in the paper. \n\n**Suggestion Related to Figure 1**\n\nThank you for your suggestion! We replaced Figure 1 with a smaller example where we can show the entire neural tree. This change also allowed us to get rid of the equations from the figure. Please see the new example on Page 4 of the revised version of the paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541291282,
                "cdate": 1700541291282,
                "tmdate": 1700581101686,
                "mdate": 1700581101686,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Uxwd5fKxjh",
                "forum": "a8VETFwcVR",
                "replyto": "cOpdAtgd9i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1426/Reviewer_voTZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1426/Reviewer_voTZ"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, thank you for your answer.\n\nRegarding the baseline - please explain to me, why is not possible to compute the Levin loss with randomly generated policies. E.g., you randomly generate the policies, sample their trajectories and use Alg. 1.\n\nThank you for adding additional baselines. However, Option-Critic, ez-greedy, and DCEO are not explained and a reader would be forced to read their papers. I acknowledge the lack of space, so maybe a brief explanation in appendix would be possible?\n\nGood job on Fig. 1, it is now much clearer."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563976352,
                "cdate": 1700563976352,
                "tmdate": 1700563976352,
                "mdate": 1700563976352,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eb0nrcFCyV",
                "forum": "a8VETFwcVR",
                "replyto": "CnvnW7hQM5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1426/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1426/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your quick response. We are happy to hear you like our new Figure 1. We will add paragraphs explaining Option-Critic, DCEO, and $\\epsilon$z-greedy to the appendix.\n\nIn the meantime, we would like to ask a couple of clarifying questions about your suggested baseline. There is a chance we will also answer your question *\"why is not possible to compute the Levin loss with randomly generated policies\"* while asking our questions. \n\n**Our Question:** By randomly generated policies do you mean neural networks with their weights randomly generated or policies that execute a fixed and repeated random set of actions as we used in our baseline Random in Appendix E?\n\n**Re:** *\"you randomly generate the policies, sample their trajectories and use Alg. 1\"*\n\nThere are a few ways of how the randomly generated policies $R$ can be used in Algorithm 1. Recall that Algorithm 1 requires a trajectory $\\mathcal{T}$ and a set of options $\\Omega$. \n\n1. **Interpretation (a):** We can use $R$ to generate a trajectory $\\mathcal{T}$ that is used in Algorithm 1. \n2. **Interpretation (b):** We can use a set of policies $R$ to be used as the set of options $\\Omega$ in Algorithm 1.\n3. **Interpretation (c):** We can use policies $R$ to replace both $\\mathcal{T}$ and $\\Omega$ in Algorithm 1. \n\n**Interpretation (a)** still requires us to define the set of options $\\Omega$ used in Algorithm 1. In Dec-Options, $\\Omega$ is defined as the sub-policies of trained neural networks. If we use with the baseline the same set of options $\\Omega$ used with Dec-Options, then we cannot skip steps 1 and 2 of Dec-Options because $\\Omega$ is generated in those two steps. \n\n**Interpretation (b)** still requires us to define the trajectories $\\mathcal{T}$ of Algorithms 1. In Dec-Options, $\\mathcal{T}$ is given by trajectories sampled from fully trained neural networks, which are given in step 1 of our algorithm. This means we cannot skip step 1 of Dec-Options with interpretation (b).\n\n**Interpretation (c)**  allows us to skip steps 1 and 2 of Dec-Options and it offers a baseline that is somewhat similar to the baseline Random we added to Appendix E. If $R$ is given by a fixed and repeated random set of actions, then the trajectory $\\mathcal{T}$ will be given by such a random sequence. In this case, the Levin loss will be minimized by a selection of policies $R$ that choose trajectories similar to those in $\\mathcal{T}$. This is similar to randomly choosing some of these policies $R$ to be used as options, as we did in Appendix E. \n\nPlease let us know whether we answered your question. If not, please let us know which interpretation you had in mind: (a), (b), or (c), and we will get back to you."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589311828,
                "cdate": 1700589311828,
                "tmdate": 1700589361108,
                "mdate": 1700589361108,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TuInjTiGhL",
                "forum": "a8VETFwcVR",
                "replyto": "eb0nrcFCyV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1426/Reviewer_voTZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1426/Reviewer_voTZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the explanation! I had basically the (b) variant in mind, but it's clear that the trajectories T has to be generated as well and the variant (c) now does not seem very sensible. Given the my new understanding, I am now unsure whether the baseline I previously suggested adds any value to the paper, but I will leave it to your decision.\n\nI am happy with the answers and changes, so I am updating the rating."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605285797,
                "cdate": 1700605285797,
                "tmdate": 1700605285797,
                "mdate": 1700605285797,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WXMkJ3gCN7",
            "forum": "a8VETFwcVR",
            "replyto": "a8VETFwcVR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1426/Reviewer_i5GW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1426/Reviewer_i5GW"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of extracting useful skills/options from existing neural network policies. The proposed method involves collecting a set of neural networks (that use piecewise linear activation functions) trained to perform a variety of tasks and then constructing tree structures (neural trees) representing the activation patterns of these networks. Every subtree of a neural tree can be interpreted as a separate policy giving rise to a collection of sub-policies which are used to construct options. Since this collection of options can be large and contain very specialized policies that may not be useful in general, the authors propose a greedy method for selecting a set of useful options. Experiments in grid world environments show that the extracted options are effective in learning to perform new unseen tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Inferring skills/options that can be used to perform new tasks is an important problem and the proposed approach provides an intuitive way of extracting options from pretrained neural network policies. This method leverages the compositional structure of neural networks to identify sub-policies that might be useful for performing new tasks. Based on my understanding, any such sub-policy matches the original policy on inputs triggering a specific set of activations in the network which represents a specific region of the state space where this option is used in the original policy. Therefore, this appears to be a principled way of extracting options from pretrained policies.\n- The idea of directly extracting options from NN policies instead of constructing them during training appears to be novel and interesting. As far as I know, this is the first paper to propose this idea and this might lead to further research on such approaches.\n- Experimental results look very promising with the proposed approach outperforming existing baselines for constructing options as well as transfer learning approaches.\n- The paper is well-written and examples are provided to illustrate key concepts."
                },
                "weaknesses": {
                    "value": "- The main weakness appears to be scalability of the approach w.r.t. the size of the neural network. The total number of options considered is exponential in the size of the network. The option selection method involves enumerating all candidate options which doesn\u2019t help address the computational challenges of such an approach.\n- Experiments are limited to grid world environments. Experiments in more complex environments with continuous state and action spaces would strengthen the paper significantly. Although the current experiments demonstrate the value in the proposed method for constructing options, it would be good to confirm its applicability in a wider range of scenarios."
                },
                "questions": {
                    "value": "1. It appears that Levin loss corresponding to the uniform policy is equivalent to the minimum number of actions (or steps) required to explain a trajectory. Since the general Levin loss is described, are there other natural candidates (which would simply weight different actions/steps differently) besides the uniform policy to use for option selection?\n1. Algorithm 1 uses $O(|\\mathcal{T}|^2)$ space. It looks like the algorithm can be modified to use $O(|\\mathcal{T}|)$ space (the loss after position j is independent of d). Is there a specific reason for using the version in the paper? \n1. It looks like Dec-Options-Whole is doing reasonably well in most cases. Could this suggest a heuristic to reduce the computational complexity by not considering all subtrees of the neural tree? From my understanding, Dec-Options-Whole is only considering a single subtree per tree which is the whole tree, so maybe there is a middle ground between the two extremes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1426/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1426/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1426/Reviewer_i5GW"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1426/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698631948116,
            "cdate": 1698631948116,
            "tmdate": 1699636071002,
            "mdate": 1699636071002,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6UX2AQAJdY",
                "forum": "a8VETFwcVR",
                "replyto": "WXMkJ3gCN7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1426/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1426/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Answer to Q1:** Wonderful question! We chose the uniform policy because it is the only safe choice to be made. Since randomly initialized neural networks represent near-uniform policies, we know that at least in the first steps of training our choice of policy will be correct. Any other choice could be harmful. For example, if we consider a policy that favors one of the options, then this could hurt exploration in a target task if the favored option is not actually helpful. \n\n**Answer to Q2:** Well spotted! Our implementation actually uses the linear version you mention. We decided to explain the quadratic because we felt it is easier to understand. In the revised version, we have added the linear version in Appendix A and added a sentence about it in the main text, after we explain the algorithm's complexity. \n\n**Answer to Q3:** Thank you for pointing this out about Dec-Options-Whole. We have a few ideas of how to move forward with the selection process with very large neural networks. The activation pattern of the network in the early tasks provides valuable information. For example, if a neuron is always active or always inactive, then we can \"prune\" this neuron from consideration and that would represent a three-fold reduction in the size of the space. Also, the number of activation patterns is bounded by the number of samples in the tasks, so even if we have a very large neural network, the number of patterns that matter will always be bounded by how much data we have. All this information can potentially be used to guide local search algorithms in the activation pattern space. Also, following your observation about Dec-Options-Whole, we could try to bias the selection to consider sub-policies more similar to them. We think this is an exciting research direction."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541142830,
                "cdate": 1700541142830,
                "tmdate": 1700541142830,
                "mdate": 1700541142830,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bjUisjxejZ",
                "forum": "a8VETFwcVR",
                "replyto": "6UX2AQAJdY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1426/Reviewer_i5GW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1426/Reviewer_i5GW"
                ],
                "content": {
                    "title": {
                        "value": "Author Response Acknowledgement"
                    },
                    "comment": {
                        "value": "I thank the authors for their answers to my questions. I have read the rebuttal and skimmed through the changes in the paper. Overall, the presentation seems to have improved and the concepts are explained more clearly. Some minor concerns regarding scalability and lack of experiments on complex environments still remain. Hence, I am keeping my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703295824,
                "cdate": 1700703295824,
                "tmdate": 1700703295824,
                "mdate": 1700703295824,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yjtdcu6kLH",
            "forum": "a8VETFwcVR",
            "replyto": "a8VETFwcVR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1426/Reviewer_j32D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1426/Reviewer_j32D"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to decompose piece-wise linear neural networks into options. To do so the authors build on the idea of decomposing a neural network into a neural tree, a quantity closely related to oblique decision trees. As each node of the tree is a sub-policy, the authors propose to use a Levin loss to prune the important sub-policies from which options will be derived. The authors evaluate their approach on a series on grid worlds."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The authors propose a novel way to learn options, directly from neural networks that are learning, or that have been retrained\n* The overall presentation is rigorous as well as the empirical evaluation (10 seeds with 95% CI)\n* The method seems to perform better compared to proposed baselines\n\nOne of the strong points of the method is the originality in the way the options are discovered. Although the empirical evaluation is limited, the method clearly has a lot of potential as stated by the authors, for example by learning options from \"legacy agents\". I think the community would do well in integrating such unusual ways of learning options/skills. Moreover, the empirical evaluation is rigorous and clearly shows statistical advantages."
                },
                "weaknesses": {
                    "value": "* The presentation is heavy and sometimes confusing. Efforts in addressing this are done but it is not close to being enough\n* The HRL baseline of Option-Critic is outdated and does not reflect progress in the field\n* The qualitative experiments are very limited\n\nThe whole of section 4.1 and 4.2 would require a deep rewriting. Many references to oblique trees are made, yet most of the readers will have no idea what that is. A better visualization than Figure 1 would be needed, and it should be on the same page as the description in 4.1.  I would suggest starting with a simple case of 2 actions and leave the generalizations too much later. The notation of Z is confusing, it adds many sub- and super-scripts that are not well presented. Are the Z functions really necessary to understand the method? The whole section of 4.2 suffers from similar problems. I would strongly suggest the authors to consider the point of view of someone who knows nothing about the specifics of their method and to write these sections from that point of view. It will be most helpful to the paper.\n\nOne of the HRL baseline is Option-Critic which is an outdated algorithm that has been beaten many times. [1] recently set strong performance across a wide range of environments. To adequately understand the merits of the method such a baseline should be included.\n\nThe qualitative experiments are very limited. Much more on this aspect is needed as interpretabiltiy is one of the hallmarks of HRL. I would suggest heatmaps that show option activation or trajectories that highlight when options are activated."
                },
                "questions": {
                    "value": "What would be required to scale the method to larger neural networks?\n\nWhy investigate the ComboGrid environment? What is interesting about this task?\n\n\"we consider small neural networks with one hidden layer, so we can evaluate all sub-policies of a neural policy.\" This should be highlighted more.\n\n\n======================================================================\n\n[1] Deep Laplacian-based Options for Temporally-Extended Exploration. Klissarov and Machado. 2023"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1426/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1426/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1426/Reviewer_j32D"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1426/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765875809,
            "cdate": 1698765875809,
            "tmdate": 1700672471565,
            "mdate": 1700672471565,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yPMRgkIYuu",
                "forum": "a8VETFwcVR",
                "replyto": "yjtdcu6kLH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1426/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1426/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Comments on Presentation**\n\nThank you for the suggestions related to the presentation of the paper. We believe we made major improvements to the original version. Namely, the example from Figure 1 is much simpler and complete now. Instead of using a neural network with 3 neurons in the hidden layer, now we have a network with 2 neurons in the hidden layer. This allowed us to draw the entire neural tree and get rid of the formulas and of the $Z'$ notation, as you had suggested. \n\nAlso aiming at improving readability, we added an example of how the Levin loss is computed with our dynamic programming procedure in Appendix A. \n\n**Suggestion of Using Klissarov and Machado's DCEO as a Baseline**\n\nThank you for suggesting us to include the work of Klissarov and Machado (2023) as a baseline. We ran experiments with their codebase and added the results to our plots (see Figures 3 and 4 in the revised version of the paper). You will notice from the plots that we also included another recent baseline, $\\epsilon$z-greedy [1]. \n\nHowever, note that DCEO (the same goes to Option-Critic and $\\epsilon$z-greedy) is not a direct competitor to the method we propose. DCEO learns options while learning a policy for a given problem. By contrast, Dec-Options are extracted from \"legacy policies\" and re-used in downstream problems; the options are learned even before the agent sees the target task. In some ways, Dec-Options have more information than DCEO because they use legacy policies. In other ways, DCEO has more information than Dec-Options because the options are learned for the target task, while Dec-Options relies on how well the options generalize across similar, but different tasks. Both methods make their unique and valuable contributions. \n\n**More Qualitative Results**\n\nOur way of visualizing what the options are doing is through the sequence of actions with the curly upper and lower brackets as shown at the end of Section 5.2. We also added more of such sequences in Appendix D, following your suggestion. \n\n**Question:** What would be required to scale the method to larger neural networks?\n\n**Answer:** This is a great question! We need to be able to search in the space of sub-programs and we have a few ideas of how this could be done, but we did not want to conflate the problem of searching in the space of sub-policies with the experiments that evaluate whether the sub-policies of existing networks could encode good options. We believe that the activation pattern of networks can offer valuable information on which sub-policies encode valuable information that could be transformed into options. This is a research question that needs to be addressed. \n\n**Question:** Why investigate the ComboGrid environment? What is interesting about this task?\n\n**Answer:** The Combo domain is a hard exploration problem that allows us to easily understand the sub-policies extracted from the neural networks. Moreover, the domain has clear dynamics where options could be helpful. Human designers would possibly design one option for each movement the agent can perform (up, down, left, and right). We wanted to see whether Dec-Options would be able to find something similar without having any prior information of the problem. We were pleased to see that our method discovered options similar to what we would have designed. \n\n**Comment:** \"we consider small neural networks with one hidden layer, so we can evaluate all sub-policies of a neural policy.\" This should be highlighted more.\n\n**Answer:** Good point! We added a sentence in the last paragraph of the Introduction highlighting this methodological choice. \n\n**Reference**\n\n[1] Dabney, W., Ostrovski, G., and Barreto, A. Temporally-extended \u03f5-greedy exploration. In International Conference on Learning Representations, 2021."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541051360,
                "cdate": 1700541051360,
                "tmdate": 1700541051360,
                "mdate": 1700541051360,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JHpkCKCryC",
                "forum": "a8VETFwcVR",
                "replyto": "yPMRgkIYuu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1426/Reviewer_j32D"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1426/Reviewer_j32D"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nthank you for your detailed rebuttal and for adjusting the paper with clarifications and a new baseline.\n\nThe papers has cleared improved in terms of presentation and claims. I also appreciate the visualizations that help understand the method. I would still have some concerns in terms of scalability and fair comparisons to baselines, but the novelty and originality of the work outweighs these concerns. I am hopeful that this paper could propose a different line of work on learning temporal abstractions. I am raising my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672647742,
                "cdate": 1700672647742,
                "tmdate": 1700672647742,
                "mdate": 1700672647742,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]