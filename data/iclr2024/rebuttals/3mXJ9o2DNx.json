[
    {
        "title": "Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization"
    },
    {
        "review": {
            "id": "pCtsCg9P3I",
            "forum": "3mXJ9o2DNx",
            "replyto": "3mXJ9o2DNx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3588/Reviewer_2QYf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3588/Reviewer_2QYf"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of distribution shifts between training and testing datasets in domain generalization (DG). Contrary to expectations, applying contrastive learning (CL) directly in DG settings leads to performance deterioration due to the lack of domain connectivity. To address this, the authors propose domain-connecting contrastive learning (DCCL), which introduces aggressive data augmentation and cross-domain positive samples to improve domain connectivity. They also propose model anchoring to exploit domain connectivity in pre-trained representations. Experimental results on standard DG benchmarks demonstrate that DCCL outperforms state-of-the-art baselines, even without domain supervision."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper presents the paper clearly, making it accessible to a wide range of readers. The exploration of why contrastive learning (CL) is detrimental to Domain Generalization is an intriguing topic.\n2. The paper offers a noteworthy contribution by highlighting the finding that pre-trained models with better domain connectivity can lead to improved performance. The authors provide a clear and reasonable definition of domain connectivity, enhancing our understanding of this important aspect.\n3. Through extensive experiments on standard Domain Generalization benchmarks, the authors demonstrate the effectiveness of their proposed domain-connecting contrastive learning (DCCL) method, surpassing state-of-the-art baselines."
                },
                "weaknesses": {
                    "value": "1. One potential weakness of the paper is that it may overstate its claims regarding the theoretical analysis of why Contrastive Learning harms the performance of Domain Generalization models. The current version lacks rigorous theoretical analysis and relies more on heuristic motivation rather than providing concrete theoretical explanations.\n2. Although the paper proposes improving the similarity of representations across different domains as a key contribution, similar ideas have been previously proposed and utilized in the literature, such as in [1]. This could be seen as a limitation in terms of originality.\n3. The proposed Generative Transformation Loss for Pre-trained Representations raises concerns. The approach of fine-tuning the model to align $z$ and $z^{pre}$ appears counterintuitive, as it may reduce performance when using $z$ for prediction. This introduces a potential trade-off between the ERM loss and the Generative Transformation Loss for DCCL, which needs to be carefully considered.\n4. The paper fails to adequately explain the relationship between domain connectivity and the classification/regression performance, which is the ultimate goal of Domain Generalization. While domain connectivity is an important metric to consider, the authors do not sufficiently address how it directly impacts the predictive performance of the models in real-world scenarios.\n\n[1] Improving Out-of-Distribution Robustness via Selective Augmentation. ICML'22"
                },
                "questions": {
                    "value": "Please see the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3588/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3588/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3588/Reviewer_2QYf"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3588/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698507011136,
            "cdate": 1698507011136,
            "tmdate": 1699636314300,
            "mdate": 1699636314300,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rNZR5DwNOZ",
                "forum": "3mXJ9o2DNx",
                "replyto": "pCtsCg9P3I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3588/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3588/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2QYf"
                    },
                    "comment": {
                        "value": "We appreciate your helpful feedback. We hope the following answers can address your concerns.\n\n\n> **Q1:** It may overstate its claims regarding the theoretical analysis of why Contrastive Learning harms the performance of Domain Generalization models.\n\nWe hope to argue that our claim regarding the theoretical analysis is conservative. It's important to note that our paper's central theme is not just introducing the concept of connectivity and theoretical analysis, but rather emphasizing connectivity enforcement **from both data and model perspectives**.\n\nMeanwhile, to clearly convey the main contribution of our paper to the audience, we don't present any decorative theorems/propositions in the paper. \n\nWe instead aim to propose useful DG methods for practitioners and attribute most of the theoretical analysis to the motivating theory paper [2]. We agree we leverage the theoretical motivation behind [2], as clearly indicated in Section 3.1, for the new setting of DG; if the readers are interested in the theoretical derivative, they will find the analysis in [2] can be applied to DG settings with proper adaptations and regularity conditions. We do not take that as our contribution, while as suggested by the reviewer we've added a remark to the next revision regarding the theoretical analysis.\n\n> **Q2:** The idea of improving the similarity of representations across different domains is proposed and utilized in the literature.\n\nWe want to emphasize the key idea of our paper is not aligning the representations across different domains, but **a unified approach**, combining data and model perspectives with self-supervised objectives to bolster **connectivity**, thereby advancing domain generalization. \n\nFrom the data angle, we build upon the shortcomings of traditional methods that align augmentation views of the same input, as explained in Appendix C. We propose two direct improvements for enhancing domain connectivity through data: \n1. implementing more aggressive data augmentation, and \n2. broadening the range of positive samples, extending beyond self-augmented outputs to include intra-class samples across different domains. Nevertheless, this data alignment strategy merely enhances the clustering of representations within known domains, and may not effectively bridge the gap between unseen test domains and trained domains within the same class. \n\nTherefore, we extend our focus to the model perspective. In the paper, we observed that using a pre-trained ResNet-50 model, intra-class samples from training and testing domains are scattered yet interconnected. By integrating this observation, our paper presents a holistic approach.\n\n> **Q3:** The proposed Generative Transformation Loss for Pre-trained Representations raises concerns.\n\nThanks for bringing up the issue. We'd like to clarify the role of generative transformation as a pivotal proxy objective that facilitates model anchoring. We've described the details in our [general response](https://openreview.net/forum?id=3mXJ9o2DNx&noteId=X8nuMF5cH6) and incorporated them in Section 3.4.\n\n\n\n> **Q4:** The paper fails to adequately explain the relationship between domain connectivity and classification/regression performance. \n\nWe will add the following remark to the next revision for better clarity, while we also hope to argue in the first paragraph of Section 3.1 we have clearly indicated the relationship between domain (intra-class) connectivity and the classification/regression performance is already studied in [2].\n\n\nWe remark that after connecting different domains in the DG settings, we can directly follow the derivation in the contrastive learning theory paper [2] and similarly conclude the classification/regression performance benefits from connectivity. Rather than the theoretical analysis, the primary aim of our paper is to thoroughly improve domain connectivity, addressing both data and model aspects, and ultimately bolstering domain generalization capabilities.\n\n\n[1] Promoting Semantic Connectivity: Dual Nearest Neighbors Contrastive Learning for Unsupervised Domain Generalization. CVPR 2023. \n\n\n[2] Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap. ICLR 2022."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544561323,
                "cdate": 1700544561323,
                "tmdate": 1700544561323,
                "mdate": 1700544561323,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "18ZR5HvgSV",
                "forum": "3mXJ9o2DNx",
                "replyto": "rNZR5DwNOZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3588/Reviewer_2QYf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3588/Reviewer_2QYf"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I have carefully reviewed your feedback as well as the other reviews. I appreciate the authors' rebuttal and understand that emphasizing connectivity enforcement from both data and model perspectives is a novel idea. However, I think that the current version of the paper still requires substantial revisions, such as incorporating the provided clarifications, in order to enhance the clarity of the motivation. Therefore, I have made the decision to maintain my current rating."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646644110,
                "cdate": 1700646644110,
                "tmdate": 1700646644110,
                "mdate": 1700646644110,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ibcVeBTs12",
            "forum": "3mXJ9o2DNx",
            "replyto": "3mXJ9o2DNx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3588/Reviewer_fbsW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3588/Reviewer_fbsW"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on self-supervised learning, especially contrastive learning for domain generalization settings. They analyze the phenomenon with the CL theory and discover the lack of domain connectivity in the DG setting causes the deficiency. Thus they propose domain-connecting contrastive learning (DCCL) to enhance the conceptual connectivity across domains and obtain generalizable representations for\nDG.  Some data augmentation strategies are introduced. The experiments demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. I think applying contrastive learning to DG is very interesting, and might inspire more work in this direction.\n2. The proposed idea is very simple but effective."
                },
                "weaknesses": {
                    "value": "1. My main concern is the discussion of domain connectivity. Is this connectivity based on sample connectivity? Recent work, such as [1], also proposes similar connectivity for domain adaptation but is overlooked in the paper. \n\n2. What is the high-level motivation for the generative transformation loss\uff1fIs it necessary to use augmentation of the same sample, acquiring the class knowledge more effectively?\n\n\n[1] Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation, ICML 2022"
                },
                "questions": {
                    "value": "Please refer to the above weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3588/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767592153,
            "cdate": 1698767592153,
            "tmdate": 1699636314199,
            "mdate": 1699636314199,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "scllWRsnte",
                "forum": "3mXJ9o2DNx",
                "replyto": "ibcVeBTs12",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3588/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3588/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fbsW"
                    },
                    "comment": {
                        "value": "We appreciate your helpful feedback. We hope the following answers can address your concerns.\n\n> **Q1:** Is this connectivity based on sample connectivity? The discussion with [1] is overlooked.\n\nOur definition of connectivity is based on cross-domain sample connectivity within the class. In [1], the assumption is that data adheres to the **stochastic block model (SBM)**  and they use SBM's edge probability to define connectivity. We've referenced [1] and expanded our discussion in relation to [1] to better articulate our approach. \nIt's important to note that our paper's central theme is not just introducing the concept of connectivity, but rather emphasizing its enforcement **from both data and model perspectives**. \n\nFor an intuitive understanding of the connectivity in our paper, we introduced another quantitative metric to help evaluate whether the representation space is ''well-connected'' in Appendix A.4. For images within the same class, we take those images as nodes and construct a graph, only connecting two nodes when their distance on the pre-trained space is smaller than a threshold. We denote the smallest possible threshold which makes the graph \\textbf{connected} as $\\tau$, and denote the mean and the std of the pairwise distances respectively as $\\mu$ and $\\sigma$. We can thus use $(\\tau - \\mu)/\\sigma$ as a metric to describe the connectivity of the representations. \n\n> **Q2:** What is the high-level motivation for the generative transformation loss?\n\nThanks for bringing up the issue. We'd like to clarify the role of generative transformation as a pivotal proxy objective that facilitates model anchoring. We've described the details in our [general response](https://openreview.net/forum?id=3mXJ9o2DNx&noteId=X8nuMF5cH6) and incorporated them in Section 3.4.\n\n\n[1] Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation, ICML 2022"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544498894,
                "cdate": 1700544498894,
                "tmdate": 1700544498894,
                "mdate": 1700544498894,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qqz3VOy2tp",
            "forum": "3mXJ9o2DNx",
            "replyto": "3mXJ9o2DNx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3588/Reviewer_BueW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3588/Reviewer_BueW"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on addressing the shortcomings of self-contrastive learning in Domain Generalization (DG). To enhance domain connectivity within Contrastive Learning (CL), the authors introduce two strategies. Firstly, they suggest anchoring learned maps to pre-trained models that already exhibit the needed connectivity between training and testing domains. Secondly, they introduce a Generative Transformation Loss to further enhance alignment. The paper showcases the effectiveness of their approach, termed DCCL, through extensive experimentation on five DG benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The majority of the paper is straightforward to understand."
                },
                "weaknesses": {
                    "value": "Regarding the motivation behind the proposed method, I have three inquiries:\n1. Could you elaborate on the challenges faced when implementing self-contrastive learning in DG?\n2. What drove the need for more intensive data augmentation and the inclusion of cross-domain positive samples?\n3. Please shed light on the rationale for model anchoring, particularly leveraging domain connectivity in pre-trained representations and its synergy with generative transformation loss.\n\nNotably, the model is constructed on the foundation of SWAD. When evaluated across five benchmarks, the performance improvements over SWAD are recorded as 1.0, 2.9, 3.7, 0.9, and 1.0 respectively. An ablation study assessing the effectiveness of the three components of the proposed method was specifically conducted on the benchmark with a 2.9 gain. Would it be possible to also conduct the ablation study on benchmarks that achieved a performance gain of 1.0 or less? My curiosity stems from questioning whether all three components consistently contribute to the effectiveness as presented in the paper."
                },
                "questions": {
                    "value": "See weaknesses above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3588/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839424990,
            "cdate": 1698839424990,
            "tmdate": 1699636314099,
            "mdate": 1699636314099,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gFbkDlEYHG",
                "forum": "3mXJ9o2DNx",
                "replyto": "qqz3VOy2tp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3588/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3588/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BueW"
                    },
                    "comment": {
                        "value": "We appreciate your helpful feedback. We wish the following answers can address your concerns.\n\n> **Q1:** Could you elaborate on the challenges faced when implementing self-contrastive learning in DG?\n\nThanks for the question. Self-Contrastive Learning (SCL), which aligns the augmentation views of the same input, has achieved successful performance in unsupervised pre-training tasks. \n\nHowever, it does not naturally fit the domain generalization setting since it assumes the ability to sample $x$ from the whole data distribution; in the training stage of domain generalization, we instead are only able to access partial domains. This mismatch can lead to sub-optimal performance in domain generalization if the users mechanically adopt the classical contrastive learning loss. \n\nIn Appendix C of our paper, we provide an example and detailed explanation that even attaining optimal self-contrastive learning loss cannot guarantee good performance in the domain generalization setting. To be specific, in Figure 6, we deliberately visualize the example. Slashes and spots are used to represent domains $d_1$ and $d_2$; orange and blue rectangles respectively denote classes 1 and 2. The mapping function $\\varphi (\\theta(x)) := (\\cos (\\theta), \\sin(\\theta))$ with $\\theta(x) = (x - \\mathrm{sgn}(y)) \\pi$ learned on domain $d_1$ can perfectly classify the samples, and the mapping attains perfect alignment and uniformity (the objective of SCL). However, when applied to a new domain $d_2$, the classifier completely fails (0\\% acc). This example highlights the potential challenges in applying self-contrastive learning effectively in the domain generalization context. We have also updated Appendix C accordingly to better elaborate on the challenges. \n\n\n> **Q2:** What drove the need for more intensive data augmentation and the inclusion of cross-domain positive samples?\n\nBuilding on the response to Q1, SCL in the previous example fails to obtain **intra-class connectivity** due to insufficient data augmentation and domain-separated (rather than class-separated) representations, which ultimately causes poor generalization performance. \n\nInspired by the above analysis, we thus propose two approaches to improve domain connectivity: \n1. applying more aggressive data augmentation and \n2. expanding the scope of positive samples, from solely self-augmented outputs $a(x)$ to the augmentation of intra-class samples across domains.\n\n> **Q3:** Please shed light on the rationale for model anchoring, particularly leveraging domain connectivity in pre-trained representations and its synergy with generative transformation loss.\n\nThe previous connectivity improvements mentioned in Q1 and Q2 only involve data transforms, while this data alignment strategy merely enhances the clustering of representations within known domains, and may not effectively bridge the gap between unseen test domains and trained domains within the same class. \n\nTherefore, we extend our focus to the model perspective. In the paper, we observed that using a pre-trained ResNet-50 model, intra-class samples from training and testing domains are scattered yet interconnected. \n\nBy integrating this observation, our paper presents a unified approach, combining data and model perspectives with self-supervised objectives to bolster connectivity, thereby advancing domain generalization. \n\nRegarding generative transformation, it's a pivotal proxy objective that facilitates model anchoring. We've described the details in our [general response](https://openreview.net/forum?id=3mXJ9o2DNx&noteId=X8nuMF5cH6) and incorporated them in Section 3.4.\n\n> **Q4:** Would it be possible to also conduct the ablation study on benchmarks that achieved a performance gain of 1.0 or less?\n\nThank you for your interest. We are glad to offer more empirical evidence to support our findings.\n\nOur previous decision to conduct the ablation study exclusively on the OfficeHome dataset was primarily to reduce computational demands. Here we additionally performed an ablation study on the VLCS dataset, as shown in the table below, where the performance gain is 0.9. These results further confirm that the three components we identified contribute consistently to the effectiveness, as detailed in our paper. We've included the experimental results in Appendix A.5 of our revision.\n\n|   Algorithm  |   C    |    L   |   S   |    V|Avg          |\n|:------------:|:---------------:|:------:|:---------------:|:---------------:|:----------------------:|\n|     SWAD     |       98.8      |  63.3  |       75.3      |       79.2      |          79.1          |\n| DCCL w/o CDC |       98.9      |  63.8  |       75.6      |       79.5      |          79.4          |\n| DCCL w/o PMA |       98.6      |  63.7  |       75.7      |       79.3      |          79.3          |\n|  DCCL w/o GT |       98.7      |  **64.3**  |       75.2      |       80.2      |          79.6          |\n| DCCL| **99.1** | 64.0 | **76.1** | **80.7** | **80.0** |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544379181,
                "cdate": 1700544379181,
                "tmdate": 1700544450813,
                "mdate": 1700544450813,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bLlidvTcZZ",
            "forum": "3mXJ9o2DNx",
            "replyto": "3mXJ9o2DNx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3588/Reviewer_ttfL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3588/Reviewer_ttfL"
            ],
            "content": {
                "summary": {
                    "value": "Authors incorporate contrastive learning into domain generalization by modifying the contrastive strategy with the help of cross-domain samples and pretrained representations. Experiments on benchmarks show the effectiveness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea and logic are basically complete. \n\nExperiments show the effectiveness of the proposed algorithm, including ablation studies for three components each."
                },
                "weaknesses": {
                    "value": "- The toy experiment of illustrating the drawbacks of directly applying SCL to DG is put in appendix, which is a key part of the entire logic of this paper and should be emphasized. However, this part currently seems confusing and hard to follow, and the removal of data augmentation in this experiment make it too unrealistic. \n- The term \"domain connectivity\" is a little confusing. It is more like \"domain-invariant intra-class connectivity\", while \"domain connectivity\" seems like inner-domain connectivity. \n- Experimental improvement is generally not large compared with current SOTA."
                },
                "questions": {
                    "value": "- In Sec 3.2, whether samples come from the same domain or not, they belong to positive pairs as long as they are from the same class. I understand this has the benefit of not using domain labels, but if truly targeting at cross-domain positive pairs, only samples from the same class but belonging to different domains should be treated as positive pairs, otherwise it cannot be determined whether \"cross-domain\" is important here. \n- In Sec 3.4, why is the generative transformation loss needed when there is pretrained model anchoring already? I think both of them serve as regularization to constrain the representation space not far away from the pretrained representation space. From this perspective, generative transformation loss seems a little ad-hoc. \n- In appendix C, paragraphs are duplicated. As stated above, I think this part is an important component of the whole paper story, so I wonder if authors can modify it into a clearer version."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3588/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3588/Reviewer_ttfL",
                        "ICLR.cc/2024/Conference/Submission3588/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3588/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698919380203,
            "cdate": 1698919380203,
            "tmdate": 1700662833675,
            "mdate": 1700662833675,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pZUflok6HA",
                "forum": "3mXJ9o2DNx",
                "replyto": "bLlidvTcZZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3588/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3588/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ttfL"
                    },
                    "comment": {
                        "value": "We appreciate your helpful feedback. We hope the following answers can address your concerns.\n\n> **Q1:** The toy experiment of illustrating the drawbacks of directly applying SCL to DG is put in the appendix, which is a key part of the entire logic of this paper and should be emphasized. In Appendix C, paragraphs are duplicated. As stated above, I think this part is an important component of the whole paper story.\n\nThank you for your suggestion. We have updated Appendix C accordingly. \n\nThis example is mainly motivational (i.e., not as indispensable as parts), and thus we put it in the Appendix due to space limit. We'll incorporate this revised section into the main body of the arxiv version for coherent narrative flow. \n\nTo elaborate, in Figure 6, we deliberately visualize the example. Slashes and spots are used to represent domains $d_1$ and $d_2$; orange and blue rectangles respectively denote classes 1 and 2. The mapping function $\\varphi (\\theta(x)) := (\\cos (\\theta), \\sin(\\theta))$ with $\\theta(x) = (x - \\mathrm{sgn}(y)) \\pi$ learned on domain $d_1$ can perfectly classify the samples, and the mapping attains perfect alignment and uniformity (the objective of SCL). However, when applied to a new domain $d_2$, the classifier completely fails (0\\% acc).  \n\n\n> **Q2:** Experimental improvement is generally not large compared with the current SOTA.\n\nWe hope to argue that our method's enhancement is at least on par with, if not greater than, existing baseline methods. \n\nDetailed examples: In the VLCS and OfficeHome datasets, our methods \u2013 DCCL, SelfReg, PCL, and MIRO \u2013 show improvements over the state-of-the-art (SOTA) benchmarks by margins of 0.4/1.1, 0.3/-0.8, -1.0/1.0, and 0.5/0.8, respectively. Furthermore, within the domain generalization community,  with the same backbone, fixed data splitting, and long-term development, it's important to note that achieving significant absolute improvements is challenging. It often requires the combined efforts of multiple research papers.\n\n> **Q3:** The term \"domain connectivity\" is a little confusing. It is more like \"domain-invariant intra-class connectivity\", while \"domain connectivity\" seems like inner-domain connectivity.\n\nThanks for the advice. We follow the usage in [1] and have updated the term 'domain connectivity' to 'intra-class connectivity' for greater clarity and to more accurately convey our intent.\n\n[1] Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation, ICML 2022\n\n> **Q4:** Question regarding whether samples come from the same domain or not, they belong to positive pairs as long as they are from the same class.\n\n\nWe clarify that the 'cross-domain' positive samples in our study encompass both samples from the same and different domains. \n\nThis is because we lack domain labels for the samples. In Appendix A.5 of our submitted paper, we provide a rationale for using cross-domain contrast (CDC), which shows an accuracy of 71.8\\% as noted in Table 2. To contrast, we conducted a baseline experiment using only within-domain positive samples and observed a substantial drop in accuracy when compared to CDC (from 71.8\\% to 70.4\\%). Furthermore, an oracle experiment exclusively using cross-domain positive pairs yielded similar results (from 71.8\\% to 71.9\\%). These results validate our approach and suggest that, if domain information is available, its effective utilization might require thoughtful design to achieve notable improvements.\n\n> **Q5:** why is the generative transformation loss needed when there is pretrained model anchoring already?\n\nThanks for bringing up the issue. We'd like to clarify the role of generative transformation as a pivotal proxy objective that facilitates model anchoring. We've described the details in our [general response](https://openreview.net/forum?id=3mXJ9o2DNx&noteId=X8nuMF5cH6) and incorporated them in Section 3.4."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544110613,
                "cdate": 1700544110613,
                "tmdate": 1700544110613,
                "mdate": 1700544110613,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a03gp7lCW4",
                "forum": "3mXJ9o2DNx",
                "replyto": "pZUflok6HA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3588/Reviewer_ttfL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3588/Reviewer_ttfL"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply! Many of my concerns have been addressed, and I appreciate the efforts authors have made in their rebuttal and paper revision. Generally I like the story and logic of this paper, but the experimental part is still not that convincing to me. I think there is still some space for improvements in the current version. \n\nIn summary, I decide to raise my score from 3 to 5."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662787339,
                "cdate": 1700662787339,
                "tmdate": 1700662787339,
                "mdate": 1700662787339,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zm10akUWtg",
            "forum": "3mXJ9o2DNx",
            "replyto": "3mXJ9o2DNx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3588/Reviewer_NxUh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3588/Reviewer_NxUh"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new domain generalization method that consists of a ERM loss, a contrastive learning loss with cross-domain data augmentation and a generative transformation loss that exploits the supervised signal at the inter-sample. Experimental results seem to validate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is clearly written and generally easy to follow.\n\n- The experimental results look good and the proposed DCCL achieves very competitive performance on a few benchmarks."
                },
                "weaknesses": {
                    "value": "- The combination of contrastive learning and continual learning has been explored and shown effective multiple times. See [1,2]. The contribution of this work is mostly on how you augment the positive samples. The way the paper augments the samples across different domains is similar to [3,4].\n\n- I fail to understand why generative transformation helps. Could the authors elaborate more on why learning an additional decoder for the generative transformation can help domain generalization? I can see that this is to exploit more information from the sample itself, but why it can make domain generalization better is unclear.\n\n- Overall, I find the proposed method a bit ad-hoc, as it combines multiple disconnected components to improve the final performance. Why these components can work coherently to benefit the domain generalization task is unclear and also poorly motivated. It is insufficient to simply state that some losses are for intra-sample level and some for inter-sample level. It still does not explain why it works.\n\n[1] SelfReg: Self-supervised Contrastive Regularization for Domain Generalization. ICCV 2021\n\n[2] PCL: Proxy-based Contrastive Learning for Domain Generalization. CVPR 2022\n\n[3] Towards Principled Disentanglement for Domain Generalization, CVPR 2022\n\n[4] Model-based domain generalization, NeurIPS 2021"
                },
                "questions": {
                    "value": "See the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3588/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699254334124,
            "cdate": 1699254334124,
            "tmdate": 1699636313948,
            "mdate": 1699636313948,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "819QWdtLHY",
                "forum": "3mXJ9o2DNx",
                "replyto": "zm10akUWtg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3588/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3588/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NxUh"
                    },
                    "comment": {
                        "value": "We appreciate your helpful feedback. We hope the following answers can address your concerns.\n\n> **Q1:** The combination of contrastive learning and continual learning has been explored and shown effective multiple times.\n\nThanks for raising the question. \n\nPrevious research, as referenced in [1,2], primarily tackled the positive data alignment issue in domain generalization by developing proxy objectives. Meanwhile, studies in [3,4] advanced the alignment of training domains, utilizing specially designed image generators to create domain-transformed positive samples. \n\nHowever, these approaches primarily involve data transforms. In contrast, our work diverges by exhaustively exploring the concept of connectivity from both data and model perspectives. \nOn one hand (data), we build upon the shortcomings of traditional methods that align augmentation views of the same input, as explained in Appendix C. \nWe propose two direct improvements for enhancing domain connectivity through data: \n1. implementing more aggressive data augmentation, and \n2. broadening the range of positive samples, extending beyond self-augmented outputs to include intra-class samples across different domains. \nNevertheless, this data alignment strategy merely enhances the clustering of representations within known domains, and may not effectively bridge the gap between unseen test domains and trained domains within the same class. \n\nWe therefore extend our focus to the model perspective. In the paper, we observed that using a pre-trained ResNet-50 model, intra-class samples from training and testing domains are scattered yet interconnected. By integrating this observation, our paper presents a unified approach, combining data and model perspectives with self-supervised objectives to bolster connectivity, thereby advancing domain generalization.\n\n> **Q2:** Could the authors elaborate more on why learning an additional decoder for the generative transformation can help domain generalization?\n\nThanks for bringing up the issue. We'd like to clarify the role of generative transformation as a pivotal proxy objective that facilitates model anchoring. This can be understood from multiple angles: \n1. Echoing the findings in [2] that directly aligning positive pairs across vastly different domains often results in poor performance, our research similarly identifies a substantial gap in the representations of pre-trained and fine-tuned models. Direct alignment using contrastive learning, as evidenced by our findings in Table 2, tends to be sub-optimal. In response, we introduce the concept of variational generative loss to **comprehend the transformation process** and bridge these representational gaps. \n2. Additionally, the generative transformation module is designed to reconstruct the features of the pre-trained model **at an intra-sample level**. This complements the inter-sample level supervision provided by contrastive loss. The module, along with its associated loss function, is intended to provide a more enriched supervised signal, encapsulating crucial within-sample information. This, in turn, supports and enhances the anchoring of the pre-trained model.\n\n> **Q3:** Why these components can work coherently to benefit the domain generalization task is unclear.\n\nAs shown in the response to Q1, our approach combines data and model perspectives with self-supervised objectives to bolster connectivity, thereby advancing domain generalization. \n\n\n[1] SelfReg: Self-supervised Contrastive Regularization for Domain Generalization. ICCV 2021\n\n[2] PCL: Proxy-based Contrastive Learning for Domain Generalization. CVPR 2022\n\n[3] Towards Principled Disentanglement for Domain Generalization, CVPR 2022\n\n[4] Model-based domain generalization, NeurIPS 2021"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543591642,
                "cdate": 1700543591642,
                "tmdate": 1700543591642,
                "mdate": 1700543591642,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]