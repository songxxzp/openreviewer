[
    {
        "title": "InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4"
    },
    {
        "review": {
            "id": "mFPqrN23mF",
            "forum": "DNvzCsQG1D",
            "replyto": "DNvzCsQG1D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2243/Reviewer_PGky"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2243/Reviewer_PGky"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces InstructionGPT-4 which achieves better output with only 200 high-quality examples. The authors propose metrics and a trainable data selector to filter low-quality vision-language data. InstructionGPT-4 outperforms MiniGPT-4, showcasing the efficiency of using less but high-quality instruction data for improving multimodal language models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The question posed by the author is highly significant, namely the impact of instruction data quality on the performance of MLLM, which has been rarely discussed before.\n2. The method described in the article is written quite clearly and is easy to understand. The entire method is also concise, and the results on several benchmarks demonstrate its effectiveness."
                },
                "weaknesses": {
                    "value": "1. The experiments in the article are relatively limited and the dataset used has a small sample size, which raises doubts about the effectiveness of the proposed method when applied to larger datasets.\n2. The authors should have included some baseline methods for data selection, such as the mentioned instruction mining. It is not surprising that removing low-quality data can improve performance, and having these baselines would provide a better comparison and evaluation of the proposed approach."
                },
                "questions": {
                    "value": "1. Regarding the data selector, although it appears to be concise, it can be time-consuming as it requires continuous training of the model on the validation dataset for testing. This becomes particularly challenging when dealing with large amounts of data.\n2. The article suggests that the data selector can be directly applied to other multimodality datasets. It would be beneficial if the authors could provide evidence or supporting experiments. \n3. The dataset size of 3.4k in MiniGPT-4 is relatively small in terms of instruction data. It would be interesting to investigate whether the proposed method remains effective on larger instruction datasets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2243/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2243/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2243/Reviewer_PGky"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2243/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698497086307,
            "cdate": 1698497086307,
            "tmdate": 1699636157667,
            "mdate": 1699636157667,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6A4FilXFgf",
                "forum": "DNvzCsQG1D",
                "replyto": "mFPqrN23mF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PGky (1/2)"
                    },
                    "comment": {
                        "value": "Q1.1: The experiments in the article are relatively limited and the dataset used has a small sample size, which raises doubts about the effectiveness of the proposed method when applied to larger datasets. \n\nQ1.2: The article suggests that the data selector can be directly applied to other multimodality datasets. It would be beneficial if the authors could provide evidence or supporting experiments. \n\nQ1.3: The dataset size of 3.4k in MiniGPT-4 is relatively small in terms of instruction data. It would be interesting to investigate whether the proposed method remains effective on larger instruction datasets.\n\nA1 for Q1.1, Q1.2 and Q1.3: Thanks for your questions. In particular, we want to find out **if there exists a subset making InstructionGPT-4 achieve better performance**. As a result, our proposed method indeed shows the existence of the subset. Using only 6% of the data, we manage to make InstructionGPT-4 surpass MiniGPT-4 in most tasks, which is quite surprising. \n\nTo address your concern about scalability and generalizability, we also extend our investigation to include additional models and datasets. **Notably, we have applied our method to Qwen_VL [1] and the detail_10k [2] dataset without retraining the data selector**.\nHere are our experimental results:\n\n| detail_10k   | Qwen_VL (1k) | Qwen_VL (2k) | Qwen_VL (10k) | MiniGPT-4 (1k) | MiniGPT-4 (2k) | MiniGPT-4 (10k) |\n|:------------:|:------------:|:------------:|:-------------:|:--------------:|:--------------:|:---------------:|\n| MME          |    1802.50   |   **1806.81**|    1769.28    |   **614.04**   |      608.54    |      604.47     |\n| Perception   |    1423.57   |   **1426.10**|    1398.92    |   **434.04**   |      431.04    |      428.54     |\n| Cognition    |    378.93    |   **380.71** |    370.36     |   **180.00**    |      177.50    |      173.93     |\n\nBy comparing the performance between models tuned from selected subsets and the whole dataset, we observe that less instruction data for better performance still work for various MLLMs. Our findings indicate promising results, suggesting that our method is not only effective with MiniGPT-4's datasets but also exhibits potential for broader applicability. \n\n[1] Bai, et al. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.\n\n[2] Liu, Li, et al. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.\n\nQ2: The authors should have included some baseline methods for data selection, such as the mentioned instruction mining. \n\nA2: Thanks for your comment. We would like to highlight that **we are the first to propose a data selection method working on multimodal LLM for generative tasks**. Though there are no other data seletion methods for multimodal LLMs, we actually compare our method with single-modal baselines in our ablation study. In particular, **we compare our proposed selector with the recent methods such as Alpagasus (GPT Score only) in Table 5 and Instruction Mining (Linear Selector) in Figure 5**, demonstating that data selected by our multimodal selector works much better than these two methods.  \nMoreover, **we also conduct additional experiments on other data pruning methods proposed in single vision modal, including EL2N [1] and prototypicality [2]**. We represent our experiment results below:\n\n| Benchmark | InstructionGPT-4 |  EL2N  | Prototypicality | Random |\n|:---------:|:----------------:|:------:|:---------------:|:------:|\n| MME       |      **648.26**      | 627.27 |     569.46      | 527.26 |\n| VQA       |      **22.30**       |  20.62 |     21.83       |  19.87 |\n\nWe observe that these previous methods can't achieve competitve performance for multimodal LLM compared to our data selector. It showcases that our novel data selection method for multimodal datasets is quite necessary. We also add these experiments to the revised version along with the above detailed discussion.\n\n[1] Paul, Ganguli, Dziugaite G K. Deep learning on a data diet: Finding important examples early in training. NeurIPS, 2021.\n\n[2] Sorscher, Ben, et al. Beyond neural scaling laws: beating power law scaling via data pruning. NeurIPS, 2022."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700341470836,
                "cdate": 1700341470836,
                "tmdate": 1700396907759,
                "mdate": 1700396907759,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fiqHbsLPfw",
                "forum": "DNvzCsQG1D",
                "replyto": "mFPqrN23mF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PGky (2/2)"
                    },
                    "comment": {
                        "value": "Q3: Regarding the data selector, although it appears to be concise, it can be time-consuming as it requires continuous training of the model on the validation dataset for testing. This becomes particularly challenging when dealing with large amounts of data.\n\nA3: Thank you for your comment. In fact, we have designed our method with efficiency in mind, balancing the need for accurate data selection against the constraints of resource usage.\n\nFirst of all, based on our data selection method mentioned in Section 2.3, **the time and resource can be lowered by reducing the number of divided subsets, which means training fewer models to gain quality labels**. \n\nSecondly, **the cost of our data selector's training is acceptable and reasonable compared to constructing new high-quality datasets**. Inspired by LIMA [1]'s approach of manually curating 1000 high-quality instruction data for fine-tuning, we take this concept further by **selecting only 200 high-quality multimodal data points from raw datasets**. Our data selection method avoid the need of curating high-quality instruction data from scratch, but select high-quality data from exisiting datasets. This finding is particularly significant given the complexity and resource demands of constructing new datasets.\n\nMost importantly, once our data selector is trained on one dataset, **it can be applied to other datasets without the need for retraining** as mentioned in A1 above. This reusability makes the initial investment in training the selector more cost-effective over time. When dealing the large amounts of data, the data selector can directly identify high-quality data points based on the criteria learned during its initial training.\n\nThus, our data selector is practical and resource-efficient when dealing with new datasets.\n\n[1] Zhou, Liu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700341506450,
                "cdate": 1700341506450,
                "tmdate": 1700400348132,
                "mdate": 1700400348132,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qcCzIY6ZkG",
                "forum": "DNvzCsQG1D",
                "replyto": "fiqHbsLPfw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2243/Reviewer_PGky"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2243/Reviewer_PGky"
                ],
                "content": {
                    "title": {
                        "value": "Response for the rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your detailed rebuttal. Most of the initial concerns have been addressed satisfactorily. The results obtained on the new dataset without retraining are promising.\n\nHowever, I share the same concern as reviewer dCjW regarding the inclusion of a large number of experiments within a short time frame. Other reviewers have also expressed similar concerns, focusing on the simplicity of the chosen baselines and the practicality and generalizability of the proposed method. These concerns should be addressed in the initial version of the paper as they are so crucial for this study. \n\nI also suggest that the authors provide more specific examples that are filtered by the algorithm, and more dialogue examples to show what aspect of the model has been enhanced for each dataset. This will enable readers to better comprehend the effectiveness of your method.\n\n\nI appreciate the clarification you have provided and the additional experiments conducted during the rebuttal period. I believe they will greatly enhance the impact of the paper. However, at this time, I have chosen to maintain my original score. I do not deny the importance of your research. It's just that, at present, it doesn't seem to be ready yet."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634919338,
                "cdate": 1700634919338,
                "tmdate": 1700634919338,
                "mdate": 1700634919338,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5VFgiJNqpF",
                "forum": "DNvzCsQG1D",
                "replyto": "mFPqrN23mF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PGky"
                    },
                    "comment": {
                        "value": "Dear Reviewer PGky,\n\nThank you for reviewing our rebuttal and provide further comments. We are happy that most of the initial concerns have been addressed satisfactorily.\n\nIn response to the concern about the large number of experiments conducted within a short timeframe, **we have added comprehensive details about the experiments in the revised manuscript**. This includes the rationale behind the selection of experiments, the methodologies employed, and the significance of their outcomes. This additional information aims to alleviate concerns regarding the scope and pace of our experimental work.\n\nRegarding your concern about the simplicity of the chosen baselines and the practicality and generalizability of the proposed method, we have clarified this in our latest version. Specifically, previous data pruning methods as baselines data pruning methods either rely on the model \"loss\" from the \"training dataset\" where there exists a gap to the \"performance\" on the \"test dataset\" in our problem, or require accesses to the evaluation metric which involve extensive computations and are impractical for large models. In contrast, we adopt a learning-based method by training a data selector, which can be applied to new datasets independent of certain models. Specifically, we extract the multimodal data indicators to train the selector based on MLLM performance, which can reduce the gap of data distribution in evaluation set. Furthermore, data pruning aims at cutting off bad data for training, while we concentrate on identifying a minimal yet effective subset of data, offering a novel perspective in MLLM's training.\n\nWe would like to emphasize that reviewer dCjW's concerns regarding the inclusion of a large number of data pruning methods as baselines come from their misunderstanding of our motivation. In fact, **it is unfair for data pruning methods to compare with our method** in this scenario (as they indeed empirically underperform our method). Instead, **we design a learning-based method** by formulating a set of indicators for assessment and train a neural network as a data selector to fit the indicators to the genuine quality labels for selection. Thus, our related baselines should be Instruction Mining [1] and Alpagasus [2], which are included in our initial version of paper.\n\nIn addition, **the added experiments on Qwen-VL and detail_10K dataset have demonstrated the practicality and generalizability of the proposed method**. Given specific dataset, we only need to extract their indicators for the trained data selector without conducting any training or inferencing on other MLLMs. The promising results indeed demonstrate the practicality and generalizability of the proposed method compared with other data pruning methods.\n\nTo enhance the reader's understanding of our method's impact, **we have incorporated a selection of filtered examples and dialogues in the latest version in Appendix C**. These examples should illustrate how the data selector improves the data quality, providing a clearer understanding of the data selector's impact and effectiveness. Besides, the chatbot demos in Appendix B.6 show how our method enhances different aspects of the model in generation.\n\nGiven that most of your concerns have been addressed satisfactorily and we provided further experiments to respond to your remaining questions, we would like to humbly request a reconsideration of the scoring based on our latest version. We believe our research can provide valuable insights and contributions that would be of great interest to the ICLR community.\n\n[1] Cao, Kang, Sun. Instruction mining: High-quality instruction data selection for large language models. arXiv preprint arXiv:2307.06290, 2023.\n\n[2] Chen, Li, et al. Alpagasus: Training a better alpaca with fewer data. arXiv preprint arXiv:2307.08701, 2023."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662075345,
                "cdate": 1700662075345,
                "tmdate": 1700673703287,
                "mdate": 1700673703287,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BuI9mUYLhP",
            "forum": "DNvzCsQG1D",
            "replyto": "DNvzCsQG1D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2243/Reviewer_61XF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2243/Reviewer_61XF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a data selection method for finetuning Multimodal Large Language Models (MLLMs), specifically for MiniGPT4 using the training dataset \"cc_sbu_align\". The authors evaluate the data quality by dividing the whole dataset into some subsets and evaluating the model performance on validation sets. A data selector is trained based on the obtained data quality as labels. The data selector is used to choose a small subset from the whole dataset for final MLLM finetuning. Specifically, 200 data samples are chosen from around 3k samples, and the resulting MLLM has comparable performance as the model finetuned with the whole dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The main strength is that the authors managed to train an MLLM with fewer samples and obtained a comparable performance as using the whole dataset."
                },
                "weaknesses": {
                    "value": "- The authors did not demonstrate the generality of the method. There are many nuances in the method and the experiments are done on only one dataset \"cc_sbu_align\". It is unclear whether the method will work on another dataset or another MLLM.\n- There are many systematic methods in the literature that do something similar to select a subset from a dataset based on the model performance on a validation set. A notable one is Meta-Weight-Net [1] based on meta learning. How does this InstructionGPT-4 compare with the Meta-Weight-Net?\n- The method obtained slightly better scores than the MiniGPT4 for some scores, but it is partially weaker than MiniGPT4. Moreover, in the benchmarks MME and MMBench, MiniGPT4 is a weak baseline. The marginal improvement over MiniGPT4 is not convincing. \n- If the data selector $F$ takes a data sample as input (see questions below for this), then in the paper the authors actually assign the same label to every data sample in the same subset $D_i$, which does not make sense because the label is due to the contribution of the whole subset (the model performance after being trained on the subset).\n\n[1] Shu et al., Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting"
                },
                "questions": {
                    "value": "- What features of the data samples did the authors use for the clustering algorithms? Are the features flattened images or some extracted image features?\n- The description in Section 2.3 is confusing. In the \"Training\" paragraph, it seems that the data selector $F$ maps from the feature of a __whole__ subset to a score, because the index $i$ is for the subsets and it seems to mean $y_i = F(e_i)$. But in the \"Testing\" paragraph, it seems that the data selector $F$ maps from the feature of a single data sample to a score. Can the authors clarify it?\n- Why did the authors not simply use the quality scores to select a subset for finetuning the final MLLM? It seems that the training data and the testing data for the data selector are the same. What is the meaning of performing the testing if we already have the labels on the data?\n- What is the relationship between the validation data used to generate the quality labels and the evaluation data? Why could the model performance on validation data be used as an indicator for model performance on the evaluation (test) data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2243/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698569611402,
            "cdate": 1698569611402,
            "tmdate": 1699636157589,
            "mdate": 1699636157589,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wHmS9joFuD",
                "forum": "DNvzCsQG1D",
                "replyto": "BuI9mUYLhP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 61XF (1/3)"
                    },
                    "comment": {
                        "value": "Q1: The authors did not demonstrate the generality of the method.\n\nA1: Thank you for your comments. Following LIMA [1] that proposes \"less is more for alignment\" only using LLaMA in textual modal, our main motivation is to prove that less but high-quality instruction data can outperform the whole dataset for multimodal LLMs. In particular, we want to find out **if there exists a subset making InstructionGPT-4 achieve better performance**. As a result, our proposed method indeed shows the existence of the subset. Besides, we want to focus on MiniGPT-4 because it's the first and most popular multimodal model utilizing LLM. \nTo address your concern about scalability and generalizability, we also extend our investigation to include additional models and datasets. **Notably, we have applied our method to Qwen_VL [2] and the detail_10k [3] dataset without retraining the data selector**.\nHere are our experimental results:\n\n| detail_10k   | Qwen_VL (1k) | Qwen_VL (2k) | Qwen_VL (10k) | MiniGPT-4 (1k) | MiniGPT-4 (2k) | MiniGPT-4 (10k) |\n|:------------:|:------------:|:------------:|:-------------:|:--------------:|:--------------:|:---------------:|\n| MME          |    1802.50   |   **1806.81**|    1769.28    |   **614.04**   |      608.54    |      604.47     |\n| Perception   |    1423.57   |   **1426.10**|    1398.92    |   **434.04**   |      431.04    |      428.54     |\n| Cognition    |    378.93    |   **380.71** |    370.36     |   **180.00**    |      177.50    |      173.93     |\n\nBy comparing the performance between models tuned from selected subsets and the whole dataset, we observe that less instruction data for better performance still work for various MLLMs. Our findings indicate promising results, suggesting that our method is not only effective with MiniGPT-4 but also exhibits potential for broader applicability. \n\n[1] Zhou, Liu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.\n\n[2] Bai, et al. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.\n\n[3] Liu, Li, et al. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.\n\nQ2: There are many systematic methods in the literature that do something similar to select a subset from a dataset based on the model performance on a validation set. A notable one is Meta-Weight-Net [1] based on meta learning. How does this InstructionGPT-4 compare with the Meta-Weight-Net?\n\nA2: Thanks for your suggestion. It's important to clarify that while both our method and Meta-Weight-Net aim to improve model performance through selective data, **they are based on different learning mechanisms**.\nMeta-Weight-Net, as you mentioned, is grounded in meta-learning. It typically involves generating weights for training samples in a classification task, which are then used to adjust the loss function during the training of a model like ResNet. This approach is particularly suited for classification tasks where the output of a MLP can effectively influence the training process of the model.\n\nIn contrast, **our work focuses on subset selection for generative tasks within the multimodal LLM domain**. Our approach is not about adjusting weights during training but rather about selecting a subset of data that improves the efficacy of fine-tuning for generative tasks. This involves developing a multimodal data selector that identifies the most impactful data for model alignment, which is a different challenge compared to what Meta-Weight-Net addresses.\n\nGiven the generative nature of our task and the multimodal aspects of our model, applying a meta-learning approach like that of Meta-Weight-Net directly to our context would be challenging. The complexity of generative tasks in multimodal settings requires a different strategy, which is what we have developed and focused on.\n\nWe have included citations for Meta-Weight-Net in the revised version along with the above detailed discussion.\n\nBesides, we conduct additional experiments on several data pruning methods including EL2N [1] and prototypicality [2], which were proposed in vision modal. We represent our experiment results below:\n      \n| Benchmark | InstructionGPT-4 |  EL2N  | Prototypicality | Random |\n|:---------:|:----------------:|:------:|:---------------:|:------:|\n| MME       |      **648.26**      | 627.27 |     569.46      | 527.26 |\n| VQA       |      **22.30**       |  20.62 |     21.83       |  19.87 |\n\n\nWe observe that these previous systematic methods for single modal can't achieve competitve performance for multimodal LLM compared to our data selector. It showcases that our novel data selection method for multimodal datasets is quite necessary. We also add these experiments to the revised version along with the above detailed discussion."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700341340477,
                "cdate": 1700341340477,
                "tmdate": 1700396872799,
                "mdate": 1700396872799,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bqMgb2CvYk",
                "forum": "DNvzCsQG1D",
                "replyto": "BuI9mUYLhP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 61XF (2/3)"
                    },
                    "comment": {
                        "value": "Q3: The method obtained slightly better scores than the MiniGPT4 for some scores, but it is partially weaker than MiniGPT4. Moreover, in the benchmarks MME and MMBench, MiniGPT4 is a weak baseline. The marginal improvement over MiniGPT4 is not convincing. \n\nA3: Thanks for your question. **Our primary goal in this research is to explore and demonstrate the impact of using a smaller but higher-quality dataset on the performance of multimodal LLMs**. Indeed, our findings show that with tuned by only 6% of the original data, InstructionGPT-4 is able to surpass the performance of MiniGPT-4 in most tasks, which is a notable accomplishment. This outcome challenges the conventional notion that more data invariably leads to better model performance, highlighting instead the value of high-quality, well-selected instruction data.\n\nWhile the improvements over MiniGPT-4 may appear marginal in some aspects, they are still significant in our research objectives. **Our achievement lies in not only surpassing an established baseline but also firstly proposing a novel method for selecting high-quality multimodal data**. This finding has important implications for the efficiency and sustainability of training large LLMs, suggesting that strategic data selection can reduce resource requirements without compromising performance.\n\nFurthermore, we choose MiniGPT-4 for experiments because it is a widely recognized and accessible multimodal LLM, making it a practical choice for benchmarking. Additionally, **we extend our experiments to include Qwen_VL, a state-of-the-art open-sourced model with a strong baseline, to validate our approach across different multimodal LLMs, which is also mentioned in A1 above**.\n\nQ4: If the data selector $F$ takes a data sample as input (see questions below for this), then in the paper the authors actually assign the same label to every data sample in the same subset $D_i$, which does not make sense because the label is due to the contribution of the whole subset (the model performance after being trained on the subset).\n\nA4: Thanks for your comment. **The decision to assign the same label to every data sample in the same subset $D_i$ is indeed a tradeoff, and I'd like to clarify the rationale behind this approach**. \n\nIf we were to fine-tune our pre-trained model on each individual data sample and then evaluate the performance of the fine-tuned model to determine quality labels, the process would require training and evaluating thousands of models. This approach would be prohibitively expensive and time-consuming, making it impractical for our purposes. At the same time, for the second-stage fine-tuning of multimodal LLMs, the core purpose is to align the instructions, responses and images. If the model is only fine-tuned on a very small dataset, the model training is not sufficient to obtain good alignment results, and the evaluation will be meaningless at this situation. \n\nThus, we need to divide the original dataset into several groups. The size of each group cannot be too large or too small. On the one hand, if each group contains too much data, then there will be too few groups to fit an appropriate mapping relationship when training the data selector. On the other hand, if each group contains too little data, then the MLLM cannot be fully fine-tuned for alignment. So in this tradeoff, we choose to divide the data into 30 groups to ensure that there are enough samples during fitting and that the MLLM can achieve good alignment results when fine-tuned on each group. \n\n\nQ5: What features of the data samples did the authors use for the clustering algorithms? Are the features flattened images or some extracted image features?\n\nA5: Thank you for your question. As outlined in Section 2.1 of our paper, **we use image embeddings for the clustering process**. Specifically, we do not use raw, flattened images directly. Instead, we extract and encode image features to create these embeddings, which then serve as the input for our spectral clustering algorithm.\nThese image embeddings are derived from a pre-trained ViT, which is capable of converting the visual content of the images into a more abstract and condensed representation. This representation captures the essential characteristics of the images. By using these encoded image embeddings, our spectral clustering algorithm can more effectively categorize the data into ten distinct groups based on the inherent similarities and differences in the visual features."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700341375881,
                "cdate": 1700341375881,
                "tmdate": 1700395977124,
                "mdate": 1700395977124,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vt7QUiaRKH",
                "forum": "DNvzCsQG1D",
                "replyto": "BuI9mUYLhP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 61XF (3/3)"
                    },
                    "comment": {
                        "value": "Q6: The description in Section 2.3 is confusing. In the \"Training\" paragraph, it seems that the data selector F maps from the feature of a whole subset to a score, because the index i is for the subsets and it seems to mean yi=F(ei). But in the \"Testing\" paragraph, it seems that the data selector F maps from the feature of a single data sample to a score. Can the authors clarify it?\n\nA6: Thank you for your inquiry regarding the features used to train the data selector F. Assume that each data point in our dataset is represented by a D-dimensional feature vector. When we form a cluster of K data points, these D-dimensional vectors are concatenated to create a feature vector with a shape of (K, D). **In training the data selector F, we utilize these (K, D) shape vectors**. Each cluster, represented by its (K, D) vector, is associated with a \"genius quality label\" that reflects the collective quality of all K data points within the cluster. This approach allows us to train the data selector on a rich and detailed representation of data clusters.\n\nRegarding the application of F on a single data point as discussed in Section 2.3, it's important to clarify that F is trained on clusters but is capable of evaluating individual data points as well. **When applied to a single data point, F operates on its (1, D) feature vector**. This flexibility is a key aspect of F, allowing it to function effectively both at the cluster level during training and at the individual data point level during testing.\n\nTo address the potential confusion this might cause, we have revised the relevant sections of our paper to more clearly articulate how F is trained on clusters but can also evaluate individual data points. This explanation should make it clearer how F operates across different levels of data granularity.\n\n\n\nQ7: Why did the authors not simply use the quality scores to select a subset for finetuning the final MLLM? It seems that the training data and the testing data for the data selector are the same. What is the meaning of performing the testing if we already have the labels on the data?\n\nA7: Thank you for your question. We do not use the \"genuine quality label\" as the criterion for data selection because **different data points in the same subset divided by K-Means++ share the same \"genuine quality label\"**. We need to distinguish these data points in same subsets by conducting a testing stage. \n\nBy employing spectral clustering after the data selector has been trained, it helps to prevent the homogenization of selected data and ensures that our model can handle a variety of data patterns. This diversity is crucial for capturing a wide range of data distribution patterns in the multimodal dataset, which can not be adequately represented if we solely rely on \"genuine quality labels\" for data selection.\n\nWe have made efforts to clarify this distinction and the importance of each clustering step in our revised paper.\n\nQ8: What is the relationship between the validation data used to generate the quality labels and the evaluation data? Why could the model performance on validation data be used as an indicator for model performance on the evaluation (test) data?\n\nA8: Thank you for your question. In our study, **the validation data consists of four widely recognized VQA datasets**. We choose these four VQA datasets to produce \"genuine quality labels\" because they are sufficiently diverse and contain various question-answer pairs.\n\n**The evaluation data, on the other hand, include additional VQA datasets and multiple-choice questions specifically designed for multimodal large language models**. A key dataset we use for evaluation is MME, a standard benchmark in the field. Unlike the validation datasets, MME is not split into training and test sets; it's used entirely for evaluation purposes.\n\nThe relationship between the validation and evaluation datasets is founded on the principle of **generalization**. In real-world applications of MLLMs, the specific downstream tasks may not always be known in advance. Therefore, it's crucial to train and validate models on a range of datasets that collectively encompass a broad scope of possible scenarios and tasks. By ensuring our model performs well on the diverse validation datasets, we can reasonably infer that it will also perform effectively on the evaluation datasets, including MME.\n\nAlthough dividing MME into a training and test set might potentially yield better results due to more targeted training, our approach aims to make multimodal LLM generalized to different tasks. In fact, our validation strategy is designed to strike a balance between specificity and generalizability, ensuring that our model is well-equipped to handle a variety of multimodal tasks."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700341412146,
                "cdate": 1700341412146,
                "tmdate": 1700396027064,
                "mdate": 1700396027064,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YIAenuh3hT",
            "forum": "DNvzCsQG1D",
            "replyto": "DNvzCsQG1D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2243/Reviewer_Wn4g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2243/Reviewer_Wn4g"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method for automatically selecting high-quality diverse subsets of large multimodal instruction datasets. The method works by using a small network to learn the relationship between a set of indicator features and dataset quality (measured through loss on a validation set). Additionally, clustering is performed before data selection in order to ensure the selected dataset is diverse. The authors evaluate over a number of benchmarks and find that their model trained on only 200 examples outperforms a model trained on all available data (MiniGPT-4) and a model trained on 200 randomly chosen examples, showing the efficacy of their data selection method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed data selection method is reasonable and the evaluation seems to indicate it is effective. The method itself is reasonably novel, not just adapting a text-only method for data selection to the multimodal space, but also improving on approaches such as InstructMining through additional steps.\n- Having a more powerful model trained on less data, and a method for automatically selecting quality data, is interesting and useful for improving overall model performance and reducing the compute required to train quality models.\n- The evaluation is reasonably extensive, covering both benchmarks and open-ended (GPT-4-based) evaluation.\n- The authors ablate each aspect of their method (clustering, indicators, data selector) and justify each decision made. In particular, the ablations exploring the use of single indicators may be useful for future examining data quality."
                },
                "weaknesses": {
                    "value": "- As far as I can tell, results with the selected data are from single runs. I worry that when selecting a small number of examples, the variance of the results might be quite high. Seeing results over multiple random seeds with regards to the data selection (training and selection itself) would be useful. However, considering the variance over the random selection is not massive, I think it's clear the selection method is effective in most cases.\n- The authors only explore one model/architecture, so it is unclear how well their method would apply to larger/different models. In particular, as models scale, they benefit less from instruction data, so it is unclear if the rules for selecting quality data derived from smaller models will transfer to larger models.\n- The GPT-4 evaluation is performed over only 60 questions, which seems quite small. In particular, I worry that 60 questions is not enough to thoroughly test the model over a diverse set of queries that may expose gaps in performance due to the small number of examples seen in finetuning compared to a fully-finetuned model. This can be seen e.g. in Table 2, where InstructionGPT-4 performs significantly worse than MiniGPT-4 in commonsense reasoning and OCR tasks. \n\nOverall, I think the work is solid, and presents an interesting and useful method for data selection. It would be nice to see better validation of the method through repeated trials and over different architectures/datasets, but I think the results presented here are solid, with extensive evaluation."
                },
                "questions": {
                    "value": "- Why do you think there is a big drop in MME performance when moving from two layers to three layers in table 5?\n- In Figure 3, it appears that MiniGPT outperforms IntructionGPT-4 in a smaller number of tasks, especially looking at the natural relation and object localisation task. Why do you think this is?\n- It would be useful and interesting to see when using the data selection method becomes no better than random selection (if this happens) when increasing the number of data points selected (i.e. extending Figure 7 and adding a line for random data selection)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2243/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2243/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2243/Reviewer_Wn4g"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2243/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698611905473,
            "cdate": 1698611905473,
            "tmdate": 1700331103300,
            "mdate": 1700331103300,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qzystewtB6",
                "forum": "DNvzCsQG1D",
                "replyto": "YIAenuh3hT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Wn4g (1/2)"
                    },
                    "comment": {
                        "value": "Q1: Seeing results over multiple random seeds with regards to the data selection (training and selection itself) would be useful.\n\nA1: Thanks for your suggestion. Our main motivation is to prove that less but high-quality instruction data can outperform the whole dataset. In particular, we want to find out **if there exists a subset making InstructionGPT-4 achieve better performance**. As a result, our proposed method indeed shows the existence of the subset. \n\nMoreover, in response to your suggestion, we conduct additional experiments to try different random seeds. These experiments help us evaluate the stability and generalizability of our selection method under different initialization conditions. Our experimental results are shown below:\n\n| Benchmark |  Seed1  |  Seed2  |  Seed3  | MiniGPT-4 |\n|:---------:|:-------:|:-------:|:-------:|:---------:|\n| MME       |  648.26 |  637.58 |  628.99 |   625.20  |\n| VQA       |  22.30  |  21.78  |  22.14  |   20.54   |\n\n\nWe observe that the performance variance across different seeds is not obvious. This consistency of the performance underscores the effectiveness of our data selection method, suggesting that it is not overly sensitive to the initial random seed and can reliably identify high-quality data subsets across different scenarios. We also add these experiments to the revised version.\n\nQ2: The authors only explore one model/architecture, so it is unclear how well their method would apply to larger/different models. In particular, as models scale, they benefit less from instruction data, so it is unclear if the rules for selecting quality data derived from smaller models will transfer to larger models.\n\nA2: Thank you for your comments. Following LIMA [1] that proposes \"less is more for alignment\" only using LLaMA in textual modal, our main motivation is to prove that less but high-quality instruction data can outperform the whole dataset for multimodal LLMs, which is also mentioned in A1 above. Besides, we want to focus on MiniGPT-4 because it's the first and most popular multimodal model utilizing LLM. \nTo address your concern about scalability and generalizability, we also extend our investigation to include additional models and datasets. **Notably, we have applied our method to Qwen_VL [2] and the detail_10k [3] dataset without retraining the data selector**.\nHere are our experimental results:\n\n| detail_10k   | Qwen_VL (1k) | Qwen_VL (2k) | Qwen_VL (10k) | MiniGPT-4 (1k) | MiniGPT-4 (2k) | MiniGPT-4 (10k) |\n|:------------:|:------------:|:------------:|:-------------:|:--------------:|:--------------:|:---------------:|\n| MME          |    1802.50   |   **1806.81**|    1769.28    |   **614.04**   |      608.54    |      604.47     |\n| Perception   |    1423.57   |   **1426.10**|    1398.92    |   **434.04**   |      431.04    |      428.54     |\n| Cognition    |    378.93    |   **380.71** |    370.36     |   **180.00**    |      177.50    |      173.93     |\n\nBy comparing the performance between models tuned from selected subsets and the whole dataset, we observe that less instruction data for better performance still work for various MLLMs. Our findings indicate promising results, suggesting that our method is not only effective with MiniGPT-4 but also exhibits potential for broader applicability. \n\n[1] Zhou, Liu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.\n\n[2] Bai, et al. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.\n\n[3] Liu, Li, et al. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.\n\nQ3: The GPT-4 evaluation is performed over only 60 questions, which seems quite small. In particular, I worry that 60 questions is not enough to thoroughly test the model over a diverse set of queries that may expose gaps in performance due to the small number of examples seen in finetuning compared to a fully-finetuned model.\n\nA3: Thank you for your comments. Firstly, in our evaluation, we want to cover both close-ended and open-ended benchmarks. For open-ended benchmarks, **LLaVA-Bench is the only one requiring GPT-4 for evaluation proposed during our work to our best knowledge**. Secondly, while LLaVA-Bench only contains 60 questions, it includes suffcient senarios such as indoor and outdoor scenes, memes, paintings, and sketches, and each image is associated with a carefully designed selection of high-quality questions. Thirdly, LLaVA-Bench is widely adopted by most recent related works, such as [1] and [2]. In addition, besides LLaVA-Bench, we also conduct sufficient closed-ended experiments, showing the effectiveness of our proposed method. We will try other multimodal open-ended benchmarks as future work. \n\n[1] Sun, Shen, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023.\n\n[2] Liu, Li, et al. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700341075553,
                "cdate": 1700341075553,
                "tmdate": 1700341143399,
                "mdate": 1700341143399,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HZqA51k13K",
                "forum": "DNvzCsQG1D",
                "replyto": "YIAenuh3hT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Wn4g (2/2)"
                    },
                    "comment": {
                        "value": "Q4: Why do you think there is a big drop in MME performance when moving from two layers to three layers in table 5?\n\nA4: Thanks for your question. Given a fixed size of training set, **more layers containing more parameters tend to overfit**. Therefore, there will be a drop in MME performance when the number of layers increases. In our specific case, we cluster the datasets into 30 groups for training so that the training set is extremely small, and thus a drop in MME performance was observed from two layers to three layers.\n\nQ5: In Figure 3, it appears that MiniGPT outperforms IntructionGPT-4 in a smaller number of tasks, especially looking at the natural relation and object localisation task. Why do you think this is?\n\nA5: Indeed, in a limited set of tasks, such as natural relation and object localization, MiniGPT-4 appears to outperform InstructionGPT-4. This outcome aligns with our core objective, which is to demonstrate that **InstructionGPT-4 can surprisingly achieve superior performance in most tasks using a significantly reduced dataset** (only 6% of the total data).\n\nWhen we increase the size of selected data, InstructionGPT-4 does start to lead in these specific tasks. However, our primary goal is not to make InstructionGPT-4 outperform MiniGPT-4 across every individual task, but rather to showcase the effectiveness of using a well-curated subset for training large multimodal models. This is a significant finding in itself, as it suggests that careful data selection can lead to substantial improvements in model performance without the need for extensive datasets.\n\nAdditionally, it is important to recognize that large multimodal models may not excel in all types of tasks. For instance, in tasks like OCR, we observe that when these models integrate image information with large language model capabilities, there can be a loss of detail due to the compression of image tokens. This results in a reduced granularity of information, which in turn weakens their OCR performance. This specific limitation highlights a current challenge in the field and suggests a direction for future research. \n\nQ6: It would be useful and interesting to see when using the data selection method becomes no better than random selection (if this happens) when increasing the number of data points selected (i.e. extending Figure 7 and adding a line for random data selection).\n\nA6: Thank you for this suggestion. We have incorporated this experiment into our revised paper, extending the analysis in Figure 7 to include a comparison with random data selection. We also display the results below:\n\n| Data Size             |    50   |   100  |   150  |   200  |   250  |   300  |   400  |   600  |   800  |  1000  |\n|:---------------------:|:-------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|\n| Data Selector (MME)   |  515.98 | 599.59 | 617.17 | 648.26 | 648.97 | 649.68 | 651.83 | 643.00 | 622.21 | 598.44 |\n| Random Selection (MME)|  435.94 | 535.93 | 523.63 | 527.26 | 602.12 | 615.82 | 608.07 | 612.10 | 622.19 | 616.75 |\n| Data Selector (VQA)   |  22.75  |  22.03 |  21.99 |  22.30 |  22.17 |  21.88 |  21.92 |  21.67 |  21.00 |  20.70 |\n| Random Selection (VQA)|  20.30  |  20.62 |  19.83 |  19.87 |  20.21 |  20.23 |  21.36 |  20.73 |  20.38 |  20.68 |\n\n\nOur findings reveal that our data selection method outperforms random selection in all cases except the size of 1000.\nThrough our observation, there is an interesting trend as the size of the selected subset increases: the performance gap between our method and random selection begins to narrow.\n\nThis decrease in the performance differential can be attributed to the nature of our data selection process. As the size of the subset selected by our method increases, it inevitably starts to include more data points of lower quality. This inclusion of lower-quality data diminishes the overall effectiveness of the selected subset, thereby reducing the gap in performance compared to a randomly selected subset.\n\nDespite this trend, it is important to note that our data selection method continually maintains a performance advantage over random selection in most cases. This finding underscores the efficacy of our approach, particularly when working with smaller subsets. It also highlights a key insight: the quality of data, not just the quantity, is crucial for improving model performance.\n\nWe believe these results add a valuable dimension to our analysis, providing a clearer understanding of the scalability and efficiency of our data selection method. The revised paper includes a detailed discussion of these findings and their implications for the use of data selection in training large multimodal models."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700341108847,
                "cdate": 1700341108847,
                "tmdate": 1700341108847,
                "mdate": 1700341108847,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "T6Lxxg28Y4",
            "forum": "DNvzCsQG1D",
            "replyto": "DNvzCsQG1D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2243/Reviewer_dCjW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2243/Reviewer_dCjW"
            ],
            "content": {
                "summary": {
                    "value": "The authors performed a series of steps aimed to selecting training data for multimodal large langauge models (MLLMs). They first extracted freatures using a number of heuristics, such as CLIP score, length of the answer, and so on. After that, they cluster the data, train miniGPT-4 on each cluster, and test the results on VQA datasets to get \"genuine quality labels\". In the next step, they train a network to predict quality labels from the feature vectors. Finally, they recluster the data, and use the trained network to select the best data points. \n\nThe authors compared against the original miniGPT and a random baseline for data selection. The selected 200 data points outperform both."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper achieves good results. 200 data points they selected did lead to performance improvements over the original miniGPT-4. \n\nThe paper contains few grammatical errors. I thank the authors for proofreading the paper."
                },
                "weaknesses": {
                    "value": "The most important weakness of the paper is the lack of baselines. Many data quality and data pruning metrics have been proposed over the years. The authors should consider comparing against Influence Functions, Data Shapley, HyDRA, Diverse Ensembles [1], prototypicality [2], etc. A random baseline is way too weak. \n\n1. Kristof Meding, Luca M. Schulze Buschoff, Robert Geirhos, and Felix A. Wichmann. Trivial or impossible\u2014dichotomous data difficulty masks model differences (on ImageNet and beyond). In International Conference on Learning Representations, 2022.\n\n2. Sorscher, Ben, et al. Beyond neural scaling laws: beating power law scaling via data pruning. NeurIPS. 2022.\n\nIt is not clear how the propose method could scale or generalize to different datasets and models, as it is only tested on a single dataset and a single MLLM. It is also not clear why only four VQA datasets are selected to produce \"genuine quality labels\", whereas the model is evaluated on bigger benchmarks. \n\nThe writing contains a number of vague descriptions and logic disconnects (see below and the questions). The authors seem to be in a habit of fancy word choices, which may be the result of some large language model. However, word choices, however literary, cannot compensate for logic issues. \n\n- Page 3: ... steer multimodal language models in learning to generate responses in a particular manner --> what manner is that?\n- Page 4: .. can more completely cover the various aspects of multimodal data quality -> More than what? What exactly are the aspects of multimodal data quality?\n- Page 4: In equation (1), the dimensionality reduction (DR) technique is written as P(f(x_image), g(x_response)). Does it mean that DR is performed on the concatenation of image features and text features?\n- Page 5: I don't understand what is x in the unnumbered equations of Section 2.3. Is x a data point or a cluster? Btw, it's a good idea to number all equations, in case your reviewers want to refer to them. \n- Page 5: What features is the data selector F trained with? The text says \"we concatenate these embeddings into a single composite embedding\". If a data point has D-dimensional features, and a cluster contains K data points, do we have a KD-dimensional feature vector? If that's the case, how can we apply F on a single data point, as implied in the later part of Section 2.3?"
                },
                "questions": {
                    "value": "- How do you make sure K-means and spectral clustering return clusters of equal size?\n- What is the purpose of reclustering the data points? Why not use the \"genuine quality label\" as the criterion for data selection?\n- Why is length such an important feature? Although the feature is very simple, it alone achieves the third best result in the ablation study of Table 5, only inferior to CLIP and the whole feature set. Does this imply the data selection process is utilizing a shortcut feature overfitted to the test?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2243/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2243/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2243/Reviewer_dCjW"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2243/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743257134,
            "cdate": 1698743257134,
            "tmdate": 1700748181557,
            "mdate": 1700748181557,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pR1Gqi7x4I",
                "forum": "DNvzCsQG1D",
                "replyto": "T6Lxxg28Y4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dCjW (1/4)"
                    },
                    "comment": {
                        "value": "Q1: The most important weakness of the paper is the lack of baselines. Many data quality and data pruning metrics have been proposed over the years. The authors should consider comparing against Influence Functions, Data Shapley, HyDRA, Diverse Ensembles [1], prototypicality [2], etc. A random baseline is way too weak.\n\nA1: Thanks for your suggestion. We would like to highlight that **we are the first to propose a data selection method working on multimodal LLM for generative tasks**. Since there are no other data seletion methods for multimodal LLMs, we have to compare our method with single-modal baselines in our ablation study. In particular, **we compare our proposed selector with the recent methods such as Alpagasus (GPT Score only) in Table 5 and Instruction Mining (Linear Selector) in Figure 5**, demonstating that data selected by our multimodal selector works much better than these two methods.  \nWhile our data selection method and data pruning methods both aim to reduce the size of training data, we would like to emphasize several important differences.\nOur data selection method is used for multimodal LLMs in generative tasks. In contrast, data pruning methods are mostly used for vision models (e.g., ResNet, ViT) in classification tasks.\nMoreover, in response to your suggestion, we conduct additional experiments on data pruning methods including EL2N [1] and prototypicality [2]. We represent our experiment results below:\n      \n| Benchmark | InstructionGPT-4 |  EL2N  | Prototypicality | Random |\n|:---------:|:----------------:|:------:|:---------------:|:------:|\n| MME       |      **648.26**      | 627.27 |     569.46      | 527.26 |\n| VQA       |      **22.30**       |  20.62 |     21.83       |  19.87 |\n\n\n\nWe observe that these previous methods can't achieve competitve performance for multimodal LLM compared to our data selector. It showcases that our novel data selection method for multimodal datasets is quite necessary. We also add these experiments to the revised version along with the above detailed discussion.\n\n[1] Paul, Ganguli, Dziugaite G K. Deep learning on a data diet: Finding important examples early in training. NeurIPS, 2021.\n\n[2] Sorscher, Ben, et al. Beyond neural scaling laws: beating power law scaling via data pruning. NeurIPS, 2022.\n\nQ2: It is not clear how the propose method could scale or generalize to different datasets and models, as it is only tested on a single dataset and a single MLLM.\n\nA2: Thank you for your comments. Following LIMA [1] that proposes \"less is more for alignment\" only using LLaMA in textual modal, our main motivation is to prove that less but high-quality instruction data can outperform the whole dataset for multimodal LLMs. In particular, we want to **find out if there exists a subset making InstructionGPT-4 achieve better performance**. As a result, our proposed method indeed shows the existence of the subset. Besides, we want to focus on MiniGPT-4 because it's the first and most popular multimodal model utilizing LLM. \nTo address your concern about scalability and generalizability, we also extend our investigation to include additional models and datasets. **Notably, we have applied our method to Qwen_VL [2] and the detail_10k dataset [3] without retraining the data selector**.\nHere are our experimental results:\n\n| detail_10k   | Qwen_VL (1k) | Qwen_VL (2k) | Qwen_VL (10k) | MiniGPT-4 (1k) | MiniGPT-4 (2k) | MiniGPT-4 (10k) |\n|:------------:|:------------:|:------------:|:-------------:|:--------------:|:--------------:|:---------------:|\n| MME          |    1802.50   |   **1806.81**|    1769.28    |   **614.04**   |      608.54    |      604.47     |\n| Perception   |    1423.57   |   **1426.10**|    1398.92    |   **434.04**   |      431.04    |      428.54     |\n| Cognition    |    378.93    |   **380.71** |    370.36     |   **180.00**    |      177.50    |      173.93     |\n\n\nBy comparing the performance between models tuned from selected subsets and the whole dataset, we observe that less instruction data for better performance still work for various MLLMs. Our findings indicate promising results, suggesting that our method is not only effective with MiniGPT-4 but also exhibits potential for broader applicability. \n\n[1] Zhou, Liu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.\n\n[2] Bai, et al. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.\n\n[3] Liu, Li, et al. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700340552293,
                "cdate": 1700340552293,
                "tmdate": 1700396834885,
                "mdate": 1700396834885,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dsVF4se1jf",
                "forum": "DNvzCsQG1D",
                "replyto": "T6Lxxg28Y4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dCjW (2/4)"
                    },
                    "comment": {
                        "value": "Q3: It is also not clear why only four VQA datasets are selected to produce \"genuine quality labels\", whereas the model is evaluated on bigger benchmarks.\n\nA3: Thanks for your question. We choose these four VQA datasets to produce \"genuine quality labels\" because **they are sufficiently diverse and contain various question-answer pairs**. In particular, GQA focuses on reasoning skills and combined language understanding skills; IconQA requires perceptual skills such as object recognition and text understanding; OKVQA is a large-scale dataset requiring external knowledge; ScienceQA can well diagnose whether the multimodal LLM has multi-step reasoning capabilities and interpretability. These four datasets are wide enough to cover multiple aspects of multimodal tasks. Thus, we choose to select these four datasets to produce \"genuine quality labels\".\n\nQ4: Page 3: ... steer multimodal language models in learning to generate responses in a particular manner --> what manner is that?\n\nA4: Thanks for your question. In our paper, **the \"manner\" refers to the stylistic and structural characteristics of the generated content**, which are significantly influenced by the the instruction data. Instruction data can greatly affect models' generation during instruction tuning. Thus, **the manner of the generated content of the multimodal model is closely related to the manner of the instruction data**. To illustrate, if the instruction data mostly consist of lengthy and complex sentences, the model is more possible to generate responses that follow this style -- extensive and intricate. Conversely, if the instruction data are composed of concise, straightforward sentences, the model tends to produce briefer and more direct responses.  We have edited this to a clearer description in the revised version.\n\nQ5: Page 4: .. can more completely cover the various aspects of multimodal data quality -> More than what? What exactly are the aspects of multimodal data quality?\n\nA5: Thanks for your question. In our paper,  it means that **more than using only one quality method to define multimodal data quality**, which is mentioned behind this sentence: \"Using a single score to filter data can be useful, but it may not provide a comprehensive measure of data quality. Therefore, it is necessary to combine multiple indicators as an embedding to assess data quality collectively\". When we state that our method \"more completely covers the various aspects of multimodal data quality\", we are comparing it to approaches that rely on a single metric for data quality assessment. Specifically, some existing methods, like Alpagasus [1], use the GPT score to evaluate textual data quality. However, we argue that multimodal data quality is multi-faceted and cannot be accurately captured by a single metric. To address this problem, our approach incorporates a combination of metrics: CLIP score, length score, reward score, and GPT score. This multi-metric method allows for a more complete evaluation of multimodal data quality. \n\n[1] Chen, Li, et al. Alpagasus: Training a better alpaca with fewer data. arXiv preprint arXiv:2307.08701, 2023.\n\nQ6: Page 4: In equation (1), the dimensionality reduction (DR) technique is written as P(f(x_image), g(x_response)). Does it mean that DR is performed on the concatenation of image features and text features?\n\nA6: Thanks for your question. To clarify, in our method, **dimensionality reduction is applied separately to the image and text features before concatenation**. Once these features have been independently reduced to the same dimension, we then concatenate them. This process is visually represented in Figure 2 of our paper. This approach ensures that both image and text features are compressed and represented before they are combined, which helps to preserve the unique characteristics of both visual and textual data and allow for an effective integration of multimodal data. \n\nWe have amended the relevant section to better explain the sequential steps of dimensionality reduction followed by concatenation to clear up any ambiguities regarding our process.\n\nQ7: Page 5: I don't understand what is x in the unnumbered equations of Section 2.3. Is x a data point or a cluster?\n\nA7: Thanks for your question. From our paper in Section 2.3 Testing: \"Given a multimodal dataset D of triplets x = (image, instruction, answer)\", **x refers to an individual data point in our multimodal dataset D**. Specifically, each data point x is a triplet containing an image, an instruction, and an answer."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700340619529,
                "cdate": 1700340619529,
                "tmdate": 1700340772941,
                "mdate": 1700340772941,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HmvjCPvyV6",
                "forum": "DNvzCsQG1D",
                "replyto": "T6Lxxg28Y4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dCjW (3/4)"
                    },
                    "comment": {
                        "value": "Q8: Page 5: What features is the data selector F trained with? The text says \"we concatenate these embeddings into a single composite embedding\". If a data point has D-dimensional features, and a cluster contains K data points, do we have a KD-dimensional feature vector? If that's the case, how can we apply F on a single data point, as implied in the later part of Section 2.3?\n\nA8: Thank you for your inquiry regarding the features used to train the data selector F. Each data point in our dataset is represented by a D-dimensional feature vector. When we form a cluster of K data points, these D-dimensional vectors are concatenated to create a feature vector with a shape of (K, D). **In training the data selector F, we utilize these (K, D) shape vectors**. Each cluster, represented by its (K, D) vector, is associated with a \"genius quality label\" that reflects the collective quality of all K data points within the cluster. This approach allows us to train the data selector on a rich and detailed representation of data clusters.\nRegarding the application of F on a single data point as discussed in Section 2.3, it's important to clarify that F is trained on clusters but is capable of evaluating individual data points as well. **When applied to a single data point, F operates on its (1, D) feature vector**. This flexibility is a key aspect of F, allowing it to function effectively both at the cluster level during training and at the individual data point level during testing.\n\nTo address the potential confusion this might cause, we have revised the relevant sections of our paper to more clearly articulate how F is trained on clusters but can also evaluate individual data points. This explanation should make it clearer how F operates across different levels of data granularity.\n\nQ9: The writing contains a number of vague descriptions and logic disconnects (see below and the questions). The authors seem to be in a habit of fancy word choices, which may be the result of some large language model. However, word choices, however literary, cannot compensate for logic issues.\n\nA9: Thanks for your comment. We have undertaken a thorough review of our manuscript, focusing specifically on areas where the descriptions were vague or the logic was unclear. We also pay particular attention to the sections and points you've highlighted, ensuring that our arguments are presented clearly and logically.  We believe these revisions will address the issues you've raised and enhance the overall readability and coherence of our paper.\n\nQ10: How do you make sure K-means and spectral clustering return clusters of equal size?\n\nA10:  Thanks for your question. In the selector's training stage, we observe that the data quantity in each group from K-means clustering is roughly the same, while other clustering algorithms results in more significant differences in group sizes. This is a characteristic of K-means, as it partitions data into clusters that minimize the variance within each cluster. To acquire genuine quality labels (which necessitate training a multimodal model on each cluster group for evaluation) without letting the quantity of data impact the capabilities of the trained multimodal models, we employ a simple post-processing technique. This involves identifying clusters with either excessively high or low sample counts, and **redistributing some samples from the larger clusters to the smaller ones based on their distance to different cluster centroids**, thereby equalizing the number of samples in each cluster.\n\nIn the testing stage, we use spectral clustering to select a diverse set of data points. Unlike in the training stage, the requirement here is not to have clusters of equal size but rather to select the top 200 data points that best represent the diversity of the dataset. Hence, in this stage, the equal size of clusters is not a concern as our focus shifts to ensuring the diversity and representativeness of the selected data.\n\nWe have revised our paper to clearly explain this differentiation in the use of clustering techniques between the training and testing stages, and how we ensure appropriate cluster sizes and data diversity in each stage."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700340660967,
                "cdate": 1700340660967,
                "tmdate": 1700340793428,
                "mdate": 1700340793428,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZqN4rF0B4p",
                "forum": "DNvzCsQG1D",
                "replyto": "T6Lxxg28Y4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2243/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dCjW (4/4)"
                    },
                    "comment": {
                        "value": "Q11: What is the purpose of reclustering the data points? Why not use the \"genuine quality label\" as the criterion for data selection?\n\nA11: Thank you for your question, which we address in Appendix A.3 of our paper. The primary purpose of reclustering data points using **spectral clustering** (denoted as \u039b) in the data selection stage is to **ensure the diversity of the selected data**. This diversity is crucial for capturing a wide range of data distribution patterns in the multimodal dataset, which can not be adequately represented if we solely rely on \"genuine quality labels\" for data selection.\n\nTo elaborate, the initial clustering using **K-Means++** (denoted as \u0393) serves to **divide the data into subsets for obtaining \"genuine quality labels\"**. These labels are assigned based on the collective characteristics of data points within each subset. While these labels are valuable for assessing overall data quality, they may not fully capture the diversity within the dataset. This is where spectral clustering comes into play.\n\nWe do not use the \"genuine quality label\" as the criterion for data selection because **different data points in the same subset divided by K-Means++ share the same \"genuine quality label\"**. We need to distinguish these data points in same subsets by conducting a testing stage. By employing spectral clustering after the data selector has been trained, it helps to prevent the homogenization of selected data and ensures that our model can handle a variety of data patterns. \n\nIn addition, **we have conducted detailed experiments to study the effectiveness of applying spectral clustering within the data selector mechanism in Figure 5**. These experiments demonstrate the significance of maintaining diverse vision-language instruction data and validate the necessity of the reclustering process.\nWe have made efforts to clarify this distinction and the importance of each clustering step in our revised paper, ensuring a clearer understanding of our methodology and the rationale behind the use of different clustering techniques.\n\nQ12: Why is length such an important feature? Although the feature is very simple, it alone achieves the third best result in the ablation study of Table 5, only inferior to CLIP and the whole feature set. Does this imply the data selection process is utilizing a shortcut feature overfitted to the test?  \n\nA12: Thank you for your question. The significance of the length feature in our study indicates that there is a meaningful correlation between the length of the data and its quality or utility for our specific task. \n\nHowever, **strong correlation and shortcuts are two different things**. Our experimental results in Table 5 only demonstrate strong correlation between the length indicator and data quality, but they do not confirm whether the combination of the four features results in learning shortcuts. Solely based on this experiment, it's inconclusive. We cannot assert that the learning of our data selector is focused on length just because the performance of using all the features together is similar to using only length on a single dataset.\n\nIn fact, to properly clarify that the effectiveness of the length feature doesn't imply an overfitting issue, we should refer to experiments in Tables 2, 3, and 4, which show the evaluation across multiple datasets with significant domain gaps. To ensure robustness against overfitting, we've employed a diverse set of datasets for training and testing. Specifically, we use four different VQA datasets for training. For testing, we apply our method to other VQAs, MME, and MMBench datasets. The significant domain gap between these datasets helps mitigate the risk of overfitting to a particular data characteristic. We observe that InstructionGPT-4 outperforms MiniGPT-4 in all these tasks. It can be concluded that InstructionGPT-4 still demonstrates a strong generation ability to out-domain evaluation datasets.\n\nIn conclusion, **as there is no overfitting observed across these datasets, it indicates that length is not a shortcut**.\nIn light of your query, we have added additional explanations in our revised paper to articulate more clearly why the length feature is impactful and to assure readers that our results aren\u2019t a product of overfitting to test data, but a reflection of genuine correlations found in diverse datasets."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700340835492,
                "cdate": 1700340835492,
                "tmdate": 1700341678241,
                "mdate": 1700341678241,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]